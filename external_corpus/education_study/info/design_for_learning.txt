Design for Learning

Principles, Processes, and Praxis

Jason K. McDonald & Richard E. West
Table of Contents

Introduction                                                             1

Part I. Instructional Design Practice                                    5

Understanding                                                            7

1. Becoming a Learning Designer                                          9

2. Designing for Diverse Learners                                        21

3. Conducting Research for Design                                        39

4. Determining Environmental and Contextual Needs                        53

5. Conducting a Learner Analysis                                         65

Exploring                                                                79

6. Problem Framing                                                       81

7. Using Task Analysis to Inform Instructional Design                    91

8. Documenting Instructional Design Decisions                            109

Creating                                                                 119

9. Generating Ideas                                                      121

10. Instructional Strategies                                             133

11. Instructional Design Prototyping Strategies                          139

Evaluating                                                               153

12. Design Critique                                                      155

13. The Role of Design Judgment and Reflection in Instructional Design   169

14. Instructional Design Evaluation                                      179

15. Continuous Improvement of Instructional Materials                    195

Part II. Instructional Design Knowledge                                  205

Sources of Design Knowledge                                              207

16. Learning Theories                                                    209

17. The Role of Theory in Instructional Design                           221

18. Making Good Design Judgments via the Instructional Theory Framework  231

19. Using Theory as a Learning and Instructional Design Professional     245
20. The Nature and Use of Precedent in Designing                                      253

21. Standards and Competencies for Instructional Design and Technology Professionals  265

Instructional Design Processes                                                        275

22. Design Thinking                                                                   277

23. Robert Gagné and the Systematic Design of Instruction                             289

24. Designing Instruction for Complex Learning                                        299

25. Curriculum Design Processes                                                       307

26. Agile Design Processes and Project Management                                     319

Designing Instructional Activities                                                    333

27. Designing Technology-Enhanced Learning Experiences                                335

28. Designing Instructional Text                                                      347

29. Audio and Video Production for Instructional Design Professionals                 363

30. Using Visual and Graphic Elements While Designing Instructional Activities        377

31. Simulations and Games                                                             389

32. Designing Informal Learning Environments                                          405

33. The Design of Holistic Learning Environments                                      413

34. Measuring Student Learning                                                        419

Design Relationships                                                                  431

35. Working With Stakeholders and Clients                                             433

36. Leading Project Teams                                                             441

37. Implementation and Instructional Design                                           453

Appendices                                                                            459

Author Biographies                                                                    461
EdTech Books

CC BY-NC: This work is released under a CC BY-NC license, which means that you are free to do with it as you please
as long as you (1) properly attribute it and (2) do not use it for commercial gain.
The publisher EdTech Books does not have a physical location, but its primary support staff operate out of Provo,
UT, USA.
The publisher EdTech Books makes no copyright claim to any information in this publication and makes no claim as
to the veracity of content. All content remains exclusively the intellectual property of its authors. Inquiries regarding
use of content should be directed to the authors themselves.
DOI: 10.59668/id
ISBN: 978-0-578-85497-7
URL: https://edtechbooks.org/id
McDonald, J. K. & West, R. E. (2021). Design for Learning: Principles, Processes, and Praxis (1st ed.). EdTech Books.
https://dx.doi.org/10.59668/id
 Stephen Downes

 National Research Council of Canada

 This open access book came out last year but I didn't get around to reading it
 until today. It's part of a series from EdTech Books. The structure of the book
 (434 page PDF) is captured in the title. The first part focuses on the design
 practice itself, while the second part underpins that practice in theory. It makes
 an excellent introductory text, covering a lot of ground, but in limited depth. The
 book's 36 chapters, each written by different authors, are short (10-15 pages)
 overviews of their topics. The presentation is often visual, using diagrams and
 tables, and many chapters have application exercises (in OLDaily,
 https://www.downes.ca/post/74179).

Jason K. McDonald

Brigham Young University

Dr. Jason K. McDonald is a Professor of Instructional Psychology & Technology at
Brigham Young University. He brings twenty-five years of experience in industry and
academia, with a career spanning a wide-variety of roles connected to instructional
design: face-to-face training; faculty development; corporate eLearning; story
development for instructional films; and museum/exhibit design. He gained this
experience as a university instructional designer; an executive for a large,
international non-profit; a digital product director for a publishing company; and as
an independent consultant.

Dr. McDonald's research focuses around advancing instructional design practice
and education. In particular, he studies the field's tendency to flatten/redefine
educational issues in terms of problems that can be solved through the design of
technology products, and how alternative framings of the field's purpose and
practices can resist these reductive tendencies.

At BYU, Dr. McDonald has taught courses in instructional design, using stories for
learning purposes, project management, learning theory, and design theory. His
work can be found at his website: http://jkmcdonald.com/
Richard E. West

Brigham Young University

Dr. Richard E. West is an associate professor of Instructional Psychology and
Technology at Brigham Young University. He teaches courses in instructional
design, academic writing, qualitative research methods, program/product
evaluation, psychology, creativity and innovation, technology integration skills for
preservice teachers, and the foundations of the field of learning and instructional
design technology.

Dr. West's research focuses on developing educational institutions that support
21st century learning. This includes teaching interdisciplinary and collaborative
creativity and design thinking skills, personalizing learning through open badges,
increasing access through open education, and developing social learning
communities in online and blended environments. He has published over 90
articles, co-authoring with over 80 different graduate and undergraduate students,
and received scholarship awards from the American Educational Research
Association, Association for Educational Communications and Technology, and
Brigham Young University.

He tweets @richardewest, and his research can be found on
http://richardewest.com/

Like this? Endorse it and let others know.  Endorse
Introduction

Jason K. McDonald & Richard E. West

Our purpose in this book is twofold. First, we introduce the basic skill set and knowledge base used by practicing
instructional designers. We do this through chapters contributed by experts in the field who have either academic,
research-based backgrounds, or practical, on-the-job experience (or both). Our goal is that students in introductory
instructional design courses will be able to use this book as a guide for completing a basic instructional design project.
We also hope the book is useful as a ready resource for more advanced students or others seeking to develop their
instructional design knowledge and skills.

Our second purpose complements the first: to introduce instructional designers to some of the most current views on
how the practices of design thinking contribute towards the development of effective and engaging learning
environments. While some previous books have incorporated elements of design thinking (for example, processes like
prototyping), to date no instructional design textbook focuses on design-oriented thinking as the dominant approach for
creating innovative learning systems. Our aim is to provide resources to faculty and students for learning instructional
design in a manner consistent with a design-oriented worldview. But because the classic approaches to instructional
design are still important for many professionals, we also include chapters that introduce some of the traditional,
systematic processes for designing instructional environments. We hope this blend of traditional and innovative views
provides readers with a competitive advantage in their own work, providing them with a larger set of conceptual tools to
draw on as they address the professional challenges they face.

This book is divided into two major sections. The first, Instructional Design Practice, covers how instructional
designers understand, explore, create, and evaluate situations requiring educational interventions and the products or
systems used to support them. In this section, chapters address how we understand diverse learners and their needs;
how to explore and frame the educational problems one is solving; how to analyze the context and tasks associated
with the problems; how to iteratively generate decisions, prototypes, and solutions; and how to evaluate and understand
the effectiveness of an instructional design.

The second part, Instructional Design Knowledge, covers the sources of design knowledge, a variety of instructional
design processes, approaches for designing instructional activities, and the relationships important for instructional
design practice. This section includes chapters addressing learning/instructional theory, design precedent, both
systematic and agile design processes, and practical strategies for using technology wisely, managing projects, and
creating instructional activities.

This book was developed as part of the EdTechBooks.org library of open textbooks. Thus, this book is openly licensed
(CC-BY-NC) and free to use, reuse, revise, remix, and redistribute, with proper citation. This platform provides many
innovative features for students and faculty, including the following:

                                                                                         1
      Openly Licensed for Continuous Improvement--Because the book is openly licensed, it can be updated
      continuously as needed. If you notice errors in the book or content that is out of date, please inform us or the
      author of the chapter.
      Chapter Surveys--At the end of each chapter is a survey to provide feedback on the chapter's content and writing.
      Please fill out these surveys as they will help us to improve future versions of the book.
      Available for Customization--Because of its open license, each department can customize the book to meet their
      needs, including customization to support both graduate and undergraduate education. The following is potential
      wording you could use in your remixed version of the book: "This textbook is a revision of Design for Learning:
      Principles, Processes, and Praxis, available at https://edtechbooks.org/id edited by Dr. Jason K. McDonald and Dr.
      Richard E. West of Brigham Young University."
      Different Versions To Improve Accessibility--Each chapter can be read online or downloaded as a PDF for offline
      reading, in addition to audio versions of some chapters. You can also share the book or any chapter through the QR
      codes available in the top right of the window, or the social media icons.
      Online/Social Annotation--Online and social annotation of the chapters is possible through Hypothes.is integration
      (free Hypothes.is accounts available at https://web.hypothes.is/), through a menu available in the upper right of the
      window.
      Analytics--Powerful chapter/book analytics provide authors with data about the significance of their work.
To cite a chapter from this book in APA, please use the suggested citation found at the chapter's end.
If you are an instructor who has adopted this book for a course, or modified/remixed the book for a course, please
complete the following survey so that we can know about your use of the book and update you when we push out new
versions of any chapters.
Survey to Receive Updates
If you are willing, we would appreciate your feedback on the quality of this textbook, along with a short review paragraph
that we can use in promoting this book. To contribute your review Go to this survey.
To contribute a resource to a chapter (e.g. multimedia element, quiz question, application exercise), fill out this survey.
One of the exciting things about the field of instructional design and learning technology is how quickly it evolves. As
soon as new technologies are introduced we see instructional designers experimenting with how they might be put to
use for learning purposes. The same is true regarding new scientific findings from psychology, sociology,
communications, or other human sciences, with professionals in our field scrutinizing them to understand what
relevance they might have for improving the learning or teaching process. We hope this book becomes a similar,
cutting-edge resource that helps readers implement our growing understanding regarding how to design effective and
engaging learning environments.
Good luck!

                                                                                         2
This content is provided to you freely by EdTech Books.
Access it online or download it at https://edtechbooks.org/id/introduction.

                                                          3
4
  Part I

Instructional Design Practice

The first part of this book concerns instructional design practice, or what is it that makes instructional design
recognizable as instructional design. Of course, instructional design shares many activities, methods, processes, and
techniques with other design, education, or social science fields. So one should focus on instructional design as a
particular constellation of practices, the particular configuration of which being what allows instructional designers to
make the unique contributions they are prepared to make.
We divide our study of instructional design practice into four subsections - understanding, exploring, creating, and
evaluating - each of which consists of 3 - 5 chapters.

     Understanding
          Becoming a Learning Designer
          Designing for Diverse Learners
          Conducting Research for Design
          Determining Environmental and Contextual Needs
          Conducting a Learner Analysis

     Exploring
          Problem Framing
          Using Task Analysis to Inform Instructional Design
          Documenting Instructional Design Decisions

     Creating
          Generating Ideas
          Instructional Strategies
          Instructional Design Prototyping Strategies

     Evaluating
          Design Critique

                                                                                         5
The Role of Design Judgment and Reflection in Instructional Design
Instructional Design Evaluation
Continuous Improvement of Instructional Materials

                 This content is provided to you freely by EdTech Books.
                 Access it online or download it at https://edtechbooks.org/id/instructional_design_practice.

                                                                             6
Understanding

Understanding practices are what instructional designers do to understand their learners, the environments in which
learners live, work, and learn, and the demands for learning that stakeholders may contribute towards a situation.

     Becoming a Learning Designer
     Designing for Diverse Learners
     Conducting Research for Design
     Determining Environmental and Contextual Needs
     Conducting a Learner Analysis

                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/understanding.

                                                                                         7
8
  1

Becoming a Learning Designer

Ellen D. Wagner

   Editor's Note

     Because of the close connection between these skills and the discipline of instructional design, many of the
     chapters in this book refer to the profession as instructional design, and professionals as instructional
     designers, even though many, like Dr. Wagner, prefer the term learning designer. The actual name of the
     discipline is continually evolving, as Dr. Wagner addresses in this chapter.

Learning design is the name of the professional practice that, in the views of many education, training, learning, and
development professionals, is a next iteration in the evolution of the craft dedicated to creating, producing, evaluating,
and improving resources and experiences that help people and organizations learn more and perform better.
A learning design is a creative pathway, with steps along the way, that guides someone from a point of introduction to a
permanent change in knowing, doing, or being. By naming learning design as the focus of our collective activity, we
make the declaration that our focus is on learning enablement, regardless of where, when, or with whom our design
efforts will be taking place. Designs may revolve around the creation of a course, programming an application, or
producing a webcast. Resources being designed as catalysts to induce learning may be as small as an element in a
presentation or as big as an immersive environment.
Learning design consists of an amalgamation of several contemporary design traditions actively used within current
teaching, learning, training, and development professions. As learning designers, we have profound opportunities to
develop conditions, strategies, resources, tools, and platforms that will keep learners engaged and inspired. We can
help people make new connections and meanings, spark new interests, and develop new abilities so that new learning
will occur.
In order to understand what learning design is, it is helpful to understand its precedents and how they are related to
each other. In this chapter, I will first describe several of the most notable precedents. From there, we will consider
some of the current professional expectations for learning designers in the contemporary learning and development
marketplace. I will then reflect upon some of the big variables shaping "Learning Designer Identity."

                                                                                         9
Instructional Design

Perhaps the most familiar of learning design's earlier traditions is instructional design. Instructional design (ID) is a
foundational part of the profession dedicated to systematically improving the learning and performance outcomes of
individuals completing a deliberate course of study. Originally, instructional design described a practice of creating
lessons and courses. In this context, design is describing an activity or set of activities that result in a documented set
of specifications for creating a lesson or a course. Following are the steps designers take when designing lessons or
courses:

  1. Assessing Content for the Course: What needs to be covered?
  2. Assessing the Learners: Who is taking this course? What will they need to know and do? How will you know if they

      have accomplished those things?
  3. Creating a Design Document: What needs to happen for this course to become real?
  4. Asking What Needs to Be Developed: Who is going to produce it? How much will it cost?
  5. Implementing the Design: What is needed for the lessons to be offered, for the students to respond, and for the

      course to be completed?
  6. Evaluating the Design: Did the lessons work? How do you know? How could it be better?

At the end of this process, the designer would end up with a design document. This serves as a specification to guide
the construction of a course. Design documents are a great way to review how you solved your design challenges--
what worked and what didn't work. They are important instruments for formative review and essential for summative
review. A design document also forms the basis for a professional portfolio that will serve as evidence of your work
over time. It is your record of how you communicated your plans for what needed to get done, both to yourself and to
your stakeholders.

Over time, the term instructional design has also come to be used as an overarching term for any formal activity
undertaken when designing and building learning resources or experiences, formal or informal. This causes some
confusion when it comes to creating job titles for people working in the learning and development field in various
capacities. Instructional design positions continue to represent a good percentage of today's jobs in the learning and
development industry by virtue of the industry's emphasis upon the creation of digital courseware and digital virtual
environments, especially after COVID-19 school and work closures in the spring of 2020. This is the case even for
positions which may not actually be engaged in designing or developing formal learning programs, lessons, or
courseware.

   Additional Resources

     For more information on the history of the instructional design approach, refer to the Foundations of Learning
     and Instructional Design Technology textbook available on EdTech Books, particularly the chapters on
     programmed instruction by Molenda and instructional design models by Dousay. Students might also
     appreciate perusing back issues of the Journal of Applied Instructional Design.

Instructional Systems Design

Given that so many learning experiences transcend instruction and must address bigger contextual consideration,
sometimes the activities associated with this practice are described more broadly as Instructional Systems Design
(ISD), where a significant nod is given to the impact of the broad conditions under which course, content, and
experience conceptualization, as well as prototyping and production will be taking place. ISD is based on a process
model for managing the establishment of a system within which instruction is a component. ISD calls for the following:

                                                                                        10
      Assess the needs and support requirements of target audiences and determine needs for the content presentation.
      Design for interventions or create solutions to improve outcomes, including baselines and methods for
      instructional measurement.
      Create development specifications: How will this solution be constructed?
      Create implementation plans: How will we get the new system to work? How will we engage learners?
      Determine formative and summative evaluation plans: How will we know if it is working? How will we make our
      revisions? How will we know if all our efforts have been worth it?

Depending upon the degree to which a program may feature multimedia or web technology systems as a part of their
practice, one may still find practitioners of instructional technology--even though many using the moniker "IT" in 2020
are more actively engaged in the practices associated with information technology, the domain of enterprise computing,
network management. In education and training, instructional technology is the place where one finds learning
management systems, learning content management systems, knowledge management systems and, increasingly,
platforms and programs that enable the tracking and analysis of resource use and user performance data.

User Experience Design

Another major set of influences upon the learning design profession have come from the world of User Experience
Design. Since the mid-1990s, web browsers brought the World Wide Web to life and as web technologies and service
platforms such as content and learning management systems became a more active component in systems developed
for sharing, delivering, and distributing content, courses, and experiences. From this evolution, User Experience (UX)
Design emerged as a field that has explored and influenced design considerations for how a website, online product, or
digital product user would experience a product.

Coined in the mid-1990s by Donald Norman during the time when he was vice president of advanced technology at
Apple Computer, UX describes the relationship between a product and a human. Back then, Norman argued that
technology must evolve to put user needs first--the opposite of how things were done at the time. It was not until 2005
that UX gained mainstream relevance as 42 million iPods were sold that year and the mass market experienced great
design at scale. Not long after, job descriptions and expectations shifted from putting information online to tailoring the
online experience to the needs of end users. The field of User Experience Design had been born (Kilgore, 2016).

   Additional Resources

     For more information on UX design, see the chapter by Earnshaw, Tawfik, and Schmidt (2018), or their full open
     access book on the topic at https://edtechbooks.org/ux.

Design Thinking

The need for better user experience with technology hardware and software was undeniable in the 1990s and 2000s as
tech systems, platforms, and tools evolved from being tools for the technologically proficient to being tools that were
intuitive enough for "regular folks." As the focus on considering user experiences shifted product design, a set of
processes and design approaches known as "Design Thinking" grew popular.

The Interaction Design Foundation noted that Design Thinking emphasizes developing an understanding of the people
for whom products or services are being designed (Dam & Siang, 2020). It helps develop a sense of empathy with the
user. Design Thinking helps by continually questioning the problem, assumptions, and implications. Design Thinking is
useful for tackling ill-defined or unknown problems, by reframing the problem in human-centric ways, developing many

                                                                                        11
ideas in focus groups, and adopting a hands-on approach in prototyping and testing. Design Thinking also involves
ongoing experimentation through sketching, prototyping, and testing new ideas.
All variants of Design Thinking embody similar principles which were first described by Nobel Prize laureate Herbert
Simon in The Sciences of the Artificial (1969). The Hasso-Plattner Institute of Design at Stanford University, also known
as the d.school, was at the forefront of applying and teaching Design Thinking.
The five-phased model developed by the d.school to explain Design Thinking included the following steps:

      empathize with users
      define users' needs and problems, along with your insights about those needs and problems
      ideate by challenging assumptions and creating ideas for innovative solutions
      prototype to start creating solutions
      test solutions
These five phases are not necessarily sequential. They do not have to follow any specific order and can occur in parallel
and be iteratively repeated. They are offered as an overarching conceptual framework.

   Additional Resources

     For more information on Design Thinking approaches, see the chapter by Svihla in this book, along with a
     similar chapter on agile design approaches by Cullen.

Figure 1
The IDEO Design Thinking Model

                                                                                        12
While Design Thinking does not address the requirement of designing for learning products, services, or experiences,
per se, the recognition of the relationship between experiences that can engage and inspire, and conditions that must
be present for learning were recognized in the early days of World Wide Web development.

Learning Experience Design: Unifying Design Traditions

While many informal discussions around learning experience began happening in the mid-2000s [1] , Niels Floor and his
colleagues in the Netherlands began actively exploring Learning Experience Design (LXD). They met in 2012 to unify the
principles of UX Design with learning principles and instructional design principles, even if some of those ID principles
might not necessarily be used to create direct instruction (N. Floor, personal communication, February 20, 2019). Where
UX designers' responsibilities would include designing prototypes and wireframes, graphic and visual design,
constructing user journeys or flows, collaborating with subject matter experts, and carrying out qualitative usability
tests (Rosala & Krause, 2019), learning experience designers would bring a focus on rich multimedia experiences,
learning outcomes, and performance improvement metrics.

Kilgore (2016) noted that LX designers develop experiential, multi-layered, complex, and contextual courses and lessons
that do not necessarily end when a course closes. These experiences aim to provide learners with enhanced
engagement, retention, affordance, and overall a more memorable learning experience. This requires advanced skills in
planning, production, development, design, and a clearer understanding of modern learners and learning trends than
what is required for more traditional instructional design undertakings. LXD appears to be less dependent upon both
supporting the infrastructure of technological systems and upon formative and summative evaluation than more
traditional ID and ISD practices have purported to be.

                                                                                        13
Learning Engineering

In recent years, learning engineering has emerged as a practice with the potential to serve as a strong complement to
learning design. Learning engineering focuses on using data analytics, computer-human interaction, modeling,
measurement, instrumentation, and continuous improvement to optimize learning and learning decision-making. It
offers a renewed focus on formative evaluation and on experimentation in the learning workflow.

Learning engineering started to emerge as a new field of interest in the mid-2010s with the increased popularity of
MOOCs, which served student populations in the hundreds of thousands in a single course. Suddenly, there were
opportunities for conducting "big-data" research and analyses--the scope of which had only previously been available to
commercial business analysis firms or to customers of online services. Furthermore, now "big data" were available to
educational researchers, meaning that educational research was no longer confined to social science methods based
on small sample sizes or random-controlled trial studies. Instead, machine learning, deep learning, data mining, and
artificial intelligence could be applied to research on course-related behaviors, achievements, retention, persistence, and
completion patterns. Initial contemporary interest in learning engineering began at institutions hosting MOOCs such as
Harvard, MIT (EdX), and Stanford (Udacity, Coursera). Carnegie Mellon University had maintained an engineering-as-
problem-solving tradition since the 1960s. Their Simon Institute openly licensed CMU's Open Learning Initiative
products in 2019 for educators to bring continuous improvement to classroom instruction (Young, 2019). This was a
nod to encouraging continuous improvement and classroom experimentation as an open education practice (OEP)
associated with learning engineering and empirical education.

Learning engineering's first appearance can be traced back to 1966, and, as with Design Thinking, is attributed to
Herbert Simon. At the time, Simon was a professor of Computer Science and Psychology in the Graduate School of
Industrial Administration at what was then the Carnegie Institute of Technology. He was asked to give a speech (later
published as an article) at the Presidents Institute at Princeton University. In this speech, "The Job of a College
President," he took higher education to task for its approach to institutional management and operation: "Comparing
colleges with other organizations, one sees that their most striking peculiarity is not their product, but the extent to
which they are operated by amateurs. They are institutions run by amateurs to train professionals" (Simon, 1967).
Among his suggested strategies for making colleges and universities more professional settings for teaching and
learning, Simon believed there might be value in providing college presidents with a learning engineer--an expert
professional in the design of learning environments.

As Simon envisioned this role, the learning engineer would be an institutional specialist with several responsibilities
related to optimizing university productivity. Specifically, they would be responsible for working collaboratively with
faculty to design learning experiences in particular disciplines. They would also be expected to work with administration
to improve the design of the broader campus environment to facilitate student learning and faculty improvements. They
would also be expected to introduce new disciplines such as cognitive psychology, along with learning machines and
computer-assisted instruction (remember, this was 1966), to various disciplines on campus.

Simon and his colleagues instilled a tradition of linking research and measurement of results to the improvement of
teaching and learning on his campus. Continuing in his tradition, a center was named for him at Carnegie Mellon to
harness his vision for a cross-disciplinary learning engineering ecosystem.

With recent 2019 announcements from Carnegie Mellon University describing the Simon Institute's plans to open-
source their huge collections of digital learning software, there has been much excitement that this will be a catalyst for
encouraging interest in continuous formative improvement in direct instruction, learning, and performance support.
There is hope that these efforts will have both direct impacts on learning engineering and indirect complementary
impacts on learning design practices going forward.

                                                                                        14
Current Demand in Learning Design Still Calls for Instructional
Designers

The term learning designer is still not being used broadly in the learning technology industry. For the most part, job
postings continue to seek instructional designers. Dr. Jane Bozarth, Director of Research for the Learning Guild,
reported that "In what was no surprise at all, I found the term Instructional Designer encompassed an ever-expanding,
soup-to-nuts array of tasks. The title has become a catch-all for anything related to creating, launching, delivering, or
even facilitating instruction in any capacity, and at any level of complexity" (Bozarth, 2019).

In a 2019 report from the eLearning Guild, Bozarth noted that in 2014 when applying for ID jobs, instructional designers
were expected to be able to do the following:

      Conduct needs analyses
      Conduct task assessments
      Write learning objectives
      Know the ADDIE process
      Understand supplier management
      Use desktop publishing
      Create graphic designs
      Use authoring tools
      Create with PowerPoint
      Produce and manage live & recorded webinars
      Support the training database
      Work with subject matter experts
      Create instructor-led training

The eLearning Guild's 2019 review shows even more skills lumped into the ID job skill category (Bozarth, 2019). In
addition to the list above, postings for jobs focused primarily on instructional design included a desire for expertise in

      Video production and editing
      Audio production and editing
      Web design/HTML5
      Game design/badges
      Dashboard creation
      Digital products
      Mobile app design
      Social and collaboration tools
      Assorted learning platforms
      Data analysis
      Content curation
      Augmented, virtual, and mixed realities

On top of this was the overlap between titles. Designer and developer were often used interchangeably. This is
supported by eLearning Guild membership data. Many of those employed as instructional designers say their work
actually entails doing "a little of everything," while those with more task-specific job titles (like multimedia developer)
say they spend a lot of their time engaged in instructional design.

Some large technology company HR departments continue to vacillate on whether to classify instructional design
positions along with technical communication positions (a fine job classification if you want to be a technical
communicator, less so if your design and interactive technology skills are about to be relegated elsewhere). Some IDs
are expressing interest in learning engineering job titles, thinking that it may bring a stronger recognition of technical

                                                                                        15
skills back to a job that has been held hostage by job descriptions that, in their worst iterations, have become catch-all
positions for "all tech duties as assigned."

Apart from the job stress of trying to wear a dozen hats, Bozarth has noted that the role confusion about what it is that
IDs should do or ought to be doing makes it very difficult to pin down essential competencies (Hogle, 2019), educational
and other background requirements, and correlating salary. "Calling yourself a learning experience wizard on Twitter
probably isn't helping," Bozarth confides, "but calling yourself an instructional technologist, and being able to explain
what that means, might" (2019).

Establishing a Learning Designer Identity

What we should remember from Bozarth's breakdown of instructional design job skill expectations is that the position
descriptions advertised on job sites such as LinkedIn and Glass Door are generally defined by hiring managers. Hiring
managers are always interested in getting the most out of their hiring dollars. While we must certainly pay attention to
what the job postings say a company is looking for, the learning design profession also has a responsibility to articulate
what we expect from our colleagues. Let us consider learning design with our own professional identity in mind. If we
establish our own vision of what we expect from our fellow practitioners of learning design, this will help set
expectations for what we want from one another in our work together. The following is a suggested list of expectations
for collections of knowledge that we would expect qualified learning designers to obtain.

  1. Understanding of Human Learning. We should expect each other to be familiar with the major schools of thought
      that explain the phenomenon of human learning. Whether we gain our understanding through the study of learning
      sciences, or through studies of human cognition, human behavior, or some combination thereof, we need to have
      an appreciation for the myriad explanations for how people learn. Furthermore, we need to appreciate the degree to
      which learning is likely to manifest in the wide variety of conditions, both formal and informal, that can elicit
      learning responses. We will need to know about the steps, stages, and processes that constitute the various
      phases of learning. We need to understand how learning outcomes may change under different conditions, and
      how conditions change in different populations, at different ages, under different kinds of support structures.

  2. Understanding of Design. We should have a basic understanding of what design is. Because design is a creative
      process, there are many different ways that a design process may manifest. However, there are currently two major
      schools of thought related to how design processes are categorized.

Schools of Thought Models

One school of thought, called the Rational Model, tends to follow a sequence of stages or steps as a means of problem
solving. The Rational Model proposes that

  1. Designers attempt to optimize a design candidate to account for known constraints and objectives.
  2. The design process is plan-driven.
  3. The design process is understood in terms of a discrete sequence of stages.

Instructional design process models, such as the Dick and Carey model, the ADDIE model, and the ASSURE model, are
all examples of rational process models. Much of instructional design and instructional systems design work over the
years has been led by the development of rational process models.

The other common school of design thought is called the Action-Centric Model. The Action-Centric Model suggests that

  1. Designers use creativity and emotion to generate design candidates.
  2. The design process is improvised.
  3. No universal sequence of stages is apparent - analysis, design, and implementation are contemporaneous and

      inextricably linked.

                                                                                        16
Both rational models and action-centric models see design as informed by research and knowledge. However, with the
action-centric model of design, research and knowledge are brought into the design process through the judgment and
common sense of designers--by designers "thinking on their feet"--more than through the predictable and controlled
process stipulated by the rational model, which is presented as a more formal approach toward hypothesis testing
("Design," n.d.).

While action-centric models have not generally been part of the instructional design and ISD tradition, they have been
more commonly found in settings where experience design, learner experience design, and Design Thinking process
models are used. With their focus on serving the needs of learners first, the newly emergent fields of open pedagogy
(e.g., Jhangiani & Biswas-Diener, 2017) and open education practices (A. Gunder, personal communication, December
30, 2020) are likely to use action-centric design process models as a central part of their orientation.

This shift away from rational process models, especially at a time when learning engineering is likely to provide "data
science cover" in post-COVID remote learning explorations, is likely to bring about interesting opportunities for dialogue.

With these key foundational pillars in place, learning designers will continue developing skills in analysis and evaluation,
communications and media arts, creative learning design and production, and research and measurement.

Analysis and Evaluation

Much of our work will consist of figuring out how to organize information so that it can be easily understood.
Sometimes we may need to determine if what we are dealing with is an information problem or a performance problem.
Sometimes we might need to determine if it is a problem for some but not all. Will people be best served with training?
Might they be better served with performance support tools? Where and when will they need it?

Understanding techniques of needs assessment and content and task analysis will be essential. So will reviews of
literature, knowing how to build a survey, and conducting market analysis. Formative and summative evaluation can
help us determine whether or not the designs we provide will achieve the results we hope to achieve.

Communications and Media Arts

Effective communication is central to the role and function of learning design. We are often the people working with
subject matters and learners, to help translate complex expertise into more easily understood, step-by-step procedural
pathways. Creative arts, including writing, graphic arts, photography, videography, and web design are among the means
of expression we have at our disposal for translating ideas and actions into words, images, recordings, and code
strings.

Learning designers will find that the time spent developing good writing skills will serve them well. Regardless of the
specific role, or the sector in which one is working, writers will always find their skills needed for a wide variety of tasks.
These tasks may include, but not be limited by, writing scripts and screenplays; press releases and public relations
documents; opinion/editorial articles and columns; research reports; executive briefing documents; grants; professional
presentations; and professional articles. The more that one moves away from rational process models and depends on
action-centric models that are produced in the moment, the more likely we are to depend upon project documentation
to guide progress.

Media professionals will also discover the same value for time spent developing skills in digital photography and
videography production and post-production skills. From still images to complex, multi-layered 3-D immersive
environments, we can use visual representations to help extend understanding in profound ways.

Creative Learning Design and Production

Learning how to work as a member of a team is an important part of being a learning designer. Production teams bring
together groups of individuals who can bring a learning product from concept to product. For example, a relatively small
learning product team producing web products may need a product manager, graphic artist, programmer, writer, web

                                                                                        17
designer, and evaluator. These teams come together with a shared design document guiding the production of each
stage of development.

Research and Measurement

One of the likely outcomes of the increasing number of enterprise technology systems (including web conferencing,
LMS, SIS, ERP, and other similar platforms) is that it is more likely that student/user data is collected within these
systems. As a result, the expectation that these data are going to be used in future learning design scenarios is already
on the rise. Learning designers may find it beneficial to increase competence in statistical and machine learning skills.
Test item development and creation of measurement instruments will be key skills.

Conclusion

The role of a learning designer has continued to evolve to make room for emergent technologies and frameworks.
Always the goal has been to design the most effective learning using all theories, processes, or technologies at our
disposal. In the modern version of the field, there are simply more of these theories, processes, and increasingly
advanced technologies to assist us. Understanding how various design disciplines can inform our work as learning
designers is both intimidating and exciting. This is a discipline where one never ceases to learn new skills and ideas. We
can never be stagnant as a field and must increasingly improve our ability to learn from and collaborate with designers
from a wide variety of backgrounds.

This book focuses on using design to create learning by focusing on key principles and various helpful processes, but
most importantly, it focuses on the praxis or application of ideas in practice. Embracing the praxis inherent in action-
centric design will help you develop a design identity that will bring you success in your work--no matter what your
official job title or design context may be.

References

Bozarth, J. (2019). Nuts and bolts: The ID (job description) bucket overfloweth. Learning Solutions.
         https://www.learningsolutionsmag.com/articles/nuts-and-bolts-the-id-job-description-bucket-overfloweth

Dam, R. F., & Siang, T.Y. (2020). What is design thinking and why is it so popular? Interaction Design Foundation.
         https://www.interaction-design.org/literature/article/what-is-design-thinking-and-why-is-it-so-popular

Design. (n.d.). In Wikipedia. Retrieved from https://en.wikipedia.org/wiki/Design

Hogle, P. (2019). What L&D professionals need to know to get hired or promoted. Learning Solutions.
         https://learningsolutionsmag.com/articles/what-ld-professionals-need-to-know-to-get-hired-or-promoted?
         utm_campaign=lspub&utm_medium=link&utm_source=lspub

Jhangiani, R. S., & Biswas-Diener, R. (2017). Open: The philosophy and practices that are revolutionizing education and
        science. London: Ubiquity Press. https://doi.org/10.5334/bbc

Kilgore, W. (2016). UX to LX the rise of learner experience design. EdSurge. https://edtechbooks.org/-wwV

Rosala, M., & Krause, R. (2019). User experience careers: What a career in UX looks like today. Nielsen Norman Group.
         https://edtechbooks.org/-oiv

Simon, H. (1967). The job of a college president. Educational Record, 48, 68-78. https://edtechbooks.org/-KhWE

Young, J. R. (2019). Hoping to spur `learning engineering,' Carnegie Mellon will open-source its digital-learning software.
         EdSurge. https://www.edsurge.com/news/2019-03-27-hoping-to-spur-learning-engineering-carnegie-mellon-will-
         open-source-its-digital-learning-software

                                                                                        18
[1] For example, after Adobe System had acquired Macromedia (the company that had previously owned products
including Dreamweaver and Flash) in 2005, members of the former Macromedia Global Education team now at Adobe
Worldwide Education continued to promote the "web user experience" in learning, and they referred to the work of
creating interactive eLearning tools with their then market-leading products as "learner experience design." Wagner and
her colleagues were offering presentations at the eLearning Guild Community Gathering conferences in 2005 and 2006,
describing learning experience design features related to interaction and engagement.

                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/learning_designer.

                                                                                        19
20
  2

Designing for Diverse Learners

Susie L. Gronseth, Esther Michela, & Lydia Oluchi Ugwu

Diversity  Universal Design For Learning

Designing educational programs and curricula involves developing understandings of the learner and instructional
environment characteristics that could impact learning success. While there may be some commonalities among
learners, it is important for designers to recognize that there will likely be a great diversity of learning preferences,
abilities, and experiences that learners will bring to a course or other learning experience. Rose (2015) remarked that
the notion of an "average" learner is a misnomer, and learner diversity (rather than uniformity) is actually the norm.
When learner variability is not addressed in a design, it is inevitable that many learners will experience obstacles to their
learning, limiting the effectiveness of the learning experience for them and inducing additional costs in time and
resources to make adjustments and accommodations (Brinck, 2005). Planning for learner variability from the outset is
therefore a valuable step in the design process that can lead to more robust, accessible, and impactful designs. Being
able to plan for diverse learners begins with developing empathetic understandings of the characteristics in which
learners will vary. This chapter first describes ways that instructional designers can become familiar with the diverse
needs of target learners and then offers recommendations for next steps in implementing inclusive design practices as
part of curricular planning.

Recognizing Learner Needs

Learners vary along many different dimensions, with a learner's profile as "individual as DNA or fingerprints" (Rose &
Strangman, 2007, p. 388). In general, people have different preferences and habits for how they approach learning that
are worth noting in the design. Some learners may have specific disabilities that can impact how they absorb, process,
and express information. Disabilities can affect sensory areas such as vision, hearing, speech, and motor control. They
can also be characterized by neurodiversity in that there are distinct differences in an individual's neural networks
involved with cognitive processes that impact how learners attend to, organize, and remember information. Learners
may have varied needs in their social-emotional tendencies, which can drive how they work in groups, initiate and
sustain engagement through the learning process, and create meaningful connections with content. It is also important
for designers to recognize learner diversity in linguistic proficiency and cultural backgrounds that can play into how
learners bridge their prior knowledge with new learning and the kinds of scaffolds and tools that could enable learning
success.

Further, the use of technology as part of instruction and learning can pose challenges to ensuring equal access among
learners. Digital educational materials and tools can introduce accessibility and usability issues. For example, some
learners may use screen readers or closed captioning to review content; some learners may use voice-command,
keyboard navigation, or gestural movements to interact with digital applications. When instructional designs do not

                                                                                        21
support these varied means of access and interactivity, learners will experience barriers to being able to fully engage
and benefit from the instruction.

Educational programs that require the use of specific technology equipment for access of computer-based instruction
can be met with barriers to obtaining the equipment in parts of the world that have limited financial resources or under-
developed infrastructures. For instance, the International Telecommunications Union (ITU, 2018) reports that just under
half of households worldwide have a computer in the home. Similarly, web-based instruction is often dependent on
learners having sufficient bandwidth through which to access the materials and activities, and this is not yet available in
some areas. In the Americas, for example, about 70% of broadband subscriptions in 2017 reported access 10 Mbit/s or
faster (ITU, 2018), which is generally sufficient speed for streaming video and making fast downloads. However, in least
developed countries (LDCs, as designated by the United Nations according to their low socioeconomic development
and Human Development Index ratings), access to high-speed Internet is not as prevalent. In 2017, 30% of broadband
connections were at very slow speeds of less than 2 Mbit/s, which would make content streaming and course material
downloads quite difficult. Designers can simulate slow internet in a variety of ways to understand how this impacts their
learners.

Therefore, it is important in instructional design practice to recognize such elements and characteristics of the target
learners and learning environments that relate to how learners will access, participate in, and show what they have
learned through the instruction. Planning strategically to enable learners to navigate learning pathways that best meet
their needs may involve greater investment of designer attention, time, and resources at the front-end. However,
accessibility is necessary, and workaround solutions and accommodations are often costly and can have social
implications that make them less than equal access for all learners.

Intentional effort in developing empathetic understandings of target learners during initial design phases can support
more sustainable implementation of the educational program. This approach is characterized as universal design (UD),
or designing for all people. UD "defines ways of thinking about and designing environments and products that work for
the greatest number of people possible" (Null, 2014, p. 12). Robert Mace coined the UD term, noting that UD is "a
process, rather than an achievement" (Story et al., 1998, p. 2). Applied to education, UD involves designing instruction
that will be usable to the greatest extent possible by the target learners. The design should facilitate equitable use,
offering equivalent means of access and engagement for learners with diverse abilities, and flexible use, providing
options that accommodate varied learning preferences and abilities (Story et al., 1998). Thus, designing for diverse
learners yields great benefits. Harris (2018) provides an example from nursing education, "Implementing UD concepts in
nursing classrooms which support equity and inclusion of students with diverse learning needs is a practical and
sustainable alternative to granting reasonable adjustments to students on a case-by-case basis" (p. 180).

Developing Empathy in Design

Designers of all types, and especially novice designers, can be somewhat self-centered. This is not to say that they are
selfish, but they can be self-referential, reflecting their own needs, experiences, and preferences in their designs rather
than those of the learners. For example, Molenbroaek and de Bruin (2006) related the story of a hearing aid designer
who fit the shape of his designed hearing aid to the comfort of his own ears instead of those of older people who would
actually wear them. This created great frustration for those who purchased the hearing aids when they found that they
could not find a comfortable fit in their ears. (For more examples, search for "bad design style" or read The Design of
Everyday Things by Don Norman.)

So, too, in designing for education, attempts at universally designed instruction can fail to meet the actual needs of the
learners. While self-referential design can certainly be used as a starting point, designers should not stop there but
continue to develop empathic understanding for the target learners who will be using their designed materials.
Empathic understanding is not binary, that is, it is not simply present or absent; rather, it is a skill that can be developed
and deepened over time through experience and effort. As Brinck (2005) related in the book Cost-Justifying Usability,
the investment of time and attention will be well worth it.

                                                                                        22
There are many ways that instructional designers can build empathic understanding for target learners. Fila and Hess
(2015) described five techniques often used by instructional designers. First, designers can directly observe learners,
both within the target learning context and in related places beyond. By watching how learners interact with
environments, tools, and problems, designers can see barriers and points of confusion, as well as learner-initiated
workarounds and strategies. Another technique is for designers to directly interact with sample target learners. Face-to-
face, phone, and email conversations can lead designers to ask pointed questions that can help them learn more about
the learner's experiences. Having a conversation with someone close to a target learner can also yield insights, such as
discussing learning needs with parents of young target learners.
Designers may also project themselves into the viewpoint of a target learner in order to envision what his/her
experience within the planned instruction might be like. To do so, designers can imagine how learners with various
characteristics and abilities would experience the exercise, activity, or lesson and where they may encounter barriers,
misalignments, or other frustrations. Finally, designers can simulate participation by piloting drafted designs and
materials to gain understanding for how learners may experience interacting in the learning context.

   Tools for Understanding Target Learners' Experiences

           Dyslexia
           Vision Disabilities
           Hearing Loss
           Slow Internet

For example, Dr. Temple Grandin uses a simulation technique when designing livestock facilities to build
understandings for how to improve the designs for the users (Raver, 1997). Her ability to empathize with the reactions
of livestock have made her an international expert on designing humane animal processing plants.

                                                                                        23
Explanatory Videos With Dr. Temple Grandin
Animal Behavior

                                                              Watch on YouTube
                                                               Link to transcript

Visual Thinking and Animal Behavior

                                                              Watch on YouTube
                                                               Link to transcript

                                                                                  24
Applying Empathy in Design

Empathic understandings of target learners can then be applied to design parameters, such as how content will be
communicated to learners through the designed instructional experience, how learners will practice concepts and skills
during a lesson, or how learning will be assessed formatively and summatively. As designers generate ideas for these
parameters, they can integrate their empathic understandings of the target learners with expectations and requirements
from stakeholders and the realistic constraints of available resources and the target learning environment. See Table 1
for a sample of learner characteristics, potential instructional barriers, and supports that can be built into a learning
experience.

Table 1

Non-Exhaustive List of Potential Considerations, Barriers, and Supports

Considerations                    Potential Instructional Barriers  Supports
Hearing difficulties
                                        Video                             Captions (complete and synchronized)
                                        Podcasts                          Interpreters
                                        Screencasts                       Audio transcripts
                                        Lecture

Vision difficulties (such as low  Presentation materials and        Audio descriptions of visible motion on a
vision and color blindness)       demonstrations                    video
                                  Printed texts                     Zoom functionality
                                  Color use in presentations        Screen reader accessibility
                                  Tasks requiring color             Braille alternatives
                                  differentiation                   Image alt-text
                                                                    Designations other than color for conveying
                                                                    key information

Physical mobility difficulties    Using a mouse                     Keyboard accessibility
                                  Physical requirements             Furniture rearrangement for increased
                                  Inaccessible spaces               mobility
                                  Stairs and platforms              Varied seating options

Information processing            Assessment time limits            Remove time limits
difficulties                      Extensive, complex tasks          Chunk information
                                  Language comprehension            Support strategy development (small goals,
                                  Technical jargon                  organize tasks, more deadlines for smaller
                                                                    sections)
                                                                    Flexible schedules
                                                                    Use simple language and/or provide
                                                                    vocabulary support

                                  25
Considerations          Potential Instructional Barriers  Supports
Language differences
Low Internet bandwidth        Spoken language                   Translation tools
Cultural differences          Written language                  Vocabulary instruction
                              Collaborative activities          Captioning
Digital literacy              Writing tasks                     Transcripts
                              Idiomatic language                Starter text for writing

                        Slow loading of large files       Provide alternatives to video
                        (video, audio, images)            Reduce image file size
                        Poor connections for real-        Have options for asynchronous
                        time interactions                 participation
                        Multimedia streaming              Mobile-friendly interface
                        limitations                       Chunk content in smaller sections

                        Gender roles or relationships     Collaboration with knowledgeable
                        between genders                   stakeholders
                        Power differences between         Guided group collaboration structure and
                        students and instructors          specified roles
                        Concepts of authority and         Communicated expectations
                        respect                           Examples of expected contributions and
                        Behavior expectations             activities
                                                          Connections between learner culture and
                                                          new content

                        Tasks requiring technical         Specific instruction or tool tutorials
                        skills                            Emotional support and encouragement
                        Navigation of online              Time and scheduling guidance
                        environments                      Just-in-time help desk support
                        Learning curve for digital
                        tools
                        Frustration or
                        discouragement

Learner voice can be a valuable contributor to applying empathy in design. Checking in with learners and giving them a
chance to respond to the design throughout the development process will likely result in meeting pertinent needs and
avoiding miscommunications and misinterpretations. This can be done through formal and informal presentations of a
drafted design to learners for feedback and further suggestions. Thus, instructional design is an iterative process of
continual refinement through such feedback loops and checks for congruency and alignment across components of a
module or educational program.

To illustrate how empathy can be applied in the instructional design process, two cases will be described. First, a case
mentioned in Meeks, Jain, and Herzer (2016) related how medical students with color blindness experienced difficulty in
histology courses when they were asked to identify microscopic structures, as the slides used to depict these
structures were often stained using red or green colors that tended to obscure some key distinguishing features. The
instructors addressed this barrier by converting the slides into grayscale, which enabled all students to view the
structures. Thus, a recommended practice in designing instructional materials is to use shapes, labels, or other means
to differentiate elements in illustrations, graphs, and other visuals, rather than color only. Doing so will facilitate a more
universally designed experience for target learners.

                                                                                        26
Figure 1
Using Stain to Help Students with Color Blindness Identify Microscopic Structures

                                                                                        27
CC-BY-SA Wikimedia commons

A second illustrative case is from the Industrial Design program at the University of Illinois at Urbana-Champaign.
Students in this program are coached to build empathy for users of their designed products and then use these
empathetic understandings to refine their designs. One strategy that they use is to explore what it feels like to
intentionally impair each of their senses and attempt to use their designs in representative home, school, and public
spaces. This pushes them to develop insights regarding users who may have specific sensory impairments and how
they may experience use of the design in varied environments. The design students also team up with non-design
students who have both visible and invisible disabilities to review and pilot their drafted designs. Doing so allows them
to build empathy through the interactions and dialogues with their team members, then they incorporate their user
experience insights into future revisions (McDonogh, 2015).

Design Approaches to Address Learner Variability

Differentiated Instruction (DI)

Since learner variability is the norm rather than the exception, it is important that designers incorporate instructional
approaches that will meet the needs of individual students and optimize their capacity to learn. One such approach is
differentiated instruction (DI). Stradling and Saunders (1993) defined differentiation as "the process of matching
learning targets, tasks, activities, resources and learning support to individual learners' needs, styles and rates of
learning" (p. 129). This means incorporating flexibility in the modes of learning, types of provided resources, and
assessments in order to respond to specific learner differences. Instructional designs can be differentiated in content,
process, and product (Tomlinson, 2017). Each of these dimensions will be discussed in further detail.

Differentiation of content involves varying the concepts and skills students will learn. While engaging in instructional
planning, designers may work alongside subject matter experts or instructors to identify learning goals and outcomes

                                                                                        28
for a course. Within the goals and outcomes, there can be variance in the levels of knowledge, skills, and dispositions
that learners could be expected to gain from the course. For example, the content can be differentiated into concrete
and abstract concepts, and students could be provided with a range of options (additional links, supplementary
material, multimedia) to access learning materials and to work at their own pace. A pre-assessment could be used to
gauge prior content mastery among learners and identify areas of additional needed support. Pre-assessments may
also be used to determine learner readiness levels, interests, and learning preferences (Tomlinson & Allan, 2000).
Gaining insights into learner interests and learning preferences (including preferences regarding individual/group work,
personality traits, and internal/external motivators) will enable appropriate matching of course design to these learner
characteristics. A pre-assessment can be in written form (such as a survey or test), or it can take the form of one-to-one
interviews, focus groups, or demonstrations.
Differentiation of process refers to the varied ways that students make sense of learning materials and take ownership
of their own learning. For a designer, it means factoring in activities that are engaging and intellectually challenging and
that lead students to practice and apply targeted concepts and skills. Some examples are problem solving, mind
mapping, and reflective journaling. What learners create through such activities, that is, the products of their learning,
can also be varied. Products should demonstrate knowledge and skills that learners have gained from a course, but
they can be in various forms, such as written, physical demonstration, spoken performance, or a video compilation.
Designers can develop performance expectations to guide learners to incorporate critical thinking and connections to
real-world applications through their products.

Universal Design for Learning (UDL)

Universal Design for Learning (UDL) calls for a flexible approach to learning that supports all students. Similar to the
tenets of Universal Design mentioned earlier, UDL aims to minimize barriers for learners as part of the design of
curricula and learning environments so that they are accessible to as many people as possible. UDL involves building in
flexibility into the curricula from the outset instead of retrofitting and adapting inaccessible curricula after the fact
(Meyer et al., 2014).
It is worth noting that UDL differs from DI because it provides learners with multiple options to pursue self-directed
learning whereas DI is often more instructor-directed.

                                                                                        29
   Differences Between UDL and DI With Katie Novak

                                                                   Watch on YouTube
                                                                    Link to transcript

The UDL framework contains three key principles:
      Provide multiple means of engagement that stimulate interest and persistence in learning, thereby producing
      learners that are purposeful and motivated;
      Provide multiple means of representation so that content is delivered in varied formats, enabling learners to
      become resourceful and knowledgeable; and
      Provide multiple means of action and expression in which learners can show their developing knowledge in varied
      ways, supporting them to become strategic and goal-directed (CAST, 2018).

Each principle has guidelines and checkpoints that detail implementation strategies.

     To access the UDL framework, visit http://udlguidelines.cast.org/.

Hall, Strangman and Meyer (2003) offer four steps for implementing UDL in the planning and delivery of curriculum: set
goals, analyze status, apply UDL, and teach the UDL lesson. When setting goals, it is important to establish the context
for the instruction. Designers may need to consider, for example, if target goals would need to align with state or
organizational standards. Designers can also consider if the methods that students use to accomplish the learning
goals can be separated from the goals themselves. For instance, a goal that requires students to "write a paragraph
about how the circulatory system works" may be reframed to prompt learners to "describe a complete cycle in the
circulatory system," which would facilitate flexibility in the means that learners could achieve that goal.

                                                                                        30
Analyzing the status of instructional materials involves evaluating the methods, materials, and assessments that will be
used, considering their accessibility and flexibility in the ways that students engage and demonstrate their learning and
identifying potential barriers. UDL can then be applied to elements of the instruction wherein potential barriers and
opportunities for flexibility have been identified. Ultimately, the intentional flexibility in the UDL approach to design is
aimed to position learners to be more self-directed and self-regulated, as learners are provided options for their learning
pathways that align with their individual needs.
So, how might that look in practice? To provide multiple means of engagement, students are provided with tools that
enable them to take ownership of their learning. Challenge levels should match their readiness, and there should be
built-in opportunities for mastery-oriented feedback. This could begin with a well-designed syllabus that clearly states
learning goals and objectives, course expectations and structure, information on how to navigate the learning
environment, methods of assessment, and options for participation. Learning environments should support varied
navigation and control methods that are accessible to all learners. Designers may also consider incorporating
checkpoints that can help learners chart their progress in a course and provide opportunities for feedback and self-
reflection after completing a unit of study.
Providing multiple means of representation offers learners options to customize the display of information, make sense
of language and symbols, and enhance their levels of comprehension. Course materials can be presented in a variety of
formats to provide varied means for students to connect with the content. Materials may be customizable, enabling
learners to adjust text size, color, contrast, etc. and access content in varied forms, such as video, interactive
simulations, audio, and text-to-speech.
In providing multiple means of action and expression, designers can incorporate planned flexibility in learner response
options, navigation, access to tools and assistive technologies, forms of communications, and demonstration of
learning. One strategy to achieve this is to maintain uniformity in the design of the content, both across functionalities
and through consistency of visual appearance. Another strategy is to offer multiple options for learners to demonstrate
their mastery of the content, such as through text, mind maps, audio, and video.

Culturally Relevant Education

Culturally relevant education is built on the premise that culture is an essential component of students' learning, as
instructional practices, curriculum, and modes of assessment that are couched in "mainstream ideology, language,
norms, and examples often place culturally diverse students at a distinct educational disadvantage" (Howard, 2012, p.
550). Culturally relevant education is characterized by several frameworks, including culturally responsive pedagogy,
culturally relevant teaching, and culturally congruent teaching. It is empowering to students intellectually, socially,
politically and emotionally by using culturally relevant frameworks to convey knowledge, abilities, and attitudes (Ladson-
Billings, 2009). Consequently, a culturally relevant education recognizes the culture, attributes, and knowledge that
ethnically diverse students bring to their learning experiences and uses those resources to maximize their learning
(Howard, 2012).

                                                                                        31
   Culturally Relevant Pedagogy With Irvine, Gay, & Gutierrez

                                                                   Watch on YouTube
                                                                    Link to transcript

The question then becomes, how can instructional methods and materials be designed for cultural relevancy to
learners, especially those on the fringes of dominant culture? An initial step for designers is to develop cultural
sensitivity through becoming familiar with target learner interests, core values, traditions, modes of communication, and
backgrounds. Knowledge about the learners can then be strategically integrated into plans for instructional methods
and materials (Gay, 2002). To help learners see the relevance of instructional materials to themselves, instructional
resources can be situated within the cultural and ethnic contexts of the target learners. Designers can incorporate
materials and activities that reflect multiple voices and perspectives rooted in the personal experiences and cultures of
the learners. Learner autonomy can be enhanced through the provision of varied options for expression. For example,
learners can be provided an array of materials and activities to choose those that are relevant to their backgrounds of
experience. Designers can also plan for ways that learners can share personal experiences as they are related to course
topics, creating meaning-making opportunities.

Conclusion

Universally designing instruction involves recognition and intentional planning for components and features that often
do create accessibility challenges for learners so that all learners can access and engage in learning experiences
equitably. As learners vary in their characteristics, preferences, and experiences, so do the approaches through which
designers can develop empathetic understandings and incorporate flexibility to meet diverse learner needs. This
chapter offers an initial look into these strategies, and designers are encouraged to revisit these strategies in the
instructional design process so that they can anticipate variability in their target learners and address this variability
strategically.

                                                                                        32
Activity/Exercise Ideas

  1. Explore built-in accessibility features. There are built-in accessibility features in many of today's tools that support
      varied vision, hearing, mobility, and learning needs. Explore the built-in accessibility features of one of the following:
        1. Mac OS: https://www.apple.com/accessibility/mac/
        2. Windows: https://www.microsoft.com/en-us/accessibility/
        3. iOS: https://www.apple.com/accessibility/iphone/
        4. Android: https://www.android.com/accessibility/
        5. Chrome OS: https://edu.google.com/why-google/accessibility/chromebooks-accessibility/
        6. Other Google tools: https://www.google.com/accessibility/products-features/

  2. Share in a discussion board post, blog, video post, Tweet, etc. about what you learned in your exploration of the
      built-in accessibility features. Did you find any that you would like to use in the future?

  3. Experience accessibility of digital resources. Choose a website, app, or program, and access it in a different way
      than you usually do. For example, you can use some of the built-in accessibility tools from Activity #1, such as
      trying to do research through an online library website using a screen reader and voice-input (such as VoiceOver
      and Dictation on MacOS). You could also try navigating around a course site using keyboard-only (no mouse,
      touchscreen, or touchpad). Or, you could try using a web application on a mobile device that you usually access via
      laptop/desktop computer. Spend about a half hour accessing the digital resource in one or more different ways and
      then reflect on your experience. How accessible was the resource for the means that you accessed it? What did
      this experience prompt you to think about in regards to your own design of digital educational resources? Create
      and share a summary of your experience and related thoughts as an audio clip, discussion board posting, graphic
      (could include screenshots or sound clips), etc.

  4. Observe universal design. Spend 30-60 minutes observing people using universally designed features in different
      contexts, such as the automatic door openers, ramps, buses, playgrounds, water fountains, food service centers,
      libraries, etc. What do you notice about who is using them and how? Collect pictures of examples and non-
      examples of universally designed features around campus. How might these impact people with different needs?

  5. Using technology to implement UDL. Choose a guideline (see http://udlguidelines.cast.org/) associated with one of
      the UDL principles and find a technology tool that supports the implementation of the guideline. For example, you
      may find a tool that supports the guideline "recruiting interest" under the principle of engagement. How would the
      tool optimize individual choice and autonomy, optimize relevance, value and authenticity, and minimize threats and
      distractions?

  6. Create accessible materials. Use the Accessibility Evaluation and Implementation Toolkit (AIET) to evaluate and
      improve accessibility in one of your own Word documents, PowerPoint presentations, Excel spreadsheets,
      a WordPress website, or a Canvas module. Use the links in the checklist to identify accessibility barriers and then
      resolve all errors.

                                                                                        33
Resources

      Accessibility Resource List from Designers for Learning based on "POUR" - Perceivable, Operable, Understandable,
      Robust recommendations related to website accessibility.
      Culturally Responsive Teaching & the Brain by Zaretta Hammond offers tools and recommendations for applying
      CRT into instruction.
      Dive Into UDL by Kendra Grant and Luis Pérez provides a UDL self-assessment and a variety of resources to explore
      UDL more deeply.
      Global Accessibility Awareness Day (GAAD) is an annual event in May that focuses on the design, development,
      and usability of technology for users around the world.
      Inclusive Learning Network of ISTE (International Society for Technology in Education) provides professional
      learning opportunities and resources on inclusive design and technology.
      National Center on Accessibility Education Materials (AEM) provides resources and technical assistance on
      producing learning materials that meet accessibility standards.
      Techniques for Empathy Interviews in Design Thinking is a resource with ideas for how to set up and conduct
      exploratory interviews with potential learners.
      The UDL Toolkit is a collection of UDL resources for teachers, coaches, and instructional leaders.
      UDL-IRN (The Universal Design for Learning Implementation and Research Network) provides resources and
      professional learning opportunities to connect with other educators and designers regarding implementation of
      UDL.
      UDL Progression Rubric by Katie Novak and Kristan Rodriguez provides specific examples of UDL practices across
      the three principles of providing multiple means of engagement, representation, and action and expression.

References

Brinck, T. (2005). Return on goodwill: Return on investment for accessibility. In R.G. Bias & D.J. Mayhew (Eds.) Cost-
        justifying usability: An update for an internet age (2nd ed.) (pp. 385-414). Morgan Kaufmann Publishers.

CAST. (2018). Universal design for learning guidelines version 2.2. Retrieved from http://udlguidelines.cast.org

Gay, G. (2002). Preparing for culturally responsive teaching. Journal of Teacher Education, 53(2), 106-116. doi:
         10.1177/0022487102053002003

Hall, T., Vue, G., Strangman, N., & Meyer, A. (2003). Differentiated instruction and implications for UDL implementation.
         Wakefield, MA: National Center on Accessing the General Curriculum. Retrieved from
         http://aem.cast.org/about/publications/2003/ncac-differentiated-instruction-udl.html

Harris, C. (2018). Reasonable adjustments for everyone: Exploring a paradigm change for nurse educators. Nurse
        Education in Practice, 33, 178-180.

Fila, N. D., & Hess, J. L. (2014). Exploring the role of empathy in a service-learning design project. Design Thinking
         Research Symposium 10. Purdue University, West Lafayette, IN, United States. doi: 10.5703/1288284315952

Howard, T. C. (2012). Culturally responsive pedagogy. In J.A. Banks (Ed.), Encyclopedia of Diversity in Education (pp.
         549-552). SAGE Publications. doi: 10.4135/9781452218533.n174

International Telecommunication Union (ITU). (2018). Measuring the information society report (Vol. 1).
         ITUPublications.

Ladson-Billings, G. (2009). The dreamkeepers: Successful teachers of African American children (2nd ed.). Jossey-Bass
         Publishers.

                                                                                        34
McDonogh, D. (2015). Design students foreseeing the unforeseeable: Practice-based empathic research methods.
        International Journal of Education through Art, 11(3), 421-431. doi: 10.1386/eta.11.3.421_1

Meeks, L., Jain, R., Herzer, K. (2016). Universal design: Supporting students with color vision deficiency (CVD) in medical
        education. Journal of Postsecondary Education and Disability, 29(3), 303-309.

Meyer, A., Rose, D.H., & Gordon, D. (2014). Universal design for learning: Theory and practice. CAST Professional
         Publishing.

Molenbroek, J., & de Bruin, R. (2006). Anthropometry of a friendly restroom. Assistive Technology, 18(2), 196-204.
         doi:10.1080/10400435.2006.10131918

Null, R. L. (2014). Universal design: Principles and models. CRC Press.
Raver, A. (1997, August 5). Qualities of an animal scientist: Cow's eye view and autism. The New York Times, pp. 1C.
Rose, L. T. (2015). The end of average: How we succeed in a world that values sameness. HarperCollins Publishers.
Rose, D. H., & Strangman, N. (2007). Universal design for learning: Meeting the challenge of individual learning

        differences through a neurocognitive perspective. Universal Access in the Information Society, 5(4), 381-391.
Stradling, B., & Saunders, L. (1993). Differentiation in practice: responding to the needs of all pupils, Educational

        Research, 35(2), 127-137, doi: 10.1080/0013188930350202
Story, M. F., Mueller, J. L., & Mace, R. L. (1998). The universal design file: Designing for people of all ages and abilities

         (Revised ed.). NC State University, The Center for Universal Design.
Tomlinson, C. A., & Allan, S. D. (2000). Leadership for differentiating schools & classrooms. Association for Supervision

         and Curriculum Development.
Tomlinson, C. (2017). How to differentiate instruction in academically diverse classrooms (3rd ed.). ASCD.

                                                                                        35
Susie L. Gronseth

University of Houston
Dr. Susie Gronseth is a Clinical Associate Professor in the Learning, Design, and
Technology program area in the College of Education at the University of Houston
(Houston, Texas, USA). She specializes in learning technologies, teaching
strategies, instructional design, health sciences education, and applications of
Universal Design for Learning (UDL) to address diverse learner needs in online, face-
to-face, and blended contexts. Her research interests involve learner engagement,
accessibility, inclusive instructional design, distance education, and self-directed
learning. She has a background in primary education, specializing in supporting
students with disabilities, and coordinated educational technology programs at
middle school and college levels. She completed her Ph.D. in instructional systems
technology at Indiana University, honored with the Distinguished Alumni Award. She
has received recognition for her course design and university teaching, including the
International Society for Technology in Education (ISTE) Online Learning Network
Award and the University of Houston College of Education Teaching Excellence
Award. Her recent book, Universal Access Through Inclusive Instructional Design:
International Perspectives on UDL (Routledge, 2020), addresses inclusive
instructional foundations, policies, design approaches, technology applications,
accessibility challenges, curricular quality issues, research, and case studies from
around the world. More information is available at https://bit.ly/SGronseth.

Esther Michela

University of Tennessee
Currently a PhD student at the University of Tennessee Knoxville, Esther is exploring
issues of accessibility, empathy, Universal Design for Learning, and student
interaction in the context of instructional design. A former special education
teacher, she received a M.Ed in the Mind, Brain, and Education program from the
Harvard Graduate School of Education and an M.S in Instructional Psychology and
Technology from Brigham Young University.

                                       36
            Lydia Oluchi Ugwu

                University of Houston
                Lydia Oluchi Ugwu is a PhD student of curriculum and instruction at the University
                of Houston. Her doctoral research focuses on teaching strategies for diverse
                learners, teaching skills and competencies for online learning, and blended learning.
                She has a Masters in human resource education from Louisiana State University
                and a bachelors in culture, media and communication from the University of Surrey.
This content is provided to you freely by EdTech Books.
Access it online or download it at https://edtechbooks.org/id/designing_for_diverse_learners.

                                                         37
38
  3

Conducting Research for Design

Daniel R. Winder

Every field of expertise has elements that need to be investigated, and regardless of the discipline, how you discover
evidence for learning is very similar. In this chapter I will present common issues I have encountered in the field of
instructional design when planning a research study. This is followed by discussion of when and how to use certain
research methodologies. Finally, I will discuss how to conduct research and report results.

When and How to Use Research Methodologies

Novice researchers will often limit themselves as only being a qualitative or quantitative researcher. In practice, the
nature of the question will dictate the most appropriate research method that should be used. In addition, using mixed
methods often yields stronger results. For example, survey teachers about what to cut or reduce in a curriculum, then
follow up with focus groups to yield stories and experiences that explain the survey numbers.

When to Use Qualitative and Quantitative Methods

Qualitative methods generally seek to answer questions that center around experiences that involve understanding
values, beliefs, perceptions, emotions, culture, growth, paradigm shifts, processes, taboos, morality, reasoning, and
acquiring learning. In short, the more a question centers around "the human experience," the more it lends itself to
qualitative means which gather rich experiences, stories, and examples. An example of qualitative research could be
using grounded theory to develop a new theory of language acquisition by analyzing data from journals, portfolios, and
focus group interviews of persons learning a new language.
Quantitative methods generally seek to answer questions that center around how much of something has been
acquired such as knowledge, behaviors, or attitudes. Experiments, correlations, causes, and descriptions are often the
reasons why how much of something is being measured. An example of experimental research is when a pilot program
is compared with an existing instructional program to measure the effect of different instructional treatments on how
much students learn. An example of correlational research would be the effect of parental involvement on GPA. The
hypothesis of such a study may be that higher parental involvement will correlate with higher GPA. An example of a
causal research study is the effect of dogmatic political preferences on macro-moral reasoning abilities. Perhaps the
assumption is that dogmatic political party views may inhibit one's ability to morally think through all aspects of social
issues. An example of descriptive research is when a researcher describes how many hours of homework 7th grade
math teachers give per week.

                                                                                        39
General Steps to Qualitative Research

Identify Your Bias

The first step in qualitative research is to identify your biases via reflective means. Reflexivity activities help a person go
through a meta-cognitive process of identifying their preconceived notions, judgements, and values that may influence
the research project. This can be as simple as journaling your thoughts about a research topic. This video clip explains
some qualitative research methods with a few examples. There is a great description of reflexivity about eight minutes
into the clip.

Plan the Research

The second step begins by writing a problem statement and research question. Once these and variables of interest are
identified, an important part of the research plan is to design interview or observation guides. This may involve
designing interview or observation guides. Guides take various forms such as a checklist, a standardized open-ended
interview guide, or an informal guide. A few tips in designing the guide are to a) have colleagues review questions
or guide, b) pilot test your questions or guide, c) check for leading questions or leading interviewers or observation bias,
and d) be open to scrutiny.

During this phase, plan reliability and validity measures are

  1. Credibility (internal validity). Credibility of the research can be established via triangulation, prolonged contact,
      member checks of data or analysis, saturation, reflexivity, and peer review.

  2. Transferability (external validity). The generalizability of the research can be enhanced by using thick descriptions
      and variation in participant selection.

  3. Dependability (reliability) is established by extensive audit trails and using triangulation. Subjectivity audits are
      used during the data collection process to evaluate how a researcher's presence, questions, or biases may be
      impacting the research.

  4. Confirmability is established with reflexivity and intra- or inter-coder reliability.

Gain Entry

The third step is to gain entry with the group or individuals the researcher is studying. This involves gaining trust,
understanding the culture and environment, and helping the participants feel at ease. The amount of time and effort
here depends on many factors such as the sensitivity of the topic and whether the researcher is taking a cultural native
approach or acting in an apparent authority role. To further explore this concept of gaining entry, read this article.

Collect Data

The fourth step is to collect data. This can be done via observation, interviews, focus groups, open-ended surveys, and
journals. During this phase, be prepared to adjust your thinking and be aware of your bias. Gather existing data and
artifacts also. Because this is the step where the most mistakes are made, below are some additional practical tips and
tools for qualitative data collection.

Observation

The most common ways to observe are live, virtual, recorded video or audio files, or a combination of the three. A rubric
or observation guide can be designed to focus observation on certain concepts or phenomena. The benefit to live
observations is the ability to view holistically all the nuances, expressions, non-verbal cues, and visual or auditory
expressions that are not captured by other methods.

The benefit of recording video or audio files of observations is the ability to review them multiple times during
transcription and analysis. One great tool for video observation is GOReact, where a pre-specified codebook can tag live
or pre-recorded video files when coded phenomena occur.

                                                                                        40
Focus Groups

A focus group is one of the most common methods of qualitative research. Focus groups are group interviews with less
than 10 persons. These persons are usually intentionally selected (e.g. students who meet certain demographic
characteristics). Focus groups work well when participants feel non-threatened and permissive. They are effective
because the interplay between participants can stimulate other's thoughts, beliefs, perceptions, etc.

Because of the ease of gathering these small groups, some researchers mistakenly use focus groups when other
methods would be more appropriate. For example, novice researchers may gather a focus group and proceed to ask the
group survey-like questions rather than gather rich descriptive experiences, stories, and thoughts. Another common
mistake is that researchers default to focus groups for topics best answered by individual interviews; for example,
asking an online cohort about their group cohesion in a focus group setting presents a situation where participants may
give socially desirable responses (individual, confidential interviews may yield more truthful responses).

Common Mistakes in Focus Groups: Communication Errors

The most common mistake for focus groups (and individual interviews) are communication errors from the interviewer
which hinders responses such as:

      Restating too often. In teaching situations and normal friendly discussion, restating is a great communication skill.
      However, in interviewing, it's not always a great method, especially with a novice researcher who can't tell when
      they are putting words in participants' mouths. For example, if you are interviewing a group to find out opinions on a
      particular LMS platform and they say, "It feels so isolating," a great follow-up to get more out that concept is to ask,
      "Can you tell me more about that?" or "Can you think of a story or experience that illustrates that isolated feeling?"
      In a focus group, it is best to seek clarification from the participants with additional questions, stories, experiences,
      or elaboration rather than assume you know what they mean.
      Leading the witness. Some interviewers inadvertently revert to their bias and ask leading questions.
      Overly emotional responses from the interviewer. Remember, you are an interviewer with a beating heart, but not a
      counselor or therapist. You don't have to validate or comment on everything or nervously giggle at comments when
      humor is not intended.
      Not qualified to discuss. Interviewing about content for which you are not qualified (e.g. impacts of mental illness,
      causes of anxiety, marriage counseling issues, or any other topics that most often involve a trained professional).

Common Mistakes in Focus Groups: Failure to Control for Groupthink

Groupthink is when one person in the group shares an opinion or thought and the group finds it hard to break away into
any other vein or divergent thinking. Sometimes, there is an actual group consensus. Other times, it's groupthink.
Discerning between the two takes experience. Some common indicators of groupthink are that a few people are not
talking or give trite agreements ("what she said"). Discern if the persons not talking or tritely agreeing are just less
articulate, or don't want to share their contrasting views. You may have to curb bold or strongly opinionated persons
who may seek to bully or monopolize the group. For example, "Let's hear from someone who hasn't shared yet" or "Hold
your thought for a minute while we hear from...".

To control for groupthink, first introduce the session with some ground rules for an open discussion (and repeat many
of these introductory rules throughout):

                                                                                        41
      There are no right or wrong answers.
      In this group, it's okay to disagree and still be friends. If everyone is saying their experience was great, but for you it
      was horrible, you need to speak up so all perspectives can be heard.
      It's safe to be positive or negative, better or worse. Explain to participants that saying something positive does not
      make them a better teacher or student. Similarly, saying something negative does not make them a worse teacher
      or student.
      It's okay to remove the filter between the brain and tongue, just for 45-60 minutes in this focus group. "Don't worry if
      you are saying it right or wrong, just say what you experienced."
      Distance yourself as the researcher from the product. For example, "I didn't create this company or this training so
      however you feel about it won't affect me."

To control for groupthink during a focus group, use the following suggestions:

  1. Have participants write out thoughts or stories before the interview begins. Refer back to these in the interview if
      groupthink emerges. Did anyone write something different?

  2. Invite contrast throughout the interview by asking, "Did anyone have a different experience that I need to hear?" or
      "Have you or someone you know had a different experience?"

  3. If the groupthink is very strong, use hypotheticals, such as "What type of person would have had a different
      experience?" or "Could someone have a different experience? Why or why not?" Follow up by asking, "Did any of you
      experience that at all?"

  4. Employ indirect questioning. Indirect questioning seeks to control for socially desirable response sets. For
      example, if you were asking about a program designed to remove racial biases, there may be a strong socially
      desirable response set. To remove the social context, you could ask, "If you were to describe how well this program
      runs using an analogy of a car, what kind of car would it be? Why?" or "If this program were a TV show, what genre
      would it be? Why?" With indirect questioning, the goal is to remove persons from a contextual response set by
      asking about the program indirectly. This can control the parroting response set by making respondents think and
      respond in a new context about similar issues, impressions, or values, without having time to prepare a socially
      desirable response. The result is insights with some contextual limitations.

  5. In conclusion, ask, "What do you want to make sure I heard from you?" (I often have participants to write this
      down). Or ask, "is there anything I should have asked but didn't?"

Ethnography

Ethnography is a method of study that educational researchers adopted from anthropology. For example, a researcher
may be a participant or observer in an online class and may conduct several interviews and focus groups. They may
write about the setting, social implications, typical and best behavior, and seek a perspective from several groups
(parents, teachers, students, administrators, etc.). The researcher could also report about the tensions between groups,
or even explore power struggles within groups.

Analyze the Data

The fifth step is to analyze the data. After qualitative data is collected and cleaned (such as removing identifying names
or using codes for participants instead of names), it is important to go back to your original questions and study plan.
Although you can adapt and explore interesting concepts that emerge, it is also important to keep your focus on your
original questions. Thoroughly explore the data and be open to new concepts, but do not be sidetracked by all of them.
Word maps may help initially to see what phrases or concepts are prevalent.

In qualitative analysis, all artifacts are loaded into a software such as Delve (https://delvetool.com/), Quirkos
(https://edtechbooks.org/-hVz), Dedoose (https://www.dedoose.com/), or MAXQDA (https://www.maxqda.com/) . The
researcher then creates a codebook. A codebook is a list of concepts, behaviors, actions, thoughts, etc. that summarize
themes in a group of qualitative data (such as responses to open-ended questions). My initial code book will also
contain the original research questions. If you are working with a group of researchers you should make sure you all

                                                                                        42
agree on the codebook and use it consistently when analyzing your data (often called "interrater reliability"--see this
training for more information).

Common Mistakes in Analyzing Qualitative Data

      Failure to focus the codebook on the research questions.
      Failure to identify proximal relationships (e.g. math division, anxiety, and home support are all close together). For
      example, if less anxiety and parental involvement are often near each other in participant comments there may be a
      connection there.
      Myopic analysis--overly focusing on a powerful story that is not a generalizable trend.
      Failure to make your findings defensible by employing validity and reliability measures discussed in the second
      step (credibility, transferability, dependability, confirmability).

Report Findings

The final step is to report the findings. A good report involves the research question or problem statement; background,
theory, and lit review; the study design; presentation of the data with rich descriptions; and an explanation of the data or
findings. You can present findings in chronological order, or by theme, frequency, or rich narratives such as in a case
study. To learn more, see this lecture: https://edtechbooks.org/-FNEs.

Common Mistake in Reporting Qualitative Data: Extrapolation Error

A common mistake is a tendency to want to extrapolate an ancillary finding into a generalizable trend. For example, a
researcher may say, "I was talking to a student at lunch today and they mentioned... This is something I've heard many
times." When a researcher makes a generalizable error, they are often including a finding that was never asked for,
planned to explore, or agreed upon. The anecdotes are often not related to the initial research purposes.

The danger of making such anecdotes into general findings is, in reality, it was only a few powerful stories from a few
isolated interviews. These powerful stories should not be ignored, and can be explored further, but one should ask if the
anecdotes alone are sufficient to represent a generalizable trend. For example, in one study, an educational
administrator in Africa asked for a budget to buy a gun to scare the lions away from the school. Although this was an
impressive story, it was not an administrative issue mentioned in any other program. However, all programs mentioned
power or internet outages and lack of technological resources.

In summary, researchers who have planned, collected, analyzed, coded, and reported data from qualitative research
understand that the benefits are rich descriptions and understanding concepts or phenomena in depth and context.
However, a drawback is the significant time and resources spent in transcribing, reading, organizing, coding, analyzing,
and reporting. In addition, the amount of information gathered is usually very focused and can be limited in scope. For
this reason, many chose to use quantitative methods.

General Steps to Quantitative Methods

Quantitative research begins by developing an understanding of an instructional problem and possible theories to solve
the problem. The key elements of this step involve knowing much of the background of a product or problem and
identifying a client's needs.

Write a Problem Statement and Research Question

The result of a good review of literature is to be able to write a good problem statement. A well-written problem
statement will bring up past research or needs that lead to an instructional design question, then lead naturally into
defining the scope of the questions that will be answered in the study. Here is an example of a problem statement for
teachers that are simultaneously learning a foreign language and teaching skills:

                                                                                        43
Research shows that when second language learners seek to achieve multiple learning objectives within a second
language, the acquisition of those multiple learning objectives may be impeded due to increased cognitive load and
learning anxiety.

The problem statement is general enough to be read by top administrators and specific enough to narrow down the
project scope. The research question builds on the problem statement to define the specific questions of the study
plan. Here is a sample research question from the previous example:

Does separating instruction of teaching skill training from language acquisition training affect student's: a) teaching
skills, b) language acquisition, and c) foreign language learning anxiety when three pilot groups are compared with three
control groups and baseline historical data?

   Writing a Research Question

     "I didn't have time to write you a short letter so I wrote you a long one instead." Mark Twain

     This is applicable when writing a problem statement and research question(s). It is more difficult to be concise
     than verbose.

Another approach to writing a research question is to operationalize one's theories into a hypothesis. A hypothesis
usually involves an if-then statement and defines the variables of interest. For example, if I use metacognitive
strategies in my reading curriculum, then students will more efficiently learn to read at a 5th grade level. It is quite easy
to turn a well-written hypothesis into a research question. For example, if I use metacognitive strategies in my reading
curriculum, will students more efficiently learn to read at a 5th grade level? In this phase, there will be some
operationalization of terms here such as efficiently, 5th grade level, and identifying some specific metacognitive
strategies.

Develop a Study Plan

Once a research question is designed and variables are operationalized, it's time to develop a research plan. A good
research plan builds on the problem to be solved and further operationalizes variables to be studied and controlled for.

The Study Plan Matrix

At this point, the client and instructional designer can create a study plan matrix as shown in Table 1 (based on the prior
example of language acquisition):

Table 1

Sample Spreadsheet

Research        Method or   When will data be collected and by  Analysis
Question(s):    Instrument  whom?
Does
separating
instruction of
teaching skill
training from
language
acquisition

                            44
training affect
learners'...

...teaching      Teaching      Teaching assessment administered         Independent samples t-test or Chi-Square to
skills?          assessment    at 6 practice teaching sessions, one     compare pilot and control groups.
                 (existing     per week, filled out by a trained rater
                 internal      receiving the teaching.                  One sample t-test to compare pilot with
                 instrument)                                            baseline data.

...language      Opic          Opic language assessment                 ANCOVA to control for Self-efficacy score.
acquisition?     (existing     administered during the last week of
                 instrument)   the program by trained test proctors.    Interviewer will undergo reflective journaling
...language                    ACTFL categories reported by testing     to identify bias prior to interviews.
learning         Foreign       company.                                 Interviews, observation notes, and focus
anxiety?         Language                                               group data will be transcribed and analyzed
                 Anxiety       Pilot & Control--FLAS Survey Monday      via MAXQDA to identify main ideas and
                 Scale (FLAS)  of week 2. FLAS Survey Wednesday         themes. Coded comments will be rated by
                 with Self-    of week 3. Two 60-minute focus           two raters and inter-rater reliability statistics
                 Efficacy      groups for pilot only--1) end of         reported. Participants will check focus
                 Scale, prior  English instruction, 2) last week of     group findings.
                 language      language instruction (focus group will
                 self-report   have the top 50% of language scores      Cronbach's alpha on each scale will be
                 items, and    in one focus group, bottom 50% in        reported for internal validity.
                 focus         another).
                 groups.
                               Incoming and exit survey questions--
                               existing internal survey.

The study plan matrix operationalizes the study in a clear way. Instruments from the literature review are specifically
identified. Analysis methods are clearly spelled out. In addition, the study plan will involve agreed-upon methods to
control for extraneous variables and employ accepted reliability and validity measures to control for threats to validity. A
good study plan can also control for scope-creep--when a research project is either ill-defined or a client attempts to
pork barrel the project so as to make it much larger than it should be or was originally agreed upon. The study plan can
be the basis for a business requirement document--a document that spells out timelines, cost, and deliverables for a
client.

Data Collection

Sampling

Sampling is how one determines the selection of participants in a research study. Sampling methods result in a
selection of a subset of a population. Random sampling methods (everyone in a population has an equal chance of
being selected) aid in the ability to generalize one's sample to be representative of an entire population. For example, all
fourth graders in the district are put into a random sample generator and 400 of them are randomly selected to be
representative of all 4th graders in the district. Non-random sampling is when decisions such as researcher judgment or
convenience determine one's sample. For example, selecting all fourth graders at the particular school at which one
works. Differing methods have benefits and drawbacks but the ultimate goal in all sampling methods is to seek a
representative sample of a population. For a simple explanation of types of sampling and their advantages and
disadvantages see: https://edtechbooks.org/-hcm.

                               45
Experimental Research

The simplest type of experimental research is a single treatment and a single observation. Various designs seek to
control for threats to validity (to learn more about how each design controls for threats to validity see
https://edtechbooks.org/-Woh). The following chart shows various types of experimental designs.

Table 2

Experimental Designs (R = random selection, X = experimental treatment, O = observation)

1. One-shot case study                         5. Posttest only, control group design
X O                                            R X O
2. One-group, pretest-posttest design          R O
O X O
3. Time-series design                          6. Solomon four-group design
O O O O X O O O O                              R O X O
4. Pretest-posttest, control-group design      R O O
R O X O                                        R X O
R O O                                          R O

Survey Research

Surveys are a very common method of data collection and a great tool to use when the question you are seeking to
answer can be easily responded to in categorical selections or written comments. For example, asking about the
frequency of a known behavior, how well students like a method of instruction, how well they agree or disagree with
statements about an instructional treatment, or conscious perceptions potential learners have about their learning
environment or teacher. Essentially, questions about what or how much of something are great candidates for survey
research. For example, how much do you agree or disagree with the following statement(s) about your instruction, or,
how often did your teacher follow up on your homework? An open-ended item might be as simple as: "Explain your
rating." Because surveys are so common, I will offer greater depth on this method of data collection.

General Survey Writing Tips

The following tips will help you design a better survey:

Pilot test. Test your survey out with 5-10 typical responders. Use a think-aloud protocol, where you ask people to say out
loud what they are thinking when completing the survey. Conducting a pilot test will identify a majority of any usability
or misinterpretation issues you will need to fix with your survey. In addition, asking 30 respondents to reply before
sending it to all 1000 can give you a good idea of how your categories are performing. You may find a ceiling or floor
effect for several items (all participants are selecting the highest or lowest category). This may be grounds for removing
an item or revising it to be more discriminatory. However, it may also serve as a confirmatory item.

Review sample survey output files. If you respond to a few surveys and then review the output file, it can save you hours
in data clean up later because you see how you can most effectively change the format for later analysis. For example,
perhaps the output file has data from the same respondent on different rows. Or perhaps you notice where survey logic
accidently skipped over a section that was not intended to be missed. Sometimes embedded data is not being properly
gathered. Categorical responses could be in text format rather than numeric. In addition, you can test the import of your
selected output file into your statistical software of choice. Part of that import may help to determine if the data is

                                           46
appropriate for your study plan. For example, linear regression may not be appropriate for nominal data. Nominal
logistic regression may be a more appropriate analysis method (for a further discussion of regression, see:
https://edtechbooks.org/-nWXt). Sometimes seeing the data can help inform whether your method of analysis is
appropriate.

Do not gather what you do not need. Do not waste valuable survey response time in gathering already existing
information. For example, many organizations, conferences, or workshops have participant information already
gathered. If the organization has the appropriate data-sharing agreements in place, you can embed prior gathered
demographic information and only verify its accuracy.

There are various types of items. Most survey software will offer multiple choice (a, b, c, d), Likert scales (1, 2, 3, 4, 5),
semantic differential items (agree-disagree), ranking and ordering (place the highest on top), dichotomous (true-false),
and open-ended items. For a discussion of when to use these differing items see https://edtechbooks.org/-yUXF. For an
item writing workshop or lecture from the author, see: https://edtechbooks.org/-vgvS.

Establish criteria to measure and align your items to each criteria. For example, if you are using a survey for an
implementation study, establish what teachers and students must do for a successful implementation. Then write
items for each criteria identified. I use a spreadsheet for this. Here's a simple sample of criteria and items in an
implementation survey design.

Table 3

Sample Survey Criteria

Definition of Effective Implementation of Canvas               Survey    Question Stem
                                                               Question
Teachers ensure students have technical abilities to use       Type      1. I was adequately trained on how I
Canvas.                                                                  should use Canvas.
                                                               Agree-

                                                               disagree

Teachers introduce the course resources and refer students to  Agree-    2. After initial training, there was no
them throughout the unit. Teachers are not having to provide   disagree  need to ask a teacher for directions
direction for locating assigned activities.                              on how to use Canvas.

Students know and feel Canvas will help answer their           Agree-    3-5. I knew I could go to Canvas to
questions.                                                     disagree  find answers to my questions about:
                                                                         3. class scheduling
                                                                         4. class preparation
                                                                         5. assignment due dates

Avoid leading items. For example, "How easy was it to use the app?" is a leading item because this assumes it was easy
to use. A better way to word the item would be, "Rate your experience with the app" (then use categories like 1 = easy to
use, 5 = difficult to use, etc.).

Avoid double-barreled items. Double barreled items ask about multiple elements. For example: Rate your experience
regarding class preparation and knowledge of due dates. It is better to separate these into two distinct items.

                                                          47
Use a common stem to avoid repetition. When several items or options begin with the same words, use a single stem
and put the items or options beneath it or in a survey matrix. Here's an example of a stem with a multiple choice item

What determines a person's eye color? Their parent's genetic...

a. centrioles

b. chromosomes

c. organelles

Use the right categories. There is a tendency to default to the categories your software provides, but the software may
not always give you the right options. For example, the default may be a 1-5 scale but a 1-10 scale may be more
appropriate based on participant responses. Or the software may give a numerical scale or a preset categorical scale,
but a unique categorical scale should be developed for the audience or topic. For example, in one survey I created for
teenagers, I took several days interviewing teens and testing categories to get the right "teen-speak" categories that
they could effectively use to differentiate their level of belief in certain topics.

Timing. For volunteers, survey gathering should only be a few minutes. For employees that are required or strongly
encouraged to take the survey, you can create a survey somewhat longer (7-10 minutes), while still respecting people's
time. Test the time it takes to respond to your survey as part of your pilot. In addition, consider the time your survey will
launch. When persons are busy and overloaded, they will not respond as well. For example, if you launch a survey at the
same time HR requires a 60-minute online module, fewer people will respond.

The survey invitation. The survey invitation is just as important as the survey itself. Often, people will receive survey
invitations in an email. Your email should include a brief description of the survey, the time it will take to respond, a
deadline, who is asking for the information, how the information will be used, as well as any guarantees of
confidentiality or anonymity. If there are any rewards for survey completion, how to collect the reward should be
specified. The most obvious thing the invite should include is a working survey link with an option to cut and paste the
link. During the data collection period, several follow-up invitations should be sent. Depending on the nature of the data
collected and decisions to be made, it may be worth the effort to seek out non-responders via phone interviews,
additional invites, or paper surveys to compare their responses with prior responders. Generally, your first responders
are more positive than non-responders.

Sampling. Sampling methods should be employed to avoid oversampling or survey burnout. For example, if your
population is 10,000 ready respondents, consider a random sample of 300-400 persons, especially if you are conducting
several surveys throughout the year for this population. Survey sampling websites can help you determine the
appropriate sample size. If a ready sample is not available, a researcher can pay persons to take the survey or use
social media or snowball sampling methods to find their target audience (see prior section on sampling).

As efficient as surveys are, they are not the best method for questions that require complex explanations or for studying
multiple overlapping concepts in developing fields. For example, a researcher may be seeking to design a user interface
for an app or may be forming a theory where grounded theory methods would be more appropriate.

Statistical Analysis

There are four main types of quantitative analysis: descriptive, causal, experimental, and correlational (predictive).
Descriptive analysis describes your data in a summary form. The mean score, a histogram, a standard deviation,
frequencies, and skew are all examples of descriptive statistical data. Most often, descriptive data is used to determine
the appropriate type of further analysis. For example, if your data is highly skewed, you would use a different correlation
technique than simple correlation (r). This introduces the concept of a statistical decision tree. A statistical decision
tree helps a researcher make the right decisions about using the appropriate methods for analysis. Click here to see an
image of a statistical decision tree or here for a computerized model.

                                                                                        48
Analyzing Group Differences (Causality and Experimental Analysis)

T-tests are used to compare differences in two groups, most often the mean (average) difference of two groups.
Essentially, all statistical tests are measuring whether differences are due to more than chance alone. The assumption
behind comparing two differing tests are that some experimental treatment caused the differences; in instructional
design, usually the designed curriculum or tool is assumed to be causal.

The most common t-test analysis is an independent samples t-test. This type of test compares two different samples
on a common measure. For example, online and live student's final test scores in a course are compared. A paired
samples t-test analysis is used when you have two measurements on the same person (or thing). For example, a pre-
or posttest where student one's pretest score is compared with student one's posttest score. A third type of t-test is a
one-sample t-test. This compares the mean of a single group with a known group. For example, comparing a current
cohort's attitudes towards learning math with baseline historical data from prior years.

But how do you analyze differences in samples when there are three or more groups? You could perform several
different t-tests, but this can become very complicated when there are several groups. Analysis of Variance (ANOVA)
analyzes mean differences among several samples and yields similar statistics to t-tests to show differences are more
than due to chance alone.

Analysis of Covariance is similar to ANOVA but seeks to remove the effects of known variables. ANCOVA can also be
used when simultaneously analyzing categorical variables and continuous variables and how they affect a third
variable. For example, suppose you are testing three different curricula, but you cannot assign students at random. To
control for this lack of random sampling, you administer a pretest score. The pretest score is a very strong predictor of
your posttest score. With ANCOVA, if the relationship between a pretest and posttest score can be statistically
quantified, the effects of the pretest level can be controlled to examine the overall effect of the three curricula
(essentially seeking to remove the impact of pre-existing knowledge).

Analyzing Relationships (Predictive Analysis)

Correlational research seeks to examine relationships between two variables, such as the relationship between the
amount of books read and five paragraph essay scores. Often, the goal is to find predictive relationships (e.g. those who
read 5 books a month are likely to have 3 times higher scores on 5 paragraph essay scores than those who read 1 book
a month).

The simplest of relationships is a linear relationship, or a line. As one variable changes (increases or decreases) another
variable changes in a consistent manner (increases or decreases). Pearson's r is used with simple continuous data
(foreign language anxiety scores relationship with language acquisition scores). Spearman's rho is used with rank order
data (rank in graduating class relationship with rank on the SAT). Phi coefficient is used with dichotomous categorical
variables (Instagram account or not and retired or not).

Regression statistics measure the relationships between many variables. Linear, or simple regression produces a best
fit line for prediction between two variables (e.g. score on one test and score on a second test). Multiple regression is
when a dependent variable is predicted by many or multiple variables (amount of time studying, days in class, and
pretest scores all predict final exam score). There are many specialized types of regression models used in predictive
modeling and to exhaust them all would be a much larger paper, but linear and multiple regression are the most
common types of regression models used in instructional design research. To learn more about correlational, predictive,
descriptive, and experimental research analysis, enroll in a quantitative research methods course.

Common Mistakes in Quantitative Data Analysis

Following are the most common mistake in quantitative data analysis:

                                                                                        49
  1. Inappropriate sampling. For example, too small of a sample, not using a random sample when the analysis method
      requires it, and over generalizing about all persons in a group when only a sub-population of the group was
      sampled (a.k.a. extrapolation error).

  2. Inappropriate methods. For example, using Pearson's r for correlation with rank order data. Or, using a simple t-test
      when the data is highly skewed and a Wilcoxon method is more appropriate. A statistical decision tree can help you
      avoid this mistake.

  3. Failure to report descriptive statistics. Novice researchers may jump right into their analysis or report of their
      findings without explaining why a method was chosen or not chosen. A simple qualifier in your analysis such as, "A
      histogram showed the data was not normal. Therefore assumptions of normality were not met and XYZ method
      was chosen for analysis," would suffice.

  4. Failure to review and clean data. Your data must be in the right format for the statistical package you choose to
      use. Therefore, reviewing data, often in spreadsheet form, or reviewing the first few lines in your statistical package
      can help you see if the variables are all aligned with the data, if they are consistently coded, and if you are analyzing
      all or just some of your data.

  5. Miscoding variables. Most statistical packages allow for some transformations or computing of variables. For
      example, strongly agree to strongly disagree could be recoded as 1 through 5. Or a total score can be computed
      with several or all of the variables. When recoding, be consistent. If 1 = strongly disagree on one variable, it should
      be consistent throughout the dataset. It's always good to double check when transforming data or recoding data.

  6. Failure to account for missing data. Novice researchers often forget to decide how to account or code missing
      data. Others mistakenly treat missing data as a 0, and the results can be inaccurate.

Reporting Research to Stakeholders

In academic research, you report to a committee, usually with differing opinions, preferences, and specialties. In
business, industry, and government, reporting is not very different. Learning what your stakeholders are interested in
and how they prefer reports is important. For example, if they prefer visuals in the forms of graphs, charts, and process
flowcharts, create those. If they prefer quantitative data (such as means, standard deviations, correlations, regression
lines) over qualitative data (stories, experiences, and personas), then it may be worth reporting such data. However,
most stakeholder groups are diverse enough it is often best to include several differing types of reporting, visuals,
tables, flowcharts, diagrams, and rich experiences and stories.

Often, a persona of a typical person in a group or subgroup illustrates poignant areas of findings. For example, one
client wanted research to identify who applies to teach for them. After analyzing data from 1500 applications, we
created the following persona:

This is Sophia. Sophia is a Latin American Studies Major, with a Spanish Minor. She currently does not have a job but is
looking. One of her friends suggested a job at [your organization]. She is bilingual and has just returned home from
abroad in a Latin American country. She loves [the organization's] environment and mission and is always looking for
opportunities to share what she has learned, especially with new language learners. Statistically, Sophia's persona is the
most likely to apply to [your organization].

After the reports are presented and, data is shared and reminded, do not take it personally if the findings are not
immediately acted upon or if every recommendation from the committee or research team is regarded. It often takes
time for organizations to act upon new findings, and organizations are often juggling many initiatives. Often,
researchers are not involved in strategic planning, so if your research findings are not immediately acted upon, there
may be a strategic plan for acting on them at a later date or there may be other more pressing needs to address for the
organization.

In one office I worked for, we had a phrase we used to describe a concept of waiting to share findings: "Don't share the
wine, before the time." A common mistake of researchers is to share their findings before they have been fully gathered
or analyzed. For example, the first 100 of 300 replies are analyzed and the most common finding is that students are
enjoying the new LMS. However, there are still 200 responses left to code and analyze. Suppose the other findings differ

                                                                                        50
but the findings have already been shared or reported as positive. And no matter how many times you say "this is
preliminary data," all a stakeholder hears is a fact.

Conclusion

This chapter is an introduction to research that can be useful to inform design decisions. The tools of research in
instructional design are similar as the tools of research in most other disciplines. Once the basic principles of research
are mastered in one setting, it is easier to begin using them in others.
Even though there is a science filled with appropriate and inappropriate research decisions, there is also an art to
research. With experience, a researcher sharpens their research skills and knowledge of when to use which method to a
point where they see the art in the science, and the science in the art. As a researcher, I enjoy this process of aiding an
organization or client in the art of discovery. Among my colleagues, I often joke about researchers being a special type
of breed. We are curious by nature, so curious, that it leads us to almost crave discovery. And, in my opinion, that is why
you will find the most curious minds are always engaged in research.

                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/research_for_design.

                                                                                        51
52
  4

Determining Environmental and Contextual Needs

Jill E. Stefaniak

Because instructional design emphasizes facilitating learning and improving performance, instructional designers must
begin by acquiring necessary information about their learners' educational journeys. Needs assessment can assist
instructional designers to make recommendations and design appropriate solutions (both instructional and non-
instructional solutions) that will assist their learners in translating what is taught to their successful implementation.

The purpose of a needs assessment is to identify the gap between the current state of performance and the desired
state of performance (Altschuld & Kumar, 2010). This gap in performance is what then becomes the need. While needs
assessment can be a powerful and informative tool, the instructional designer cannot get lost in analysis and delay their
design work (Stefaniak, Baaki, Hoard, & Stapleton, 2018). They need to be able to work within the scope of their design
space, rely on the resources they have available to them, and make decisions to the best of their knowledge.

This chapter will address how validating needs and contextual factors influencing learner performance can be
accounted for in instructional design to ensure the transfer of learning in real-world contexts. It will also demonstrate
how information gathered from needs assessment can be leveraged to identify and develop the necessary scaffolds to
manage the learning experience.

Exploring the Intersection Between Needs Assessment, Needs
Analysis, and Instructional Design

Richey, Klein, and Tracey (2011) defined instructional design as "the science and art of creating detailed specifications
for the development, evaluation, and maintenance of situations which facilitate learning and performance" (p. 3). If we
were to dissect this definition, I would point out that instructional designers are responsible for the following: (1)
creating detailed specifications; (2) conducting evaluations; and (3) maintaining of situations that facilitate learning and
performance.

The information that a needs assessment yields provides the details and specifications needed for an instructional
designer to create an instructional product that is customized and accounts for the unique needs of the learning
audience. It also provides benchmark data regarding the current level of performance (or situation) that the
instructional designer and their team can evaluate and compare after instructional interventions have been designed
and implemented. Instructional designers and the team members will also be better positioned to monitor the
instructional delivery and transfer of knowledge to the job or desired application if they have been presented with
sufficient data concerning these phases.

It is important to differentiate between needs assessment and needs analysis as they are not synonymous with one
another but are often used interchangeably. Needs assessment is the process of gathering information to determine
whether there is a gap between the current state and the desired state. This gap yields the need. Needs analysis is the

                                                                                        53
process of further investigating the situation to understand why this gap exists in the first place. The data that is
gathered during the needs assessment is analyzed to determine what is contributing to or causing the gap (Kaufman &
Guerra-Lopez, 2013).

Needs assessment and needs analysis provide an opportunity for an instructional designer to develop instructional
materials that can have a meaningful impact on their learning audience. In more cases than not, when instructional
designers are brought onto a project, the solution (need) has already been decided:

     We need to design an online degree program
     We need to design a safety course for incoming employees
     We need to design a team training course for the hospital staff

If you look closely, you will see that each of the above-mentioned statements contained the word need. Whether it is
your client or a supervisor, the need has already been decided. Another caveat is that there are a lot of times where the
need has been decided with no needs assessment ever having been conducted (Peterson & Peterson, 2004).
Oftentimes when this occurs, the instructional designer begins work on their tasks only to find that they have a lot of
unanswered questions:

      Why are the learners experiencing this problem?
      How will they use the instruction after training takes place?
      How will we know if they are implementing what they have learned in their actual jobs?
      How do we know that the instruction we have designed is doing what it was meant to do?
      Has the organization tried this type of instructional method in the past?
      What is the rationale for proposing online instruction?
      Are we sure that instruction is going to solve the problem?
      Is there a subject matter expert that we can speak with to provide some more guidance on what the learners need?

All of these questions are very specific and unique to the learning audience of a project. Some of these questions may
be related to the instructional environment while others may be looking ahead to how learners will be expected to
transfer this knowledge to a real-world setting (i.e. the classroom, a job).

Regardless of what needs assessment model may be referenced, a typical assessment will consist of five-steps:
problem identification, identification of data sources, data collection, data analysis, and recommendations. Table 1
provides an overview of each of these steps. While these steps are usually completed linearly, the individual who is
conducting the needs assessment needs to continue to modify the problem and identify additional data sources as
more information is uncovered during the assessment. With that in mind, the needs assessment process is very similar
to the instructional design process in that both processes are recursive.

Table 1

Overview of Needs Assessment Process

Needs Assessment   Description
Step

Identification of  This step is typically completed in consult with a client (or the individual(s)) requesting
Problem            instructional design services. During this phase, the purpose of the needs assessment (the
                   problem) is identified for the instructional designer to begin gathering data to address the gap
                   in performance.

Identification of  Once the problem to be explored has been identified, instructional designers must identify
Data Sources       data sources that will help them better understand the situation. Instructional designers must
                   gather data that will help them explore the situation from multiple angles. Examples of data

                                54
Needs Assessment  Description
Step
                  sources include, but are not limited to, task analyses, direct observations, focus groups,
                  interviews, document analysis, reviews of existing work products, and surveys.

Data Collection   This phase involves the instructional designer gathering data based on the data sources that
                  were identified in the previous step.

Data Analysis     Once data collection is complete, the instructional designer begins to analyze all data to
                  identify patterns and factors contributing to the problem identified at the beginning of the
                  assessment. Depending on the findings from the data collection and analysis phases, the
                  problem may be modified to be more consistent with the actual situation as depicted by the
                  data.

Recommendations   Upon identifying patterns contributing to the problem, the instructional designer makes a list
                  of recommendations to present to their client. These recommendations are typically
                  prioritized according to the severity of need and level of urgency.

Figure 1 provides an overview of how needs assessment and needs analysis can help leverage instructional design
practices to support the transfer of learning. Conducting a needs assessment provides the instructional designer with
the opportunity to contextualize their project. It provides them with an opportunity to gain insight into things they should
include in their designs, as well as things they should consider to avoid. Regardless of the situation, a needs
assessment will help an instructional designer identify or verify the project needs. This is especially helpful when the
needs have already been identified without the guidance of a needs assessment.

Needs analysis also aids the instructional designer by providing some context as to why these needs exist in the first
place. If learners are facing recurring challenges completing a particular task, instructional designers should
understand the causes so that they can account for these issues in their designs. By developing a better understanding
of factors that contribute to or inhibit the transfer of learning, instructional designers will be able to develop a more
realistic approach to the instructional solution. It will also provide them with the opportunity to determine if certain non-
instructional interventions are needed to support the transfer of learning.

Figure 1

The Relationship Between Needs Assessment, Needs Analysis, and Instructional Design

                  55
The Role of Context in Needs Assessment

Needs assessment is recognized as being an important component of the instructional design process (Dick, Carey, &
Carey, 2009, Morrison, Ross, Kalman, & Kemp, 2013; Smith & Ragan, 2005; Cennamo & Kalk, 2019); however, it often
tends to be minimized to focus more on learner analysis. Contextual analysis is also a term that is used synonymously
with needs assessment in a lot of instructional design literature. A seminal piece written by Tessmer and Richey (1997)
suggested that contextual analysis should account for factors influencing performance in the orienting, instructional,
and transfer contexts. Figure 2 provides an overview of the more common factors that influence each of these contexts.
Tips for how to address these three contexts will be discussed further in this chapter. By addressing these factors in
instructional design practices, designers put themselves in a better position to design experiences that were relevant to
the learning audience.
Figure 2
Common Contextual Factors Influencing Instructional Design Adapted from Tessmer & Richey (1997

                                                                                        56
While contextual analysis aims at understanding the learner's work practice, needs assessment further delves into
identifying, classifying, and validating the needs of users as they pertain to the context (environment). It is imperative
that a designer fully understand the intricacies and nuances of the context (environment) so that they can design a
prototype that addresses particular contextual factors that may support or inhibit the transfer of learning into the real-
world environment (Smith & Ragan, 2005). These factors, both good and bad, ultimately influence the instructional
designer's design.

While there are different types of analyses that an instructional designer may be required to employ during a project, it is
important to recognize that while they are all different, they are not mutually exclusive. While each has different foci, all
of these foci fall under the needs assessment umbrella.

Table 2

Overview of Analyses an Instructional Designer May Utilize to Inform Their Design

Method of       Description                                                Resources and Studies for
Analysis                                                                   References
                Analysis that occurs after a needs assessment has been
Needs Analysis  conducted to understand the root causes contributing to a  Brown (2002)Crompton,
                problem.                                                   Olszewski, and Bielefeldt
                                                                           (2016)Dick and Carey
                                                                           (1977)Stefaniak et al.
                                                                           (2018)Stefaniak, Mi, and Afonso
                                                                           (2015)

                57
Method of         Description                                                     Resources and Studies for
Analysis                                                                          References
                  The process of analyzing factors that may contribute to or
Contextual        inhibit knowledge acquisition and transfer of learning.         Arias and Clark (2004)Morrison,
Analysis                                                                          Ross, and Baldwin (1992)Perkins
                                                                                  (2009)Tessmer and Wedman
                                                                                  (1995)

Environmental     The process of focusing on the impact that the learner may      Lowyck, Elen, and Clarebout
Analysis          have on the environment outside of the organization such as     (2004)Marker (2007)Rothwell
                  customers, competitors, industry, and society.                  (2005)Tessmer (1990)

Learner Analysis  The process of capturing an in-depth understanding of an        Baaki et al. (2017)Dudek and
                  instructional designer's learning audience. Demographic data,   Heiser (2017)Öztok
                  prerequisite skills, and attitudinal information are typically  (2016)Stefaniak and Baaki
                  gathered to inform the instructional designer.                  (2013)van Rooij, S. W. (2012)

Task Analysis     The process of conducting direct observations of individuals    Jonassen, Tessmer, and Hannum
                  performing job-related tasks and documenting in a step-by-      (1998)Militello and Hutton
                  step fashion. Task analyses are done to help instructional      (1998)Schraagen, Chipman, and
                  designers design instruction that is aligned with how the job   Shalin (2000)
                  will be performed in a real-world setting.

Table 2 provides an overview of the various types of analyses that may be used. Examples of instructional design
studies that have explored these topics in more detail are also included for reference. A commonality among all of
these analyses is that they typically involve collecting data from multiple sources to gain a better understanding of the
situation. Out of all of the analyses listed in Table 2, needs assessment is most often the most time-consuming
because it requires instructional designers to identify appropriate data sources, collect data, conduct data analysis, and
consult with their client on recommendations for moving forward. Direct observations, document analysis, interviews,

                                                                                        58
focus groups, and surveys are all examples of the types of data collection tools an instructional designer may utilize
when conducting an analysis.

The use of the above-mentioned data sources has been used to inform the development of learner personas in
instructional design (Anvari & Tran, 2013; Avgerinou & Andersson, 2007; Baaki, Maddrell, & Stauffer, 2017; van Rooij,
2012). With more emphasis being placed on user experience design practices, more attention is being placed on who
our learners are as opposed to generalizing the learning audience. Learner analyses and contextual analyses are
complementary in that both yield data that will inform the other. Environmental analyses add an additional layer by
focusing on the impact that the learner may have on the environment outside of the organization such as customers,
competitors, industry, and society (Rothwell, 2005).

The Reality of Instructional Design Work and Needs
Assessment

While I would love to see every instructional designer be an advocate for needs assessment and push back when
clients or supervisors present need statements with no assessment validating that the identified needs warrant
instruction, the reality is that most instructional designers will have a hard time arguing the need to pause a project and
conduct a thorough needs assessment (Hoard, Stefaniak, Baaki, & Draper, 2019; Stefaniak et al., 2018). Needs
assessments are conducted; but often because the client has recognized the importance of needs assessment before
approaching an instructional designer to work on a project. It is also important to note that a needs assessment is only
as good as the data that is collected.

Table 3

Needs Statements and Further Inquiries

Client Need Statements      Instructional Designer Inquiries

We need to design an        How are courses currently being offered?What is the market for online instruction?
online degree program.      What is the rationale for moving towards the development of an online degree?

We need a new learning      How are training materials currently being stored?What features are used in the existing
management system.          LMS?What features are needed?How are the instructors and students currently using
                            the LMS?

We need to design a safety  What do incoming employees need to know about safety upon starting a new job?What
course for incoming         incident(s) occurred that suggests there is an immediate need to create a safety
employees.                  course?What other training courses are incoming employees expected to complete?

What does this mean for the instructional designer? Recognizing that the absence of a thorough needs assessment is a
common issue in our field, there are strategies that instructional designers can employ to gather additional data and
information relevant to the project they have been assigned.

If a client has decided to conduct a needs assessment, it is important for the instructional designer to participate in
framing the needs by asking appropriate questions. Table 3 provides an overview of examples of needs statements and
questions an instructional designer can ask to gain further clarification of the situation. Like most projects, there are
varying degrees of complexity an instructional designer can delve into when addressing needs assessment (Rossett,
1999). The amount of time and resources that an instructional designer can apply towards gathering additional data for
a project will ultimately determine the scalability of the level of analysis that is completed (Stefaniak, 2018; Tessmer,
1990).

                            59
Just because a client or a supervisor may not allocate the time or funding needed to support a needs assessment, that
does not mean that the instructional designer has to abandon the idea altogether. At the very least, there are key
components that an instructional designer should address during an initial intake meeting with the client or kick-off
meeting with the instructional design team. Table 4 provides examples of different steps instructional designers can
take if they were to scale a needs assessment project.

Table 4

Scalability of Instructional Design Needs Assessments

Level of Scale  Tasks

Low (1-2              Review existing training materials.
weeks)                Review documents explaining job processes.
                      Meet with a subject matter expert (in the organization) to provide guidance on content that
                      should be emphasized in the instructional product.
                      Obtain an overview of the learning audience by the client.

Medium (1       Review existing training materials.
month)          Conduct observations of employees performing job tasks.
                Update existing task analyses.
                Meet with individuals that represent multiple levels of authority within the organization related
                to the instructional project.
                Obtain an overview of the learning audience by the client.

High (several   Review existing training materials.
months)         Review strategic planning documents.
                Meet with individuals that represent multiple levels of authority within the organization.
                Conduct observations of employees performing job tasks.
                Update existing task analyses.
                Conduct interviews and/or focus groups to understand factors that are inhibiting the transfer
                of learning.
                Triangulate information from multiple sources to understand patterns contributing to or
                inhibiting employee/learner performance on the job.

Table 5 provides an example of a form that instructional designers can use to gather the data they need to ensure their
instructional design work is contextually relevant to the learners' needs. This form is not meant to be an exhaustive list
of questions instructional designers should ask at the beginning of a project; rather, it is intended to help instructional
designers spark conversation with their client about the contextual factors and needs of the project that should be
addressed throughout the design. Depending on the information provided in the intake form, instructional designers will
decide whether a detailed task analysis is required to understand specific tasks expected of the learning audience.

Table 5

An Example of an Instructional Design Intake Form

INSTRUCTIONAL DESIGN PROJECT INTAKE FORM      Client:
Date:

                                          60
INSTRUCTIONAL DESIGN PROJECT INTAKE FORM

Instructional Designer:                                                Project Name:

PROJECT OVERVIEW

1. What is the purpose of the project (instructional need)?
2. What is the scope of the project?
3. Learning platform (i.e., face-to-face, blended, online)
4. Overarching course goal
5. Learning objectives
6. What level of importance is the training? (i.e., severe, moderate, mild)

LEARNING AUDIENCE

  1. Who is the intended learning audience?
  2. What are the learners' experiences with the project topic?
  3. What challenges do learners typically experience with this topic?
  4. What are the learners' overall attitudes toward training?
  5. What information will the instructional designer have access to regarding the learning audience? (i.e., job

      observations, meetings with learners, work products, interviews, etc.)

INSTRUCTIONAL ENVIRONMENT

  1. How will the instruction be delivered?
  2. How will learners access the material?
  3. What is the length of the course?
  4. What are the learners' roles during instruction?
  5. What is the instructor's role during instruction?
  6. What types of assessment need to be included in the instruction?

TRANSFER (APPLICATION CONTEXT)

  1. How soon after the training will learners apply their newly acquired skills?
  2. What are the anticipated challenges with applying these new skills in a real-world environment?
  3. What resources are available to support learners during this transfer phase (i.e., job aids)?
  4. Who is responsible for monitoring learners with transference?

EVALUATION

  1. How and when will the instructional training be evaluated for effectiveness?
  2. Who will be responsible for conducting an evaluation?
  3. What methods of evaluation will be used to determine the efficiency and effectiveness of the instruction?

OTHER COMMENTS

                                          61
Conclusion

To adhere to Richey et al.'s (2011) definition of instructional design encompassing the facilitation of learning,
instructional designers must task themselves with gathering as much information as they can to understand the
contexts that their learners will experience (i.e. the learning and transfer contexts). Not only is it necessary for the
instructional designer to understand the instructional environment, but they must also have insight into how their
learners will apply the knowledge obtained from instruction and apply it to a real-world setting. The purpose of this
chapter is to provide instructional designers with an introduction to the potential that needs assessment offers
instructional designers and provide some strategies and tools that can be applied to an instructional design project
regardless of the context.

References

Altschuld, J. W., & Kumar, D. D. (2010). Needs assessment: An overview. Los Angeles, CA: SAGE.

Anvari, F., & Tran, H. M. T. (2013, May). Persona ontology for user centred design professionals. In Proceedings of the
        ICIME 4th International Conference on Information Management and Evaluation (pp. 35-44).

Arias, S., & Clark, K. A. (2004). Instructional technologies in developing countries: A contextual analysis approach.
        TechTrends, 48(4), 52-55.

Avgerinou, M. D., & Andersson, C. (2007). E-moderating personas. The Quarterly Review of Distance Education, 8(4),
         353-364.

Baaki, J., Maddrell, J., & Stauffer, E. (2017). Designing authentic and engaging personas for open education resources
        designers. International Journal of Designs for Learning, 8(2).

Brown, J. (2002). Training needs assessment: A must for developing an effective training program. Public personnel
        management, 31(4), 569-578.

Cennamo, K., & Kalk, D., (2019). Real-world instructional design: An iterative approach to designing learning experiences
         (2nd ed.). New York, NY: Routledge.

Crompton, H., Olszewski, B., & Bielefeldt, T. (2016). The mobile learning training needs of educators in technology-
        enabled environments. Professional Development in Education, 42(3), 482-501.

Dick, W., & Carey, L. M. (1977). Needs assessment and instructional design. Educational Technology, 17(11), 53-59.
Dick, W., Carey, L. M., & Carey, J. O. (2009). The systematic design of instruction (7th ed.). Upper Saddle River, NJ:

         Pearson.

Dudek, J., & Heiser, R. (2017). Elements, principles, and critical inquiry for identity-centered design of online
        environments. Journal of Distance Education (Online), 32(2), 1-18.

Hoard, B., Stefaniak, J., Baaki, J., & Draper, D. (2019). The influence of multimedia development knowledge and
        workplace pressures on the design decisions of the instructional designer. Educational Technology Research and
        Development 67(6), 1479-1505.

Jonassen, D. H., Tessmer, M., & Hannum, W. H. (1998). Task analysis methods for instructional design. New York, NY:
         Routledge.

Kaufman, R. & Guerra-Lopez, I. (2013). Needs assessment for organizational success. Alexandria, VA: ASTD Press.

                                                                                        62
Lowyck, J., Elen, J., & Clarebout, G. (2004). Instructional conceptions: Analysis from an instructional design perspective.
        International Journal of Educational Research, 41(6), 429-444.

Marker, A. (2007). Synchronized analysis model: Linking Gilbert's behavior engineering model with environmental
        analysis models. Performance Improvement, 46(1), 26-32.

Militello, L. G., & Hutton, R. J. (1998). Applied cognitive task analysis (ACTA): A practitioner's toolkit for understanding
        cognitive task demands. Ergonomics, 41(11), 1618-1641.

Morrison, G. R., Ross, S. M., & Baldwin, W. (1992). Learner control of context and instructional support in learning
        elementary school mathematics. Educational Technology Research and Development, 40(1), 5-13.

Morrison, G. R., Ross, S. M., Kalman, H., & Kemp, J. (2013). Designing effective instruction (7th ed.). San Francisco, CA:
         John Wiley & Sons.

Öztok, M. (2016). Cultural ways of constructing knowledge: The role of identities in online group
        discussions. International Journal of Computer-Supported Collaborative Learning, 11(2), 157-186.

Perkins, R. A. (2009). Context-oriented instructional design for course transformation. New Directions for Teaching and
        Learning, 118, 85-94.

Peterson, T. O., & Peterson, C. M. (2004). From Felt Need to Actual Need: A multi-method multi-sample approach to
        needs assessment. Performance Improvement Quarterly 17(1), 5-21.

Richey, R. C., Klein, J. D., & Tracey, M. W. (2011). The instructional design knowledge base: Theory, research, and
        practice. New York, NY: Routledge.

Rossett, A. (1999). First things fast: A handbook for performance analysis. San Francisco: CA: John Wiley & Sons.

Rothwell, W. (2005). Beyond training and development: The groundbreaking classic on human performance
        enhancement (2nd ed.). New York, NY: Amacom.

Schraagen, J. M., Chipman, S. F., & Shalin, V. L. (2000). Cognitive task analysis. Mahwah, NJ: LEA.

Smith, P. L. & Ragan, T. J. (2005). Instructional Design (3rd ed.). San Francisco, CA: John Wiley & Sons.

Stefaniak, J. E. (2018). Performance Technology. In R. E. West, Foundations of learning and instructional design
        technology: The past, present and future of learning and instructional design technology. EdTech Books.
         Retrieved from https://edtechbooks.org/lidtfoundations/performance_technology

Stefaniak, J. E., & Baaki, J. (2013). A layered approach to understanding your audience. Performance Improvement,
        52(6), 5-10.

Stefaniak, J., Baaki, J., Hoard, B., & Stapleton, L. (2018). The influence of perceived constraints during needs
        assessment on design conjecture. Journal of Computing in Higher Education, 30(1), 55-71.

Stefaniak, J. E., Mi, M., & Afonso, N. (2015). Triangulating perspectives: A needs assessment to develop an outreach
        program for vulnerable and underserved populations. Performance Improvement Quarterly, 28(1), 49-68.

Tessmer, M. (1990). Environment analysis: A neglected stage of instructional design. Educational Technology Research
        and Development, 38(1), 55-64.

Tessmer, M., & Richey, R. C. (1997). The role of context in learning and instructional design. Educational Technology
        Research and Development, 45(2), 85-115.

Tessmer, M., & Wedman, J. (1995). Context-sensitive instructional design models: A response to design research,
        studies, and criticism. Performance Improvement Quarterly, 8(3), 38-54.

                                                                                        63
van Rooij, S. W. (2012). Research-based personas: teaching empathy in professional education. Journal of Effective
        Teaching, 12(3), 77-86.
                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/needs_analysis.

                                                                                        64
  5

Conducting a Learner Analysis

José Fulgencio & Tutaleni I. Asino

As mentioned at the outset: designing a course that best fits the needs of learners requires both an understanding of
who the learners are, as well as actual efforts to evaluate and understand their needs. The chapter reviewed both
conceptual issues that concern learner analysis as well as practical approaches you can use to analyze actual learner
needs.

Because of this, learner analysis is an important aspect of the instructional design process. It is important to remember
that learners are not empty containers in which knowledge can simply be poured. They have experiences through which
they understand the world and through which they will understand or evaluate the instruction. In this way, learning is a
process that involves change in knowledge; it is not something that is done to learners but instead something that
learners do themselves (Ambrose et al., 2010). Hence, "consideration of the learners' prior knowledge, abilities, points
of view, and perceived needs are an important part of a learner analysis process" (Brown & Green, 2015, p.73).

Although various scholars may use different verbiage, broadly, a learner analysis can be understood as the process of
identifying critical aspects of the learner, including demographics, prior knowledge, and social needs (Adams Becker et
al., 2014), and "is characterized as an iterative process that informs vital instructional design decisions from front-end
analysis to evaluation" (Saxena, 2011, p. 94) by customizing the instruction to the previous knowledge of each individual
learner so that the learner controls their own learning and has a deeper understanding of the classroom material
(Reigeluth & Carr-Chellman, 2009). For example, an instructor teaching a biology master's program can expect learners
to have a solid foundational knowledge of biology. At an undergraduate level however, the instructor may expect
students to have a somewhat limited understanding of biology. The instructor will also have to take into consideration
the learner group characteristics such as first-generation students, international students, adult learners, and learners
with accessibility needs (e.g. requiring note-taking accommodations and extra time on exams), all of which may
influence teaching of content, distribution of content, and pace of content distribution in the classroom. Another
characteristic is the learning preferences within the group of learners, such as whether they prefer and respond better to
small group learning, hands-on experiences, or case studies.

Much has been written about learner analysis, in terms of definition and the process by which it can be accomplished.
However, regardless of the definition advanced, what is important to discern is that through a learner analysis, the
learner contributes to the instructional design of the course and miscommunications between the learner, instructor,
and course goals are identified (Adams Becker, 2014; Dick et al., 2009; Jonassen et al., 1999; Fink, 2013). A learner
analysis ensures that the learner benefits from a productive learning environment that can leave a lasting impact on
their lifelong learning (Adams Becker et al., 2014; Dick et al., 2009; Jonassen et al., 1999; Fink, 2013).

The focus of this chapter is on how to conduct a learner analysis. This process often includes identifying learners'
characteristics, their prior knowledge, and their demographics, all of which are key factors to consider when designing a
learning environment (Adams Becker et al., 2014; Dick et al., 2009; Jonassen et al., 1999; Fink, 2013). Demographics
include the environment in which the learner lives and works, ethnicity, accessibility to technology, and educational

                                                                                        65
background. Other factors--such as motivation, personal learning style, and access to content--also play a role in how
individuals learn (Adams Becker et al., 2014; Dick et al., 2009; Jonassen et al., 1999; Fink, 2013).

The chapter begins with explaining the components of a learner analysis, describing reasons for a learner analysis, and
providing a learner analysis worksheet. The next section of the chapter explains an area that the authors believe is often
not discussed when writing about learner analysis: the ethics of working with learners, developing personas, and
experience mapping. The last section of the chapter includes a learner analysis design project to enable the reader to
put into practice some of what is covered in the chapter.

Components of a Learner Analysis

When designing learning environments, there needs to be a birds-eye view of the entire process from who the learner is,
the environment, background of the learner, and the goal of the learning environment. An educator cannot make
assumptions about learners based on the educator's experience. The following are key factors of the learner analysis to
consider.

1. Learner Characteristics

Understanding the characteristics of learners can help shape the design of the course. For example, if your class is an
executive-level course for Fortune 500 high-level officers, you may expect learners with professional experience, and
who have different goals for learning and their careers, which is different from a class of undergraduate students who
have little to no work experience.

In examining factors of learner characteristics, these are key questions to think about (Adams Becker et al., 2014; Dick
et al., 2009; Jonassen et al., 1999; Fink, 2013):

      Who are the learners?
      What personal characteristics do these learners possess?
      What are the dimensions of the learner?
      What contributes to the reason for learning about the topic?
      What is the reason for enrolling in the course?
      What is it about the topic that motivates the learner?

2. Prior Knowledge

Time is a finite resource for most people, so instructional time should not be wasted covering material that learners
already know, but instead building on their prior knowledge. Students' prior knowledge influences how they interpret and
filter new information given in the classroom (Ambrose et al., 2010; Cordova et al., 2014; Dochy et al., 2002; Umanath &
Marsh, 2014).

In examining factors of prior knowledge, there are key questions to think about:

      What do learners already know?
      How might this information contribute to the content and order of what you teach?

3. Demographics

Understanding who the learners are and their demographics can directly impact the instructional material. It is
important, for example, not to include instructional material that may be culturally insensitive or that has no connection
to students. This is particularly important when using media such as film that could be considered historic to one group
and offensive to another. Culture is integral to learning and plays a central role in "determining the learning preferences,
styles, approaches and experiences of learners" (Young 2014, p. 350). It is worth noting that culture can also relate to
organisational cultures. For example, using learning materials or illustrations that promote collaboration amongst

                                                                                        66
employees in an organization that does not have or prioritize such a practice, may run contrary to the typically
established culture.

In examining factors of demographics, key questions to think about are:

      Where are the learners coming from in terms of their education level, ethnicity, demographic, hobbies, area of study,
      grade level?
      Why are these demographics important for the material you will be teaching?

4. Access to Technology

In education, it is important to make sure that all learners have access to the educational material. As technology
becomes a necessity to participate in learning opportunities, it is also important to gauge whether or not students have
access to technology. Material should be flexible, but you can imagine if you are assigning work through an app that is
only available for Apple devices, how this can affect learners who own Android phones. Thus, make sure that
throughout the course, educational material is universally accessible.

Sometimes issues of access can be tricky or surprising. For example, if there is only one computer, or limited internet
bandwidth, but two parents and two children all need to access it for their job or homework, then there is not sufficient
access. Similarly, the computer or internet access may be too old to play the instructional multimedia in a module. Thus,
it is important to look beyond the statistics to truly understand the level of access.

In examining factors of access, key questions to think about are:

      How accessible is technology to every learner in my class?
      Are learning materials universally accessible for individuals with disabilities?
      If access is not universal, how can I adapt my course curriculum to include all learners?

Put Your Skills to Use: The Learner Analysis Worksheet

When conducting a learner analysis, a collection of learner information will help develop a positive learning
environment. The Learner Analysis Worksheet below is one way to collect and record key factors and general
information about the learners, using information available from student enrollment data. This worksheet can be
adapted for designing instruction for various learning environments. Student information is often provided when a
student enrolls, and academic advisors or student enrollment professionals may also be able to share this information
with you. Another way to gather demographic information is to speak with the colleagues in your department. Who are
the students who usually register for this course?

For example, a community college will have higher enrollment of non-traditional and first-generation students who are
older than 25 and who are full-time workers compared to the conventional student body of 18 to 22-year-olds at a
traditional institution who are part-time workers. The more information you can gather for the Learner Analysis
Worksheet in Table 1, the more equipped you will be in designing the best learning environment for your learners.

Table 1

Learner Analysis Worksheet

Demographic Characteristics                        Learner
                                                   Details
Size of target audience
Are there any subgroups that may participate?

                                               67
 Age ranges

 Educational/grade level, or academic program year. How long have they been out of an educational
 setting?

 Gender breakdown

 Cultural backgrounds

 Primary language

 Employment status

 Socioeconomic status

 Traditional/non-traditional/first generation learners?

 Geographic location(s)

 Internet connectivity?

 Access to technology?

Note. Adapted from https://en.wikiversity.org/wiki/Instructional_design/Learner_analysis/what_when_why

Ethics of Working With Learners

There is now an ever-increasing amount of information on students available on the internet broadly, and specifically
through learning management systems and social media that institutions and designers can access. Data on learners
includes but is not limited to: personal information, enrollment information, academic information, and other data
collected by educational institutions. What was once kept private between the learner and institution on paper can no
longer be assumed as safe. Records which are now held in digital format are vulnerable to hackers and are enticing to
outside agencies that are seeking to monetize the data. How, then, do institutions assure ethical use of learners' data
that may be needed or used for learner analysis? How much data is reasonable to share? If institutions are asking
learners to be ethical in their academic assignments, shouldn't institutions do the same when it comes to working with
learners? This section covers professional expectations regarding ethical conduct towards learners.

Professional Expectations

In the context of conducting a learner analysis, a professional is expected to be "committed to the needs and best
interests of their clients who are basically their learners" (Wainaina et al., 2015, p. 68). There are various code of
conducts from which one can draw guidance for ethical practice as most professional organizations have codes of
conduct or ethics. An example is the Association of Educational and Communication Technology (AECT), which is
available at (https://edtechbooks.org/-RXIX) and aims to aid all members of AECT both individually and collectively in
maintaining a high level of professional conduct. However, it is critical to know that just because one adheres to a code
of ethics, it does not mean there will never be conflict. What is unfortunately inherent in all human relationships is a
level of conflict, even when one has good intentions. So the question then is what happens when conflicts or perceived
ethical violation occurs especially when a designer is engaged in collecting data needed for learner analysis? There are
various approaches, but here we suggest the following ethical framework developed by Mathur and Corley (2014) which
suggests considerations and questions to ask:

                                                                                        68
      Fact-finding - Most conflicts are related to communication or lack thereof. Hence one of the first steps is to
      engage in fact-finding exercises. What are the facts? What is known and what is not known?
      Who is involved - who are the people that care about this case or incident? What has been (mis)communicated?
      Who are the individuals involved?
      What is the conflict? - Is the conflict about the frameworks being used? If so, what are those frameworks and what
      is conflicting? If the conflicts concern the values, morals, or policies, establish what those are and what needs to be
      adhered to.
      Potential consequences to actions -- What are some of the possible consequences for any actions taken to solve
      the dilemma? How would the people involved like to be treated? What is the role of the designer in solving the
      conflict (whether or not the designer is involved in causing this conflict)?
      Reflection - Lastly, reflect on the actions taken. What are the repercussions, if any, to the actions taken from the
      difficulty?

Educators have a responsibility entrusted upon them when educating learners. The duties include but are not limited to,
creating a safe environment and being professional not just in virtual space but also in digital space. When educators
neglect their responsibility to be professional and ethical (an expectation that we often have for students), this can be
detrimental to learners.

Developing Personas in Learner Analysis

It is often stated that if you want to know a person, you must walk in their shoes. This idiom captures the goal of a
learner analysis by helping us figuratively walk in someone's shoes and come to understand them more deeply. One way
to do this is through personas. Personas are fictional characters that embrace the needs and goals of a real user or
group of learners (Faily & Flechais, 2011). Personas help generate an understanding of learners and what their key
attributes are that learning designers need to know for their designs (Dam & Siang, 2019). Personas may be fictional
characters, but they are built based on real learner analysis data and thus embrace the needs and goals of real learners.

Effective personas do five things (from the following website: https://edtechbooks.org/-bXV):

  1. Represent the majority of learners
  2. Focus on the major needs of the learner
  3. Provide clear understanding of the learners' expectations
  4. Provide an aid to uncovering universal features
  5. Describe real individuals

To develop your own persona, the following chart in Table 2 can be helpful.

Table 2

Questions to Ask During Persona Development

Objective                         Questions
                                  What is the purpose of the course?
Define the purpose/vision of the  What are the goals of the course?
course

Describe the user                 Personal
                                  What is the age of the learner?
                                  What is the gender of the learner?

                                  69
User motivation  What is the highest level of education this learner has received?
                 Professional
                 How much work experience does your learner have?
                 What is your learner's professional background?
                 Why will the learner take the course?
                 Technical
                 What technological devices does the learner use on a regular basis?
                 What software and/or applications does the learner use on a regular basis?
                 Through what technological device does your user primarily access the web for
                 information?

                 What is the learner motivated by?
                 What are the learner's needs?

Note. From the U.S. Government usability website (U.S. Department of Health & Human Services, 2020) "questions to
ask during persona development" chart.

When developing your persona, remember to organize the information in an easy-to-read logical format, and make it as
visual as possible to convey the greatest sense of the "humanness" of the learners. Key pieces of information to include
are the persona group (i.e. learner), fictional name, personal demographics, goals and tasks for the course,
physical/social/technical environment, and a casual picture representing their learning environment.

Following in Figures 1-4 are some examples that provide an illustration of worksheets and examples for creating
personas.

Figure 1

Persona Worksheet 1

                 70
                                                                                 Note. Persona worksheet from Open Design Kit

https://edtechbooks.org/-oyBd
Figure 2
Persona Worksheet 2

                                                                                        71
Note. Persona example from https://edtechbooks.org/-SCmQ
Figure 3
Persona Example 1

                                                                                        72
Note. Persona example from https://edtechbooks.org/-GLf
Figure 4
Persona Example 2

                                                                                        73
Note. Persona example from https://edtechbooks.org/-GLf

Personas are a helpful way for designers to create a more engaging, more productive, and more effective educational
experience for learners. Follow the guidelines provided in Table 2 when creating personas and be flexible and open to
new information, as the personas may not be the same from start to finish.

Understanding Learners Through Experience Mapping

The popular adage of "the customer is always right," is often used to emphasize the importance of providing excellent
customer service (Samson et al., 2017). While educational institutions are different from traditional service industries,
they can still benefit from paying attention to learners' experiences. An experience map is a strategic tool that captures
the journey of customers from point A to point B and generalizes critical insights into learner interactions that occur
across such experiences. The journey captured in experience mapping, which is adapted from Schauer (2013), is split
into four characteristics that generalize the experience of a learner:

  1. uncover the truth
  2. chart the course
  3. tell the story
  4. use the map

The first step, uncover the truth, includes studying the learner's behavior and interactions across channels and
touchpoints. Channels are the interactions a person has with a product or service. Touchpoints are the interactions of a
person with an agent or artifact of an organization. In the first part of the experience mapping, a designer finds various
data and insights relevant to the experiences in the mapping process, including actually talking to the learners. Previous
learner surveys and evaluations of the course or program are a good data source to begin. In order for the map to be
believable, it needs to tell an authentic story and provide strong insights.

                                                                                        74
The second step, chart the course, collects the takeaways from learners to create actionable results. After you have
collected data, obtained key aspects of the learner's journey, and obtained quotes from learners, it is time for the third
characteristic: tell the story visually in a way that creates empathy and understanding. The goal of this characteristic is
for the experience map to stand on its own, inspire new ideas, and foster strategy decisions.

The last step is to show the map to stakeholders that have insights and interactions with learners. Telling the story to
stakeholders provides insights into the learner's experiences. The experience must go beyond the physical location and
create an experience of usability such as identity, familiarization, memorability, and satisfaction (Ghani et al., 2016).
Failure to meet the learner's needs can result in loss of interest, bad reviews, and challenges to getting the learners to
accomplish the task.

As with personas, there are a number of examples of what format an experience map might take. Most are considered
copyrighted and proprietary to the organizations developing them and so cannot be included here, but you can find
examples of experience maps at the following sites (each also provides some practical tips for developing your own
experience maps):

      What is a Customer Experience Map? How to Create an Effective Customer Experience Map?
      The Ultimate Guide to Creating a Customer Experience Map

Conclusion

As we said at the outset: designing a course that best fits the needs of learners requires both an understanding of who
the learners are, as well as actual efforts to evaluate and understand their needs. We reviewed both conceptual issues
that concern learner analysis as well as practical approaches you can use to analyze actual learner needs.

At this point, the best the authors can offer is to wish you luck! Your learner analysis activities will lay a strong
foundation for the rest of your project, and it is worth the time it will take to set your project off right.

Practice: Learner Analysis Design Project

This learner analysis design exercise provides an opportunity to apply knowledge gained from this chapter. Imagine, you
have been hired by a company based in New York City to design a Security Awareness course that teaches newly hired
and senior employees to identify and prevent security breaches. The course focuses on teaching the company's staff
the different types of security awareness, email and phishing attacks, malware, ransomware, social media awareness,
and password security.

For your project you must do the following:

  1. Complete a full learner analysis worksheet.
  2. Complete a learner-centered design process based on the description of the course.
  3. Develop two learner personas for the course.

Upon completing the project, share and discuss with others how you completed the learner analysis worksheet, how
you developed the user-centered design and what resources were used to create the personas.

This exercise is meant to help you consider learner analysis from a practical perspective. However, realize that every
company has their own style of course design for their employees, and their own methods for conducting learner
analysis. While the principles discussed in this chapter should remain the same, the ways they are applied within any
instructional design organization may vary. Despite this variety of approaches, our goals remain the same: all
instructional designers agree on the important need to understand and empathize with learners in order to create
instruction that best meets their needs.

                                                                                        75
References

Adams Becker, S., Caswell, T., Jensen, M., Ulrich, G., and Wray, E. (2014). Online Course Design Guide. (n.d.). Cambridge,
         Massachusetts: Massachusetts Institute of Technology. Retrieved https://edtechbooks.org/-Snvzp

Ambrose, S. A., Bridges, M. W., DiPietro, M., Lovett, M. C., & Norman, M. K. (2010). How learning works: Seven research-
         based principles for smart teaching. San Francisco, CA: John Wiley & Sons.

Anitha, C., & Harsha, T. S. (2013). Ethical perspectives in open and distance education system. Turkish Online Journal of
        Distance Education, 14(1), 193-201.

Brown, A. H., & Green, T. D. (2015). The essentials of instructional design: Connecting fundamental principles with
         process and practice. Routledge.

Cordova, J. R., Sinatra, G. M., Jones, S. H., Taasoobshirazi, G., & Lombardi, D. (2014). Confidence in prior knowledge, self-
        efficacy, interest and prior knowledge: Influences on conceptual change. Contemporary Educational Psychology,
        39(2), 164-174

Dam, R., & Siang, T. (2020, June 5). Personas-A Simple Introduction. Interaction Design Foundation.
         https://edtechbooks.org/-AZfN

Dick, W., Carey, L., & Carey, J.O. (2009). The systematic design of instruction (7th ed). Columbus, Ohio. Pearson.

Faily, S., & Flechais, I. (2011, May). Persona cases: a technique for grounding personas. In Proceedings of the SIGCHI
         Conference on Human Factors in Computing Systems (pp. 2267-2270). ACM.

Fink, L. D. (2013). Creating significant learning experiences: An integrated approach to designing college courses. San
         Francisco, CA: John Wiley & Sons.

Ghani, A. A. A., Hamid, M. Y., Haron, S. N., Ahmad, N. A., Bahari, M., & Wahab, S. N. A. (2016). Methods in Mapping
         Usability of Malaysia's Shopping Centre. In MATEC Web of Conferences (Vol. 66, p.117). EDP Sciences.

Glossary of Education Reform. (n.d.). Learner. https://www.edglossary.org/

Jonassen, D.H., Tessmer, M., & Hannum, W.H. (1999). Task analysis methods for instructional design. Mahwah, New
         Jersey. Lawrence Erlbaum Associates, Publishers.

Mathur, S. R., & Corley, K. M. (2014). Bringing ethics into the classroom: Making a case for frameworks, multiple
        perspectives and narrative sharing. International Education Studies, 7(9), 136-147.

Raj Urs, S. V. R., Harsha, T. S., & Vijay, R. A. J. U. (2013). Ethical issues in open and distance education with special
        reference to expectations and reality. Turkish Online Journal of Distance Education, 14(4), 46-53.

Reigeluth, C. M., & Carr-Chellman, A. A. (Eds.). (2009). Instructional-design theories and models: Building a common
         knowledge base (Vol. 3). New York, NY. Routledge.

Samson, S., Granath, K., & Alger, A. (2017). Journey mapping the user experience. College & Research Libraries, 78(4),
         459.

Saxena, M. (2011). Learner analysis framework for globalized e-learning: A case study. The International Review of
        Research in Open and Distributed Learning, 12(5), 93-107. https://doi.org/10.19173/irrodl.v12i5.954

Schauer, B. (2013). Adaptive path's guide to experience mapping. Accessed October 8, 2019 https://edtechbooks.org/-
         CbPF

                                                                                        76
Umanath, S., & Marsh, E. J. (2014). Understanding how prior knowledge influences memory in older adults. Perspectives
        on Psychological Science, 9(4), 408.

U.S. Department of Health & Human Services. (2020). Questions to ask during persona development chart.
Wainaina, P. K., Mwisukha, A., & Rintaugu, E. G. (2015). Professional conduct of academic staff in public universities in

        Kenya: Learners' perception. International Journal of Education and Social Science, 2(6), 67-72. Retrieved from
         https://edtechbooks.org/-bXV
Young, P. A. (2014). The presence of culture in learning. In Handbook of research on educational communications and
         technology (pp. 349-361). New York, NY: Springer.

                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/learner_analysis.

                                                                                        77
78
Exploring

Exploring activities are what instructional designers do to investigate the shape of the design space in which they are
working. Given what they understand about learners, stakeholders, and a situation, what kinds of problems should
designers solve? What are the most important contributions they can offer?

     Problem Framing
     Using Task Analysis to Inform Instructional Design
     Documenting Instructional Design Decisions

                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/exploring.

                                                                                        79
80
  6

Problem Framing

Vanessa Svihla

Is design a problem solving process? To answer "No" suggests that designers do not produce solutions to design
problems. However, in order to produce such solutions, designers must first frame--and typically reframe--the problem.
Understanding this can help newcomers recognize the need for a different approach, rather than jumping straight to
solutions. What does it mean to frame a problem? In this chapter, detailed below, I define it as follows:

Problem framing: To take ownership of and iteratively define what the problem really is, decide what should be included
and excluded, and decide how to proceed in solving it.

To understand what problem framing looks like in practice, this chapter introduces and illustrates key terms that help us
speak consistently about design problems and how they differ from other problems. Vignettes highlight how designers
direct their problem framing process. The chapter concludes with tools for framing problems and diagnostics for
common pitfalls.

The Problem of the Problem: What Makes Design Problems
Different from Other Problems?

Most of us have had abundant opportunities to solve problems, beginning in elementary school. But these problems
were predominantly well-structured (Jonassen, 2000), meaning there was a single correct answer and the instructor
knew what that answer was. Repeated experiences with such problems can lead us to privilege accuracy and efficiency
over spending time dwelling with the problem. And surely getting to the right answer quickly is valuable in many
situations. But design problems differ--there is not a single right answer or even a best way to come to a solution.

As a result, when tackling these ill-structured problems we must first frame them (Jonassen, 2000). Framing a problem
involves defining the problem and bounding it, then deciding what to include and exclude and how to proceed (Dorst &
Cross, 2001; Schön, 1983). This, in turn, relies on activities described in other chapters in this text, including the
following: (a) gathering information about the task, learners, and context; (b) generating tentative ideas about the
problem and solution; (c) making and revising decisions about the problem (often influenced by precedent); and (d)
evaluating tentative ideas in light of design requirements and learner needs. Some therefore treat problem framing as a
higher-level category that includes all of these activities. Others treat problem framing as an activity threaded through
the design process. Regardless, problem framing is the process by which designers take ownership of a problem. This
means that two designers, given identical design briefs, would not only produce different solutions, but would have
solved different problems (see Figure 1.; cf., Dorst, 2003; Harfield, 2007).

Figure 1

An Example of How the Same Initial Problem may be Framed and Solved Differently

                                                                                        81
To see how this might play out, try finding multiple problems in the scenarios below (Table 1). Can you frame the
problem as an instructional design problem? Can you also frame the problem not as an instructional problem?

Once you have framed possible problems in the scenarios, consider the ill-structuredness, complexity, and domain
specificity of each problem (Jonassen, 2000). Each problem you framed may differ in these dimensions (Figure 2):

      Structure refers to the degree to which a problem has a single solution and most-efficient solution path (well-
      structured) or many possible solutions and solution paths (ill-structured). Problems are sometimes presented to
      instructional designers as well-structured, but design problems are ill-structured by definition.
      Complexity refers to the number of variables involved and how interrelated they are. Most--but not all--design
      problems are complex. This characteristic can be tricky to assess simply because we use the term informally as a
      synonym for "difficult."
      Domain specificity refers to whether domain general strategies would suffice; Jonassen (2000) described domain
      specific problems in terms of both "abstract" and "situated," knowledge, generally placing such knowledge in formal
      domains. Almost all ID problems are domain specific.

Table 1

Framing Both Instructional Design and Other Problems

Scenario  Management at a chemical plant identifies that the most expensive chemical is not typically used
1         efficiently, unless it is used under specific conditions. They contract an instructional designer to create a
          job aid to ensure the chemical reactor is operated optimally. The reactor includes 15 stages, six
          chemicals, and gauges for setting pressure, temperature, and rate at each stage. Data suggest workers
          tend to apply settings from a similar reactor, resulting in waste.

Scenario  Beth is hired by a dietician to create instructional materials--printed handouts--for parents/guardians of
2         children with special dietary needs based on a specific disability. The dietician provides published,
          effective dietary standards based on the specific disability and shares that some of the terms in the
          standards are hard for families to understand. The production budget is small and timeline tight. The
          organization provides a set of images they previously created and want used in the handouts.

Scenario  A university's instructional technologies committee selects and implements a learning management
3         system (LMS), heavily guided by their own expertise, along with issues related to copyright law, tight
          institutional budget concerns, and interfacing with systems for registration and grading. As a
          consequence, instructional designers are hired primarily based on their capacity to provide technical
          support for the cumbersome, difficult to use LMS. To ensure they can support the faculty, they create a
          highly structured course shell.

Scenario A district purchases science kits and curricula for teachers in Phoenix, AZ. While the resources seem

4         useful, the teachers realize there are issues. For instance, the curriculum teaches "Fall is when the leaves

          82
          change colors," but the teachers know their students have never seen leaves change color. They meet
          during a cross-school professional development session to address these issues, guided by curriculum
          leads who graduated from an instructional design program.

Scenario  An instructional designer is tasked with migrating courses from a decommissioned LMS to a newly
5         adopted one. None of the content or sequencing is to be changed. The two LMSs differ greatly in many
          ways (e.g., how objects are connected to courses, the order in which settings must be selected, and the
          number of features available).

Figure 2
Problem Structure, Complexity, and Domain Specificity Differentiate Between Problem Types

Note. Design problems are always ill-structured, and usually complex and domain specific. The letters refer to the
problems above related to scenario 1 in Table 1.

How Do Designers Frame Problems?

Problem framing can occur through both overt and covert activities. Some activities and deliverables make the problem
visible, but other problem framing work happens through talk or individual thinking. Covert framing activities involve
abductive reasoning--filling in gaps in knowledge (e.g., Kolko, 2010). This kind of thinking is heavily influenced by past
precedent, and designers contend with the salience and limitations of their own experience.
From their first contact with the problem, designers consider whether the problem seems like one encountered
previously and how the current problem seems to differ from past precedent.

                                                                                        83
     How might you convey to a client why framing the problem is important?

Their framing of the problem is visible in the project objectives and learning goals they set. As they seek to understand
and explore, researching the task, context, learners, and possibly other precedents, they reframe the problem and
consider whether the client will accept their reframing, which is made visible in learning objectives and problem
statements. Prototypes may likewise reveal their problem framing. Evaluation of a prototype's feasibility, desirability, and
ability to meet identified learning needs may lead to further reframing.
Clients often undervalue and underestimate the time and effort needed to frame the problem. Clients may request a
specific deliverable or solution, yet may not have a deep understanding of the actual needs. Making a value proposition
can sometimes help. This means communicating clearly about what problem framing is and why it can benefit the
organization by preventing ineffective training.

What Does It Mean to Have Agency to Frame a Design Problem?

Experienced designers--regardless of discipline--know to direct their framing of the problem. They make consequential
decisions that lead them to new understandings and reframings of the problem. This "framing agency" is a hallmark of
design in which designers rely on information they gather and on their past precedent--as described in other chapters
of this volume. What does framing agency look like in practice?
In the case below (Figure 3) a team faces some challenges in part because not all members understand that they need
to frame the problem; this is visible in their expectations about their roles and in their talk. In the vignettes, words are
highlighted to draw attention to ways the team members are talking that help us notice whether they are framing the
problem or not. Designers often share framing agency with other designers, with envisioned stakeholders, and
sometimes even with the materials in their designs. In ID, this happens when they reference the learning and transfer
contexts, and the modes of learning (e.g., face-to-face, online, etc.) to justify decisions. Another indicator of framing
agency is staying tentative, staying with the problem. Using verbs that show possible actions (e.g., could, might, etc.)
and hedge words (e.g., maybe, kind of, etc.) invites both the designer and others to revise their thinking about the
problem. In contrast, using verbs that show a lack of control (have to, need to, etc.) over the situation tends to shut
down problem framing, unless the verb refers to a design requirement (like Yen's use on "need to" in vignette 2).
Read through the vignettes in figure 3 and answer the following questions:

      Who treats the problem as not needing to be framed?
      How does the instructional designer encourage them to frame the problem?
      Who else shows framing agency?
Figure 3
Vignettes From a Design Team: Who Shows Framing Agency? Who Does Not?

                                                                                        84
     If you are on a team that is resisting framing the problem, how will you communicate with your team? Using the
     key in Figure 3, how can you use tentative language to invite them to frame the problem with you? How will you
     help them understand the importance of framing the problem?

Learning to notice how you talk with your team may help you diagnose whether or not you are framing the problem.
When someone sounds tentative, consider it an invitation to engage in framing with them. Try to avoid no-control talk
that shuts down problem framing.

What Tools Help Designers Frame Problems?

Mapping unknowns, assumptions, and conjectures can help clarify the work needed to frame problems. In addition to
the tools that other chapters in this text have offered, I have found the following tools help make problem frames
explicit yet open to revision:

      Problem statements
      Storyboarding
      KWL charting
      Design conjecture maps
      Root cause + sphere of influence analysis

It is important to remain tentative in using these tools. Just as we saw with design talk, staying open to revision is key.
For this reason, I typically use pencil and paper or whiteboards for these kinds of activities. Rather than polishing and

                                                                                        85
perfecting them, staying in draft mode can help you stay open.

Problem Statements

Problem statements are concise and provide clarity about the problem frame. Your problem statement should begin
with one or two sentences describing a vision of what is possible if the problem is solved. Next, describe--in one to two
sentences--what the specific issues are. This should include who, what, when, where and why. Finally, in one to two
sentences, describe the primary symptoms of and evidence for the problem. You should not include a solution! Expect
to write your problem statement multiple times to capture changes in your understanding of the problem.

Problem Statement Worksheet

Storyboarding

Vividly depict the problem--not the solution--as a sequence of events from a particular point of view. You may hand
draw this, use photos, use a graphics program, or try out one of the many free storyboard/comic strip creation websites
(see Additional readings and resources). When depicting the problem, consider other points of view, and represent
these in another storyboard, with thought bubbles, or as a branching storyline. Avoid depicting the solution!

KWL Charting

KWL charting is adapted from tools commonly used in project-based learning classrooms and supports learners to
identify what they do and do not know, as well as what they still need to know (Ogle, 1989). This tool is useful for
designers as they manage the ambiguity of the design problem. Using it frequently as a means to track progress can
help teams direct their own progress in bringing information into the problem.

Table 2

Example of KWL Charting

Date  What do we know about the design   What precedent do we want to     What do we want to learn
      problem, learner needs, and other  (or not want to) bring into the  about the problem and how
      requirements or constraints?       problem?                         will we learn it?

Design Conjecture Maps

Design conjecture maps are based in tools like logic models and design-based research conjecture maps (Sandoval,
2014). They help designers coherently link the task to learning objectives and to their design ideas. First, place the
learning objectives on the same page as the task analysis, then make links between them. Second, after generating
tentative ideas, try connecting these to the task analysis and learning objectives using yarn or string. Third, as you begin
to develop more solid designs, try connecting these back to the task analysis and learning objectives.

Design Conjecture Mapping

Root Cause Analysis

Root cause analysis techniques, like the five whys (Ohno, 1978; Serrat, 2017), can help designers identify underlying
causes rather than treating symptoms. While some use this approach to craft a linear set of causes and effects,
creating a network of whys is more effective for framing problems from multiple points of view. In this way, for each
problem, you should ask "Why does this happen?" and "Why else does this happen?" This results in a network of
possible root causes. Pairing this analysis with sphere of influence analysis--meaning, deliberately analyzing whether
each cause is within your capacity to influence the problem through instructional design--provides an opportunity to

                                                                                        86
consider the feasibility and impact for any particular cause. To do this, for each cause you should consider whether it is
a problem you can influence and whether it is an instructional design problem (Figure 4). Which of these causes
suggest an instructional design problem?
Figure 4
Example of the Five Whys as a Network

Do-It-Yourself

Now that we have learned about several tools, here are some specific ways you can apply these. First, you can try out
the tools in the previous section using the scenarios above in Table 1. Second, if you are in a class that includes
developing an instructional design, you can use these tools for your class. When teaching instructional design, I always
require students to work for clients on real design projects because I have observed issues that come up without
clients. Students spend as much or more time inventing fake clients as they would learning how to assess needs.
Without a real client and context, it can be hard to learn to frame problems authentically, to really understand that even
though you are designing something for others, by framing the problem, you are taking ownership of it. That is
challenging to do if it is a problem of your own invention. Likewise, without a client, the reasons for reframing are likelier
to stem from challenges you encounter than new understanding of the problem space. Of course, working with real
clients can be challenging in other ways. Make sure your client understands that you have course deadlines and are just
learning to design. Agree on the scope of work beforehand using a formal design brief.

                                                                                        87
Conclusion

While problem framing is typically treated as something that happens at the beginning of a design project, it is
important to remember that it is a process that continues until the design is finalized. You may revisit and revise along
the way, especially for short deliverables like problem statements and KWL charting. Prototypes, especially low fidelity
prototypes, and evaluation often reveal the need for reframing. And, as contexts and needs change by location or over
time, a solution may no longer function, and the problem can pop back open. In considering the iterative nature of
problem framing, how will you use these tools to guide and document reframings of the problem?

Finally, my advice to you as new designers is this: Dwell with the problem. Wallow in some uncertainty. Stay tentative!

   Additional Readings and Resources

   Problem Framing Resources

           Gause, D. C., & Weinberg, G. M. (1990). Are your lights on? Dorset House.
           ISIXSIGMA
           Atlassian

   Root Cause Tools

           Google Drawings
           Draw.io
           MindMeister
           Mindomo
           Coggle.it
           Edrawsoft
           MindMup
           LucidChart

   Storyboarding Tools

           Storyboard That
           Pixton
           Witty Comics
           Strip Generator
           Make Beliefs Comix

Acknowledgments

This material is based upon work supported by the National Science Foundation under Grant No. EEC 1751369. Any
opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not
necessarily reflect the views of the National Science Foundation.

References

Dorst, K. (2003). The problem of design problems. Design Thinking Research Symposium, Sydney, 17(19.11).

                                                                                        88
Dorst, K., & Cross, N. (2001). Creativity in the design process: co-evolution of problem-solution. Design Studies, 22(5),
         425-437. doi:10.1016/S0142-694X(01)00009-6

Harfield, S. (2007). On design `problematization': Theorising differences in designed outcomes. Design Studies, 28(2),
         159-173. doi:10.1016/j.destud.2006.11.005

Jonassen, D. H. (2000). Toward a design theory of problem solving. Educational Technology Research and
         Development, 48(4), 63-85. doi:10.1007/BF02300500

Kolko, J. (2010). Abductive thinking and sensemaking: The drivers of design synthesis. Design Issues, 26(1), 15-28.
         doi:10.1162/desi.2010.26.1.15

Ogle, D. M. (1989). The know, want to know, learn strategy. Children's comprehension of text: Research into practice,
         205-223.

Ohno, T. (1978). Toyota production system: Beyond large-scale production. Vol. 1: Cambridge, MA: Productivity Press.
Sandoval, W. (2014). Conjecture mapping: An approach to systematic educational design research. Journal of the

         Learning Sciences, 23(1), 18-36. doi:10.1080/10508406.2013.778204
Schön, D. A. (1983). The reflective practitioner: How professionals think in action. New York: Basic Books.
Serrat, O. (2017). The five whys technique. Knowledge solutions (pp. 307-310): Springer.

                                  Vanessa Svihla

                                           University of New Mexico
                                           Dr. Vanessa Svihla is an assistant professor at the University of New Mexico with
                                           appointments in the learning sciences and engineering, and she directs the
                                           Interaction and Disciplinary Design in Educational Activity (IDDEA) Lab. Her
                                           research has been supported by the NSF and USDA, and she was selected as a
                                           2014 National Academy of Education / Spencer Postdoctoral Scholar. Dr. Svihla
                                           received her MS (Geology) and PhD (Science Education) from The University of
                                           Texas at Austin. She served in the Peace Corps and was a post-doctoral scholar at
                                           UC Berkeley. She draws inspiration from her own practice in fashion design and
                                           instructional design, as her research focuses on how people learn when they
                                           design. She is particularly interested in how people find and frame problems, and
                                           how these activities relate to identity, agency and creativity.

                                                                                        89
This content is provided to you freely by EdTech Books.
Access it online or download it at https://edtechbooks.org/id/problem_framing.

                                                         90
  7

Using Task Analysis to Inform Instructional Design

Jill E. Stefaniak

The emphasis of this chapter is to explore the value that task analysis brings to the design process. Task analysis is an
important skill; if instructional design is a process where instructional designers create "detailed specifications" (e.g.,
Richey et al., 2011) for learning systems, it is important that the information we provide our learners is accurate,
relevant, and delivered in a timely manner. Task analysis allows us the opportunity to do just that.

The instructional design process encompasses five phases that involves the analysis, design, development,
implementation, and evaluation (otherwise known as ADDIE) of instructional interventions. The role of analysis
encompasses the phase whereby the instructional designer gathers information to gain a clearer understanding of the
situation warranting instruction, the needs of the learning audience, and other contextual factors that will influence the
design and implementation of instruction as well as transfer of knowledge (Stefaniak, 2021). This information is
typically gleaned from conducting needs assessments, learner analyses, contextual analyses, and task analyses.

What is Task Analysis?

Task analysis is the process of breaking down tasks and activities into a series of steps to understand how they are
performed. Conducting a task analysis places emphasis on understanding the tasks and documenting them in a step-
by-step fashion (Annett, 2003). The goal of a task analysis is to understand how a task should be performed. While the
techniques and guidelines for conducting task analyses can be applicable to a variety of industries, I will discuss task
analysis as it relates to instructional design.

Task analysis is not supposed to be about how the instructional designer will design instruction; rather it's meant to
focus on understanding what that task will look like when engaged in by an expert. Taking an in-depth look at the steps
required to complete the task can help instructional designers identify the complexities of the task, brainstorm
strategies to compartmentalize tasks, and begin to establish a plan for how these tasks may be further broken down
and explained to a learning audience. The results of a task analysis are often used to inform instructional design
decisions such as what to cover in training materials, develop performance checklists to guide on-the-job performance
and assessments, construct job analyses and job descriptions that accurately reflect work that is assigned to specific
roles within an organization, and helping to aid in the development of automated decision trees (Hackos & Redish, 1998;
Jonassen et al., 1999).

Task Analysis in Instructional Design

Task analyses are not something that can be rushed. Task analyses are similar to any other form of analysis performed
in instructional design; it is important that sufficient data is collected to provide an accurate portrayal of the task and
the situation where it is desired to be performed. When we conduct a rigorous task analysis, it becomes quite apparent
how much work is needed to really dissect a task in its entirety. A good task analysis provides a detailed breakdown of a

                                                                                        91
task or topic that will aid the instructional designer with providing sufficient information to their learning audience as
well as contribute to overall performance within the organization (Brown & Green, 2016; Morrison et al., 2013).

Jonassen et al. (1999, p. 3) noted several common assumptions about task analysis as it relates to instructional design:

  1. "Task analysis is essential to good instructional design.
  2. "Task analysis is the least understood component of the instructional design process.
  3. "Task analyses are uncertain.
  4. "Different contexts demand different task analysis methods; one size does not fit all."

Jonassen et al.'s (1999) assumptions about task analysis still ring true. Consider this saying in regards to the
instructional design process: "Measure twice. Cut once." The results of a task analysis can be used to inform many
initiatives within an organization and can support the development of both instructional and non-instructional
interventions to support learning and performance. The more detail we can gather during task analysis, the better our
instruction tends to be.

Task analysis also faces similar challenges when compared to needs assessment. Jonassen et al. (1999) note that it is
the least understood component of the instructional design process. In fact, while Dousay & Branch (2022) note that
task analysis is an integral part of the design process, some evidence suggests that many instructional designers do
not consider it an important part of the instructional design process at all. In their survey of instructional design models,
for example, only three models specifically call out task analysis (e.g., Branson, 1975; Morrison et al., 2013; Seels &
Glasgow, 1998). Further, there is a high degree of variability within and among the instructional design models in our
field when it comes to task analysis as well as many of the other relevant processes of design.

Another challenge is that there is a lot of uncertainty associated with the actual steps used during a task analysis. While
there are typical processes that can be followed when conducting a task analysis, a lot of the process is driven by the
individuals involved, the unique needs of the system, and what the instructional designer (and anyone else assisting)
has access to that can inform the breakdown and compartmentalization of tasks. When we talk about instructional
strategies, we know that different learning outcomes warrant different instructional design strategies (Richey et al.,
2011). The same can be said about task analysis. Different types of tasks warrant different forms of task analyses.
Different types of tasks may require different forms of data collection to inform analysis. This makes it challenging
when trying to convey best practices, particularly in instructional design courses.

Where to Start? What to Focus On?

Determining what tasks to include in a task analysis can sometimes be challenging. Thus, it is important to prioritize
when conducting a task analysis. Focusing on repetitive tasks enables the identification of potential areas for
improvement and provides opportunities to improve efficiency, accuracy, and overall performance. An instructional
designer is not expected to include every single task that an individual may complete. Instead, think about the tasks
broadly the same way you would if you were crafting a job description. You would not include every single task in a job
description; rather, you would prioritize the most important and recurring tasks.

Some questions you may consider when determining which tasks you should focus on during a task analysis may
include, but are not limited to, the following:

      How often is the task completed?
      How important is the task in relation to the job?
      How important is the task in relation to plans for training?
      Do individuals encounter challenges while completing the tasks?
      Are there specific protocols that need to be followed while completing a task due to safety?
      Are there safety, ethical, and/or legal implications associated with completing the task incorrectly?

                                                                                        92
When determining what tasks to focus on, Jonassen et al. (1999) suggest 5 criteria to use to help guide task selection.
These criteria and questions to consider are included in Table 1. Making an exhaustive list of tasks related to a
particular job or related to the topic you plan on designing instruction for and then weighting them with these criteria
can help to prioritize what should be explored in a task analysis. Typically, tasks that are ranked higher in priority will
most likely be the tasks you would want to focus on when designing instruction.

Table 1

Criteria for Task Selection (Adapted from Jonassen et al., 1999)

Criterion        Questions to Consider
Criticality            How important is the task to the overall job?
                       How does performing the task contribute to overall job and/or organizational goals?
                       What the implications (legal, ethical, and/or safety) if the task is not completed
                       correctly?

Universality     How often is the task completed?
                 How widely is the task completed across different contexts within the organization?

Standardization  Is the task completed the same way across different contexts?
                 Are there instances when a task may be performed differently? How often does that
                 occur?
                 What types of situations or contextual factors warrant the task being completed
                 differently?

Feasibility      Is there support for individuals responsible for completing the task?
                 Are adequate resources provided to support training individuals how to perform the
                 task correctly?

Difficulty       How difficult is it to perform the task?
                 How difficult is it to learn to perform the task?
                 Do people encounter challenges completing the task correctly?

A Typical Process for Conducting a Task Analysis

As previously mentioned, there is a lot of uncertainty that accompanies establishing standard practices for conducting
task analyses because a one-size-fits-all approach does not work. It is important to know that while most instructional
designers and performance improvement experts will agree on the broader guidelines, these steps may be repeated as
new information becomes available. Figure 1 provides an overview of the common steps for completing a task analysis
(Beven et al., 2012; Jonassen et al., 1999)

Figure 1

Task Analysis Process

                 93
Step 1: Identify the Task

The first thing to occur during a task analysis is to identify the task(s) to be examined. During this step, you would begin
to identify what the task analysis is focusing on. From an instructional design perspective, there are times where
training has already been identified as necessary, and particular topics and related tasks have already been identified as
being a priority. The goal of the task analysis would be to break down tasks to develop a more thorough understanding
for what to include in instruction. Other times, an organization may conduct a task analysis as part of a larger needs
assessment project. The results could inform the development of new jobs, the refinement of existing job descriptions,
or more accurate records (i.e., organizational reports and performance metrics) that may inform training and
assessment activities.

Step 2: Identify Data Sources

It is important to ensure that you have adequate data sources that can inform your understanding of the task that is
being analyzed. The more data sources that you can gain access to, the deeper understanding you should gain. Data
sources are often dependent on a number of variables such as access and time. Table 2 provides an overview of the
types of data sources you may want to consider using.

Table 2

Possible Data Sources for a Task Analysis

Data Source          Examples
Direct Observations        Observe subject-matter-experts perform the tasks correctly.
                           Observe individuals performing the tasks to note best practices as well as any
                           challenges they may encounter.

Interviews           Interview individuals who may be responsible for completing the task.
                     Try to interview individuals with varying levels of expertise as this will help you to
                     identify areas to focus on in training.

Document Analysis    Review manuals, handbooks, job aids, and protocols that may provide insight into
                     how the task is supposed to be performed.

Focus Groups         Conduct a focus group with 6-8 individuals at a time.
                     Create questions that provide insight into how tasks are completed?

Surveys              Surveys allow you to gather feedback from multiple individuals who complete the
                     task (or may be expected to complete the task).

                                           94
Step 3: Conduct a Task Inventory

Steps 2 and 3 are often completed in tandem. As more information becomes available about the potential tasks being
included in the analysis, the assessor would create a list of the tasks to be explored. Focusing the criteria outlined in
Table 1, the assessor would rate the tasks based on criticality, universality, standardization, feasibility, and difficulty.
Data gathered from your various sources will help you rate the tasks. It is also important that you ask individuals (e.g.,
subject matter experts, employees, supervisors) questions that can help you address their criteria.

Step 4: Select Task Analysis Method

There are a variety of task analysis methods, and, similar to many instructional design processes, different tasks
warrant different task analysis methods. Four common task analysis methods include procedural task analysis,
cognitive task analysis, hierarchical task analysis, and time-and-motion studies. Procedural and cognitive task analyses
are the most common in instructional design.
Procedural task analyses focus on the step-by-step procedures and sequences involved in executing a particular task. It
breaks down tasks into detailed procedures, specifying the order of actions, decision points, and potential variations
(Morrison et al., 2013). This type of analysis is valuable in instructional contexts, where it helps create instructional
materials and training programs that guide individuals through the correct execution of tasks. Procedural analysis
ensures that individuals understand not only what needs to be done but also the specific sequence and nuances
associated with each step of a task. The following is an example from an instructional design project that demonstrate
how a procedural task analysis may be conducted to break down a task.
Table 3
Example: The Basic Journey of Polymer Clay
Task: Conditioning polymer clay by hand

                                                                                        95
1. With an X-ACTO knife, cut off a chunk of clay that can fit into the palm of your hand.
      1. If you do not have an X-ACTO knife, you can tear off a chunk of clay with your hand.

2. Put the chunk of clay on the palm of your hand.
      1. Any hand works.
      2.

3. With both of your hands, begin rolling the chunk of clay back and forth in the palm of your hands.

      1.

4. As you are rolling the block back and forth, your clay should become smooth and snake-like.

      1.

5. Once your clay is smooth and snake-like, with both hands, smush the clay together.
      1. By smushing, you are pressing your hands together to flatten the clay.

            1.

6. After the clay is smushed, roll the clay back and forth in a circular motion in the palms of your hands until it
   becomes a ball.

                                                                                    96
     1.

    7. Press a finger into the ball of clay.

     1.

  8. Ask yourself, "Does the clay feel warm and soft?"
        1. By soft, there should be little to no resistance when you press into the clay.

  9. If yes, your clay is fully conditioned. If not, repeat steps 3-8 again until your clay is warm and soft.
 10. If you have multiple pieces of clay that need to be conditioned, repeat steps 3-8 until all of your clays are

      conditioned.

Source: Nguyen, A. (2023). Task analysis. From clay to creation: The basic journey of polymer clay [Unpublished paper].
Workforce Education and Instructional Technology. University of Georgia.

Cognitive task analysis delves into the mental processes, knowledge, and problem-solving strategies that individuals
employ while performing a task (Militello & Hutton, 1998). It goes beyond the observable actions and aims to uncover
the underlying cognitive skills involved. Commonly applied in psychology, education, and human-computer interaction,
CTA helps identify expertise, design effective training programs, and improve the usability of systems by understanding
how users think and approach tasks (Clark et al., 2008; Wei & Salvendy, 2004). The following are examples from
instructional design projects that demonstrate how a cognitive task analysis may be conducted to break down a task.

Table 4

Helping the Caregiver. How to Meal Plan for a Senior

Task: Identify common meal planning challenges

1.       Identify common meal planning challenges

1.1      Financial resources

                                                      97
1.1.1  Determine how much you will contribute financially, if applicable.

1.1.3  Discuss whether the senior will make a financial contribution. NOTE: Prepare to be flexible as the amount

       from either or both parties can change.

1.1.3.1 Verify type and amount of resource (e.g. cash, debit card, EBT card)

1.1.3.2 Verify when resource (s) is available.

1.1.3.3 Standardize a time to access resource(s), if reoccurring, that coincides with shopping.

1.1.3.3 Standardize a time to access resource(s), if reoccurring, that coincides with shopping. NOTE: If senior

       provides a debit of EBT card, ensure that you ahve the card's PIN.

1.2    Time

1.2.1  Use calendar(s) to confirm no conflicts with appointments, work deadlines, and/or extra-curricular events

       to create a shopping list for meals.

1.2.2  Use calendar(s) to confirm no conflicts with appointments, work deadlines and/or extra-curricular events to

       establish when you will shop for ingredients.

1.2.3  Use calendar(s) confirm no conflict with appointments, work deadlines and/or extra-curricular events to

       determine when you will prepare meals.

1.3    Food access

1.3.1  Identify nearby traditional food retailers

1.3.1.1 Verify hours of operation.

1.3.2  Determine availability of specialty food stores (e.g. ethnic food stores).

1.3.3  Determine if you will need to seek supplemental food sources depending on the financial resources

       available.

1.3.3.1 Acquire a list of food banks or pantries, if needed. NOTE: Consult your local chamber of commerce or

       United Way agency for such a list.

1.3.4  Identify a nearby farmer's market for seasonal produce.

1.3.4.1 Verify hours and days of operation.

Source: Green, S. (2023). Task analysis: How to meal plan for a senior [Unpublished paper]. Workforce Education and

Instructional Technology. University of Georgia.

Table 5
Leading Change in an Organization
Task: Ensure that individuals make the change initiative their own and create innovative ways to use and improve it.

                                                      98
1. Foster a culture of ownership within the change initiative.

                                                                                    99
1. Create a culture within the organization that encourages and values individual ownership of the change
   initiative.
      1. This involves leadership support and recognition of innovative contributions.
            1. Spot bonuses
            2. Additional time off
            3. Verbal recognition
      2. Highlight previous innovations
      3. Demonstrate how innovative suggestions and processes have been incorporated into the company

2. Clearly define the boundaries and constraints within which individuals can make changes to the initiative.
      1. Constraints and boundaries can be:
            1. Financial
            2. Time bound
            3. Technical
            4. Scope related
      2. This ensures alignment with organizational goals and objectives.

3. Empower individuals by granting them the autonomy to make decisions related to the change initiative.
      1. Allow them to experiment with innovative ideas.
            1. Give them a test environment to try their ideas
            2. Develop an innovation grant program
            3. Allow X-number of hours for innovative play

4. Continuously communicate the importance of individual contributions to the success of the initiative.
      1. Engage in open discussions about the value of innovation.
      2. Celebrate previous innovation successes to change initiatives.
      3. Have previous innovators discuss their innovations at town halls.

5. Collaborate with Learning and Development to provide training and resources on innovation tools,
   methodologies, and best practices to equip individuals with the skills needed to generate and implement
   innovative ideas.
      1. This could include workshops, guest speakers, infographics, podcasts, and other training development.
      2. Develop a budget for employees to purchase books or courses on their own to develop their innovation

6. Create channels for individuals to brainstorm and share innovative approaches to the change initiative.
      1. Channels may include:
            1. Submission portal on the intranet
            2. Workshops
            3. Community communications boards
            4. Interdepartmental competitions

7. Establish a system to recognize and reward innovation.
      1. This may include a list of:
            1. Incentives
            2. Awards
            3. Different levels of recognition.

8. Create feedback channels so individuals can get feedback on their innovations.
      1. Feedback channels can include:
            1. Scheduled surveys
            2. Dedicated instant messaging channel
            3. Anonymous question boxes
            4. Presentation panels with Q&As
            5. Demo spotlight
      2. Encourage them to iterate and refine their ideas.

9. Document and share best practices on a common platform.

                                                                             100
               1. Documentation is dependent on above discussed communication styles
               2. Common platform could be:

                     1. Intranet
                     2. Cloud file share system
                     3. Instant messaging channel
       10. Ensure ongoing innovation by cultivating a learning culture.
               1. Foster a culture of continuous learning and improvement, where individuals are encouraged to learn from
                  failures and successes and apply these lessons to future innovation efforts.
                     1. Invest in employee learning through learning technologies, certifications, courses
                     2. Make learning a stated company value
                     3. Recognize employees for learning achievements

                           1. Spot bonuses
                           2. Promotions
                           3. Learning spotlights
                           4. Gamification within the Learning Management System
                     4. Make training easily accessible
                     5. Set aside time for employees to learn
Source: Curry, K. (2023). Task analysis: Becoming a champion of change. [Unpublished paper]. Workforce Education
and Instructional Technology. University of Georgia.

Table 6
Developing a Workshop to Motivate Learners
Task: Differentiate between motivational constructs.

                                                                                       101
1. List mainstream motivation construct used in education.
      1. Self-determination Theory
      2. Social Cognitive Theory (Self-efficacy)
      3. Expectancy-Value Theory
      4. Interest Theory

2. Define those motivation constructs and identify their characteristics.
      1. Self-determination Theory
            1. Introduced by Richard Ryan and Edward Deci.
            2. SDT is a formal theory that defines intrinsic and varied extrinsic sources of motivation and describes the
               respective roles of intrinsic and extrinsic motivation in cognitive and social development and individual
               differences.
            3. Two types of motivation:
                  1. Intrinsic motivation
                  2. Extrinsic motivation
            4. Three needs of learners:
                  1. Competence
                  2. Relatedness
                  3. Autonomy
      2. Social Cognitive Theory (Self-efficacy)
            1. Introduced by Albert Bandura
            2. Social means "people are influenced by others," and cognitive means "behavior is not enough to explain
               motivation."
            3. Sources of self-efficacy:
                  1. Mastery experiences (performance outcomes)
                  2. Vicarious experiences
                  3. Verbal persuasion
                  4. Physiological arousal
      3. Expectancy-value Theory
            1. Introduced by Jacquelynne S. Eccles.
            2. Expectancy is an individual's prediction for success, and value is the extent to which the individual likes or
               wants something.
            3. Types of values:
                  1. Interest-enjoyment Value
                  2. Attainment Value
                  3. Utility Value
      4. Relative Cost (Negative Value)
         Interest Theory
            1. Introduced by Paul R. Pintrich.
            2. Interest is an emotional state of liking and willful engagement tied to an activity.
            3. The phases for interest development
                  1. Triggered Situational Interest
                  2. Maintained Situational Interest
                  3. Emerging Individual Interest

3. Well-Developed Individual Interest
4. Compare main motivational constructs from their characteristics and focus to find their differences.
5. Recognize how the differences in constructs could facilitate online learning in different ways.
6. Analyze how each construct can be applied in online settings.
7. List potential motivational strategies with each construct.
8. Consider integrating strategies in one of the online classes to motivate students.

                                                                                    102
Source: Yang, L. (2023). Task analysis: Motivating learners in online learning environments [Unpublished paper].
Workforce Education and Instructional Technology. University of Georgia.

Hierarchical Task Analysis (HTA) is a systematic method for breaking down complex tasks into a structured hierarchy
of sub-tasks and steps. This analysis provides a visual representation of the relationships and dependencies between
different components of a task, allowing for a comprehensive understanding of the overall workflow (Annett, 2003;
Salmon et al., 2010). HTA is particularly useful in fields such as systems engineering and human factors, where the goal
is to gain insights into task structure and identify critical decision points. By organizing tasks hierarchically, HTA aids in
designing efficient processes and user interfaces, ultimately enhancing system usability. Figure 2 provides an example
of what a HTA may look as it relates to the example provided earlier for taking care of a senior and assisting with meal
planning.
Figure 2
Hierarchical Task Analysis Outline for Meal Planning for a Senior

Time-and-Motion Studies involve the systematic observation and recording of the time required to complete each step
of a task and the associated physical motions. Widely employed in industrial and manufacturing settings, this analysis
aims to optimize efficiency, identify bottlenecks, and streamline workflow processes. By quantifying the time spent on
each task element and analyzing motion patterns, organizations can make informed decisions to enhance productivity,
reduce errors, and improve overall operational effectiveness (Arndt et al., 2017; Reed et al., 2018).

Step 5: Decompose Task

Once you have selected a task to focus on, you can initiate task decomposition by identifying the overall goal of the
task. Once the goal is established, break it down into major subtasks using one of the task analysis approaches listed in
Step 4, focusing on the sequential order of actions (Annett, 2003). For example, if the task is "baking a cake," major
subtasks might include gathering ingredients, preparing the batter, and baking. Subsequently, delve deeper into each
subtask, identifying the specific actions required. In the "preparing the batter" subtask, for instance, actions could
include measuring ingredients, mixing, and ensuring a smooth consistency. Continue this process until the task is
broken down into its most granular elements, often referred to as individual actions or steps. This comprehensive
breakdown allows for a thorough understanding of the task's intricacies, facilitating effective training, troubleshooting,
and optimization (Morrison et al., 2013).

                                                                                       103
It's important to consider the cognitive and physical demands associated with each step. Identify decision points, where
choices must be made, and critical points where errors could lead to task failure. For instance, in the How to Meal Plan
for a Senior example mentioned earlier, there is a note provided in step 1.1 to alert the individual performing the task
that they may have to deviate depending on whether a senior provides a debit or EBT card. It is also important to note
any contextual factors or external factors that may impact task performance. The goal is to create a detailed map of the
task, providing an overview of not only the sequential flow of actions but also the cognitive processes and
environmental considerations.

Step 6: Sequence Tasks

This last phase of the task analysis involves the sequencing of tasks. At this time, the task assessor would review the
information and determine a logical order for the task(s) and subtasks. This systematic sequencing not only aids in
understanding the task's workflow but also serves as a foundation for designing efficient training programs, allowing
individuals to grasp the step-by-step progression and develop mastery in a structured manner.

   Points of Discussion

        1. How can task analysis serve as a foundation for designing effective instructional materials and training
           programs?

        2. How does task analysis contribute to creating learner-centered instructional experiences? What role does
           learner variability play in the task analysis process, and how can it be addressed in instructional design?

        3. Task analysis is often an iterative process. How can instructional designers adapt their analysis based on
           feedback from learners or changes in the learning environment? Discuss the importance of flexibility and
           continuous refinement in the task analysis phase of instructional design.

   Activity 1: Task Analysis Observation Exercise

     There are several methods instructional designers may employ to gather data for a task analysis. Direct
     observations provide instructional designers with an opportunity to see tasks carried out in completion in
     situated contexts. With a partner, choose a public setting (e.g., a coffee shop, a grocery store, local library,
     computer lab) where you can observe an individual performing a specific task.
     You and your partner should take detailed notes on the task being observed, breaking it down into individual
     steps. Make note of any instances where the individual may have been interrupted or had to make adjustments
     to do the environment. Compare your notes with your partner's. What were the key steps you identified in the
     task? Where there specific steps that should be emphasized? Did the individual encounter any challenges
     completing the task?

                                                                                       104
   Activity 2: Task Instructions Challenge

     When individuals have a lot of familiarity with a particular subject matter, completing certain tasks becomes
     intuitive. Tasks that we do every day or on a regular basis become second nature. This can sometimes pose
     challenges when the time comes and we're asked to breakdown a task and explain it to someone else.

     In the spirit of design, you will work in a small group to practice your shoe tying skills. Working in a group of 3
     people, you will assign the following roles: instructional designer, learner, and observer.

     The instructional designer will take a few minutes and write down the steps needed to tie a shoe with
     shoestrings. Once they have completed the steps, the instructional designer will read out the tasks. The learner
     will follow their instructions in an attempt to tie a shoe. It is important that the learner only performs the tasks
     as given in the instructional designer's instructions. While the learner is working to complete the task, the
     observer will take notes to see if any confusion arises while instructions are being delivered. The observer will
     be responsible for noting if steps appear to be missing or if additional cues may be needed to support the
     learner's understanding and ability to complete the task. Afterwards, the team will debrief on what they
     experienced and observed.

References

Annett, J. (2003). Hierarchical task analysis. In E. Hollnagel (Ed.), Handbook of cognitive task design (pp. 67-82).
         Lawerence Erlbaum.

Arndt, B. G., Beasley, J. W., Watkinson, M. D., Temte, J. L., Tuan, W. J., Sinsky, C. A., & Gilchrist, V. J. (2017). Tethered to
         the EHR: primary care physician workload assessment using EHR event log data and time-motion observations.
        The Annals of Family Medicine, 15(5), 419-426. https://doi.org/10.1370/afm.2121

Branson, R. K. (1975). Interservices procedures for instructional systems development: Executive summary and model.
         Tallahassee, Fla.: Center for Educational Technology, Florida State University. (National Technical Information
         Service, 5285 Port Royal Rd., Springfield, VA 22161. Document Nos. AD-A019 486 to AD-A019 490) Public
         domain document.

Brown, A.H., & Green, T.D. (2016). The essentials of instructional design: Connecting fundamental principles with
        process and practice (3rd ed.). Routledge.

Clark, R. E., Feldon, D. F., Van Merrienboer, J. J., Yates, K. A., & Early, S. (2008). Cognitive task analysis. In J. M. Spector,
        M. D. Merrill, J. J. G. van Merriënboer, & M. P. Driscoll (Eds.), Handbook of research on educational
        communications and technology (3rd ed., pp. 577-593). Lawrence Erlbaum.

Curry, K. (2023). Task analysis: Becoming a champion of change. [Unpublished paper]. Workforce Education and
         Instructional Technology. University of Georgia.

Dousay, T.A., & Branch, R.M. (2022). Survey of instructional design models (6th ed.). Brill.

Green, S. (2023). Task analysis: How to meal plan for a senior [Unpublished paper]. Workforce Education and
         Instructional Technology. University of Georgia.

Hackos, J.A.T., & Redish, J. (1998). User and task analysis for interface design. Wiley.

Jonassen, D.H., Tessmer, M., & Hannum, W.H. (1999). Task analysis methods for instructional design. Routledge.

                                                                                       105
Militello, L. G., & Hutton, R. J. (1998). Applied cognitive task analysis (ACTA): a practitioner's toolkit for understanding
        cognitive task demands. Ergonomics, 41(11), 1618-1641. https://doi.org/10.1080/001401398186108

Morrison, G.R., Ross, S.M., Kalman, H.K., Kemp, J.E. (2013). Designing effective instruction (7th ed.). Wiley.
Nguyen, A. (2023). Task analysis. From clay to creation: The basic journey of polymer clay [Unpublished paper].

         Workforce Education and Instructional Technology. University of Georgia.
Reed, C. C., Minnick, A. F., & Dietrich, M. S. (2018). Nurses' responses to interruptions during medication tasks: a time

        and motion study. International Journal of Nursing Studies, 82, 113-120.
         https://doi.org/10.1016/j.ijnurstu.2018.03.017
Richey, R.C., Klein, J.D., & Tracey, M.W. (2011). The instructional design knowledge base: Theory, research, and practice.
         Routledge.
Seels, B., & Glasgow, Z. (1998). Making instructional design decisions (2nd ed.). Merrill Prentice-Hall.
Salmon, P., Jenkins, D., Stanton, N., & Walker, G. (2010). Hierarchical task analysis vs. cognitive work analysis:
        Comparison of theory, methodology and contribution to system design. Theoretical Issues in Ergonomics
        Science, 11(6), 504-531. https://doi.org/10.1080/14639220903165169
Stefaniak, J.E. (2021). Needs assessment for learning and performance: Theory, process, and practice. Routledge.
Tessmer, M., & Richey, R. C. (1997). The role of context in learning and instructional design. Educational Technology
        Research and Development, 45(2), 85-115. https://doi.org/10.1007/bf02299526
Wei, J., & Salvendy, G. (2004). The cognitive task analysis methods for job and task design: review and reappraisal.
        Behaviour & Information Technology, 23(4), 273-299. https://doi.org/10.1080/01449290410001673036
Yang, L. (2023). Task analysis: Motivating learners in online learning environments [Unpublished paper]. Workforce
         Education and Instructional Technology. University of Georgia.

                                                                                       106
            Jill E. Stefaniak

                University of Georgia
                Jill Stefaniak is an Associate Professor in the Learning, Design, and Technology
                program in the Department of Workforce Education and Instructional Technology at
                the University of Georgia. Her research interests focus on the professional
                development of instructional designers and design conjecture, designer decision-
                making processes, and contextual factors influencing design in situated
                environments.
This content is provided to you freely by EdTech Books.
Access it online or download it at https://edtechbooks.org/id/task_and_content_analysis.

                                                        107
108
  8

Documenting Instructional Design Decisions

Jill E. Stefaniak

Instructional designers are tasked with making countless decisions in every project they complete. Questions ranging
from "Who is my learning audience?" to "How will this project be evaluated for effectiveness upon implementation?" all
require the instructional designer to make a variety of decisions to ensure that their instructional design efforts are
contributing to efficiency, effectiveness, and ease of learning (Morrison, Ross, Kalman, & Kemp, 2013). As the utility of
instructional design continues to be recognized across industries, the complexities of design will continue to grow. With
more options available in terms of how instructional solutions are to be designed and disseminated to a range of
different learning audiences, the complexities of design decisions facing instructional designers are insurmountable.

There is a large body of literature in other design disciplines that outline strategies for engaging in decision-making and
documenting design decisions. Many of these strategies lend themselves to the ID field, particularly with working on
complex, ill-structured design problems. Marston and Mistree (1997) argue the importance of decision-making in
design practices stating that decisions serve as markers to identify the progress that is made on designing a solution.

The purpose of this chapter is to help instructional designers differentiate between the different types of decisions they
may be responsible for during a project. Various approaches for engaging in decision-making will be discussed and
tools will be provided to assist the instructional designer with documenting their design decisions.

Types of Decisions

Instructional design problems can be classified as well-structured (Jonassen, 2000). Well-structured problems typically
have one possible solution, whereas ill-structured problems may have multiple solutions. Instructional designers will
often find themselves tasked with designing instructional solutions for problems of an ill-structured nature. While some
problems may require a quick decision by the designer, other problems may be more complex; thus, requiring several
interrelated decisions (Jonassen, 2011).

Decisions can be categorized according to types such as choices, acceptances/rejections, evaluation, and
constructions (Yates & Tschirhart, 2006). Choices consist of selecting an option from a large set of options.
Acceptance/rejection decisions consist of a binary decision where the option (or solution) is accepted or not. Evaluative
decisions involve an individual assigning worth to a possible option and determining their level of commitment if they
were to proceed with that option (Fitzpatrick, Sanders, & Worthen, 2011; Guerra-Lopez, 2008). Decisions of a more
constructive nature involve trying to "identify ideal solutions given available resources (Jonassen, 2012, p. 343).

Table 1 provides an overview of their typology along with the types of decisions an instructional designer may
encounter during a project.

Table 1

Decision Typologies as They Relate to Instructional Design

                                                                                       109
Type                    Example of Instructional Design Decisions

Choices                 An instructional designer has been asked to help a local museum with developing learning
                        materials for their patrons. During their brainstorming meeting with the museum staff, they
                        discuss the possibility of using audio headsets, mobile learning, QR codes, online learning
                        modules, and face-to-face training programs as training options.

Acceptances/Rejections  An instructional designer submits a proposal to present their project at a national
                        instructional design conference. Reviewers responsible for reading the proposal must
                        decide to accept or reject the conference proposal.

Evaluation              An instructional design firm in a metropolitan city meets with a not-for-profit organization
                        to discuss their training needs. During a few of the initial conversations, the firm realizes
                        that their client would not be able to pay the typical fees they charge for their instructional
                        design services. The CEO of the instructional design firm sees the impact that the not-for-
                        profit has made in the local community and decides that they can offer a few of their
                        services pro bono.

Constructions           An instructional design program discusses the options for offering two special topics
                        courses to their students in the upcoming year. Program faculty discuss possible topics
                        and discuss which ones might be of the most interest to their students. During their
                        discussions, they identify potential instructors for the courses and look to see how this
                        might impact regular course offerings and instructor assignments.

Jonassen (2012) suggests that decisions fall under two models of decision-making: normative and naturalistic.
Normative models involve an individual evaluating the situation and considering several options before deciding on a
solution that yields the optimal solution given any constraints or resources related to the situation. He further
categorizes normative models of decision-making as falling into three categories (rational choice, cost-benefit, and risk
assessment).

Rational choice models involve the instructional designer evaluating alternative options for addressing a problem and
weighing the option to determine what is the most viable of the solutions. Oftentimes, the instructional designer will
evaluate the strengths and weaknesses of each solution using decision-making tools such as SWOT or force field
analyses. A cost-benefit analysis seeks to select solutions based on the potential for their return-on-investment. There
may be instances where it is worth foregoing training if an organization cannot justify incurring the costs associated
with training. A risk assessment model is when an instructional designer will evaluate the risks associated with not
proceeding with a particular solution.

Naturalistic models are suggested to assist in the decision-making process when decisions are more contextually-
embedded. These models "stress the role of identity and unconscious emotions in decision-making" (Jonassen, 2012,
p. 348). Narrative-based models place value on the explanations that accompany the various decision options. More
emphasis is placed on the explanation rather than the cost-benefit analysis associated with a particular solution.
Identify-based decisions are centered around how any individual relates to solutions on a personal level. Table 2
provides examples of instructional design decisions that may fall under normative or naturalistic decision-making
models.

Table 2

Examples of Normative and Naturalistic Instructional Design Decisions

Type of        Model    Examples in Relation to Instructional Design
Decision-

                        110
Making        Rational      A manufacturing company is looking to conduct Kaizen events as a means to create
Normative     choice        a lean manufacturing environment. To date, there have been many issues reported
Decision-                   and logged by employees related to inefficiencies related to production. The
Making                      manufacturing supervisors and the director of continuous improvement meet to rank
                            the performance issues. They will begin by developing training and Kaizen events
Naturalistic                around the top three issues that have been prioritized by the team.
Decision-
Making        Cost-benefit  A call center is interested in investing in the development of new training modules to
              analysis      assist their call attendants on strategies to troubleshoot common calls they have
                            been receiving about new products. Investing in training has the potential to reduce
                            each customer call by five minutes.

              Risk          A local hospital has sought input from its training department to explore whether
              assessment    training is needed regarding patient safety for their volunteers. The organization is
                            looking at what the cost would be to host training sessions every month with
                            incoming volunteers versus the risks of not training them on patient safety practices.

              Narrative-    A sociology department at a research-intensive university is meeting to discuss if
              based         there is a need to modify and update their curriculum for their graduate programs. A
                            faculty member has mentioned to the group that they do not believe the existing
                            curriculum places enough emphasis on vulnerable populations. As they talk during
                            the meeting, they keep referring to some existing students and asking the program
                            faculty to consider what they would do if they were these students.

              Identity-     The curriculum committee at a medical school is discussing options for offering
              based         graduate certificates in Patient Safety and Quality or Global Health in addition to their
                            medical degree programs. Three of the members on the curriculum committee
                            participated in global health trips during their medical training and recall it being a
                            very engaging experience. They are more inclined to support the certificate in global
                            health because they identify with that program on a personal level.

Normative Decision-Making Example: An Accident Occurs on
the Plant Floor

Mike is an instructional designer who works in the Department of Employee Development for an automotive aftermarket
manufacturer. Over the weekend, an employee had a fatal accident operating a piece of machinery during the night
shift. Mike and his supervisor have been included in meetings to explore whether modifications are needed to the
company's existing health and safety modules.

It is most likely that Mike and his supervisor will employ a normative approach to decision-making by conducting a risk
assessment to determine the need for updating existing modules or developing new courses. The following are
examples of some questions that Mike may ask during his meeting with the organizational leadership:

      How many accidents have occurred on the plant floor in the past year?
      How many of these accidents were related to the particular machine?
      What training had the injured employee received before operating the machinery?
      Are safety practices related to the machine covered in the existing health and safety training modules?

                            111
   Application Exercises

     Make a list of all of the potential options you might consider if you were to assist Mike with the project.

Naturalistic Decision-Making Example: Transitioning Human
Resource Mandatory Training

Angela has recently been hired as an instructional designer and trainer in support of employee development initiatives
for a local hospital. In a recent meeting that was held with managers in human resources, there was a discussion about
whether mandatory training courses should be offered in an online format. At her previous organization, Angela
remembers that there were a lot of issues with transferring courses to an online format and she wonders if the
employee development team has the necessary manpower and resources to support these modules.

   Application Exercises

     How might Angela's previous employment experience influence her position during this discussion about
     offering online training modules?

Fostering the Development of Instructional Design Decision-
Making

Several studies have been conducted exploring the development of instructional designers' design judgment (Demiral-
Uzan, 2015; Gray et al., 2015; Honebein, 2017; Korkmaz & Boling, 2014). These studies have explored how instructional
designers engage in making decisions based on resources available in real-world settings. The results of these studies
have supported the idea that instructional design is not limited to a linear approach for designing and developing
instructional solutions; it is complex, and heavily influenced by contextual factors that are uniquely situated in relation
to the project goals.

Other studies have sought to explore the role of experience and instructional designers' abilities to make decisions.
There are several differences inherent in terms of how novice instructional designers engage in decision-making
compared to experts (Ertmer et al., 2008, 2009; Hoard, Stefaniak, Baaki, & Draper, 2019; Perez & Emery, 1995; Stefaniak,
Baaki, Hoard, & Stapleton, 2018). Novice instructional designers are more apt to rely on instructional design models to
guide their design process in a linear fashion whereas expert designers design in a more recursive manner. Several of
the abovementioned studies also reported that novices tend to revert back to instructional design solutions they have
used in previous projects; experts are more prone to customize solutions to meet the unique needs of their learning
audience.

Several researchers in the instructional design field have suggested that an apprentice model can be beneficial to
novice instructional design students as they are acquiring and developing design skills. The use of a cognitive
apprenticeship provides a framework for instructors and expert instructional designers to model behavior and design
practices in addition to providing the necessary instructional scaffolding to support instructional designers as they
engage in design decision-making (Bannan-Ritland, 2001; Ertmer & Cennamo, 1995, Moallem, 1998; Shambaugh &
Magliaro, 2001; Stefaniak, 2017)

                                                                                       112
Tools to Facilitate and Log Decision-Making During the Design
Phase of Instruction

Instructional design is an iterative and recursive process that requires the instructional designer to continuously
monitor and revisit their designs to ensure alignment between instructional components from conception to
implementation. Table 3 provides an overview of various tools that an instructional designer can utilize throughout their
design process to log and reflect upon their instructional design decisions. Also, examples of studies and resources
that discuss the use of these tools in detail are included in the table.

Table 3

Overview of Tools to Assist Instructional Designers with Logging Decisions

Tool             Description                                Examples of Studies and Uses

Design           A document that serves as a blueprint for  Boot, Nelson, van Merrienboer, and Gibbons (2007)
documents        the entire instructional project. This     Martin (2011)
                 document typically includes information    Piskurich (2015)
                 related to course goals, learning
                 objectives, instructional strategies,
                 assessments, project timelines, and
                 budgets.

External         The knowledge and structure in the         Baaki and Luo (2019)
representations  environment, as physical symbols,          Boling and Gray (2015)
                 objects, or dimensions (e.g., written      Fischer and Mandl (2005)
                 symbols, beads of abacuses, dimensions     Huybrechts, Schoffelen, Schepers, and Braspenning
                 of a graph, etc.), and as external rules,  (2012)
                 constraints, or relations embedded in      Luo and Baaki (2019)
                 physical configurations (e.g., spatial     Verschaffel, de Corte, de Jong, and Elen (2010)
                 relations of written digits, visual and    Yanchar, South, Williams, Allen, and Wilson (2010)
                 spatial layouts of diagrams, physical
                 constraints in abacuses, etc.)" (Zhang,
                 1997, p. 180).

Group            Space where an instructional design team   Gustafson (2002)
repositories     can track the progress of a project and    Spector (2002)
                 share notes. This space is typically       Stefaniak, Maddrell, Earnshaw, and Hale (2018)
                 housed by an online platform.

                                                            Van Rooij (2010)

Rapid            An instructional design approach that is   Roytek (2010)
Prototyping      used to create a sample of an              Tripp & Bichelmeyer (1990)
                 instructional design product that is       York and Ertmer (2011)
                 scalable according to the needs of the
                 project. Rapid prototyping allows
                 instructional designs to combine multiple
                 phases of the instructional design

                              113
            process to facilitate discussions and
            decisions about results.

Reflection  A journal where an instructional designer    Baaki, Tracey, and Hutchinson (2017)
journals    can log any ideas they might help,           Bannan-Ritland (2001)
            reactions to different phases of the         Gray et al. (2015)
            instructional design process, or notes that  Luppicini (2003)
            might be beneficial for a future project.    Moallem (1998)
            The use of a journal helps an instructional  Tracey and Hutchinson (2013)
            designer keep track of their thoughts and    Young (2008)
            ideas that might not be suitable to be
            documented in a design document while
            still promoting a reflection-in-action
            mindset (Schon, 1983).

Conclusion

While decision-making is recognized as a common form of problem-solving in instructional design practices, Jonassen
(2012) contends that there is a need for empirical research to assess decision-making in our field. To date, there is a
growing body of literature exploring the decision-making practices of instructional designers; however, we, as a field,
have just begun to skim the surface. More studies are needed to explore the types and quality of decisions made by
instructional designers of all levels in a variety of contexts. We know that contextual factors contribute to or hinder the
effectiveness of instructional designers' final designs (Morrison et al., 2013; Smith & Ragan, 2005). Researchers have
criticized that the role of context continues to be an aspect of design that still warrants further explanation and
understanding (Tessmer, 1990; Tessmer & Wedman, 1995). This continues to be an issue facing our field. Additional
studies on factors influencing instructional designers' abilities to engage in decision-making will better equip our field to
prepare the future of instructional design (Ertmer et al., 2009; Jonassen, 2008; Stefaniak et al., 2018; Tracey & Boling,
2014).

In the meantime, instructional designers can continue to focus on cultivating their designer identity (Tracey &
Hutchinson, 2016, 2018) by documenting their thoughts and making use of the tools mentioned in this chapter to track
their design decisions during projects. Over time, the aspiring instructional designer will begin to identify patterns in
terms of how they approach various types of design problems, identify and utilize design resources and space, and
articulate their rationale to fellow designers and clients. This continual practice of design documentation will serve the
field well by informing both theory and practice.

References

Baaki, J., & Luo, T. (2019). Instructional designers guided by external representations in a design process. International
         Journal of Technology and Design Education, 29(3), 513-541.

Baaki, J., Tracey, M. W., & Hutchinson, A. (2017). Give us something to react to and make it rich: Designers reflecting-in-
         action with external representations. International Journal of Technology and Design Education, 27(4), 667-682.

Bannan-Ritland, B. (2001). Teaching instructional design: An action learning approach. Performance Improvement
         Quarterly, 14(2), 37-52.

Boling, E., & Gray, C. M. (2015). Designerly tools, sketching, and instructional designers and the guarantors of design. In
         B. Hokanson, G. Clinton, & M.W. Tracey (Eds.), The design of learning experience (pp. 109-126). New York, NY:

                                                                                       114
         Springer.

Boot, E. W., Nelson, J., Van Merriënboer, J. J., & Gibbons, A. S. (2007). Stratification, elaboration and formalisation of
         design documents: Effects on the production of instructional materials. British Journal of Educational
         Technology, 38(5), 917-933.

Demiral-Uzan, M. (2015). Instructional design students' design judgment in action. Performance Improvement Quarterly,
         28(3), 7-23.

Ertmer, P. A., & Cennamo, K. S. (1995). Teaching instructional design: An apprenticeship model. Performance
         Improvement Quarterly, 8(4), 43-58.

Ertmer, P. A., Stepich, D. A., York, C. S., Stickman, A., Wu, X., Zurek, S., & Goktas, Y. (2008). How instructional design
         experts use knowledge and experience to solve ill-structured problems. Performance Improvement Quarterly,
         21(1), 17-42.

Ertmer, P. A., Stepich, D. A., Flanagan, S., Kocaman-Karoglu, A., Reiner, C., Reyes, L., ... & Ushigusa, S. (2009). Impact of
         guidance on the problem-solving efforts of instructional design novices. Performance Improvement Quarterly,
         21(4), 117-132.

Fischer, F., & Mandl, H. (2005). Knowledge convergence in computer-supported collaborative learning: The role of
         external representation tools. The Journal of the Learning Sciences, 14(3), 405-441.

Fitzpatrick, J.L. Sanders, J.P., & Worthen, B.R. (2011). Program evaluation: Alternative approaches and practical
         guidelines (4th ed.). Upper Saddle River, NJ: Pearson.

Gray, C. M., Dagli, C., Demiral-zan, M., Ergulec, F., Tan, V., Altuwaijri, A. A., ... & Boling, E. (2015). Judgment and
         instructional design: How ID practitioners work in practice. Performance Improvement Quarterly, 28(3), 25-49.

Guerra-Lopez, I. (2008). Performance evaluation: Proven approaches for improving program and organizational
         performance. San Francisco, CA: John Wiley & Sons, Inc.

Gustafson, K. (2002). Instructional design tools: A critique and projections for the future. Educational Technology
         Research and Development, 50(4), 59-66.

Hoard, B., Stefaniak, J., Baaki, J., & Draper, D. (2019). The influence of multimedia development knowledge and
         workplace pressures on the design decisions of the instructional designer. Educational Technology Research and
         Development, 67(6), 1479-1505.

Honebein, P. C. (2017). The influence of values and rich conditions on designers' judgments about useful instructional
         methods. Educational Technology Research and Development, 65(2), 341-357.

Huybrechts, L., Schoffelen, J., Schepers, S. & Braspenning, L. (2012). Design representations: Connecting, making, and
         reflecting in design research education. In Boutsen, D. (ed.) Good practices best practices: Highlighting the
         compound idea of education, creativity, research, and practice (pp. 35-42). Brussels: Sint-Lucas School of
         Architecture.

Jonassen, D. H. (2000). Toward a design theory of problem solving. Educational Technology Research and
         Development, 48(4), 63-85.

Jonassen, D. H. (2008). Instructional design as design problem solving: An iterative process. Educational Technology,
         48(3), 21-26.

Jonassen, D. H. (2011). Learning to solve problems: A handbook for designing problem-solving learning environments.
         New York, NY: Routledge.

                                                                                       115
Jonassen, D. H. (2012). Designing for decision making. Educational Technology Research and Development, 60(2),
         341-359.

Korkmaz, N., & Boling, E. (2014). Development of design judgment in instructional design: Perspectives from instructors,
         students, and instructional designers. In B. Hokanson & A. Gibbons (Eds.), Design in educational technology:
         Design thinking, design processes, and the design studio (pp. 161-184). New York, NY: Springer.

Luppicini, R. (2003). Reflective action instructional design (RAID): A designer's aid. International Journal of Technology
         and Design Education, 13(1), 75-82.

Luo, T., & Baaki, J. (2019). Graduate students using concept mapping to visualize instructional design processes.
         TechTrends, 63(4), 451-462.

Marston, M., & Mistree, F. (1997, October). A decision-based foundation for systems design: A conceptual exposition. In
         CIRP 1997, International Design Seminar Proceedings on Multimedia Technologies for Collaborative Design and
         Manufacturing, University of Southern California, Los Angeles, CA (pp. 1-11).

Martin, F. (2011). Instructional design and the importance of instructional alignment. Community College Journal of
         Research and Practice, 35(12), 955-972.

Moallem, M. (1998). An expert teacher's thinking and teaching and instructional design models and principles: An
         ethnographic study. Educational Technology Research and Development, 46(2), 37-64.

Perez, R. S., & Emery, C. D. (1995). Designer thinking: How novices and experts think about instructional design.
         Performance Improvement Quarterly, 8(3), 80-95.

Piskurich, G. M. (2015). Rapid instructional design: Learning ID fast and right. Hoboken, NJ: John Wiley & Sons, Inc.

Roytek, M. A. (2010). Enhancing instructional design efficiency: Methodologies employed by instructional designers.
         British Journal of Educational Technology, 41(2), 170-180.

Schon, D.A. (1983). The reflective practitioner: How professionals think in action. New York, NY: Basic Books.

Shambaugh, N., & Magliaro, S. (2001). A reflexive model for teaching instructional design. Educational Technology
         Research and Development, 49(2), 69-92.

Spector, J. M. (2002). Knowledge management tools for instructional design. Educational Technology Research and
         Development, 50(4), 37-46.

Stefaniak, J. E. (2017). The role of coaching within the context of instructional design. TechTrends, 61(1), 26-31.

Stefaniak, J., Baaki, J., Hoard, B., & Stapleton, L. (2018). The influence of perceived constraints during needs
         assessment on design conjecture. Journal of Computing in Higher Education, 30(1), 55-71.

Stefaniak, J., Maddrell, J., Earnshaw, Y., & Hale, P. (2018). The evolution of designing e-service learning projects: A look
         at the development of instructional designers. International Journal of Designs for Learning, 9(1), 122-134.

Tessmer, M. (1990). Environment analysis: A neglected stage of instructional design. Educational Technology Research
         and Development, 38(1), 55-64.

Tessmer, M., & Wedman, J. (1995). Context-sensitive instructional design models: A response to design research,
         studies, and criticism. Performance Improvement Quarterly, 8(3), 38-54.

Tracey, M.W., & Boling, E. (2014). Preparing instructional designers: Traditional and emerging perspectives. In J.M.
         Spector, M.D. Merrill, J. Elen, & M.J. Bishop (Eds.), Handbook of research on educational communications and
         technology (4th ed., pp. 653-660). New York, NY: Springer.

                                                                                       116
Tracey, M. W. & Hutchinson, A. (2013). Developing Designer Identity Through Reflection. Educational Technology, 53(3),
         28-32.

Tracey, M. W., & Hutchinson, A. (2016). Uncertainty, reflection, and designer identity development. Design Studies, 42,
         86-109.

Tracey, M. W., & Hutchinson, A. (2018). Reflection and professional identity development in design education.
         International Journal of Technology and Design Education, 28(1), 263-285.

Tripp, S. D., & Bichelmeyer, B. (1990). Rapid prototyping: An alternative instructional design strategy. Educational
         Technology Research and Development, 38(1), 31-44.

Van Rooij, S. W. (2010). Project management in instructional design: ADDIE is not enough. British Journal of Educational
         Technology, 41(5), 852-864.

Verschaffel, L., de Corte, E., de Jong, T., & Elen, J. (Eds.). (2010). Use of representations in reasoning and problem
         solving: Analysis and improvement. New York, NY: Routledge.

Yanchar, S. C., South, J. B., Williams, D. D., Allen, S., & Wilson, B. G. (2010). Struggling with theory? A qualitative
         investigation of conceptual tool use in instructional design. Educational Technology Research and Development,
         58(1), 39-60.

Yates, J. F. and Tschirhart, M. D., 2006. Decision-making expertise. In K.A. Ericsson, N. Charness, P.J. Feltovich, and R.R.
         Hoffman (Eds.), The Cambridge handbook of expertise and expert performance (pp. 421-438). New York, NY:
         Cambridge University Press.

York, C. S., & Ertmer, P. A. (2011). Towards an understanding of instructional design heuristics: An exploratory Delphi
         study. Educational Technology Research and Development, 59(6), 841-863.

Young, P. A. (2008). Integrating culture in the design of ICTs. British Journal of Educational Technology, 39(1), 6-17.
Zhang, J. (1997). The nature of external representations in problem solving. Cognitive Science, 21(2), 179-217.

                                                                                       117
            Jill E. Stefaniak

                University of Georgia
                Jill Stefaniak is an Associate Professor in the Learning, Design, and Technology
                program in the Department of Workforce Education and Instructional Technology at
                the University of Georgia. Her research interests focus on the professional
                development of instructional designers and design conjecture, designer decision-
                making processes, and contextual factors influencing design in situated
                environments.
This content is provided to you freely by EdTech Books.
Access it online or download it at https://edtechbooks.org/id/documenting_decisions.

                                                        118
Creating

Creating practices are those that help instructional designers imagine new ideas, activities, products, or services. Done
well, what instructional designers create will directly address the issues or challenges they have been exploring
throughout their process.

     Generating Ideas
     Instructional Strategies
     Instructional Design Prototyping Strategies

                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/creating.

                                                                                       119
120
  9

Generating Ideas

Vanessa Svihla

Brainstorming, ideation, generating ideas. These terms and the kinds of practices they refer to are familiar to many, even
outside of design fields. As instructional designers, we use such techniques to come up with more ideas--and more
creative ideas. But how do these techniques help designers develop ideas? And when and why should we use them?

In this chapter, I first discuss the typical purposes and desired outcomes for ideation. I review some common as well as
new techniques and briefly discuss evidence of their effectiveness, in part to draw attention to the kinds of challenges
designers face when using such techniques. Finally, I re-center the purpose of generating ideas as reframing the
problem.

What Is Ideation? When and Why Do We Typically Generate
Ideas?

Designers commonly generate ideas about possible solutions after the problem is initially framed. Or at least, typical
texts on design suggest this is when designers should generate ideas. We will reconsider that later in this chapter.

Many ideation techniques focus on generating many ideas, going on the assumption that if you generate many ideas,
some of them will surely be creative. This probabilistic reasoning is not always accurate, however. This is because even
if we generate many ideas, they may still be similar to each other. Researchers who study ideation techniques argue
that novelty comes from having dissimilar ideas. This means that variety is more important than quantity. But coming
up with dissimilar ideas can be challenging because of fixation--the experience of getting stuck on previous ideas.
Compared to designers who are not shown an existing solution, designers who are given an example tend to reproduce
features from the example (Jansson & Smith, 1991), even when the example is known to be flawed (Purcell & Gero,
1996). Often, designers are unaware they have incorporated such features, and this is why overcoming fixation can be
so challenging--it is often a covert process.

Research suggests that designers who have less diverse precedent to consider may be more prone to fixation (Purcell &
Gero, 1996). Who has a less diverse precedent? Some may think this would be novice designers because they have not
been exposed to the concepts and materials with which they are designing. But in some fields, like mechanical
engineering and instructional design, we commonly encounter designs, but many of us do not encounter much diversity
in those designs (e.g., a lot of sedans look like one another, and many school lessons look like one another). Repeated
exposure to a limited set of ideas covertly shapes our vision of what could be. And, without deliberate engagement with
diverse precedent, we might not be very influenced by that precedent.

New designers also tend to commit to design ideas prematurely (Rowland, 1992; Shum, 1991), and once committed,
can feel invested and unwilling to change, a phenomenon referred to as sunk cost (Kahneman & Tversky, 1979). In my
own teaching of design, I require messy, hand-drafted first versions of ideation and prototypes and impose a -10%

                                                                                       121
penalty to any such assignment that looks to have been tidied up. This appears to help, but it is still very easy to fall in
love with a first idea. Consider the following vignettes in Tables 1 and 2, in which a supervisor (Sunil) requests fire
extinguisher training to comply with regulations and the design team (newcomer Noel, experienced Eli, and subject
matter expert Marley) considers their options.

   Vignette 1. Meeting With Supervisor

     Sunil
     Of course, we want to make sure our employees are exposed to proper fire extinguisher use. We have to comply
     with these new regulations ASAP.
     Marley
     Some units, like mine, have already been certifying employees because we really have to know how to use an
     extinguisher. But we rely on an external provider.
     Eli
     It seems like that won't scale to the entire organization, given the cost you shared with us earlier.
     Noel
     We can just put together a short online training using the PASS model, with a quiz to certify them. I think the
     pass score should be rather high, though, right? Like 100%. I know we sometimes go with 80%.
     Sunil
     What is the difference between a pass model and pass score?
     Noel
     Oh! Sorry. The PASS model--I googled it before the meeting--is a mnemonic to use the fire extinguisher. It
     means pull the pin, um, aim, and sweep. I forget what the other S stands for, give me a sec--
     Sunil
     How long would it take you to put that together?
     Eli
     Before we get to that, I think we need to consider options.

In the vignette, who shows fixation? Premature commitment? What precedent might shape how the design team and
supervisor evaluate design ideas? How might they overcome fixation and premature commitment? To answer that, let's
look first at the origins of idea generation.

What Are Some Tools for Ideation?

In 1939, Osborn began developing techniques for more creative advertising. He devised classic brainstorming and
published techniques based on years of practice (Osborn, 1957). He advocated for the following techniques as part of

                                                                                       122
brainstorming:
      suspending critique
      considering wild ideas
      coming up with as many ideas as possible
      combining ideas, and
      working in a large group of designers.

Several of these ideas were later empirically challenged, especially group size (Mongeau & Morr, 1999). Generally,
support has been found for more structured ideation methods (Crilly & Cardoso, 2017; Runco et al., 2011; Santanen,
Briggs, & Vreede, 2004; Sosa & Gero, 2013; Yilmaz, Seifert, & Gonzalez, 2010). For instance, an early, somewhat more-
structured approach was lateral thinking, meaning thinking in generative ways (as opposed to analytical "vertical
thinking") (De Bono & Zimbalist, 1970). De Bono described general methods for lateral thinking, such as:

      generating alternatives with a pre-set quota (number of ideas),
      challenging assumptions by repeatedly asking why,
      suspending or delaying judgement, and
      restructuring or reorganizing elements.
In the vignette below, what techniques (from the bulleted lists above) do they use? Where do they stray from the
guidelines for brainstorming and lateral thinking?

                                                                                       123
   Vignette 2. Design Team Meeting: Classic Brainstorming in a Group

     Eli
     I am a little worried that if we just deliver a compliance training, Sunil will consider that sufficient, even for units
     like Marley's, because the cost savings will be so appealing. So, I think we should generate some ideas before
     we commit. So, let's come up with at least 20 ideas. Let's not evaluate them yet, just list any ideas that pop in.
     Noel
     Well, I think we should do the PASS model, followed by a quiz.
     Marley
     That makes me think about job aids. Like we could have a sign, maybe next to or on fire extinguishers?
     Noel
     Nice. And we should make the job aid similar to the training, so the instructional and transfer contexts are
     similar.
     Marley
     That's a good idea. We can use the same font and pictures even.
     Eli
     Sometimes asking "why" helps. Like, why do all employees need this training? Why don't they know how to use a
     fire extinguisher already?
     Marley
     In the certification course we take in my unit, people think they should aim at the top of the flames, but it's the
     base. So, we could focus on that aspect.
     Noel
     And that is also part of the PASS model. And they need it because of compliance though, right?
     Eli
     Let's really try to get some other ideas on the table.
     Noel
     We could make our own model. SAPS? APSS?
     Marley
     Or it could be just like a handout they get.

In this vignette, you may have noticed that although Eli encouraged them not to evaluate ideas, Noel and Marley reacted
in evaluative ways to each other's ideas. Although they did not critique ideas, even providing positive evaluation can

                                                                                       124
shape how others respond because it signals that poor ideas are unwelcome. This in turn can impinge on creative
thinking.

Noel's suggestion to make their own model by rearranging the steps is something those of us who teach design see
often. Coming up with flawed versions of existing ideas accomplishes two things well--it gets you toward whatever
preset quota you need, and it guarantees your favorite idea won't be ruled out--but it does not lead to more creative
ideas. Yet, this approach is common when ideation feels forced or artificial, as can happen when one designer prompts
ideation that others do not see a need for (or when ideation is assigned, such as in an ID class!). Knowing when to
deploy ideation techniques is critical, but this is learned through experience. For practicing designers, ideation is not
always a formal step; they often generate ideas ad hoc. Experienced designers do not always find benefit from typical
ideation techniques (Laakso & Liikkanen, 2012; Linsey et al., 2010; Sio, Kotovsky, & Cagan, 2015; Tauber, 1972;
Vasconcelos & Crilly, 2016), but research suggests these may hold benefit for newcomers.

Below, I have summarized some common structured ideation techniques. I have included a couple that are not common
in instructional design because methods developed in other design fields, like engineering and creativity, are
transferrable outside of product design fields (Moreno, Yang, Hernández, & Wood, 2014). This is important in part
because our most prominent design approach--ADDIE--has relatively little to say about ideation, and even newer
models like SAM do not provide clarity about where new ideas might come from (Allen, 2012).

Take the fire extinguisher training problem described in the vignettes, and try out two of the techniques in Table 1 below.

Table 1

Common Structured Ideation Techniques

Technique                                          Outcomes                       Use in ID

SCAMPER

An elaboration of traditional brainstorming, this  Studies suggest that           SCAMPER has been commonly used
technique structures ideation by providing         SCAMPER may result in          with elementary students. It is a
questions tied to actions that form the            more high-quality novel        particularly promising technique for
SCAMPER acronym: substitute, combine, adapt,       ideas compared to              making incremental changes to
modify/magnify/minimize, put to other uses,        unguided methods               typical instructional settings, where
eliminate, and reverse/rearrange (Eberle, 1972).   (Moreno, Yang, et al.,         major changes may be viewed as
For instance, ask "What can I substitute?"         2014).                         threatening or problematic.

Design Heuristics

Based on expert performance in product and         Design heuristics can          While many of the strategies are
engineering design (Yilmaz, Daly, Seifert, &       support newcomer               specific to engineering or product
Gonzalez, 2015, 2016) this is a well-studied set   designers to develop more      design, many are salient to
of 77 strategy cards--such as add levels, adjust   elaborated and practical       instructional design, especially if we
functions for specific users, repeat,              ideas (Daly, Seifert, Yilmaz,  change "user" to "learner." For
compartmentalize, contextualize, build user        & Gonzalez, 2016).             instance, several focus on user
community, change flexibility, scale up or down,                                  agency, which we could frame as
and incorporate environment--for designers to                                     learner agency--allow the learner to
use as inspiration as they generate ideas.                                        customize, reconfigure, reorient.
                                                                                  What other heuristics might we
                                                                                  identify from expert ID practice? The

                                                   125
                                                                                 list of ID heuristics could be a place
                                                                                 to start (York & Ertmer, 2011).

Design-by-Analogy

These methods include various forms--               Design-by-analogy            Although not commonly used in ID,
Synectics (Gordon, 1961), biomimicry--and           methods can help             this is a promising technique to
include techniques like mapping related words       designers produce more       overcome exposure to traditional
in a network like a concept map or exposure to      novel ideas (Moreno,         precedent.
near or far examples. The latter mirrors intuitive  Hernandez, et al., 2014)
as well as professional design practice in which    especially if the designers  Developing clarity about tensions is
designers rely on precedent. However it             use far analogies (Chan et   also promising. Common
involves deliberately considering ideas that may    al., 2011) which can help    contradictions salient in instructional
be similar or wildly different as sources of        them think more broadly      design are breadth versus depth,
inspiration.                                        about a problem (S. M.       efficiency versus understanding, and
                                                    Smith & Linsey, 2011).       convenience versus learning.
Common to engineering design, the TRIZ
(Altshuller, 1996) approach involves first          TRIZ has led to more
identifying "contradictions" then looking at        varied ideas (Belski,
ways others have resolved the same kind of          Hourani, Valentine, &
contradiction.                                      Belski, 2014).

Nominal group

In a group, individuals silently generate ideas.    Compared to an               Nominal group techniques are
Each member shares ideas. After all have been       unstructured group,          beneficial when generating ideas with
shared, members clarify and evaluate ideas          nominal groups generate      stakeholders or in groups with power
collectively then vote individually.                more ideas (Ven &            imbalances because it opens space
                                                    Delbecq, 1974).              for all members to participate.

Bodystorming

Rather than attempting to generate ideas            Bodystorming has been        Although not commonly used (in ID
removed from context, bodystorming involves         helpful when designing       or other fields), bodystorming can be
acting out the problem and possible solutions       with new or unfamiliar       particularly generative when
in situ (Oulasvirta, Kurvinen, & Kankainen,         learning technologies (B.    considering the configuration of
2003).                                              K. Smith, 2014).             learning spaces, ways to arrange
                                                                                 collaborating learners, and mobile
                                                                                 learning.

Contrast the two techniques you tried out:

      Which did you prefer and why?
      Which led you to produce more ideas?
      Which do you think led you to generate more novel ideas?
      Which do you think led you to generate higher quality ideas?

                                                    126
If you found answering the last two questions more difficult, you are not alone. Researchers have long debated the best
ways to measure novelty and quality of ideas. While counting the number of ideas generated is straightforward, as
mentioned earlier, this does not necessarily result in better ideas. Novelty is often characterized by the variety or
breadth of ideas of a single designer as well as the frequency of their ideas compared to other designers (Hernandez,
Okudan, & Schmidt, 2012). Quality is sometimes measured as feasibility, usability (Kudrowitz & Wallace, 2013) or the
degree to which needs are met without violating constraints.
Others have also considered characteristics such as ethics and empathy. This means evaluating the just distribution of
risks and benefits for multiple and especially marginalized groups (Beever & Brightman, 2016). Although not commonly
used, techniques that sensitize the instructional designer to the experiences of marginalized groups and connect this to
their own experiences prior to generating ideas has potential for addressing persistent inequities and structural
oppression (Kouprie & Visser, 2009; Visser & Kouprie, 2008). Such approaches also tend to more clearly change the
problem space.

How Can Ideation Reshape the Problem Space?

So far, we have mostly focused on the solution space, but due to the ill-structured nature of design problems, ideation
also changes the problem space (Cardoso, Badke-Schaub, & Eris, 2016) as designers reframe the problem during
ideation (Daly, Yilmaz, Christian, Seifert, & Gonzalez, 2012). Designers sometimes relax constraints and this can reshape
the problem space (Chan, Dang, Kremer, Guo, & Dow, 2014; Silk, Daly, Jablokow, Yilmaz, & Rosenberg, 2014). By
temporarily ignoring a key constraint, sometimes we can notice something new about the problem space.
Similarly, my own approach--the Wrong Theory Protocol (WTP, https://edtechbooks.org/-IAVb)--likewise tends to
reshape the problem space. In this approach, we ask designers to first come up with ideas that would cause harm and
humiliation prior to generating beneficial ideas. I was inspired by a magazine article on artists and designers
deliberately creating displeasing and wrong works (Dadich, 2014). When we first incorporated it into an ideation
session, we noticed that the most humiliating ideas led to more empathetic insights and changed problem frames.
Consider the vignette below to understand why this might be.

                                                                                       127
   Vignette 3. Design Team Meeting: Wrong Theory Protocol

     After individually generating harmful and humiliating ideas, the team discusses their insights:

     Eli

     I think my worst idea was locking the learner in a room with a small fire burning and a sort of Rube Goldberg fire
     extinguisher with terribly complex instructions. They first can't get it started, and once set in motion, the
     extinguisher has too many steps to get through and the fire grows and grows.

     Noel

     Wow. That's terrible. Mine was giving them a depleted extinguisher with no instructions and putting them on one
     of those weird game shows, where if they can't make the extinguisher go, they have to eat spiders.

     Marley

     Ew! You both had much worse ideas than me. I think mine was just lazy. I said just give them no instructions
     and wait for a fire, then put up a list in the hall of those who messed up. Eli, your wrong design makes me think
     of how--in some of our units, it could go really wrong if someone who got basic training used the wrong kind of
     extinguisher. Some of our labs have two or three kinds for different situations.

     Noel

     You know, at first, I thought, we just need to make sure everyone knows how to use a basic model, but now I
     wonder if that could actually lead to accidents. If we tackle this just as a compliance problem, we could make it
     worse.

In this vignette, how did the problem change as a result of insight gained from generating wrong ideas? Why do you
think it changed?

In our work on WTP, designers' beneficial ideas, though not numerous, tend to be both creative and empathetic. We have
several reasons for why WTP might work. Perhaps designers feel beholden to stakeholders after coming up with
harmful and humiliating ideas? Or perhaps they simply gain empathy? Maybe they notice something new about the
problem situation? Or, perhaps in absence of the pressure to be right, they are able to be more creative? Afterall,
research on suspending judgment suggests that it is difficult to accomplish.

Conclusion

Instead of treating ideation as the tipping point between being problem- and solution-focused, try generating ideas
across the depth and duration of your design process to help you frame the problem with empathy and design learning
experiences that meet needs without unintentionally widening gaps. By depth, I mean that it can help to drill down and
ideate on a particular aspect.

While this chapter introduced a few techniques, there are many more available.

Finally, ideation can be an effective tool when employed at any sticking point. Low fidelity prototypes, use-cases and
early storyboards often reveal issues that can be dealt with through ideation. Even in pilot implementation, having
ideation techniques ready-to-hand can avert disaster when issues come up. This kind of generative thinking--How can it
work? How else could it work?--serves designers well throughout their design work.

                                                                                       128
   Additional Readings and Resources

     There are many texts that illustrate additional ideation/creativity techniques. I always recommend keeping an
     eye out for one that appeals to you.
     Kelley, T., & Littman, J. (2006). The ten faces of innovation: IDEO's strategies for defeating the devil's advocate

              and driving creativity throughout your organization: Crown Business.
     Michalko, M. (2011). Cracking creativity: The secrets of creative genius: Ten Speed Press.
     Michalko, M. (2010). Thinkertoys: A handbook of creative-thinking techniques: Ten Speed Press.
     Sawyer, K. (2013). Zig Zag: the surprising path to greater creativity: John Wiley & Sons.

Acknowledgments

This material is based upon work supported by the National Science Foundation under Grant No. EEC 1751369. Any
opinions, findings, and conclusions or recommendations expressed in this material are those of the author and do not
necessarily reflect the views of the National Science Foundation.

References

Allen, M. (2012). Leaving ADDIE for SAM: An agile model for developing the best learning experiences: American
         Society for Training and Development.

Altshuller, G. (1996). And suddenly the inventor appeared: TRIZ, the theory of inventive problem solving (2nd ed.).
         Worchester, MA: Technical Innovation Center, Inc.

Beever, J., & Brightman, A. O. (2016). Reflexive principlism as an effective approach for developing ethical reasoning in
         engineering. Science and Engineering Ethics, 22(1), 275-291.

Belski, I., Hourani, A., Valentine, A., & Belski, A. (2014). Can simple ideation techniques enhance idea generation? Paper
         presented at the 25th Annual Conference of the Australasian Association for Engineering Education: Engineering
         the Knowledge Economy: Collaboration, Engagement & Employability.

Cardoso, C., Badke-Schaub, P., & Eris, O. (2016). Inflection moments in design discourse: How questions drive problem
         framing during idea generation. Design Studies, 46, 59-78. doi:10.1016/j.destud.2016.07.002

Chan, J., Dang, S., Kremer, P., Guo, L., & Dow, S. (2014). Ideagens: A social ideation system for guided crowd
         brainstorming. Paper presented at the Second AAAI Conference on Human Computation and Crowdsourcing.

Chan, J., Fu, K., Schunn, C., Cagan, J., Wood, K., & Kotovsky, K. (2011). On the benefits and pitfalls of analogies for
         innovative design: Ideation performance based on analogical distance, commonness, and modality of examples.
         Journal of Mechanical Design, 133(8), 081004.

Crilly, N., & Cardoso, C. (2017). Where next for research on fixation, inspiration and creativity in design? Design Studies,
         50, 1-38. doi:10.1016/j.destud.2017.02.001

Dadich, S. (2014, September 23). Why getting it wrong is the future of design. Wired, 126-133.

                                                                                       129
Daly, S. R., Seifert, C. M., Yilmaz, S., & Gonzalez, R. (2016). Comparing ideation techniques for beginning designers.
         Journal of Mechanical Design, 138(10), 101108.

Daly, S. R., Yilmaz, S., Christian, J. L., Seifert, C. M., & Gonzalez, R. (2012). Design heuristics in engineering concept
         generation. Journal of Engineering Education, 101(4), 601-629.

De Bono, E., & Zimbalist, E. (1970). Lateral thinking. London, UK: Penguin.

Eberle, R. F. (1972). Developing imagination through scamper. Journal of Creative Behavior.

Gordon, W. J. (1961). Synectics: The development of creative capacity.

Hernandez, N. V., Okudan, G. E., & Schmidt, L. C. (2012). Effectiveness metrics for ideation: Merging genealogy trees and
         improving novelty metric. Paper presented at the ASME 2012 International Design Engineering Technical
         Conferences and Computers and Information in Engineering Conference.

Jansson, D. G., & Smith, S. M. (1991). Design fixation. Design Studies, 12(1), 3-11.

Kahneman, D., & Tversky, A. (1979). Prospect theory: An analysis of decision under risk. Econometrica, 47(2), 363-391.

Kouprie, M., & Visser, F. S. (2009). A framework for empathy in design: stepping into and out of the user's life. Journal of
         Engineering Design, 20(5), 437-448.

Kudrowitz, B. M., & Wallace, D. (2013). Assessing the quality of ideas from prolific, early-stage product ideation. Journal
         of Engineering Design, 24(2), 120-139.

Laakso, M., & Liikkanen, L. A. (2012). Dubious role of formal creativity techniques in professional design. Paper
         presented at the DS 73-1 Proceedings of the 2nd International Conference on Design Creativity Volume 1.

Linsey, J. S., Tseng, I., Fu, K., Cagan, J., Wood, K. L., & Schunn, C. (2010). A study of design fixation, its mitigation and
         perception in engineering design faculty. Journal of Mechanical Design, 132(4), 041003.

Mongeau, P. A., & Morr, M. C. (1999). Reconsidering brainstorming. Group Facilitation: A Research and Applications
         Journal, 1(1), 14-21.

Moreno, D. P., Hernandez, A. A., Yang, M. C., Otto, K. N., Hölttä-Otto, K., Linsey, J. S., . . . Linden, A. (2014). Fundamental
         studies in Design-by-Analogy: A focus on domain-knowledge experts and applications to transactional design
         problems. Design Studies, 35(3), 232-272.

Moreno, D. P., Yang, M. C., Hernández, A. A., & Wood, K. L. (2014). Creativity in Transactional Design Problems: Non-
         Intuitive Findings of an Expert Study Using Scamper. Paper presented at the DS 77: Proceedings of the DESIGN
         2014 13th International Design Conference.

Osborn, A. F. (1957). Applied imagination. New York, NY: Scribner.

Oulasvirta, A., Kurvinen, E., & Kankainen, T. (2003). Understanding contexts by being there: case studies in
         bodystorming. Personal and ubiquitous computing, 7(2), 125-134.

Purcell, A. T., & Gero, J. S. (1996). Design and other types of fixation. Design Studies, 17(4), 363-383.

Rowland, G. (1992). What do instructional designers actually do? An initial investigation of expert practice. Performance
         Improvement Quarterly, 5(2), 65-86.

Runco, M. A., Noble, E. P., Reiter-Palmon, R., Acar, S., Ritchie, T., & Yurkovich, J. M. (2011). The genetic basis of creativity
         and ideational fluency. Creativity Research Journal, 23(4), 376-380.

                                                                                       130
Santanen, E. L., Briggs, R. O., & Vreede, G.-J. D. (2004). Causal relationships in creative problem solving: Comparing
         facilitation interventions for ideation. Journal of Management Information Systems, 20(4), 167-198.

Shum, S. (1991). Cognitive dimensions of design rationale: Citeseer.
Silk, E. M., Daly, S. R., Jablokow, K. W., Yilmaz, S., & Rosenberg, M. (2014). Interventions for Ideation. AERA.
Sio, U. N., Kotovsky, K., & Cagan, J. (2015). Fixation or inspiration? A meta-analytic review of the role of examples on

         design processes. Design Studies, 39, 70-99.
Smith, B. K. (2014). Bodystorming mobile learning experiences. TechTrends, 58(1), 71-76.
Smith, S. M., & Linsey, J. (2011). A three-pronged approach for overcoming design fixation. The Journal of Creative

         Behavior, 45(2), 83-91.
Sosa, R., & Gero, J. S. (2013). The creative value of bad ideas. Paper presented at the Conference on Computer-Aided

         Architectural Design Research in Asia (CAADRIA 2013).
Tauber, E. M. (1972). HIT: Heuristic ideation technique. A systematic procedure for new product search. The Journal of

         Marketing, 58-61.
Vasconcelos, L. A., & Crilly, N. (2016). Inspiration and fixation: Questions, methods, findings, and challenges. Design

         Studies, 42, 1-32. doi: 10.1016/j.destud.2015.11.001
Ven, A. H. V. D., & Delbecq, A. L. (1974). The effectiveness of nominal, Delphi, and interacting group decision making

         processes. Academy of Management Journal, 17(4), 605-621.
Visser, F. S., & Kouprie, M. (2008). Stimulating empathy in ideation workshops. Paper presented at the Proceedings of

         the Tenth Anniversary Conference on Participatory Design 2008.
Yilmaz, S., Daly, S. R., Seifert, C. M., & Gonzalez, R. (2015). How do designers generate new ideas? Design heuristics

         across two disciplines. Design Science, 1, e4.
Yilmaz, S., Daly, S. R., Seifert, C. M., & Gonzalez, R. (2016). Evidence-based design heuristics for idea generation. Design

         Studies, 46, 95-124. doi:10.1016/j.destud.2016.05.001
Yilmaz, S., Seifert, C. M., & Gonzalez, R. (2010). Cognitive heuristics in design: Instructional strategies to increase

         creativity in idea generation. Artificial Intelligence for Engineering Design, Analysis and Manufacturing, 24(03),
         335-355.
York, C. S., & Ertmer, P. A. (2011). Towards an understanding of instructional design heuristics: An exploratory Delphi
         study. Educational Technology Research and Development, 59(6), 841-863.

                                                                                       131
This content is provided to you freely by EdTech Books.
Access it online or download it at https://edtechbooks.org/id/generating_ideas.

                                                        132
  10

Instructional Strategies

Joshua Hill & Linda Jordan

   Editor's Note

     This is a condensed version of a chapter on Instructional Strategies from the book Experiential Learning in
     Instructional Design and Technology, by Joshua Hill and Linda Jordan. It is printed here under a similar license
     as the original.

Introduction

A well designed course, whether it be face-to-face, blended, or online, must be well structured with careful attention
to instructional strategies in the selection of instructional material, the planning of learning activities, and the selection
of media. An instructional strategy describes the instructional materials and procedures that enable students to
achieve the learning outcomes. Learning outcomes are what the student should know, or be able to accomplish at the
end of the course or learning unit. Your instructional strategy should describe the instructional materials' components
and procedures used with the materials that are needed for students to achieve the learning outcomes. The strategy
should be based on the learning outcomes and information from the other previous instructional design steps. You can
even base your strategy on how you or others have solved similar problems. You can save time and money by not re-
inventing the wheel. However, be careful; a lot of existing instructional material is designed poorly. Use the instructional
strategy as a framework for further developing the instructional materials or evaluating whether existing materials are
suitable or need revision. As a general rule, use the strategy to set up a framework for maximizing effective and efficient
learning. This often requires using strategies that go beyond basic teaching methods. For example, discovery-learning
techniques can be more powerful than simply presenting the facts.
This chapter reviews some basic information to help you choose appropriate instructional strategies for the learning
outcomes you hope your learners will be able to accomplish. Rather than reviewing specific details about any of the
hundreds of instructional strategies that have been developed, this chapter describes considerations that should go into
the selection of any instructional strategy.

Goal Analysis

Goal analysis includes classifying the instructional goal into the domain, or kind of learning that will occur. The domains
can be verbal information where learners state, list, describe, name, etc., intellectual skills such as learning how to
discriminate, identify, classify, demonstrate, generate, originate, create, etc., psychomotor skills where learners make,

                                                                                       133
draw, adjust, assemble, etc., and attitudes such as making choices or decisions. If you used a guide like Bloom's
Taxonomy when generating your learning outcomes, you likely have a good handle on the type of learning you hope will
occur. Establishing the domain is important in determining what instructional strategies to use in subsequent steps.

Learning Domain Strategies

Each learning domain classification (i.e., verbal information, intellectual skills and cognitive strategies,
psychomotor skills, and attitudes) is best taught with different instructional strategies.

Verbal Information

Verbal information is material, such as names of objects, that students simply have to memorize and recall.

When teaching verbal information:

      Organize the material into small, easily retrievable chunks.
      Link new information to knowledge the learner already possesses. For example, use statements such as
      "Remember how", or "This is like ...". Linking information helps the learner to store and recall the material.
      Use mnemonics and other memory devices for new information. You may recall that the musical notes of the treble
      clef staff lines can be remembered with the mnemonic Every Good Boy Deserves Fudge.
      Use meaningful contexts and relevant cues. For example, relating a problem to a sports car can be relevant to
      some members of your target audience.
      Have the learners generate examples in their minds, such as create a song or game with the information or apply
      the knowledge to the real world. If the student only memorizes facts then the learning will only have minimal value.
      Avoid rote repetition as a memorization aid. Rote learning has minimal effectiveness over time.
      Provide visuals to increase learning and recall.

Intellectual Skills

Intellectual skills are those that require learners to think (rather than simply memorizing and recalling information).

When teaching intellectual skills:

      Base the instructional strategy and sequencing on an analysis such as a topic or a procedural analysis. Always
      teach subordinate skills before higher-level skills.
      Link new knowledge to previously learned knowledge. You can do this explicitly (e.g., the bones in your feet are
      comparable to the bones you learned about in your hands) or implicitly (e.g., compare the bones in your feet to
      other bone structures you have learned about).
      Use memory devices like acronyms, rhymes, or imagery for information such as rules or principles. You can use the
      first letters of words to help memorize information. For example, "KISS" means "Keep It Simple Stupid". General
      rules can often be remembered through rhymes such as "i before e except after c". Remember that rules often have
      exceptions. Tell your learners about the exceptions. Memory devices are best for limited amounts of information.
      Use examples and non-examples that are familiar to the student. For instance, when classifying metals, iron and
      copper are examples while glass and plastic are non-examples.
      Use discovery-learning techniques. For example, let students manipulate variables and see the consequences.
      Use analogies that the learners know. However, be careful that learners do not over-generalize or create
      misconceptions.
      Provide for practice and immediate feedback.

Psychomotor Skills

Psychomotor skills are those that require learners to carry out muscular actions.

                                                                                       134
When teaching psychomotor skills:
      Base the instructional strategy on an analysis such as a procedural analysis or a critical incident analysis.
      Provide directions for completing all of the steps.
      Provide repeated practice and feedback for individual steps, then groups of steps, and then the entire sequence.
      Remember that, in general, practice should become less dependent on written or verbal directions.
      Consider visuals to enhance learning.
      Consider job aids, such as a list of steps, to reduce memory requirements. This is especially important if there are
      many procedures or if the procedures are infrequently used.
      After a certain point, allow learners to interact with real objects or do the real thing. How much can you learn about
      swimming without getting wet?

Note that some skills involve other learning-domain classifications. For example, when learning how to operate a
camcorder, many of the skills are psychomotor. However, deciding how to light an image is an intellectual skill. Also,
note that the required proficiency level can affect the instructional strategy. There is a big difference between being able
to imitate a skill and being able to automatically do a skill.

Attitudes

Attitudes involve how a student feels about the instruction, whether they will value or care about the material presented
to them.
When teaching attitudes:

      Base the instructional strategy on the instructional design steps done earlier.
      If you can, show a human model to which the students can easily relate. One consideration is that it may be better
      if the model is of the same socioeconomic group.
      Show realistic consequences to appropriate and inappropriate choices.
      Consider using video.
      Remember that attitudes taught through computer technology are not guaranteed to transfer to the real world. If
      appropriate and possible, consider arranging for practice opportunities to make the choice in real life. Alternatively,
      use role-playing to reinforce the attitudes taught.
Note that it can be difficult to test whether the attitudes taught have transferred to real situations. Will learners behave
naturally if they know that they are being observed? If learners have not voluntarily permitted observations, then you
must consider whether it is ethical to make the observations.

Strategies to Sequence Learning Outcomes

Another aspect of your instructional strategy will be to determine the sequence of how the learning outcomes will be
taught. In general, to best facilitate learning you should sequence the learning outcomes from:

                                                                                       135
      easy to hard
            You could teach adding fractions with common denominators and then with different denominators. Your
            lesson could first deal with writing complete sentences and then writing paragraphs.

      simple to complex
            As an example, teach recognizing weather patterns and then predicting the weather.
            Cover replacing a washer and then replacing a faucet.

      specific to general
            You could teach driving a specific car and then transfer the skills to driving any car. Similarly, you could cover
            adjusting the brakes on a specific mountain bike and then generalize the procedure to other mountain bikes
            Note that some students like to learn through an inductive approach (that is, from the general to the specific).
            For example, students could be presented with a number of simple examples, and based on those, be asked to
            generalize a rule. That general rule can then be applied to solving specific examples. Since some students will
            not enjoy an inductive approach, do not use it all of the time. Rather consider an inductive approach as a way
            to provide some variation and occasionally address other learning preferences.

      concrete to abstract
            As an example, teach measuring distances with a tape measure and then estimating distances without a tape
            measure. Cover writing learning outcomes and then evaluating learning outcomes.

      the known to the unknown
            You could do this by starting with concepts learners already know and extending those concepts to new ideas.
            In other words, build on what has been previously taught.

Each of these methods of sequencing learning outcomes enables students to acquire the needed knowledge base for
learning higher-level skills. Note that these guidelines are not black and white rules.

Strategies to Motivate Students - The ARCS Model

As described by Keller, motivation can be enhanced through addressing the four attributes of Attention, Relevance,
Confidence, and Satisfaction (ARCS). Try to include all of the attributes since each alone may not maintain student
motivation. Your learner analysis may have provided useful information for motivating students. You should build
motivational strategies into the materials throughout the instructional design process. This is challenging since each
learner is an individual with unique interests, experiences, and goals.

Attention

Gain attention and then sustain it. You can gain attention by using human-interest examples, arousing emotions such as
by showing a peer being wheeled into an ambulance, presenting personal information, challenging the learner, providing
an interesting problem to solve, arousing the learner's curiosity, showing exciting video or animation sequences, stating
conflicting information, using humor, asking questions, and presenting a stimulus change that can be as simple as an
audio beep. One way to sustain attention is by making the learning highly interactive.

Relevance

Relevance helps the student to want to learn the material by helping them understand how the material relates to their
needs or how it can relate to improving their future. For example, when teaching adult students how to solve percent
problems, having them calculate the gratuity on a restaurant bill may be more relevant than a problem that compares
two person's ages. You can provide relevance through testimonials, illustrative stories, simulations, practical
applications, personal experience, and relating the material to present or future values or needs. Relevance is also
useful in helping to sustain attention. For material to be perceived as being relevant, you must strive to match the
learner's expectations to the material you provide.

                                                                                       136
Confidence

If students are confident that they can master the material, they will be much more willing to attempt the instruction.
You will need to convince students with low confidence that they can be successful. You can do this through presenting
the material in small incremental steps, or even by stating how other similar students have succeeded. Tasks should
seem achievable rather than insurmountable. You should also convince students who are overconfident that there is
material that they need to learn. You can do this by giving a challenging pre-test or presenting difficult questions.

Satisfaction

Satisfaction provides value for learning the material. Satisfaction can be intrinsic from the pleasure or value of the
activity itself, extrinsic from the value or importance of the activity's result, for social reasons such as pleasing people
who's opinions are important to them, for achievement goals such as the motive to be successful or avoid failure, or a
combination of these. Examples of intrinsic satisfaction include the joy or challenge of learning, increased confidence,
positive outcomes, and increased feelings of self-worth. Examples of extrinsic satisfaction include monetary rewards,
praise, a certificate, avoidance of discomfort or punishment for not doing it, and unexpected rewards. Some evidence
suggests that extrinsic motivation, such as a certificate for completing a course, does not last over time. Nonetheless, it
is better to assume that some students need extrinsic motivation. To be safe, try to provide your learners with both
intrinsic, which should have more of the focus, and extrinsic rewards. If the intrinsic motivation is high for all learners,
you will not need to plan as much for extrinsic motivation. Note that satisfaction can be provided by enabling learners to
apply the skills they have gained in a meaningful way. Remember to let the students know that the material to be
learned is important. Consider increasing extrinsic motivation through quizzes and tests.

Strategies for Sequencing Instructional Events

As Robert Gagné described, that instructional events (gaining attention, informing the learner of the learning outcome,
stimulating recall of prerequisites, presenting the material, providing learning guidance, eliciting the performance,
providing feedback, assessing performance, and enhancing retention and transfer) represent what should be done to
ensure that learning occurs. If you address each instructional event, you will have a solid foundation for creating
effective instructional materials. You will need to determine what will be done for each instructional event for each
learning outcome.
You can learn more about sequencing instructional events from another chapter in this textbook, Robert Gagné and the
Systematic Design of Instruction.

Conclusion

The emphasis in this review of instructional strategies was on getting the fundamentals right. Regardless of what
revolutionary tools or teaching approaches are being used, what we know of how people learn does not change a great
deal over time, and we do know that learning is a process, and you ignore the factors that influence that process at your
peril.
For learning leading to successful outcomes, it is important to remember that most students need:

                                                                                       137
      well-defined learning goals;
      instructional strategies linked with the appropriate learning domains;
      a proper sequencing of instructional events; providing a clear timetable of work, based on a well-structured
      organization of the curriculum;
      appropriate and engaging learning activities; with regular feedback
      manageable study workloads appropriate for their conditions of learning;
      a skilled instructor; regular instructor communication and presence;
      a social environment that draws on, and contributes to, the knowledge and experience of other students;
      other motivated learners to provide mutual support and encouragement.
There are many different ways these criteria can be met, with many different tools.
Remember these key takeaways when designing your instructional strategies:
      Learning domain classification (i.e., verbal information, intellectual skills and cognitive strategies, psychomotor
      skills, and attitudes) are best taught with different instructional strategies.
      Teach learning outcomes in the order that best facilitates learning.
      The four attributes of the Keller's ARCS Motivational Model are; Attention, Relevance, Confidence, and Satisfaction.
      Including all of the attributes may increase student motivation.
      The unique interests, experiences, and goals of each learner influence motivation.
      Instructional events include; gaining attention, informing the learner of the learning outcome, stimulating recall of
      prerequisites, presenting the material, providing learning guidance, eliciting the performance, providing feedback,
      assessing performance, and enhancing retention and transfer. (as reviewed in the chapter Robert Gagné and the
      Systematic Design of Instruction).

   Application Exercise

     You have been tasked with designing a university orientation course for freshmen community college students.
     Everyone at the institution is aware that students feel an orientation course is not necessary and that it is a
     waste of their time. Explain what portion of the ARCS Motivational Model might be applied into the design of the
     course to help students understand why this course is important for their success.

                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/instructional_strate.

                                                                                       138
  11

Instructional Design Prototyping Strategies

Jacquelyn Claire Johnson & Richard E. West

One of the differences between design as practiced in our field and traditional art is that our designs must not only be
interesting, engaging, and even beautiful, but they must also be useful for someone--the end users or learners. Over
2,000 years ago, Marcus Vitruvius--a Roman architect--articulated that good architecture should rise to three ideals:
firmitas (strength), utilitas (functionality), and venustas (beauty). In other words, a building should be strong and not fall
down, it should accomplish its purpose (e.g. as a home or an office), and it should be beautiful to enjoy.

        Instructional designers seek the same three ideals in our products. For us, we desire the learning
        environments we create to work well, teach well, and, well, be beautiful and enjoyable to experience!

Prototyping is an essential skill and process for instructional designers to achieve these three goals. Despite careful
and rigorous front-end analysis, user research, and attention to detail during development, it is nearly impossible to
produce instruction that works perfectly the first time. However, through iterations of prototypes, we can evaluate how
well our instructional designs are working, teaching, and being enjoyed by a group of potential users. This will increase
the likelihood that final designs will be successful. In addition, digital technologies have reduced the cost of creating
prototypes, which has led to a new focus on agile, lean, and rapid prototyping design models where prototypes are not a
single step in the process, but instead, each stage of design development can be tested as a new prototype--and this
continual refinement of the design through continuous evaluation may never cease (see Wiley & Bodily's chapter in this
book).

How can we effectively prototype and test our designs? We can learn much about prototyping from other design fields.
For example, it is standard practice to use visual representations of ideas--such as pictures--during the creative
process in many design fields such as architecture (Bilda et al., 2006), film and cinematography (Teng et al., 2014), and
engineering (Perry & Sanderson, 1998). This skill is so meaningful, graphic design instructors insist that it is vital to
"equip students with the ability to make well-informed decisions about tool choice and tool use during design ideation"
(Stones & Cassidy, 2010, p. 439).

Though graphic design is an inherently visual field, the use of prototypes has application in other design fields as well.
For instance, extensive research demonstrates the usefulness of visuals in product development as a means of
exploring problems and generating possible solutions. Prototypes help designers understand specific design
challenges and make inferences about the situation (Suwa & Tversky, 1997). They also contribute to many aspects of
problem solving (Dorst & Cross, 2001; Do et al., 2000). Research in cognitive psychology has established that the
cognitive load of processing ideas is reduced for designers through the use of visuals.

Furthermore, studies show it is easier for designers to process complex ideas with visual prototypes rather than relying
on working memory (Cash, StankoviÄ, & Storga, 2014). Vicarious experiences can be provided through visuals, which
allow designers to glean and evaluate the pertinent information without investing as much time or effort into creating

                                                                                       139
the experience (Menezes & Lawson, 2006). Prototypes also can guide important design conversations "if they lead the
team visually into a fruitful sequence of conversation steps" (Eppler & Kernbach, 2016, p. 96).

Key Prototyping Principles

Dam and Siang (2018) argued that during prototyping you should pay attention to the following:

      People--including those whom you are testing and the observers. Because we design for humans, we are
      particularly interested in how humans interact with and perceive the usefulness of our designs.
      Objects--including the prototype and other objects people interact with, because what people choose to do and the
      objects they choose to interact with can provide clues into why they like or do not like our design.
      Location--such as places and environments, because we can learn from where people choose to use designs, and
      why they use them in those locations, and what affordances those locations provide for using the design.
      Interactions--including digital or physical interactions between people, the objects, and the environment. This is
      particularly essential because the interactions we observe provide clues into how the design could be used, and
      any unintended outcomes.

Similarly in our field, Andrew Gibbons (2013) has argued that every instructional design is comprised of various layers,
such as the following:

     Content, or the actual material to be learned
     Strategy, or the unifying framework about how the teaching/learning is theorized to happen, or how the tasks
      involved in learning should take place
     Control, or how students interact with and provide input back into the learning material
     Message, or the intended meaning the instruction is meant to communicate to the learner
     Representation, or how the layers of the design are presented to learners (visual, audio, touch, etc.)
     Media-logic, or the background structures that activate each component of the instruction at the proper time and in
      the proper way
     Management, or how data about people's use of the instruction is collected and managed to improve learning and
      communicate about outcomes to stakeholders.

A design prototype, then, should serve to test one or all of these components from Dam and Siang and/or Gibbons. In
other words, a high fidelity prototype, created close before implementation, would likely try to test all of these
components. An earlier prototype may focus on one or two, perhaps testing primarily the validity of the content or
messaging layers, the ability of the learner to control the interface, or the reliability of the media.

Prototyping Stages and Goals for Each Stage

In our opinion, there are three key stages for prototyping, and there are different primary goals for each stage, as
described in Table 1.

Table 1

Prototyping Stages and Goals

Prototyping Stage                             Prototyping Goals

Static/paper--These prototypes can be         The primary goal is to test the logic of the design with users, experts,
created on paper or digitally, but typically  and clients. Do they think this is likely to succeed? Which aspects or
are static and do not involve interactivity,  attributes of the design do they think warrant full development? Does
graphic design, or other expensive features.  this design seem like a good answer to the instructional problem? Are

                                              140
These are often "Wizard of Oz" or paper       we using the best content? What insights do they have now about how
prototypes, described below.                  to present the final product (e.g. what media format, location, or scale
                                              should we aim for?)? This is also a good time to estimate the potential
                                              costs in time and money to develop the design, and to ensure all
                                              parties feel the scope is accurate.

Low fidelity product/process--These           Low fidelity prototypes are produced to give users and clients a better
prototypes have minimal interactivity and     idea about how a design may look and interact, and how instructional
visual storyboards instead of full graphics.  content and strategies will be presented. Things do not work perfectly,
                                              but the focus is on testing the ideas, interaction, and potential of the
                                              design.

High fidelity product/process--These          First impressions often matter a great deal, so before launching a
prototypes should be nearly completed         product with actual users, ready-to-launch prototypes should be
designs, and ready for rigorous internal      rigorously tested internally or with a sample of users. This process is
testing.                                      usually repeated multiple times with larger groups of people until there
                                              is confidence that most of the design bugs have been identified, the
                                              product works reasonably well, and users will be able to use the
                                              product as intended.

Beta or soft launch of the design--Many       The goal of this stage is to fully test all aspects of the design, including
designers now choose to launch a design in    user satisfaction and implementation costs. However, by keeping the
beta form, allowing users full range of       design in beta, there is still flexibility to redesign an aspect not working
access to the design, but without a promise   very well, and usually users will be more forgiving.
that everything will work perfectly.

Full launch/implementation                    Even when we feel a design is "done" or ready for launch, we continue
                                              to collect confirmative or "continuous" (Wiley and Bodily, 2020)
                                              evaluation data on how well it is working and make adjustments as
                                              needed.

Prototyping Strategies

There are many strategies to prototyping ideas. Essentially, whatever you as a designer can do to test out any aspect of
your design is a prototype. For example, this can be something visual, tactile, auditory, or performance-related.
Following are some of the most common prototyping strategies.

Sketching

Sketches are "rough drawings representing the chief features of an object or scene and often made as a preliminary
study" (Sketches, n.d.). For an example of a sketch, see Figure 1. Because sketches are simple and easily created, they
are used by designers in the automotive industry to develop new design concepts. Researchers studied six designers at
the Ford design studio to understand the physical and mental processes these designers go through as they sketch.
They compared the process of these professional designers to student designers to ascertain the differences between
the two groups. Findings indicated that, when compared to novice designers, professionals have a greater
understanding of physical dimension and used an iterative design approach in which they used sketches to facilitate
problem solving and creative thought (Tovey et al., 2003).

                                              141
Figure 1
Sketch of Exhibit Design Layout

Note. Many of the examples provided in this chapter come from museum exhibit design, which was the background of
the lead author.
As illustrated by the automotive designers, sketches elucidate aspects of the parallel development of the designer and
the product. Sketches allow designers to set out ideas spontaneously (Bilda et al., 2006; Segers et al., 2005) without
investing much in terms of time (Rodgers et al., 2000; Stones & Cassidy, 2010) and money (McGown et al., 1998).
Expert designers are more adept at using visuals, suggesting that visuals are often a part of their professional
development (Bilda et al., 2006). These visuals also contribute significantly to the design process (Dörner, 1999; Jonson,
2005; Kavakli & Gero, 2001; Suwa & Tversky, 1997; Teng et al., 2014) and are said to be essential for conceptual
designing (Bilda et al., 2006). Designers use sketches to focus their non-verbal thinking (Rodgers et al., 2000), consider
the idea as both its component parts and as a whole (Bilda et al., 2006), and tap into the deeper meaning and
implications of their ideas (Eppler & Kernbach, 2016). Sketching enlivens previously only imagined designs (Bilda et al.,
2006; Tovey et al., 2003). Through sketching, designers can embody and explore ideas that are not fully developed
(Rodgers et al., 2000), communicate the physical nature of an idea (McGown et al., 1998), and subsequently clarify its
characteristics to determine what will and will not work (Dörner, 1999). All of these activities are critical in the product
development process.

Storyboarding

Sketch methods lead to the creation of storyboards because key ideas and images can be created and then organized
in a storyboard sequence (Teng et al., 2014). Storyboards are "a panel or series of panels on which a set of sketches is
arranged depicting consecutively the important changes of scene and action in a series of shots" (Storyboards, n.d.).

                                                                                       142
Storyboards are an exploration, analysis, and conceptualization tool generally used later in the design process once
ideas from sketches have been evaluated and selected for development.
The development of storyboards often starts with a collection of individual drawings that represent single scenes,
which are part of the whole design being drawn. Each separate depiction in the storyboard represents a specific scene
or perspective. Taken together, they represent the sequence in which things will flow.
Storyboards are utilized in cinematography, live television, animation, and special effects to plan the details of how a
story will be portrayed (Teng et al., 2014). In architecture, they are used to visualize presentations of projects by creating
analog versions of proposed buildings that will later be digitally designed (Cristiano, 2007). In other design contexts
such as industrial design, storyboarding is a way of visually recording social, environmental, and technical factors that
affect the context of how end users will interact with the product (Martin & Hanington, 2012).
Storyboards were used by students at Georgia Institute of Technology in their industrial design classes. When working
on a product development project to redesign travel luggage, students performed research about the needs of
consumers as well as market standards as a basis for beginning their design project. After completing the research,
students storyboarded their designs to show how luggage is handled through the whole travel experience from storage,
packing, passing security, walking through the airport, boarding the airplane, loading it into the overhead bins, and
ultimately back into storage. These storyboards facilitated discussions about various design features and how to
prioritize them to meet user needs (Reeder, 2005b).
As this example demonstrates, storyboards can contribute to product development because they are drawn with the
target audience in mind (Martin & Hanington, 2012) and visually describe how users will interact with the product. When
designers examine design challenges in depth using storyboards, they can understand the complexity of the situation
and consider individual portions of the situation while not losing sight of the whole (Reeder, 2005a). They can visually
document how users will interact with the product and use this documentation to develop innovative product solutions
that address the needs and expectations of users (Reeder, 2005a). In general, storyboards act as a visual budget, which
helps the production process run more smoothly by planning and allocating resources effectively (Cristiano, 2007).
Because nothing is fixed or unchangeable, storyboarding is a flexible way of trying out ideas and incorporating changes;
ideas can easily evolve as they are drawn in storyboards (Glebas, 2013), as was the case with the exhibit pictured in
Figure 2.
Figure 2
Storyboard of Ostraka Layout

                                                                                       143
Figure 3
Storyboard Example

                                                                                       144
Note. CC-BY from Rosenfeld Media, available at https://edtechbooks.org/-kzST.

Product Builds

Product builds are any three-dimensional representation of an idea that an audience and designer can manipulate and
experience. They can be as complex as working versions of a tool, 3-D prints, or even Lego/fabric-based lower fidelity
builds. They can also be of varying levels of fidelity, as initial product builds may include a few layers of the design (such
as the physical shape and visual coloring/representation). However, later prototypes can have increasing more fidelity,
including prototyping various versions of audio, music, content, and dynamic interactivity to test how effective each new
design element is.

Product builds are seen as an essential design activity because it allows designers to learn by doing as they explore
ideas (Camere & Bordegoni, 2015). This is a practice common to many fields, including experience design (Buchenau &
Suri, 2000), education (Barab & Plucker, 2002), engineering (Alley et al., 2011), social innovation (Brown & Wyatt, 2015),
and instructional design (Merrill & Wilson, 2007).

As an example, engineers at a precision pump manufacturing organization were tasked with creating a new line of
pumps for a food processing chain. The pumps needed to be more efficient and have fewer parts than the originals. The
core design team was co-located and created prototypes to test their new designs. The use of prototypes contributed to
the direct aural and visual communication team members had with each other. The prototypes were critiqued and
approved, and in this way they structured the design process for the engineers (Perry & Sanderson, 1998).

As this engineering example illustrates, product builds are a valuable communication tool. They can provide a shared,
tangible view of an idea and facilitate answering questions concretely (Yang, 2005). They can also be used to persuade
others to adopt a new mindset because they tangibly demonstrate the merit of an idea. Prototypes can be a source of
positive peer pressure to move forward with the development of ideas (Norris & Tisdale, 2013).

                                                                                       145
Product builds also reveal information about the designs through the process of fabrication. Creating prototypes
reduces design risk because designers can learn about the product-to-be without investing the time and cost required
for full production (Yang, 2005). This technique helps designers determine how to fulfill the tasks and requirements that
must be accomplished for a given project (Smith, 2014). Designers learn from the mistakes they make on prototypes
and the feedback they receive about their prototypes, which then leads to improved designs, as was the case with the
prototype pictured in Figure 4. This is an iterative process that continues until they reach a product that will accomplish
the desired results.
Figure 4
Product Build of an Early Iteration of a Museum Exhibit

Bodystorming, or Role-Playing

Bodystorming is a method in which brainstorming is made physical. During bodystorming, role-playing and simulation
with simple prototypes is done to create informative performances that illustrate what it might be like to use a product
that is under development (Martin & Hanington, 2012). Bodystorming is a way of developing greater user empathy:
designers immerse themselves in situations end users might experience and then focus on the decisions, emotional
reactions, and interactive experiences users might have. This approach is based on the premise that the best way to
understand an interaction is to experience it personally (Smith, 2014).
Participating in the interactions users might have can reduce the time designers spend studying documents of user
observation. It allows them to tap into aspects that are unobservable because they have experienced these elements
firsthand (Oulasvirta et al., 2003). This technique has the potential to help designers communicate better with their
peers, clients, and end users because of the performance aspect of this type of visual (Burns et al., 1994).
Designers at the Helsinki Institute for Information Technology enlisted 10 researchers and industry representatives to
use bodystorming to innovate ubiquitous computing technologies. They spent a full day bodystorming the interactions

                                                                                       146
an elderly user group would have at an old age service house, subway station, the subway, the mall, and a grocery store.
They identified problems related to activities performed at each of these locations and framed them as design
questions. Those involved were split into two groups to perform the bodystorming. One researcher acted as a
moderator, while another served as a group leader. These researchers recorded ideas that emerged and facilitated the
experience. They found that bodystorming inspired researchers to become familiar with new contexts and improve their
design abilities (Oulasvirta et al., 2003).
This example of bodystorming presents how this visual tool can support the product development process through
facilitating communication across peers, clients, and users. Like the other forms of visual representation, it offers a
shared perspective to all involved, which provides opportunities for further discussions (Burns et al., 1994). However, it
contributes differently than other visuals. It allows designers to experience, discuss, and evaluate their ideas in context,
and helps designers to understand how the settings in which a design is used can affect their intended use (Smith,
2014).
This approach is believed to be less error-prone than brainstorming because it allows designers to experience realistic
constraints that can affect the user experience (Smith, 2014). In bodystorming, designers rapidly prototype ideas, which
allows for immediate feedback on how the product works (Oulsavirta et al., 2003). Discussing the feedback brings up
new issues for designers to explore (Flink & Odde, 2012).

Wizard of Oz Prototypes

In the movie/book, The Wizard of Oz, Dorothy and her companions seek the wisdom and power of the Great Oz to grant
their wishes. However, what they thought was an all-powerful wizard was really a man behind the curtain, pulling levers
and pushing buttons to give the effect of something magical happening. Similarly, in Wizard of Oz prototyping, the
designer creates a low fidelity or paper prototype, but without the interactivity or dynamic responses from the system.
Instead, when a user or prototype tester wants to do something, they indicate where they would go, or what they would
click, and the designer provides the next low fidelity prototype example. In this way, they simulate the interaction that
they will eventually build into the system. In essence, as Dam and Siang (2018) explained these are "prototypes with
faked functions."
Sometimes this "faking" can be more complex, with a human on one side of a screen typing responses to the user that
appear to come from the computer. As another example, a popular experience at Disneyland theme parks is Turtle Talk
with Crush (shown in Figure 5), where children talk to Crush, the popular turtle from Finding Nemo, through a computer
screen. On the other side of the screen, the performers make Crush respond to the children in authentic ways that make
Crush seem real. This perhaps also exemplifies an ethical issue with Wizard of Oz prototyping as many young children
really do think Crush is real. Even with adults, some Wizard of Oz prototyping can appear realistic, and participants
should be informed that they are not, in reality, interacting with a real product.
Figure 5
Turtle Talk at Disney World

                                                                                       147
Note. Photo CC-BY/SA from Josh Hallet and available at https://edtechbooks.org/-SmA.

User-Driven Prototypes

Dam and Siang (2018) described one final prototyping strategy, where instead of designers creating prototypes for
users, the users create prototypes for the designers. They explained that this can be a way of understanding the users
and developing empathy. "When you ask the user to design a solution, rather than provide feedback on a prototype, you
can learn about the assumptions and desires that the user possesses. The purpose of a user-driven prototype is not to
use the solutions that the users have generated; instead, it is to use their designs to understand their thinking."

According to Dam and Siang (2018), a designer sets up user-driven prototyping by asking users to design specifically to
answer questions designers have. They provide the example of airport designers asking users to sketch or build what
they think an ideal experience would look like.

Conclusion

Prototyping is an essential strategy for testing out emerging designs and refining ideas before expensive
implementation launches. In addition, prototyping is an essential part of the design process itself because prototypes
help to structure the collaborations on a design team and represent the distributed cognition of design teams and how
ideas are negotiated by team members (Henderson, 1998). Thus, design cultures or styles are intrinsically tied to the
way in which each constructs representations of their ideas. Such prototypes--e.g. sketches, drawings, bodystorming,
etc.--are the heart of design work and constitute the space in which ideas are defined, refined, and negotiated.
(Henderson, 1998, p. 141). A team's ability to create, interpret, and communicate with prototypes can facilitate or
restrict how they interact as a group, making these prototypes "primary players in the social construction of the design
culture or design style of the designing group" (Henderson, 1998, p. 140). Thus, it is essential that designers think
deliberately about how they use prototypes as part of an effective team design culture.

                                                                                       148
References

Alley, M., Atman, C., Finelli, C., Diefes-, H., Kolmos, A., Riley, D., & Weimer, M. (2011).

Engineering education and the development of expertise. Journal of Engineering Education, 100(1), 123-150.

Barab, S., & Plucker, J. (2002). Smart people or smart contexts? Cognition, ability, and talent
         development in an age of situated approaches to knowing and learning. Educational
         Psychologist, 37(3), 165-182.

Bilda, Z., Gero, J. S., & Purcell, T. (2006). To sketch or not to sketch? That is the question.
         Design Studies, 27(5), 587-613. doi.org/10.1016/j.destud.2006.02.002

Brown, T., & Wyatt, J. (2015). Design thinking for social innovation. Stanford Social Innovation
         Review, 8(1), 30-35. doi.org/10.1017/CBO9781107415324.004

Buchenau, M., & Suri, J. F. (2000). Experience prototyping. IDEO San Fransisco. Pier 28
         Annex, The Embarcadero

Burns, C., Dishman, E., Verpiank, W., & Lassiter, B. (1994). Actors, hairdos & videotape:
         Informance design. Conference Companion, April, 119-120.

Camere, S., & Bordegoni, M. (2015). A strategy to support experience design process: The
         principle of accordance. Theoretical Issues in Ergonomics Science, 16(4), 347-365.
         doi.org/10.1080/1463922X.2015.1014069

Cash, P., StankoviÄ, T., & Storga, M. (2014). Using visual information analysis to explore
         complex patterns in the activity of designers. Design Studies, 35(1), 1-28.
         doi.org/10.1016/j.destud.2013.06.001

Cristiano, G. (2007). Storyboard design course (1st ed.). Hauppauge, NY: Barron's Educational
         Series, Inc.

Dam, R., & Siang, T. (2018). Prototyping: Learn Eight Common Methods and Best Practices.
         Interaction Design Foundation Website. https://edtechbooks.org/-SZt

Do, E. Y. L., Gross, M. D., Neiman, B., & Zimring, C. (2000). Intentions in and relations among
         design drawings. Design Studies, 21(5), 483-503. doi.org/10.1016/S0142-
         694X(00)00020-X

Dörner, D. (1999). Approaching design thinking research. Design Studies, 20(5), 407-415.
         doi.org/10.1016/S0142-694X(99)00023-X

Dorst, K., & Cross, N. (2001). Creativity in the design process: Co-evolution of problem-
         solution. Design Studies, 22(5), 425-437. doi.org/10.1016/S0142-694X(01)00009-6

Eppler, M. J., & Kernbach, S. (2016). Dynagrams: Enhancing Design Thinking Through
         Dynamic Diagrams. Design Thinking for Innovation, 85-102. doi:10.1007/978-3-319-
         26100-3_6

Flink, C., & Odde, D. J. (2012). Science + dance = bodystorming. Trends in Cell Biology, 22(12),
         613-616. doi.org/10.1016/j.tcb.2012.10.005

Gibbons, A. S. (2013). An architectural approach to instructional design. Routledge.

                                                                                       149
Glebas, F. (2013). The animator's eye. Burlington, MA: Focal Press.

Henderson, K. (1998). The role of material objects in the design process: A comparison of two
         design cultures and how they contend with automation. Science, Technology, and
         Human Values, 23(2), 139-174.

Jonson, B. (2005). Design ideation: The conceptual sketch in the digital age. Design Studies,
         26(6), 613-624. doi.org/10.1016/j.destud.2005.03.001

Kavakli, M., & Gero, J. S. (2001). Sketching as mental imagery processing. Design Studies,
         22(4), 347-364. doi.org/10.1016/S0142-694X(01)00002-3

Martin, B., & Hanington, B. (2012). Universal methods of design. Beverly, MA: Rockport
         Publishers. doi.org/1592537561

McGown, A., Green, G., & Rodgers, P. A. (1998). Visible ideas: Information patterns of
         conceptual sketch activity. Design Studies, 19(4), 431-453.
         doi.org/http://dx.doi.org.proxy.library.dmu.ac.uk/10.1016/S0142-694X(98)00013-1

Merrill, M. D., & Wilson, B. (2007). The future of instructional design: The proper study of
         instructional design. In R. A. Reiser & J. V. Dempsey (Eds.), Trends and Issues in
         Instructional Design and Technology (2nd ed., pp. 335-351). Upper Saddle River, NJ:
         Pearson Education, Inc.

Norris, L., & Tisdale, R. (2014). Creativity in museum practice. Walnut Creek, CA: Left Coast
         Press.

Oulasvirta, A., Kurvinen, E., & Kankainen, T. (2003). Understanding contexts by being there:
         Case studies in bodystorming. Personal and Ubiquitous Computing, 7(2), 125-134.
         doi.org/10.1007/s00779-003-0238-7

Perry, M., & Sanderson, D. (1998). Coordinating joint design work: The role of communication
         and artefacts. Design Studies, 19(3), 273-288.

Reeder, K. (2005a). Using storyboarding techniques to identify design opportunities. The
         Technology Teacher, April, 9-11.

Reeder, K. (2005b). Visual storyboarding provides a conceptual bridge from research to
         development. The Technology Teacher, November, 9-12.

Rodgers, P. A., Green, G., & McGown, A. (2000). Using concept sketches to track design
         progress. Design Studies, 21(5), 451-464. doi.org/10.1016/S0142-694X(00)00018-1

Segers, N. M., De Vries, B., & Achten, H. H. (2005). Do word graphs stimulate design? Design
         Studies, 26(6), 625-647. doi.org/10.1016/j.destud.2005.05.002

Smith, B. K. (2014). Bodystorming mobile learning experiences. TechTrends, 58(1), 71-76.

Stones, C., & Cassidy, T. (2010). Seeing and discovering: How do student designers reinterpret
         sketches and digital marks during graphic design ideation? Design Studies, 31(5), 439-
         460. doi.org/10.1016/j.destud.2010.05.003

Suwa, M., & Tversky, B. (1997). What do architects and students perceive in their design
         sketches? A protocol analysis. Design Studies, 18(4), 385-403.

                                                                                       150
         doi.org/http://dx.doi.org/10.1016/S0142-694X(97)00008-2
Teng, P. S., Cai, D., & Yu, T. K. (2014). The relationship between individual characteristics and

         ideation behavior: An empirical study of storyboards. International Journal of Technology
         and Design Education, 24, 459-471. doi.org/10.1007/s10798-014-9264-1
Tovey, M., Porter, S., & Newman, R. (2003). Sketching, concept development and automotive
         design. Design Studies, 24(2), 135-153. doi.org/10.1016/S0142-694X(02)00035-2
Yang, M. C. (2005). A study of prototypes, design activity, and design outcome. Design Studies,
         26(6), 649-669. doi.org/10.1016/j.destud.2005.04.005

                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/Prototyping_strategies.

                                                                                       151
152
Evaluating

Evaluating practices help instructional designers judge the contribution their work is offering. Are they encouraging
positive change? Is their work effective? What opportunities exist for improvement? Evaluating includes both public
activities such as a formal evaluation procedure and personal activities like reflection based on one's own judgment.

     Design Critique
     The Role of Design Judgment and Reflection in Instructional Design
     Instructional Design Evaluation
     Continuous Improvement of Instructional Materials

                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/evaluating.

                                                                                       153
154
  12

Design Critique

Brad Hokanson

Design  Critique  Formative  Discussion  Active Learning

Central to design and design education is the critique (Dannels, 2005; Gray, 2013). The methodology and practice of
critique is how designs are improved and how design skills are developed in workplaces and within studio education
around the world. It is where work is presented by a designer, criticized by others, its virtues and limitations debated,
and the work improved.

By itself, designing is a challenge to any individual's abilities. Information must be gathered and analyzed and a guiding
principle or idea must be developed and communicated to others. Designers must expose their work to the criticism of
others and answer critiques with the quality of their arguments and improvement in the design. Critique looks at an idea
--created through analysis and an inventive process, which is shared by the learner/designer--and advances its quality.

The design critique can provide instructional design with a means for intensifying the learning process as well as
improving the design project itself. As a process, critique benefits the learner, other members of a class, and the critic.

Defining Critique

Used throughout the design and creative fields, "critique" is a formative, conversational method of interaction and
assessment. It is the systematic and objective examination of an idea, phenomenon, or artifact. Critique is a semi-
structured method of sharing work for evaluation and commentary by others; it is a discussion with a project focus.
While there are a number of different forms and terms for the process, critique is used here to refer to formal and
informal discussions involving design disclosure and criticism.

This writing focuses on the formative aspects of critique, and does not address final critiques or formal reviews,
processes meant to conclude and evaluate a design project (see Figure 4). For less formal and individual scaled
interactions, the terms "crit" and "desk crit" are commonly used. Here the focus is on critiques which happen during the
design process.

Design critique can be compared with user testing. Both allow the evaluation of design projects and provide important
feedback to the designer. Here the focus is on critiques which happen during the design process. In contrast, in user
testing, most of the understanding of the quality of design work comes from observation of appropriate test users.
Comments from the test users can be helpful, but often are limited by their experience with design or the project at
hand. On the other hand, critique generally deals with peers or mentors with experience in designs of this project type.

In the first major studies of these interactions in the design studio, Donald Schön (1983, 1985, 1987) directly observed
architectural education. His writing described the individual consultations between studio instructors and individual

                                                                                       155
students. The intensity and focus of this type of learning event is the essence of an effective studio education, it is not
didactic. At its most positive, a critique is meant to "coach" or "guide" the learner to a more effective answer, develop
judgment, and model tacit design/problem setting and solving skills. Per Schön, "The student cannot be taught what he
needs to know, but he can be coached" (1987, p. 17).

Forms of Critique

There are a number of different structures for critique. Blythman et al. (2007) describe a variety of critique forms which
range from final reviews to industry presentations to individual critiques. In this writing three of these forms are
described as central to design and education: desk critiques, peer crits, and group critiques. Each of these types is
formative, designed to encourage and direct design progress, and are qualitatively the most effective.
Within a studio learning experience, the development of design skills is commonly sought through a form of informal
critique or desk crit (see Figure 1). A desk crit is "... an extended and loosely structured interaction between designer
and critic (expert or peer) involving discussion and collaborative work on a design in progress" (Shaffer, 2003, p. 5). In
general, most of the activity during scheduled class time in a design studio will be individual students receiving criticism
of their work from instructors or visitors.
Figure 1
Desk Critique (illustration by the author, photo courtesy of University of Minnesota College of Design)

                                                                                       156
"During a crit, a student describes his or her work to the professor...As students present possible solutions, the
professor explores the implications of various design choices, suggesting alternative possibilities, or offering ways for

                                                                                       157
the student to proceed in his or her exploration of the problem" (Shaffer, 2000, pp. 251-252).
The desk crit is a personal conversation between a designer and a critic (who may be a visiting professional, expert, or
professor). The length varies with the discussion. "This model of social interaction between student and instructor
involves a critical conversation about the student's design, and usually involves both people working towards solving a
problem" (Conanan et al., 1997, p. 2). It is inherently formative, guiding the work toward a more successful conclusion. It
is also subjective, and when successful, provides not only objective answers but directions focused on developing the
designer's ideas and thought process.
An important concept in effective critiques is the focus of the criticism on the work itself and not on the designer. A
positive, formative atmosphere is essential to an effective critique; grading and evaluation occur elsewhere. Shaffer
described this nature:
"The tone of desk crits was almost always supportive and nonjudgmental. On the other hand, pinups and reviews,
although constructive, were quite blunt and sometimes extremely critical--particularly in the case of formal reviews.
Judgment was, in effect, off-loaded from the more private desk crits to the more public presentations" (Shaffer, 2003, p.
2).
Non-participants also can benefit from the individual desk critique both through direct observation and through
incidental listening to the process. While not as formalized as a lecture, within a studio space, frequently there are
informal observers who gain from hearing another's desk critique.
Talking to two student designers at a time may be more effective as it allows designs to be compared and more
designers to be critiqued in a given time period. It does, however, lack the focus and attention found through the
individual critique.
While access to instructors is limited, other members of a class or team are available at any time to provide opinions,
clarifications, and evaluations through a peer critique, inside or outside of formal meeting hours. This is the simplest
form of critique in design, the "peer crit", where design work and ideas are discussed between colleagues (see Figure 2).
Figure 2
Peer Critique (illustration by the author, photo courtesy of University of Minnesota College of Design)

                                                                                       158
Any critique develops both the critic and designer. While they can provide an external review of one's design decisions,
peer crits also provide the critic with the opportunity to extend their own skills. Peer critics review the validity and logic

                                                                                       159
of a particular design idea or set of design choices. While peer crits may be the least formal format, they are the basis
for an extended professional understanding of the use of critique. This practice occurs in a range of fields from graphic
design to architecture to user-experience design.
An individual working session with a single student can change learners' minds and their thinking process, providing, as
Shaffer describes it involves social scaffolding of learning the design process. At its core, critique as part of an
educational experience is constructivist. While the focus is on an external project, the overall goal of the critique is to
develop the designs skills of the learner.
"He [sic] has to see on his own behalf ... Nobody else can see for him, and he can't see just by being "told," although the
right kind of telling may guide his seeing and thus help him see what he needs to see." (Dewey 1974, p. 151)
The importance of the informal critique in the development of learners in the studio classroom is clear. Frequent
engagement and discussion of ideas scaffold the experience, while the designer tacitly recognizes the value of
engagement and collaboration with other professionals by seeking criticism from others.
Designers who participate in critique may do so as a critic or as one being critiqued. Both roles have cognitive benefits
to the individual designer and to their broader understanding of design. Designs are developed conversationally,
building from the initial ideas of the designer, but tested and improved through the argumentation like process of a
critique. Criticism of the work can help improve the quality of the end-product. Over time, exposure to critique can also
help develop thinking skills of the designer building their capability to analyze, anticipate, and respond. For a beginning
designer, a first critique may be challenging and helpful comments may be rejected. Often the criticism of the work is
conflated with criticism of the designer themselves, when they should be separated. Discussions must focus on the
work, and not on the designer.
Small groups can also observe and participate in formative group critique as well, with selected projects serving to
trigger discussion and interaction with all present. In studio format learning, intermediate group critiques can have
much of the same coaching or generative functions as individual critiques. Whether as a group crit or pin-up, these can
highlight specific milestones in a project development. While similar in form to final reviews or "juries", the
distinguishing quality is one of development and advancing the work of the individual designer and benefiting the group
from generalizable comments. It is inherently formative and positive.
Figure 3
Group Critique (illustration by the author, photo courtesy of University of Minnesota College of Design)

                                                                                       160
A general, but often tacit goal of design education is to instill a habit of critique, and an ongoing practice of generative
evaluation of creative work. Critique supports reflection and engagement among designers of all types.

                                                                                       161
Figure 4
Final Review (illustration by the author, photo courtesy of University of Minnesota College of Design)

                                                                                       162
163
Use of Critique in Studio

Studio-based education is learning by designing, a rich and complex process. Designers in all fields examine problems,
advance possible exploratory resolutions, and iteratively evaluate their own work as a regular part of the design
process. This process occurs through personal reflection and evaluation, but it can also be improved through the
interaction with others through as Shaffer calls "...a variety of structured conversations..." (2003, p. 5). An important
aspect of learning design is developing the professional practice of seeking and giving critique; the formal and informal
evaluation of the work. It is one of the consistent aspects across design programs and schools worldwide, and
importantly, in design culture. As a generative format, the critique process focuses on the improvement and
development of the design project.

The use of the studio model in instructional design has become increasingly common over the past ten years (Clinton
and Rieber, 2010). Studios are based on the ideas of project-based learning and modeled directly from pedagogical
methods in the creative fields such as studio art, architecture, and product design. "The originators of the studio
curriculum [at the University of Georgia] ... envisioned the learning of educational multimedia design to that of an art or
architectural studio in which a group of people learn skills and develop expertise while working on authentic projects in
a public space comprised of tools and work areas" (Clinton and Rieber, 2010).

Application in Instructional Design

Instructional design education can benefit from the models presented in studio-format classes. Instructional designers
also can utilize the general concept of critique in various ways in design products of their own. However, not everyone is
experienced with critique or even studio-based learning in an educational environment. Design schools have the
advantage of a well-developed and expected critique model; the scaffolding is explicit and the instructors are well
versed in the process.

It is valuable to start using and employing critique as a method as a learner, as an instructor, and as an instructional
designer. The suggestions below intersperse these roles, describing critique from these three different orientations.

Designers, even those without experience in studio-based learning, can start by opening themselves to critique as an
educational method. Beginning can be as simple as developing a habit of asking peers or friends for informal feedback
on a project. The author's own second year architecture critic began the year by saying "You have to expose yourself.",
encouraging our own sharing and interaction regarding design ideas. (Stageberg, 1973).

Peer critiques can be done at any time, whether during scheduled class time or at off-hours, exposing project ideas to
others' opinions and assessments. Critique can also be done between designers, developing their skills of synthesis
and evaluation, and expands the learning process...and importantly as a way to improve the design work itself. [An
application exercise is included at the conclusion of this writing.]

Designers seeking input on their work can begin by specifically focusing the critique on areas for improvement. A peer
critique should start with briefly describing the problem or design and outlining the objectives of the project. Present is
an understanding of the immediate goal of the critique being improvement of the design solution (Gibbons, 2016). As
with writing, the goal is to seek a larger understanding of the logic and tone as opposed to a copy edit.

While a critique is in progress, designers can help steer the direction of discussion to more important issues by
focusing on discourse within the design work, and by seeking evidence and the reasoning behind any criticisms.

Critiquing a colleague, peer, or student helps in developing one's own reflective ability to analyze and criticize design
work. Critiquing the work of others can help make one a better designer in the long term, and improve design projects in
the present.

                                                                                       164
While giving a critique involves evaluating the work for errors and problems, it can also delve into the more
philosophical and theoretical aspects of the project. For example, an instructional design could begin from a behavioral
basis or a constructivist basis, which is a place for philosophical advocacy.

For instructors, individual critiques can be described as a regular system of tutoring individual learners, driven by
attention and engagement. Critique is contemporary and formative feedback, engaging, and scaffolds the design
process. The skills of critique should be consciously developed in learners both as recipients and for their role as critic.
The critique model is extendable, as individuals can be paired or grouped as need be, building collaborative learning
events. A formative critique is comparable to reviewing a written article draft for a colleague, building on their ideas and
their thinking.

Critics or instructors themselves will need to begin by modeling a positive and formative approach to a constructive
conversation. Faculty will need to have a consistent pattern of using critique for helping learners develop their ideas as
well as their thinking process. Explicit standards for both the interaction and the quality of the work are helpful.
Individual "desk" crits can be either private or public, and faculty can encourage other students to informally listen in. As
individual critiques can be face-to-face or online, they can continue to allow others to participate or view. Establishing
individual critiques as an educational practice in a course can build to conducting small group critiques as well.

Instructional designers have the opportunity to build into their designs open frameworks for critique. A framework can,
for example, support student peer critiques, user testing of interactive designs, verbal critique of visual layouts, or a
shared review of a colleague's writing. In most cases it would be important to develop critique skills in learners to help
improve responses. The goal of any particular critique is progress toward improvement of the finished design, with the
overarching goal of improving learner thinking. It is valuable for a learner or critic to review over all ideas and evaluate
their validity and consistency, and to be present, positive, and engaged. Critique is a structure that can be built into
instructional designs.

While critique is valuable for both face-to-face and online learning, there are challenges that exist with the increasing
use of technology-enhanced learning. The fluidity of conversation, whether online or in-person adds much to a critique,
even if done through sharing screens and talking synchronously, which is now possible with some course management
systems. Critiques should be done in a manner providing the highest fidelity of communication possible; while face-to-
face is valuable, most synchronous critique can be done through video conference software. A current example would
be online music lessons connecting, say, a violin player in Japan with an instructor in Finland (Furui et.al., 2015;
Nishimura, 2017).

Asynchronous critique may be less effective, but can still provide direction and formative assessment through mark-up
and annotation. Unfortunately, there isn't the same interaction with a "Track Changes" review or with software such as
VoiceThread as with a face-to-face conversation, but with investigation, structuring of the conversation could be
improved. Online written texts can be combined with synchronous audio for editing sessions as well.

Conclusion

As with any educational practice, there are limits to the use of critique in education. More commonly, the limits on the
use of critique are due to time and the one-on-one nature of an instructor critique. Modern economics necessarily
constrains the amount of time spent reviewing, analyzing, and being involved with individual critiques. Lecture classes
and objective evaluations are simpler and much more financially viable in 'presenting' a large class than is a single
design instructor working with individuals in a smaller studio class. This is a continuing source of pressure on design
departments. Pragmatically, class size and time limit the availability of critique as an educational method.

Critiques do vary in quality as well as scale. Some critiques are helpful and advance the work, others challenge the
designer's thoughts, leading to new insight for future work. Other critiques, of course, are less successful, perhaps
focusing on the traits of the designer and not on the design itself. Critiques which focus primarily on minor details, facts
or factual error are often distracted from larger, more important issues. Critiques which are simplistic and present

                                                                                       165
criticism without evidence are not helpful, nor are those which are overwhelmingly negative or positive. The goal of a
good critique is to make the design and designer better, and not to express a power relationship.

The skills of the reviewer, whether educator or peer, are also important--recognizing the social and formative nature of
the interaction. However, it is within the systemic role of instructional designers to extend a valued and effective model
to the technology-enhanced learning of today.

Critique can be integrated into instructional design models and education. It can be the way instructional designers
learn, and an important aspect of how they practice.

   Application Exercises

     As a concluding exercise for this writing, try the following process. At some point in a design project, whether
     with early sketches or more developed ideas, contact a peer who is working at a comparable scale. It might be
     the same type of project or one that has similar requirements and standards. Ask if they would be willing to
     critique your work, and offer the same input on theirs. Review the following process and set a reasonable time
     scale for the critique, with enough for discussion of both efforts.

     For the critique itself, first give your colleague a brief outline of the current progress of your work and focus the
     critique on areas of concern you may have. Solicit a comparable set of information from your partner. Spend a
     reasonable amount of time examining the project, depending on the scope of the project and on your agreed
     upon time commitment. Take notes, and try to synthesize your understanding and experience with their work.
     With a goal of seeking to improve the work, discuss your findings with them, and in turn, learn of their findings.
     Restate what you heard in your own words to them for confirmation and clarification.

References

Blythman, M., Orr, S., & Blair, B. (2007). Critiquing the crit. Retrieved March 19, 2010, from https://edtechbooks.org/-tsL

Clinton, G., & Rieber, L. (2010). The studio experience at the University of Georgia: An example of constructionist
         learning for adults. Educational Technology Research & Development.

Conanan, D., & Pinkard, N. (2000). Studio zone: Computer support for reflective design. In B. Fishman & S. O'Connor-
         Divelbiss (Eds.), Fourth international conference of the learning sciences (pp. 176-177). Mahwah: Erlbaum.

Dannels, D. (2005). Performing Tribal Rituals: A Genre Analysis of ``Crits'' in Design Studios, Communication Education.
         54(2). P. 136-160.

Dewey, J. (1974). John Dewey on Education: Selected Writings. (R. D. Archambault, ed.) Chicago: University of Chicago
         Press.

Furui, H., Nishimura, A., Nakai, N., & Nishimua, S. (2015). Feasibility study on international violin lesson using video call.
         Waseda University research report.

Gibbons, S. (2016). Design critiques: Encourage a positive culture to improve products. [weblog]
         https://www.nngroup.com/articles/design-critiques/

Gray, C. M. (2013). Informal peer critique and the negotiation of habitus in a design studio. Art, Design &
         Communication in Higher Education, 12(2), 195-209.

                                                                                       166
Nishimura, S. (2017). e-mail communication.
Schön, D. A. (1983). The reflective practitioner: How professionals think in action. Burlington: Ashgrove.
Schön, D. (1985). The design studio: An exploration of its traditions and potentials. London: RIBA.
Schön, D. (1987). Educating the reflective practitioner. San Francisco: Jossey-Bass.
Shaffer, D. W. (2000). Design, collaboration, and computation: The design studio as a model for computer-supported

         collaboration in mathematics. In CSCL `97 proceedings (pp. 249-255). Toronto: Ontario Institute for Studies in
         Education.
Shaffer, D. W. (2003). Portrait of the Oxford studio: An ethnography of design pedagogy. Madison: Wisconsin Center for
         Education Research.
Stageberg, J. (1973). Personal conversation.

                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/design_critique.

                                                                                       167
168
  13

The Role of Design Judgment and Reflection in
Instructional Design

Ahmed Lachheb & Elizabeth Boling

Design  Instructional Design  Reflection  Design judgments

This chapter discusses the importance of design judgment in instructional design. It defines design judgment
and distinguishes it from decision-making. The chapter also discusses the different types of design judgments
that instructional designers make, and the factors that influence these judgments. Finally, the chapter provides
guidance on how instructional designers can develop the skill to evoke strong design judgments.

Introduction

As a student of instructional design (ID), or as a future ID practitioner, you will have to make many decisions that allow
you to move forward in your design work. Such decisions will be informed in part by the particular situation you are
dealing with, influenced by precedent design experience (see more about design precedent here), and inflected with
your values and ideals. Making decisions is a fundamental human capacity. When designing, designers make decisions
specific to design. The capacity to make solid design decisions is what distinguishes excellent designers from
mediocre designers. So how can designers make solid decisions? The answer is through evoking good design
judgments and constantly reflecting on their design work.

What is Design Judgment?

When designers face complex situations--a constraint, a problem with a client or another design stakeholder, a block in
the design process--they need to make design judgments to reduce the complexity of the situation or solve the issue
that has been encountered. Depending on the situation, a specific design judgment is invoked to make design
decisions. In the area of general design theory, Nelson and Stolterman (2012) have identified design judgment as
"essential to design. It does not replicate decision making but it is necessary" (p. 139). In this definition, the authors
distinguish judgment from decision-making. Design judgments for them are the means to achieve "wise action" (p. 139),
or--in other words--good design decisions. In this way, designers can think about design decisions as the "what and
how," of design, whereas design judgments have to do with the "why" a design decision has been made (see Figure 1).

Figure 1

                                                                                       169
How Are Design Judgments Related to Design Decisions?

Design judgments rely on different types of logic than rule-based systems. For example, imagine you have been tasked
to design an instructional module to be delivered online. You are now faced with the choice of making this instructional
module in the form of video, or text, or text with visuals and a video. Eventually, you will make a decision about which
form the instructional module will take. That is the design decision. What allows you to make such a decision are your
design judgments (e.g. preference for videos over text, based on constraints you perceive in the design project or your
past history as a designer). Design judgments are based on your own knowledge. This knowledge cannot be separated
from you, the designer, but it is not arbitrary either. Your appreciation of media and understanding of time and tools are
disciplined, based on intuitive but very rational logic, generated from "the particularity or the uniqueness of a situation"
(Nelson and Stolterman, 2012, p. 141).

Designers' Design Judgments

Nelson and Stolterman (2012) have proposed a construct comprising eleven design judgments that designers invoke. A
summary of these design judgments is available in Table 1. We, as authors, recommend that you read more in-depth
about these design judgments in Nelson and Stolterman's (2012) book, The Design Way.

Table 1

General Summary of Design Judgments and Examples

Design    Definition                                   Example of Design Judgment in Action
Judgment

Core      Designer's own value or thinking             Designer advocates and insists on a designing discussions
          that can lead to invoke all other            activity because they firmly believe that learning is
          above design judgments                       interaction

                                                       170
Instrumental   Selecting and using design            Drawing icons using a digital tool or using a paper and
               tools/means to reach established      pencil, or selecting to use a MAC vs. a PC for design work
               design goals

Framing        Defining the boundaries of the        Deciding whether to design an academic course, a
               design project by emphasizing its     workshop, a performance-support handout, etc.
               focus and outcomes

Default        Generating "automatic" response to    Asking an SME to meet for a design project kick-off meeting
               a situation without hesitation, and   because that is the first thing you do in all of your design
               without too much thinking             projects, no matter what

Deliberated    Recall of previous successful         Emailing an SME about a first meeting and providing them
Offhand (DOH)  default judgments, consciously        with options of when to meet and whether the meeting is
                                                     face-to-face or online

Appreciative   Emphasizing certain aspects of a      Appreciating the work a media developer has done but not
               design, and backgrounding others      emphasizing the challenging relationship they had with
                                                     other project's stakeholders

Quality        Finding out the match/mismatch        Discussing the quality of a slide deck presentation with a
               between aesthetic                     critical eye and through referencing branding guidelines of
               norms/standards and the particular    the organization and/or aesthetic design norms in regards
               proposed design artifacts             to colors, visuals, and typeface, such as CARP principles

Appearance     Assessing the overall quality of the  Examining the overall path of a learning experience in a
               design                                course design, and stating whether it feels cumbersome,
                                                     boring, clunky or smooth/friendly

Navigational   Considering a path/direction to       Consider inviting an external SME to provide expertise about
               follow in completing a design task    a specific subject area that the current/available SMEs lack
                                                     so you can fill a content gap that other SMEs and designers
                                                     identified

Connective     Making connections of objects         Considering how a design of a lecture in an academic
               together for the specific design      course is related to another learning activity/assessment,
               situation                             and whether there is a connection and or alignment
                                                     between these two design objects or not

Compositional  Bringing all elements of design       Considering how to place learning objects within structures
               together to form a whole              of modules to form a whole, complete, and smooth 16-
                                                     weeks long academic course, in a way that lectures and
                                                     discussions precede exams and major assignments

As a student of instructional design, there are the three of these design judgments that play a critical role in

                                                                                       171
instructional design practice, and that will be examined more completely.
The first is core judgment--"buried deep within each individual, but unlike off-hand judgments, they are not easy to
access" (p. 154). Designers invoke core judgments often in an unconscious manner because it stems from designers'
own values or thinking that can be revealed through "why" questions (e.g., a designer advocates and insists on
designing discussion activities because they firmly believe that learning is interaction--that is their core judgment). Core
judgment is behind every other design judgment. It is, in a sense, our human capacity to have tacit knowledge, beliefs,
and own philosophies. Designers invoke core design judgment to make design decisions, including those based on prior
experience.
The second is instrumental judgment--"interaction with their [designers'] materials and the tools" (p. 152). Instrumental
is used here to mean `instrument' and not to mean `important.' Designers invoke instrumental judgments to decide on
which design tools to use or not, and how to use them for their design projects (e.g., drawing icons using a digital tool
or using a paper and pencil). This judgment is one of the most invoked judgments as it is concerned with design tools--
all kinds of means that designers use to design, regardless of their form--and because design tools encompass almost
every design activity. Design tools could be abstract/theoretical or tangible, analog, or digital. If you are curious, you can
read more about design tools in instructional design practice in Lachheb and Boling (2018).
The third is framing judgment--"defining and embracing the space of potential design outcomes ... [it] forms the limits
that delineate the conceptual container" (Nelson & Stolterman, 2012, p. 148). In evoking this design judgment, designers
discuss the goal of their design project (e.g., designing an academic course, a workshop, a performance-support
handout, etc.) in order to frame what the design project is about. Framing judgment is also invoked throughout the
progress of a project that involves deciding what is important to focus on next.

Guidance to Develop and Invoke Design Judgments

You might be asking now, "What design judgments should I make? Which ones are the best design judgments? How will
I know? How does a designer learn to make good design judgments?" These are very legitimate and important
questions. Frankly, these questions are what actually spark many research studies on design practice; answering them
is not as straightforward as one sometimes wishes.
First, it is important to think about these design judgments as not isolated units, but rather like pearls that are
connected to each other with strings. If you take one pearl and you hold it up, then the other ones just hang as a cluster
underneath because they are connected to each other (E. Stolterman, personal communication, November 18, 2013).
Designers often invoke a number of design judgments together--always interconnected and often overlapping (see
Figure 2). That being said, as you are practicing design, you will be making these design judgments at all times, most of
the time unconsciously. Now that you read about them, you can think about them in a conscious manner and watch for
when a design judgment you invoke does not lead to the desired result.
Figure 2
How Are Design Judgments Interconnected?

                                                                                       172
Second, all of these design judgments are important to help you navigate the complexities of your design projects.
However, as mentioned earlier, the most important design judgments are core, instrumental, and framing. Core design
judgment is connected to every design decision--there is always a personal belief behind every design decision you
make. Instrumental design judgments are concerned with design tools--every aspect of your design projects involve
using design tools of all kinds. Framing design judgment allows you to set up the whole design project for success or
failure, from the very beginning--if you frame your design project incorrectly, there will be money and resources wasted,
not to mention upset clients and supervisors. Third, knowing which design judgments are you evoking, what design
judgment you should or you should not evoke, and how you learn to make good design judgments are always matters of
deliberate reflection on your design practice--a topic addressed in the second part of this chapter.

Finally, as a designer, you will face many situations when you feel uncertain; you can make several design judgments
but you are not sure what is the right choice to make. Uncertainty is a hallmark of the design profession and our advice
is to embrace it, not to be afraid of it. We, the authors, also advise you to trust your instinct and remember your rigorous
design training. Additionally, taking time to think and studying your design context should equip you with powerful
insights to help you make the right choice (e.g., using the instructional theory framework to inform your design
judgments). You can also seek mentorship and consulting from senior designers to help you deal with uncertainty, and
ultimately make good design judgments.

Examples of Design Judgments Invoked by Instructional Design Students

Some researchers in the field have studied design judgments and how students of instructional designers invoke them
(Demiral-Uzan, 2015; 2017, Korkmaz & Boling, 2014). From the studies of Demiral-Uzan (2015; 2017), the
authors provide the following examples of design judgments invoked by ID students as they are designing instruction
during graduate-level instructional design courses.

Example 1: An instructional design student was asked to design an instruction for their final project. This student
decided to design a course for advanced Chinese ESL learners about business emails. This student invoked a framing
and navigational design judgment because when asked by the researcher how they decided to design this instruction,
they said: "How I came up with that specific topic and why is that, very simple. I mean, what's the most practical, more
effective easy to me, something that I am at least familiar with, something I already know" (adapted from Demiral-Uzan,
2017).

                                                                                       173
Example 2: A group of instructional design students came together to discuss their group project in their instructional
design course. Their discussion was focused on the content of the instruction and the flow of information they wish to
present in the instruction. A researcher observed and recorded their interaction. Each statement that a student says
points to design judgments being invoked and overlapped:

Student A: Should this go first or after the overview? (Appearance, Quality and
Connective design judgment)

Student B: This comes after the content on my part. (Default, Appearance, Quality and Connective design judgment)

Student A: Your approach is different than mine, which is okay. For learners to understand what networking is, the
definition of networking will come here first, then tell and show them what is networking is not. (Appreciative, Core,
Quality and Compositional design judgment)
(adapted from Demiral-Uzan, 2015).

What is Design Reflection?

Reflection is the personal and the internal building of knowledge through considering and interpreting one's experiences
or beliefs (Tracey et al., 2014). It is usually a method to solve problems, as well as to define and refine one's beliefs,
values, and perspectives. Reflecting on your design work, its qualities, process, and outcomes, allows you to become
aware of your tacit knowledge and learn from your design experience. Donald Schön--a prominent design scholar and a
design educator--has identified two types of design reflections (Schön, 1983): (1) Reflection-in-Action; and (2)
Reflection-on-Action.

Reflection-in-action is that internal dialogue that designers have as they are engaged in solving a particular design
problem, or while using a specific design tool. For example, you could be working on designing a training program. You
face a complex situation where the capacity of the software you are using to develop the training materials is not
allowing you to create a specific interaction you wish to create. You could ask yourself something like "How do I get
around this? Should I use another software or try to think of another way to create the interaction the client and I want
to see?" This self-questioning is essentially the internal dialogue you could have, which constitutes "reflection-in-action."
You could have this internal dialogue without being aware of it, as most of us think and reflect unconsciously and in
silence. Eventually, reflection-in-action allows you to establish a ground to make decisions and work toward a resolution
of the problem (Schön, 1983).

Reflection-on-action has to do with looking back at past design experiences, to make sense of what happened, what
worked well, what did not work well, why taking one design approach seemed to be better than the other, etc. Many
designers recognize reflection-on-action as what happens in design "post-mortem" meetings--a dialogue between
designers who reflect upon their experiences, practices, and beliefs. Reflection-on-action dwells upon subjective
interpretations of events, situations, and ideas. It is personal and can be hard to express. Nevertheless, this type of
reflection is proven to be an effective practice to learn from past designer experience, so future design experiences are
optimized. Additionally, when designers experience failure, only reflection-on-action could allow them to process that
failure, learn from it, and essentially become aware of future modes of failure that might come their way.

An Example of a Design Reflection by an Instructional Design Student

Kaminski et al. (2018) has illustrated several design reflections written by ID students. One example is shared in this
chapter, and you are encouraged to read more--as shared by Kaminski et al. (2018).

"One of the hardest lessons I learned from the instructional design course (and still struggle with), is articulating my
decisions and actions onto paper. The best advice I received from Dr. Kaminski is to approach instructional design with
the mindset that you are making something that another instructor (without any prior experience) can recreate. I think
my difficulty comes from the many steps that I personally revisit and parts that I revise with research and experience. It

                                                                                       174
is hard to describe all those directions that my mind takes to come to a final product. The picture I drew allows me to
provide an abstract visual of all those steps. The student is the 'key,' the center of my purpose. As the 'doorknob, it is my
responsibility to make sure all the working parts are in place so that the individual can open the door to knowledge.
Begin by identifying the goals of the training event, analyze the learner, and the method for instruction, and verify the
performance objectives. Start on outer edges and spiral toward the middle, and then through evaluation back out and
spiral back in again until you get through the door. Goals are set - look at the learner and environment to make it work.
Facilitators need to address all the pieces and parts of the classroom component, so the facilitator ensures the student
has what they need to open the door."

As you can see in this example, the student expressed how hard it was for them to make design decisions. The student
reflected on what they believe to be the appropriate design moves, how to begin, and what the design should be
focused on. Toward the end of the reflection, the student expressed a set of values and ideals--core design judgments.

Design Judgments and Reflection: Recommendations for
Instructional Design Students

The Reflection Journal

Some designers keep a journal. Some designers turn parts of their journal into a blog or a website where they reflect on
their design work publicly. Commit to a journal in the format you prefer-a simple notebook, a Google document, a blog,
a video, a podcast. Start your first entry about the last design project you completed in class and address the following
prompt (adapted from Tracey et al, 2014):

Describe a time when you felt totally uncertain while working on this design project. Try to remember how you felt and
what was the greatest challenge(s) you faced because of the uncertainty you felt. What actions did you take to
overcome such uncertainty? How did it go? Why did you take certain actions and not other actions? What did you
believe to be happening vs. what actually happened? Knowing that you will feel uncertain in future design projects, how
do you feel about becoming a designer?

Start writing the reflection post using descriptive language. You should not worry about grammar and typos at this
stage. Let the words and thoughts flow and make their way from your head to the journal. Pay attention to how you felt
and what thoughts you had at the moment. Think about if someone reads this reflection, will they understand what was
going on in the design project? Will they get to feel how you felt? Don't limit yourself to formal writing. Write as you think
and speak. Once you complete this first entry, share it with your ID faculty or another designer if you feel comfortable.

The following are some ways to document design reflections:

      Pick what you want to focus on for each reflection--a challenge with a designer, a moment of design failure, a
      harsh critique from a client, or an SME; you pick.
      Describe your design actions by addressing the Five Ws (What, When, Where, Why, Who).
      Elaborate on the "Why" part so you can reveal the design judgments you made that led to these design actions.
      Speculate on the "Why" when speaking about other's actions, unless you are certain.
      Conclude with what you have learned from this design project--what you will not forget to do next time? To what
      extent this design project will be similar to future projects you anticipate?

Exploring Your Core Design Judgments

People are surrounded by designs they use every day, and some they cannot live without. Commit to a week of noticing
and collecting--through photographs--designs that you appreciate and designs that you do not like at all. These could
be the items you use every day, such as your phone, the showerhead in your bathroom, a specific app, or a favorite
frying pan in your kitchen, or anything else. Challenge yourself to notice as many designs as possible. Such design
could include instruction or performance support materials around you (e.g., a flyer that teaches people how to wash

                                                                                       175
their hands or the instructional book that comes with IKEA furniture). Take about an hour or more to write down notes
about each design--why you appreciate it and why you do not. Keep asking yourself "why do I like/dislike this?" and
record your answers. Repeat this activity until you cannot think of any more answers to. For example, the authors
appreciate public libraries. We like them because we find the books we like to borrow and not buy, they are accessible to
us, they are free, they are diverse, and they provide quiet places for us to concentrate. We can say more why we like
public libraries, but essentially we like public libraries because we believe in the noble cause of public goods, and public
libraries represent such a cause.

Now examine your answers to the "why" questions and try to think about how such answers represent values you hold.
These could be transparency, ease of use, elegance, democratic, accessible, inclusive/exclusive, soft, strong, and so on.
These values constitute your core judgments and influence all kinds of design judgments you make. You may not be
able to access all of them completely, but you are aiming to heighten your awareness of what your design values really
are. Once you have spent some time on this exercise, consider revisiting it in the future to see any change you might
notice in terms of the values you have--write a reflection post on such change. You can focus the noticing experience
on specific types of designs (e.g., phone apps), or you can mix designs together that you see belong to each other (e.g.,
instructional posters and cooking books). Essentially, noticing designs and why you appreciate them will become a
somewhat automatic habit for you. By these means you can question and refine your judgement across a whole career.

   Additional Information

     Ways to document design judgments and decisions:

           Document your design through documents and project management tools--you will have an audit trail at
           the end of the project that helps you or anyone else to trace back what design decisions you have made.
           Use the margins of such documents to add comments/thoughts and explanations on design decisions you
           have made.
           Archive written conversations (emails, chats, etc....) between you and other design stakeholders that
           include design decisions (and most likely your "defense" or such decisions).
           Leverage your design reflections, notes, and design documents to write a design a case which you can
           publish in the International Journal of Designs for Learning (IJDL).

Conclusion

You may have heard the common wisdom that to become a better professional, you should engage in at least 10,000
hours of practice in your profession. While there is truth in the advice that many, many hours of practice are required to
develop expertise, the authors are also confident in an additional claim that there is more to expertise than just putting
in a certain number of hours. Without deliberately reflecting on your design actions and the design judgments that lead
to those actions, not even 10,000 hours of instructional design practice will be enough to make you an expert designer.
Explicitly reflecting on your design judgments, in addition to reflecting on your practice, is what will help you become a
more engaged and expert designer. Make reflection on your design judgments an intentional aspect of your efforts to
develop your growing competence as a member of the instructional design profession.

References and Suggested Readings

Boling, E., Alangari, H., Hajdu, I. M., Guo, M., Gyabak, K., Khlaif, Z., Kizilboga, R., Tomita, K., Alsaif, M., Lachheb, A., Bae, H.,
         Ergulec, F., Zhu, M., Basdogan, M., Buggs, C., Sari., R., & Techawitthayachinda, R. I. (2017). Core judgments of

                                                                                       176
        instructional designers in practice. Performance Improvement Quarterly, 30(3), 199-219.
         https://doi.org/10.1002/piq.21250

Boling, E., & Gray, C. M. (2015). Designerly tools, sketching, and instructional designers and the guarantors of design. In
         B. Hokanson, G. Clinton, & M. Tracey (Eds.), The design of learning experience: Creating the future of educational
         technology (pp. 109-126). Springer. https://doi.org/10.1007/978-3-319-16504-2

Cross, N. (2001). Designerly ways of knowing: Design discipline versus design science. Design Issues, 17(3), 49-55.
         https://doi.org/10.1162/074793601750357196

Dabbagh, N. & Blijd, C. W. (2010). Students' perceptions of their learning experiences in an authentic instructional
        design context. Interdisciplinary Journal of Problem-Based Learning, 4(1), 6-29. https://doi.org/10.7771/1541-
         5015.1092

Demiral-Uzan, M. (2017). The Development of Design Judgment in Instructional Design Students During a Semester in
         Their Graduate Program (Doctoral dissertation, Indiana University).

Demiral-Uzan, M. (2015). Instructional design students' design judgment in action. Performance Improvement Quarterly,
        28(3), 7-23. https://doi.org/10.1002/piq.21195

Ericsson, K. A. (2006). Protocol analysis and expert thought: Concurrent verbalizations of thinking during experts'
         performance on representative tasks. The Cambridge handbook of expertise and expert performance, 223-241.

Gray, C. M., Dagli, C., Demiral-Uzan, M., Ergulec, F., Tan, V., Altuwaijri, A. A., Gyabak, K., Hilligoss, M., Kizilboga, R., Tomita,
        K. & Boling, E. (2015). Judgment and instructional design: How ID practitioners work in practice. Performance
        Improvement Quarterly, 28(3), 25-49. https://doi.org/10.1002/piq.21198

Kaminski, K., Johnson, P., Otis, S., Perry, D., Schmidt, T., Whetsel, M., & Williams, H. (2018). Personal Tales of
         Instructional Design from the Facilitator's Perspective. In B. Hokanson, G. Clinton & K. Kaminski (Eds.),
         Educational Technology and Narrative (pp. 87-101). Springer. https://doi.org/10.1007/978-3-319-69914-1

Korkmaz, N., & Boling, E. (2014). Development of design judgment in instructional design: Perspectives from instructors,
        students, and instructional designers. Design in Educational Technology (pp. 161-184). Springer, Cham.
         https://doi.org/10.1007/978-3-319-00927-8_10

Lachheb, A., & Boling, E. (2018). Design tools in practice: instructional designers report which tools they use and why.
        Journal of Computing in Higher Education, 30(1), 34-54. https://doi.org/10.1007/s12528-017-9165-x

Nelson, H. G., & Stolterman, E. (2012). The design way: Intentional change in an unpredictable world (2nd ed.). The MIT
         Press.

Schön, D. (1983). The Reflective practitioner: How professionals think in action. Temple-Smith.

Schön, D. A. (1987). Educating the reflective practitioner: Toward a new design for teaching and learning in the
         professions. John Wiley & Sons.

Smith, K. M., & Boling, E. (2009). What do we make of design? Design as a concept in educational technology.
        Educational Technology, 49(4), 3-17.

Stolterman, E., McAtee, J., Royer, D., & Thandapani, S. (2009). Designerly tools. http://shura.shu.ac.uk/id/eprint/491

Tracey, M. W., Hutchinson, A., & Grzebyk, T. Q. (2014). Instructional designers as reflective practitioners: Developing
        professional identity through reflection. Educational Technology Research and Development, 62(3), 315-334.
         https://doi.org/10.1007/s11423-014-9334-9

                                                                                       177
Yanchar, S. C., South, J. B., Williams, D. D., Allen, S., & Wilson, B. G. (2010). Struggling with theory? A qualitative
        investigation of conceptual tool use in instructional design. Educational Technology Research and Development,
        58(1), 39-60. https://doi.org/10.1007/s11423-009-9129-6
                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/design_judgment.

                                                                                       178
  14

Instructional Design Evaluation

Cheryl Calhoun, Shilpa Sahay, & Matthew L. Wilson

Evaluation  Formative Evaluation  Summative Evaluation

   Editor's Note

     This is a remixed version of an earlier chapter on evaluation in instructional design that can be found at the
     ADDIE Explained website, and is printed here under the same license as the original.

Evaluation sits at the center of the instructional design model. It provides feedback to all other stages of the design
process to continually inform and improve our instructional designs. In this chapter we will discuss the Why, What,
When, and How of evaluation. We will explore several of the most cited evaluation models and frameworks for
conducting formative, summative, and confirmative evaluations. It is important to note that instruction can occur in
formal instructional settings or through the development of instructional products such as digital learning tools.
Throughout this chapter we will discuss interchangeably instructional programs and/or products. Effective evaluation
applies to all of these forms of instructional design.
Figure 1
ADDIE Model of Design (Fav203, 2012)

                                                        179
Why Do We Evaluate?

Evaluation ensures that the instruction being designed both meets the identified need for instruction and is effective in
achieving the intended learning outcomes for participants. It helps to answer questions such as:

      Are our instructional goals aligned with the requirements of the instructional program?
      Are our lesson plans, instructional materials, media, and assessments, aligned with learning needs?
      Do we need to make any changes to our design to improve the effectiveness and overall satisfaction with the
      instruction?
      Does the implementation provide effective instruction and carry out the intended lesson plan and instructional
      objectives?
      Have the learners obtained the knowledge and skills that are needed?
      Are our learners able to transfer their learning into the desired contextual setting?

These questions help shape the instructional design, confirm what and to what extent the learner is learning, and
validates the learning over time to support the choices made regarding the design--as well as how the program holds
up over time.

What Is Evaluation?

Evaluation is the process of reviewing both the instructional components and the resulting outcomes of instruction to
determine whether instruction achieves the desired outcomes. Kirkpatrick's model of evaluation proposes four levels of
evaluation: reaction, learning, behavior, and results (Kirkpatrick & Kirkpatrick, 2016). While this is a fairly simplistic
model, it provides a framework for understanding evaluation and has provided a significant model of evaluation to the
field of instructional design.

Figure 2

Kirkpatrick's Model of Evaluation

                                                                                       180
Reaction

In order to have effective instruction, one requires frequent feedback from the learners to check learning progress and
monitor efficacy of the pedagogical process selected for instruction (Heritage, 2007). An instructional designer can
evaluate both the teacher and the learner's reaction to a new pedagogical instruction. Once it is determined that there is
engagement by the learners, one may assume that learners will not drop out due to their reaction to the quality or
applicability of instruction. It also helps the evaluator to control the pace of the program as one moves ahead in the
training phase. It leaves less frustration and vagueness in the evaluator's mind if one knows that all the learners are
positively oriented towards undertaking the training.

Learning

Evaluating learning is an ongoing process in instructional development. It is important to evaluate whether materials
developed solve the problems that were identified. When learners master the content of the training or exhibit proper
learning through assessment, one can assume the effectiveness of the program and identify what did not work if the
learning outcomes show adverse results. Several studies in the field of educational measurement have suggested that
assessments and evaluations lead to higher quality learning. Popham (2008) called this new aspect of assessment in
the evaluation process as "Transformative Assessment" where an evaluator identifies learning progression of the
learners by analyzing the sequence of skills learned over the period of study program. This also helps the evaluator or
the instructional designer to develop methods to assess how much the learners mastered the learning material.

Behavior

Attitudes and behavior are important indicators towards the acceptance and success of an instructional program. Dick,
Carey, and Carey (2015) mentioned that an evaluator needs to write directions to guide the learner's activities and
construct a rubric (e.g. a checklist or a rating scale) in order to evaluate and measure performance, products, and
attitudes. A learner develops several intellectual and behavioral skills, and an evaluation can uncover what changes
have been brought in the attitude and behavior of the learners.

                                                                                       181
Results

With every instructional product, evaluating results is the most significant task by an evaluator, and is done to determine
how closely one has been able to achieve success in the implementation of the program. An evaluator conducts an
evaluation in order to test the effectiveness of the instruction to create the desired learning outcome (Morrison et al.,
2019). Morrison et al. (2019) suggested evaluators measure the efficiency of learning by comparing the skills mastered
with the time taken; cost of program development; continuing expenses; reactions towards the program; and long-term
benefits of the program.

When Do We Evaluate?

Three commonly used types of evaluation for instruction are formative, summative, and confirmative (Morrison et al.,
2019; Ross & Morrison, 2010). Formative evaluation is conducted during the design process to provide feedback that
informs the design process. Summative evaluation is conducted at the end of the design process to determine if the
instructional product achieves the intended outcomes. Confirmative evaluation is conducted over time to determine the
lasting effects of instruction. Each of these stages of evaluation is examined in detail here, both through the definition
of the form itself and through a discussion of some of the key tools within each.

"When the cook tastes the soup that's formative; when the guests taste the soup, that's summative." - Robert E. Stake
(M. Scriven, 1991, p. 169)

Formative

Formative evaluation occurs during instructional design. It is the process of evaluating instruction and instructional
materials to obtain feedback that in turn drives revisions to make instruction more efficient and effective. One way to
think about this is to liken it to a chef tasting his food before he sends it out to the customer. Morrison et al. (2019)
explained that the formative evaluation process utilizes data from media, instruction, and learner engagement to
formulate a picture of learning from which the designer can make changes to the product before the final
implementation.

Boston (2002, p. 2) stated the purpose of formative evaluation as "all activities that teachers and students undertake to
get information that can be used diagnostically to alter teaching and learning." Formative evaluation results in the
improvement of instructional processes for the betterment of the learner. While making formative changes are best
conducted during earlier stages of the design process, these changes may come later if the situation dictates it.
According to Morrison et al., (2019), when summative and confirmative evaluations demonstrate undesirable effects,
then the results may be used as a formative evaluation tool to make improvements.

Instructional designers should consider a variety of data sources to create a full picture of the effectiveness of their
design. Morrison et al. (2019) proposed that connoisseur-based, decision-oriented, objective-based, and constructivist
evaluations are each appropriate methodologies within the formative process. More recently Patton (2016) introduced
developmental evaluation which introduces innovation and adaptation in dynamic environments.

Types of Formative Evaluation

Connoisseur-Based

Employs subject matter experts (SMEs) in the review of performance objectives, instruction, and assessments to verify
learning, instructional analysis, context accuracy, material appropriateness, test item validity, and sequencing. Each of
these items allow the designer to improve the organization and flow of instruction, accuracy of content, readability of
materials, instructional practices, and total effectiveness (Morrison et al., 2019).

                                                                                       182
Decision-Oriented

Questions asked may develop out of the professional knowledge of an instructional designer or design team. These
questions subsequently require the designer to develop further tools to assess the question, and as such should be
completed at a time when change is still an option and financially prudent (Morrison et al., 2019).

Objective-Based

Through an examination of the goals of a course of instruction, the success of a learner's performance may be
analyzed.

Constructivist

Takes into account the skills students learned during the learning process as well as how they have assimilated what is
learned into their real lives.

Developmental

Responsive to context and more agile, allowing for quicker response and support of innovative designs (Patton, 2011).

Summative

Dick et al. (2015, p. 320) claimed the ultimate summative evaluation question is "Did it solve the problem?" That is the
essence of summative evaluation. Continuing with the chef analogy from above, one asks, "Did the customer enjoy the
food?" (M. Scriven, 1991). The parties involved in the evaluation take the data and draw a conclusion about the
effectiveness of the designed instruction. However, over time, summative evaluation has developed into a process that
is more complex than the initial question may let on. In modern instructional design, practitioners investigate multiple
questions through assessment to determine learning effectiveness, learning efficiency, and cost effectiveness, as well
as attitudes and reactions to learning (Morrison et al., 2019).

Learning Effectiveness

Learning effectiveness can be evaluated in many ways. Here we are trying to understand:

      How well did the student learn?
      Are the students motivated to change behavior?
      Did we engage the intended population of learners?
      Even, did we teach the learner the right thing?

Measurement of learning effectiveness can be ascertained from assessments, ratings of projects and performance,
observations of learners' behavior, end of course surveys, focus groups, and interviews. Dick et al. (2015) outlined a
comprehensive plan for summative evaluation throughout the design process, including collecting data from SMEs and
during field trials for feedback.

Learning Efficiency and Cost-Effectiveness

While learning efficiency and cost-effectiveness of instruction are certainly distinct constructs, the successfulness of
the former impacts the latter. Learning efficiency is a matter of resources (e.g., time, instructors, facilities, etc.), and how
those resources are used within the instruction to reach the goal of successful instruction (Morrison et al., 2019). Dick
et al. (2015) recommended comparing the materials against an organization's needs, target group, and resources. The
result is the analysis of the data to make a final conclusion about the cost effectiveness based on any number of
prescribed formulas.

Attitudes and Reactions to Learning

The attitudes and reactions to the learning, while integral to formative evaluation, can be summatively evaluated as well.
Morrison et al. (2019) explained there are two uses for attitudinal evaluation: evaluating the instruction and evaluating
outcomes within the learning. While most objectives within learning are cognitive, psychomotor and affective objectives

                                                                                       183
may also be goals of learning. Summative evaluations often center on measuring achievement of objectives. As a
result, there is a natural connection between attitudes and the assessment of affective objectives. Conversely,
designers may utilize summative assessments that collect data on the final versions of their learning product. This
summative assessment measures the reactions to the learning.

Confirmative

The purpose of a confirmative evaluation is to determine if instruction is effective and if it met the organization's
defined instructional needs. In effect, did it solve the problem? The customer ate the food and enjoyed it. But, did they
come back? Confirmative evaluation goes beyond the scope of formative and summative evaluation and looks at
whether the long-term effects of instruction is what the organization was hoping to achieve. Is instruction affecting
behavior or providing learners with the skills needed as determined by the original goals of the instruction? Confirmative
evaluation methods may not differ much from formative and summative outside of the fact that it occurs after
implementation of a design. Moseley and Solomon (1997) described confirmative evaluation as maintaining focus on
what is important to your stakeholders and ensuring the expectations for learning continue to be met.

How Do We Evaluate?

Formative Evaluation

Formative evaluation is an iterative process that requires the involvement of instructional designers, subject matter
experts, learners, and instructors. Tessmer (2013) identified four stages of formative evaluation including expert review,
one-to-one, small group, and field test evaluation. Results from each phase of evaluation are fed back to the
instructional designers to be used in the process of improving design. In all stages of evaluation, it is important that
learners are selected that will closely match the characteristics of the target learner population.
Figure 3
The Cycle of Formative Evaluation

                                                                                       184
Expert Review

The purpose of the expert review is to identify and remove the most obvious errors and to obtain feedback on the
effectiveness of the instruction. The expert judgment phase can include congruence analysis, content analysis, design
analysis, feasibility analysis, and user analysis. Results from expert review can be used to improve instructional
components and materials before a pilot implementation. This phase is conducted with the instructional designer, the
subject matter experts, and often an external reviewer. Target learners are not involved in this stage of evaluation.
Figure 4
The Expert Judgment Phase (Dick et al., 2015)

                                                                                       185
One-to-One

The one-to-one evaluation is much like a usability study. During this evaluation, IDs should be looking for clarity, impact,
and feasibility (Dick et al., 2015, p. 262; Earnshaw, Tawfik, & Schmidt, 2017). The learner is presented with the
instructional materials that will be provided during the instruction. The evaluator should encourage the learner to
discuss what they see, write on materials as appropriate, and note any errors. The ID can engage the learner in dialog to
solicit feedback on the materials and clarity of instruction. There are many technological tools that can facilitate a one-
on-one evaluation. The principles of Human Computer Interaction and User Center Design can inform the instructional
design review (Earnshaw et al., 2017). In Don't Make Me Think, Krug (2014) described a process of performing a
usability study for website development. The steps he provided are a good guide for performing a one-to-one
evaluation. Krug recommended video recording the session for later analysis. If instruction is computer based, there are
tools available that can record the learner interaction as well as the learner's responses. Morae from Techsmith
(https://www.techsmith.com/morae.html) is a tool that allows you to record user interactions and efficiently analyze the
results.

Small Group

Small group evaluation is used to determine the effectiveness of changes made to the instruction following the one-to-
one evaluation and to identify any additional problems learners may be experiencing. The focus is on consideration of
whether learners can use the instruction without interaction from the instructor. In a small group evaluation, the
instructor administers the instruction and materials in the way they are designed. The small-group participants
complete the lesson(s) as described. The instructional designer observes but does not intervene. After the instructional
lesson is complete, participants should be asked to complete a post-assessment designed to provide feedback about
the instruction.

Field Trial

After the recommendations from the small group evaluation have been implemented, it is time for a field trial. The
selected instruction should be delivered as close as possible to the way the design is meant to be implemented in the
final instructional setting, and instruction should occur in a setting as close to the targeted setting as possible. Learners

                                                                                       186
should be selected that closely match the characteristics of the intended learners. All instructional materials for the
selected instructional section, including the instructor manual, should be complete and ready to use. Data should be
gathered on learner performance and attitudes, time required to use the materials in the instructional context, and the
effectiveness of the instructional management plan. During the field trial the ID does not participate in delivery of
instruction. The ID and the review team will observe the process and record data about their observations.
Figure 5
Field Trial

Summative Evaluation

The purpose of a summative evaluation is to evaluate instruction and/or instructional materials after they are finalized.
It is conducted during or immediately after implementation. This evaluation can be used to document the strengths and
weaknesses in instruction or instructional materials, to decide whether to continue instruction, or whether to adopt
instruction. External evaluators for decision makers often conduct or participate in summative evaluation. Subject
matter experts may be needed to ensure integrity of the instruction and/or instructional materials. There are several
models we can consider for summative evaluation including the CIPP Model, Stake's Model, and Scriven's Model.

CIPP Model

The CIPP evaluation model by Stufflebeam (1971) describes a framework for proactive evaluation to serve decision
making and retroactive evaluation to serve accountability. The model defines evaluation as the process of delineating,
obtaining, and providing useful information for judging decision alternatives. It includes four kinds of evaluation:
context, input, process, and product. The first letters of the names of these four kinds of evaluation gave the acronym -
CIPP. The model provides guidelines for how the steps in evaluation process interact with these different kinds of
evaluation.

                                                                                       187
   The CIPP Model of Evaluation by Mallory Buzun-Miller

                                                                   Watch on YouTube
                                                                    Link to transcript

Stake's Model

Stake in 1969 created an evaluation framework to assist an evaluator in collecting, organizing, and interpreting data for
the two major operations of evaluation (Stake, 1967; Wood, 2001). These include (a) complete description and (b)
judgment of the program. W. J. Popham (1993) defined that Stake's schemes draw attention towards the differences
between the descriptive and judgmental acts according to their phase in an educational program, and these phases can
be antecedent, transaction, and outcome. This is a comprehensive model for an evaluator to completely think through
the procedures of an evaluation.

                                                                                       188
   Dr. Robert Stake by Education at Illinois

                                                                   Watch on YouTube
                                                                    Link to transcript

Scriven's Goal-Free Model

Scriven provides a transdisciplinary model of evaluation in which one draws from an objectivist view of evaluation
(Michael Scriven, 1991a, 1991b). Scriven defined three characteristics to this model: epistemological, political, and
disciplinary. Some of the important features of Scriven's goal free evaluation stress on validity, reliability,
objectivity/credibility, importance/timeliness, relevance, scope, and efficiency in the whole process of teaching and
learning. Youker (2013) expanded on the model to create general principles for guiding the goal-free evaluator. Younker
proposed the following principles:

  1. Identify relevant effects to examine without referencing goals and objectives.
  2. Identify what occurred without the prompting of goals and objectives.
  3. Determine if what occurred can logically be attributed to the program or intervention.
  4. Determine the degree to which the effects are positive, negative, or neutral.
The main purpose of the goal-free evaluation is to determine what change has occurred that can be attributed to the
instructional program. By conducting the evaluation without prior knowledge of learning outcomes or goals, the
evaluator serves as a check to see if the program produced the outcome desired by the instructional designer(s).

                                                                                       189
   The Past, Present, and Future of Evaluation: Possible Roles for the University of
   Melbourne, by The University of Melbourne

                                                                   Watch on YouTube
                                                                    Link to transcript

Confirmative Evaluation

The focus of confirmative evaluation should be on the transfer of knowledge or skill into a long-term context. To
conduct a confirmative evaluation, you may want to use observations with verification by expert review. You may also
develop or use checklists, interviews, observations, rating scales, assessments, and a review of organizational
productivity data. Confirmative evaluation should be conducted on a regular basis. The interval of evaluation should be
based on the needs of the organization and the instructional context.

Conclusion

Evaluation is the process of determining whether the designed instruction meets its intended goals. In addition,
evaluation helps us to determine whether learners can transfer the skills and knowledge learned back into long-term
changes in behavior and skills required for the target context. Evaluation provides the opportunity for instructional
designers to ensure all stakeholders agree that the developed instruction is meeting the organizational goals.
In this chapter we reviewed what evaluation looks like and its relationship within the instructional design process. We
looked at several models of evaluation including Kirkpatrick's Model and the four levels of evaluation: Evaluating
Reaction, Evaluating Learning, Evaluating Behavior, and Evaluating Results. We also looked at the three phases of
evaluation including formative, summative, and confirmative evaluation, and introduced several different models and
methods for conducting evaluation from many leading evaluation scholars.

                                                                                       190
Discussion

      Where does evaluation stand in the instructional design model? How will your flow chart look when you
      describe evaluation in relation to the other stages of instructional design?
      Describe the three stages of evaluation. Give an example to explain how an instructional designer will use
      these three stages in a specific case of a learning product.
      Which are the five types of formative evaluation methods mentioned in the chapter that assist in collecting
      data points for the initial evaluation? Which two of these methods will be your preferred choice for your
      formative evaluation and why?
      What will be the parameters to evaluate the success of the instructional training?
      What are some of the techniques to conduct formative and summative evaluation?
      Several models of evaluation have been discussed in the chapter. Discuss any two of these models in detail
      and explain how you will apply these models in your evaluation process.

Application Exercises

For the following exercises, you may use an instructional module that you are familiar with from early childhood,
k-12, higher ed, career and technical, corporate, or other implementation where instructional design is needed.
Be creative and use something from an educational setting that you are interested in. Be sure to describe your
selected instructional module as it relates to each of these exercises. You may need to do some additional
online research to answer these questions. Be sure to include your references in your responses.

  1. Describe how you would conduct the three phases of the formative evaluation. Define your strategies,
      populations, and methodologies for each stage within the process.

  2. Draw a diagram of the iterative formative evaluation process. What specific pieces of the instructional
      intervention are considered within each stage of the process? How is the data gathered during this process
      employed to improve the design of instruction?

  3. Describe the context and learner selection process you would use for setting up a formative evaluation field
      trial. What special considerations need to be made to conduct this stage of evaluation effectively?

  4. What materials should the designer include in a field trial? How do the materials used for field trials
      contrast with the one-to-one and small group evaluations?

You have been asked to serve as an external evaluator on a summative evaluation of a training model designed
by one of your colleagues. Explain the phases of the summative evaluation that you may be asked to participate
in as an external reviewer. Imagine you have created a rubric to help you evaluate the instructional intervention.
What items might that rubric contain to help you effectively and efficiently conduct a review?

                                                                                 191
   Group Assignment

     Conduct an evaluation study to understand how successful an instructional intervention has been in achieving
     the goals of the designed instruction. Keep in mind the group project conducted in the previous development
     and implementation chapters and conduct an evaluation study to assess the success of achieving the goals
     and objectives of the instruction. To achieve these goals, you should conduct several rounds of evaluation:

           Conduct a one-on-one evaluation with a student from the target population. Make observations of the
           student's actions within the instruction and reactions to the materials, content, and overall design. Propose
           changes to the instructional intervention based on the sample student's feedback.
           Conduct a small group evaluation with a group of 3 to 5 learners. This evaluation should reflect the
           changes you made after the one-to-one stage and evaluate nearly operational materials and instruction.
           You must recruit an instructor to deliver instruction. Make observations and have the instructor administer
           a summative assessment after instruction. Analyze the data gathered and create a one-page report on the
           results.
           Implement a field test with an instructor and a student population of at least 15 people. Instructional
           materials, including the instructor manual, should be complete and ready to use. Gather data on learner
           performance and attitudes, as well as time required for instruction and the effectiveness of the
           instructional management plan. Observe the process and record data, and create a final report detailing the
           full evaluation cycle.

References

Boston, C. (2002). The concept of formative assessement. In ERIC Clearinghouse on Assessment and Evaluation.

Dick, W., Carey, L., & Carey, J. (2015). The systematic design of instruction (8th ed.). USA: Pearson.

Earnshaw, Y., Tawfik, A. A., & Schmidt, M. (2017). User experience design. In Foundations of Learning and Instructional
        Design Technology.

Fav203. (2012). ADDIE_Model_of_Design.jpg. In. Wikipedia.com: is licensed under CC BY-SA 3.0 via Wikimedia
         Commons. Retrieved from http://commons.wikimedia.org/wiki/File:ADDIE_Model_of_Design.jpg#filelinks

Heritage, M. (2007). Formative assessment: What do teachers need to know and do? Phi Delta Kappan, 89(2), 140-145.

Kirkpatrick, J. D., & Kirkpatrick, W. K. (2016). Kirkpatrick's four levels of training evaluation: Association for Talent
         Development.

Krug, S. (2014). Don't Make Me Think, Revisited (Vol. 3): New Riders.

Morrison, G. R., Ross, S. J., Morrison, J. R., & Kalman, H. K. (2019). Designing effective instruction: Wiley.

Moseley, J. L., & Solomon, D. L. (1997). Confirmative Evaluation: A New Paradigm for Continuous Improvement.
         Performance Improvement, 36(5), 12-16.

Patton, M. Q. (2011). Developmental evaluation: Applying complexity concepts to enhance innovation and use: Guilford
         Press.

Patton, M. Q. (2016). The state of the art and practice of developmental evaluation. Developmental evaluation
         exemplars, 1-24.

                                                                                       192
Popham, W. (2008). Transformative Assessment: Association for Supervision and Curriculum Development. 1703 North
         Beauregard Street, Alexandria, VA 22311-1714. In: Tel.

Popham, W. J. (1993). Educational Evaluation: Allyn and Bacon. Retrieved from https://books.google.com/books?
         id=_UolAQAAIAAJ

Ross, S. M., & Morrison, G. R. (2010). The Role of Evaluation in Instructional Design. In Handbook of Improving
         Performance in the Workplace: Instructional Design and Training Delivery.

Scriven, M. (1991a). Beyond formative and summative evaluation. Evaluation and education: At quarter century, 10(Part
         II), 19-64.

Scriven, M. (1991). Evaluation thesaurus (4th ed. ed.). Newbury Park, CA: Sage Publications.
Scriven, M. (1991b). Prose and cons about goal-free evaluation. Evaluation Practice, 12(1), 55-62.
Stake, R. E. (1967). The countenance of educational evaluation: Citeseer.
Stufflebeam, D. L. (1971). The relevance of the CIPP evaluation model for educational accountability. Retrieved from

         http://search.proquest.com/docview/64252742?accountid=10920
Tessmer, M. (2013). Planning and conducting formative evaluations: Routledge.
Wood, B. B. (2001). Stake's countenance model: evaluating an environmental education professional development

         course. The Journal of Environmental Education, 32(2), 18-27.
Youker, B. W. (2013). Goal-free evaluation: A potential model for the evaluation of social work programs. Social Work

         Research, 37(4), 432-438.

                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/instructional_design_evaluation.

                                                                                       193
194
  15

Continuous Improvement of Instructional Materials

David Wiley, Ross Strader, & Robert Bodily

     Continuous Improvement

From time to time new technologies provide us with a qualitatively different ability to engage in previously possible
activities. For example, 20 years ago it was already possible to publish an essay online. You simply used the command
line program Telnet to login to a remote server, navigated into the directory from which your webserver made html files
available to the public, launched the pico editor from the command line, wrote your essay, and manually added all the
necessary html tags. Today, open source blogging software like Wordpress makes publishing an essay online as easy
as using a word processor. Yes, it was possible to publish essays online before, but the modern experience is
qualitatively different.
"Evaluate" is the final step in the traditional ADDIE meta-model of instructional design, and it has always been possible--
if, at times, expensive and difficult--to evaluate the effectiveness of instructional materials. Modern technology has
made the process of measuring the effectiveness of instructional materials a qualitatively different experience.
Gathering data in the online context is orders of magnitude less expensive than gathering data in classrooms, and open
source analysis tools have greatly simplified the process of analyzing these data.
Historically, any needed improvements discovered during the evaluation process would take a significant amount of
time to reach learners, as they could only be accessed once new editions of a book were printed or new DVDs were
pressed. Again, modern technology makes the delivery of improvements a qualitatively different exercise. When
instructional materials are delivered online, instructional designers can engage in continuous delivery practices, where
improvements are made available to learners immediately, as often as multiple times per day.
The modern approach to continuous improvement designed for use in the context of online services described by Ries
(2011), called the "build - measure - learn cycle," is illustrated in Figure 1.
Figure 1
The Build - Measure - Learn Cycle

                                                                                       195
In this chapter we adapt the build - measure - learn cycle for use by instructional designers who want to engage in
continuous improvement. Because our focus is on the improvement of instructional materials, our discussion below
does not include a discussion of the creation of the first version of the materials. (The first version of the materials
could be open educational resources created by someone else or a first version that you created previously.)

The chapter will proceed as follows:

      Conceptual Framework: We argue that all instructional materials are hypotheses, or our best guesses, informed by
      research, about what instructional design approach will support student learning in a specific context. Thinking this
      way will naturally lead us to collect and analyze data to test the effectiveness of our instructional materials.
      Build: We describe the implications of designing for data collection, together with the instrumentation and tooling
      that must be built in order to collect the data necessary for continuous improvement.
      Measure: We describe the process of analyzing data in order to identify portions of the instructional materials that
      are not effectively supporting student learning.
      Learn: We discuss methods to use when reviewing less effective portions of the instructional materials and
      deciding what improvements to make before beginning the cycle again.
      Technical Note: We briefly pause to discuss the role of copyright, licensing, and file formats in continuous
      improvement.
      Worked Example: We demonstrate one trip through the cycle with a worked example.
      Conclusion: We end with some thoughts about the imperative implied for instructional designers by the existence
      and relative ease of use of continuous improvement approaches like the build - measure - learn cycle.

                                                                                       196
Conceptual Framework

Instructional Materials Are Hypotheses

People who design instructional materials (who we will refer to as instructional designers throughout) make hundreds
of decisions about how to best support student learning. Each decision is a hypothesis of the form "in the context of
these learners and this topic, applying this instructional design approach in this manner will maximize students'
likelihood of learning." The ways in which these individual decisions are interwoven together creates a network of
hypotheses about how best to support student learning.

Hypotheses Need to Be Tested

It reveals a fatal lack of curiosity for an instructional designer to simply say "these materials were designed in
accordance with current research on learning" without following through to measure their actual effectiveness with
actual learners in the actual world. While designing instructional materials in accordance with research is a positive first
step, to our minds the most important measure of the quality of instructional materials is the degree to which they
actually support student learning. Questions of whether or not the materials are informed by research, are finished on
schedule and on budget, are stunningly beautiful, render correctly on a mobile device, or were authored by a famous
academic become meaningless if students who use the materials do not learn what the designers intended.

Initial Hypotheses Are Seldom Correct

Hypotheses need to be refined in an ongoing cycle of improvement. Data collected during student use of content and
from assessments of learning can be used to identify specific portions of the instructional materials (i.e., specific
instructional design hypotheses) that are not successfully supporting student learning. Once these underperforming
designs (hypotheses) are identified, they can be redesigned, improved, and incorporated into a new version of the
instructional materials. The updated collection of instructional design hypotheses can then be deployed for student use,
and the cycle of continuous improvement can begin again.

Build: Designing for Data, Instrumentation, and Tools for Data
Collection

In order to be able to engage in continuous improvement, instructional materials must be designed for data collection.
There must be a unifying design framework that will allow data from a wide range of sources to be aggregated
meaningfully. The method we will describe throughout this chapter organizes instructional materials around a network
of learning outcomes. In this method of designing for data collection, all instructional materials (e.g., readings,
simulations, videos, practice opportunities) are aligned with one or more learning outcomes. All forms of assessment,
both formative or summative, are also aligned with one or more learning outcomes (this alignment must be done at the
individual assessment item level.)

Once instructional materials have been designed for data collection, tools and instrumentation must be created so that
the data can actually be collected and managed. The system that mediates student use of the instructional materials
(e.g., a learning management system) must be capable of (a) expressing the relationships between learning outcomes,
instructional materials, and assessments, (b) capturing data about student engagement with these instructional
materials, and (c) capturing item-level data about student engagement with, and performance on, assessments. The
data collected by the system should be able to answer questions such as, for any given learning outcome, what
instructional materials in the system are aligned with that outcome? (If instructional activities are "aligned with" a
learning outcome, student engagement with the instructional activities should support mastery of the outcome.) For
any given learning outcome, what assessment items in the system are aligned with that outcome? (If assessments are
"aligned with" a learning outcome, student success on these assessments should provide evidence that they have
mastered the outcome).

                                                                                       197
Measure: Using RISE Analysis to Identify Less Effective
Learning Materials

As described in Bodily, Nyland, and Wiley (2017), activity engagement data and assessment performance data can be
analyzed together to identify learning outcomes whose aligned instructional materials are not sufficiently supporting
student mastery (as demonstrated by performance on aligned assessments). The purpose of Resource Inspection,
Selection, and Enhancement (RISE) analysis is to identify learning outcomes where students were highly engaged with
aligned instructional materials, but simultaneously performed poorly on aligned assessments.
Each point in Figure 2 represents a learning outcome. The x-axis is engagement with instructional materials and the y-
axis is assessment performance, both converted to z-scores. The bottom-right quadrant (high engagement, low
performance) indicates which outcomes should be targeted for improvement and are numbered to indicate the order in
which they should be addressed.
Figure 2
A RISE Analysis Plot

An open source software implementation of RISE analysis is described in Wiley (2018). This greatly simplifies the
process of running RISE analyses, as long as appropriate data on learning outcome names, content engagement, and
assessment performance are available.

                                                                                       198
Learn: Understanding Why Learning Outcomes End up in the
Bottom Right Quadrant

Once learning outcomes are identified as being in the bottom right quadrant of a RISE analysis plot, the cause of the
problem can be isolated. For brevity, we will refer to learning outcomes in the bottom right quadrant of a RISE analysis
plot as "underperforming learning outcomes" below. The root of the problem can generally be identified in two steps.

The first step in isolating the problem with an underperforming learning outcome is evaluating assessments aligned
with each learning outcome. Are the assessments accurately measuring student learning? Questions to ask at this
stage include: are there technical problems with the assessment? Are items miskeyed? Are other sources of spurious or
construct-irrelevant difficulty present? Are measures of reliability, validity, or discrimination unacceptably low? If the
answer to any of these questions is yes, improvements should be made to problematic assessments, after which the
instructional designer can stop working on this learning outcome and move onto the next. There is likely no need to
make improvements to instructional materials aligned with this learning outcome.

If the aligned assessments are functioning as intended, the instructional designer can move on to the second step--
reviewing the instructional materials to determine why they aren't sufficiently supporting student learning. This process
is highly subjective and brings the full expertise of the instructional designer to bear. The instructional designer reviews
the instructional materials aligned with the learning outcome and asks questions about why students might be
struggling here. For example:

      Is there a mismatch between the type of information being taught and the instructional design approach originally
      selected? For example, if students are learning a classification task, are examples and non-examples provided
      without a specific discussion of the critical attributes that separate instances from non-instances?
      Is there a mismatch in Bloom's Taxonomy level between the learning outcome, the instructional materials, and the
     assessment? (For example, are the learning outcome and instructional materials primarily the Remember level,
     while the assessments require students to Apply?)
      Have the instructional materials failed to provide learners with an opportunity to practice in a no/low-stakes setting
      and receive feedback on the current state of their understanding?

We cannot list every question an instructional designer might ask, but we hope these examples are illustrative. Talking
with students can also be incredibly helpful at this stage. These conversations are an effective way for the instructional
designer to zero in on root causes of students' misunderstandings.

Once the instructional designer believes they have identified the problems (i.e., they have a new hypothesis about how
to better support student learning), new or existing instructional materials and assessments can be created, adapted, or
modified. Students can also be powerful partners and collaborators in creating improvements to the instructional
materials (e.g., OER-enabled pedagogy as described by Wiley and Hilton (2018)).

When this (Build) process is completed, the new or improved materials can be released to students immediately. Once
students are using the new version of the materials, this use will result in the creation of new data which the
instructional designer can examine using RISE analysis (Measure). These analyses support the instructional designer in
forming new hypotheses about why students aren't succeeding (Learn). When this continuous improvement process is
followed, instructional materials should become more effective at supporting student learning with each trip through
the cycle.

Technical Note: The Role of Copyright and File Formats

Before adaptations or modifications can be made, instructional designers must have legal permission to make changes
to the instructional materials. Because copyright prohibits the creation of derivative works that are often the result of
the improvement of instructional materials, one of two conditions must hold. In the first condition, the instructional

                                                                                       199
designer (or their employer) must hold the copyright to the instructional materials, making the creation and distribution
of improved versions legal. In the second condition, the instructional materials must be licensed under an open license
(like a Creative Commons license) that grants the instructional designer permission to create derivative works (aka
improved versions of the instructional materials).
Legal permission to create derivative works can be rendered ineffective if the instructional materials are not available in
a technical format amenable to editing (e.g., HTML). ALMS analysis as described in Hilton, Wiley, Stein, and Johnson
(2010) includes four factors to consider regarding the "improvability" of instructional materials. The first factor is
Access to editing tools--is the software needed to make changes commonly available (e.g., MS Word) or obscure (e.g.,
Blender)? The second factor is the Level of expertise required to make changes--is the content easy to change (e.g.,
Powerpoint) or difficult to change (e.g., an interactive simulation written in Javascript)? The third factor is whether or
not the instructional materials are Meaningfully editable--is the document a scanned image of handwritten notes (this
text is not easily editable) or an HTML file (easily editable)? The final factor is Source file access--is the file format
preferred for using the resource also the format preferred for editing the resource (e.g., an HTML file) or are the
preferred formats preferred for using and editing the files different (e.g., PSD versus JPG)?
If the instructional materials you are working with do not belong to you or your employer, are not openly licensed, or are
available only in file formats that are not conducive to adaptation and modification, you may not be able to engage in
continuous improvement.

A Worked Example

Lumen Learning, a company that offers instructional materials for college classes that can be adopted in place of
traditional textbooks, offers a Biology for Non-majors course in its Waymaker platform. This platform allows
instructional designers to enter learning outcomes and align all instructional materials and assessment items with the
learning outcomes. A RISE analysis was conducted using the content engagement data and assessment performance
data for all students who took the Biology for Non-majors course during a semester. Among the top 10 underperforming
learning outcomes it identified, the RISE analysis revealed that students were performing poorly on assessments
aligned with the learning outcome "compare inductive reasoning with deductive reasoning" despite the fact that
students were engaging with the aligned instructional materials at an above average rate (see outcome 1 in Figure 3
below). This learning outcome was selected for continuous improvement work.
Figure 3
Biology for Non-Majors RISE Analysis Plot

                                                                                       200
A review of the aligned assessment items by an instructional designer revealed that the items appeared to be keyed
correctly and free from other problems. Following this review of the aligned assessments, the instructional designer
reviewed the aligned instructional materials guided by the question, "why are students who use these instructional
materials not mastering the outcome?" The analysis revealed that the instructional materials for this outcome were
comprised of two paragraphs of text content, each of which defined one of the terms. No other instructional materials
were provided in support of mastery of this learning outcome and students appeared to be unable to remember which
of these similar sounding terms was which.

The instructional designer decided to make minor edits to the existing paragraphs to improve their clarity and also to
create an online interactive practice activity (Koedinger et al., 2017) in support of this learning outcome. This activity
provided students with mnemonic tools to help them remember which term is which, and combined these mnemonics
with practice exercises in which students classify examples as either inductive or deductive and receive immediate,
targeted feedback on their performance. The online interactive practice activitity can be viewed in context at
https://edtechbooks.org/-QwUE.

These new and updated instructional materials are now integrated into the existing materials and are being used by
faculty and students across the United States. After another semester is over, the RISE analysis will be rerun. This new
analysis will either confirm that the improvements to the instructional materials have improved student learning, in
which case other underperforming learning outcomes will be selected for continuous improvement, or they will confirm
that there is still work to do to better support student learning of this outcome.

Conclusion

Modern technologies, including the internet and open source software, have radically decreased the cost and difficulty
of collecting and analyzing learning data. Where evaluation alone was once prohibitively difficult and expensive, today
the entire continuous improvement process is within reach of those who design instructional materials for use in online

                                                                                       201
classes and other technology-mediated teaching and learning settings. While Ries (2011) described the build - measure
- learn cycle as a way to rapidly increase a company's revenue, we see a clear analog in which similar approaches can
be used to rapidly increase student learning. We now live in a world where it is completely reasonable to expect
instructional materials to be more effective at supporting student learning each and every term.
We invite the reader to help us make this possible state of affairs the actual state of affairs by engaging in continuous
improvement activities in their own instructional design practice. And in the spirit of continuous improvement, we
further invite the reader to join us in developing and refining the processes described in this chapter--in part by
completing the survey at the end of this chapter and providing us feedback on how the chapter can be improved.

References

Bodily, R., Nyland, R., & Wiley, D. (2017). The RISE framework: Using learning analytics to automatically identify open
         educational resources for continuous improvement. The International Review of Research in Open and
         Distributed Learning, 18(2). https://edtechbooks.org/-ymD

Hilton, J., Wiley, D., Stein, J., & Johnson, A. (2010). The four R's of openness and ALMS analysis: Frameworks for open
         educational resources. Open Learning: The Journal of Open and Distance Learning, 25(1), 37-44.
         https://edtechbooks.org/-vqPr

Koedinger, K. R., McLaughlin. E. A., Zhuxin, J., & Bier, N. L. (2016). Is the doer effect a causal relationship?: How can we
         tell and why it's important. LAK '16: Proceedings of the Sixth International Conference on Learning Analytics &
         Knowledge. https://edtechbooks.org/-wNfS

Ries, E. (2011). The Lean Startup. Crown Business: New York.
Wiley, D. (2018). RISE: An R package for RISE analysis. Journal of Open Source Software, 3(28), 846,

         https://edtechbooks.org/-LKLb
Wiley, D. & Hilton, J. (2018). Defining OER-enabled Pedagogy. International Review of Research in Open and Distance

         Learning, 19(4). https://edtechbooks.org/-tgNj

                                                                                       202
David Wiley

Lumen Learning
Dr. David Wiley is the chief academic officer of Lumen Learning, an organization
offering open educational resources designed to increase student access and
success. Dr. Wiley has founded or co-founded numerous entities, including Lumen
Learning, Mountain Heights Academy (an open high school), and Degreed. He was
named one of the 100 Most Creative People in Business by Fast Company, currently
serves as Education Fellow at Creative Commons, and leads the Open Education
Group in Brigham Young University's instructional psychology and technology
graduate program. He has been a Shuttleworth Fellow, served as a Fellow of
Internet and Society at Stanford Law School, and was a Fellow of Social
Entrepreneurship at BYU's Marriott School of Management.

Ross Strader

Lumen Learning

Robert Bodily

Lumen Learning
Dr. Bob Bodily is a Senior Data Scientist and Educational Researcher at Lumen
Learning. He focuses on building educational data pipelines, creating actionable
reports, and generating insights to improve teaching and learning. His interests
include using data to continuously improve course materials, building nudges to
influence student and faculty behavior, and crowdsourcing educational content
improvements and assessment items. He is currently working on educational
technologies to improve teaching and learning at wadayano.com and
statstest.com.

                                      203
This content is provided to you freely by EdTech Books.
Access it online or download it at https://edtechbooks.org/id/continuous_improvement.

                                                        204
  Part II

Instructional Design Knowledge

Bulding upon the foundation of instrucitonal design practice, we now turn to the sources of knowledge instructional
designers rely on to carry out their practice. Instructional design uses both academic sources of knowledge as well as
practical forms of know-how. Both are needed to successfully solve instructional design problems or address
instructional design challenges. Some design knowlege is personal to the designer, while other forms are codified into
processes or other techniques.
After reviewing some of the explicit and tacit sources of design knowledge upon which instructional designers rely, we
then address instructional design processes as forms of design knowledge. We also include chapters that summarize
the practical knowledge invovled in designing different kinds of instructional activities. We conclude with knowledge
that is useful for designers to develop and sustain the working relationships they need for the challenges they address.
Each of these four subsections contains between 3 - 8 chapters.

     Sources of Design Knowledge
          Learning Theories
          The Role of Theory in Instructional Design
          Making Good Design Judgments via the Instructional Theory Framework
          Using Theory as a Learning and Instructional Design Professional
          The Nature and Use of Precedent in Designing
          Standards and Competencies for Instructional Design and Technology Professionals

     Instructional Design Processes
          Design Thinking
          Robert Gagné and the Systematic Design of Instruction
          Designing Instruction for Complex Learning
          Curriculum Design Processes
          Agile Design Processes and Project Management

     Designing Instructional Activities

                                                                                       205
     Designing Technology-Enhanced Learning Experiences
     Designing Instructional Text
     Audio and Video Production for Instructional Design Professionals
     Using Visual and Graphic Elements While Designing Instructional Activities
     Simulations and Games
     Designing Informal Learning Environments
     The Design of Holistic Learning Environments
     Measuring Student Learning
Design Relationships
     Working With Stakeholders and Clients
     Leading Project Teams
     Implementation and Instructional Design

                      This content is provided to you freely by EdTech Books.
                      Access it online or download it at https://edtechbooks.org/id/instructional_design_knowledge.

                                                                                 206
Sources of Design Knowledge

The chapters in this section review different sources of design knowledge, both academic and practice-oriented. As
instructional designers find ways to synthesize both, they find their designs become richer, more effective, and more
recognizable by learners and stakeholders as a valuable resource upon which to rely.

     Learning Theories
     The Role of Theory in Instructional Design
     Making Good Design Judgments via the Instructional Theory Framework
     Using Theory as a Learning and Instructional Design Professional
     The Nature and Use of Precedent in Designing
     Standards and Competencies for Instructional Design and Technology Professionals

                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/sources_of_design_kn.

                                                                                       207
208
  16

Learning Theories

Beth Oyarzun & Sheri Conklin

     Learning Theory

Learning theories are the foundation for designing instructional solutions to achieve desired learning outcomes.
Analogies can assist in understanding new concepts, so imagine that you have purchased a new home and are
considering the best options for furniture placement in the living room. Your desired outcome is a furniture arrangement
that is aesthetically pleasing yet also functional. Many factors can play into the decision depending on how you view the
problem, and there can also be more than one solution that meets the desired outcomes.

Similarly, theories and models provide a foundation and framework for any instructional design project. Theories serve
as lenses to view the problem from different perspectives, much like interior design styles and preferences may affect
decisions about which furniture to purchase for your new home based on your overall aesthetic. Models then provide
guidance about how to build the solution or where to place the furniture in the home. Depending on the theory and
model used, the solution might look different, much like a living room would look very different using modern vs.
western-style decor with various arrangements. However, the desired outcomes can still be achieved. It is essential to
conduct a thorough analysis to ensure the theory and/or selected strategy will support the desired outcomes and the
targeted learners.

Learning theories help instructional designers understand how people retain and recall information and stay motivated
and engaged in learning. There are three main families of learning theories and an emerging fourth: behaviorism,
cognitivism, constructivism, and connectivism. Referring back to the house analogy, these could be different decorative
styles (lenses) used to view a room in the house or to view an instructional problem and how to address it. Much like
decorative styles have evolved and changed over time, so have learning theories. This chapter will define the four main
families of learning and then explore some additional social and motivational learning theories that have derived from
some of the families of learning.

Behaviorism

Behaviorism grew from the work of many psychologists in the early 20th Century, such as Watson (1913), Thorndike
(1898), and Skinner (1953), who hypothesized that learning occurs through interaction with the environment. Hence,
observable behaviors resulting from a response to a stimulus followed by a reward or punishment based on the
behavior is how a behaviorist would condition learners to elicit the desired outcome. Conversely, if the stimulus is
removed, then the behavior will stop over time. This phenomenon is called extinction.

This type of behavior modification can be considered conditioning. Two types of conditioning were defined by Pavlov
(1960) and Skinner (1953): classical and operant respectively. An example of classical conditioning is Pavlov's dog in

                                                                                       209
which he trained the dog to salivate with a bell ringing by providing food every time a bell rang. Extinction occurred
when the food was not delivered when the bell rang over time. Operant conditioning relies on positive and negative
consequences occurring to shape behavior. This method is focused on changing the learner's external behavior using
stimuli (an event that evokes a specific functional reaction) with positive and negative reinforcement. Reinforcements
(positive or negative) are environmental responses that increase the probability of a behavior being repeated.
Punishment, on the other hand, decreases the likelihood of a behavior being repeated, yet weakens the behavior. As an
illustration, a simple way to shape student behavior is to provide feedback on learner performance. Through positive
feedback (e.g., praise, compliments, encouragement), students are reinforced on learning a new behavior. Over time, as
the performance improves, the feedback occurs less frequently until only exceptional outcomes are reinforced. Over
time the behavior changes given the response to or removal of the stimulus. In the elementary school environment,
operant conditioning methods are often used for behavior modification. Behavior charts in which learners earn stickers
for displaying good behavior and have stickers removed for displaying bad behavior during the week is an example. A
reward or punishment is delivered at the end of the week based upon the number of stars accumulated or removed. The
rewards for learners might be a class party, or the punishment might be taking away privileges.

Behaviorist theory informs key aspects of the instructional design process such as the task analysis. The task analysis
involves identifying observable behaviors or steps learners need to take to achieve the desired learning outcome. A
designer often observes learners from various expertise levels completing the task to create a thorough task analysis to
inform the design of instruction. Behaviorism has been criticized due to the emphasis on external behaviors only, which
led to the development of a new learning theory in the mid-1900s.

Cognitivism

A contrast to the external nature of behaviorism is the internal natured cognitivism learning theory. Cognitivism focuses
on how the brain internally processes, retains, and recalls information based upon how the learner organizes
information into existing knowledge schemas. Schemas are structures of existing information in the learner's mind. To
ensure new information is retained for recall, instruction can be designed to enhance the probability that the new
information will be added to the learner's existing schema. For instance, if the desired learning outcome is to explain the
water cycle, then the instructor may use questions to have learners recall information in their existing schemas about
water and weather by having them tell stories about storms, clouds, lakes, and oceans. Once they have activated those
schemas, the instructor could then relate the new information about the water cycle to the stories they told, in order to
help learners integrate this new information into their existing knowledge about water.

A common tool used by cognitivist learning theorists are taxonomies of learning outcomes that specify what mental
processes are relied upon for various types of learning. Perhaps one of the more well-known and used taxonomies is
Bloom's taxonomy (1956), which was later revised (Anderson & Krathwohl, 2001). The revised taxonomy has six levels:
remember, understand, apply, analyze, evaluate, and create (see Figure 1). Using this taxonomy to identify the level of
desired learning can assist in writing learning objectives, selecting appropriate instructional methods, and designing
assessments to increase the probability that the desired learning outcome is achieved. The taxonomy relies on the use
of action verbs to ensure learning outcomes are measurable. Many resources such as this one from the University of
Nebraska-Lincoln provide a variety of verbs to use for each level of the taxonomy (Anderson & Krathwohl, 2001,
available at https://executivevc.unl.edu/documents/4-Revised-Blooms-Taxonomy.pdf ).

Figure 1

Bloom's Revised Taxonomy

                                                                                       210
For example, if the desired learning outcome were for a student to solve a simple algebraic equation, that would fall
under the application level of Bloom's revised taxonomy because the learners will apply previously learned concepts to
solve the problem. The instructor may use the suggested verbs (eLearning Heroes, 2020, available at
https://community.articulate.com/articles/blooms-taxonomy-interactive-examples) to write a clear instructional
objective such as "given an algebraic equation, the learner will solve the equation by selecting the appropriate method,
showing work, and checking the solution." Next, the instructor would design assessment items that measure the
attainment of that objective. In this case several equations would serve as assessment items (i.e. x + 5 = 7, x - 8 = 12, 7
+ x = 9). Lastly, the instructional methods would be designed to align with the objective and assessment. Here,
presenting examples with and without manipulatives, and practice problems with and without manipulatives, would be
appropriate.

Cognitivism also brought about the shift from learning theory to instructional theory, which focused on the design of
instruction instead of how learners process information or learners' behavior. This is an important shift that provided
the foundation for the instructional design field. In 1971, a revolutionary project entitled TICCIT, an acronym for Time-
shared, Interactive, Computer-Controlled Information/Instructional Television, was funded by the National Science
Foundation and MIT research corporation to test computer-assisted delivery of instruction using a cognitive approach.
This project produced learner-controlled instruction that was adaptive to learner choices (Gibbons & O'Neil, 2014). Other
projects followed that similarly sought to apply new cognitive theories to emerging educational technologies, leading to
the explosion of computer-assisted instruction applications.

Constructivism

Cognitivism added a new perspective based upon research of brain functionality during the learning process. However,
another learning theory gained attention in the mid-1990s, which combined learner's interactions with the external

                                                                                       211
environment and their internal learning process: constructivism. Constructivism is divided into two major schools of
thought: cognitive constructivism and social constructivism.

Cognitive constructivism is based upon the work of Dewey (1938), Bruner (1966), and Piaget (1972). This theory
revolves around the concept that learners construct their knowledge through individual personal experiences. For
example, when learners are exploring complex concepts through project-based learning, some learners may grasp the
concepts quickly while others may struggle. Facilitating knowledge development through probing questions to help
learners identify where they are having difficulty is part of an inquiry method to alleviate misinterpretation. It can also
help learners reflect on their knowledge, misconceptions, and progress. Anchored instruction is an example of a
cognitive constructivist theory that incorporates instructional technology such as video (Bradsford et al., 1990).
Anchored instruction suggests that learning is anchored in a realistic, evolving context with guiding resources available
to help the learners solve the instructional problem presented. The Adventures of Jasper Woodbury is a mathematics
video series that was designed using the anchored instruction theory (Cognition and Technology Group at Vanderbuilt,
1992, available at http://jasper.vueinnovations.com/).

Social exchange and collaboration are foci of the social constructivist theory grounded in the work of Vygotsky (1978).
A major theme of social constructivist theory is that social interaction plays a fundamental role in the development of
cognition. Vygotsky postulated that cultural development happens twice, first on the social level (between people), then
later on the individual level (inside the mind). One example of social constructivist theory is the development of
language. If you are building a house, you may have basic language skills but may be unaware of terms associated with
construction. As you continue to work with your peers, you begin to learn various tools and terms associated with
construction through your interactions with them. Think about learning another language. Language mobile applications
now offer the ability to have conversations with a native speaker electronically. This social interaction allows learners to
first hear and engage with correct grammar and pronunciation. Over time, the learner can begin to process and think in
another language, using proper grammar and pronunciation.

This perspective deepens our experiences in the world and aids our construction of new knowledge through the
exchange of ideas with others. Often group activities such as projects, experimentation, and discussions are utilized.
Learners engage with the content and then decompress with one another to develop or construct meaning from various
activities. The teacher acts as a guide or translator by setting up the instruction to allow the learners to explore
concepts. As the learners explore the concepts, the teacher then assists the learners in translating what they have
found into the learner's current state of understanding.

Quest Atlantis is an example of an instructional design and technology product based on social constructivist theory
(Barab et al., 2005). The goal of Quest Atlantis was to provide an immersive learning environment that combined
academics and play with interdisciplinary cultural quests that supports learning, development, and social
transformation. Players created a persona and by completing quests they engaged in educational activities while
interacting with other users and mentors. The authors described the design as socially responsive because the quests
adapted to the decisions of the players.

Connectivism

Early in the 21st Century, a new learning theory emerged from the digital age: connectivism. Connectivism is based on
the work of Siemens (2004) and is the first theory that defines learning as more than an internal and individual process
(see https://edtechbooks.org/lidtfoundations/connectivism for a republishing of this article). The connectivist theory
posits that learning takes place when learners make connections between ideas located throughout personal learning
networks (e.g., other individuals, databases, social media, Internet, learning management systems). The connection of
the right individuals to the right resources can enhance the learning for all within the network.

Technology increases learners' access to information and their ability to be a part of a greater learning community
(Siemens, 2004). There are premises around connectivism. One premise is that learners need to distinguish between

                                                                                       212
important and unimportant information, as well as valid information, since there is a continuous flow of new
information. If we go back to the house example, you are working on building your house, and you want to install a
fireplace. You can go to the Internet and join a builder's community on YouTube or a Do-It-Yourself (DIY) forum. You may
also be able to access reviews for various types of fireplaces and what has worked and what has not. Once you have
built the fireplace, you can share your experience with these communities to enhance the experiences of others.
To summarize the four families of thought on learning theories, Figure 2 identifies some possible instructional methods
for each learning theory presented so far.
Figure 2
Methods Used for Learning Theories Adapted from Morrison (2013)

   Additional Readings and Resources

        1. Foundations of Learning and Instructional Design Technology Book - Chapters 9, 10, 11 and 19. (West,
           2018)

        2. Learning Theory and Instructional Design (Mcleod, 2003)
        3. Understanding the practices of Instructional Designers through the lens of different Learning Theories (Yeo,

           2013)
        4. How People Learn I (National Research Council, 2000)

                                                                                       213
Social Learning Theories

As noted above, interaction with both individuals and the environment is embedded in learning theories. From these
types of interactions, multiple social learning theories emerged during the late 1990s that enhanced or deepened some
of the ideas from the major families of thought around learning at that time. We will discuss the following social
learning theories: social cognitive theory, social development theory, collaborative learning, and cooperative learning.

Social Cognitive Theory

Social cognitive theory teaches that people learn by observing others and is based upon the work of Bandura (1986). He
believed that people construct knowledge from learning from others' experiences. By observing others' behavior,
learners derive conceptions regarding the behavior being modeled. This observation can happen directly or through the
media. Reflection is a crucial component of this theory as once the learner observes the action, they reflect and
determine whether this is something they want to incorporate or use. Four processes coincide with observational
learning techniques: attention, retention, reproduction, and motivation. Within the social cognitive theory, motivation is
seen as depending upon one's self-efficacy and agency. In order to proceed through all four processes, the learner must
have the confidence to exhibit control over a desired behavior or self-efficacy. Social cognitive theory is rooted in the
view of human agency in which individuals are agents proactively engaged in their development and can make things
happen through individual actions. For example, if a learner struggles with learning a particular behavior or task,
allowing the learner to work with another person that has mastered the behavior or task will allow the learner to view
how the ideal behavior or task is performed successfully.

Collaborative Learning

Instructors and designers sometimes want learners to work together to construct new knowledge deliberately.
Collaborative learning is a social learning theory that involves learners grouping themselves together to explore a
concept or to work on a project collectively. Collaborative learning is a loosely structured, discovery learning approach in
which learners have much control. It is an "umbrella" term that encompasses a variety of educational approaches
involving joint efforts by learners working together. Group members capitalize on the skills of one another through the
sharing of information and ideas that build towards a common group goal.

Cooperative Learning

Cooperative learning is a carefully structured type of collaborative learning. In both of these social learning theories, the
instructor's role is that of facilitator, and the tasks for the groups should be open-ended and complex. Cooperative
learning is rooted in social interdependence theories (Deutsch, 1949; Lewin, 1935). Johnson and Johnson (1989)
conducted extensive research on defining the parameters of cooperative learning, which requires these five
components: interaction, positive interdependence, group processing, individual accountability, and social skills. In
other words, groups need to interact, depend on one another, monitor their progress, be responsible for their work, and
be able to work together. For example, a team research project could require each team member to find several
resources, and an annotated bibliography of those resources could be submitted individually (individual accountability).
The team could then co-write and edit the research paper with all of the resources (interaction, social skills, and positive
interdependence). The group could use a cloud-based text editor to ensure all team members are contributing in a
timely fashion (group processing). Cooperative learning requires intentional planning by the instructor or the designer to
ensure all five components are present.

                                                                                       214
   Additional Readings and Resources

        1. Collaborative vs. cooperative learning video (wufei87, 2018)
        2. Social Cognitive Theory video (Bandura, 2010)

Motivational Theories

Keeping learners motivated and engaged is just as important as understanding how they learn best. Therefore,
motivation and engagement theories are essential to include when discussing learning theory. We will discuss three
motivation theories (self-determination, hierarchy of needs, ARCS), and one engagement theory (flow).

Self-Determination Theory

Self-determination theory is a motivational theory that suggests learners can become self-determined when their needs
for competence, connection, and autonomy are satisfied (Deci & Ryan, 1985). Self-determination theory views
internalization as a process for transforming external regulations into internal regulations and thereby integrating them
into one's self (Deci, Eghrari, Patrick, & Leone, 1994). Social support, along with intrinsic and extrinsic motivators are
important factors for developing self-determination. Extrinsic motivators can hinder self-determination, whereas
intrinsic motivators can enhance self-determination. Intrinsic motivators such as joy and self-fulfillment allow learners
to be autonomous and engage with learning. When learners complete their work or a challenge, they feel competent.
Both competence and autonomy are components necessary to maintain intrinsic motivation. Extrinsic motivators can
hinder self-determination, whereas intrinsic motivators can enhance self-determination. External motivators, such as
being rewarded for making an A on a test, can hinder learning. Social support should be considered over extrinsic
rewards to foster self-determination. For example, ensuring every member of a team can play a role and understand
their contributions are valuable. Methods to complete that could be establishing roles based on team member talents
and providing positive feedback. Allowing individual learners and teams to set their own learning goals can also be
beneficial. Another example of utilizing intrinsic motivators is giving learners an assignment where learners teach the
concepts to other learners (internal) rather than teaching the learners to take a test on the concepts (external). This type
of motivation is fostered and encouraged by fostering autonomous support for the learners rather than controlling.

Maslow's Hierarchy of Needs

Creating an autonomous environment may not always motivate learners as there are basic needs that need to be in
place before learners can begin to move in the direction of self-fulfillment. Maslow's hierarchy of needs (Maslow, 1943)
is a second motivational theory. Maslow stated that some needs take precedence over others, such as basic needs for
survival. Maslow developed a hierarchy stating the needs at the bottom should be met first and then move their way up
(see Figure 3).
Figure 3
Maslow's Hierarchy of Needs

                                                                                       215
At the bottom of the pyramid are the physiological needs such as air, food, shelter. Next is safety needs, such as
protection from the elements, order, and freedom from fear, followed by love and belongingness. Next, are esteem
needs, which are achievement, mastery, and the desire for reputation or respect from others. Finally, the self-
actualization needs are realizing personal potential or the ability or desire to become capable. Although the order of the
needs seems rigid, they are flexible, depending on the external circumstances or individual differences. For example, if a
learner is concerned about where they are going to sleep or eat that night, they will not be as inclined to learn new
concepts as their basic needs are not met. However, if a learner who is well-fed and is loved and has a sense of
belonging, whether it is part of a social group or family, they are more inclined to strive to learn new concepts.

Keller's ARCS Model

Within the motivational theories, there are models that provide guidance to assist designers in planning to ensure
learners' motivation. For example, Keller's Attention, Relevance, Confidence, and Satisfaction (ARCS) is a motivational
model that can be used to guide instructional planning to be intentionally motivational for learners (Keller, 1987). This
model focuses on promoting and sustaining motivation throughout the learning process. First, gain the attention of the
learner by piquing their curiosity. Games, roleplay, humor, or the use of inquiry are all techniques to gain learner
attention, particularly when introducing a new concept. Next, to increase the learner's motivation, relevance needs to be
established. To establish relevance, you need to present the worth of knowledge gained, what does it mean to the
learner? How will this knowledge directly affect the learner? Next, provide confidence and give the learners control over
their learning while providing feedback. The instructor can achieve this by providing the learner's opportunities for short
term wins and small steps of growth during the learning process. Finally, the learning needs to be rewarding or
satisfying in some way, either from a sense of achievement or external means; however, without patronizing the learner
through over-rewarding easy tasks.

                                                                                       216
Flow

Once a learner's attention is gained, the instructor or designer's focus should turn to keep the learner engaged. Flow is
an engagement theory that is sometimes described as "being in the zone." Flow was defined by Csíkszentmihályi
(1990), who was inspired by watching artists, athletes, chess players, and others who become immersed in completing
tasks. Flow tends to happen when someone is engaged in an activity they enjoy, either due to their skill level or other
intrinsic stimuli. Csíkszentmihályi defined 10 components of flow, but not all 10 have to happen for flow to occur. These
10 components are: (1) clear and challenging goals, (2) strong concentration, (3) intrinsic motivation, (4) serenity
feeling, (5) timelessness, (6) immediate feedback, (7) a balance between challenge and skill level, (8) feeling of control,
(9) loss of awareness of other needs, and (10) complete focus. To create flow for learners, designers should allow
some choice of activity to build on the learner's strengths and interests and strive to match and personalize the
challenge level of the learning to the learner's abilities. "Genius Hour" (West & Roberts, 2016) is an example of applying
Flow theory to education. In this approach, learners are given an hour each day, or every other day, to be "geniuses" in
whatever topic they are excited about. They work for an extended period of time to complete a major project in their
area before sharing their ideas with the class or families. These types of projects often produce substantial learning
benefits by encouraging conditions where learners are more likely to be in flow.

Conclusion

Learning theories and models are tools that help to shape and guide learning. Like decorating a living room in a new
house, various tools can be employed to move an empty room to one with a functional design and a pleasing look and
feel to the designer and client. Instructional designers can rely on learning theories and models to design learning
solutions that meet the needs of their clients. The theories and models also give designers language and structure to
communicate their designs and research to give evidence that their designs will be effective. Consider if you were the
client that bought the house and received several proposals from interior designers for the living room decorations.
Proposal 1 was a diagram and a budget. Proposal 2 had a narrative description that justified the attached diagram and
budget. The justification was based on their interior design philosophy and detailed how the diagram would prove to be
functional for the client. Provided the design philosophies match, you would probably select proposal 2. Using
instructional design theories and models helps guide your design or learning solution and helps justify your design
solution as an effective one for potential clients.

   Additional Readings and Resources

        1. Foundations of Learning and Instructional Design Technology Book - Chapters 12, 13, 14 and 16. (West,
           2018)

        2. Development and use of the ARCS model in instructional design article (Keller, 1987)
        3. Flow TED talk (Csikszentmihalyi, n.d.)
        4. Edward Deci--Self-determination theory (Deci, 2017)

   Application Exercises

        1. Create a reference guide or chart of theories, characteristics, methodologies, and how you may best apply
           them to your own design context and situation.

        2. Create a timeline of the evolution of learning theories.

                                                                                       217
References

Anderson, L.W. & Krathwohl, D.R. (2001). A taxonomy for teaching, learning, and assessing: A revision of Bloom's
         taxonomy of educational objectives. New York, NY: Longman

Bandura, A. (1986). Social foundations of thought and action: A social cognitive theory. Englewood Cliffs, NJ: Prentice-
         Hall.

Bandura, A. (2010, June 22). Bandura's Social Cognitive Theory: An Introduction (Davidson Films, Inc.) [Video file]
         Retrieved from https://youtu.be/S4N5J9jFW5U

Barab, S., Thomas, M., Dodge, T., Carteaux, R., & Tuzun, H. (2005). Making learning fun: Quest Atlantis, a game without
         guns. Educational Technology Research and Development, 53(1), 86-107. https://doi.org/10.1007/BF02504859

Benware, C.A., and Deci, E.L. (1984). Quality of learning with an active versus passive motivational set. American
         Educational Research Journal, 21, 755-65. https://doi.org/10.3102/00028312021004755

Bloom, B.S. (1956). Taxonomy of educational objectives: The classification of educational goals. New York, NY:
         Longmans, Green.

Bruner, J. S. (1966). Toward a theory of instruction, Cambridge, Massachusetts: Belkapp Press.

Cognition and Technology Group at Vanderbilt. (1992). The Jasper experiment: An exploration of issues in learning and
         instructional design. Educational Technology Research and Development, 40, 65-80.
         https://doi.org/10.1007/BF02296707

Csikszentmihalyi, M. (1990). Flow: The Psychology of Optimal Experience. Journal of Leisure Research, 24(1), 93-94.
         https://doi.org/10.1080/00222216.1992.11969876

Csikszentmihalyi, M. (n.d.). Flow, the secret to happiness. Retrieved from
         https://www.ted.com/talks/mihaly_csikszentmihalyi_flow_the_secret_to_happiness?lang%5C

Deci, E. L., & Ryan, R. M. (1985). Intrinsic motivation and self-determination in human behavior. New York, NY: Plenum.

Deci, E. L., Eghrari, H., Patrick, B. C., & Leone, D. R. (1994). Facilitating internalization: The self-determination theory
         perspective. Journal of Personality, 62(1), 119-142. https://doi.org/10.1111/j.1467-6494.1994.tb00797.x

Deci, E. L., (2017, October 17). Edward Deci--Self-Determination Theory [Video file] Retrieved from
         https://youtu.be/m6fm1gt5YAM

Dewey, J. (1938). Experience and education. New York, NY: Collier.

E-Learning Heroes. (2020). Learn About Bloom's Taxonomy with These Interactive Examples #141. [online] Available at:
         https://community.articulate.com/articles/blooms-taxonomy-interactive-examples [Accessed 14 Feb. 2020].

Gibbons, A., & O'Neal, A. (2014). TICCIT: Building theory for practical purposes. International Journal of Designs for
         Learning, 5(2). https://doi.org/10.14434/ijdl.v5i2.12894

Keller, J. (1987). Development and use of the ARCS model of motivational design. Journal of Instructional Development,
         10(3), 2-10.https://doi.org/10.1007/BF02905780

Johnson, D. W., & Johnson, R. T. (1989). Cooperation and competition: Theory and research. Interaction Book Company.

Lewin, K. (1935). A Dynamic Theory of Personality. New York: McGraw Hill.

Maslow, A. H. (1943). A theory of human motivation. Psychological Review, 50(4), 370-96.
         https://doi.org/10.1037/h0054346

                                                                                       218
McLeod, G. (2003). Learning theory and instructional design. Learning Matters, 2(1), 35-43.
Morrison, D. (2013). How course design puts the focus on learning not teaching. Retrieved from

         https://onlinelearninginsights.wordpress.com/2013/05/15/how-couse-design-puts-the-focus-on-learning-not-
         teaching/
National Research Council. (2000). How people learn: Brain, mind, experience, and school: Expanded edition. National
         Academies Press.
Pavlov, I. (1960). Conditioned reflexes; an investigation of the physiological activity of the cerebral cortex. New York:
         Dover Publications.
Piaget, J. (1972). The psychology of the child. New York, NY: Basic Books.
Siemens, G. (2004). "Connectivism: a learning theory for the digital age", Retrieved from
         www.elearnspace.org/Articles/connectivism.htm.
Skinner, B. F. (1953). Science and Human Behavior. New York, NY: Macmillan.
Thorndike, E. L. (1898). Animal intelligence: an experimental study of the associative processes in animals. The
         Psychological Review: Monograph Supplements, 2(4), https://doi.org/10.1037/h0092987
Vygotsky, L. S. (1978). Mind in society: The development of higher psychological processes. Cambridge, MA: Harvard
         University Press.
Watson, J. B. (1913). Psychology as the behaviorist views it. Psychological Review, 20(2), 158.
         https://doi.org/10.1037/h0074428
West, R. E. (2018). Foundations of Learning and Instructional Design Technology: The Past, Present, and Future of
         Learning and Instructional Design Technology (1st ed.). EdTech Books. Retrieved from
         https://edtechbooks.org/lidtfoundations
West, J. M., & Roberts, K. L. (2016). Caught up in curiosity: Genius hour in the kindergarten classroom. The Reading
         Teacher, 70(2), 227-232. https://doi.org/10.1002/trtr.1497
wufei87. (2018). Retrieved 25 February 2020, from https://www.youtube.com/watch?v=uwvtfYa169k
Yeo, S.S. (2013). Understanding the practices of instructional designers through the lenses of different learning theories
         (unpublished master's thesis). Bowling Green State University, Career and Technology Education/Technology,
         Bowling Green, OH. retrieved from http://rave.ohiolink.edu/etdc/view?acc_num=bgsu1367862206

                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/learning_theories.

                                                                                       219
220
  17

The Role of Theory in Instructional Design

Some Views of an ID Practitioner

Trudy K. Christensen

   Editor's Note

     This is a version of Christensen, T. K. (2008). The role of theory in instructional design: Some views of an ID
     practitioner. Performance Improvement, 47(4), 25-32. https://doi.org/10.1002/pfi.199, and has been
     republished here with permission from the publisher.

        This article describes how an experienced instructional designer thinks about and uses learning theories
        to inform instructional design decisions. It uses a vision metaphor to provide a simple heuristic framework
        for identifying the nature of instructional problems and relating different types of problems to useful
        theoretical perspectives, methods of instructional analysis, and assessment strategies. Finally, it provides
        a synopsis of major learning theory perspectives and situations that could be addressed by applying
        models and strategies representing the different theoretical perspectives.
There has been considerable discussion in recent years about the role that learning theories play in instructional design
practice (Wilson, 2005; Christensen & Osguthorpe, 2004; Reigeluth, 1999; Hannafin, Hannafin, Land, & Oliver, 1997;
Bednar, Cunningham, Duffy, & Perry, 1992). Do instructional designers actually think about and apply the theories they
learned in college? Are these theories really useful? In a recent survey of over 100 instructional designers, about half of
the designers indicated that they regularly use specific learning or instructional theories or research to make
instructional strategy decisions (Christensen & Osguthorpe, 2004). This study did not reveal, however, when and how
instructional practitioners use these theories. What theories do they think about? How do they think about the theories?
Do they use more than one theory at a time? How do they use these theories to inform their decisions? After almost 25
years of instructional design practice, I have developed some ways of thinking about learning theories that have proven
useful for me. I have shared these ideas with students and other designers over the years, and many have found them
helpful. I share them here as a type of think-aloud exercise, not to claim they are the only way to think about and apply
theory to instructional practice but as a type of heuristic that might help novice designers. I encourage other
experienced practitioners to reflect on and record their strategies for applying theory to practice as a means of
documenting and comparing best practices.

                                                                                       221
When I design instruction, I do not usually start with a particular theory. My main focus is the problem and the problem
situation. I start by considering the nature of the instructional problem and then ask: "What theory or models would be
most useful and appropriate to help address this problem?" Deciding on a theoretical perspective early in the design
process not only helps later when it comes to designing the instruction, but also serves as a guide for deciding how to
analyze the learning tasks or content and how to assess learning.

What Is the Nature of the Problem?

I have found it useful to use a vision metaphor when considering the nature of instructional problems. I decided on this
metaphor because I am very conscious of vision issues since I have poor eyesight and come from a family of eye
doctors. One time I visited my eye-doctor brother for help with my worsening vision. In the process of discussing what
would be the best solution for my problems, he mentioned that he is continually confronted with a range of trade-offs
and alternatives when trying to come up with the most appropriate prescription for his patients. He described one
occasion when a woman walked into his office with a thread and needle and simply stated, "Doctor, I want glasses that
will help me thread this needle." That was all she seemed to care about. She had specific and measurable criteria for the
solution, and in no time he was able to determine the most appropriate prescription for her.

My brother admitted that these kinds of cases are the most straightforward to solve. But most often he must devise an
all-purpose prescription that will allow his patients to perform in many situations--some known, but most of them
unknown. This is when it is more difficult to determine the appropriate solution. It is usually less clear in these cases
what the optimal solution should be because it is impossible to evaluate the adequacy of the prescription in all the
potential situations where the glasses may be needed.

I often think of instructional problems according to the continuum shown in Figure 1. On one end of the continuum are
problems that are usually fairly easy to describe: the nature of the task can be defined, and the conditions under which it
must be performed can be specified. I call these training problems. On the other end of the continuum are the problems
that may require a more all-purpose prescription, where it is not possible to define or anticipate all the task
requirements or the conditions under which the tasks may need to be performed. I refer to these types of problems as
education problems. The importance of evaluating the overall goal or nature of an instructional problem at the outset
should not be underestimated. Other designers have advocated a similar strategy. For instance, Wilson, Teslow,
and Osman-Jouchaoux (1995) advise:

Distinguish between educational and training goals. Acknowledge that education and training goals arise in every
setting. Schools train as well as educate; and workers must be educated--not just trained in skills-- to work effectively
on the factory floor. Discerning different learning goals in every setting provides a basis for appropriate instructional
strategies. (p. 149)

I refer to the middle of the instructional problem continuum as the preparation domain. Problems that fall near the
center are not as focused or easily measured as the training problems, but they still represent more readily definable
ranges of needs than the education problems. For example, using the vision metaphor, if someone came to the doctor
and asked for help passing the driver's vision exam so that she would later be able to drive, that would be a preparation
problem. Preparation problems represent an intermediate range of goals--ones that may be necessary to achieve the
more application-oriented ends of training and education. Preparation goals undergird or provide critical prerequisite
skills or knowledge for training and education. Clearly many instructional problems have elements of all of these
instructional goals, but I try to identify the overriding goal, the goal with the highest priority, in the problem situation I am
addressing. This helps me focus and optimize my efforts throughout the remaining design process.

                                                                                       222
How Does Learning Theory Relate to the Different Types of
Problems?

It is important to remember that unlike the field of physics, which has been fairly successful in finding unifying theories
to help guide work in that area, there is no one unifying theory of learning or instruction. Many theories have been
devised over the years, with varying degrees of success in guiding practice. As these theories prove inadequate to
explain or help with some types of learning, they usually fall out of favor. This is just what Thomas Kuhn (1996) in his
famous treatise, The Structure of Scientific Revolutions, would predict. Most often there is a current favorite theory or
paradigm that guides practice in education. However, we should not be so eager to use a particular theory just for the
sake of being current. We may be ignoring or overlooking some effective and important strategies for the situation at
hand. Many of the earlier learning paradigms and theories (e.g., behaviorism) are still useful for certain types of learning
problems.
Figure 1
Continuum of Instruction Problems

Many theories and models of learning and instruction have been developed over the years--so many that it is often
difficult to assimilate and remember them all, let alone use them to help guide instructional practice. Fortunately, some
educational psychologists group these theories and practices into three or four main categories. For instance, Ormrod
(2008) categorizes learning theories according to three main perspectives of learning: cognitive psychology,
behaviorism, and social cognitive theory. Woolfolk (2007) describes four main views of learning: behavioral, information
processing, psychological/individual constructivist, and social/situated constructivist views.
Figure 2 summarizes my synthesis of different learning theory perspectives as they relate to instructional design and
metaphors that I have found useful. The figure also lists assumptions regarding the nature of knowledge underlying
each perspective, as well as the role of the instruction, the role of the learner, and the main instructional and
motivational strategies suggested by these perspectives. When reading about a new idea or model, I ask myself, "What
are the assumptions underlying this model, and where would it fit under these major theoretical perspectives?" Finally, I
ask, "Does this theory or model reveal a useful new idea that distinguishes it from others?" Then I try to remember that
idea so I can apply it in my designs if appropriate.
Figure 3 shows the theoretical perspectives that seem to have the most to say about each type of problem. Training
problems, as I have defined them, represent more limited, specific behaviors or tasks to be completed under definable
conditions. Therefore, many of the strategies underlying behaviorism are still very useful to help address these types of
problems. In addition, some aspects of social cognitive theories (e.g., Bandura, 1986), such as vicarious reinforcement
and modeling, can be useful to address many training problems. Learners benefit from seeing others model specific
tasks or behaviors and noting the consequences of correct or incorrect actions.
Figure 2
Major Learning Theory Perspectives and IImplications for Instructional Design

                                                                                       223
Figure 3
Instruction Problem Types and Related Goals, Theoretical Perspectives, Types of Instructional Analysis, and
Assessment Strategies

                                                                                       224
Many preparation problems may be addressed by strategies suggested by an information-processing perspective that
focuses on the capabilities and limitations of human memory and cognitive processes. These theories (e.g., Norman,
1982) suggest strategies for presenting and chunking information for optimal encoding and retrieval, assimilating new
information into existing schemas, and encouraging and enhancing meaningful learning. In addition, as social cognitive
theory suggests, learners may benefit from having others model how to apply effective learning strategies for
remembering, understanding, and extending ideas.

Constructivist approaches, both cognitive and social, provide strategies for addressing many education problems. They
suggest ways of helping learners develop expertise and problem-solving skills to function effectively in complex, social,
unpredictable, and nuanced real-world environments. In addition, education problems may be addressed by
emphasizing aspects of social cognitive theory that focus on strategies for helping students become self-regulated
learners--able to define problems effectively, identify possible solutions, predict consequences, choose best solutions,
identify how to carry out the solution, implement solutions, and evaluate results (Bandura, 1986).

To some, this high-level synopsis may sound like a gross overgeneralization, but it helps me address a vast and
complex array of theories, models, and strategies. This high-level approach is just a heuristic. When I design, I still may
draw from multiple theories or perspectives to address a particular problem. For instance, I might decide to use a
particular social constructivist strategy such as team-based learning (Michaelsen, Knight, & Fink, 2004) as my main
strategy to help students learn to solve real-world problems by coconstructing knowledge as learning teams. But in the
process of implementing this approach, I might also incorporate the use of incentives, such as posting team scores or
giving extra points for exceptional team performance, motivational strategies derived from behaviorist theories.

                                                                                       225
How Do the Theoretical Perspectives and Problem Types Guide
My Design?

It is important to remember that there is no formula for great design. By definition, this is a problem-solving process
that cannot be described step-by-step. Nevertheless, Figure 3 summarizes what I think the major theoretical
perspectives and learning goals generally imply for the type of assessment strategies and analysis techniques most
appropriate for each problem type. The instructional analysis strategies listed--job, procedural, and skill analysis;
content analysis; learning analysis; cognitive task analysis; and activity analysis--reflect the main categories of analysis
outlined by Jonassen, Tessmer, and Hannum (1999). Job, procedural, and skill analyses are frequently the best
approaches to use for training problems because they involve creating specific task, skill, or procedural descriptions as
they relate to an organizational context or larger system. For preparation problems, content and learning analyses,
which focus on hierarchical relationships among concepts, principles, tasks, or behaviors, are useful strategies for
analyzing content or skill domains that may be prerequisites to training or problem-solving tasks. These approaches
focus more on ways of representing content for optimal retention and retrieval rather than sequencing tasks for actual
performance. Finally, cognitive task analysis (CTA) techniques and strategies are best suited to capture the explicit and
implicit knowledge that experts use to perform complex tasks (Clark, Feldon, van Merriënboer, Yates, & Early, 2007).
CTA is often used to create expert systems and complex simulations. In addition, activity analysis, based on activity
theory, is frequently used to analyze education problems. Activity analysis focuses on understanding the rich contexts
in which people live and work; it is used to examine the activities in which experts engage, the tools they use, and the
social context and interrelationships among participants in real-world environments (Jonassen et al., 1999).

Thinking in terms of the instructional problem continuum shown in Figure 3 also helps to identify useful assessment
strategies. Training problems generally lend themselves to performance assessment or mastery-testing strategies.
Performance assessment "is assessment based on observation and judgment" (Stiggins, Arter, Chappuis, & Chappuis,
2004, p. 191). The main goal in using performance assessment is to describe a skill or task and the criteria that will be
used to judge the performance. This type of assessment may use checklists, rating scales, or rubrics to measure
achievement or mastery.

Since it is often not practical or even possible to test mastery of large areas of underlying content knowledge or
expertise, preparation problems are frequently assessed by sampling from a domain of potential terms, concepts, and
principles that represent critical content underlying an area of study. This approach is sometimes referred to as domain-
referenced assessment. According to the technical definition, domain-referenced assessment "requires the
specification of rules that determine membership in the domain and a procedure for sampling individual elements so
that inferences can be made from the sample to the domain" (Gipps, 1994, p. 82). I use this term more loosely to
describe an attempt to sample from a content domain by using a table of specifications or other forms of systematic
analysis to represent critical content in an area of study. Typically, domain-referenced assessment uses standard
written test item formats, such as multiple-choice, true-false, or short-answer questions, to test learner knowledge or
understanding. Finally, education problems lend themselves to alternative forms of assessment, including holistic
assessments, or portfolio or authentic assessments, where the goal is to measure the application of principles and
concepts through the production of outcomes or performance of behaviors in realistic, complex settings.

To illustrate how the three main problem categories I have defined could be applied to different settings and situations,
Table 1 shows how I might use the problem types to categorize representative situations, examples, and instructional
models.

Table 1  Example Situations, Applications, and Models Related to Different Types of Instructional Problems

         Types of Instructional Problems

         Training                         Preparation  Education

                                          226
When to use       To improve performance on a     To gain fluency in the              To know how and when to apply
                  specific job or task            vocabulary, concepts, skills, and   content or process knowledge
                                                  strategies of a particular subject  under differing circumstances
                  To know or learn skills to      area
                  achieve mastery                                                     To be able to solve a variety of
                                                  To promote in-depth                 unique problems
                  To know how to use a new        understanding of a subject
                  product, process, or skill to   matter or content domain            To learn how to work
                  some required level of mastery                                      cooperatively to solve problems
                  or proficiency                  To acquire critical prerequisite    in a given area
                                                  concepts necessary for
                  To achieve automaticity in a    performing a job or pursuing a
                  critical skill                  profession

                                                  To provide needed background
                                                  knowledge for completing a
                                                  task or solving a problem

Examples          To learn                        To learn about                      To learn

                  The features and functions      Human anatomy in                    How to diagnose a disease
                  of a new computer program       preparation for a health            How to conduct a technical
                  How to handle a new             care profession                     systems analysis for a large
                  machine                         Different types of computer         corporation
                  The steps of a new              networks to become a                How to apply principles of
                  development process             systems analyst                     physics to daily life
                                                  Various learning theories to
                                                  become an educator
                                                  Mathematical concepts and
                                                  principles in preparation for
                                                  a science career

Useful teaching   Bloom's mastery learning model Ausubel's meaningful reception       Cognitive
or instructional
models            (Bloom, 1976)                   learning (1978)                           Discovery learning (Bruner,
                                                                                            1966)
                  Programmed instruction          Gagné's theory of instruction
                  (Skinner, 1968)                 (1985)                              Social

                  Personalized systems of                                                   Cognitive apprenticeship
                  instruction (Keller, 1968)                                                (Collins, Brown, & Newman,
                                                                                            1989)
                                                                                            Goal-based scenarios
                                                                                            (Schank, 1992)
                                                                                            Problem-based learning
                                                                                            (Savery & Duffy, 1995)
                                                                                            Team-based learning
                                                                                            (Michaelsen et al., 2004)
                                                                                            Service-learning (Campus
                                                                                            Compact, 2003)

                                                  227
When Might It be Useful to Combine Approaches?

Now that I have differentiated problem types and theoretical approaches, I want to highlight a connective thread that
has emerged in recent years to help me tie these approaches together when circumstances allow. The emphasis on
situated cognition (Wilson & Myers, 2000) proposed by social-constructivist approaches to learning has implications for
combining strategies from the different learning perspectives. Situated cognition suggests that learning should be
taking place in the context in which it is used. Therefore, when I am creating learning environments to address
educational needs, I try to find ways of incorporating preparation activities into the setting where the problem solving
takes place. Strategies for accomplishing this goal include using simulations, apprenticeships, internships, service-
learning, and other approaches. The implication of this perspective for both training and educational goals has also led
to an emphasis on work-based, just-in-time learning. With the increasing speed and accessibility of electronic media,
this notion has become the basis of a new field emphasizing the design and development of electronic performance
support systems (Gery, 1991). Whenever possible, I watch for opportunities to use an electronic performance support
system or even non-electronic job aids to help learners achieve preparation objectives in training and educational
contexts.
If I were to show the implications of situated cognition on the problem continuum, I would show more preparation goals
being addressed at the ends of the continuum in the performance contexts, as illustrated by Figure 4. This means
learners would have ready access to important supporting skills or knowledge in the same context where they are
performing the training task or trying to solve problems.
Figure 4
Situated Cognition and the Problem Continuum

Summary and Implications for Practice

In summary, I use learning theory to guide design by first deciding on the nature of the instructional problem and the
main goal of the instruction. Then I decide which theoretical perspective or perspectives best match the needs of the
situation. Next, I often investigate specific teaching models that reflect the theoretical perspectives I have determined
are most useful and appropriate for addressing the problem. I may apply a particular model or simply rely on basic
strategies related to the different theoretical perspectives to help guide, inform, and justify my design. I also watch for
opportunities to use technology to help support and achieve preparation goals in performance contexts. HPT
professionals who use this type of process can balance instructional decisions with the time, cost, and contextual
constraints of the situation.

                                                                                       228
In reflecting on the use of theory in instructional design practice, I tend to concur with Wilson and Myers's (2000)
assessment of the way practitioners generally use theory:

        Most clinical psychologists are reportedly "eclectic" in their stance towards the various theories of
        psychotherapy. Many teachers and instructional designers take the same non-committal stance toward
        theory. They prefer a menu or toolbox metaphor instead of an application/consistency metaphor.
        Practitioners tend to be opportunistic with respect to different theoretical conceptions. This stance toward
        theory might be termed "eclectic" or "grab-bag," but we prefer to think of it as problem- or practitioner-
        centered. People rather than ideologies are in control. The needs of the situation rise above the dictates of
        rules, models, or even standard values. (p. 82)

References

Ausubel, D.P. (1978). Educational psychology: A cognitive view. New York: Holt.

Bandura, A. (1986). Social foundations of thought and action: A social cognitive theory. Upper Saddle River, NJ: Prentice
         Hall.

Bednar, A.K., Cunningham, D., Duffy, T.M., & Perry, J.D. (1992). Theory into practice: How do we link? In T.M. Duffy &

D.H. Jonassen (Eds.), Constructivism and the technology of instruction: A conversation (pp. 17-34). Mahwah, NJ:
         Erlbaum.

Bloom, B.S. (1976). Human characteristics and school learning. New York: McGraw-Hill.

Bruner, J.S. (1966). Toward a theory of instruction. Cambridge, MA: Harvard University Press.

Campus Compact. (2003). Introduction to service-learning toolkit: Readings and resources for faculty (2nd ed.).
         Providence, RI: Brown University.

Christensen, T.K., & Osguthorpe, R.T. (2004). How do instructional-design practitioners make instructional-strategy
         decisions? Performance Improvement Quarterly, 17(3), 45-65.

Clark, R.E., Feldon, D., van Merriënboer, J., Yates, K., & Early, S. (2007). Cognitive task analysis. In J.M. Spector, M.D.
         Merrill, J.J.G. van Merriënboer, & M.P. Driscoll (Eds.), Handbook of research on educational communications and
         technology (3rd ed.). Mahwah, NJ: Erlbaum.

Collins, A., Brown, J.S., & Newman, S.E. (1989). Cognitive apprenticeship: Teaching the crafts of reading, writing, and
         mathematics. In L.B. Resnick (Ed.), Knowing, learning, and instruction: Essays in honor of Robert Glaser (pp.
         453-494). Hillsdale, NJ: Lawrence Erlbaum.

Driscoll, M.P. (2005). Psychology of learning for instruction (3rd ed.). Boston: Pearson Education.

Gagné, R.M. (1985). The conditions of learning and theory of instruction (4th ed.). Fort Worth, TX: Harcourt Brace.

Gery, G. (1991). Electronic performance support systems. Boston: Weingarten.

Gipps, C.V. (1994). Beyond testing: Towards a theory of educational assessment. Bristol, PA: Falmer Press.

Hannafin, M.J., Hannafin, K., Land, S.M., & Oliver, K. (1997). Grounded practice and the design of constructivist learning
         environments. Educational Technology Research and Development, 45(3), 101-117.

Jonassen, D.H., Tessmer, M., & Hannum, W.H. (1999). Task analysis methods for instructional design. Mahwah, NJ:
         Erlbaum.

                                                                                       229
Keller, F.S. (1968). "Goodbye, teacher . . ." Journal of Applied Behavior Analysis, 1, 79-89.
Kuhn, T. S. (1996). The structure of scientific revolutions (3rd ed.). Chicago: University of Chicago Press.
Michaelsen, L.K., Knight, A.B., & Fink, L.D. (Eds.). (2004). Team-based learning: A transformative use of small groups in

         college teaching. Sterling, VA: Stylus Publishing.
Norman, D.P. (1982). Learning and memory. New York: Freeman.
Ormrod, J.E. (2008). Educational psychology: Developing learners (6th ed.). Upper Saddle River, NJ: Pearson Prentice

         Hall.
Reigeluth, C.M. (1999). What is instructional-design theory and how is it changing? In C.M. Reigeluth (Ed.), Instructional-

         design theories and models (Vol. 2, pp. 5-29). Mahwah, NJ: Erlbaum.
Savery, J.R., & Duffy, T.M. (1995). Problem based learning: An instructional model and its constructivist

         framework. Educational Technology, 35(5), 31-38.
Schank, R. (1992). Goal-based scenarios (Institute for the Learning Sciences Technical Report 36). Evanston, IL:

         Northwestern University.
Skinner, B.F. (1968). The technology of teaching. New York: Appleton-Century-Crofts.
Stiggins, R.J., Arter, J.A., Chappuis, J., & Chappuis, S. (2004). Classroom assessment for student learning: Doing it right,

         using it well. Portland, OR: Assessment Training Institute.
Wilson, B., & Myers, K.M. (2000). Situated cognition in theoretical and practical context. In D. Jonassen & S. Land (Eds.),

         Theoretical foundations of learning environments (pp. 57-88). Mahwah NJ: Erlbaum.
Wilson, B., Teslow, J., & Osman-Jouchoux, R. (1995). The impact of constructivism (and postmodernism) on ID

         fundamentals. In B. Seels (Ed.), Instructional design fundamentals: A reconsideration (pp. 137-157). Englewood
         Cliffs, NJ: Educational Technology Publications.
Wilson, B.G. (2005). Broadening our foundation for instructional design: Four pillars of practice. Educational Technology,
         45(2), 10-15.
Woolfolk, A.E. (2007). Educational psychology (10th ed.). Boston: Pearson Education.

                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/the_role_of_theory.

                                                                                       230
  18

Making Good Design Judgments via the
Instructional Theory Framework

Peter C. Honebein & Charles M. Reigeluth

Many instructional designers who design innovative learning experiences, and then conduct research that investigates
the usefulness of those learning experiences, fail to fully apply the instructional theory framework as a design
foundation. This reduces the usefulness of their designs and ultimately leads to learners and other stakeholders not
fully adopting and benefitting from the designer's learning experiences.

The aims of this chapter are to (1) help both designers and researchers improve the usefulness of their instructional
designs and subsequent research, and (2) reduce diffusion barriers that impact the dissemination and adoption of
learning experiences. The sections of this chapter include:

      Real Instructional Designers Use Theory
      Using the Instructional Theory Framework for Good Design
      Conclusion

Formally linking instructional design and research to the instructional theory framework and its related design principles
enables designers and researchers to answer questions about the relative advantage, compatibility, complexity,
observability, and trialability of their innovations (Rogers, 2003).

Real Instructional Designers Use Theory

What is a theory? Simply put, it is a set of ideas about how something might work. For example, Darwin's theory of
evolution contains the ideas of genetic variation and natural selection, among a host of others. In the education field,
learning theory is a set of ideas about how people learn, such as behaviorism, cognitivism, and constructivism.
However, what should most interest instructional designers is instructional theory, a set of ideas for how best to help
people learn.

Here is a simple example of an instructional theory: Drill-and-practice is a useful method for efficiently helping a learner
memorize such things as the names of all the U.S. states. This theory contains the idea of an instructional method (drill-
and-practice) to help a person remember things. How well will it work? We will not know until we actually deliver our
instructional theory to learners in the intended type of situation.

Yanchar et al. (2010) suggested some instructional designers feel that instructional theory has little relevance in how
they design instruction: "There is clearly an uneasiness about the applicability of theories and other conceptual tools in
everyday design work" (p. 41). Honebein and Honebein (2014), on the other hand, suggest designers do use
instructional theory, "but their usage of theory is tacit--e.g. not apparent, even to them" (p. 2).

                                                                                       231
Thus, the number-one job for instructional designers is to create or modify instructional theories more overtly in a way
that meets a client's requirements. To accomplish this, designers should consider using the instructional theory
framework (Honebein & Reigeluth, 2020; Reigeluth & Carr-Chellman, 2009a), illustrated in Figure 1.
Figure 1
A Simple Representation of the Instructional Theory Framework in Action

Note. We call the product (outcome) of this framework an instructional theory.
The instructional theory framework is a design theory, a set of ideas focused on how to "create" instruction rather than
"describe" instruction. Central to this idea of creating things is the concept of a method, which encapsulates the know-
how a designer uses to create something.
There are several categories of methods that instructional designers use in their design work, such as process methods
(e.g., ADDIE), instructional methods (e.g., demonstrations and practice with feedback), media methods (e.g., words,
pictures, or video to communicate content), and data-management methods (e.g., gradebooks and learning
management systems). While the instructional theory framework guides all of these types of design decisions, our
specific interest in this chapter is instructional methods, for example lecture and project-based instruction, which
promote learning.
Designers use the instructional theory framework as a way to select instructional methods that promote learning. To
select the most useful instructional methods, designers rely on the instructional situation to guide them. Front-end
analysis (the "A" part of ADDIE) is where the instructional theory framework begins its journey to deliver value. As shown
in Figure 1, the instructional situation has two parts: conditions and values.

                                                                                       232
Conditions are matters of fact about the situation that a designer can elicit empirically and objectively from
stakeholders and documents. Conditions include information about:

  1. Learners, such as number, demographics, geographics, psychographics, and behaviors, potentially represented as
      "personas"

  2. Content, the subject matter the learning experience will teach
  3. Context, which reflects place, resources, and tools
  4. Instructional development constraints, which includes money, time, and person-hours

This type of information is usually what designers and clients focus on collecting during front-end analysis.

However, what designers often fail to collect during front-end analysis is information about values. Values are matters
of opinion that are subjective in nature. For example, a client might say "I hate lectures and I don't want them in my
course!" In that statement, the client is expressing a value--an opinion--that is true for them but may not be true for
others. A designer elicits values empirically and multidimensionally from a variety of stakeholders. Values can have a
huge impact on the success of an instructional design project. The instructional theory framework specifies four unique
types of values:

  1. Values about learning goals, which reflect different opinions stakeholders have about what the learners should
      learn

  2. Values about priority, which reflect different opinions about whether a learning experience should favor
      effectiveness (mastering the behavior), efficiency (delivering the lowest time or cost), or appeal (whether people
      like the learning experience or not)

  3. Values about methods, which reflect what instructional methods stakeholders see as being most useful (or not)--
      the "I hate lecture" example above

  4. Values about power, which reflect which stakeholders should have the most power to get their way regarding the
      learning-experience design.

Like conditions and values, instructional methods have their own unique characteristics, such as:

  1. Scope, which describes whether a method teaches a single idea (micro) or multiple ideas (macro)
  2. Generality, which describes whether the designer should use a method broadly or only within specific narrow

      situations
  3. Precision, which reflects the detail to which a designer specifies a method, in terms of its parts, kinds, and criteria
  4. Power, which describes how much a method contributes to attaining a learning goal
  5. Consistency, which describes how reliable a method is in attaining a learning goal within a specific situation.

Once a designer understands the situation (conditions and values), the designer uses their knowledge of the situation in
combination with method characteristics to select the best methods. In other words, the instructional theory framework
is like a conditional heuristic (Figure 1) whereby a manager or client gives a designer a situation (a mess), and the
designer must then consider the conditions and values to select methods that enable the designer to create a solution
that cleans up the mess.

The framework shown in Figure 1 is a pattern that produces and characterizes all instructional theories. Essentially, an
instructional theory is the product of the instructional theory framework. It contains a collection of one or more
instructional methods that best fit one or more designated situations. An instructional theory is different from learning
theories, such as behaviorism, cognitivism, and constructivism. As shown in Figure 2, learning theory descriptively
explains the "what happens" of the learning process, typically what might be going on in one's head. For example, a
cognitivist learning theory suggests that information received by a learner is first processed in short-term memory, and
then transferred to long-term memory. Also, notice that a learning theory does not include any methods.

Figure 2

Learning Theory Describes the "What Happens" of Various Learning Processes

                                                                                       233
Note. The arrow indicates the flow of knowledge. Instructional theory prescribes possible methods for "how" one might
effectively, efficiently, and appealingly learn. The arrow indicates that drill & practice is a possible instructional method
that enables a learner to recall U.S. state names.

Examples of instructional theories include those summarized in the four volumes of the "Green Book" (Instructional-
Design Theories and Models) (Reigeluth, 1983; Reigeluth 1999; Reigeluth & Carr-Chellman, 2009a; and Reigeluth et al.
2017), such as Shank and colleague's (1999) goal-based scenarios, and Huitt and colleague's (1999) direct approach to
instruction. Instructional theories do not need to be published in a book to be instructional theories. Because
instructional theory is situational, anyone can create instructional theories for situations that are narrow or wide in
scope, and they can improve, change, or "mash up" existing instructional theories in any way they want to fit the
situation. In fact, we believe that all people have their own personal theory of instruction, often as tacit knowledge,
based on their experiences as a learner or instructor. The challenge for designers is to improve and expand their
personal instructional theories.

Using the Instructional Theory Framework for Good Design

Applying the instructional theory framework is not hard. In some regards, the instructional theory framework is like a
checklist of good practices. To elaborate our ideas about how to use the framework, we have synthesized six core
principles that will help instructional designers embrace the ideals of the instructional theory framework:

  1. Understand learning experiences as complex systems
  2. Value learning-experience-design fundamentals
  3. Practice unbiased consideration of instructional methods
  4. Respect the instructional design iron triangle
  5. Differentiate between methods and media
  6. Know your personal instructional design theory

These principles facilitate the transition between situations and methods that leads to superior instructional solutions,
as well as demonstrating a solution's value. We suggest instructional designers find renewed inspiration to embrace the
instructional theory framework through these six principles.

Understand Learning Experiences as Complex Systems

Instructional designers conduct the work they do in a living, self-organizing, complex system (You, 1993; Rowland, 1993,
2007; Solomon, 2000, 2002; Honebein, 2009). What this means is that learning experience designs will behave in ways

                                                                                       234
that designers cannot predict or expect; their nature is emergent "...in that [it is] shaped and developed over time
through an evolutionary process" (Honebein, 2009, p. 29). For example, an instructor can design and teach a class one
semester, and then the very next semester teach the same class again, and the experience for the instructor, the
learners, and any other stakeholders will likely be different.

Reigeluth and Carr-Chellman's (2009b) and Honebein's (2019) explorations of the "galaxy" question, which is about
whether some instructional methods have universal properties, provide evidence to support the proposition that the
instructional theory framework represents a complex system. Merrill (2002, 2009) has argued that some instructional
methods are universal, that they are present in all good instruction, such as his first principles. However, those
principles are described on a very imprecise level. The implementation of any of those principles will vary from one
situation to another, making any reasonably precise description of the principle situational, in recognition of the
complexity of instructional situations. Furthermore, given that the instructional theory framework provides categories
for conditions, values, and methods, the permutations of instances for each category a learning-experience designer
could combine is immeasurable. In other words, situations and methods represent a complex system (Honebein &
Reigeluth, 2020).

This idea of complexity is expanded upon philosophically by Cilliers (2000), who distinguishes a system as simple,
complicated, and complex based upon the distance from which one observes that system. For example, an aquarium
seen in one's home, observed as a decoration, is simple. That same aquarium can seem complicated when observed by
a person who needs to repair it, in terms of heaters, pumps, tubes, and chemicals. The aquarium becomes complex
when a person observes the aquarium as an ecosystem, with an immeasurable number of variables.

What does this mean for designers? It means that designers should be comfortable knowing their design situation
qualitatively, whereby a variety of learning-experience "experiences" are possible due to the number of elements present
in a situation and the interaction between those elements (Honebein & Reigeluth, 2020).

Value Learning Experience Design Fundamentals

A design fundamental is a "good practice" that one expects a learning-experience designer to overtly apply when
designing a learning experience. For us, learning-experience-design fundamentals focus on three key instructional
design practices: (1) clearly synthesized situations (conditions, including the nature of the content, and values) that
should be stated as instructional objectives, (2) aligned assessments, and (3) formative evaluation that demonstrates a
learning experience can achieve the mastery standard specified in the objective.

Instructional Objectives

When a learning-experience designer conducts an instructional analysis, the designer gathers data about the situation
in the form of conditions and values. The designer then synthesizes the situation's primary, actionable factors into a
form that enables the selection of instructional methods, an instructional objective.

A well-formed instructional objective has three parts: the conditions for performing the behavior, the behavior, and a
standard of performance (criteria for mastery). There are specific rules for each part that maintain logical consistency
and hierarchy of the instructional objective (Mager, 1984). In instructional theory framework terms, the specification of
mastery is called values about goals, and since it is a value (a matter of opinion), a designer can define it quantitatively,
qualitatively, or some mixture of both.

Aligned Assessments

Instructional designers must specifically link and align instructional objectives with assessments. Assessments not
only confirm mastery of desired behaviors, but also provide data about formative improvements.

What is typically missing in criterion-referenced assessments is an indication of acceptable mastery. For example,
learning experience "A" might report test performance of 83%, while learning experience "B" might report test
performance of 89%. If the instructional objective guiding both learning experiences lacks mastery criteria, it becomes

                                                                                       235
very difficult to assess the efficacy of each learning experience across the outcomes of effectiveness, efficiency, and
appeal. Designers must identify acceptable mastery so that other designers and researchers can assess the improved
learning effects within the context of efficiency and appeal. You can learn more about this in the Measuring Student
Learning chapter.

Formative Evaluation

Learning-experience design must be more about "improving" and less about "proving". Research methods to prove the
usefulness of an instructional method or theory make little sense when the instructional situation surrounding the
learning experience can vary so much that the level of usefulness does not generalize (Honebein & Reigeluth, 2020;
Reeves & Lin, 2020). What makes more sense is research that aims to improve the instructional theory, such as
formative evaluation using single-subject techniques (Brenneman, 1986) or expert reviews, and formative research
(Reigeluth & An, 2009; Reigeluth & Frick, 1999). This improvement orientation is particularly important when a method
or theory is at a relatively early stage in its development. However, the iterative nature of research to improve also
allows a general method to be tailored to a specific situation. This enables designers to, over time, offer multiple
versions of the general method for different situations. Designers can learn more about this in the Continuous
Improvement of Instructional Materials chapter.

When designers implement a learning experience (delivery to actual learners), they should collect formative data about
all three metrics: its effectiveness, efficiency, and appeal. From that point, the learning experience may undergo any
number of formative improvements over various iterations in its design lifecycle. Why? Because the learning experience
will never be perfect; the situation (a complex system) is always changing, forcing the learning experience to change
and adapt to deliver the right proportions of effectiveness, efficiency, and appeal. Thus, designers must not only
consider the changes in methods and the resulting changes to effectiveness, efficiency, and appeal, but also how the
changes in the situation influence the various effects of those methods.

What do these three design fundamentals mean for designers? It means that no matter where you work or who your
clients are, you will have core data that enables you to defend your designs against nit-picking know-it-alls. And if the
client has new or additional data, you will have a structure by which to collaboratively improve the design rather than a
fight to prove a design.

Practice Unbiased Consideration of Instructional Methods

Instructional designers must adopt a mindset that considers all instructional methods as having unknown or neutral
usefulness until the instructional situation is known. It is only at this time when data is present that a designer can
assess an instructional method's usefulness (Honebein, 2016, 2019).

This practice helps avoid philosophical bias. Honebein's research showed that designers have a pre-existing bias
toward certain instructional methods; the instructional theory framework calls this values about methods. For example,
as shown in Figure 3, many designers view authentic tasks as very useful, whereas those same designers view peer-
based or cooperative methods as less useful. This type of biased thinking can lead designers to reject instructional
methods that might be very useful in a given situation, just because those methods are incompatible with their biases.
There are situations in which behaviorist methods (e.g., drill and practice) are useful and many situations in which they
are not.

Figure 3

Chart Illustrating Designer Bias

                                                                                       236
Note. Instructional designers in introductory and capstone instructional design courses at two different universities
were asked to rate the usefulness of various instructional methods in the absence of any type of "condition." This chart
illustrates designer bias (values about methods), where more useful (powerful) methods are to the left, and less useful
methods are to the right. If there was no bias, all bars in the chart should be the same, at 3.5. From Honebein (2019).
What does unbiased consideration of instructional methods mean for designers? It means that the solution to a thorny
instructional design situation might just be an instructional method that you or your client hates. So, anytime you find
yourself saying, "I hate lectures," watch Randy Pausch's Last Lecture (20+ million viewers), and find the hidden value and
inspiration present in all instructional methods.

Respect the Instructional Design Iron Triangle

All instructional designs involve some sort of sacrifice (Gropper & Kress, 1965; Tosti & Ball, 1969; Clark & Angert, 1980;
Hannifin & Rieber, 1989). Honebein and Honebein's (2015) research into this topic suggested that an instructional
design iron triangle likely exists in all instructional design projects (see Figure 4). The theory of the iron triangle is that if
you have three competing factors, you can only maximize two of them; you always sacrifice one. In instructional design,
the competing factors (outcomes) are effectiveness, efficiency, and appeal. For example, if a designer favors
effectiveness and appeal, the designer will sacrifice efficiency. Favoring efficiency and appeal sacrifices effectiveness,
and favoring effectiveness and efficiency sacrifices appeal.
Figure 4
The Instructional Design Iron Triangle

                                                                                       237
Note. The triangle depicts the three outcomes (or constraints) associated with instructional methods: effectiveness,
efficiency, and appeal. An instructional theory, model, or method typically involves the sacrifice of one or more of the
outcomes.

What does the iron triangle mean for designers? It means that you'll always have to give up something in your designs,
and that is okay. Perfect is the enemy of the good.

Differentiate between Methods and Media

Methods and media have a unique influence on effectiveness, efficiency, and appeal. We have already defined
instructional methods earlier in this paper. Examples of instructional methods include lecture, drill and practice, and
apprenticeship, as well as others depicted in Figure 3. Media is, of course, the communication channel that carries
instructional methods to learners (Heinich et al.1989). Media itself is a method, and as such one should differentiate
between instructional methods and media methods. Media methods include such things as words, diagrams, pictures,
films, models, and realia--organized across categories of enactive, iconic, and symbolic (Bruner, 1966).

There has been much debate in our field about how instructional methods and instructional media contribute to
effectiveness, efficiency, and appeal (Clark, 1985, 1986, 1994). Following Clark's arguments and our own design
experiences, we feel that instructional methods influence effectiveness, efficiency, and appeal, whereas instructional
media influence only efficiency and appeal.

Why should the designer be aware of this point of view? Because as designers explore our field's research to help
construct their own personal instructional theory, they will find studies and examples that describe what Tennyson
(1994) calls "big wrench" solutions, which take the form of a panacea. Big wrenches, which are typically media
methods, are sprinkled throughout the history of our science, from Thomas Edison's "motion pictures" which promised
to make books obsolete, to today's mobile game-based learning (mGBL) solutions. The studies and examples may
suggest it is the media method that drives effectiveness, whereas in reality it is more likely the instructional method that
does.

What does this mean for designers? It means that you should be cautious about media in terms of touting its influence
on effectiveness. No one doubts its strong impact on efficiency and appeal. As noted previously, learning experiences
are complex systems that mash together a variety of methods to deliver results. A designer may never know what
design element (or more likely, combination of elements) served as the secret sauce for effectiveness. But more than
likely, it will be an instructional method (or combination of instructional methods).

                                                                                       238
Know Your Personal Instructional Design Theory

For more than 15 years now, the authors have taught a capstone instructional theory framework course for graduate
students at Indiana University. The course culminates in students writing their own personal theory of instruction (see
the application exercise below). In writing their personal theory paper, students consider the conditions (learner,
content, context, constraints) and values (about goals, outcomes, methods, power) associated with their situation, and
discuss the instructional methods that reflect their "stamp" as a designer.

   Application Exercise

     Your Task:
     Write a reflection paper that outlines your personal theory of
     instruction. This should be an in-depth, APA-style paper that answers the questions
     below and provides citations and references describing the literature that has
     influenced your personal theory of instruction.

        1. What is the nature of the situation (conditions and values) that governs your instructional design work?
        2. How do you think you are "wired" to design instruction?
        3. What "stamp" does the instruction you design have, in terms of the methods you use, that enables others to

           recognize it was you who designed it?

     Rubric for Quality:
           Paper is equal to or less than eight, single-spaced pages.
           Paper is logically structured in APA style, with clear sequence and clear organization of thoughts. Reader
           can read it once quickly and comprehend its contents.
           Paper is free of grammar and spelling errors.
           Cites/references in APA style literature from readings to support positions and ideas described in the
           personal instructional theory.
           Clearly describes the personal instructional theory through the situation-methods structure.
           Clearly discusses and references the theoretical foundations and influences of the personal theory.
           Gives an example of the "stamp" one would see on instruction one would design.

The activity our students complete should be an activity that all designers regularly engage in as well, since the activity
is all about drawing a line in the sand about your design principles (Brown & Campione, 1996; Stolterman & Nelson,
2000; Collins et al., 2004; Boling et al., 2017). We see such design principles connected to important emerging ideas
from the above authors related to design character, design judgment, core judgment, and accountability. As we
understand these terms, one's design character represents inherent, assumed responsibilities for both creative process
and outcomes. Design judgment involves creativity and innovation, integrating multiple forms of judgment associated
with those aims. Stolterman and Nelson (2000) refer to design judgment as "an act of faith" (p. 8). After a designer
experiences the results of their design judgments, design judgments contribute to core judgment, in which certain
judgments over time become fixed and very hard to change. For example, the learning-experience-design fundamentals
we discuss in this paper are, for us, core judgments. Ultimately, designers must be accountable for their designs in
terms of effectiveness, efficiency, and appeal, and avoid the temptation to move, hide, or remove accountability to some
other stakeholder. The aggregate of these ideas represents one's design character and one's belief "in his or her
capacity to make good judgments" (p. 8). That belief is reinforced in terms of how one reflects on their actions.

What does this mean for designers? We think one's design principles were meant to be dynamic, not static. As the
comedian Groucho Marx once said, "Those are my principles, and if you don't like them ..., well, I have others." Groucho
was wise, as he appears to have known the instructional theory framework's foundational idea that situation drives

                                                                                       239
methods, or in this case, principles. Whether a learning-experience designer is eclectic or orthodox in their adoption of
learning theories and instructional methods (Yancher & Gabbitas, 2011; Honebein & Sink, 2012), the designer's choice of
methods must be dependent on the situation. Designers should not assume even Merrill's (2002, 2009) first principles
to be appropriate in all situations (Honebein, 2019). It is through the ideas of formative evaluation, design research, and
reflection-in-action/reflection-on-action that one's principles increase and decrease in strength.

Conclusion

The instructional theory framework is a cornerstone that must guide our field's practice and research. The foundations
of situation and methods are simple, logical, and aligned with good practice. Philosophically, the instructional theory
framework addresses both the objective world (conditions) and the subjective world (values), which mimics what
instructional designers encounter in the real world. It provides designers a means to assess and articulate their design
judgment, enabling them to be more confident in assuming accountability (Stolterman & Nelson, 2000). And as shown
by Honebein and Honebein (2014, 2015) and Honebein (2017, 2019), the instructional theory framework functions as
expected. The instructional theory framework's key benefit is that it guides designers in creating learning experiences
that have a higher relative advantage (effectiveness, efficiency, and appeal), better compatibility, lower complexity,
easier observability, and actionable trialability.

However, a word of advice: avoid using the "theory" word when you are talking with your clients and subject-matter
experts about how you design learning experiences. It will scare them. Keep it as your little, tacit secret.

References

Boling, E., Alangari, H., Hajdu, I. M., Guo, M., Gyabak, K., Khlaif, Z., Kizilboga, R., Tomita, K., Alsaif, M., Lachheb, A., Bae, H.,
         Ergulec, F., Zhu, M., Basdogan, M., Buggs, C., Sari, A., & Techawitthayachinda, R.I. (2017). Core judgments of
        instructional designers in practice. Performance Improvement Quarterly, 30(3), 199-219. DOI:
         10.1002/piq.21250

Brenneman, J. (1989, March). When you can't use a crowd: Single-subject testing. Performance & Improvement, 22-25.

Brown, A., & Campione, J. (1996). Psychological theory and the design of innovative learning environments: On
         procedures, principles, and systems. In L. Schauble & R. Glaser (Eds.), Innovations in learning: New environments
         for education (pp. 289-325). Mahwah, NJ: Lawrence Erlbaum Associates.

Bruner, J. S. (1966). Toward a theory of instruction. Cambridge, MA: Belknap Press.

Cillers, P. (2000). Complexity and postmodernism. London: Routledge.

Clark, R. E. (1985). Evidence for confounding in computer-based instruction studies: Analyzing the meta-analyses.
        Educational Communications and Technology Journal, 33(4), 249-262.

Clark, R. E. (1986). Absolutes and angst in educational technology research: A reply to Don Cunningham. Educational
        Communications and Technology Journal, 34(1), 8-10.

Clark, R. E. (1994). Media will never influence learning. Educational Technology Research & Development, 42(2), 21-29.

Clark, F. E., & Angert, J.F. (1980). Instructional design research and teacher education. Paper presented at the Annual
         Meeting of the Southwest Educational Research Association (San Antonio, TX, February 8, 1980). ERIC
         Document 183528.

Collins, A., Joseph, D., & Bielaczyc, K. (2004). Design research: Theoretical and methodological issues. The Journal of
        the Learning Sciences, 13(1), 15-42. https://doi.org/10.1207/s15327809jls1301_2

                                                                                       240
Czeropski, S., & Pembroke, C. (2017). E-learning ain't performance: Revising HPT in an era of agile and lean.
        Performance Improvement, 56(8), 37-47. 10.1002/pfi.21728

Gibbons, A. S., & Rogers, P. C. (2009). The architecture of instructional theory. In C. M. Reigeluth & A. Carr-Chellman
         (Eds.), Instructional-design theories and models: Building a common knowledge base (Vol. III) (pp. 305-326).
         Hillsdale, NJ: Lawrence Erlbaum Associates.

Gropper, G. L. & Kress, G.C. (1965). Individualizing instruction through pacing procedures. AV Communications Review,
        13(2), 165-182.

Hannifin, M. J., & Rieber, L.P. (1989). Psychological foundations of instructional design for emerging computer-based
        instructional technologies, part II. Educational Technology Research and Development, 37(2), 102-114.

Heinich, R., Molenda, M., & Russell, J. D. (1989). Instructional Media (3rd Ed.). Macmillian: New York.

Honebein, P. C. (2009, January-February). Transmergent learning and the creation of extraordinary learning experiences.
        Educational Technology, 27-34.

Honebein, P. C. (2016). The influence of values and rich conditions on designers' judgments about useful instructional
        methods. Educational Technology Research and Development, 65(2), 341-357. https://doi.org/10.1007/s11423-
         016-9485-y

Honebein, P. C. (2019). Exploring the galaxy question: The influence of situation and first principles on designers'
        judgments about useful instructional methods. Educational Technology Research and Development, 67(3), 665-
         689. https://doi.org/10.1007/s11423-019-09660-9

Honebein, P. C., & Honebein, C. H. (2014). The influence of cognitive domain content levels and gender on designer
        judgments regarding useful instructional methods. Educational Technology Research and Development, 62(1),
         53-69. https://doi.org/10.1007/s11423-013-9322-5

Honebein, P. C., & Honebein, C. H. (2015). Effectiveness, efficiency, and appeal: pick any two? The influence of learning
        domains and learning outcomes on designer judgments of useful instructional methods. Educational Technology
        Research and Development, 63(6), 937-955. https://doi.org/10.1007/s11423-015-9396-3

Honebein, P. C., & Reigeluth, C. M. (2020). The instructional theory framework appears lost. Isn't it time we find it again?
        Revista de Educación a Distancia, 64(20). http://dx.doi.org/10.6018/red.405871

Honebein, P. C., & Sink, D. L. (2012). The practice of eclectic instructional design. Performance Improvement, 51(10),
         26-31.

Huitt, W. G., Monetti, D. D., & Hummel, J. H. (1999). Direction approach to instruction. In C.M. Reigeluth (Ed.),
         Instructional-design theories and models: A new paradigm of instructional theory, volume II (pp. 73-97).
         Hillsdale, NJ: Lawrence Erlbaum Associates.

Mager, R. F. (1984). Preparing instructional objectives. Belmont, CA: Lake.

Merrill, M. D. (2002). First principles of instruction. Educational Technology Research and Development, 50(3), 43-59.

Merrill, M. D. (2009). First principles of instruction. In C. M. Reigeluth & A. Carr-Chellman (Eds.), Instructional-design
         theories and models: Building a common knowledge base (Vol. III) (pp. 41-56). Hillsdale, NJ: Lawrence Erlbaum
         Associates.

Molenda, M. (2003). In search of the elusive ADDIE model. Performance Improvement, 42(4), 34-36.

Nelson, H. G., & Stolterman, E. (2012). The design way: Intentional change in an unpredictable world (2nd ed.).
         Cambridge, MA: MIT Press.

                                                                                       241
Reeves, T. C., & Lin, L. (2020). The research we have is not the research we need. Educational Technology Research and
        Development, 68, 1991-2001. https://doi.org/10.1007/s11423-020-09811-3

Reigeluth, C. M. (1983). Instructional design: What is it and why is it? In C.M. Reigeluth (Ed.), Instructional-design
         theories and models: An overview of their current status (pp. 3-36). Hillsdale, NJ: Lawrence Erlbaum Associates.

Reigeluth, C. M. (1999). What is instructional-design theory and how is it changing? In C.M. Reigeluth (Ed.), Instructional-
         design theories and models: A new paradigm of instructional theory, volume II (pp. 5-29). Hillsdale, NJ:
         Lawrence Erlbaum Associates.

Reigeluth, C. M. & An, Y. (2009). Theory building. In C. M. Reigeluth & A. Carr-Chellman (Eds.), Instructional-design
         theories and models: Building a common knowledge base (Vol. III) (pp. 365-386). New York, NY: Routledge.

Reigeluth, C. M., Beatty, B. J., & Myers, R. D. (Eds.). (2017). Instructional-design theories and models, Volume IV: The
         learner-centered paradigm of education. New York, NY: Routledge.

Reigeluth, C. M. & Carr-Chellman, A. (2009a). Understanding instructional theory. In C. M. Reigeluth & A. Carr-Chellman
         (Eds.), Instructional-design theories and models: Building a common knowledge base (Vol. III) (pp. 3-26).
         Hillsdale, NJ: Lawrence Erlbaum Associates.

Reigeluth, C. M. & Carr-Chellman, A. (2009b). Situational principles of instruction. In C. M. Reigeluth & A. Carr-Chellman
         (Eds.), Instructional-design theories and models: Building a common knowledge base (Vol. III) (pp. 57-68).
         Hillsdale, NJ: Lawrence Erlbaum Associates.

Reigeluth, C. M., & Frick, T. W. (1999). Formative research: A methodology for creating and improving design theories. In
         C.M. Reigeluth (Ed.), Instructional-design theories and models: A new paradigm of instructional theory, volume II
         (pp. 633-651). Hillsdale, NJ: Lawrence Erlbaum Associates.

Reigeluth, C.M. & Keller, J. B. (2009). Understanding instruction. In C. M. Reigeluth & A. Carr-Chellman (Eds.),
         Instructional-design theories and models: Building a common knowledge base (Vol. III) (pp. 27-35). Hillsdale,
         NJ: Lawrence Erlbaum Associates.

Reigeluth, C. M., Myers, R. D., & Lee, D. (2017). The learner-centered paradigm of education. In C. M. Reigeluth, B. J.
         Beatty, & R. D. Myers (Eds.), Instructional-design theories and models, Volume IV: The learner-centered paradigm
         of education (Vol. IV, pp. 5-32). New York, NY: Routledge.

Rogers, E. M. (2003). Diffusion of Innovations (5th Ed.). New York: Free Press.

Rowland, G. (1993). Designing and instructional design. Educational Technology Research and Development, 41(3), 79-
         91.

Rowland, G. (2007). Performance improvement assuming complexity. Performance Improvement Quarterly, 20(2), 117-
         136.

Shank, R. C., Berman, T. R., & Macpherson, K. A. (1999). Learning by doing. In C.M. Reigeluth (Ed.), Instructional-design
         theories and models: A new paradigm of instructional theory, volume II (pp. 161-181). Hillsdale, NJ: Lawrence
         Erlbaum Associates.

Solomon, D. L. (2000). Toward a post-modern agenda in instructional technology. Educational Technology Research and
        Development, 48(4), 5-20.

Solomon, D. L. (2002). Rediscovering post-modern perspectives in IT: Deconstructing Voithofer and Foley. Educational
        Technology Research and Development, 50(1), 15-20.

Stolterman, E., & Nelson, H. (2000). The guarantor of design. Proceedings of IRIS 23. Laboratorium for Interaction
         Technology, University of Trollhättan Uddevalla, 2000. L. Svensson, U. Snis, C. Sørensen, H. Fägerlind, T. Lindroth,

                                                                                       242
         M. Magnusson, C. Östlund (eds.)
Tosti, D. T. & Ball, J. R. (1969). A behavioral approach to instructional design and media selection. AV Communications

        Review, 17(1), 5-24.
Yanchar, S. C., & Gabbitas, B. W. (2011). Between eclecticism and orthodoxy in instructional design. Educational

        Technology Research and Development, 59(3), 383-398. https://doi-org.proxyiub.uits.iu.edu/10.1007/s11423-
         010-9180-3
You, Y. (1993). What can we learn from chaos theory? An alternative approach to instructional systems design.
        Educational Technology Research and Development, 41(3), 17-32.

                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/making_good_design.

                                                                                       243
244
  19

Using Theory as a Learning and Instructional Design
Professional

Jason K. McDonald

   Editor's Note

     This is a condensed version of McDonald, J. K. (2022). A framework for phronetic LDT theory. In H. Leary, S. P.
     Greenhalgh, K. B. Staudt Willet, & M-H. Cho (Eds.), Theories to influence the future of learning design and
     technology. EdTech Books. Retrieved from
     https://edtechbooks.org/theory_comp_2021/framework_phronetic_LDT_mcdonald

Practitioners in the field of learning and instructional design are commonly told that "theories are the foundation for
designing instructional solutions to achieve desired learning outcomes" (Oyarzun & Conklin, 2021). But if this is true,
why do designers often report that theory is "too abstract and inapplicable" to address common problems of practice
(Yanchar et al., 2010, p. 50)? Or, alternatively, that theories are so "rigid" (p. 51) and prescriptive that they lead to one-
size-fits-all solutions that do not fit the circumstances in which designers are working?
In my studies, I have come to believe that part of the problem is that designers think about theory the wrong way. They
often assume it is like a tool (a power drill, for instance, or a circular saw). In this view, theory has some kind of capacity
built into it that is independent of the person using it. Anyone can pick it up and produce results (if they have received
the proper training, of course). But this perspective misunderstands something fundamental about human-centered
work like learning and instructional design. Theories do not solve problems. People do. This does not mean theory is
useless. It just means it plays a different role in designers' work than being a tool that they apply. So, if designers want
theory to be applicable and usable they need to first put it into its proper place--a place that recognizes that they--the
designers--are most central to the work of improving education, and not a set of abstract, theoretical ideas that are
presumed to have the power to solve problems. From this perspective, theory becomes one of many supports for
practice, but not the most important nor the most decisive.
My purpose in this chapter is to explain these issues. First, I review some of the challenges with the field's traditional
views of theory. Next, I offer a different view of theory that conceptualizes it as a support that helps designers
strengthen their own capacities for better judgement. Finally, I briefly describe different kinds of theory that apply to
learning and instructional design practice, and how they support designers' judgement in differing ways.

                                                                                       245
The Field's Traditional Views of Theory

Many practitioners' views of theory come from a desire to turn the field into a science, so they can predict and control
learning the same way people predict and control natural phenomena, like how to fly a plane or start a fire. There have
traditionally been two versions of this: a strong view, that has attempted to treat instructional design knowledge as a
system of cause-and-effect laws (Gilbert, 1971; Gropper, 2017; Merrill et al., 1996), and a softer view that does not
guarantee results while still maintaining that good theory increases the probability that certain outcomes will occur
(Jonassen et al., 1997; Reigeluth, 1997, 1999; Winn, 1997).

While these views of theory are useful in many fields, in learning and instructional design--where designers' purpose is
to improve the countless ways that people can learn about and experience the world--both the strong and soft views
present challenges. An extended reviews of these classic positions, and their accompanying challenges, are found in
McDonald (2022), on which this current chapter is based. Here, I only summarize major points of my discussion there.
The traditional views tend to presume that theories sit independent of any researcher or practitioner, serving as
storehouses of knowledge, and containing power in themselves to solve problems of practice (Gropper, 2017). They
reduce the complexity of the world, condensing it into technical models or methodologies that attempt to eliminate, or
at least minimize, the possibility of a misstep (Bednar et al., 1991; Elen & Clarebout, 2007). Ideally, theories are objective
and so can be picked up and used by anyone (possibly with some level of intellectual preparation) to control or optimize
situations with some degree of precision (Honebein & Reigeluth, 2020; Reigeluth, 1997). When one views theory this
way, it is logical to prioritize it over the seemingly less-dependable, idiosyncratic practitioner know-how that is taken to
be the alternative (Clark & Estes, 1998; Klauer, 1997).

But despite the seeming logic of these ideas, a tool-like use of theory just does not help designers really understand--
much less address--problems of practice. The world is fluid and intricate, and too complex for designers to match up
the environments they find with instructions provided by rules or rule-like information (Dunne, 1997). There "are just too
many features [in any situation] . . . to determine which rule or concept should be applied" (Dreyfus, 2014, pp. 231-232).
In a practical, human-centered world, such attempts to use theory are like "using a flow-chart or decision tree" to carry
out a conversation. Not only are the range of possible responses too great, even attempting to try misunderstands the
nature of interpersonal relationships, where the point is not to manage them through "technical rationality," but to let
them unfold according to their own, emergent logic (cf. McDonald & Michela, 2022, p. 63).

Reconsidering How Theory Supports Practice

If theory should not be thought of as tool that practitioners apply, what role should it play? Insights can be found by
considering what defines an expert in a domain. Expertise is

        discerning and responding appropriately to the subtle features and specific requirements of each
        situation. . . . without any explicit sense of effort, responding intuitively to the unfolding of circumstances
        without having to stop and think about what we are trying to accomplish--or otherwise needing to
        represent the conditions of satisfaction of our activity. (Wrathall & Londen, 2019, p. 651)

Wrathall (2011) illustrated this by describing a person's skillful use of a knife. One's expertise is not based on knowing
more facts about knives than someone else. While explicit information about a knife may be useful for some purposes,
anything an expert can say about it is secondary to the way he wields it without thought, and uses it in relation with
other kitchen equipment. He may or may not be able to articulate what he is doing at any moment, and, in fact, his
thinking may be completely wrong in even important ways. Yet proof of his expertise is still observable in his actions.
This ability to do something, even if one cannot always articulate what one is doing, is what Schön (1983) called
"knowing-in-action" (p. 50), exemplified by the way that

        a tightrope walker's know-how . . . lies in, and is revealed by, the way he takes his trip across the wire . . .
        [and] a big-league pitcher's know-how is in his way of pitching to a batter's weakness, changing his pace,

                                                                                       246
        or distributing his energies over the course of a game. (pp. 50-51)

Unlike some other forms of knowledge, knowing-in-action is not improved by following rules or even softer guidelines.
Situations move too quickly for people to stop, deliberate, and make a choice about what rules to follow. Instead,
experts act based on something subtle they notice in a situation that cues them in to where they should place their
efforts. But these nuanced details are often difficult to put into words. Consider how a baker just knows the dough is
ready by a certain spring she feels when kneading it. There is not really a rule that describes when "springiness" has
been reached. It is something that she just learns by doing. In addition, experts also have a sense of "connoisseurship"
(Belland, 1991, p. 23; see also Parrish, 2012) that helps them make fine-tuned discriminations between good and bad, or
better and worse, options (cf. Wrathall, 2019).

None of this should be taken to mean that guidelines have no value, however. Rules, guidelines, or principles might be
very useful when something does not go as planned or when one is inexperienced in navigating an environment on their
own (Dreyfus, 2014). They may also be useful to the experienced in certain cases to help them check their biases or
consider whether their individual perceptions are accurate or should be tempered. When such moments occur,
designers might find explicit information--like the guidelines and frameworks found in many theories--helpful to get
through a challenge. But strict adherence to those guidelines also interferes with designers elevating their performance
to expert levels. Thus, while there may be a place for novices to use theory in the sense of following a set of
instructions, for experts this often hinders instead of helps their ability to perform (Wrathall & Londen, 2019).

Understanding this shows designers better ways to use theory in practice--ways that do not treat it like a tool or a set of
instructions. If what matters is paying attention to situational cues, or developing a sense about what choices are better
or worse, theory can be conceived as an aid that supports designers' development of such dispositions. Instead of a
tool, then, theory is a way of changing designers themselves--helping them experience situations differently so they
come to see and feel things the way experts do (Dunne, 1997; Wrathall, 2011). It also helps them discern fine-grained
situational cues, while orienting them towards previously unseen possibilities (Wrathall & Londen, 2019). Further, theory
models the character of good practice (Yanchar & Faulconer, 2011), and moves designers' feelings, desires, and values,
drawing them in towards a full, wholehearted commitment to the field and its practices (Wrathall, 2019). From these
perspectives, the scholars who develop theory could be seen as offering a set of propositions about what the world is,
or what it could be, that strengthen designers' capacities through engagement with theoretical ideas (studying the
theory, critiquing it, extending it, and so on).

Instead of tools or instruction, then, perhaps a better analogy is to consider theory to be like pianists' practicing of
scales. The goal is not to play the scales flawlessly as an end in itself, but to use the scales to strengthen players'
fingers, and develop the ability to play notes quickly and in certain orders--the muscle memory that allows skilled
players to hit the right notes at the right time, without ever looking at the keys. Similarly, if the goal is for designers to
extend their capacities, then studying theory can be a way to develop their "design muscle memory," allowing them to
recognize patterns and select alternatives without conscious deliberation. In this way, theories change the designers
themselves. Their capacities are increased as their theoretical repertoire grows.

What might this look like in practice? Consider this analogy: marathon runners do not try to tie their racing performance
to any particular training session. It is not any individual practice run, it is all of them together that make the difference--
along with all the other exercises the runners used to strengthen themselves. Likewise, if designers are regularly
engaged in exercising their theoretical capacities, when the time comes to design something, they find that they just see
something in the situation that is relevant. Or they find themselves caring about and valuing a certain option--they just
want to follow a certain path. They might not even be able to explicitly tie what they do to a specific theory (or other
design activity like studying precedent or analyzing the needs and constraints of their students). Rather, it is all those
sources of insight together that lead them to good decisions. In other words, they have used theory to develop stronger
intuitive design judgments. Most of the time there is little need to try and segment out the effects of an individual theory
from anything else that strengthens designers' capacities for a particular moment. There may be some particularly
difficult cases, of course, where they do explicitly refer to a theory for guidance. But even then, it is more productive if
designers mull over it, letting it work on them to spark ideas, rather than try to use it like a recipe. And there may be

                                                                                       247
times that designers are asked to justify a choice, in which case they might refer back to a theoretical concept to assist
them in explaining. Yet they should keep in mind that what they articulate are after-the-fact rationalizations--hopefully
helpful in building confidence in what they have done, but perhaps not exactly what led them to the path they chose.

Different Types of Theory for Different Practical Needs

Just like there are multiple forms of piano practice to strengthen different aspects of a musician's capabilities, there are
multiple forms of theory that help designers practice various dispositions needed to act more expertly. Building on the
work of Dunne (1997), I've developed a framework of at least some of the different kinds of theory we can find, along
with what each type of theory offers. A full account of each, and how they differ from each other, can be found in
McDonald (2022). Here, I only summarize how they apply to instructional design practice, and how they support
designers' judgement in differing ways.

   Different Forms of Practical Theory

           Compelling accounts of the dispositions and skills associated with resourceful design practice.
           Describing the different affordances possessed by instructional materials or technologies.
           Drawing attention to opportunities to act.
           Describing principles for organizing relationships and structures between situationally relevant people,
           resources, activities, and events.
           Articulating forms of excellence in practice that the field of learning and instructional design strives
           towards.

First, theories can articulate compelling accounts of the types of dispositions and skills associated with resourceful
design practice. For instance, Belland (1991) articulated a notion of connoisseurship in educational technology that
"attends to the affective needs of the learners" (p. 26) and qualitatively changes how practitioners appreciate, evaluate,
and design new learning systems (cf. Parrish, 2012). As examples he suggested knowing students "as people and not
just as the `kid in station 86'" (p. 27), and recognizing where "kindness leaves off and obsequiousness or patronizing
begin" in the tone of an instructional text (p. 30).

Second, theories can help designers understand the affordances possessed by the different kinds of materials or
technologies with which they work, along with what might better draw out potentials a material offers. Many of the
models, principles, frameworks, or guidelines common in the field describe or explain these kinds of issues. Consider
Mayer's (2014) multimedia principles of instruction, that provide useful information about how one can take advantage
of learning affordances offered by visual and aural media technology. Such suggestions can act like a person one is
having a design conversation with (Dunne, 1997; Schön, 1983), where they draw attention to relevant forces, help
designers question their assumptions, explore possibilities, their our aims, or take a step towards something useful. In
this view, theories contain wise advice that can inform designers' situational understanding (even if they do not offer
universal laws).

Third, another form of theory draws designers' attention to opportunities to act, so they can take advantage of
suitable moments that appear. An analogy Dunne (1997) used to explain this kind of theory was of a sailor who could
recognize that when the waves break in a certain way, it is her best chance to cut across them safely. As with the other
types of instructional design theory, opportunities to act are not formulas to follow. They function more like a focusing
device--sharpening designers' view, and accentuating what it might look like when a useful opening appears. For
instance, Richardson et al. (2019) studied the perceptions of university faculty and instructional designers on the nature
of their working relationships. Their research uncovered several factors that indicate the faculty-designer collaboration

                                                                                       248
is going well, such as when relationships are "egalitarian" (p. 862), and when the parties involved had clear expectations
about what to expect from each other. Such conditions do not guarantee success in any collaborative effort. But where
they exist (or when one can arrange a situation to bring them about), they can legitimately be viewed as creating a more
fruitful opportunity in which to act, allowing designers a space in which they can attempt further actions that move
them and their collaborators towards ends they find mutually desirable.

Fourth, theory can also describe principles for organizing relationships and structures between situationally relevant
people, resources, activities, and events. These describe another form of influence designers can have, where they
enable or facilitate certain kinds of activities based on how they arrange a setting (or, alternatively, prevent or
discourage other kinds of activities). For example, consider the patterns definitive of problem-based learning. This
approach is characterized by students who have responsibility for their learning, collaboration between participants, ill-
structured problems as the basis of inquiry, a tutor (or facilitator) who guides students through the learning process,
and informational, spatial, and/or technological resources that facilitate participants' free interactions (Cowdroy, 1994).
Designers can use the patterns of problem-based learning in such a way as to encourage the kinds of outcomes for
which it is known, even though once the situation begins they must realize that participants will always take it over and
shape it to their own ends (McDonald, 2021).

Finally, theory can articulate forms of excellence in practice that the field of learning and instructional design strives
towards. Yanchar and Slife (2017) defined these as the sense designers have of "what is good or right to do in relevant
situations, [and] what counts as satisfactory or unsatisfactory conduct" (p. 154). Forms of excellence are usually tacit.
They are the values and related considerations that inform good practice, but that are often in the background and so
designers are only implicitly aware of their influence most of the time. But by clearly articulating the field's values and
aims in compelling ways, designers can be drawn towards options or outcomes that the field finds valuable, and helps
ensure their work is aimed towards important ends. Unfortunately, forms of excellence have not traditionally been a
focused topic of inquiry in the field, but there are some illustrative exceptions such as Matthews and Yanchar's (2018)
study of designing instruction that encourages learners to take responsibility for their own learning.

Concluding Thoughts

The central message of this chapter is that instead of reducing the world of practice into abstract models or techniques,
theory takes its proper place when it supports designers as they learn how to cope with practice in all its color, vibrancy,
and liveliness. As a field, learning instructional design is in a strong position to produce this kind of theory, perhaps
more so than other fields that are not as tightly connected to practice or that have more direct interest in scientific
forms of theorizing (as is often the case in fields like psychology; cf. Wilson, 2005). I urge all of us in the field, both
researchers and designers, to be more thoughtful in considering the theories we develop and use, and aim towards
theoretical contributions that are truly aligned with the field's core purpose of creating excellent learning experiences.

References

Bednar, A. K., Cunningham, D. J., Duffy, T. M., & Perry, J. D. (1991). Theory into practice: How do we link? In T. M. Duffy &
        D. H. Jonassen (Eds.), Constructivism and the technology of instruction: A conversation (pp. 17-34). Lawrence
         Erlbaum Associates.

Belland, J. C. (1991). Developing connoisseurship in educational technology. In D. Hlynka & J. C. Belland (Eds.),
        Paradigms regained: The uses of illuminative, semiotic and post-modern criticism as modes of inquiry in
        educational technology (pp. 23-35). Educational Technology Publications.

Clark, R., & Estes, F. (1998). Technology or craft: What are we doing? Educational Technology, 38(5), 5-11.

Cowdroy, R. M. (1994). Concepts, constructs and insights: The essence of problem-based learning. In S. E. Chen, R. M.
        Cowdroy, A. Kingsland, & M. Ostwald (Eds.), Reflections on problem based learning (pp. 45-56). Australian

                                                                                       249
         Problem Based Learning Network.

Dreyfus, H. L. (2014). Skillful coping: Essays on the phenomenology of everyday perception and action (M. A. Wrathall,
         Ed.). Oxford University Press.

Dunne, J. (1997). Back to the rough ground: Practical judgment and the lure of technique. University of Notre Dame
         Press.

Elen, J., & Clarebout, G. (2007). Theory development. In J. M. Spector, M. D. Merrill, J. J. G. Van Merriënboer, & M. P.
        Driscoll (Eds.), Handbook of research on educational communications and technology (3rd ed., pp. 705-713).
         Routledge.

Gilbert, T. F. (1971). Mathetics: The technology of education. In M. D. Merrill (Ed.), Instructional design: Readings (pp.
         214-263). Prentice-Hall, Inc.

Gropper, G. L. (2017). Instructional design: Science, technology, both, neither. Educational Technology, 57(1), 40-52.

Honebein, P. C., & Reigeluth, C. M. (2020). The instructional theory framework appears lost. Isn't it time we find it again?
        Revista de Educacion a Distancia, 20(64), 1-24. https://doi.org/10.6018/RED.405871

Jonassen, D. H., Hennon, R. J., Ondrusek, A., Samouilova, M., Spaulding, K. L., Yueh, H. P., Nouri, V., DiRocco, M., &
         Birdwell, D. (1997). Certainty, determinism, and predictability in theories of instructional design: Lessons from
        science. Educational Technology, 37(1), 27-34.

Klauer, K. J. (1997). Instructional design theory: A field in the making. In R. D. Tennyson, F. Schott, N. M. Seel, & S.
        Dijkstra (Eds.), Instructional design: International perspectives: Vol. 1: Theory, (pp. 447-453). Lawrence Erlbaum
         Associates, Publishers.

Matthews, M. T., & Yanchar, S. C. (2018). Instructional designers' perspectives on learners' responsibility for learning.
        Journal of Computing in Higher Education, 30(1), 111-124. https://doi.org/10.1007/s12528-018-9175-3

Mayer, R. E. (2014). Multimedia instruction. In J. M. Spector, M. D. Merrill, J. Elen, & M. J. Bishop (Eds.), Handbook of
        research on educational communications and technology (pp. 385-399). Springer.

McDonald, J. K. (2021). Instructional design as a way of acting in relationship with learners. In B. Hokanson, M. Exter, A.
        Grincewicz, M. Schmidt, & A. A. Tawfik (Eds.), Learning: Design, engagement, and definition (pp. 41-55). Springer
         Nature Switzerland AG. https://doi.org/10.1007/978-3-030-85078-4_4

McDonald, J. K. (2022). A framework for phronetic LDT theory. In H. Leary, S. P. Greenhalgh, K. B. Staudt Willet, & M.-H.
        Cho (Eds.), Theories to influence the future of learning design and technology (pp. 29-46). EdTech Books.
         https://edtechbooks.org/theory_comp_2021/framework_phronetic_LDT_mcdonald

McDonald, J. K., & Michela, E. (2022). "This uncertain space of teaching": How design studio instructors talk about
        design critiques along with themselves when giving critiques. Journal of the Scholarship of Teaching and
        Learning, 22(1), 48-66. https://doi.org/10.14434/josotl.v22i1.30888

Merrill, M. D., Drake, L., Lacy, M. J., Pratt, J. A., & The ID2 Research Group at Utah State University. (1996). Reclaiming
        instructional design. Educational Technology, 36(5), 5-7.

Oyarzun, B., & Conklin, S. (2021). Learning theories. In Design for learning: Principles, processes, and praxis. EdTech
         Books. https://edtechbooks.org/id/learning_theories

Parrish, P. (2012). What does a connoisseur connaît? Lessons for appreciating learning experiences. In Samuel. B. Fee &
        B. R. Belland (Eds.), The role of criticism in understanding problem solving: Honoring the work of John C. Belland
         (pp. 43-53). Springer.

                                                                                       250
Reigeluth, C. M. (1997). Instructional theory, practitioner needs, and new directions: Some reflections. Educational
        Technology, 37(1), 42-47.

Reigeluth, C. M. (1999). What is instructional-design theory and how is it changing? In Instructional-design theories and
        models: A new paradigm of instructional theory (Vol. 2, pp. 5-29). Lawrence Erlbaum Associates.

Richardson, J. C., Ashby, I., Alshammari, A. N., Cheng, Z., Johnson, B. S., Krause, T. S., Lee, D., Randolph, A. E., & Wang, H.
        (2019). Faculty and instructional designers on building successful collaborative relationships. Educational
        Technology Research and Development, 67(4), 855-880. https://doi.org/10.1007/s11423-018-9636-4

Schön, D. A. (1983). The reflective practitioner: How professionals think in action. Basic Books, Inc.
Wilson, B. G. (2005). Foundations for instructional design: Reclaiming the conversation. In J. M. Spector, C. Ohrazda, A.

        Van Schaack, & D. A. Wiley (Eds.), Innovations in instructional technology: Essays in honor of M. David Merrill (pp.
         237-252). Lawrence Erlbaum Associates.
Winn, W. D. (1997). Advantages of a theory-based curriculum in instructional technology. Educational Technology, 37(1),
         34-41.
Wrathall, M. A. (2011). Heidegger and unconcealment: Truth, language, and history. Cambridge University Press.
Wrathall, M. A. (2019). The task of thinking in a technological age. In A. J. Wendland, C. Merwin, & C. Hadjioannou
        (Eds.), Heidegger on technology (pp. 13-38). Routledge.
Wrathall, M. A., & Londen, P. (2019). Anglo-American existential phenomenology. In K. Becker & I. D. Thomson (Eds.),
        The Cambridge history of philosophy, 1945-2015 (pp. 646-663). Cambridge University Press.
         https://doi.org/10.1017/9781316779651.052
Yanchar, S. C., & Faulconer, J. E. (2011). Toward a concept of facilitative theorizing: An alternative to prescriptive and
        descriptive theory in educational technology. Educational Technology, 51(3), 26-31.
Yanchar, S. C., & Slife, B. D. (2017). Theorizing inquiry in the moral space of practice. Qualitative Research in Psychology,
        14(2), 146-170. https://doi.org/10.1080/14780887.2016.1264517
Yanchar, S. C., South, J. B., Williams, D. D., Allen, S., & Wilson, B. G. (2010). Struggling with theory? A qualitative
        investigation of conceptual tool use in instructional design. Educational Technology Research and Development,
        58(1), 39-60. https://doi.org/10.1007/s11423-009-9129-6

                                                                                       251
            Jason K. McDonald

                Brigham Young University
                Dr. Jason K. McDonald is a Professor of Instructional Psychology & Technology at
                Brigham Young University. He brings twenty-five years of experience in industry and
                academia, with a career spanning a wide-variety of roles connected to instructional
                design: face-to-face training; faculty development; corporate eLearning; story
                development for instructional films; and museum/exhibit design. He gained this
                experience as a university instructional designer; an executive for a large,
                international non-profit; a digital product director for a publishing company; and as
                an independent consultant.
                Dr. McDonald's research focuses around advancing instructional design practice
                and education. In particular, he studies the field's tendency to flatten/redefine
                educational issues in terms of problems that can be solved through the design of
                technology products, and how alternative framings of the field's purpose and
                practices can resist these reductive tendencies.
                At BYU, Dr. McDonald has taught courses in instructional design, using stories for
                learning purposes, project management, learning theory, and design theory. His
                work can be found at his website: http://jkmcdonald.com/

This content is provided to you freely by EdTech Books.
Access it online or download it at
https://edtechbooks.org/id/future_views_of_theory_in_learning_and_instructional_design.

                                                        252
  20

The Nature and Use of Precedent in Designing

Elizabeth Boling

As a student, or as a practicing designer, you may have noticed that moment when, even if you are following a detailed
model, you have to figure out what is this material, this experience, this system I am designing actually going to be?
Whether you have consciously done so or not, you have turned to your own memories, your store of precedent
knowledge, in order to tackle these questions. Precedent knowledge is a form of knowledge specific to the activities
and goals of design--and you do have some, whether you realize it consciously or not. When you do understand what
precedent is and think about how you obtain it and use it, you have increased both the discipline and the imagination
that you bring to the act of designing.

Precedent as a Form of Design Knowledge

One of the fundamental elements of design knowledge is precedent (Lawson, 2004; Lawson, 2019). Unlike in law, where
the term precedent refers to the accretion of decisions made over time and constraining future decisions, in design
precedent refers to the store of experiential (episodic) memories each designer accumulates over time--expanding their
future possibilities for actions or decisions. And unlike in science, where past discoveries or established facts form a
solid foundation of knowledge which must be accepted or definitively proven incorrect, precedent knowledge in design
is gathered by individual designers through their experiences of the world. Each designers' store of experiences is
unique to that designer. Even when multiple designers share the same experiences, they do not necessarily pay
attention to the same aspects of those experiences, or recall them later in the same way. Some designers possess
more experience and some less; no single designer's store of experiences is comprehensive or the same as any other
one, and none can be transferred in an abstract way to another designer. Consider something you have experienced
yourself, something that left a vivid memory with you. If you want to share this memory with someone else, you will
likely use concrete means to do so--photos, video, audio--providing you have those means. If you do not, it can be
difficult to transmit to another person the quality of what you have experienced. Now think about how you might share a
career full of design experiences with another designer. You might summarize your memories as principles, or as
lessons learned, but this would not reproduce for that other designer what you know. Some design knowledge, like
principles, can be stated in abstract form for the benefit of others. But precedent knowledge, a designer's store of
experiences, cannot be communicated easily or completely to someone else.

In architecture education, building precedent knowledge has long been a highly structured activity, overtly and rigorously
pursued by means of memorization (Lawson, 2019), and of the requirement to refer to celebrated structures from the
past in support of, or in contrast to, decisions made in the present (Eastman, 2001). Conflict persists over the canon, the
body of works deemed worthy of this intensive study. Some argue that the canon is narrow and discriminatory (Gürel &
Anthony, 2006), while others bemoan moves in architecture education to eliminate the canon because they argue that
the benefits of this form of education outweigh the drawbacks (Breitschmid, 2010).

                                                                                       253
Although fields like instructional design do not maintain a canon, less formal means of noting, storing, and applying
precedent knowledge in architecture also exist. Reviewing publications across multiple fields in which design is the
primary practice, it is possible to see that building and using precedent knowledge is common across all of them
(Boling et al., 2019), although the term precedent is not always the term used and sometimes the references are just
brief glimpses of how precedent is actually used. For example, Rowe (1987) talks about architects and other designers
using literal analogies, "borrowing known or found forms" either in canonic form ("`ideal' proportional systems" as in the
architectural canon), or iconic form ("objects from the natural world ... imagery from some scene, painterly conception,
or narrative account of real or imagined circumstances") (p. 80-83).

In the canonic form of precedent use, an architect may use forms (columns, arches, proportions) from classic
structures in a current design. Without an existing canon in instructional design, it does not make sense to offer an
example of canonic precedent use by instructional designers. Consider, however, examples of the iconic use of
precedent. Madhavan (2015) quotes engineer John Shepherd-Barron, inventor of the ATM cash dispenser as saying, "I
hit upon the idea of a chocolate bar dispenser, but replacing chocolate with cash" (p. 70), and Zimmerman (2003)
mentions in passing that the graphics in his widely-known video game SiSSY FiGHT were "inspired by Henry Darger's
outsider art and retro game graphics" (p. 178). And as an instructional designer, a co-instructor and I used our
experiences with buffet restaurants to offer multiple mini-lessons on technology to our students, letting them choose a
"plateful" of learning in the multimedia production class we were developing.

How Precedent Is Collected

Goldschmidt (2014, p. 1) addresses the way informal, or iconic precedent is collected, saying the "designer possesses a
`prepared eye' which is able to take advantage of stimuli it encounters, randomly or intentionally, in any environment." In
other words, building precedent knowledge is a disciplined practice in which the preparation of experience allows
designers to notice more that is potentially useful and relevant to them than novices or non-designers do. To picture
this, imagine that an instructional designer working for an insurance company takes her children to a theme park where
employees explain to guests, quickly but clearly, how to enter each ride and buckle themselves in safely. This designer
is experiencing a happy day with her kids as many parents do, but because she is a designer, she is also noticing these
just-in-time instructions. Without knowing when she might retrieve and use this memory, she stores it automatically; she
has developed the habit of noticing and remembering experiences that may be relevant to her work.

Within the mind of each designer, precedent knowledge is structured over time into multiple schemata; "precedent
stored in the form of episodic schemata is used by experts to recognize design situations for which gambits are
available" (Lawson, 2004, p. 1). Lawson does not imply that precedent knowledge becomes, or should become, abstract
knowledge by being transformed into generalized principles. He discusses schemata as patterns in which the original
experiential elements remain intact as potential "gambits," or design actions, recognized as possibly applicable to the
immediate design situation. Considering the instructional designer who took her children to the theme park, it is likely
that when she noted park employees giving instructions to guests as they boarded rides, she did not simply store that
memory. This memory probably joined memories of experiences she had stored previously as part of a schema that
might be thought of as, perhaps, "super-condensed instructions." It may also have joined other schema, possibly
"scripted instructions easy for employees to learn," or "minimal scripts."

The Nature of Precedent Knowledge

Drawing on the discussions of precedent in the literature, and the ways in which designers refer to precedent, it is
possible to consider the nature of this special form of knowledge.

Precedent Is Concrete

As noted, precedent knowledge is composed of the memory of experiences, not the abstract meaning we impose on
those experiences. These experiences can be ones in which an object was held and used, a building walked through or

                                                                                       254
lived in, a class taken or taught, an ocean beheld or sailed upon. They may, with equal validity, be vicarious, formed
through encountering pictures, diagrams and narratives that represent designs to those who are not interacting with the
designs directly. Whatever way this form of knowledge has been acquired, it is stored the same way that memories of a
vacation trip or a day at school would be. It contains the details that struck the designer at the time of the experience,
making it flexible in the ways that it can be used because more than one aspect of the experience can be related to a
new design situation.

Precedent Is Neither Good or Bad; Its Value Is Determined When It Is
Used

Precedent knowledge is neutral. The original precedent experience may have been a positive or negative one, and the
designer recalling that experience may have thought at the time, "That's a weak design," or "That's a great design." We
call the knowledge itself neutral, however, because later it will not be confined to use as an exemplar or as a cautionary
tale. A weak, or even a failed, design can yield an affordance or an analogy that proves useful in a future design
situation. In some situations, therefore, a designer might need to know whether an instructional design was proven to
be effective when it was implemented. However, in many more designs its value as precedent is dependent on what it
offers as part of a schema, or of multiple schemata, as inspiration for a design action or as a way to frame a new
design problem.

Precedent Is Relevant When It Is Used; It May or May Not Be Relevant
When It Is Collected

The relevance of any precedent memory to the work of the designer who holds that memory is determined at the time
the precedent is used. As we will see in the discussion of precedent knowledge in use, designers sometimes seek
examples of design intentionally to use them right away as models or inspirations for the work at hand. However, they
also notice and store memories of designs continuously without knowing how they are going to use those memories
later. This means that the exact relevance, even the vague relevance, of much precedent knowledge cannot be assessed
in advance. In order to have precedent knowledge available when it is needed, designers who have been trained and
encouraged to do so form the habit of attending to their environments with a generalized focus on potentially useful
experiences, but also with a productive lack of boundaries as to which experiences they should note.

Precedent Can Be Used Repeatedly, and May Be Used Differently Each
Time

As a form of knowledge that is simultaneously detailed and non-specific, precedent offers rich possibilities that can be
connected by the designer to multiple design situations. Unlike case-based problem-solving-in which there is a match
between the problem and the case being used to solve, or illuminate, it-design precedent does not have to be well-
matched to the situation where it is being used. In some cases, there may be little to indicate that the precedent is
related to the design situation at all. As we will see during the discussion of precedent in use, it is the designer who
perceives the possibility that precedent knowledge affords an insight, a possibility for addressing a design problem (a
gambit), or a bumper that pushes their thoughts in a new direction. Therefore, the designer's perception may be
different in a precedent memory based on the current design situation than based on a previous one. Because this
knowledge has not been abstracted into a fixed, declarative form, the designer is free to use it differently each time they
recall it.

Precedent Knowledge in Use

In a current study of precedent knowledge across the literature in multiple fields of design, Boling et. al. (2019), have
identified several primary modes of precedent use.

                                                                                       255
Linear

A linear use of precedent is one in which the bridge between precedent and a design decision or action (Lawson, 2019)
is conscious, direct, and simply connected to the design. A designer might face a situation in which a particular style of
design is required and look for examples of that style in order to perceive and reproduce its key elements. An
instructional designer may have framed a project as one for which many precedent examples already exist and decide,
appropriately, that drawing on one or more prior designs known to be effective will provide a reasonable template.
Similarly, designers may seek, or draw upon, precedent knowledge to understand what a class or type of design looks
like, sounds like, or how it is constructed. This happens when, for example, an inexperienced designer is preparing to
develop a student workbook and collects examples of existing workbooks to learn more about how this class of design
is put together. This is a kind of deliberate reverse-engineering in which the application of the precedent experience is
determined in advance.

Field-Specific Sources and Validation of Judgment

Using the architectural canon, or less systematized bodies of recognized precedent (sometimes the bodies of work
produced by famous designers), designers can draw on precedent knowledge that they share with many other
designers and use it to guide or validate their own design decisions or actions. In this type of use, schema within the
body of precedent knowledge may be less personal to an individual designer than understood across a professional
community. A majority of precedent experiences for many of these designers may be vicarious-gathered through
photographs and descriptions made available during their studies, found in curated collections published in books and
periodicals. A product designer, for example, may be well aware of a shift toward rounded surfaces and complex
"dashboards" of buttons on household appliances because designs like these appear in trade magazines and win
professional design awards. They do not refer to any single prior design when they develop a dishwasher for the
manufacturer employing the designer, but the widely-known schema informs their design and they refer to that schema
to support their decisions. It may be difficult to picture this form of precedent use among instructional designers
because the field does not now build, or disseminate, organized bodies of precedent, or acknowledge individual
practitioners to the extent of making them famous.

Direct Model for Invention

Engineers in particular use precedent knowledge in a combinatory way, incorporating precedent designs directly into
new ones when subsystems are required for a complex situation and existing examples can be used with minimal
adaption. In what is termed normal design, when the requirement for invention is low, Vincenti (1990) describes a
special form of precedent termed normal configurations, in which the designer's experience includes both the elements
directly usable for the situation and examples of how those elements will work together. Every engineer who needs to
include a pump in a design does not re-invent the pump if there is an existing pump design compatible with the larger
system being created. It is easy to observe a similar form of precedent use among program designers who maintain,
share and draw upon libraries of code. Instructional designers may recognize that this form of precedent use shares
characteristics with reusable learning objects.

Abduction/Analogic Reasoning/Inspiration

Cross (2011) explains that abductive thought suggests "what may be," instead of figuring out what must be (deduction)
or determining what is (induction) (p. 33). The abductive use of precedent involves allowing the experience of what
exists to suggest possibilities for that which is still to be designed. To understand this use of precedent, consider an
instructional designer who is a relay runner in their off-time. They are working on a web-based design for a high-
enrollment college course in which undergraduates are supposed to be learning collaboratively. As they consider that
students are not always excited about group work, it occurs to the designer that the feeling of handing off a baton
during a relay race is both intense (motivating) and positive (satisfying). Without literally building the course as a relay
race, the designer decides to try dividing the class into small teams and incorporating "hand-off-ness" into the process
of working together. The students will set a goal for their final assignment together, then use an online collaborative
writing tool that is open to each of them sequentially for additions and revisions until they complete the assignment.

                                                                                       256
Still inspired by their running experience, the designer builds in some practice in sequential writing ("handing off") as
part of smaller assignments during the semester.

While many fruitless forays may be conducted into one's store of precedent, or there may be only a tenuous connection
between a possibility discovered there with the problem in hand, abduction is not just random exploration. Because
precedent tends, with experience, to gather into schema (Lawson, 1994), analogic use of precedent is likely a key factor
in the efficacy of abductive thought. Analogic reasoning "is a method of activating stored schema based on the
identification of connections, parallels, or similarities between, what are typically perceived as dissimilar items"
(Daugherty & Mentzer, 2008, p. 9). In the case of what we perceive as inspiration, analogic reasoning utilizing multiple
schema may occur and, because these processes are not linear (not propositional or easily converted into rationalized
form), they appear to be--or are experienced as--unexplainable leaps from what is known to something entirely new.
Consider again the instructional designer inspired by their experiences as a relay runner. Let's suppose that in addition
to being a relay runner currently, the designer also participated in improvisational theater as a high school student and
performed in a short-lived jazz ensemble during college. Each of these experiences involves handing off from one
participant to another (a baton, a story line, a musical theme), and by the time they begin designing this college course,
the designer's use of the schema for handing off may not have been a conscious design act as described above. They
may have experienced the idea of sequential authorship in this online class as something that "just came to them;" they
drew on a schema for parallels between it and their design problems that are not obvious on the surface and were not
deliberately sought.

Problem Framing

Dorst and Cross (2001) discuss how a "problem-solution pair is framed" (p. 435) by designers, defining the design
situation by considering the insight that a possible solution can provide. Such possible solutions are drawn from, or
suggested by, the designer's store of precedent knowledge. In this use of precedent, the designer's knowledge is not
being used to guide specific design actions, but to explore, understand and define the situation overall. Many designers
can bring to mind the point in a project where someone throws out an idea; "what if we put together something like a kit
that the instructors in the field could use to assemble lessons on the fly? Like IkeaTM lessons!" The project may or may
not follow this direction, but considering the idea can bring to light factors in the design situation that may or may not
have been evident before--or suggest new information that a project team may need to gather which was not
considered previously.

Design Talk

As designers work together, they engage in design talk, a specialized form of discourse described by Fleming (1989), of
which a central component is discussion of the object (or system, or experience) being designed. Lawson (1994) offers
a vivid description of such talk among architects in which they all used a single term derived from separate but
overlapping, bodies of precedent knowledge and probably from experiential memories the team also shared. While a
comparative lack of precedent dissemination in instructional design can limit this element of design talk, you may be
able to recognize a discussion in which team members share an educational background and use terms like
"WebQuest" or "MOOC" that carry an entire set of experiential meanings for the participants.

Design Models and Precedent

Design models are one of the most widely discussed forms of design knowledge discussed and used in the field of
instructional design (Smith & Boling, 2009). These are a declarative form of knowledge, meaning that they are abstract
and fixed; they can be passed from one person to another through explanation and memorization. Such models are
useful (Branch, 2009), but they do not serve the same purpose for designers that precedent knowledge serves. In fact,
without the judgment of designers (Archer, 1965; Holt, 1997; Merrill; Vickers, 1983; Gibbons et al. 2014; Smith and
Boling, 2009) and their precedent knowledge, design models are not actually effective. Discussion of design judgment

                                                                                       257
may be found elsewhere (Boling et al., 2017; Dunne, 1999; Gray et al., 2015; Nelson & Stolterman, 2014). Here we will
consider the role that precedent knowledge plays within design models.
In each model of design that exists, and there are many (e.g.; Archer, 1965; Dick et al., 2000; Dubberly, 2019; Gustafsen
& Branch, 2002; Lawson & Dorst, 2009; Morrison et al., 2012; Reigeluth & Carr-Chelman, 2009), close examination will
uncover a point at which many aspects of a design situation may be known, but all the rational sources of knowledge
and decision-making have reached the limits of their usefulness. The results of analysis, and the application of
established principles or prescriptions, may have precluded some design moves, or implied fruitful directions for others
(Krippendorf, 2005). But now--what to do precisely? What, exactly, will come to exist that did not exist before all the
preparation was done?
Bruce Archer's (1965) early, influential, and detailed engineering design model, created at the start of high excitement
regarding systematic design, was presented as a long diagram that extended for yards, and included a short text of
fifteen pages explaining it. Of those fifteen pages, ten are devoted to discussing the human activity and perspective
actually required to make the model function, pointing specifically to the one place in the model where nothing but the
human designer can bridge from one step to the next by saying, "there is no escape for the designer from the task of
getting his own creative ideas" (p. 11). And where do those ideas come from? Archer explains that looking at other
people's end results (designs) "including phenomena and artefacts in ... unlikely fields," and "a rich, wide and fruitful
experience ... as well as the capacity for flexibility and fantasy in thought" (p. 12) are required; in other words, building
and using precedent knowledge.
Looking at a more recent and familiar prescriptive model for developing instruction, consider the 4C-ID Model, focused
specifically on designing instruction for complex tasks, and summarized by van Merriënboer et al. (2002).
Figure 1
Ten Steps of the 4C/ID Model. Obtained From www.4cid.com

                                                                                       258
This model is quite detailed, focusing on prescriptions for breaking down complex skills, providing practice of part-
tasks and whole-tasks, and providing materials for support and for just-in-time information. Explanations for using the
model do not address explicitly, as Archer did, what is required from designers to carry out the steps of the model. If we
examine it, though, we will see that the model can only be used when designers employ precedent knowledge.
For example, in the case example the authors provide, the complex task to be learned is literature searching. They
describe a scenario in which a designer has, in step 1, broken down "literature searching" into several "task classes,"
and specified that "learners receive three worked-out (good) examples of literature searches (step 4). Each example
contains an elaborate search query in which Boolean operators are used" (p. 56). Guidelines are offered as to what a
task class may be, and the characteristics that practice items or informational materials should have. However, neither
the model nor the explanation of it acknowledge the invention required to move from knowing what kind of example is
to be offered, to actually inventing this example--or to deciding the nature of the event in which the examples will be
introduced and used.
The very language--"learners receive"--masks what actually has to happen; unless the appropriate worked-out example
of a literature search is readily available, one must be made to exist where previously it did not. Even if the appropriate
example is readily available, its relationship to this instructional event must be created. While this is not a criticism of
the model, it is important that designers recognize the additional forms of knowledge they need to use such models.

Conclusion

While precedent knowledge interacts with other forms of knowledge that designers possess (like their knowledge of
guidelines, theories or principles), it is different in important ways. Designers need to understand those differences so
that they can build and use this knowledge effectively.

                                                                                       259
Application Exercises

The Noticing Journal

The beginning of a disciplined practice in accumulating and using precedent knowledge is to develop the simple
habit of noticing. Commit to a week of noticing and challenge yourself to notice as many kinds of instruction or
performance support around you as you can for that week. Jot down a note about each one, or take a photo
with your phone, so that you can see how many have built up over the week.

Not sure where to begin? Consider how many things you use or see in a day that carry instructions on them --
shampoo, instant noodles, fire extinguishers, bus and subway maps, vending machines. Pay attention to digital
experiences like videogame and software tutorials, or website navigation instructions. Don't limit yourself to
formal instruction either. Did you overhear a parent teaching something to a child or a child showing a parent
how to use a smartphone app? It all counts!

Once you have spent a week on this exercise, consider continuing with it, adding items as you come across
them. While noticing precedent becomes automatic at some point, there is no harm in remaining conscious of
the discipline of noticing.

Exploring Your Existing Store

Set aside 30 minutes to an hour in a quiet place where you can bring to mind past experiences. Begin with the
earliest learning experiences you can remember. From the perspective of an instructional designer, call up as
many as you can. Don't worry if some of them are negative. Precedent knowledge is built from all experiences,
not just exemplary ones. While I recall a great experience with the SRA Reading System in 4th grade, that same
year yields the painful memory of "math races" in which two students had to run to the blackboard and solve a
problem written there quickly, trying to beat each other to the answer.

As you bring these memories of learning to mind, resist the urge to try to turn them into lessons learned, to
diagnose what happened, or to draw conclusions about what happened. What you are doing right now is just
taking stock of how many experiences you already have in your store of precedent, and recognizing that it
belongs to you. You have probably been using it; you may well be conscious of that. And if you have not been,
then this exercise may prove illuminating!

As with the first exercise, consider spending 30 minutes this way more than once. You probably have more than
30 minutes of learning memories!

Deconstruct Your Present

If you are studying in school now, begin to take note of the way one of your courses is structured and of the
materials you are using in this class. Don't stop there, though. The experience of a course is not the same thing
as a syllabus or a textbook. It is the experience that you remember and that forms part of your precedent
knowledge. Write the story of this class--take several pages to do so. While this is your experience, pretend that
you are an observer trying to give someone else a vicarious experience of what it is like to be in the course.

As an example, a short time ago I participated in a square dancing club as a student for a year. While the
structure of the lessons was straightforward--3-4 new calls introduced each week, with several repeated each
time as a refresher, and each student dancing with an experienced partner--the experience of these lessons
would take more time to describe. The experienced dancers were uniformly elderly and enthusiastic. Every
student was greeted warmly at the start of the session, encouraged and praised throughout each dance, and
treated to homemade goodies by the members of the club. Actually, concentrating on learning and dancing at
the same time is surprisingly strenuous, so the goodies were welcome. So was the encouragement! While the
steps we were learning were each pretty simple, they were not called out in a set order. The caller changed the

                                                                                 260
     sequence constantly and more than one student stepped on more than one toe. Every so often the entire group
     came to a halt when one or more students swirled left instead of right. In these instances, I'm sure some of the
     experienced dancers were frustrated but no one complained and we all formed up to begin again. I could go on
     for several more pages, explaining in more detail about the sequence of the steps we learned and how the caller
     handled the dances, what the room was like, the "final exam." Once you get started on this exercise, you will find
     that you have plenty to say as well.

     If you are not studying right now, you can choose a learning experience that, like mine, took place over an
     extended period. Or, if you teach, complete the exercise using one of your own courses, trying to keep that
     observer perspective. And no matter what experience you use for this exercise, once you have completed it,
     read it over and ask yourself what kind of schema this experience may be, or could be, part of. You are not trying
     to abstract this experience, but to consider what others come to mind and what patterns they might both be
     part of. There could be several or many.

     NOTE: As you carry out these exercises, focus on the fact that you are building awareness of your design
     knowledge and thinking. These exercises are not intended to become part of your design process; although I
     have recommended repeating them for the sake of building awareness, they will not tell you what to design or
     how to design. They will strengthen abilities you already have and use.

References

Archer, B. (1965). Systematic method for designers. London, UK: The Council of Industrial Design.

Boling, E., Alangari, H., Hajdu, I., Guo, M., Gyabak, K., Khlaif, Z., Kizilboga, R., Tomita, K., Alsaif, M., Bae, H., Ergulec, F.,
         Lachheb, A.,Zhu, M., Basdogan, M., Buggs, C., Sari, A., Techawitthaychinda, R. (2017). Core judgments of
         instructional designers in practice. Performance Improvement Quarterly, 30(3): 199-219.

Boling, E., Lachheb, Basdogan, M., Abremanka, V., Guo, M., Alghamdi, K., Nadir, H., Zhu, M. & Bhattacharya, P. (2019).
         Design precedent: Critical knowledge as it is defined and used across fields of design. AECT Las Vegas, October
         21-25.

Branch, R. (2009). Instructional design: The ADDIE approach. New York, NY: Springer.

Breitschmid, M. (2010). In defense of the validity of the "canon" in architecture. In Proceedings of the panel "Still on the
         Margin: Reflections on the Perspective of the Canon in Architectural History." 1st conference of the European
         Architectural History Network, Guimaraes, Portugal, 17-20 June 2010.

Buchanan, R. (1995). Rhetoric, humanism and design. In R. Buchanan & V. Margolin (Eds.). Discovering design:
         Explorations in design studies. Chicago, IL: The University of Chicago Press.

Cross, N. (2011). Design thinking: Understanding how designers think and work. Oxford, UK: Berg Publishers.

Dick, W., Carey, L. & Carey,J. (2000). The systematic design of instruction. Boston, MA: Allyn & Bacon.

Dorst, K. & Cross, N. (2001). Creativity in the design process: Co-evolution of problem-situation. Design Studies, 22(5);
         425-437

Dubberly, H. (2019). Models. http://www.dubberly.com/models

Dunne, J. (1999). Professional judgment and the predicaments of practice. European Journal of Marketing, 33(7/8);
         707-719.

                                                                                       261
Eastman, C. (2001). New directions in design cognition: Studies of representation and recall. In C. Eastman, M.
         McKraken & W. Newstetter (eds.). Design knowing and learning: cognition in design education. Oxford, UK:
         Elsevier Science, Ltd.

Fleming, D. (1989). Design talk: Constructing the object in studio conversations. Design Issues, 14(2), 41-62.s
Gray, C.M., Dagli, C., Demiral-Uzan, M., Ergulec, F., Tan, V., Altuwaijri, A., Gyabak, K., Hilligoss, M., Kizilboga, R. & Tomita,

         K., Boling, E.(2015). Judgment and instructional design: How ID practitioners work in practice. Performance
         Improvement Quarterly, 28(3), 25-49.
Gibbons, A., Boling, E. & Smith, K. (2014). Design models. In (M. Spector, D. Merrill, M.J. Bishop & J. Elen, Eds.)
         Handbook for research in Educational Communications and Technology, 4th Ed. New York, NY: Springer.
Gustafson, K. L., & Branch,R. M. (2002). Survey of instructional development models (4th ed.). Syracuse, NY: ERIC
         Clearinghouse on Information & Technology. ED 477517.
Gürel, Ö. & Anthony, K. (2006). The canon and the void: Gender, race, and architectural history texts. Journal of
         Architectural Education, 59(3): 66-76.
Holt, J. E. (1997). The designer's judgement. Design Studies 18(1); 113-123.
Krippendorf, K. (2005). The semantic turn. Boca Raton, FL: CRC Press.
Lawson, B. & Dorst, K. (2009). Design expertise. New York, NY: Routledge.
Morrison, G., Ross, S., Kalman, H. & Kemp, J. (2012). Designing effective instruction. New York, NY: Wiley.
Nelson, H. & Stolterman, E. (2014). The design way: intentional change in an unpredictable world. 2nd Ed. Bpston, MA:
         The MIT Press.
Lawson, B. (2019). The design student's journey: Understanding how designers think. New York, NY: Routledge.
Lawson, B. (2004). Schemata, gambits and precedent: Some factors in design expertise. Design Studies, 24(5); 443-
         457.
Oxman, R. (1994). Precedents in design: A computational model of the organization of precedent knowledge. Design
         Studies, 15(2); 141-157.
Reigeluth, C. M. & Carr-Chellman, A. (2009). Instructional-design theories and models, Volume III: Building a common
         knowledge base. New York, NY: Routledge.
Rowe, P. (1987). Design thinking. Cambridge, MA: The MIT Press.
van Merriënboer, J., Clark, R. & de Crook, M. (2002). Blueprints for complex learning: The 4C/ID-Model. Educational
         Technology Research and Development, 50(2), 39-64.
Vickers, G. V. (1983). The art of judgement. London, England: Harper & Row.
Vincenti, W. G. (1990). What engineers know and how they know it: Analytical studies from aeronautical history.
         Baltimore, MD: Johns Hopkins University Press.
Zimmerman, E. (2003). Play as research: The iterative design process. Design research: Methods and perspectives,
         2003, 176-184.

                                                                                       262
Additional Readings and Resources

Cross, N. (2011). Design thinking: Understanding how designers think and work. Oxford, UK: Berg Publishers.
Lawson, B. (2019). The design student's journey: Understanding how designers think. New York, NY: Routledge.

                      This content is provided to you freely by EdTech Books.
                      Access it online or download it at https://edtechbooks.org/id/precedent.

                                                                                 263
264
  21

Standards and Competencies for Instructional
Design and Technology Professionals

Florence Martin & Albert D. Ritzhaupt

Students entering the field of instructional design must possess a wide array of competencies to be successful in their
future roles (Ritzhaupt & Martin, 2014). Competencies are the knowledge, skills, and abilities professionals need in their
roles, while standards speak to a pre-defined level of quality or attainment of those competencies. Competencies and
standards are essential aspects to advance professionals in this field. Several professional organizations guide the
development of competencies and standards. They also have certification programs for instructional designers and
instructional programs. In this chapter, we review the instructional design standards and competencies both from
professional organizations and those proposed by researchers who guide the educational preparation of instructional
designers and also support their academic and work experiences.

Competency and Standard

In this section, we review the term competency and standard before we introduce instructional design competencies
from professional organizations and from research. Richey et al. (2001) defined competency as "a knowledge, skill or
[ability] that enables one to effectively perform the activities of a given occupation or function to the standards
expected in employment" (p. 26). Spector and De la Teja (2001, p. 2) refer to the term competency as "a state of being
well qualified to perform an activity, task or job function" and competency refers to the "way that a state of competence
can be demonstrated to the relevant community." Thus, competencies are specific to a community of endeavor in which
professionals determine the competencies valuable to the profession. As competencies are identified and developed,
professionals express these competencies as standards to assist professionals, professional associations, academic
programs, and the larger community to better understand the domain of interest.

The KSA framework, comprised of Knowledge, Skills, and Abilities, has been used by researchers to study competencies
in the field. Ritzhaupt et al. (2010) used the KSA framework to categorize educational technology competencies into
knowledge, skills and abilities statements. Figure 1 illustrates this framework in light of three domains used to
characterize the field: creating, using, and managing. The KSAs represent the core processes and resources used by
those practicing in the field, which are the creation of instructional materials, learning environments, and instructional
products using systematic approaches and based on research to improve learning and performance. Using refers to
selecting, using, and implementing educational technologies and processes to support student learning and to enhance
their pedagogy. Management refers to managing people, processes, physical infrastructures, and financial resources to
create diverse learning environments and provide supportive learning communities to improve learning and
performance (AECT Standards 2012, 2008).

Figure 1

Knowledge, Skill, and Abilities Statements for Educational Technologists (Adapted from Ritzhaupt et al., 2010)

                                                                                       265
Standards are critically important to establish a foundation for a field. For instance, the field of project management
established the well-known American National Standards Institute's (ANSI) Guide to The Project Management Body of
Knowledge (PMBOK), which is used as the basis for the Project Management Professional (PMP) certification program
and as the official body of knowledge for the profession.

Instructional Design and Technology Competencies and
Standards From Professional Organizations

The field of instructional design is comprised of several professional organizations, several of which define
competencies and standards for the profession. Table 1 provides a summary of these professional organizations and
the following section provides more details about each. Each organization has a different focus and provides standards
and competencies for their relevant programs. Students should be reminded that these standards and competencies
serve as ideal frameworks, and should not be discouraged by their scope.

Table 1

Professional Organizations Who Publish Instructional Design and Technology Standards

Professional Organization                                  Website Address

International Board of Standards for Training, Performance and Instruction http://ibstpi.org/

International Society for Performance Improvement          https://www.ispi.org/

Association for Talent Development                         https://www.td.org/

Association for Educational Communications and Technology  https://www.aect.org/

                                                   266
Online Learning Consortium                                    https://onlinelearningconsortium.org/
International Society for Technology in Education             https://www.iste.org/
University Professional and Continuing Education Association  https://upcea.edu/

International Board of Standards for Training, Performance and
Instruction (IBSTPI)

http://ibstpi.org/

Ibstpi Vision: To be the leader in setting international standards in the areas of training, instruction, learning, and
performance improvement.

Ibstpi Mission: Develop, validate, and promote implementation of international standards to advance training,
instruction, learning, and performance improvement for individuals and organizations.

Ibstpi has competency sets for various learning and development roles, including the instructional designer. They also
have competency sets for other roles such as training manager, evaluator, instructor, and learner. For the instructional
designer, Ibstpi (2012) developed 22 competencies across five domains.

  1. Professional Foundations
  2. Planning and Analysis
  3. Design and Development
  4. Evaluation and Implementation
  5. Management

Each of these competencies has detailed performance statements and a level of expertise (essential, managerial and
advanced) identified for each of them. Ibstpi goes through a rigorous development model to identify and validate these
competencies. The steps in the model include preliminary analysis of job roles, identification of foundational research,
competency drafting by directors and experts, validation study design, translation of research instruments in multiple
languages and implementation worldwide with working professionals, data analysis and competency validation,
publishing final competencies and performance statements and disseminating the competencies to practitioners,
researchers and organizations.

International Society for Technology in Education (ISTE)

https://www.iste.org/

ISTE Vision: ISTE's vision is that all educators are empowered to harness technology to accelerate innovation in
teaching and learning, and inspire learners to reach their greatest potential.

ISTE Mission: ISTE inspires educators worldwide to use technology to innovate teaching and learning, accelerate good
practice, and solve tough problems in education by providing community, knowledge, and the ISTE Standards--a
framework for rethinking education and empowering learners.

ISTE has developed well-adopted standards for students, teachers, administrators, coaches, and computer science
educators. The ISTE standards are widely accepted in the K-12 community, and have been transformed into assessment
systems (Hohlfeld et al., 2010) and a new professional credential offered by ISTE known as the ISTE Certification, which
is a vendor neutral teacher certification based on the ISTE Standards for Educators. The ISTE Standards for Educators
can be accessed at https://www.iste.org > standards for more information.

267
They include:

  1. Learner: Educators continually improve their practice by learning from and with others and exploring proven and
      promising practices that leverage technology to improve student learning.

  2. Leader: Educators seek out opportunities for leadership to support student empowerment and success and to
      improve teaching and learning.

  3. Citizen: Educators inspire students to positively contribute to and responsibly participate in the digital world.
  4. Collaborator: Educators dedicate time to collaborate with both colleagues and students to improve practice,

      discover and share resources and ideas, and solve problems.
  5. Designer: Educators design authentic, learner-driven activities and environments that recognize and accommodate

      learner variability.
  6. Facilitator: Educators facilitate learning with technology to support student achievement of the ISTE Standards for

      Students.
  7. Analyst: Educators understand and use data to drive their instruction and support students in achieving their

      learning goals.

International Society for Performance Improvement (ISPI)

https://www.ispi.org/

ISPI Vision: Performance improvement practices are recognized globally as an essential part of every organization's
competitive strategy.

ISPI Mission: ISPI and its members use evidence-based performance improvement research and practices to effect
sustainable, measurable results and add value to stakeholders in the private, public, and social sectors.

ISPI has proposed 10 Human Performance Practitioner Standards for instructional designers who assume the
specialized role of performance consultants. The ten standards include,

  1. Focus on Results or Outcomes
  2. Take a Systemic View
  3. Add Value
  4. Work in Partnership with Clients and Stakeholders
  5. Determine Need or Opportunity
  6. Determine Cause
  7. Design Solutions including Implementation and Evaluation
  8. Ensure Solutions' Conformity and Feasibility
  9. Implement Solutions
 10. Evaluate Results and Impact

In addition to the practitioner standards, ISPI also has accreditation standards for organizations and programs/courses.
ISPI certifies practitioners though a rigorous peer-review process and with the opportunity for the practitioners to be re-
certified every three years.

Association for Talent Development (ATD)

https://www.td.org/

ATD Vision: Create a World That Works Better

ATD Mission: Empower Professionals to Develop Talent in the Workplace

ATD certifies professionals in learning and performance (CPLP) and associate professionals in talent development. The
Certified Professional in Learning and Performance (CPLP) candidates are tested on ten (10) areas of expertise and
include:

                                                                                       268
  1. Performance Improvement
  2. Instructional Design
  3. Training Delivery
  4. Learning Technologies
  5. Evaluating Learning Impact
  6. Managing Learning Programs
  7. Integrated Talent Management
  8. Coaching
  9. Knowledge Management
 10. Change Management

ATD also has a competency model for learning and development through which they identify roles, areas of expertise,
and foundational competencies for professionals in learning and performance.

Association for Educational Communications and Technology (AECT)

https://www.aect.org/

AECT Vision: We seek to be the premier international organization in educational technology, the organization to which
others refer for research and best practices.

AECT Mission: Provide international leadership by promoting scholarship and best practices in the creation, use, and
management of technologies for effective teaching and learning.

Januszewski and Molenda (2007) defined Educational Technology as "the study and ethical practice of facilitating
learning and improving performance by creating, using, and managing appropriate technological processes and
resources" (p.1).

AECT has developed standards for educational technologists in five areas. These standards can be accessed from the
AECT website.

  1. Content Knowledge
  2. Content Pedagogy
  3. Learning Environments
  4. Professional Knowledge and Skills
  5. Research

For each of the standards, there are several indicators provided. AECT certifies graduate certificate programs in higher
education who prepare educational technologists based on these standards.

University Professional and Continuing Education Association (UPCEA)

https://upcea.edu/

UPCEA is a leading association of professional, continuing and online education. Their goal is to provide high quality,
professional, continuing and online education programs of practice in higher education.

UPCEA® Purposes:

      To promote quality in professional and continuing higher education.

UPCEA has seven standards identified to provide excellence in online learning leadership.

                                                                                       269
  1. Internal Advocacy
  2. Entrepreneurial Initiative
  3. Faculty Support
  4. Student Support
  5. Digital Technology
  6. External Advocacy
  7. Professionalism

Online Learning Consortium (OLC)

https://onlinelearningconsortium.org/

OLC® Vision: Setting the global standard in online and digital learning

OLC® Mission: Creating community and connections around quality online and digital learning while driving innovation

OLC's Five Pillars of Quality Online Education include:

  1. Learning Effectiveness
  2. Scale
  3. Access
  4. Faculty Satisfaction
  5. Student Satisfaction

Instructional Design and Technology Competencies From
Research

In addition to the professional organizations, several researchers have examined instructional design competencies and
standards over the years. Table 2 below provides details of researchers and the competencies and standards examined
for various instructional design professionals. These articles can be used to plan professional development, academic
programs, and learning experiences for our professionals and emerging professionals.

Table 2

Instructional Design and Technology Competencies From Research

Authors              Audience                Research Method       Competencies Identified

Tennyson (2001)      Instructional           Development of        Educational foundations, instructional
                     Technologists           competency worksheet  systems design methodology, and
                                                                   instructional design process experience

Liu, Gibby, Quiros,  Instructional           Interviews            Problem-solving and decision-making skills
and Demps            Designers
(2002)

Brown, Sugar and     Media Producers in      Biennial Survey       Authoring applications media producers
Daniels (2007)       entry-level multimedia                        regularly use and attributes that are most
                     production                                    important to the choice of an authoring
                                                                   application

                                                         270
Kenny, Zhang,       Instructional        Literature Review        Communication skills, knowledge of
Schwier and         Designers                                     instructional design models, problem-
Campbell (2007)                                                   solving/decision-making skills, and
                                                                  technology skills

Ritzhaupt, Martin   Educational          Job Announcement         Multimedia competencies for educational
and Daniels         Technologists        Analysis and Survey of   technologists
(2010)                                   Professionals

Lowenthal, Wilson   Instructional        Job Announcement         Instructional design experience,
and Dunlap          Designers            Analysis                 communication skills and collaboration
(2010)                                                            skills

Wakefield, Warren Instructional          Job Announcement         Communication and interpersonal skills,
and Mills (2012) Designers               Analysis                 managing multiple instructional

                                                                  Design projects, specific traits, and
                                                                  collaborative skills

Ritzhaupt and       Instructional        In-depth Interviews      Solid foundation in instructional design and
Kumar (2015)        Designers in Higher                           learning theory, possess soft skills and
                    Education                                     technical skills, and have a willingness to
                                                                  learn on the job

Kang and            Educational          Job Announcement         Instructional design, project management,
Ritzhaupt (2015)    Technologists        Analysis                 technical skills, and soft skills

Ritzhaupt, Martin,  Educational          Survey of Professionals  Instructional design, development,
Pastore and Kang    Technologists                                 facilitation, assessment, evaluation,
(2018)                                                            communication, problem-solving, and
                                                                  interpersonal skills

Learning theory also guides ethical decision-making when engaged in the creation of a wide-array of learning solutions.
Professionals must also stay abreast in emerging learning technologies and should possess both the ability to learn
independently and the commitment to lifelong learning. Other knowledge, skills, and abilities were identified in these
studies, but these areas noted were frequently observed and noted.

Conclusion

Professional competencies and standards are helpful ways to communicate the value-add of our professionals to
stakeholders outside of our community in various professional contexts (e.g., healthcare), to assist our professionals
and emerging professionals in planning professional development and lifelong learning (e.g., which webinar to attend),
and to guide our academic programs to align with the expectations of the needs in our field (e.g., selecting which topics
to cover in an instructional design course). While no list of competencies and standards is complete, those enumerated
in this chapter provide readers a glimpse of the status of the profession as described by our professional organizations
and existing research literature. Students entering the profession should spend time on learning these competencies

                                         271
and standards to identify career paths and professional development opportunities. We conclude the chapter with some
independent learning activities for your edification.

   Application Exercises

        1. How should professional competencies and standards be identified, documented, and used by
           professionals in our field? What forms of research methods have been used to identify and document these
           competencies and standards? Write a brief overview of how you think competencies and standards should
           be developed in our profession by reviewing the existing articles listed in Table 2.

        2. Read three of the recent articles listed in Table 2. Using the competencies and standards provided in these
           articles, write a short list of professional learning outcomes for yourself to achieve in the next calendar
           year.

        3. Explore one of the professional organizations discussed in this chapter to identify more detailed
           information about the organization, including when the professional organization hosts its annual
           conference, the cost of membership, the list of readings available with membership, and any of
           professional learning (e.g., webinars) provided by the organization for its members.

        4. Some scholars, such as Ritzhaupt and Martin (2010; 2014; 2018) have expressed the competencies of
           professionals using knowledge, skill, and ability statements. Using this approach, search and identify 10
           instructional design professional position announcements using tools like indeed.com. After identifying the
           announcements, code the knowledge, skill, and ability statements found in these announcements.

References

Brown, A., Sugar, B. & Daniels, L. (2007). Media production curriculum and competencies: Identifying entry-level
         multimedia production competencies and skills of instructional design and technology professionals: Results
         from a biennial survey. Paper presented at Association of Educational Communcations and Technology.

Hohlfeld, T. N., Ritzhaupt, A. D., & Barron, A. E. (2010). Development and validation of the Student Tool for Technology
        Literacy (ST2L). Journal of Research on Technology in Education, 42(4), 361-389.

Januszewski, A., Molenda, M., & Harris, P. (Eds.). (2008). Educational technology: A definition with commentary (2nd
         ed.). Hillsdale, NJ: Lawrence Erlbaum Associate

Kang, Y., & Ritzhaupt, A. D. (2015). A job announcement analysis of educational technology professional positions:
        Knowledge, skills, and abilities. Journal of Educational Technology Systems, 43(3), 231-256.

Kenny, R.F., Zhang, Z., Schwier, R.A., & Campbell, K. (2008). A review of what instructional designers do: Questions
        answered and questions not asked. Canadian Journal of Learning and Technology, 31(1).

Liu, M., Gibby, S., Quiros, O. & Demps, E. (2002). Challenges of being an instructional designer for new media
        development: A view from the practitioners. Journal of Educational Multimedia and Hypermedia, 11(3), 195-219.

Lowenthal, P., Wilson, B. G., & Dunlap, J. C. (2010). An analysis of what instructional designers need to know and be able
         to do to get a job. Presented at the annual meeting of the Association for Educational Communications and
         Technology. Anaheim, CA.

Ritzhaupt, A. D., & Kumar, S. (2015). Knowledge and skills needed by instructional designers in higher education.
        Performance Improvement Quarterly, 28(3), 51-69.

                                                                                       272
Ritzhaupt, A. D., & Martin, F. (2014). Development and validation of the educational technologist multimedia
        competency survey. Educational Technology Research and Development, 62(1), 13-33.

Ritzhaupt, A., Martin, F., & Daniels, K. (2010). Multimedia competencies for an educational technologist: A survey of
        professionals and job announcement analysis. Journal of Educational Multimedia and Hypermedia, 19(4), 421-
         449.

Ritzhaupt, A. D., Martin, F., Pastore, R., & Kang, Y. (2018). Development and validation of the educational technologist
        competencies survey (ETCS): Knowledge, skills, and abilities. Journal of Computing in Higher Education, 30(1),
         3-33.

Spector, J. M., & De la Teja, I. (2001). Competencies for online teaching. ERIC Clearinghouse on Information &
         Technology, Syracuse University.

Tennyson, R. D. (2001). Defining core competencies of an instructional technologist. Computers in Human Behavior, 17,
         355-361.

Williams van Rooij, S. (2013). The career path to instructional design project management: An expert perspective from
        the US professional services sector. International Journal of Training and Development, 17(1), 33-53.

Wakefield, J., Warren, S., & Mills, L. (2012). Traits, skills, and competencies aligned with workplace demands: What
        today's instructional designers need to master. In P. Resta (Ed.), Proceedings of society for information
        technology and teacher education international conference 2012 (pp.3126-3132).

                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/standards_and_competencies.

                                                                                       273
274
Instructional Design Processes

Some instructional design knowledge has been codified into formal processes and methodologies. Designers should
use these as guides to improve their thinking rather than as mandates they must follow. Doing so allows them to
capitalize on the collective wisdom of the field, while remaining free to apply their own judgment to issues as needed.

     Design Thinking
     Robert Gagné and the Systematic Design of Instruction
     Designing Instruction for Complex Learning
     Curriculum Design Processes
     Agile Design Processes and Project Management

                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/instructional_design.

                                                                                       275
276
  22

Design Thinking

Vanessa Svihla

   Editor's Note

     This is a condensed version of a chapter originally published in the open textbook Foundations of Learning and
     Instructional Design Technology. It is printed here under the same license as the original.

Introduction

Many depictions of design process, and a majority of early design learning experiences, depict design as rather linear--a
"waterfall" view of design (Figure 1). This depiction was put forward as a flawed model (Royce, 1970), yet it is relatively
common. It also contrasts with what researchers have documented as expert design practice.
Figure 1
Google Image Search Results of Design as a Waterfall Model

Fortunately, as instructional designers, we have many models and methods of design practice to guide us. While ADDIE
is ubiquitous, it is not a singular, prescriptive approach, though it is sometimes depicted--and even practiced--as such.

                                                                                       277
When we look at what experienced designers do, we find they tend to use iterative methods that sometimes appear a bit
messy or magical, leveraging their past experiences as precedent. Perhaps the most inspiring approaches that reflect
this are human-centered design and design thinking. However, most of us harbor more than a few doubts and questions
about these approaches, such as the following:

     Design thinking seems both useful and cool, but I have to practice a more traditional approach like ADDIE or
      waterfall. Can I integrate design thinking into my practice?
      Design thinking--particularly the work by IDEO--is inspiring. As an instructional designer, can design thinking guide
      me to create instructional designs that really help people?
      Given that design thinking seems to hold such potential for instructional designers, I want to do a research study on
      design thinking. Because it is still so novel, what literature should I review?
      As a designer, I sometimes get to the end of the project, and then have a huge insight about improvements. Is there
      a way to shift such insights to earlier in the process so that I can take advantage of them?
      If design thinking methods are so effective, why aren't we taught to do them from beginning?

To answer these questions, I explore how research on design thinking sheds light on different design methods,
considering how these methods originated and focusing on lessons for instructional designers. I then share a case to
illustrate how different design methods might incorporate design thinking. I close by raising concerns and suggesting
ways forward.

What is Design Thinking?

There is no single, agreed-upon definition of design thinking, nor even of what being adept at it might result in, beyond
good design (Rodgers, 2013), which is, itself, subjective. If we look at definitions over time and across fields (Table 1),
we see most researchers reference design thinking as methods, practices or processes, and a few others reference
cognition or mindset. This reflects the desire to understand both what it is that designers do and how and when they
know to do it (Adams, Daly, Mann, & Dall'Alba, 2011). Some definitions emphasize identity (Adams et al., 2011), as well
as values (e.g., practicality, empathy) (Cross, 1982). In later definitions, design thinking is more clearly connected to
creativity and innovation (Wylant, 2008); we note that while mentioned in early design research publications (e.g.,
Buchanan, 1992), innovation was treated as relatively implicit.

Table 1

Characterizations of Design Thinking (DT) Across Fields, Authors, and Over Time

Design research field  IDEO president introduces    Stanford d.school    Education          Design researchers
characterizes DT       DT to the business world,    (2012) & IDEO        researchers        continue to develop
(1992)                 2008                         (2011) introduce DT  characterize DT    nuanced
                                                    resources for        for education      characterizations
"how designers                                      educators            research &         of DT in practice,
formulate problems,                                                      practice, 2012     2013
how they generate
solutions, and the     "uses the designer's         "a mindset." It is   "analytic and      "a methodology to
cognitive strategies   sensibility and methods      human-centered,      creative process   generate innovative
they employ." These    [empathy, integrative        collaborative,       that engages a     ideas."
include framing the    thinking, optimism,          optimistic, and      person in
problem, oscillating   experimentalism,             experimental.        opportunities to   These include
between possible       collaboration] to match                           experiment,        switching between
                       people's needs with what     The "structured"     create and         design tasks and
                       is technologically feasible  process of design    prototype models,  working iteratively.
                       and what a viable            includes discovery,
                                                    interpretation,

                                                    278
solutions and           business strategy can  ideation,             gather feedback,   (Rodgers, 2013, p.
reframing the problem,  convert into customer  experimentation, and  and redesign"      434)
imposing constraints    value and market       evolution (d.school,
to generate ideas, and  opportunity."          2012; IDEO, 2011)     (Razzouk & Shute,
reasoning abductively.                                               2012, p. 330)
                        (Brown, 2008, p. 2)
(Cross, Dorst, &
Roozenburg, 1992, p.
4)

   Additional Reading

     For another great summary of various approaches to design thinking, see this article by the Interaction Design
     Foundation. This foundation has many other interesting articles on design that would be good reading for an
     instructional design student.
     https://edtechbooks.org/-nh

Where Did Design Thinking Come From? What Does It Mean for
Instructional Designers?

Design thinking emerged from the design research field[1]--an interdisciplinary field that studies how designers do their
work. Initially, design thinking was proposed out of a desire to differentiate the work of designers from that of
scientists. As Nigel Cross explained, "We do not have to turn design into an imitation of science, nor do we have to treat
design as a mysterious, ineffable art" (Cross, 1999, p. 7). By documenting what accomplished designers do and how
they explain their process, design researchers argued that while scientific thinking can be characterized as reasoning
inductively and deductively, designers reason constructively or abductively (Kolko, 2010). When designers think
abductively, they fill in gaps in knowledge about the problem space and the solution space, drawing inferences based on
their past design work and on what they understand the problem to be

   Lesson #1 for ID

     Research on design thinking should inspire us to critically consider how we use precedent to fill in gaps as we
     design. Precedent includes our experiences as learners, which may be saturated with uninspired and ineffective
     instructional design.

A critical difference between scientific thinking and design thinking is the treatment of the problem. Whereas in
scientific thinking the problem is treated as solvable through empirical reasoning, in design thinking problems are
tentative, sometimes irrational conjectures to be dealt with (Diethelm, 2016). This type of thinking has an argumentative
grammar, meaning the designer considers suppositional if-then and what-if scenarios to iteratively frame the problem
and design something that is valuable for others (Dorst, 2011). As designers do this kind of work, they are jointly
framing the problem and posing possible solutions, checking to see if their solutions satisfy the identified requirements

                                                                                       279
(Cross et al., 1992; Kimbell, 2012). From this point of view, we don't really know what the design problem is until it is
solved! And when doing design iteratively, this means we are changing the design problem multiple times.
Other design methods that engage stakeholders early in the design process, such as participatory design (Muller &
Kuhn, 1993; Schuler & Namioka, 1993) and human-centered design (Rouse, 1991) have also influenced research on
design thinking. While these approaches differed in original intent, these differences have been blurred as they have
come into practice. Instead of defining each, let's consider design characteristics made salient by comparing them with
more traditional, linear methods. These methods tend to be iterative, and tend to bring stakeholders into the process
more deeply to better understand their experiences, extending the approach taken in ADDIE, or even to invite
stakeholders to generate possible design ideas and help frame the design problem.
When designing with end-users, we get their perspective and give them more ownership over the design, but it can be
difficult to help them be visionary. As an example, consider early smartphone design. Early versions had keyboards and
very small screens and each new version was incrementally different from the prior version. If we had asked users what
they wanted, most would have suggested minor changes in line with the kinds of changes they were seeing with each
slightly different version. Likewise, traditional approaches to instruction should help inspire stakeholder expectations of
what is possible in a learning design.

   Lesson #2 for ID

     Inviting stakeholders into instructional design process early can lead to more successful designs, but we should
     be ready to support them to be visionary, while considering how research on how people learn might inform the
     design.

Designers who engage with end-users must also attend to power dynamics (Kim, Tan, & Kim, 2012). As instructional
designers, when we choose to include learners in the design process, they may be uncertain about how honest they can
be with us. This is especially true when working with children or adults from marginalized communities or cultures
unfamiliar to us. For instance, an instructional designer who develops a basic computer literacy training for women
fleeing abuse may well want to understand more about learner needs, but should consider carefully the situations in
which learners will feel empowered to share.

   Lesson #3 for ID

     With a focus on understanding human need, design thinking should also draw our attention to inclusivity,
     diversity, and participant safety.

We next turn to an example, considering what design thinking might look like across different instructional design
practices.

Design Thinking in ID Practice

To understand how design thinking might play out in different instructional design methods, let's consider a case, with
the following three different instructional design practices:

                                                                                       280
      Waterfall design proceeds in a linear, stepwise fashion, treating the problem as known and unchanging
      ADDIE design, in this example, often proceeds in a slow, methodical manner, spending time stepwise on each
      phase
      Human-centered design prioritizes understanding stakeholder experiences, sometimes co-designing with
      stakeholders

A client--a state agency--issued a call for proposals that addressed a design brief for instructional materials paired
with new approaches to assessment that would be "worth teaching to." They provided information on the context,
learners, constraints, requirements, and what they saw as the failings of current practice. They provided evaluation
reports conducted by an external contractor and a list of 10 sources of inspiration from other states.

They reviewed short proposals from 10 instructional design firms. In reviewing these proposals, they noted that even
though all designers had access to the same information and the same design brief, the solutions were different, yet all
were satisficing, meaning they met the requirements without violating any constraints. They also realized that not only
were there 10 different solutions, there were also 10 different problems being solved! Even though the client had issued
a design brief, each team defined the problem differently.

The client invited three teams to submit long proposals, which needed to include a clear depiction of the designed
solution, budget implications for the agency, and evidence that the solution would be viable. Members of these teams
were given a small budget to be spent as they chose.

Team Waterfall, feeling confident in having completed earlier design steps during the short proposal stage, used the
funds to begin designing their solution, hoping to create a strong sense of what they would deliver if chosen. They
focused on details noted in the mostly positive feedback on their short proposal. They felt confident they were creating
a solution that the client would be satisfied with because their design met all identified requirements, because they
used their time efficiently, and because as experienced designers, they knew they were doing quality, professional
design. Team Waterfall treated the problem as adequately framed and solved it without iteration. Designers often do
this when there is little time or budget[2], or simply because the problem appears to be an another-of problem--"this is
just another of something I have designed before." While this can be an efficient way to design, it seldom gets at the
problem behind the problem, and does not account for changes in who might need to use the designed solution or what
their needs are. Just because Team Waterfall used a more linear process does not mean that they did not engage in
design thinking. They used design thinking to frame the problem in their initial short proposal, and then again as they
used design precedent--their past experience solving similar problems--to deliver a professional, timely, and complete
solution.

Team ADDIE used the funds to conduct a traditional needs assessment, interviewing five stakeholders to better
understand the context, and then collecting data with a survey they created based on their analysis. They identified
specific needs, some of which aligned to those in the design brief and some that demonstrated the complexity of the
problem. They reframed the problem and created a low fidelity prototype. They did not have time to test it with
stakeholders, but could explain how it met the identified needs. They felt confident the investment in understanding
needs would pay off later, because it gave them insight into the problem. Team ADDIE used design thinking to fill gaps
in their understanding of context, allowing them to extend their design conjectures to propose a solution based on a
reframing of the design problem.

Team Human-centered used the budget to hold an intensive five-day co-design session with a major stakeholder group.
Stakeholders shared their experiences and ideas for improving on their experience. Team Human crafted three
personas based on this information and created a prototype, which the stakeholder group reviewed favorably. They
submitted this review with their prototype. Team Human-centered valued stakeholder point of view above all else, but
failed to consider that an intensive five-day workshop would limit who could attend. They used design thinking to
understand differences in stakeholder point of view and reframed the problem based on this; however, they treated this
as covering the territory of stakeholder perspectives. They learned a great deal about the experiences these

                                                                                       281
stakeholders had, but failed to help the stakeholders think beyond their own experiences, resulting in a design that was
only incrementally better than existing solutions and catered to the desires of one group over others.
The case above depicts ways of proceeding in design process and different ways of using design thinking. These
characterizations are not intended to privilege one design approach over others, but rather to provoke the reader to
consider them in terms of how designers fill in gaps in understanding, how they involve stakeholders, and how
iteratively they work. Each approach, however, also carries potential risks and challenges (Figure 2). For instance,
designers may not have easy access to stakeholders, and large projects may make more human-centered approaches
unwieldy to carry out (Turk, France, & Rumpe, 2002).
Figure 2
Risks and Pitfalls Associated with Different Levels of End-User Participation and Iteration

Critiques of Design Thinking

While originally a construct introduced by design researchers to investigate how designers think and do their work,
design thinking became popularized, first in the business world (Brown, 2008) and later in education. Given this
popularity, design thinking was bound to draw critique in the public sphere. To understand these critiques, it is worth
returning to the definitions cited earlier (Table 1). Definitions outside of the design research field tend to be based in
specific techniques and strategies aimed at innovation; such accounts fail to capture the diversity of actual design
practices (Kimbell, 2011). They also tend to privilege the designer as a savior, an idea at odds with the keen focus on
designing with stakeholders that is visible in the design research field (Kimbell, 2011). As a result, some have raised
concerns that design thinking can be a rather privileged process--e.g., upper middle class white people drinking wine in
a museum while solving poverty with sticky note ideas--that fails to lead to sufficiently multidimensional
understandings of complex processes (Collier, 2017). Still others argue that much of design thinking is nothing new
(Merholz, 2009), to which researchers in the design research field have responded: design thinking, as represented
externally might not be new, but the rich body of research from the field could inform new practices (Dorst, 2011).
These critiques should make us cautious about how we, as instructional designers, take up design thinking and new
design practices. Below, I raise a few concerns for new instructional designers, for instructional designers interested in

                                                                                       282
incorporating new methods, for those who teach instructional design, and for those planning research studies about
new design methods.

My first concern builds directly on critiques from the popular press and my experience as a reviewer of manuscripts.
Design thinking is indeed trendy, and of course people want to engage with it. But as we have seen, it is also complex
and subtle. Whenever we engage with a new topic, we necessarily build on our past understandings and beliefs as we
make connections. It should not be surprising, then, that when our understanding of a new concept is nascent, it might
not be very differentiated from previous ideas. Compare, for example, Polya's "How to Solve it" from 1945 to Stanford's
d.school representation of design thinking (Table 2). While Polya did not detail a design process, but rather a process
for solving mathematics problems, the two processes are superficially very similar. These general models of complex,
detailed processes are zoomed out to such a degree that we lose the detail. These details matter, whether you are a
designer learning a new practice or a researcher studying how designers do their work. For those learning a new
practice, I advise you to attend to the differences, not the similarities. For those planning studies of design thinking,
keep in mind that "design thinking" is too broad to study effectively as a whole. Narrow your scope and zoom in to a
focal length that lets you investigate the details. As you do so, however, do not lose sight of how the details function in a
complex process. For instance, consider the various approaches being investigated to measure design thinking; some
treat these as discrete, separable skills, and others consider them in tandem (Carmel-Gilfilen & Portillo, 2010; Dolata,
Uebernickel, & Schwabe, 2017; Lande, Sonalkar, Jung, Han, & Banerjee, 2012; Razzouk & Shute, 2012).

Table 2

Similarities Between "How to Solve it" and a Representation of Design Thinking

Polya, 1945 How to solve it  Stanford's d.school design thinking representation
Understand the problem       Empathize, Define
Devise a plan                Ideate
Carry out the plan           Prototype
Look back                    Test

My second concern is that we tend, as a field, to remain naïve about the extant and extensive research on design
thinking and other design methods, in part because many of these studies were conducted in other design fields (e.g.,
architecture, engineering) and published in journals such as Design Studies (which has seldom referenced instructional
design). Not attending to past and current research, and instead receiving information about alternative design methods
filtered through other sources is akin to the game of telephone. By the time the message reaches us, it can be distorted.
While we need to adapt alternative methods to our own ID practices and contexts, we should do more to learn from
other design fields, and also contribute our findings to the design research field. As designers, we would do well to learn
from fields that concern themselves with human experience and focus somewhat less on efficiency.

My third concern is about teaching alternative design methods to novice designers. The experience of learning ID is
often just a single pass, with no or few opportunities to iterate. As a result, flexible methods inspired by design thinking
may seem the perfect way to begin learning to design, because there is no conflicting traditional foundation to
overcome. However, novice designers tend to jump to solutions too quickly, a condition no doubt brought about in part
by an emphasis in schooling on getting to the right answer using the most efficient method. Design thinking methods
encourage designers to come to a tentative solution right away, then get feedback by testing low fidelity prototypes.
This approach could exacerbate a new designer's tendency to leap to solutions. And once a solution is found, it can be
hard to give alternatives serious thought. Yet, I argue that the solution is not to ignore human-centered methods in early
instruction. By focusing only on ADDIE, we may create a different problem by signaling to new designers that the ID
process is linear and tidy, when this is typically not the case.

                             283
Instead, if we consider ADDIE as a scaffold for designers, we can see that its clarity makes it a useful set of supports
for those new to design. Alternative methods seldom offer such clarity, and have far fewer resources available, making
it challenging to find the needed supports. To resolve this, we need more and better scaffolds that support novice
designers to engage in human-centered work. For instance, I developed a Wrong Theory Design Protocol
(https://edtechbooks.org/-ub) that helps inexperienced designers get unstuck, consider the problem from different
points of view, and consider new solutions. Such scaffolds could lead to a new generation of instructional designers
who are better prepared to tackle complex learning designs, who value the process of framing problems with
stakeholders, and who consider issues of power, inclusivity, and diversity in their designing.

Concluding Thoughts

I encourage novice instructional designers, as they ponder the various ID models, approaches, practices and methods
available to them, to be suspicious of any that render design work tidy and linear. If, in the midst of designing, you feel
muddy and uncertain, unsure how to proceed, you are likely exactly where you ought to be.

In such situations, we use design thinking to fill in gaps in our understanding of the problem and to consider how our
solution ideas might satisfy design requirements. While experienced designers have an expansive set of precedents to
work with in filling these gaps, novice designers need to look more assiduously for such inspiration. Our past
educational experiences may covertly convince us that just because something is common, it is best. While a traditional
instructional approach may be effective for some learners, I encourage novice designers to consider the following
questions to scaffold their evaluation of instructional designs:

     Does its effectiveness depend significantly on having compliant learners who do everything asked of them without
      questioning why they are doing it?
     Is it a design worth engaging with? Would you want to be the learner? Would your mother, child, or next-door
     neighbor want to be? If yes on all counts, consider who wouldn't, and why they wouldn't.
      Is the design, as one of my favorite project-based teachers used to ask, "provocative" for the learners, meaning, will
      it provoke a strong response, a curiosity, and a desire to know more?
      Is the design "chocolate-covered broccoli" that tricks learners into engaging?

To be clear, the goal is not to make all learning experiences fun or easy, but to make them worthwhile. And I can think of
no better way to ensure this than using iterative, human-centered methods that help designers understand and value
multiple stakeholder perspectives. And if, in the midst of seeking, analyzing, and integrating such points of view, you
find yourself thinking, "This is difficult," that is because it is difficult. Providing a low fidelity prototype for stakeholders to
react to can make this process clearer and easier to manage, because it narrows the focus.

However, success of this approach depends on several factors. First, it helps to have forthright stakeholders who are at
least a little hard to please. Second, if the design is visionary compared to the current state, stakeholders may need to
be coaxed to envision new learning situations to react effectively. Third, designers need to resist the temptation to settle
on an early design idea.

Figure 3

Designers Need to Resist the Temptation to Settle on an Early Design Idea

                                                                                       284
Finally, I encourage instructional designers--novice and expert alike--to let themselves be inspired by the design
research field and human-centered approaches, and then to give back by sharing their design work as design cases
(such as in the International Journal of Designs for Learning ) and by publishing in design research journals .

References

Adams, R. S., Daly, S. R., Mann, L. M., & Dall'Alba, G. (2011). Being a professional: Three lenses into design thinking,
        acting, and being. Design Studies, 32(6), 588-607. doi:10.1016/j.destud.2011.07.004

Brown, T. (2008). Design thinking. Harvard Business Review, 86(6), 84.
Buchanan, R. (1992). Wicked problems in design thinking. Design Issues, 8(2), 5-21. doi:10.2307/1511637

Carmel-Gilfilen, C., & Portillo, M. (2010). Developmental trajectories in design thinking: An examination of criteria.
        Design Studies, 31(1), 74-91. doi:10.1016/j.destud.2009.06.004

Collier, A. (2017). Surprising insights, outliers, and privilege in design thinking. Retrieved from https://edtechbooks.org/-
         ie

Cross, N. (1982). Designerly ways of knowing. Design Studies, 3(4), 221-227. doi:https://edtechbooks.org/-jg
Cross, N. (1999). Design research: A disciplined conversation. Design Issues, 15(2), 5-10. doi:10.2307/1511837
Cross, N., Dorst, K., & Roozenburg, N. F. M. (Eds.). (1992). Research in design thinking: Delft University Press.

d.school. (2012). An introduction to design thinking: Facilitator's guide. Retrieved from https://edtechbooks.org/-Sg
Diethelm, J. (2016). De-colonizing design thinking. She Ji: The Journal of Design, Economics, and Innovation, 2(2), 166-

         172. doi:https://edtechbooks.org/-zY

Dolata, M., Uebernickel, F., & Schwabe, G. (2017). The power of words: Towards a methodology for progress monitoring
        in design thinking projects. Proceedings of the 13th International Conference on Wirtschaftsinformatik. St.
         Gallen, Switzerland.

Dorst, K. (2011). The core of `design thinking' and its application. Design Studies, 32(6), 521-532.
         doi:10.1016/j.destud.2011.07.006

                                                                                       285
Floyd, C. (1988). A paradigm change in software engineering. ACM SIGSOFT Software Engineering Notes, 13(2), 25-38.
IDEO. (2011). Design thinking for educators: Toolkit. https://edtechbooks.org/-HV.
Kim, B., Tan, L., & Kim, M. S. (2012). Learners as informants of educational game design. In J. van Aalst, K. Thompson,

        M. J. Jacobson, & P. Reimann (Eds.), The future of learning: Proceedings of the 10th International Conference of
        the Learning Sciences (Vol. 2, pp. 401-405). Sydney, Australia: ISLS.
Kimbell, L. (2011). Rethinking design thinking: Part I. Design and Culture, 3(3), 285-306. doi:https://edtechbooks.org/-cC
Kimbell, L. (2012). Rethinking design thinking: Part II. Design and Culture, 4(2), 129-148. doi:https://edtechbooks.org/-ef
Kolko, J. (2010). Abductive thinking and sensemaking: The drivers of design synthesis. Design Issues, 26(1), 15-28.
         doi:10.1162/desi.2010.26.1.15
Lande, M., Sonalkar, N., Jung, M., Han, C., & Banerjee, S. (2012). Monitoring design thinking through in-situ interventions.
        Design thinking research (pp. 211-226). Berlin: Springer.
Merholz, P. (2009). Why design thinking won't save you. Harvard Business Review, 09-09. Retrieved from
         https://edtechbooks.org/-tR
Muller, M. J., & Kuhn, S. (1993). Participatory design. Communications of the ACM, 36(6), 24-28.
         doi:10.1145/153571.255960
Norman, D. A., & Draper, S. W. (1986). User centered system design. Hillsdale, NJ: CRC Press.
Razzouk, R., & Shute, V. (2012). What is design thinking and why is it important? Review of Educational Research, 82(3),
         330-348. doi:https://edtechbooks.org/-yk
Rodgers, P. A. (2013). Articulating design thinking. Design Studies, 34(4), 433-437. doi:https://edtechbooks.org/-TT
Rouse, W. B. (1991). Design for success: A human-centered approach to designing successful products and systems:
         Wiley-Interscience.
Royce, W. W. (1970). Managing the development of large software systems Proceedings of IEEE WESCON(pp. 1-9): The
         Institute of Electrical and Electronics Engineers.
Schuler, D., & Namioka, A. (1993). Participatory design: Principles and practices. Hillsdale, NJ: Lawrence Erlbaum
         Associates.
Tripp, S. D., & Bichelmeyer, B. (1990). Rapid prototyping: An alternative instructional design strategy. Educational
        Technology Research and Development, 38(1), 31-44. doi:https://edtechbooks.org/-nb
Turk, D., France, R., & Rumpe, B. (2002). Limitations of agile software processes. Proceedings of the Third International
        Conference on Extreme Programming and Flexible Processes in Software Engineering (pp. 43-46).
Wylant, B. (2008). Design thinking and the experience of innovation. Design Issues, 24(2), 3-14.
         doi:10.1162/desi.2008.24.2.3

                                                                                       286
 Want to Know More about the Design Research Field So You Can Contribute?

  The Design Society publishes several relevant journals:
  Design Science
  CoDesign: International Journal of CoCreation in Design and the Arts
  International Journal of Design Creativity and Innovation
  Journal of Design Research
  The Design Research Society has conferences and discussion forums.
  Other journals worth investigating:
  Design Studies
  Design Issues
  Design and Culture
  Sign up for monthly emails from Design Research News to find out about conferences, calls for special issues,
  and job announcements.

1. For those interested in learning more, refer to the journal, Design Studies, and the professional organization, Design

  Research Society. Note that this is not a reference to educational researchers who do design-based research. 

2. Waterfall might also be used when designing a large, expensive system that cannot be tested and iterated on as a

  whole and when subsystems cannot easily or effectively be prototyped. 

                        This content is provided to you freely by EdTech Books.
                        Access it online or download it at https://edtechbooks.org/id/design_thinking.

                                                                                    287
288
  23

Robert Gagné and the Systematic Design of
Instruction

John H. Curry, Sacha Johnson, & Rebeca Peacock

To begin any study of instructional design, it is beneficial to examine the roots of the field. Where did the field originate?
How did we develop into a field of study and practice? As your study continues, you can better see how the knowledge
base of the field began, how it progressed, and how it was researched and when, which will help you gain a better
understanding of the process and practice of instructional design as well as the field as a whole. Specifically,
understanding the origins of the systematic design of instruction will give the learner a greater appreciation for today's
more robust design theories and models.
As the United States entered World War II, they faced an enormous problem: How were they going to train so many
troops? The numbers are staggering. The military trained over 16 million troops. In addition, the technology of the war
had changed drastically from World War I, and the troops needed to be trained on all the skills necessary to complete
their tasks at hand, and FAST. They did not have the luxury of time--the training needed to be done quickly, effectively,
and efficiently.
After the war ended, cognitive psychologists, many of whom had served in World War II themselves, began studying
how to apply the training lessons from the war to other instructional settings to help people learn better. Combining the
work of those researchers, the systematic instructional design process was born.

                                                                                       289
Gagné's Conditions of Learning

   Conditions of Learning

                                                                   Watch on YouTube
                                                                    Link to transcript

Robert Gagné was working on his Ph.D. in Psychology when World War II began. While assigned to Psychological
Research Unit No. 1, he administered scoring and aptitude tests to select aviation cadets. After the War, Gagné joined
the Air Force Personnel and Training Research Center where he directed the Perceptual and Motor Skills Laboratory. He
held multiple academic positions throughout his career, ranging from the Connecticut College for Women to Princeton
to Florida State University. His experiences in the military and training there guided much of his research. In 1959, he
participated in the prestigious Woods Hole Conference, a gathering of outstanding educators, psychologists,
mathematicians and other scientists from the United States in response to the Soviet Union launching the Sputnik
satellite. The results of the conference were published in Bruner's The Process of Education (1961). Four years later,
Gagné published The Conditions of Learning (1965).

Taxonomy of Learning Outcomes

Gagné posited that not all learning is equal and each distinct learning domain should be presented and assessed
differently. Therefore, as an instructional designer one of the first tasks is to determine which learning domain applies
to the content. The theoretical basis behind the Conditions of Learning is that learning outcomes can be broken down
into five different domains: verbal information, cognitive strategies, motor skills, attitudes, and intellectual skills (see
Figure 1).
Figure 1
Gagné's Domains of Learning

                                                                                       290
Verbal information includes basic labels and facts (e.g. names of people, places, objects, or events) as well as bodies of
knowledge (e.g. paraphrasing of ideas or rules and regulations). Cognitive strategies are internal processes where the
learner can control his/her own way of thinking such as creating mental models or self-evaluating study skills. Motor
skills require bodily movement such as throwing a ball, tying a shoelace, or using a saw. Attitude is a state that affects a
learner's action towards an event, person, or object. For example, appreciating a selection of music or writing a letter to
the editor. Intellectual skills have their own hierarchical structure within the Gagné taxonomy and are broken down into
discrimination, concrete concepts, rule using, and problem solving. Discrimination is when the learner can identify
differences between inputs or members of a particular class and respond appropriately to each. For example,
distinguishing when to use a Phillips-head or a flat-head screwdriver. Concrete concepts are the opposite of
discrimination because they entail responding the same way to all members of a class or events. An example would be
classifying music as pop, country, or classical. Rule using is applying a rule to a given situation or condition. A learner
will need to relate two or more simpler concepts, as a rule states the relationship among concepts. In many cases, it is
helpful to think of these as "if-then" statements. For example, "if the tire is flat, then I either need to put air in the tire or
change the flat tire." Finally, problem solving is combining lower-level rules and applying them to previously
unencountered situations. This could include generating new rules through trial and error until a problem is solved.

Nine Events of Instruction

Beyond his assertion that not all learning is equal, Gagné also theorized an effective learning process consisting of nine
separate and distinct steps or events (see Figure 2). These events build naturally upon each other and improve the
communication supporting the learning process. The events facilitate learner engagement as well as retention of the
content being presented. For an instructional designer, they provide a framework or outline to structure the delivery of
instructional content.

Figure 2

Gagné's Nine Events of Instruction

                                                                                       291
Event one: Gain attention. Before learning can happen, the learners must be engaged. To gain the learners' attention,
any number of strategies can be employed. It could be as simple as turning the lights on and off, the teacher counting
down, or the teacher clapping three times. Other options could include a discussion prompt, showing a video, or
discussing current events.

Event two: Inform learners of objective. Once learners are engaged, they are informed of the objective of the
instruction, which gives learners a road map to the instruction. It allows them to actively navigate the instruction and
know where they are supposed to end up. This could be written on a whiteboard in front of the class, highlighted on
materials, spoken verbally, or posted clearly in an online context.

Event three: Stimulate recall of prior learning. Stimulating recall of prior learning allows learners to build upon previous
content covered or skills acquired. This can be done by referring to previous instruction, using polls to determine
previous content understanding (and then discussing the results), or by using a discussion on previous topics as a
segue between previous content and new content.

Event four: Present the stimulus material. Presenting the stimulus material is simply where the instructor presents new
content. According to Gagné, this presentation should vary depending on the domain of learning corresponding to the
new content.

Event five: Provide learner guidance. Providing learner guidance entails giving learners the scaffolding and tools
needed to be successful in the learning context. Instructors can provide detailed rubrics or give clear instruction on
expectations for the learning context and the timeline for completion.

Event six: Elicit performance. Eliciting performance allows learners to apply the knowledge or skills learned before
being formally assessed. It allows learners to practice without penalty and receive further instruction, remediation, or
clarification needed to be successful.

                                                                                       292
Event seven: Provide feedback. Hand in hand with eliciting performance in a practice setting, the instructor provides
feedback to further assist learners' content or skill mastery.
Event eight: Assess performance. Following the opportunity to practice the new knowledge or skill (events five, six, and
seven), learner performance is assessed. It is imperative that the performance be assessed in a manner consistent with
its domain of learning. For example, verbal knowledge can be assessed using traditional fact tests or with rote
memorization, but motor skills must be assessed by having the learner demonstrate the skill.
Event nine: Enhance retention and transfer. Enhancing retention and transfer gives the learner the opportunity to apply
the skill or knowledge to a previously unencountered situation or to personal contexts. For example, using class
discussion, designing projects, or by writing essays.

   The Nine Events: Explained by Training Cats

                                                                   Watch on YouTube
                                                                    Link to transcript

Gagné's Impact on Instructional Design

The impact Robert Gagné had on the field of instructional design cannot be understated. For example, from his initial
work we can trace the evolution of the domains of learning from the Conditions of Learning through other theories such
as Merrill's Component Display Theory (1994), to Smith and Ragan's Instructional Design Theory (1992), to van
Merrienboer's complex cognitive skills in the 4C/ID model of instructional design (1997). Beyond that, Gagné's Nine
Events of Instruction also paved the way for a systematic process for designing instruction. For the first time, those
designing instruction had a process to follow, a blueprint. And almost 60 years later, Gagné's work still serves as the
basic framework all instructional designers who use systematic processes follow.

                                                                                       293
ADDIE

In 1965, the United States Air Force created their first major instructional system. By 1970, the system had grown into a
full Five-Step Approach to designing instruction (US Air Force). The five steps for designing instruction were: Analyze
system requirements; Define education training requirements; Develop objectives and tests; Plan, develop, and validate
instruction; and Conduct and evaluate instruction. Reflexive within this circular model was feedback and intervention.
This model gave way to the conceptual framework known as ADDIE, upon which the majority of subsequent systematic
instructional design (ID) models are inherently based. It consists of five phases: Analysis, Design, Development,
Implementation, and Evaluation (see Figure 3). Each of these phases builds on the previous phase to systematically
identify and clarify an instructional problem, develop and implement a solution, and evaluate the effectiveness and
efficiency of the solution. Additionally, evaluation occurs throughout the other phases to inform the design of the
instruction.
Figure 3
The ADDIE model

The systematic process of designing instruction begins with the analysis of a problem to determine whether instruction
is a possible solution. The analysis phase includes analyzing the needs, tasks, and learners in order to clarify the
problem, goals and objectives of the instruction, the learning environment, and learner characteristics. Based on the
results of the analyses, the instructional designer clarifies the instructional problem and identifies the instructional
goals and objectives. During the design phase, the instructional designer writes the learning objectives and chooses an
ID model. The development phase consists of creating all instructional materials. Implementation is when the
instruction is delivered to learners either in a formative or summative setting. The evaluation phase is reflexive with
formative evaluation, which consists of ongoing feedback as the instruction is designed and developed, and summative
evaluation consisting of the final evaluation after full implementation. These phases are discussed more in-depth in
their respective chapters.

                                                                                       294
Dick and Carey Model

Working from the conceptual framework of the ADDIE model and building upon a systematic approach to instruction
like Gagné's Conditions of Learning, the Dick and Carey Model is one of many systematic instructional design
processes. While each model may have its own individual process, they also have many characteristics in common
such as attention to detail and precision. The Dick and Carey model is comprised of nine stages incorporating elements
from previous design models as well as elements from behaviorism, cognitivism, and constructivism (see Figure 4).
This model provides the designer with a process that incorporates flexibility and allows the designer to make
appropriate adaptations for their particular situation.
Figure 4
The Dick and Carey model

Instructional Goals

Instructional goals can be set using a variety of methods; however, the key is to determine whether instruction truly is
the solution or if there are other factors that may be contributing to a performance issue. The designer's job is to sift
through many points of data to get to the root of the problem. For example, employees in auto manufacturing may not
be meeting company-defined benchmarks due to poor training, but it could also be due to poorly defined processes that
take too much time to complete. In education, students may fall behind on benchmarks due to poor teaching, but it
could be that teachers are required to cover too many topics and the students are not able to retain all of this
information. To help gather this information, instructional designers perform a performance analysis and needs
assessment.
Performance analysis. In a performance analysis, the designer will compare a desired performance outcome to the
current performance level and identify a performance gap. This process involves reviewing data to identify the gap.
Some designers will use a SWOT (strengths, weaknesses, opportunities, threats) analysis framework to help define this
gap.
Needs assessment. In a needs assessment, the designer works to identify what the learners will need in order to bridge
the identified performance gap. Some methods to help identify this gap can be performance data, including tests,
observations, interviews, surveys, and even doing the work of the learner to help identify challenges or opportunities.

Instructional Analysis

Once goals have been established, it is important to map out the step-by-step process students will need in order to
achieve these goals. In an instructional analysis it often helps to use a flow-chart to map out each skill into its smallest

                                                                                       295
step but also to identify any additional steps or skills, often called subordinate skills, that must be mastered before
mastering the main skill.

Entry Behaviors and Characteristics

It is also essential to identify the behaviors and characteristics of the learner in order to provide the optimum learning
experience. This involves determining what the learner already knows or can do--these are called entry skills. However,
it is also important to gather information on their attitudes toward learning, their motivation for learning, education
backgrounds, ability levels, and personal characteristics such as age or experience with technology.

Performance Objectives

Performance objectives are what the learner will be able to do following instruction. While there are variations on how to
write performance objectives, a general rule is to include a condition, a behavior, and a criterion. Many designers use
Bloom's Taxonomy or Mager's ABCD model to help define measurable behaviors in their objectives. Ultimately,
objectives should be specific and measurable.

Criterion-Referenced Test Items

Criterion-referenced test items are used to measure the performance objectives. These items can be used on
assessments such as pre- and post-tests as well as performance-based measures such as performance observations
using rubrics or attitude changes.

Instructional Strategy

When the assessment has been defined, the designer can work on mapping out an instructional strategy. The designer
will need to review and sequence the content into a meaningful lesson. They will also need to decide on the types of
learning experiences and activities they want the learner to engage in. As described earlier in this chapter, Gagné's Nine
Events of Instruction is one method for structuring a learning experience.

Instructional Materials

Once the instructional framework is developed, appropriate materials are created. This can include using existing print
or media materials or creating new materials. This should be an iterative process, gathering feedback and making
improvements. Some designers will provide rough draft outlines to graphic or multimedia designers for development.

Formative Evaluation

As mentioned previously, formative evaluation is used to help a designer measure the effectiveness of their
instructional strategy and materials. The designer will work with individuals and groups to review the instruction and
identify weaknesses and/or gaps. The materials are revised based on this input to make sure the instruction is
appropriate and clear for the learners.

Summative Evaluation

Finally, the instruction is reviewed by experts and field-tested. The objective is to ensure that the instruction targets the
necessary skills defined in the instructional analysis and produces the desired results in the field.

Conclusion

The study of instructional design is eclectic and full of history. From its roots in cognitive psychology and the training of
troops in World War II to the rise of the systematic instructional design models, researchers have worked to provide
those designing instruction a process by which not only could they create meaningful instruction more quickly, but also
to consider the diversity of learners and learning contexts as well as the difference in the types of content to be learned.

                                                                                       296
If a student of instructional design looks critically at the models and theories in the field, it is not very hard to trace the
continuing influence of these early researchers into today's current practices. For example, Gagné's domains of learning
influenced Merrill's Component Display Theory (Merrill, 1983), as Merrill had similar categories of learning, but gave
them different names. However, the idea that all content falls into one distinct domain of learning shifted with the
research of van Merrienboer (1997) who wrote about complex cognitive skills that have aspects of multiple domains.
The same can be said of the systematic instructional design models. The Conditions of Learning led to the Air Force
model (Department of the Air Force, 1993) and the ADDIE framework. The ADDIE framework gave way to other
instructional design models like the Smith and Ragan (1992); ASSURE (Heinich, Molenda, Russell, and Smaldino, and
2001); and the Morrison, Ross, and Kemp (2012). Most recently, David Merrill (2002) distilled the similarities in each
model down to what he termed the "First Principles of Instruction," a model that encompasses all the others and
provides a new framework for designing problem-based instruction.

The influence of Robert Gagné and the systematic instructional design models on the field of instructional design is
clear. What was new in the 1950s and 1960s is now accepted unilaterally and generally implemented: not all instruction
is equal; there are different domains of learning and each should be presented and assessed appropriately; and an
intentional design process should lead to more effective and efficient instruction.

   Application Exercises

        1. Consider the different ID models in this chapter. What are the benefits of using these processes? What are
           the challenges with using these processes?

        2. Compare and contrast the ID models in this chapter. How might the differences in each model impact the
           overall design process?

        3. Consider instruction you have participated in at school, work, or in the community. Describe how you would
           apply Gagne's Nine Events of Instruction to improve that instruction.

        4. You have been asked to design instruction for a large company on their new telephone system. Use either
           ADDIE or the Dick and Carey Model to describe the steps you would take to provide this instruction. Be
           specific and use the language of the model to frame your discussion.

References

Bloom, B.S. (Ed.) (1956). Taxonomy of Educational Objectives. New York: Longmans, Green.

Bruner, J. (1961). The process of education. Cambridge: Harvard University Press.

Dick, W., & Carey, L. (1996). The systematic design of instruction, 4th edition. New York: Harper Collins College
         Publishers.

Gagné, R.M. (1965). The conditions of learning. New York: Holt, Reinhart & Winston.

Glaser, R. (1962). Psychology and Instructional Technology. Training Research and Education. Glaser, R. (ed). Pittsburgh:
         University of Pittsburgh Press.

Heinich, R., Molenda, M., Russell, J. D., & Smaldino, S. E. (2001). Instructional media and technologies for learning (7th
         ed.), Englewood Cliffs, NJ: Prentice Hall.

Mager, R.F. (1962). Preparing objectives for programmed instruction. Pitman Learning.

                                                                                       297
Merrill, M. D. (1983). Component display theory. In C. M. Reigeluth (Eds), Instructional-design theories and models: An
         overview of their current status (pp. 282-333). New Jersey: Lawrence Earlbaum Associates, Inc.

Merrill, M.D. (1994). The descriptive component display theory. In M.D. Merrill, Instructional design theory (pp. 111-157).
         Educational Technology Publications.

Merrill, M. D. (2002). First Principles of Instruction. Educational Technology Research and Development. 50(3), 43-59.
Skinner, B F. (1959). Teaching Machines. In A. A. Lumsdine & R. Glaser (Ed.) Teaching Machines and Programmed

         Learning. Washington, D.C.: National Education Association. Pp. 137-158.
Smith, P.L. & Ragan, T.R. (1992). Instructional Design. New York: Wiley.
U.S. Air Force. (1970). Instructional System Development. AFM 50-2. Washington DC: U.S. Government Printing Office.
Department of the Air Force. (1993). Manual 36-2234 Instructional System Development.
Van Merrienboer, J.G. (1997). Training complex cognitive skills: A four-component instructional design model for

         technical training. Educational Technology Publications.

                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/robert_gagn_and_systematic_design.

                                                                                       298
  24

Designing Instruction for Complex Learning

Jimmy Frerejean, Jeroen J.G. van Merriënboer, Paul A. Kirschner, Ann Roex, Bert Aertgeerts, &
Marco Marcellis

   Editor's Note

     This is a condensed version of Frerejean, J., van Merriënboer, J. J. G., Kirschner, P. A., Roex, A., Aertgeerts, B., &
     Marcellis, M. (2019). Designing instruction for complex learning: 4C/ID in higher education. European Journal of
     Education, 54(4), 513-524. https://doi.org/10.1111/ejed.12363, and is printed here under the same license as
     the original.

Continuing technological and societal innovations create high demands on the field of education. In order to deal with
increasing globalisation, multidisciplinarity, mobility and the complexity of current and future jobs, a strong emphasis is
placed on quality and efficiency at all levels of education and training (European Union, 2009, 2018). To prepare learners
for a job market that is continually evolving, it is imperative that educational programmes provide them with an
extensive knowledge and skills base that they can apply flexibly when encountering unfamiliar tasks in daily practice.
This requires different instructional design approaches, including a shift away from objectives-based design
approaches towards more task-centred approaches in an attempt to better address the learning of complex cognitive
skills and professional competencies.

The objectives-based approach breaks down tasks into their constituent parts and describes desired outcomes for
each of these part-tasks in learning objectives which are often classified according to a taxonomy such as Bloom's
revised taxonomy (Anderson & Krathwohl, 2001) or Marzano and Kendall's taxonomy (2007). Bloom's revised taxonomy,
for example, classifies objectives in the cognitive domain in six categories based on remembering, understanding,
applying, analysing, evaluating and creating. These taxonomies result from the idea that different instructional methods
are needed to reach objectives in different categories. Teaching concepts and principles (e.g., understanding what a
scientific paper is and how it is structured) requires different instructional methods from teaching the application of
procedures (e.g., carrying out a literature search for a paper). This objectives-based approach is suitable for tasks
where there are few relations between the objectives. However, it is less effective for those that require an integration of
knowledge, skills and attitudes and the coordination of sometimes many different constituent skills (van Merriënboer &
Dolmans, 2015). First, the compartmentalisation of learning into separate categories of objectives and using separate
methods for declarative, procedural and affective learning is ineffective because carrying out a complex professional
task requires more than just the stacking of these constituent elements. Instead, carrying out complex tasks generally
requires an integration of knowledge, skills and attitudes in so-called competencies. Instruction should therefore focus

                                                                                       299
on developing an interconnected knowledge base that allows one to activate different kinds of knowledge when
confronted with new and unfamiliar tasks (Janssen-Noordman, van Merriënboer, van der Vleuten, & Scherpbier, 2006).
Second, the objectives-based approach of teaching complex skills leads to fragmentation. Because it breaks up the
complex tasks into separate isolated parts, students only learn a limited number of skills at the same time. Instruction
is focused on parts of the task and provides little opportunity to learn how to coordinate the performance of these
separate parts into a coherent whole when confronted with a professional task (Lim, Reiser, & Olina, 2009). In an
attempt to address these problems of fragmentation and compartmentalisation, task-centred approaches centre
learning on whole real-world (i.e., authentic) problems or professional tasks as a way to better connect the learning
setting to the workplace setting and foster the necessary skills. This holistic approach advocates creating educational
programmes that contain sequences of learning tasks that are based on authentic professional tasks. Examples of
task-centred models are cognitive apprenticeship (Brown, Collins, & Duguid, 1989), elaboration theory (Reigeluth, 1999),
first principles of instruction (Merrill, 2002) and the four-component instructional design model (4C/ID model) (van
Merriënboer, 1997).
This chapter provides a brief summary of the 4C/ID model and illustrates its application in higher education by
describing an educational programme that was designed using the model. The chapter concludes with a short
reflection on the educational programme that was developed, a list of important considerations for implementing task-
centred curricula and a look at future developments in task-centred learning.

Four-Component Instructional Design (4C/ID)

The basic assumption of the 4C/ID model is that educational programmes for complex learning or the teaching of
professional competencies (i.e., the integration of knowledge, skills, and attitudes and coordination of skills and their
constituents) can be described in four components, namely learning tasks, supportive information, procedural
information and part-task practice (see Figure 1).
Figure 1
Overview of 4C/ID model, Based on Van Merriënboer and Kirschner (2018)

                                                                                       300
Learning tasks form the backbone of the instructional blueprint and are based on authentic real-life situations that are
encountered in practice because this helps the learner to acquire the knowledge, skills and attitudes in an integrated
fashion. Learning tasks can be projects, tasks, cases, problems, or other types of assignments. Importantly, they show a
variation that is representative of the variation in tasks in professional or daily life because this "variability of practice"
will help in the transfer of learning. Learning tasks of equal complexity are grouped together, creating task classes that
are sequenced from simple to complex. Learners start with the simplest tasks that a professional could encounter and
end with tasks at the level of complexity that a recently-graduated student should be able to handle (van Merriënboer,
Kirschner, & Kester, 2003). While working on these tasks, the teacher and the instructional materials provide the
necessary support and guidance to help learners to carry out the tasks to completion. In a process of scaffolding, this
support and guidance are gradually withdrawn until the learners are able to independently carry out tasks of a certain
level of complexity before engaging in more complex tasks (i.e., the next task class). The three other components are
logically connected to this backbone of learning tasks.

While Figure 1 may suggest a linear path through these learning tasks, the model allows for extensive flexibilisation and
personalisation of learning. Learners may be given the opportunity to select different paths through the designed
learning tasks, based on their interest or demonstrated proficiency. One way to support this dynamic selection is by
using electronic development portfolios that help students and teachers to monitor progress and make informed
decisions on future learning tasks that fit the learner's level and needs (Beckers, Dolmans, & van Merriënboer, 2019;
Kicken, Brand-Gruwel, van Merriënboer, & Slot, 2009).

The second component, supportive information, is often referred to as "the theory" and includes information to develop
mental models and cognitive strategies that are necessary to complete the learning tasks. Supportive information aims
at non-recurrent aspects of the task that deal with problem-solving, reasoning and decision making. It can be presented
in the form of lectures, workshops, or study materials and is available for students to study before or while they carry
out the learning tasks. These first two components help students to acquire highly-structured knowledge, or cognitive
schemas. Learning tasks stimulate the construction of such schemas through inductive learning: a process whereby

                                                                                       301
students learn from mindful abstraction from concrete experiences and examples. Supportive information helps
schema construction by elaboration: acquiring new knowledge and linking it to the existing knowledge base.

The remaining two components stimulate the automation of schemas and the development of automatic, task-specific
procedures that can be applied without much demand on cognitive resources. Procedural information aims at recurrent
and procedural aspects and provides step-by-step instructions when the learner performs those aspects. Part-task
practice, the fourth component, can be included to provide repeated practice to train routine skills until they can be
performed automatically. (For a more extensive description of the model, see van Merriënboer, 1997; van Merriënboer et
al., 2003; or van Merriënboer & Kirschner, 2018).

A 4C/ID Approach to a Blended Course in Android App
Development

Developing mobile applications is a typical complex skill, as it requires extensive knowledge of programming languages,
databases, development environments, etc. It also requires multiple skills, such as operating the development software,
writing clean and correct codes, and/or designing a user interface. A professional, critical and creative attitude is
necessary to translate clients' wishes into a working application. Therefore the development of mobile applications
lends itself well to teaching with a task-centred approach. The Amsterdam University of Applied Sciences in the
Netherlands designed a course on Android app development using the 4C/ID model (www.android-development.app).

Traditionally, the design of such courses starts from the "theory". Teachers begin with a clear picture of the information
their students should know and design a series of lectures to "transmit" this theory to the students. In addition, they
design homework assignments to practise with single, small aspects of the whole task. For example, these
assignments could focus on how to use loops in the programming language or creating, reading, updating, and deleting
information in a database. Teachers then mix lectures with small practice items until all topics have been covered. This
follows the approach of traditional design models: starting with the presentation of theoretical knowledge and coupling
it to specific practice items. The 4C/ID model topples this approach: it starts by specifying professional tasks,
translates these into learning tasks and only then investigates which "theory" should help students to complete these
learning tasks.

In their description of the design of the course, Marcellis, Barendsen, and van Merriënboer (2018) elaborate on the use
of 4C/ID and the Ten Steps to achieve a blended design consisting of the four components. The learning tasks form the
backbone of the curriculum. These are not small practice items focused on a particular aspect of the task (i.e., using
loops or updating a database). Instead, learning tasks are whole tasks, based on professional practice, grouped in task
classes that grow in complexity. The first task class starts with the least complex (i.e., simplest) whole tasks. For
example, the very first task asks the student to develop a single-screen app that simulates a dice roll and asks the user
to guess whether the next roll will be higher or lower. Subsequent task classes include learning tasks of increasing
complexity, imposing more demands on the user interface, interaction modes and handling of data. At the end of the
course, students are asked to create multi-screen applications that retrieve information (e.g., movie information,
recipes) from remote locations and allow the user to view, swipe, and manipulate these data.

Students receive strong support and guidance when starting learning tasks in a new and more complex task class. The
design achieves this by employing modelling examples and imitation tasks at the start of each task class. The teacher
demonstrates the development of an application while thinking aloud and asks students to build an identical application
on their computers. In subsequent tasks, students receive partially-completed applications they must finish (i.e.,
completion tasks), while the final tasks in a task class are conventional tasks without any support and guidance. Hence,
support gradually decreases as the student progresses through the task class.

Supportive information helps students with the non-recurrent aspects of the task by providing domain knowledge (e.g.,
how databases work) and systematic approaches to developing apps (e.g., demonstrations by the teacher). Procedural
information helps with the recurrent aspects (e.g., using the development environment or automatic highlighting of

                                                                                       302
incorrect programming syntax). For the designers, providing supportive and procedural information constituted a
serious challenge, as both are subject to frequent and unpredictable changes. For example, development software is
frequently updated with changed functionality and programming languages continually evolve, requiring programmers
to learn new syntax and unlearn old methods. To make sure that the supportive and procedural information reflect the
most recent conventions and rules in the domain, the course designers depended less on pre-prepared lectures or
recorded videos and instead referred students to a list of external sources, including manuals and Android developer
documentation. Not only is it easier to keep a list of links up to date, it also creates a more authentic situation where
students learn to study official documentation, as they would do in practice.

As the course is intended for students from all over the world who are studying full-time or part-time, the designers
chose a blended design where learning tasks, supportive information and procedural information resided in an online
learning environment. Classroom activities can be followed by students on-site and include modelling examples,
imitation tasks in small groups and feedback sessions led by the teacher. Other learning tasks are presented in the
online learning environment and are performed individually by students. Student evaluations show that students
perceive the learning tasks as very helpful for learning how to develop an Android app. The modelling examples
specifically contribute to understanding how to approach a certain challenge in Android app development, especially
when they not only show the actual coding, but also make explicit the reasoning behind each step. In addition, students
perceive the classroom sessions in which they discuss theory using guided questions as beneficial. This Android app
development course illustrates a well-executed application of fundamental 4C/ID principles leading to a course design
that is strongly informed by educational research.

Conclusion

Designing educational programmes using the 4C/ID model is different from designing using objectives-based
approaches. It requires task-centred thinking which may be challenging for designers, teachers and faculty who are
schooled and experienced in objectives-based instructional design. The 4C/ID model is well-aligned with the concept of
competency-based education, but it stresses that competencies should always be clearly related to the professional
tasks the student is expected to carry out after completing the programme. An educational programme taking a
competency framework as a backbone for its development and assessment may still hamper the transfer of learning to
the workplace if the learning activities in that programme are not strongly based on professional tasks. Designing task-
centred learning environments therefore requires assigning equal weight to tasks on the one hand and to the
competencies required to carry out these tasks on the other. The Ten Steps approach starts the design by specifying
professional tasks to serve as a basis for designing learning tasks. The professional and learning tasks are then
explicitly linked to the competencies that are necessary to carry out those tasks up to standards, for example by
generating a matrix with tasks at one end and competencies or standards at the other. As seen in the example provided
in this chapter, this shifts the balance from the atomistic, compartmentalised and fragmented teaching of isolated
objectives towards integrative acquisition of knowledge, skills and attitudes.

Concerning the design and implementation of task-centred curricula, Dolmans, Wolfhagen, and van Merriënboer (2013)
identify 12 common pitfalls and tips that may help to make such (re)designs work. The four most significant deal with
building infrastructure, multidisciplinary teaching teams, continuous progress monitoring and involving students. First,
in task-centred designs, a series of whole-tasks forms the backbone of the programme. Therefore, the main educational
activities consist of small group meetings in which students collaboratively work on these learning tasks. To facilitate
this, there should be sufficient small group rooms available that are equipped with all the necessities such as
whiteboards, projectors, and high-speed wireless Internet. Other facilities could be necessary, such as lecture halls,
simulation labs, or individual reading and studying rooms, but they are present in most schools. It is more often the lack
of sufficient small group rooms that impedes the implementation of task-centred learning.

Secondly, task-centred curricula require the design of a series of learning tasks for an integrative acquisition of
knowledge, skills and attitudes. Content that was previously taught separately by different teachers must now be taught
in an integrated fashion and teachers should therefore work together in multidisciplinary teams that preferably also

                                                                                       303
include outside domain experts working in the field. These experts can help to align the educational programme with
practice and ensure the relevance of the tasks, tools and required knowledge. Teaching staff should also be prepared to
adopt different roles, as teachers in task-centred curricula generally have a tutoring or coaching role in order to facilitate
small group learning or skills training. They may also be involved in whole-task design and continuous assessment. If
teachers are unfamiliar with these new roles, faculty development programmes may be needed to prepare staff
members for this change.

Thirdly, in the Ten Steps approach, the assessment programme is developed simultaneously with the design of learning
tasks because assessment drives learning. In a task-centred curriculum, assessment should not be used solely for
making pass/fail decisions for separate courses. Instead, it should allow for the monitoring of individual student
progress at the level of the whole curriculum. This can be done by using electronic development portfolios that combine
multiple assessment results and provide a dashboard that informs both students and teachers of the students'
progress and improvement. This approach no longer relies on traditional assessment arrangements, such as fixed-
length semesters with pre-planned exam weeks.

Lastly, students themselves should play an important role in the design of the whole curriculum and of individual
learning tasks. Involving students in the design process provides valuable insights into the curriculum's strengths and
weaknesses, as they are the only ones to experience the curriculum. As it is crucial that students experience the
learning tasks as meaningful and useful, their perceptions are very informative for designers. Thoroughly informing
students of the ideas behind the task-centred curriculum may also benefit implementation, as those students can
become advocates of the reform. Furthermore, just as teachers have different roles in a task-centred curriculum,
students also need to be prepared for their new roles. They need to function in small group meetings and learn how to
actively contribute to group discussions, how to act as group leaders or scribes and how to provide effective feedback
to peers. As is the case with faculty development, such student training preferably extends over a prolonged period.

To conclude, dealing with current and future developments in the job market requires that educational programmes
produce lifelong learners who are equipped with the knowledge, skills and attitudes to deal with familiar and unfamiliar
complex tasks in their domain. The way these programmes are designed must therefore match this goal of creating
learners who are able to transfer their knowledge from the learning to the professional setting. Task-centred
instructional design models such as the 4C/ID model stimulate this process by prescribing learning methods that lead
to a rich knowledge base allowing for creative applications of knowledge in new and innovative settings. Additionally,
they create a strong alignment between education and practice by blending learning in the educational setting with
learning in the workplace. This encourages cross-institutional and international collaboration when designing
educational programmes, which is especially important in those settings where tasks require international and
interdisciplinary work, such as the banking, aviation, or tech sector. A clear implication for educational policy is that
collaboration between employers and higher education must be strengthened: A two-way interaction is needed where
educational institutions not only prepare students for the job market, but where employers also bring state-of-the art
knowledge, future job requirements and tasks to the educational institutions.

Future Developments

In the book Ten Steps to Complex Learning, van Merriënboer and Kirschner (2018) present an extensive systematic
approach to design task-centred educational programmes with the 4C/ID model, based on evidence from research on
education and learning going back to the late 1980s. They also identify several important directions for advancing the
model. Future developments will focus on the integration of new educational technologies (e.g., blended programmes,
gamification), dealing with large learner groups (e.g., learning analytics, customisation), teaching domain-generalisable
skills (e.g., intertwining domain-general and domain-specific programmes) and promoting motivation and preventing
negative emotions. With research on these topics now maturing, designers can look forward to an expanded set of
guidelines to design effective, efficient, and enjoyable programmes for complex learning.

                                                                                       304
References

L. W. Anderson, & D. R Krathwohl. (Eds.). (2001). A taxonomy for learning, teaching, and assessing: A revision of Bloom's
         taxonomy of educational objectives. New York, NY: Longman.

Beckers, J., Dolmans, D. H. J. M., & van Merriënboer, J. J. G. (2019). PERFLECT: Design and evaluation of an electronic
        development portfolio aimed at supporting self-directed learning. TechTrends, 63, 420- 427.
         https://edtechbooks.org/-Ibh

Brown, J. S., Collins, A., & Duguid, P. (1989). Situated cognition and the culture of learning. Educational Researcher, 18,
         32- 42. https://edtechbooks.org/-YMzh

Dolmans, D. H. J. M., Wolfhagen, I. H. A. P., & van Merriënboer, J. J. G. (2013). Twelve tips for implementing whole-task
        curricula: How to make it work. Medical Teacher, 35, 801- 805. https://edtechbooks.org/-QZSb

European Union. (2009). Council conclusions of 12 May 2009 on a strategic framework for European cooperation in
        education and training (ET 2020) (2009). Official Journal of the European Union, C119, 2- 10.

European Union. (2018). Council recommendation of 22 May 2018 on key competences for lifelong learning (2018).
        Official Journal of the European Union, C189, 1- 13.

Janssen-Noordman, A. M. B., van Merriënboer, J. J. G., van der Vleuten, C. P. M., & Scherpbier, A. J. J. A. (2006). Design
        of integrated practice for learning professional competences. Medical Teacher, 28, 447- 452.
         https://edtechbooks.org/-gfC

Kicken, W., Brand-Gruwel, S., van Merriënboer, J. J. G., & Slot, W. (2009). Design and evaluation of a development
        portfolio: How to improve students' self-directed learning skills. Instructional Science, 37, 453- 473.
         https://edtechbooks.org/-giF

Marcellis, M., Barendsen, E., & van Merriënboer, J. J. G. (2018). Designing a blended course in android app development
         using 4C/ID. In Proceedings of the 18th Koli Calling International Conference on Computing Education Research
         (Koli Calling '18) (pp. 1- 5). Koli, Finland: ACM Press. https://edtechbooks.org/-YYIi

Marzano, R. J., & Kendall, J. S. (2007). The new taxonomy of educational objectives ( 2nd ed.). Thousand Oaks, CA:
         Corwin Press.

Merrill, M. D. (2002). First principles of instruction. Educational Technology Research and Development, 50, 43- 59.
         https://edtechbooks.org/-jwSq

Reigeluth, C. M. (1999). The elaboration theory: Guidance for scope and sequence decisions. In C. M. Reigeluth (Ed.),
         Instructional-design theories and models: A new paradigm of instructional theory, Vol II (pp. 425- 453). Mahwah,
         NJ: Lawrence Erlbaum Associates Publishers.

van Merriënboer, J. J. G. (1997). Training complex cognitive skills: A four-component instructional design model for
         technical training. Englewood Cliffs, NJ: Educational Technology Publications.

van Merriënboer, J. J. G., & Dolmans, D. H. J. M. (2015). Research on instructional design in the health sciences: From
         taxonomies of learning to whole-task models. In J. Cleland & S. J. Durning(Eds.), Researching medical education
         (pp. 193- 206). Chichester, UK: John Wiley & Sons, Ltd.

van Merriënboer, J. J. G., & Kirschner, P. A. (2018). Ten steps to complex learning: A systematic approach to four-
         component instructional design ( 3rd ed.). New York, NY: Routledge.

van Merriënboer, J. J. G., Kirschner, P. A., & Kester, L. (2003). Taking the load off a learner's mind: Instructional design for
        complex learning. Educational Psychologist, 38, 5- 13. https://edtechbooks.org/-LWeM

                                                                                       305
This content is provided to you freely by EdTech Books.
Access it online or download it at https://edtechbooks.org/id/complex_learning.

                                                        306
  25

Curriculum Design Processes

Bucky J. Dodd

Whether you realize it or not, we experience curriculum every single day. Curriculum influences the most obvious
learning situations like classroom lessons and workplace training sessions, but it also influences a variety of less-
obvious situations such as how we learn about products, how we learn from online tutorials (yes, to an extent this
applies to using YouTube to fix a leaky faucet!), and how organizations plan large-scale change efforts. Curriculum
influences how people learn and grow from very young ages and continues to shape learning experiences throughout
our lives.

The purpose of this chapter is to provide a survey of curriculum design processes across diverse educational and
professional contexts and to highlight essential curriculum design skills embedded in these processes. Curriculum
design is a core pillar of how we educate, train, and engage in formal learning experiences. At the core of curriculum
design is a mental model for how people learn and a design representation for how knowledge and skill transfer occurs
from theory into practice.

For emerging professionals in the instructional design field, curriculum design is one of a series of core competencies
that are necessary for professional success (Burning Glass, 2019). In the most basic of terms, curriculum design is the
process of planning formal learning experiences. Yet, there are many tacit criteria that differentiate between effective
and ineffective curriculum design processes. For the purposes of this chapter, we will examine curriculum design as a
strategic-level process for how learning experiences are designed. This differentiates from instructional design
processes, which tend to involve more operational-level processes. For example, you can differentiate curriculum
design from instructional design as curriculum design is more "big picture thinking" while instructional design is
concerned with more tactical decisions within instructional materials and interactions.

Defining Curriculum Design

Curriculum design is operationally defined for this chapter as the intentional planning, organization, and design of
learning strategies, processes, materials, and experiences towards defined learning and/or performance outcomes.
Curriculum design is concerned with much more than learning materials. In one sense, curriculum design is creating a
holistic plan for the environments where learning happens. This includes considering the physical, digital, social, and
psychological factors that define the spaces and places where people learn (American Educational Research
Association, n.d.).

Figure 1

Diagram Illustrating Elements of Curriculum Design vs. Instructional Design

                                                                                       307
Curriculum design is a team sport. The teams who engage in curriculum design processes are comprised of people
with diverse areas of expertise. Typically, a curriculum design team will include subject matter experts (e.g. faculty
member), curriculum coordinator/director, curriculum oversight groups, instructional design and development
specialists, and teaching/facilitation personnel. Depending on the nature of the curriculum, this can also include
information technology specialists, organizational development specialists, data and research specialists, and senior
leadership.
Figure 2
Diagram Illustrating an Example Curriculum Design Team

                                                                                       308
Curriculum design, when done well, is a process that is collaborative, results-oriented and transforms diverse ideas into
a focused vision for learning.

Designing Curriculum with the End in Mind

The primary goal of curriculum design is aligning learning strategies, materials, and experiences to defined outcomes.
From this standpoint, good curriculum should be results-focused and efficient. To accomplish this, curriculum designers
often use tools such as learner personas, needs analysis, and existing assessment data to determine the scope of a
project. From there, it becomes important to develop learning strategies that connect to the characteristics of the
intended learners to help them reach the desired outcomes.
Designing curriculum with the end in mind involves managing, designing, and organizing learning objectives,
competencies, and standards within a curriculum. The process of designing curriculum with the end in mind is
commonly referred to as "backward design" (Wiggins & McTighe, 1998). The major concept important to curriculum
designers is that instead of starting with content or topics (common historical practice by many educators), backward
design starts with the outcomes and then works backwards to address the content, topics, strategies, and materials.
Figure 3
Diagram Comparing Design Approaches

                                                                                       309
One of the key tools important to backward design is the use of learning objectives taxonomies. One of the most widely
used of these taxonomies is Bloom's Taxonomy (Bloom, 1956). Bloom's Taxonomy organizes learning objectives based
on a "level of learning." The revised version classifies these as: remember, understand, apply, analyze, evaluate, and
create. These levels describe cognitive learning processes that are demonstrated through various forms of behaviors.
Figure 4
Bloom's Taxonomy (Source: https://edtechbooks.org/-dpW)

                                                                                       310
Taxonomies like Bloom's provide a framework for organizing types of learning outcomes and selecting appropriate
curriculum strategies for a specific level of learning. For example, a learning objective at the understand level will likely
be designed far differently than an objective at the evaluate or create levels. This not only influences the types of
strategies used, but also the alignment of curriculum elements and appropriate level of learner (i.e. novice, intermediate,
advanced).

Standards and competency frameworks are common resources curriculum designers use in the process of conducting
their work. These frameworks vary across countries and disciplines; however, they often serve a common purpose of
aligning curriculum to common outcomes and learning/performance goals (e.g. Common Core Standards, Talent
Develop Capability Model).

Representing and Mapping Curriculum

Curriculum design can be a complex process that includes many different forms of data, information, and goals. On a
practical level, curriculum designers often use forms of representations or diagrams to help manage the complexity and
decision-making processes. Curriculum representations provide a method for communicating and collaborating with
others during the curriculum design process. This often includes representing plans for how curriculum will be
organized and made available to the learner.

When mapping curriculum, there are several major and interdependent variables of curriculum that can be important to
visualize. These variables are referred to as design "layers" (Gibbons, 2014). While there can be many different aspects
important to represent in curriculum design processes, the following list outlines major considerations, or design
variables.

      Outcomes--the intended learning or performance result from the curriculum
      Content--the topics or information included in the curriculum
      Instructional Strategies--how the curriculum is organized, structured, and/or presented to achieve a defined result
      Technology--the digital or analog tools used to support the curriculum delivery, development, or assessment
      Data--how metrics and data elements are captured, organized, stored, and represented
      Media--the physical or digital assets used to present curriculum to the learner
      Policy--the guiding principles, rules, or regulations that frame the design of the curriculum

These "layers" represent the essential variables that effective curriculum designers consider when working on
curriculum projects and initiatives. Each of these layers are interdependent and should be considered in concert with
one another and not independently. For example, both outcomes and content should align to ensure the content being
presented supports learners as they work towards achieving specified learning outcomes.

In the process of designing curriculum layers, curriculum designers often use representation tools and methods to
organize ideas and communicate this information to stakeholders. While there are many different approaches to
representing curriculum, the following list highlights common frameworks used in the curriculum design field.

      "The Canvas." Canvas tools are analog or digital documents that organize various elements of curriculum design
      decisions in a single visual field. The purpose of curriculum canvas documents is to provide a structured way of
      organizing ideas at a conceptual level and establishing a common vision for the curriculum. Canvas tools are often
      used to support collaboration and brainstorming processes; however, they can also be used as a way to organize
      individual ideas and communicate those to others in structured ways.

Figure 5

Conceptual illustration of a Canvas Curriculum Planning Tool

                                                                                       311
Visit http://www.lxcanvas.com/ for an example of a canvas-based curriculum design tool. The following video explains
the elements of the Learning Experience Canvas.

   Elements of the Learning Experience Canvas

                                                                   Watch on YouTube

                                                                                       312
      "The Lesson Plan." Lesson plans are one of the most common forms of curriculum representations across various
      education and training contexts. There are many, many different formats and approaches to creating curriculum
      lesson plans. These can range from simple outlines, to structured documents that represent many elements of
      curriculum including learning outcomes, instructional sequence, facilitator prompts, time markers, and teaching
      notes. How a lesson plan should be created is largely dependent on the intended uses and audiences for the
      documents.
Figure 6
Conceptual Illustration of a Lesson Plan

Visit https://edtechbooks.org/-TTeu for example lesson plan formats.
      "The Curriculum Matrix." Curriculum matrices are documents that represent relationships and alignment between
      key variables in the curriculum. This representation is often presented as crosstabulation tables that have one
      variable across the top row and another down the left column. Next, relationship indicators are placed in the
      interesting cells to show a relationship between the two variable elements. A curriculum matrix representation is
      commonly used to show how learning outcomes are represented across courses or units in the curriculum.

Figure 7
Conceptual Illustration of a Curriculum Matrix

                                                                                       313
Visit https://edtechbooks.org/-Jewdb for an example curriculum matrix.
      "The Blueprint." Blueprint-style curriculum representations integrate a number of design variables in a single
      diagram, or "blueprint." The primary purpose of this type of representation is to create documentation that can be
      used to develop and implement curriculum. Blueprint representations often contain instructional elements
      organized in segments and sequences as well as production notes to guide how the curriculum should be
      developed and/or implemented. They often also represent relationships between the various curriculum elements.
      For example, a blueprint may note that a learner must complete a certain set of exercises successfully at a given
      mastery level before progressing to the next set of exercises. The blueprint represents the curriculum design
      strategy in an actionable format.

Figure 8
Conceptual Illustration of a Blueprint Curriculum Diagram

                                                                                       314
Visit https://edtechbooks.org/-LyV for an example curriculum blueprint.

Comparing and Selecting Curriculum Mapping Tools

Selecting the most appropriate curriculum mapping method is often determined based on the current phase and goals
of the curriculum design process. The following table compares the curriculum mapping tools discussed in this chapter
and presents selection considerations.

Table 1

Comparison of Curriculum Mapping Tools

      Canvas                   Lesson Plan                  Matrix                       Blueprint

Uses  Use early in the design  Use to plan and facilitate   Use to align curriculum to   Use to plan the
      process for              specific lessons             outcomes                     sequence and
      brainstorming and                                     Use for assessment of        arrangement of
      ideation                                              learning outcomes            curriculum

Pros  Encourage group          Common format for many       Clearly shows alignment      Visually shows
      collaboration and        professionals in education   between curriculum and       curriculum elements,
      interaction              and training                 outcomes                     flows, and sequence.

Cons  Can lack specifics       Some may see lesson          Some matrix documents        Blueprints can be
      needed to implement      plan as limiting creativity  can be very complex which    visually complex and
      curriculum               or adaptability of           may limit their application  unfamiliar for some
                               curriculum                   in practice                  audiences.

                                            315
   Learning Environment ModelingTM--A Method for Creating Curriculum Blueprints

     A particularly critical challenge faced by many curriculum designers is the lack of a generally accepted design
     language and system in the field (Gibbons, 2014). For example, many design professions have a language to
     represent their work so that the audience versed in the language can easily understand and build from their
     work. Architects, engineers, and software programmers are all examples of professionals that use design
     languages to communicate ideas.

     Learning Environment ModelingTM was created to advance a solution to the absence of a shared design
     language for curriculum and instructional design. At the core of Learning Environment ModelingTM is a language
     that represents five "building blocks" of curriculum, four learning contexts, three transitional actions, and two
     standard notations. These language elements are combined together in a blueprint that shows how the
     curriculum is to be organized and implemented.

     Visit https://edtechbooks.org/-rqn to learn more about Learning Environment ModelingTM and how it can be used
     to design curriculum.

Over the previous several years, a number of digital platforms have become available on the market to manage
curriculum design processes. While these platforms vary in strategy, most seek to increase efficiency and provide a
common digital hub for managing information and communication about curriculum processes. These platforms are
currently distinct from content authoring tools used for creating materials, in that they focus solely on the curriculum
organization and design, rather than content development and delivery. In addition to standalone curriculum design
platforms, many learning management systems are incorporating similar features as part of their capabilities.

Examples of Curriculum Design Platforms

      Coursetune
      eLumens
      Synapes

Examples of Learning Management Systems with Integrated Curriculum Design Capabilities

      Moodle
      Canvas
      Brightspace by D2L
      Blackboard

Innovation Considerations for Curriculum Design Processes

As innovations in learning design and technology are created and scaled, curriculum design processes must adapt to
ensure these methods remain grounded in effective learning practices. This section discusses several innovation trends
and their possible implications on curriculum design processes.

One of the foundational innovations influencing curriculum design processes is a shift from individual-focused design
to team-based curriculum design. Curriculum design is becoming more and more a "team sport" where people from
diverse backgrounds, professions, and areas of expertise work together to create curriculum. The increasing influence
of technology continues to not only incorporate new backgrounds (e.g. technologists), but also allows people from all
around the world to collaborate on curriculum more efficiently. Successful curriculum design professionals are master
facilitators across different types of contexts and through the effective use of collaborative technologies.

                                                                                       316
In addition to curriculum design becoming more collaborative, it is also becoming a more strategic and holistic activity.
Traditionally curriculum was viewed like a product that was self-contained and independent. As such, curriculum design
processes mirrored product development cycles and approaches. As organizations, learning needs, and technologies
change, curriculum design is moving more towards a holistic perspective of learning environment design. This mindset
goes beyond curriculum as a product, and more about designing the collective spaces and places where people learn at
a strategic level. While this may seem like semantics at first, the implications for how curriculum is designed and
connected with other elements in a learning environment is profound.

Moving from curriculum design to learning environment design requires a systems thinking perspective that involves
not only designing elements in the learning environment, but also designing how those elements interact together. A
good example of this is the emergence of blended learning as a common instructional practice. Blended learning is the
combination of classroom and digital learning experience in a unified strategy. Curriculum designers must not only be
considered with the design of classroom curriculum and digital curriculum, but also how they interact together in a
unified learning environment.

The broad adoption of mobile devices have also caused innovations in curriculum design. For example, designing
curriculum that is responsive across different types of devices with different screen sizes is a basic innovation
influencing the field. In addition, designing curriculum for other mobile device features such as geo-positioning,
imaging, and content creation capabilities offer exciting and often challenging situations. Many modern mobile devices
now have immersive virtual space capabilities such as virtual reality and augmented reality. These capabilities highlight
the need for new curriculum design approaches that have not traditionally been required. Mobile and extended reality
learning capabilities will continue to be a major consideration for tomorrow's curriculum designers.

In addition to collaborative design processes, mobile learning, and extended reality innovations, one of the more
profound innovations influencing curriculum design processes is adaptive learning. Adaptive learning is a general
concept that describes the process of providing learners with dynamic learning experiences based on their prior
performance (Educause, 2017). This is commonly used for recommending remediated learning experiences and
encouraging peak learning performance. The reason adaptive learning is such a profound innovation for curriculum
design processes is because it introduces the dynamic layers that have not traditionally been used. For example, a
curriculum designer would create a defined path for learners to follow based on assumptions and requirements set
forth in the design process. Adaptive learning shifts this decision making to programmatic algorithms or a more
complex map of learning experience options. This requires curriculum designers to think and make design decisions
about much more complex and dynamic learning environments.

Conclusion

Curriculum design processes are essential to effective learning experiences across education and professional
contexts. Without effective curriculum design processes, learners often lack the structure and guidance necessary for
optimal learning and organizations lack the ability to effectively measure results and optimize their return on
investments. While we have all experienced curriculum, the process of designing curriculum is changing, becoming
more complex, and incorporating new technologies and strategies. One of the most profound shifts is expanding the
scope of curriculum design to consider how curriculum connects to broader and more networked learning
environments. Curriculum design is an essential skill for emerging education and learning professionals and will
continue to be a dynamic, innovative, and exciting field of practice for years to come.

References

American Educational Research Association (n.d.). Learning Environments SIG 120. Retrieved from
         https://edtechbooks.org/-CKj

                                                                                       317
Bloom, B.S. (1956). Taxonomy of educational objectives: The classification of educational goals. New York, NY:
         Longmans, Green.

Burning Glass (2019). Program Insights [Electronic Database]. Retrieved from https://www.burning-glass.com/
Educause (2017). Seven Things You Should Know About Adaptive Learning. Retrieved from https://edtechbooks.org/-

         gvaK
Gibbons, A. S. (2014). An architectural approach to instructional design. New York, NY: Routledge.
Wiggins, G. P., & McTighe, J. (2005). Understanding by design.

                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/curriculum_design_process.

                                                                                       318
  26

Agile Design Processes and Project Management

Theresa A. Cullen

Due to the changes in and flexibility of computing today, software engineering and instructional design have made
major changes in their approach to development. This evolution to a knowledge economy required processes to change
from approaches where planning and communication happen up front to more agile processes where projects are
completed in smaller chunks with greater communication between team members and clients. Adopting these agile
processes may enable instructional designers to create more flexible designs that better meet the needs of clients and
allow for greater collaboration with others involved in the development process (e.g., UX designers, programmers,
media production).

What is Agile Development?

Agile development has its roots in a document written by 17 people at a retreat in 2001, when a group of software
developers met together to decide how projects should be approached. They were frustrated by static lists of tasks that
were developed early in projects that could not easily be changed, creating a process that lacked flexibility and
feedback. This kind of static list was known within the industry as "Waterfall," referring to the slow trickle of
development that happens from a prescribed list of designs (Nyce, 2017). The group had championed different
approaches during their extensive careers, but it was not until they came together in 2001 that they laid the groundwork
that would change how many products were designed. They agreed that good programming and design had 12 key
principles. As agile processes have been adopted by other fields such as business, education, health care, finance, and
marketing (Oprins et al., 2019), the foundation of the approach has been based on these 12 principles, which make up
the Agile Manifesto (Beck et al., 2001):

                                                                                       319
  1. Our highest priority is to satisfy the customer through early and continuous delivery of valuable software.
  2. Welcome changing requirements, even late in development. Agile processes harness change for the customer's

      competitive advantage.
  3. Deliver working software frequently, from a couple of weeks to a couple of months, with a preference to the shorter

      timescale.
  4. Business people and developers must work together daily throughout the project.
  5. Build projects around motivated individuals. Give them the environment and support they need and trust them to

      get the job done.
  6. The most efficient and effective method of conveying information to and within a development team is face-to-face

      conversation.
  7. Working software is the primary measure of progress. Agile processes promote sustainable development.
  8. The sponsors, developers, and users should be able to maintain a constant pace indefinitely.
  9. Continuous attention to technical excellence and good design enhances agility.
 10. Simplicity--the art of maximizing the amount of work not done--is essential.
 11. The best architectures, requirements, and designs emerge from self-organizing teams.
 12. At regular intervals, the team reflects on how to become more effective, then tunes and adjusts its behavior

      accordingly.

   Summary of the 4 Values of the Agile Manifesto

                                                                   Watch on YouTube

Agile has become a generic term for processes that adhere to the agile principles laid out in this Agile Manifesto, much
like ADDIE is a basic process for instructional design or design thinking is a generic process for approaching design
projects. For example, there are different instructional design approaches (e.g., Dick and Carey; Morrison, Ross, Kemp,
and Kalman; and Smith and Regan), but they all include basic principles such as needs analysis and evaluation. The
same is true of agile processes, as there are different approaches to realize the key components of the agile manifesto
in practice.

                                                                                       320
One of the most prevalent agile approaches is called Scrum, which is used by businesses both in software engineering
and other areas. The value of Scrum is that it has clear roles for different individuals and a variety of agile processes
used in design. Even as agile processes are repackaged in a variety of products (Scrum, Adaptive Project Development
[ADP], Kanban, etc.) they all adhere to these 12 principles that define agile development (Portman, 2019). Key
components present in all products include constant communication with the client, support for the development team,
a focus on deliverables that are fast enough to produce forward motion, and a focus on developing a reliable and robust
product.

   Review of Agile Principles

         Watch on YouTube

To examine designing through an agile framework, let's look at some key components of Scrum. Scrum is defined as "a
framework within which people can address complex adaptive problems, while productively and creatively delivering
products of the highest possible value." (Schwaber & Sutherland, 2017). The Scrum processes and roles defined in
Table 1 support agile processes in practice.

Table 1

Key Terms Related to Scrum Processes

Backlog  A list of tasks that need to be completed as part of the project. This list is prioritized by team
         members at the beginning of each sprint. The backlog allows the team to communicate priorities
         with a client and accurately predict the timeline of a project.

Sprint   A short interval of time (often two weeks) where the team decides on a set of backlog tasks to
         achieve as a team. An example sprint dashboard, representing the backlog and completed items on
         a project, is shown in Figure 1. An example sprint team is shown in Figure 2.

         321
Sprint         As in all agile processes, reflection is an important part of Scrum. At the end of each sprint, the
Retrospective  Scrum team takes time to review how the processes went and make plans to improve processes in
               the future. They ask questions like, "What did we do well and what should continue?" or "What
               could we improve?".

Stand Up       A daily meeting that is designed to last 15 minutes or less to update the team on accomplishments,
               problems, and status. It is called a stand up because it is meant to be kept short by having everyone
               stand during the meeting. During the meeting, team members ask questions like, "What did I do
               yesterday?", "Am I blocked by anything?", or "What do I plan to do today?".

Scrum Master   The person managing the Scrum team who makes sure that all team members are getting the
               resources they need and adhering to the team plan.

Definition of  This is an agreed-upon level of fidelity for product production in each sprint. The team must agree
Done           what is the expectation of each team member's work.

Product Owner This is the person who is responsible for the backlog. They work to develop an accurate timeline
                       and keep the project on track. The Product Owner cannot be the Scrum Master.

Scrum Team     All of the people involved with the design of the product. This could include developers, UX
               designers, QA, and instructional designers, given the project. Different sprints could have different
               team members.

               322
Scrum Basics and Roles

                          Watch on YouTube

Figure 1
Example Sprint Dashboard

                          323
Note. "Sprint dashboard" by Tiendq is licensed under CC BY-NC-ND 2.0
Figure 2
Example Sprint Team

                                                                                       324
Note. "The Agile PM Game (Aug '11)" by VFS Digital Design is licensed under CC BY 2.0

Need for Agile Processes in Instructional Design

By nature of their work, instructional designers (IDs) collaborate with diverse groups such as UX designers,
programmers, media creators, and a variety of subject matter experts. It is to be expected that instructional design
processes may be influenced by those other fields and IDs may even be required to use processes from other
disciplines such as programming. One problem that many teams find is the need for quick results and to maintain good
communication with a client throughout the design process. Adnan and Ritzhaupt (2018) summarized the criticism that
traditional instructional design approaches like ADDIE are not flexible and are less able to produce dynamic projects--
especially those that require flexibility and updating. The flexibility of an agile approach allows for both speed of design,
but also better repurposing and tailoring for different design problems. Being knowledgeable about agile processes in
both instructional design and other fields will enable better team collaboration and client communication (Oprins et al.,
2019).

Fernandez and Fernandez (2006) examined agile versus traditional approaches to project management. In a traditional
approach, instructional designers may meet with a client at the beginning of a process, and then create designs, only to
unveil them when the project is done. They found that these traditional or waterfall approaches did not meet the needs
of the fast-changing markets and the need to have products available quickly to stay competitive. They found that
business practices were changing towards shared responsibility and team collaboration. Leaders were no longer in
charge of projects, but instead they were in charge of teams that have different skills but were all committed to making
the client's project a reality. Agile is a mindset above all else that includes shared responsibility and design, regular
client communication, and embracing change throughout the process.

While an agile approach is different from traditional instructional design approaches, our field has a history of flexible
design approaches too. The most notable was rapid prototyping, proposed by Tripp and Bichelmeyer in 1990. Rapid

                                                                                       325
prototyping comes from software engineering's approach to design where they create prototypes, test them, and then
quickly revise them based on the results. Tripp and Bichelmeyer (1990) argued that instructional problems cannot be
defined fully at one time and therefore a new flexible approach would allow for more adaptability and response to deep
learning issues that become apparent through the design process.

There are many similarities between rapid prototyping discussed in 1990 and agile processes now, specifically, the
focus on the product and being open to change in design through regular communication with clients. That is not to say
that most instructional designers do not communicate with clients regularly, but rather that choosing an agile approach
places the client at the forefront while still not conflicting with key components of the instructional design process like
establishing need, breaking down learning processes, and designing effective evaluation.

Now that you have some of the terminology and history, let's compare traditional instructional design approaches to
agile approaches in Table 2. Using the ADDIE acronym to compare how each method approaches important tasks in
designing effective instruction allows us to see that both approaches deal with the same information and issues and
both can produce effective instruction.

Table 2

Comparison of Traditional Instructional Design to Agile Processes

Task            Traditional Instructional Design      Agile Processes

Client          Utilizes a single or a few major      Relies on delivery points to the client in short time intervals
Involvement     delivery points and feedback points   (often 2 weeks). Focuses on constant iterations.
                with the client.

Analysis        Perform needs and task analysis at    Generates user stories throughout the process to illustrate
                the beginning of the design process.  needs which are revisited at the beginning of each sprint.
                Emphasizes depth.                     Emphasizes speed.

Design          Communicates overall design by        Communicates overall design by creating a backlog of
                producing design documentation at     tasks that the development team chooses from to set goals
                the beginning of the process that is  for each sprint. Design is revisited at the end of each sprint.
                used throughout the entire process.

Development     Produces large parts of a project at  Produces small components of content throughout the
                once based on learning objectives or  process focused on delivery to address items in the
                content topics. Emphasizes            backlog. Emphasizes forward movement on content
                producing a complete learning unit.   development.

Implementation  Implements a complete project or      Releases completed components at the end of each sprint.
                complete module with all parts of     In a web or app-based design, the team can "push out"
                instruction and assessment            parts of the project regularly. The release may not produce
                complete.                             a complete product at every update, but instead focuses on
                                                      continual improvement of released content.

Evaluation      Evaluated as a complete unit with     Engaged in constant evaluation due to the retrospective
                feedback from users and clients.      process at the end of each sprint. Project is constantly

                                                      326
                                                                              going through feedback loops and adjusting based on client
                                                                              and updated user stories at each sprint.

An Example of Scrum in Practice

At a university where I worked, our Information Technology department used Scrum processes to manage large
projects. The department set out to redesign the student and faculty portal. They started by having focus groups of
faculty and students about how they used the existing tools and what they thought was missing. This would be very
similar to learner and needs analysis processes in instructional design. The team used these focus groups to create
user stories. Each user story highlighted a different stakeholder and what they needed from the product they were
designing. Then, the Product Owner took that feedback and created a backlog of tasks with different priorities that had
to be completed (see Figure 3 for an example backlog). They created these with input from all members of the team
with a goal of forward movement and the ability to release improved functionality at regular intervals.
For example, in this project, the first sprints focused on interface design. Members of the sprint team included people
from marketing and web design to make sure that the overall look matched the brand and other components used by
faculty and students. After several sprints to design the look, the product owner moved people down the list of priorities
to begin to design the functionality. Not all tools were redesigned at once. In fact, the Scrum team decided to focus on
student tools first like enrollment and financial services. About halfway through the year-long project, members of the
Scrum team visited faculty and student meetings to ask for input on what they had designed so far. They announced
that it would be several months until faculty functions would be the priority in the backlog and continued to refine
student functions based on feedback.
Throughout the process, the Scrum team published new tools and functions in the portal and had students and faculty
start using them. They gained feedback, reflected on what they had already designed and changed their priorities and
the product moving forward. Redesigning an entire university records and communication portal is a major undertaking,
but by using Scrum processes the team was able to show results and continue to tailor their product to their
stakeholders. They were also able to push out different usable products throughout the year without waiting for the
entire project to be finished.
Figure 3
Example Backlog for the Portal Project

                                                                                       327
Conclusion

Following more agile processes can be a choice by an instructional designer, or it can be part of a company's culture.
Agile processes are not at conflict with good practices in instructional design. In fact, steps like creating a backlog that
prioritize features, gaining customer feedback on designs during the process, and being reflective is good practice.
Taking an agile approach to instructional design can benefit the team dynamic and instructional product. The team
dynamic is improved through improved client communication, flexibility, and creating components that could be better
reused in other projects with similar user stories. Tripp and colleagues (2016) found that a workplace that embraces
agile processes could increase job satisfaction among its employees. Fernandez and Fernandez (2006) found that agile
made projects more adaptable and able to produce products faster. Oprins and colleagues (2019) point out having an
agile approach emphasizes the importance of people in an organization, builds empathy, and guards against
automation. Agile processes, when followed, can improve team communication and keep team members from pursuing
dead ends or wasting important time because all of the team were not "on the same page."

There are also downsides to following agile processes. Regular communication with team members and clients takes
time and can slow down some aspects of design. Since agile processes are designed to always be flexible, it can be
frustrating to live in constant change, even if it produces a better product. For many, following agile processes requires
a change in approach and communication style which can be difficult. Finally, agile is a buzzword: There are many
companies that say that they use agile processes but do not have trained individuals, necessary resources for team
members, and do not embrace the agile mindset. This kind of workplace can be incredibly frustrating because it can
produce unpredictable results. Agile processes take commitment from all stakeholders and the leaders of an
organization or company.

Next Steps

Instructional designers have many opportunities to become more knowledgeable of agile processes.

First, there are many resources available about agile processes and thought processes available online. In addition to
the Agile Manifesto itself, many Scrum professionals start with the Scrum guide (https://edtechbooks.org/-rZPW) to
learn about agile processes in practice.

Second, talk to people working in the industry. Reach out to alumni from your instructional design program and ask
them about the project management processes that they use.

Third, for those interested in pursuing an agile philosophy further, consider pursuing a professional certification as a
Scrum Master (https://edtechbooks.org/-jNf). The certification can be earned after taking a short workshop about agile
processes and then passing an exam. The workshops can range from $1000 to $5000, but the training produces a
certification that can be included on a resume or LinkedIn profile.

Takeaways

As an instructional designer, you will work with a variety of teams within a company (instructional designers, content
experts, trainers, HR, etc.). Understanding different ways that projects are managed within a company not only makes
you more valuable within the organization, a better team member, but also helps you to be more flexible to your
approach to instructional problems. Many companies that have adopted this approach would value instructional
designers who are both aware of and have practiced agile approaches to be able meet the changing needs of the
organization and its clients. If this is the way that you enjoy working, then become more knowledgeable on agile
processes and look for a company that clearly integrates it into their culture.

                                                                                       328
Links and Resources

      Scrum Guide (A good place to start)
      https://edtechbooks.org/-rZPW)
      Scrum Alliance (https://edtechbooks.org/-jNf).
      Agile Instructional Design Course on LinkedIn Learning https://edtechbooks.org/-XDT

                                                                                 329
Agile Activities

Following are six activities designed to help you think critically about agile processes:

1. (A collaborative slides version of this activity is available view only at https://bit.ly/agileactivity. To be able to
edit, choose make a copy from the file menu, then save it to your own Google Drive.)

An instructional design project on training about workplace bullying was handed to a team that had been
designed in a traditional way. The new team uses agile processes. How could they break down this project into
smaller chunks (aka create a backlog) to allow for prioritizing parts of the task and providing logical places to
stop and receive feedback from the client throughout the process?

      The tasks developed by the traditional team included:
      Explain terminology: bullying, bystander, and victim.
      Outline the roles that each individual takes in a bullying incident.
      Outline what employees should do if they witness bullying.
      Outline what employees should do if they experience bullying.
      Create a design for the look of the materials to create consistency between a face to face and online
      learning module.
      Create a list of resources available for additional information and training.
      Outline the company policies on bullying.
      Outline the processes for reporting bullying.
      Create example stories or cases with different perspectives (bully, bystander, and victim).
      Develop face-to-face workshop that will last 90 minutes.
      Develop an online tutorial that can be used to document compliance.
      Develop discussion questions for in person training.
      Develop quiz questions for an online module which can be recorded for compliance.
      Create a video with a bullying scenario from the workplace for in person training.
      Create a video with a bullying scenario for the online training.
      Develop a script and support materials for a face-to-face facilitator.

After breaking up the task into smaller groups, then plan the backlog. Many companies use a table design to
show the progression of a project. Assign priorities to the groups you created above and explain why you
arranged them that way.

In Progress       Soon  Future  Completed

Did the original team forget any task they might need? What were the tasks? How does this agile process help
to refine the project and identify gaps?
2. You are designing a remote learning activity to be used by a teacher for a middle school classroom. Create a
user story for the stakeholders involved. Think about parent, student, teacher, and curriculum coach. Explain
what their needs may be and think about how your design may need to incorporate those needs.
3. Agile teams have been shown to be more effective than traditional teams. Why do you think this is the case?

                                                                                 330
     4. Explain how agile processes value the relationship with the client.
     5. At the end of a sprint, an agile team takes time to do a retrospective before starting the next group of tasks.
     How does scheduling time to reflect on a project in process increase efficiency when designing?
     6. Read over the agile manifesto. Give examples of how it honors collaboration and the value of stakeholders.

References

Adnan, N.H. & Ritzhaupt, A.D. (2018). Software engineering design principles applied to instructional design: What can
        we learn from our sister discipline? TechTrends, 62, 77-94. DOI: 10.1007/s11528-017-0238-5

Agile Manifesto (2019) Agile Manifesto retrieved from https://agilemanifesto.org/iso/en/principles.html
Beck, K., Beedle, M., Van Bennekum, A., Cockburn, A., Cunningham, W., Fowler, M., ... & Kern, J. (2001). Manifesto for

        agile software development.
Budoya, C.M., Kissake, M.M., & Mtebe, J.S. (2019). Instructional design enabled agile method using ADDIE model and

        feature driven development method. International Journal of Education and Development using Information and
        Communication Technology, 15(1), 35-54.
Fernandez D.J & Fernandez J.D. (2008) Agile project management--agilism versus traditional approaches, Journal of
        Computer Information Systems, 49:2, 10-17, DOI: 10.1080/08874417.2009.11646044
Monteiro, C. V., da Silva, F. Q., dos Santos, I. R., Farias, F., Cardozo, E. S., do A. Leitão, A. R., Neto, D.N.& Pernambuco
        Filho, M. J. (2011, May). A qualitative study of the determinants of self-managing team effectiveness in a scrum
        team. In Proceedings of the 4th International Workshop on Cooperative and Human Aspects of Software
         Engineering (pp. 16-23).
Nyce, C. M. (2017, December 8). Agile software development: A history. The Atlantic. Retrieved from:
         https://www.theatlantic.com/technology/archive/2017/12/agile-manifesto-a-history/547715/
Oprins, R. J., Frijns, H. A., & Stettina, C. J. (2019, May). Evolution of scrum transcending business domains and the
        future of agile project management. In International Conference on Agile Software Development (pp. 244-259).
         Springer, Cham.
Portman, H. (2019). A project manager's guide to agile methodologies. Retrieved from:
         https://thedigitalprojectmanager.com/agile-methodologies/
Tripp, S. D., & Bichelmeyer, B. (1990). Rapid prototyping: An alternative instructional design strategy. Educational
        Technology Research and Development, 38(1), 31-44.
Tripp, J. F., Riemenschneider, C., & Thatcher, J. B. (2016). Job satisfaction in agile development teams: Agile
        development as work redesign. Journal of the Association for Information Systems, 17(4), 1.
Sutherland, J., & Schwaber, K. (2013). The scrum guide. The definitive guide to scrum: The rules of the game. Scrum.org,
         268. Retrieved from: https://www.scrumalliance.org/learn-about-scrum/the-scrum-guide
Willeke, M. H. (2011, August). Agile in academics: applying agile to instructional design. In 2011 Agile Conference (pp.
         246-251). IEEE.doi: 10.1109/AGILE.2011.17.

                                                                                       331
This content is provided to you freely by EdTech Books.
Access it online or download it at https://edtechbooks.org/id/agile_design_process.

                                                        332
Designing Instructional Activities

Instructional designers have developed a large number of practical techniques for designing specific kinds of learning
activities. The chapters in this section focus on tips, principles, or other considerations important for making the
activities they design as effective as possible.

     Designing Technology-Enhanced Learning Experiences
     Designing Instructional Text
     Audio and Video Production for Instructional Design Professionals
     Using Visual and Graphic Elements While Designing Instructional Activities
     Simulations and Games
     Designing Informal Learning Environments
     The Design of Holistic Learning Environments
     Measuring Student Learning

                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/designing_instructio.

                                                                                       333
334
  27

Designing Technology-Enhanced Learning
Experiences

Richard E. West & Bohdana Allman

The field of instructional/learning design has at times been conflicted about the role of technology in helping students
learn (see the classic debate between Richard Clark and Robert Kozma in the "media debate" of the early 1990s--Clark,
1994; Kozma, 1994). While this debate effectively moved the field away from considering digital technologies as the
primary variable affecting student learning, these technologies still play an important role in how we learn about, design
for, and evaluate learners. In the 21st Century, as networked technologies undergird nearly all human activities, it is
nearly impossible to conceive of most instructional situations being devoid of technology entirely.

Indeed, technology may be considered an important layer in most instructional systems, similar to how architectural
buildings comprise various layers from the framing to the electrical to (nowadays) the technological. Gibbons (2014)
articulated this layered approach to instructional design, arguing that just as multiple layers work together to support
the purpose of the building, various design layers must similarly work together within instructional products. As we
attend to different elements within instructional design layers, we should consider the content, purposes, and
instructional strategies as well as how the instruction is represented and controlled through available technology tools.
This enables us to design more effective and purposeful instructional solutions and promote powerful learning
experiences.

In this chapter, we attempt to provide suggestions for making instructional design decisions that utilize available digital
technologies effectively. We will begin by discussing what instructional technologies are, and how we can incorporate
them into our designs. We will review design layers that are particularly relevant when using technology to design
instruction and discuss the importance of analyzing the technology's affordances and matching them to the underlying
pedagogical purposes. We will include a discussion about utilizing different models to focus the technology choices on
student learning. We then conclude with some challenges to be aware of when integrating technologies into our
designs.

What Is an Instructional Technology?

The field of learning and instructional design considers "technology" to be any tool that extends human capability or
assists us in achieving a desired learning outcome. In this definition, the technology or tool does not need to be digital.
Experts in the field of educational technology often adopt the terms "hard" and "soft" technologies. In this dichotomy,
hard technologies refer to machine-based or digital technologies, such as a computer or a web-enabled app, while soft
technologies are human-driven processes, methods, and theories that similarly extend or improve our abilities to teach
or learn. As an example, in the second edition of the Handbook of Educational Communication Technology, (Jonassen,
2004), there was a section for chapters on "hard" technologies, such as television, virtual reality, and internet-based
learning, and a separate section for "soft" technologies such as programmed instruction and game-based learning.

                                                                                       335
Many of the chapters in this textbook are, in fact, discussing "soft" technologies to support designing instruction (see
particularly the sections on instructional design knowledge and processes). However, this dichotomy is becoming less
relevant, as hard and soft technologies are increasingly considered simply "strategies" for influencing learning and
typically involve some combination of process, pedagogy, and digital tools. Our chapter continues to merge these ideas
together by discussing "hard" digital technologies specifically, but with strong consideration for their pedagogical fit.

How Can Instructional Technologies Influence Learning?

As mentioned above, technologies are tools that extend human capability, including learning. In the past, educators and
instructional designers viewed technologies as primarily hard technologies, a medium to learn from. This view was
associated with the teacher-centered instruction or transmission model of education and associated theories. The
focus was typically on content transmission, practice of basic skills through repetition, reinforcement of desired
behaviors, and evaluation of how accurately the learners could respond to pre-programmed questions. The technology
may have allowed for some interaction with peers and instructors, but mostly the learner individually interacted with the
content in isolation. The learner's role was to acquire provided information and reproduce it for evaluation. The
instructor's primary roles were to manage the content and evaluate learners' work. This perspective is still valuable for
some tasks and types of instruction. However, alone, these types of activities have only limited power in actively
engaging learners in the meaning-making process necessary for successful learning and transferring knowledge to new
situations.

As an alternative, Jonassen (1996) envisioned instructional technologies as mindtools that students learn with, not
from, requiring attention to the underlying strategies for using the technology, i.e., soft technologies, in addition to the
medium, i.e., hard technologies. This perspective acknowledges that technologies do not directly mediate
learning. Learning is mediated by thinking, collaboration, and dialogue facilitated by a variety of tools. Technologies as
mindtools support learners as they interpret and organize their knowledge, engage in critical thinking about the content,
and actively participate in knowledge construction. Examples of such tools are semantic and conceptual maps (Hwang
et al. 2011, visualization tools (Huang, 2020), microworlds and simulations (Warren & Wakefield, 2013), and even
emerging technologies such as robotics (Mikropoulos & Bellou, 2013).

Building on this idea of mindtools, and reflecting a general trend in education toward a learner-centered paradigm, the
instructional technology field began using technology to mediate meaningful learning experiences and to focus on
supporting the learner and the process of learning. Terms such as learning design and technology-mediated instruction
reflect this shift in thinking. As Ertmer and Ottenbreit-Leftwich (2013) explained, "technology integration is no longer an
isolated goal to be achieved separately from pedagogical goals, but simply the means by which students engage in
relevant and meaningful interdisciplinary work" (p. 176).

Learning experiences are now designed with greater emphasis on our understanding of how people learn (Bransford et
al., 2000). Learners are viewed as active agents who bring their own knowledge, past experiences, and ideas into the
learning process, which impacts how they learn new information. As learners engage in the learning process, they
construct and negotiate new meaning individually and with others. The goal of learning is to gain new understanding,
broaden perspective, and apply knowledge in practice rather than to reproduce a specific set of facts. The instructor
facilitates the interactions among peers to promote deeper understanding and acts as a guide and a mentor rather than
"a sage on the stage."

In this approach, technologies are used more intentionally as tools that mediate learning in a variety of ways. In this
chapter, we will briefly discuss three powerful ways that technology can improve learning through (1) simulating
authentic human activity, (2) enhancing interaction among people, and (3) enriching the learning process.

Technologies Can Simulate Authentic Human Activity

Learning, and especially learning of complex professional skills, is optimal when it is contextualized and situated in real-
life experiences and authentic activities. Certain approaches use varied technology tools to mimic real-world situations

                                                                                       336
to support learning. For example, computer simulations and problem-based learning (PBL) use technology to create
conditions that are similar to real life and encourage the learner to gain new knowledge and skills through repeated
practice and solving authentic problems. Inquiry-based learning (IBL) encourages the learner to actively explore the
material, ask questions, and discuss possible solutions modeling the real-life process of examining issues and
systematically looking for answers. Another similar approach, project-based learning (PjBL) engages learners in
authentic and complex projects, often developing a tangible product, enabling learners to actively explore real-world
problems and gain deeper knowledge and skills. In all these methods, technologies can be used to create authentic or
near-authentic problem-solving scenarios and simulations. Additionally, easier replication of digital problem scenarios
enables multiple practice opportunities, and using the actual technological tools of the discipline supports learners as
they develop professional skills to practice problem solving while in school.

Technologies Can Enhance Interactions

Digital technology has a tremendous potential to enable interactions and connections between people. Whereas
individuals were previously limited by space and time constraints, they can now interact through near ubiquitous access
and connection to each other. This has led to the development of several theories of digitally mediated social
interaction, such as the Communities of Inquiry framework. This theory describes learning as happening within a
community where technology enables different types of human presence:

      social presence (the feeling of being connected and present with each other, for example through video or text
      discussions designed for students and instructors to learn about each other),
      cognitive presence (the feeling of being intellectually present in the community, growing and developing meaning
      through interaction, for example through online question and answer sessions or group collaboration via shared
      documents), and
      teaching presence (the feeling of being supported by a teacher designing and facilitating the interactions and
      content, for example through well-designed online curriculum and opportunities for feedback).

Collaborativism (Online Collaborative Learning Theory) is another model of online learning that creates opportunities for
meaningful learning experiences through technology (Harasim, 2017). In this process-oriented model, collaborative
technology enables students to actively work together, create knowledge, and learn to use the language, analytical
concepts, and activities of the discipline while being supported by an experienced educator who helps them move
through three stages. In stage 1 (Divergent Thinking), students engage in discussions about a specific problem or a
topic. They generate ideas, questions, responses, and solutions based on their personal perspectives and experiences
and share them in a group setting. During stage 2 (Idea Organizing), conceptual changes and convergence of different
ideas begin as students clarify, organize, and narrow down options through reflection, analysis, and negotiation of ideas
that were shared previously. During Stage 3 (Intellectual Convergence), the group is actively engaged in the co-
construction of knowledge. Everyone contributes as the group works on a joint knowledge product or solution, which
may later extend to an authentic application or be further refined through another collaborative learning cycle.

Technologies Can Enrich the Learning Process

Technologies have a powerful potential to enrich and transform the learning process in ways that may be difficult or
impossible without these tools. For example, online and collaborative technologies offer unique affordances that go
beyond connecting learners across time and space by enabling easy access to multiple perspectives from diverse
populations and across the globe. The asynchronous and recorded character of technology-mediated exchanges
enables coherent organization of thoughts, clear and authentic expression, and deep analysis and reflection, which in
turn facilitates deeper learning and enhances theory-to-practice connection. The opportunity to create multidimensional
and multidisciplinary responses presents authentic evidence of a deeper understanding that goes beyond "correct"
answers. Technology also enhances participation opportunities for all types of learners, not just for the traditional
mainstream student. Those that may be timid, need more time, or are learning the language are automatically provided
with additional support to access the material and interactions in ways that meets their needs. Furthermore, through its

                                                                                       337
flexibility, technology provides access to learning for many non-traditional students as well as busy professionals who
may not be able to gain credentials or participate in ongoing professional development in more traditional ways.

How Should We Incorporate Technology In Our Designs?

Entire handbooks have been written about the topic of how to effectively design learning through the support of
technologies (see, for example, Bishop et al., 2020; Dillon, 2020; Mayer, 2014; and Stanley, 2013). This chapter cannot
expound on all of these theories and ideas, and truthfully, the path of an instructional/learning designer is one of
continuous learning--particularly in the area of instructional technologies because these technologies are continually
evolving. However, we present two key ideas that will guide you in making wise technology choices in your design work,
namely: (1) align technology with pedagogy and (2) focus on what students will do with the technology.

Principle 1: Align Technology with Pedagogy

The quality and accessibility of technology-mediated learning experiences is an issue of both technology and pedagogy.
Whether we design a single learning experience, a course, or a full program, strategic orchestration of desired results,
assessments, and instructional methods with intentional use of technology are essential. Understanding by Design
(UbD) or Backward Design (McTighe & Wiggins, 2005; see also Dodd, 2020, in this book) is a useful framework that
helps designers align these essential elements, focus on student learning, and attend to the underlying pedagogy.
Rather than the content, materials, or tools dictating what the student should learn, designers pinpoint the most
important ideas, knowledge, and skills that the students should learn, and identify appropriate assessments and
pedagogies for supporting student learning..

   Pedagogy

     Pedagogy refers to principles and practices guiding instructional action with a goal to support learning.

Recognizing that technology is a strategic tool encourages designers to deliberately align technology with underlying
pedagogical strategies. Any design can be visualized as having two main layers: a physical layer and a pedagogical
layer (see Figure 1). Each layer has distinct core attributes that make the design functional. Core attributes within the
physical layer exemplify the surface features of presentation and delivery of instruction and influence access and cost.
The pedagogical layer core attributes represent the underlying pedagogical structures and strategies, enable learning to
take place, and contribute to successful achievement of learning outcomes (Graham et al., 2014). To increase the
effectiveness of any instructional design, the layers and its core attributes should be aligned during the design and
development process.
Figure 1
A Visual Representation of Two Design Layers From Graham, et al. (2014).

                                                                                       338
Allman and Leary (2020) studied the process and identified a set of core attributes within the pedagogical layer that
drive the two design layers' alignment. This set of attributes, so-called pedagogical intent, pivot around the learning
event and encompass core components, core methods, and core strategies (see Figure 2). As designers establish
pedagogical intent related to a specific learning event, it is easier to recognize technological affordances that may be
needed and match them with available technological tools. The alignment is achieved iteratively through purposefully
utilizing available technology tools to fulfill the underlying pedagogical intent requirements.
Figure 2
Pedagogical Intent--A Set of Core Attributes Within the Pedagogical Design Layer.

                                                                                       339
   Affordances

     The concept of affordances represents what a specific technological tool can do, as well as, "afford" the user, a
     designer, a teacher, or a learner, to do. Affordances are determined by the properties of the tool but also by the
     capabilities of the user.

Although the choices of technological resources are important, it is the pedagogical purposes that should drive the
form of instructional design solutions. By allowing the function to guide the form through prioritizing pedagogical
purposes and aligning pedagogical and physical design layers, we can design more effective technology-mediated
learning experiences and use current technologies in innovative ways.

Principle 2: Identify What Students Will Do With the Technology

In the discipline of instructional/educational technology, researchers have developed many different models for
describing how teachers can integrate technology into their teaching. Most of these models focus on how teachers
utilize technology. See, for example, the following:

                                                                                       340
      The TPACK model, which focuses on teacher technological pedagogical content knowledge (Koehler and Mishra,
      2009);
      The SAMR model, which focuses on how teachers can use technological strategies to substitute, augment, modify,
      or redefine their current pedagogical practices (Hamilton et al., 2016);
      The RAT model, which similarly categorizes technology decisions according to whether the technology replaces,
      amplifies, or transforms the teachers' existing teaching practices;
      The LoTI model, which depicted seven levels of technology use by teachers in the classroom (Moersch, 1995).
While these models can be helpful in teacher preparation programs, they perpetuate a teacher-centric approach to
technology use, often ignoring the learner's experience.
PICRAT. However, a new model has been proposed that builds off of the common SAMR/RAT approaches, but turns the
emphasis away from what the teacher does with the technology and toward how the student utilizes the technology
(Kimmons et al., 2020). In this model, called PICRAT, designers still consider how to use technology to replace, amplify,
and transform the learning; but in addition, designers consider what the student is doing as part of the activity: is the
student's learning passive, active, or creative? The PICRAT model does not dictate that all good instruction must be
transformative or that students must be creative while using the technology. However, it does help teachers and
designers to diagnose how often they incorporate activities in each of the squares, and whether they are overusing
some strategies to the detriment of others. For example, we often find that designers/teachers overuse technology to
replace passive forms of learning (e.g. viewing a lecture, reading a textbook) and PICRAT can stimulate thinking about
how to engage students more actively and creatively in their learning with technology.
Figure 3
PICRAT Model

                                                                                       341
   PICRAT for Effective Technology Integration in Teaching

                                                                   Watch on YouTube
                                                                    Link to transcript
     This video was developed for preservice teachers, and discusses the basic ideas of the PICRAT model.

Challenges When Designing Learning With Technology

In this chapter, we have mostly proposed technology as a powerful asset for designers as they create effective learning,
as long as they first, focus on aligning the technology's affordances with matching pedagogies; and second, focus on
the students' experiences with the technology. By maintaining these two foci, technology can have a powerful influence
on student learning. However, research has provided several additional cautions. We highlight a few important ones
here, but be aware that there are many more, and technology, as would be the case with any tool or strategy, should be
applied judiciously after careful learner/needs analysis.

Challenge #1: Technology Can Be Distracting

While technology can enhance learning, it can also easily distract from it. We are all familiar with overworked
Powerpoint slides or videos where the core message is lost amid spinning graphics, useless animations, distracting
photos, or disconnected audio. Richard Mayer, and his collaborators, have outlined key principles for designing effective
educational multimedia in their Cognitive Theory of Multimedia Learning, or CTML (Mayer, 1995). These principles are
based on core cognitivist assumptions and theories such as dual coding theory (Paivio, 1990) and information
processing limits and activity (West et al., 2013). The core idea behind the theory is that of congruence--or that various
media should work together, not at disarray, to solidify interpretation of an idea and the development of appropriate
mental schemas. More specifically, Mayer and Moreno (1998) identified 5 key principles for designers:

                                                                                       342
  1. Multiple Representation Principle: It is better to present an explanation in words and pictures than solely in words.
  2. Contiguity Principle: When giving a multimedia explanation, present corresponding words and pictures

      contiguously rather than separately.
  3. Split-Attention Principle: When giving a multimedia explanation, present words as auditory narration rather than as

      visual on-screen text.
  4. Individual Differences Principle: The foregoing principles are more important for low-knowledge than high-

      knowledge learners, and for high-spatial rather than low-spatial learners.
  5. Coherence Principle: When giving a multimedia explanation, use few rather than many extraneous words and

      pictures.

The research on CTML is quite extensive with a great deal of applicability to designers, and you are encouraged to
continue your learning in this area by seeking out recent publications on this topic.

Challenge #2: Equity

Although technology has the potential to contribute to equity among learners, it is frequently a great source of inequality
with regards to access and usage. Technology is typically adopted faster and in more engaging and innovative ways in
schools serving affluent communities. Students in low-income schools may have comparable access to computers
while at school but their access to computers and reliable internet may be limited at home. Additionally, low-income
schools frequently employ technology for routine drills, content delivery, and in teacher-centered ways rather than
facilitating access to knowledge and learning further enlarging the digital divide (Reich, 2019; Warschauer et al., 2004).

Effective use of technology can remove barriers to learning. It can make content and materials more accessible, less
culturally biased, and less linguistically challenging. Technology can support educators to regularly assess their
learners' needs, promptly respond to their progress, and provide tailored support based on those needs. In order for
technology to promote a more equitable learning environment, access to computers, tablets or devices and reliable fast
internet connection must be ensured both at school and at home. Next, attention needs to be paid to ongoing
professional development and instructional coaching to support teachers, particularly to understand how they can
influence student equity.

However, change in teacher practice and effective technology integration occurs gradually. In order to create more
equitable learning environments and innovative uses of technology in their classrooms, teachers need to see multiple
examples and have opportunities to practice in their classrooms. Finally, to promote equity, it is imperative that we see
beyond technology integration and recognize the importance of using technology-generated data to better understand
where learners are and monitor their progress as well as utilize learner-centered educational approaches to promote
authentic and meaningful learning experiences mediated by technology.

Challenge #3: Media Centrism

The field of instructional design evolved in part from a foundation in educational media. Perhaps for this reason, there is
sometimes a bias towards overemphasizing technology in our designs. Throughout the history of our field, we see
initial, frenzied excitement over a new technology that eventually is born out to be not nearly as disruptive as originally
envisioned (e.g. virtual reality, moocs, interactive whiteboards, clickers, etc.).

Gibbons (2018) outlined succinctly a common pattern for new instructional designers, arguing they begin media centric,
because "The technology itself holds great attraction for new designers. They often construct their designs in the
vocabulary of the medium rather than seeing the medium as a . . . preferably invisible channel for learning interaction"
(para. 3). According to Gibbons, designers then evolve to focus on the instructional message, then the instructional
strategy, before finally learning to design according to an instructional model. "Model centering encourages the
designer to think first in terms of the system and model constructs that lie at the base of subject-matter knowledge. . . .
Then to this base of design is added strategy, message, and media constructs" (para 6).

                                                                                       343
Because of this inherent bias towards technology as the first solution, designers must practice discipline in not
choosing the novel technological choice first before fully analyzing its true affordances.

Challenge #4: Time/Cost/Efficiency Tradeoffs

Technology is often expensive to integrate into a learning environment--particularly if it is a new technology and
especially if access must be provided for a large number of students to maintain equity. For example, the ability to
teach mathematics to young children using virtual manipulatives using proprietary software on expensive tablets may
be superior for some learning objectives to plastic, physical manipulatives. However, would the cost of buying and
replacing the tablets be worth it? In addition, how much time will it take to train teachers and students on the new
software? How much instructional time will it take in the class period to conduct the activity, including charging the
devices, organizing them on the media cart, and retrieving them from students afterwards?

In making decisions about integrating technology into learning environments, designers must not only analyze what
decisions will help people learn best, but also which decisions are most practical.

Conclusion

It is clear that technology plays a very important role in our discipline, as many academic programs include the word in
the title of their department. However, what technology designers use in the learning environments they create is less
important than how they use it. In this chapter, two key principles have been outlined for designing effective instruction
with technology: First, match the pedagogy to the technology's affordances; and second, focus on what students will do
with the technology, more so than the teacher. Four challenges have also been outlined that are common when
technoloy is used in design, and some suggestions have been provided for confronting these challenges. Perhaps the
most important idea is to remember digital technologies, like theories, processes, and models, are tools--and tools are
only as effective as the builder and the blueprints that will utilize the tools.

   Application Exercise

     Consider a time in your life when you needed to learn something difficult. Some examples might be fractions as
     a child, learning another language, or learning a new routine at work. First, analyze what your needs were as a
     learner: what did you need to learn, and what made it challenging? Second, describe what kind of technology
     could have helped you? What affordances of the technology would have made it useful? Third, pick one of the
     challenges outlined in this chapter and discuss how an instructional designer could have utilized the technology
     effectively while minimizing those challenges. For example, how could they have reasonably provided equitable
     access? Or utilized CTML design principles?

References

Allman, B. & Leary, H. (2020). Aligning Pedagogy with Technology in Online Course Design [Manuscript submitted for
publication]. Instructional Psychology & Technology, Brigham Young University.

Bishop, M. J., Boling, E., Elen, J., & Svihla, V. (2020). Handbook of Research in Educational Communications and
         Technology (5th Ed.). Springer.

Bower, M. (2008). Affordance analysis - matching learning tasks with learning technologies. Educational Media
        International, 45(1), 3-15. https://doi.org/10.1080/09523980701847115

                                                                                       344
Bransford, J. D., Brown, A. L., & Cocking, R. R. (2000). How people learn (Vol. 11). Washington, DC: National Academy
         Press.

Clark, R. E. (1994). Media will never influence learning. Educational Technology Research and Development, 42(2), 21-
         29.

Dillon, R. (2020). The digital gaming handbook. Boca Raton, FL: CRC Press.

Ertmer, P. A., & Ottenbreit-Leftwich, A. (2013). Removing obstacles to the pedagogical changes required by Jonassen's
        vision of authentic technology-enabled learning. Computers & Education, 64, 175-182.

Gibbons, A. S. (2014). An architectural approach to instructional design. Routledge.

Gibbons, A. (2018). What and how do designers design? A theory of design structure and layers. In R. E. West (Ed.),
        Foundations of Learning and Instructional Design Technology. EdTech Books. Retrieved from
         https://edtechbooks.org/lidtfoundations/what_and_how_do_designers_design

Graham, C. R., Henrie, C. R., & Gibbons, A. S. (2014). Developing models and theory for blended learning research. In A.
        G. Picciano, C. D. Dziuban, & C. R. Graham (Eds.), Blended learning: Research perspectives (Volume 2), 2, 13-33.
         Routledge. https://doi.org/10.4324/9781315880310

Hamilton, E. R., Rosenberg, J. M., & Akcaoglu, M. (2016). The substitution augmentation modification redefinition
        (SAMR) model: A critical review and suggestions for its use. TechTrends, 60(5), 433-441.

Harasim, L. (2017). Learning theory and online technologies (2nd ed.). New York: Routledge.
         https://doi.org/10.4324/9781315716831

Huang, Y. M. (2020). What drives students to continue using social mindtools? The perspectives of social support and
        social influence. Computers in Human Behavior, 106447.

Hwang, G. J., Shi, Y. R., & Chu, H. C. (2011). A concept map approach to developing collaborative Mindtools for context-
        aware ubiquitous learning. British Journal of Educational Technology, 42(5), 778-789.

Jonassen, D. H. (1996). Computers in the classroom: Mindtools for critical thinking. Prentice-Hall, Inc..

Jonassen, D. H. (Ed.). (2004). Handbook of research on Educational Communications and Technology (2nd edition).
         Mawah, New Jersey: Lawrence Erlbaum Associates.

Kimmons, R. (2017). K-12 technology frameworks. Adapted from R. Kimmons (2016). K-12 technology integration.
        PressBooks. In R. West (Ed.), Foundations of Learning and Instructional Design Technology. Retrieved from
         https://edtechbooks.org/-dk

Kimmons, R., Graham, C. R., & West, R. E. (2020). The PICRAT model for technology integration in teacher preparation.
        Contemporary Issues in Technology and Teacher Education, 20(1), 176-198.

Koehler, M., & Mishra, P. (2009). What is technological pedagogical content knowledge (TPACK)?. Contemporary Issues
        in Technology and Teacher Education, 9(1), 60-70.

Kozma, R. B. (1994). Will media influence learning? Reframing the debate. Educational Technology Research and
        Development, 42(2), 7-19.

Mikropoulos, T. A., & Bellou, I. (2013). Educational robotics as mindtools. Themes in Science and Technology Education,
        6(1), 5-14.

Moersch, C. (1995). Levels of technology implementation (LoTi): A framework for measuring classroom technology
        use. Learning and Leading with Technology, 23, 40-42.

                                                                                       345
Mayer, R. E. (2014). The Cambridge Handbook of Multimedia Learning (2nd ed.). New York,
         NY: Cambridge University Press.

Mayer, R. E. (1995). Cognitive theory of multimedia learning. The Cambridge Handbook of Multimedia Learning, 41, 31-
         48.

Mayer, R. E., & Moreno, R. (1998). A cognitive theory of multimedia learning: Implications for design principles. Journal
        of Educational Psychology, 91(2), 358-368.

Paivio, A. (1990). Mental representations: A dual coding approach. New York, NY: Oxford University Press.
Reich, J. (2019). Teaching our way to digital equity. Educational Leadership, 76(5), 30-35.
Shulman, L. (1987). Knowledge and teaching: Foundations of the new reform. Harvard Educational Review, 57(1), 1-23.
Stanley, G. (2013). Language learning with technology: Ideas for integrating technology in the classroom. Cambridge,

         UK: Cambridge University Press.
Warren, S. J., & Wakefield, J. S. (2013). Simulations, games, and virtual worlds as mindtools. In Learning, Problem

        Solving, and Mindtools (pp. 78-99). Routledge.
Warschauer, M., Knobel, M. & Stone, L. (2004). Technology and equity in schooling: Deconstructing the digital divide.

        Educational Policy, 18(4), 562-588.
West, R. E., Hannafin, M. J., Hill, J. R., & Song, L. (2013). Cognitive perspectives on online learning environments.

        Handbook of Distance Education, 3, 125-141.
Wiggins, G., Wiggins, G. P., & McTighe, J. (2005). Understanding by design. ASCD.

                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/designing_technology.

                                                                                       346
  28

Designing Instructional Text

Shanali C. Govender & Tasneem Jaffer

The term "instructional text" has been widely used to describe a wide range of textual objects, from whole texts (such
as textbooks, manuals, guides and even narrative, reading scheme texts) to parts of a text. Historically, much of the
research on instructional text has been in relation to the linguistic construction of texts (Tarasov et al., 2015) and
document design (Misanchuk, 1992). In this chapter, we are going to narrow the focus substantially. This is not a
chapter on how to write a didactic text, in other words, how to convey subject-specific information in print texts or
online. Instead, this chapter will focus on the often overlooked "instructional text" that supports learning. Drawing on
concepts from fields such as applied linguistics, graphic and multimedia design, and diversity studies, we're going to
focus our attention on the kind of textual features that work alongside subject-specific content to direct learners'
attention and action--instructional text.

What Does Instructional Text Do?

Instructional text is commonly found hiding inside other texts: in classroom conversation, in textbooks and learner
guides, in educator-produced material such as worksheets or assignment briefs, and on websites and course sites. We
often do not really notice instructional text unless it is badly written and disrupts the flow of the learning. Instructional
text can play a number of roles in face-to-face, print, and online learning contexts, including:

      contributing to creating a learning environment,
      outlining the structure of a learning experience,
      directing learners' attention to specific areas,
      directing learners' actions and behaviours, and
      creating links between different parts of a learning experience.

Issitt (2004) noted that when we are constructing instructional text for print media, we should pay careful attention to
the placement, construction, and design of these texts, and this is equally true for online and multimodal media,
especially in asynchronous formats.

Instructional Text: In the Classroom, in Print, and Online

While most educators pay careful attention to instructional text that conveys subject-specific content, less attention is
given to instructional text that supports the learning of content.

In face-to-face spaces, instructional text often happens in direct response to learners' actions. Moving into an online or
multimodal context poses some new challenges and affordances for constructing instructional text. Poorly constructed
instructional text in online and multimedia forms can be very confusing for learners but well-constructed instructional
text can also open the door to a wonderful range of visual and verbal opportunities. So if you are an instructional
designer, you need to think intentionally about the different spaces where instruction is needed.

                                                                                       347
Towards a Theoretical Perspective

The Community of Inquiry (CoI) framework (Figure 1), developed almost 20 years ago, drew together the useful
categories of Social Presence, Cognitive Presence, and Teaching Presence to describe the "dynamics of an online
educational experience" in light of asynchronous, text-based group discussions (Garrison et al., 2010, p. 6).
Figure 1
Community of Inquiry (CoI) Framework

This model remains useful today, and can be used to think about instructional text in print media, online text, and
multimodal media. While most educators' focus tends to remain resolutely on cognitive presence, attention to social
presence and teaching presence when constructing successful instructional text can enhance learner satisfaction and
strengthen a sense of community. Social presence in an online learning experience can help participants to "identify
with the community (e.g., course of study), communicate purposefully in a trusting environment, and develop
interpersonal relationships by way of projecting their individual personalities" (Garrison, 2009, p. 352). The specific
textual choices that an educator makes in constructing instructional text will either support the achievement of this aim
or hamper it.

Principles for Constructing Instructional Text

                                                                                       348
   Additional Information

           Slides for Principles for constructing instructional text
           Video for Principles for constructing instructional text

Whether you are constructing instructional text for written, online, multimodal contexts, there are a number of key
principles that you will need to bear in mind. These principles come from a wide variety of research fields, including
perception studies in psychology, user experience research, multimodal studies, and applied linguistics.

Principle 1: Simplify: Reduce Extraneous Load

Simplifying texts and producing ``easy-to-read texts'', or ``plain language texts'' is another way of thinking about reducing
extraneous load to produce texts that align with readers' ability levels (Arfe et al., 2017). Extraneous load or processing
refers to any work that a learner might need to do that does not contribute to the learning goal or outcome (Mayer,
2019). Poorly written, excessively complicated instructional text, or instructions that are only issued verbally can
contribute to extraneous cognitive load. Instructional text should

      be short and direct
      avoid jargon
      highlight key actions
      be easily "findable"
      hyperlink where appropriate

Principle 2: Personalise: Connect Through Voice and Tone

Research suggests that belonging, achieved in part through affective connection, is a key predictor of learner success
(Masika & Jones, 2016; Trujillo & Tanner, 2017). In face-to-face classroom contexts, successful educators instinctively
use body language and tone of voice to create connection through conveying emotion. This is particularly true for
minority learners (Meeuwisse et al., 2010; Rahman, 2013) and in online learning spaces (Delahunty et al., 2014).
Instructional text, in print, online text, or multimodal texts, often suffers from an absence of emotional connection
between the writer and the reader (Issitt, 2004), failing to build belonging in support of learning. Instructional text is,
however, an opportunity to connect with learners on levels beyond the purely cognitive, enhancing learning through
building connection, belonging and trust (see Figure 2). The use of a human (as opposed to synthesized voice) and to
"you" and "I" as opposed to third person pronouns or more formal language are ways of building connections (Ginns et
al., 2013).
Figure 2
Exemplar Email Showing Tone and Personalising Practices for Instructional Text

                                                                                       349
The text, highlighted in yellow above, points to some of the language choices made in an instructional text to build
connection and convey emotion. Not everyone will do this in the same ways based on factors such as the personality of
the instructor, the age of your learners, and the context in which you work; but for this educator, this is an authentic
example of "voice" that seeks to promote connection. In certain contexts, the use of emoticons, gifs, and memes might
be an authentic and approachable way to build connection.
The pervasiveness of recording devices in the form of cellphones, cameras, lecture recording, and live video
conferencing contribute to educators across the educational sector feeling increasingly under surveillance. In the
context from which we are writing, a research-intensive university in South Africa, staff are hyper-aware and often
uncomfortable with being recorded. This can result in both teaching and learning interactions and instructional text that
is hyper-correct, hyper-formal and devoid of personality and the opportunity for connection.

Consider Multimodal Instructional Text

Instructional texts in multimodal contexts can take advantage of the affordances of video and audio to improve clarity
and strengthen connection. In blended contexts, making the instructional text available in both online and face-to-face
modes can support learning in a seamless manner.
Figure 3
Example Showing the Complexity of Conveying Tone in Different Modalities

                                                                                       350
Note. Image adapted from macrovector_official - freepik.com
In written text on the page or screen, Option 1 seems much friendlier than Option 2. But what if we were in a live video
conference call, and Option 1 was said in a monotone, and Option 2 was said with a cheery smile and a "zip the lips and
throw away the key" gesture? The tone changes entirely. The same would be true of an audio recording, video or
podcast--formal language, paired with welcoming tone and visuals can have a very different impact than just the text,
and of course, text, audio and visuals that work in support of each other would be the most powerful.

Principle 3: Communicate Regularly

The temptation with instructional text is sometimes to write or present long and complex screeds of instructions at the
outset of a process. We would encourage you, however, to communicate regularly with learners and to offer
instructional text in brief, just-in-time chunks. This is for a number of reasons. Instructional text is most likely to make
sense to learners when it is directly related to what a learner needs to do immediately. While it is useful to tell learners
at the beginning of the semester how it is that they will submit their final semester project, you really should be prepared
to come back in the final weeks of semester and remind them. Instructional text is also a way of building a connection
with learners across time. Knowing that a lecturer will email weekly with key submissions, key activities, and a summary
of what went well in the previous week is something that learners, particularly in online contexts, value. Instructional
communication over time is also an opportunity to acknowledge and encourage learners on an ongoing basis.

Purpose and Placement of Instructional Text

There are many places where you should consider inserting instructional design text. Given the broad role instructional
text plays, it is ubiquitous in course design and delivery. There are, however, some typical placements (see Figure 4) of
instructional text.
Figure 4
Example Structure of an Online Course With Typical Instructional Text Placements

                                                                                       351
Introductory, Linking, and Concluding Instructional Texts

Instructional text is often needed at the beginning of a unit or module in a course or textbook. In this location,
instructional text will usually outline the shape of the text learning experience ahead of the learner by articulating
learning outcomes and identifying key aspects of the learning activities. It might alert the learner to activities such as
live sessions, expected time on tasks, or equipment required for that week. Figure 5 is an example of introductory
instructional text.
Figure 5
Example Showing Introductory Instructional Text

While the text above performs an introductory function, similar texts can perform linking functions between sections of
text or activities, and to conclude learning "chunks."

                                                                                       352
Instructional Text to Support Navigation

Both print learning texts and online learning spaces have their own rules and norms. As an educator, you may be very
aware that bold text or italicized text requires particular attention, or that text in a particular shade of blue with an
underline is likely to be hyperlinked. However, the norms of texts and online spaces are not always immediately
apparent to all learners, who may need some assistance navigating either the text or the space. Further, learners are
almost always enrolled in multiple courses, each with its own structure, rules and norms. Figure 6 provides an example
of instructional text with a navigation purpose.
Figure 6
Example Showing Navigation Techniques in a Learning Activity

Being explicit about the structure, rules, and norms of your course can improve the learner experience by reducing
extraneous cognitive load. That is, because of the way the content is presented, learners may have to do excessive
analyzing that is not necessary to achieve the learning outcomes of the course (Mayer, 2019). It is our responsibility as
instructors to limit this load, allowing learners to focus their attention and efforts on what we really want them to learn.

Learning Activity Instructional Text

If you research older articles, you will often come across instructional text articles that refer to writing the instructions
of an assignment or manual for print. Each assignment or assessment created for learners contains instruction--
sometimes the instruction is minimal if it is an open-ended question or more intensive if it is a step-by-tutorial.
Depending on the level of the learners, the level of instruction used for assessments may differ. Figure 7 is the
instruction for a small activity on learner personas.
Figure 7
Example Showing Instructional Text in a Learning Activity

                                                                                       353
Email as Instructional Text

In an online setting, course announcements or emails are often used by the instructor to communicate with learners on
a regular basis (see Figure 8). This type of instruction typically includes, but is not limited to reminders, deadlines,
changes in course information, and feedback on assessment. While other forms of instruction may be pre-planned,
emails are done with a short turnaround time and therefore contextual to what is currently happening in the class. If we,
as instructors, are communicating sensitive or not-so-great information about learner grades, we want to pay particular
attention to our tone. It is the little things that contribute to a successful email--like the usage of greetings.

Figure 8

Example of an Instructional Text in an Email

 Example:

 Hello everyone

 We don't have any new work/input at the moment, so I hope you are all catching your breath and writing happily!

 In this week's live session, we focused on the first two sections of the Learning Design Rationale. We talked about
 trigger problems in relation to the introduction and started to think about the "Thinking like a designer" section. We
 tried two short activities to get everyone to think about the goal of both these two sections.

 Seema had a great question about how to use the feedback in the blogs. Do not use this feedback to rewrite your
 blogs--it's totally unnecessary. The blogs are, however, actually key parts of your LDR, so use this feedback to
 improve your final LDR.

 Next week, we are using our two live sessions to continue strengthening the LDRs

       Monday (4:30pm) - Context section (Saadiq)
       Wednesday (3pm) - Design and Develop sections (Sandiso, Widad)

 Thanks to our volunteers for next week--we will have some examples to think about our work in relation to!

 Also, check out the Commons tool in the left-hand menu! It's a great place for leaving messages or sharing questions
 and resources.

                                                                                       354
 Stay strong!

 Shanali

Visual Design of Instructional Text

While the content of the instructional text is central to its purpose of guiding the learner, there are many other factors
that contribute to its success. An example is focusing on accessibility in the instructional text's visual design. When
designing instructional text, we need to think about how it caters to the learner's experience from a visual perspective,
which affects the learner's ability to find information (Lonsdale, 2016). These visual guidelines draw on the work of
Hartley (1981), Universal Design for Learning (UDL), and the field of User Experience (UX) to name a few. Regardless of
a physical or digital medium, these guidelines are important considerations for the design of your instructional text.
Good instructional text will not always be noticeable to the learner, but poor instructional text or the absence of
instructional text will be evident. When designing instructional text there are a few key areas that need to be considered.

Headings

Headings improve the learner's ability to scan and navigate the content. At any point, a learner should know where they
came from, where they are, and where they are going. Headings signal the topic, and in many cases give you an idea of
whether the information under the heading will be relevant. Before lesson planning, it is a good idea to sit down and
plan the structure including the headings and subheadings for one unit or lesson. This allows you to mimic the heading
structure for future lessons, creating a consistent learning experience. For optimum accessibility, here you want to
make use of the heading styles provided by Word, Google Docs, or any text editor.

Numbering

Numbering aids in sequencing and organizing information. Furthermore, it is particularly valuable for referencing
purposes (Hartley, 1981). Where possible, numbering is encouraged, as it provides opportunities to reference by number
in announcements, emails, assignments, and other parts of the course (see Figure 9; e.g., "Please have a look at section
2.2 detailing how to use an empathy map.") If the course content changes regularly, you may want to be cautious about
the upkeep of a numbering system. Numbering also provides learners with an easy reference method when emailing an
instructor with queries.

Figure 9

Example Showing Content Structure Using Headings and Numbering

 1. Introduction to Online Learning Design
 1.1 What is online learning design?
 1.2 Knowing your context

 2. Understanding your learners
 2.1 Personas 2.2
 Empathy maps

Bullets

Break up large chunks of text into smaller numbered or bulleted lists. For example, if there are five key points for a topic,
it might be a good idea to outline this using bulleted lists (see Figure 10).

Figure 10

                                                                                       355
Example Showing a Before and After of Converting a Chunk of Text to a Bulleted List
 Tips for filming your online lectures
 Example: Large chunk of text (Before)
 Script writing is an essential part of preparing to film your lecture for an online course. Here are five key tips you need
 to think about when filming. If writing a verbatim script, use short sentences, short paragraphs and simple syntax. Be
 clear and concise: aim to convey maximum information using minimum words. Check that your script flows, that you
 are explaining the links between paragraphs or sections. Storytelling is a useful device to keep learners engaged--try
 to mimic this in your videos. Don't use abbreviations in the spoken form--e.g., say the United States instead of the US,
 University of Cape Town instead of UCT.

 Example: Bulleted list (After)
 Script writing is an essential part of preparing to film your lecture for an online course. Here are five key tips you need
 to think about when filming:

       Use short sentences, short paragraphs and simple syntax.
       Be clear and concise: aim to convey maximum information using minimum words.
       Check that your script flows, that you are explaining the links between paragraphs or sections.
       Use storytelling in your videos to keep learners engaged.
       Don't use abbreviations in the spoken form--e.g., say the United States instead of the US, University of Cape
       Town instead of UCT.

Spacing

Designers talk about "white space"--which is the space around text and images like in this textbook. Golombisky and
Hagen (2013) phrase it really well "Too much space, and visuals and type get lost or don't talk to each other. Not enough
space and they start to fight with each other"(p. 7). White space is necessary and appropriate when placing text and
images, and it contributes to the readability of the instruction. Having white space around the most important
information allows it to stand out from the rest of the text. Following the methodology of chunking (see Figure 11), you
should break up text into logical chunks which can provide the white space needed to maximize readability (Moran,
2016).
Figure 11
Example Showing How Groupings of Text May Appear to a Learner

                                                                                       356
Fonts

Font choice can impact the readability of the text. Opt for common, highly readable fonts like Arial, Tahoma, Verdana, or
others and avoid decorative fonts that are difficult to read. Some font-families are considered more readable than
others, with division among those who prefer sans-serif or serif fonts. A simple web search for the most accessible
fonts will give a selection of suitable fonts. Limit text to one or a few select fonts--having too many will be distracting
for the reader. Although they may seem boring, they are effective and easy on the eyes. Let's leave the decorative fonts
for the kids (see Figure 12).
Figure 12
Example Showing How Decorative Fonts May Not Be Easy on the Eye

Font Size

Depending on the placement of the instructional text, it's important to consider the font size. If important instructions
are too small and illegible, they might be overlooked by the reader (see Figure 13).
Figure 13
Example Showing Readability of Different Font Sizes

 This is 8px

 This is 10px

 This is 12px

This 14px

This is 18px

Emphasis Using Text Effects

When communicating important information like deadlines, it is common to bold, underline, or italicize certain parts of
the text to make it stand out to the learner. For example, the bolding on this part of the sentence signals that you
should focus on it. Here it is important to be consistent in the style of emphasis you are going to be using throughout
your course materials in order to not confuse learners.

                                                                                       357
Colors and Highlighting

If color is used to signal important text, you want a good contrast between the colors used. Poor color contrast may not
be readable and particularly affect learners who are color-blind. Using an online contrast checker like WebAIM will
determine whether the text is well-contrasted (see Figure 14). As far as possible, color should not be the only way to
show emphasis.
Figure 14
Example of a Contrast Checker Showing Inaccessible Color Usage

Conclusion

In this chapter, we have limited our focus to the kind of instructional text that works between subject-specific material
to support learning. While we hope that the principles suggested to you will continue to be useful over a period of time,
factors such as mode (face to face, blended, or online) and ever-changing technological and digital contexts, mean that
what learners find appealing and supportive will change. We encourage you to pay attention to the role of instructional
text in learning, and to take steps to develop your capacity with constructing instructional text by, above all, listening to
your learners.

                                                                                       358
   Application Exercises

   Exercise 1

     Refer to Figure 7: Example showing instructional text in a learning activity. Which principles for constructing
     instructional text do you see at work in this example? Compare your answers with a peer and see if you missed
     anything.

   Exercise 2:

     Here's a version of the email announcement example we used earlier with some creative use of font and colour.
     Copy this email into a text editor, remove all the formatting and think about font colour, size and spacing to
     improve the read-ability.

     Example:

   Hello everyone. We don't have any new work/ input at the moment so I hope you are all catching your breath and
   writing happily! In this week's live session we focused on the first two sections of the LDR. We talked about
   trigger problems in relation to the introduction and started to think about the "Thinking like a designer section".
   We tried two short activities to get everyone to think about the goal of both these two sections. Seema had a great
   question about how to use the feedback in the blogs. Do not use this feedback to rewrite your blogs - it's totally
   unnecessary. The blogs are however, actually key parts of your LDR, so use this feedback to improve your final
   LDR. Next week, we are using our two live sessions to continue strengthening the LDRs - Monday (4:30pm) -
   Context section (Saadiq) and Wednesday (3pm) - Design and Develop sections (Sandiso, Widad). Thanks to
   our volunteers for next week - we will have some examples to think about our work in relation to! Also, check out
   the Commons tool in the left-hand menu! It's a great place for leaving messages or sharing questions and
   resources.

   Kind regards

   Shanali

   Exercise 3:

     Find an existing example of instructional text. This could be something you have written previously or an
     example from a textbook or online course. Using what you have learned in this chapter about principles for
     constructing and designing instructional text, edit the example you have found. Share your edits with a peer or a
     friend for feedback.

References

Delahunty, J., Verenikina, I., & Jones, P. (2014). Socio-emotional connections: Identity, belonging and learning in online
        interactions. A literature review. Technology, Pedagogy and Education, 23(2), 243-265.

Garrison, D. R. (2009). Communities of inquiry in online learning. In Encyclopedia of Distance Learning, Second Edition
         (pp. 352-355). IGI Global.

                                                                                       359
Garrison, D. R., Anderson, T., & Archer, W. (2010). The first decade of the community of inquiry framework: A
        retrospective. The Internet and Higher Education, 13(1-2), 5-9.

Ginns, P., Martin, A. J., & Marsh, H. W. (2013). Designing instructional text in a conversational style: A meta-analysis.
        Educational Psychology Review, 25(4), 445-472.

Golombisky, K., & Hagen, R. (2013). White space is not your enemy: A beginner's guide to communicating visually
        through graphic, web & multimedia design. Taylor & Francis.

Hartley, J. (1981). Eighty ways of improving instructional text. IEEE Transactions on Professional Communication, 1,
         17-27.

Issitt, J. (2004). Reflections on the study of textbooks. History of Education, 33(6), 683-696.
Lonsdale, M. D. S. (2016). Typographic features of text and their contribution to the legibility of academic reading
materials: An empirical study. Visible Language, 50(1), 79-111.
Masika, R., & Jones, J. (2016). Building student belonging and engagement: Insights into higher education students'

        experiences of participating and learning together. Teaching in Higher Education, 21(2), 138-150.
Mayer, R. E. (2019). Thirty years of research on online learning. Applied Cognitive Psychology, 33(2), 152-159.
Mayer, R. E., & Moreno, R. (2003). Nine ways to reduce cognitive load in multimedia learning. Educational Psychologist,

        38(1), 43-52.
Meeuwisse, M., Severiens, S. E., & Born, M. P. (2010). Learning environment, interaction, sense of belonging and study

        success in ethnically diverse student groups. Research in Higher Education, 51(6), 528-545.
Misanchuk, E. R. (1992). Preparing instructional text: Document design using desktop publishing. Educational

        Technology.
Moran, K. (2016). How Chunking Helps Content Processing. www.nngroup.com/articles/chunking/
Rahman, K. (2013). Belonging and learning to belong in school: The implications of the hidden curriculum for

        indigenous students. Discourse: Studies in the Cultural Politics of Education, 34(5), 660-672.
Tarasov, D. A., Sergeev, A. P., & Filimonov, V. V. (2015). Legibility of textbooks: A literature review. Procedia-Social and

        Behavioral Sciences, 174, 1300-1308.
Trujillo, G., & Tanner, K. D. (2014). Considering the role of affect in learning: Monitoring students' self-efficacy, sense of

        belonging, and science identity. CBE--Life Sciences Education, 13(1), 6-15.

                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/designing_text.

                                                                                       360
361
362
  29

Audio and Video Production for Instructional Design
Professionals

Marshall G. Jones & Lisa Harris

Practicing instructional design professionals use digital audio and video in learning materials for a variety of reasons
including content presentation, feedback, and assessment. Some audio and video materials used for purposes like
feedback and assessment may have a narrow audience and may not require high production value (i.e. the editing
needed to make audio and video materials professional and polished). For those uses, you simply record and share.

Depending on the resources of a project, instructional designers may create these materials themselves or work with
media production professionals. Because modern smartphones have access to basic media production tools, it is
possible for anybody to capture and edit audio and video. This is an acceptable solution for materials with a narrow
audience or materials that will not be shared widely. However, media that is included in professional learning materials
needs to have a polished and professional look and feel.

This chapter will provide an overview of audio and video tools that instructional designers can use to capture, edit, and
share professional learning materials with limited resources. In addition, terms and issues associated with audio and
video production will be introduced. This introductory knowledge will help guide you as you seek to enhance your skills
and will also help you communicate effectively with media production professionals during your career.

Lossless and Lossy Media

Audio and video files are commonly referred to as media files. There will be other media files that you work with as an
instructional design professional, but these are the foundations of most media production projects.

Because so much of the work we do is shared digitally on the Internet, working with any type of media is a compromise
between the highest quality and the smallest file size. Which is why you need to learn how to compress uncompressed
media files. We sometimes refer to uncompressed media as lossless and compressed media as lossy.

A lossless file is uncompressed and is at the highest digital quality. "Lossy" media has lost some of the original quality.
This means that it has been exported to a file size and type that can be used and shared easily. An exported file is a
compromise between file size and quality. Lossless media is media captured in its highest quality. This allows us to edit
in the highest quality available and then export it in the most appropriate format. Audio and video examples are
included below.

Audio

The most common lossless audio file you will work with is a .wav file (Waveform Audio Format. Pronounced wave).
There are others, but .wav files are the most commonly used. (See https://edtechbooks.org/-Coyb for a list). A .wav file

                                                                                       363
is uncompressed and at its highest quality. After the audio is recorded, it is edited for length, clarity, and production
quality and exported to a lossy compressed format. The most common compressed audio file is an .mp3 file. The
sound quality of a .wav file is higher in quality and preferred by professional musicians and audio engineers. However, a
.wav file can be as much as ten times larger than an .mp3 file. Given the file size difference, instructional designers
often use .mp3 files as a compromise between file size and quality.

Video

The quality of the video recorded is determined by the camera used. Video quality is referred to as video resolution and
measured by pixel dimensions. See Table 1 for common types of video resolution from lowest quality to highest quality.

Table 1
Common Video Resolutions

Video Resolution             Abbreviation(s)       Pixel Dimensions

Standard Definition          SD                    720x480

High Definition              HD                    1280x720 (Also written as 720p)

Full High Definition         Full HD               1920x1080 (Also written as 1080p)

Ultra High Definition or 4K  Ultra HD or 4K        3840x2160

Note. See https://edtechbooks.org/-CgI and https://edtechbooks.org/-CgI for more details.

The quality of the camera determines the highest quality of the video available to you. As of the writing of this chapter,
newer smartphones record in HD and Full HD, though some newer phones do allow for 4K recordings. Professional
video cameras, and many consumer level cameras, will record in 4K. Once the video is edited for length, content, and
production values, the final resolution can be determined when the video is exported and saved to a lossy file type to
share, typically an .mp4 file.

How to Compromise Between Quality and File Size

This will depend on several factors such as:

      how you will deliver the materials,
      the content of the materials, and
      where you will be using the materials.

For example, if you were recording music and wanted to distribute it on a compact disc, you would want to export your
audio file in a .wav format. You would want the highest quality so that the nuances of the music could be enjoyed by the
listener. If you are recording a podcast, lecture, or some other type of voice file and your plan is to distribute that
through a learning management system or some other internet-based option, an .mp3 file is preferred. An .mp3 file has
levels of audio quality, or fidelity, which are measured by the bitrate. The lower the bitrate, the lower the quality or fidelity
to the original recording. A bitrate of 96 would be a lower quality .mp3 while a bitrate of 320 would be the highest
quality. The bitrate is chosen when the audio file is exported. For more information on bitrate, see
https://edtechbooks.org/-Zavu.

                                              364
When exporting video, it also depends on what you plan to do with the video. If you are broadcasting the video through a
television network or streaming service, you would likely want to export in 4k or Full HD (1080p). If you are distributing
your video through an online streaming service such as YouTube or Vimeo, you could likely use a lower resolution HD
file. Your choice of resolution will depend on the content of the video. If you were creating a highly technical video that
requires significant granular detail, such as repairing a piece of equipment, or performing surgery, you would want a
higher resolution, likely 4K (Ultra HD) or Full HD (1080p). If you were recording a lecture or a narrated PowerPoint
presentation a lower resolution such as HD (720p) would be fine. It would depend on the content and how you are
distributing it. It is unlikely that you would ever export in Standard Definition (SD). That is the resolution that was
common to many video cassette recorders. We include it here as you may see it if you are including older recorded
materials in a project.

Recording and Editing Digital Audio

Before you can compress your audio as an .mp3 file, you must capture, or record the audio. On the face of it, recording
audio is relatively straightforward, assuming you have a recorder:

  1. Press the record button.
  2. Talk.
  3. Press the stop button.
  4. Export it as an .mp3 file.

Recording high quality audio is more difficult. Where you record is an issue to consider. Field recording, such as
interviews in offices, classrooms, or outside areas are different from recording in a single stationary place, like a studio.
This chapter primarily covers the fundamentals of audio recording in a stationary space with limited resources.

   Additional Resources

     If you are interested in field recording, Michael Helms' podcast and website are helpful resources
     (https://michaelthesoundguy.com).

To record professional sounding audio, you have to think about a number of factors such as what microphone you are
using, how close you are to your microphone, where you are recording, and how to edit your audio before you export it
and share it. We will cover each in turn.

Microphone Quality

For professional sounding audio, you need a better microphone than the one in your computer or your smartphone. You
also need a microphone that records in stereo. Mono (also known as monaural or monophonic) audio plays in a single
channel. Stereo plays in two channels, left and right. Put more simply, mono audio plays in only one ear of your
headphones. Stereo plays in both. Space limits our discussion of every type of microphone, and there are many such as
dynamic and ribbon microphones. For more information on dynamic and ribbon microphones, see:
https://edtechbooks.org/-SHa. If you have access to recording engineers and their studio, they will have professional
microphones available. We are focusing our chapter on a versatile microphone for instructional designers, the
condenser microphone.

Condenser microphones have electronics inside of them to help provide a better-quality recording. They typically
connect directly to your computer through a USB port. They provide for greater fidelity in sound, which means the
microphone can provide a recording that is very close to the original performance. If you listen to live music and a

                                                                                       365
recording of the same performance, you will hear things differently. Your ears will pick up sounds at a live performance
that even the best microphone cannot capture. Better microphones will provide recordings as close as possible to the
original. So higher fidelity means better quality. And better microphones produce higher fidelity.
Condenser microphones capture a wide range of frequencies well. They make the low tones richer and the high tones
sound crisp without sounding "tinny." Condenser microphones can be very expensive. You can pay thousands of dollars
for a condenser microphone, and if you were a recording engineer working with symphonies or studio musicians, you
would. However, for the type of recording most instructional design professionals do--mostly voice recordings for .mp3
files or voice overs for video--you need a good microphone, not a great one. There are many to choose from and an
internet search for comparisons will yield many. What you choose will likely depend on your budget.
The "Snowball" microphone by Blue is an example of a compromise between cost and quality. It is an example of a
good, but not great, microphone. This USB microphone provides good sound quality at a reasonable price and is a good
addition to your instructional designer tool kit. It is available at many retailers or from the website directly. Our inclusion
of the Snowball microphone is used as an example only and should not be considered a commercial endorsement.
Figure 1
Blue Snowball Condenser Microphone

Note. Visit https://edtechbooks.org/-qFv for more information.

Distance from Your Microphone

The closer you are to your microphone, the better the sound quality. How close depends on the microphone, so it is best
to experiment until you get to know your microphone better, but think two-six inches away as being close.

                                                                                       366
Your microphone will record every sound in the room. When you record, think about air conditioners, fans, and other
ambient noises. Also, think about air. The air between you and your microphone is recorded as dead air. If there is a lot
of air between you and your microphone, you will get a hissing noise when you turn up the volume. While the
microphone can and will pick up all of the sounds, the mic will pick up sounds closer to the microphone more clearly.
Which is why you want to be close to the microphone. However, if you get too close to a microphone you may "pop the
mic". Popping the mic is the term used to describe the noise that you hear when hard consonants like p, t, d, etc. are
used close to the microphone. The microphone may pick these sounds up and produce an annoying popping sound on
a recording. To lessen mic popping, use a pop filter. These devices provide a mesh like fabric that rests between you
and the microphone and help diffuse the sound of the hard consonants.
Figure 2
Blue Snowball Mic with Pop Filter

Note. Source: https://edtechbooks.org/-FzAV
Pop filters are relatively inexpensive. Because a person sits close to a pop filter when speaking, they are likely to spread
aerosols as they speak. We recommend you treat them as personal items.

Room Size

The best place to record audio is in a recording studio. Recording studios have baffling, soft material on the walls and
ceilings, to lessen echo in the room. If you do not have access to a recording studio, consider the size of your room.
Large rooms may produce more echo. Small rooms will produce less. The less echo in a recording the better.
The larger the room, the greater the problems for recording. Your microphone will pick up ambient noises like your air
conditioner, but it will also pick up the echoes of the room. Rooms with a lot of tile and glass and tile (think kitchens and

                                                                                       367
bathrooms in your home) allow the sound to "bounce" off the hard surfaces. Audio recording in large rooms produces
audio that may sound thin and produce unwanted echoes.
When possible, use a small room with carpets, drapes, and other fabrics to help absorb excess sound. You can also
create a better environment with boxes to contain the microphone and soft materials to surround it. In the image below,
the person has placed their microphone in a box with foam to absorb excess sound. You could also achieve this effect
with a box and any kind of fabric like towels or small pillows. Notice that the microphone is close to the foam in the box.
This will help keep sound on the other side of the microphone from being recorded as well and helps eliminate dead air.
Figure 3
Creating a Better Recording Environment

Note. See https://edtechbooks.org/-Vod for more information.

Audio Software

To create a professional sounding recording, you need multitrack software to edit audio. A finished audio file is a
number of tracks, or layers of audio, edited together for quality and clarity. Each track is a type of audio. One track might
be your voice, and another track your background music (see Figure 4). Audio software allows you to mix your audio.
Mixing audio means that you adjust levels of each track to make your voice loud enough and your background music
soft enough to blend well together.
Figure 4
Multiple Audio Tracks

                                                                                       368
Note. The top track is a voice track and the bottom track is a background music track.

There are many professional options available, but Audacity is a free, open source tool that we recommend. It is
available for both Mac and PC computers. Below are a set of tutorials that will help you get started with Audacity
(version 2.3.2) and audio editing.

  1. Audacity: An Introduction
     A brief tutorial demonstrating the user interface and basic functions of Audacity audio software.

  2. Audacity: Adding Background Music To Your Project
     How to add background music to your podcast using Audacity. The video covers importing a music file and how to
      use "fade in" and "fade out" to make the final version sound more polished.

  3. Audacity: The Effects Menu
     This short tutorial introduces the Effects menu in Audacity and demonstrates how to apply a change in pitch effect.

  4. Audacity: The Time Shift Tool
     This tutorial introduces the Time Shift Tool in Audacity. Use the Time Shift Tool to move an audio track horizontally
      on your timeline.

  5. Audacity: Exporting Audio as an. Mp3 File
     This brief tutorial demonstrates how to export your Audacity project as an mp3 file so it can be shared with others.
      This is the stage of the production process when you would choose the bitrate for an .mp3 file.

Digital Video

Most people have a better-than-average video camera in their pocket or bag. Modern smartphones contain impressive
cameras and can record quality video. Because everybody can create video easily now, we think of video as being easy.
On the face of it, digital video is easy. Sound and lighting are hard.

                                                                                       369
Video and Expectations

Without the resources of professional videographers and filmmakers, you should not expect to make videos that will
look like the ones you see on television or in the movies. Those videos require resources including multiple cameras,
multiple camera angles, good field recording equipment, professional lighting kits, sophisticated video editing software,
personnel, and more. However, depending on the needs of your learners and the type of content you need to create, it is
possible to make compelling and professional looking videos with limited resources. What follows are some resources
to help you produce video with limited resources. Experience producing video yourself will help you work with
professional video production teams during your career.

Video Terminology

Below is an alphabetical list of terms used in capturing, editing, and sharing videos that you are likely to encounter.
While this is far from a complete list, some knowledge of these terms will help you both produce your own video and
work with professional video producers.

Table 2            The relationship between the width and height of your video. It is expressed
Video Terminology  numerically such as 4:3 and 16:9.
 Aspect Ratio
 Boom Mics         High quality microphones that can attach to your camera to allow better sound while
 B Roll            you are shooting video.

 Clip              Supplemental background footage. This is stock footage that is used while voice over
 Compression       audio plays. For example, if there is a news report about vaping, you will often see
 Credits           video clips of random people not associated with the story vaping. Those clips are B
 Dissolve          Roll footage.
 Export
 Fade              A piece of video. You can take one long video clip and split it to make multiple small
                   clips. You can take multiple small clips and order them to create a final product.
 Filters
 Preview Window    The amount of data in a video file. Smaller files have higher compression rates. High-
                   definition video has lower compression rates.

                   Information at the end of a video detailing information about the creator of the video,
                   actors, or other important information about the video.

                   An editing term. A transition between two elements in your video. You might dissolve
                   between two video clips, or between a clip and a title.

                   "Saving" your video in a format that can be shared with others or uploaded to a video
                   sharing site, typically an MP4 file.

                   An editing term. It is the transition between a visual and a black screen. As in "Fade to
                   Black".

                   An editing term. Color Corrections that can be made to a video clip. Much like filters
                   you might add to an image in image editing software.

                   The area of the screen in a video editor that lets you see the movie in part or in entirety

                   370
Project Library      within the software as you edit the video.
Timeline/Storyboard
Scrub                An area in your editing software that contains all of the video clips, still images, and
Shot List            audio files you have imported to use in your video.
Split
                     In video editing software, the area of the screen that contains clips that you have
Titles               moved from the library for editing and inclusion in your project. The
Trim                 timeline/storyboard is typically found at the bottom of the screen.
Tripod
                     An editing term. To fast forward or rewind a video or audio clip on the
                     Timeline/Storyboard.

                     A list of all of the shots, or video you need to collect, that you will need in your project.

                     An editing term. To cut a single video clip into two separate clips. You would do this to
                     isolate the video you want to remove by splitting it before and after the video you want
                     to remove and then deleting the piece in the middle. You would split a clip to insert
                     something before or after the clip, such as a title, still image, or another video clip.

                     Text that appears on a solid color or an image that helps mark the beginning of the
                     video or provides a section break descriptor for the video.

                     An editing term. To remove content from the beginning or the end of a video clip.
                     Trimming does not allow you to remove video from the middle of a clip. To do that, you
                     would need to split the clip.

                     A three-legged stand used to hold a camera still during filming.

Note. A more complete collection of terms is available from Vimeo.

Video Cameras

Professional video cameras used by professional videographers are more expensive and sophisticated than consumer
level cameras. Nevertheless, quality video can be captured with consumer level cameras. For example, most modern
Digital Single Lens Reflex (DSLR) cameras are capable of shooting Full High Definition video. Many mirrorless cameras
are capable of shooting in 4K or Ultra High Definition video. If you are recording a speaker, you would likely need an
external microphone, such as a boom microphone, for your live audio. However, if you are able to use voice over audio,
the information on audio recording in this chapter would help you create your audio track.

Smartphones are capable of shooting high quality video. As with DSLRs and Mirrorless Cameras, you will need an
external microphone to record live audio. When shooting with a smartphone, always orient the camera in landscape
mode (see Figure 5). Recording video with a smartphone in portrait mode will produce black bars on either side of the
video when you edit (see Figure 6).

Camera technology changes rapidly. While 4K video is the current highest resolution commonly available, 8K video
technology is available as of the writing of this chapter. If you are considering purchasing video equipment, it is best to
research current standards before you purchase. The website, digitaltrends.com, has buying guides that are a good
place to begin your search.

Figure 5

Still Frame of Smart Phone Video Shot in Landscape Mode

                     371
Figure 6
Still Frame of Smart Phone Video Shot in Portrait Mode

Video Editing Tools

Once the video is recorded, you will want to edit the video for quality and clarity. To edit the video you need to import the
video into a video editor. Much like word processors let you edit the length, quality, and appearance of text, video editors
do the same for video. Video editors range from the basic and introductory, such as the free Windows Video App

                                                                                       372
available in Windows 10 and iMovie available on Apple computers, to the professional level tools Adobe Premiere and
Final Cut Pro. As you might imagine, professional video tools are more sophisticated, more complicated to use, provide
better final products, and are expensive. Windows Video App and iMovie are included in your system software and
require no additional purchase.

If you have never used a video editor, Windows Video App, available in Windows 10, and iMovie, available on Apple
computers, are excellent places to start. Tutorials for both are widely available online, but we offer these introductory
tutorials on the Windows Video App. For non Windows users, these videos can also introduce you to the basic interface
of video editing software and some of the tools and features common to all video editing software applications.

Windows Video App Interface
This short video is an introduction to not only the Windows Video App interface, but also to what video editors look like.
You will see examples of the Project Library, the Storyboard/Timeline, and the Preview Window.
Introduction to Editing
This short video provides an introduction on editing video with the Windows Video App. While the tutorial is specific to
the Windows Video App, the terms and techniques are mirrored in most video editors.
Editing and Special Effects
This short video provides more editing options and demonstrates how to use special effects in the Windows Video App.

There are free open source video editing tools available as well. Shotcut and OpenShot are cross platform video editors
with sophisticated features rivaling those of paid professional video editing software.

Screencasts

Screencasts are recordings of the action displayed on a computer or mobile device screen. They are often used as
tutorials to demonstrate how to use a particular function of a piece of software. The cursor is often highlighted by a
halo of color so that it is easy for a viewer to track the cursor's movement on the screen. The person demonstrating the
task on screen typically narrates these videos in real time. Screencasts may or may not include a thumbnail video image
of the speaker overlaid on the screencast. The videos used to demonstrate the audio editing software above are
examples of screencasts. Screencasting video can be uploaded into video editing software and used as a clip in a
longer video or edited for clarity and quality.

Screencasting Software

There are multiple options for screencasting software, including free and paid versions.

Camtasia
Camtasia offers a suite of video capturing and editing tools. It is available for a free trial but does require a paid account
for the most sophisticated features.
Loom
Loom is advertised as a video communication tool, but its functions include screencasting.
Screencastify
Screencastify is popular with users because of its integration of screencasts with Google products.
Screencast-o-matic
Screencast-o-matic has a free and paid version. The paid version removes the watermark, extends your recording time,
and offers video editing tools. The free version is quite robust.

                                                                                       373
Video and Your Computer

Video is memory intensive and, depending on your computer's processor and memory, may tax your computer quickly.
You should save early and often. At times, your video editing software will start to do things that you know it is not
supposed to do. For example, you may insert a title and it will show up in the wrong place. This is a memory
management issue. Quitting and restarting the application will clear the working memory and the software should work
properly. It may be necessary to do this multiple times when editing.

Tips for Creating Video

It is possible for instructional designers and developers to create good, effective learning materials with limited video
resources. For example, adding titles and credits to a screencast will help give it a more polished and professional look.
Editing out unwanted pieces of a video will help focus the learner. When creating video with limited tools, there are
some basic things to remember.

Length

For videos produced with limited resources, typically the shorter the better. For example, instead of creating one long
video that demonstrates the five functions of the Audacity audio editing software, five shorter videos were created. This
allows learners to watch what they need without rewinding and fast forwarding. If you are recording an hour-long
lecture, break that lecture up into shorter videos. This allows the learner to watch pieces when they have time. It can
seem overwhelming to sit down to watch an hour-long lecture. If these shorter videos are labeled well when placed in a
learning management system, it also allows the learner to locate and rewatch only the parts they feel they need
remediation on.

Special Effects

Use them judiciously. Use them sparingly. When people first use video editing software and realize how easy it is to use
special effects and transitions, it can be tempting to overuse them. We advise people new to video editing to make a
video early that uses every wipe, fade, and explosion they can find and get it out of their system. After that, use them
only when they make sense in context of the content.

Lighting

When shooting video with limited resources, shoot in places that provide as much indirect natural light as possible.
Look for rooms with windows on multiple walls. Notice the direction the light is coming from as it may cast shadows on
subjects. Avoid filming in front of windows to avoid backlights making your subject appear as a shadow. If professional
lighting is not available, look for lamps that you can place on either side of your subject to balance the light and avoid
shadows.

Audio

Capturing live audio will sound better in a smaller space. If you are recording in a large room, use an external
microphone like a lapel microphone or boom microphone to capture your audio. Unless you have professional level
skills and equipment, do not record your audio source separately from your video source. Synching the audio with the
video can be difficult for users with limited resources and may result in the person's lips not matching their words. When
possible, use voice-over narration and the recommendations presented in the audio section of this chapter for the best
quality.

B Roll Footage

When added carefully and paying attention to the needs of your content, high quality B Roll footage can dramatically
improve the look and professionalism of your video project. The websites pexels.com and pixabay.com provide free
high quality B Roll footage that you can use in your projects.

                                                                                       374
Conclusion

With free and open source software and some reasonably priced pieces of equipment, it is possible for instructional
design professionals to create quality audio and video learning materials with limited resources. The tools, resources,
and tips provided here are a starting point for you to begin work with audio and video. Once you have worked with entry-
level skills you will be able to expand upon those during your career as an instructional design professional.

                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/audio_and_videoT.

                                                                                       375
376
  30

Using Visual and Graphic Elements While Designing
Instructional Activities

Justin Sentz

The time and expense of creating and obtaining visual/graphic elements, or pictures, for use within instruction is not
insignificant. Then why use visual or graphic elements during instruction at all? The use of pictures during the design of
instructional activities has been shown to have a significant impact with regard to both increased learner motivation
and improved learning outcomes (Carney & Levin, 2002). Pictures and graphics can serve to convey information to the
learner directly or facilitate the learner's understanding of related textual information within the instruction. Different
types of visual and graphic materials are particularly suited for specific functions during instruction, and there are well-
established design considerations for their use within instructional activities to increase their effectiveness. The use of
pictures and graphics plays an important role in helping to manage the intrinsic cognitive load and reducing the
extraneous cognitive load experienced by learners, who are then able to devote mental resources to learning the
material within the instruction (Sweller et al., 2019).

The Role of Visual Messages in the Communication of
Information

In order to be intentional about the use of visual and graphic elements during the design of instruction, it is important to
first consider some of the fundamental concepts related to the role of visual messages within the communication of
information more generally. What exactly is a picture, and what purpose does it serve? Knowlton (1966) proposed that
visuals, or pictures, could be categorized according to their purpose or function within instruction-realistic, logical, or
analogical. Realistic pictures look like the objects they refer to in the real world outside of the instruction. If the intention
is to communicate a concrete example of the concept being presented, then a realistic picture is a good option for
doing so. Logical pictures, on the other hand, provide a visual depiction of the structure of a concept being presented. If
the purpose is to communicate an understanding of the organization of territories within a country or how electricity
flows through a circuit, then a logical picture such as a map or diagram would be effective. Finally, an analogical picture
depicts relationships among complex concepts through the use of concrete visual elements that are more familiar to
the person. When there is a need to compare a particular phenomenon to something a learner is more likely to
encounter in everyday life, then an analogical picture is a helpful option for communicating that information.

Using Visual or Graphic Elements to Increase the Effectiveness
of Instruction

Taking into consideration the role of visual messages in the process of communicating information, it is important to
think about ways in which visual and graphic elements can be used to increase the effectiveness of instructional
materials. Peeck (1993) has suggested that the effectiveness of pictures within instruction is dependent upon the

                                                                                       377
manner in which they cause the learner to process the information contained within the visual elements provided. This,
in turn, is a product of both the characteristics of the learners themselves and the graphic materials used within the
design. For example, visual elements can be a powerful means of showing spatial relationships or positioning of
objects that are being presented within the instruction. These types of visuals can be placed before a section of text
when learners are expected to draw upon prior knowledge of the information. They may also be placed in-line with the
text when learners are unfamiliar with the spatial relationships and will benefit from a picture that shows relative
positioning of the objects being discussed. Depending on the complexity of the material relative to the expertise level of
the learner, pictures may also serve to illustrate abstract concepts that are presented textually within the instruction.
Learners can use these visual elements to supplement their comprehension of the material through these
representations or confirm their understanding of the text by reviewing the graphics and pictures provided. Yet another
element of effectively using visual and graphic elements within instruction is the potential to motivate learners to pay
particular attention to specific material and process information from the text more deeply. A learner may prefer to
clarify or reinforce their understanding of textual information through visual elements, which can in turn help with the
encoding and subsequent retrieval of that information at a later time.

Types of Visual Elements and Their Functions Within Instruction

While pictures and other visual elements can be extremely effective for learner motivation and comprehension, specific
types of visual elements are more effective than others based on their function relative to the ways in which they relate
to the instruction. Levin et al. (1987) categorize pictures into five general types according to those functions-
representation, organization, interpretation, transformation, and decoration. One of the most common types of pictures
used in instruction is representational, which illustrates the textual information being presented for the purpose of
reinforcement (see Figure 1). When the purpose of using a picture is to present a concrete visual representation of
information contained in the instruction, a representational picture is often the way to go.
Figure 1
A Representational Picture Illustrating the Components of an Atom

Note. Retrieved from http://www.whoinventedfirst.com/who-discovered-the-atom/

                                                                                       378
Another type of visual element is organizational, which shows relationships between different parts presented in the
text (see Figure 2). These can serve the purpose of illustrating a series of steps in a procedure or provide a large set of
data through graphics such as diagrams or charts.
Figure 2
An Organizational Picture Illustrating the Steps for Performing CPR

Note. Retrieved from https://www.behance.net/gallery/3164136/CPR-Chart-Re-design
An interpretational picture is a third type of visual element that is often used when the intent is to clarify complex
information provided within the text (see Figure 3). Much like Knowlton's (1966) idea of analogical pictures, these visual
analogies can be used to ground more abstract concepts in visual elements that are easier for the learner to
comprehend.
Figure 3
An Interpretational Picture Presenting the Structures in an Animal Cell as Buildings in a City

                                                                                       379
Note. Retrieved from https://www.up.ac.za/teaching-and-learning/news/post_2679382-young-lecturer-use-analogies-to-
assist-students-in-molecular-and-cellular-biology
Yet another type of graphic element that is somewhat infrequently used is the transformational picture, which provides
a mnemonic that facilitates retrieval of information from memory at a later time (see Figure 4). If the intent of the visual
element is to help the learner memorize information through the association of a related picture, then the creation of a
transformational picture may be worth the time and effort to design.
Figure 4
A Transformational Picture Using Mnemonics for the Actions of Neurotransmitters

                                                                                       380
Note. Retrieved from shorturl.at/hluJ7
A final type of visual element that is used within instruction but has no empirical support for its impact on learning is
the decorative picture, which serves to break up textual information or provide "eye candy" for the learner (see Figure 5).
While it is sometimes argued that decorative images may motivate learners, their use is not directly tied to improved
learning outcomes and are thus generally discouraged.
Figure 5
A Decorative Picture of a Mountain

                                                                                       381
Note. "Mountain" by barnyz is licensed under CC BY-NC-ND 2.0

Strategies for Structuring Visual or Graphic Elements to
Facilitate Processing

We can see that visual and graphic elements have the potential to increase the effectiveness of instruction, and specific
types of visual elements are more appropriate based on their intended function within the instruction. However, it is also
important to note that the manner in which visual and graphic elements are structured have a significant impact on the
manner in which they are processed by learners. Sweller et al. (2019) describe cognitive load as the mental effort
required by learning tasks that impact the learner's ability to both process new information and store it within long-term
memory. They propose a set of strategies for structuring visual and graphic information to help manage the intrinsic
load associated with the material itself and the extraneous cognitive load that is introduced by the instructional
techniques employed:

                                                                                       382
      Integrate textual and graphic information into one element in order to eliminate the effect of splitting the learner's
      attention. This could be accomplished by taking a list of procedural steps and overlaying them on a diagram to
      show where each step should be performed.
      Eliminate multiple stand-alone sources of textual and graphical information in order to reduce the mental effort
      needed to deal with redundant information. If a visual or graphic element can fully communicate a concept without
      additional textual information, then it should be used on its own.
      Present concepts before adding context by giving the learner increasingly realistic visual elements during the
      instruction. By starting with low-fidelity visual elements and building toward high-fidelity elements, the learner is
      able to gain an understanding of the concepts rather than being distracted by contextual details.
      Gradually present information to learners through visual and graphic elements. Through the use of this simple-to-
      complex strategy, the learner will be able to avoid getting overloaded by too much information before processing
      the required information.
      Strategies to reduce cognitive load tend to have a reverse effect on learners with greater levels of expertise, and
      thus need to be adjusted accordingly. An example of this would be if the instruction will be used with expert
      learners who are familiar with a procedural diagram, the textual instructions could be removed from the graphic
      element and replaced with numbers for each step.

Finding and Creating Visual and Graphic Elements

Locating or creating visual and graphic elements to enhance instructional activities according to the principles
discussed in this chapter may seem like a daunting task, but there are a number of resources available to make the
process manageable. If you would like to find visual elements that have already been created by someone else, a
number of sites online have collections of images that can be used without having to pay for their use. A few of these
sites include:

      Creative Commons image search
      Unsplash
      Pixabay
      StockSnap.io
      Pexels
In addition to these individual sites, you can also use the Google Image Search in order to identify visual elements that
have usage rights allowing you to employ them in your instruction without infringing on the copyrights of the owner.
Watch this video to learn how this is done:

                                                                                       383
   Google Image Search and the Usage Rights Filter Tutorial

                                                                               Watch on YouTube

If you are unable to locate existing visual or graphic elements for use in your instruction or have specific requirements in
mind, you can always create them yourself in a number of different ways. First, you can take photos with a smartphone
or digital camera and download these onto your computer for incorporation into your materials. Second, you could
create the visual element by hand with a line drawing that communicates the desired information and scan that image
into your computer using a digital scanner or printer. Finally, you can create visual elements in software packages such
as Adobe Photoshop or through the SmartArt feature in Microsoft Word or PowerPoint and export them for use in your
instruction. Google Drawings is a free, web-based alternative to these types of software that can be used to create
charts, maps, or diagrams and download them as PNG or JPEG files without a great deal of design experience. Watch
this video to see the basic use of this tool:

                                                                                       384
   Introduction to Google Drawing

                                                                               Watch on YouTube

Conclusion

Employing the use of visual and graphic elements during the instructional design process is not simply a matter of
finding or creating a set of pictures that are somehow related to the textual information in the instruction. Research has
shown that visual elements have the potential to increase motivation and foster improved learning outcomes, but only
when the appropriate role of visual messages in the communication of information is taken into account. Specific types
of visual elements can be used to serve a particular function in the instruction based on the manner in which
information is presented, such as showing spatial relationships or illustrating abstract concepts within the text. In
addition, following basic strategies for structuring visual and graphic information can facilitate learner processing
through the management or elimination of the cognitive load experienced by the learner. In the end, the creation and
curation of visual/graphic elements for instructional activities will be well worth the time and effort invested when the
purpose of using those elements is aligned with the objectives of the overall instruction.

                                                                                       385
   Application Exercises

        1. Using an existing unit of instruction or one that you are in the process of creating, explain how you would
           use visual or graphic elements to increase the effectiveness of the instruction by doing the following:
              1. Showing spatial relationships or the relative positioning of objects
              2. Illustrating abstract concepts that are presented in the text
              3. Motivating learners to pay particular attention to specific material in the text

        2. Using pre-existing instruction or materials you have created, explain how visual elements are used (or could
           be used) to serve each of the following functions:
              1. Representation
              2. Organization
              3. Interpretation
              4. Transformation

        3. Within a unit of instruction that employs visual and graphic elements, explain how at least three of the five
           strategies for structuring visual elements outlined in this chapter could be used to reduce cognitive load
           and facilitate processing for the learner.

   Additional Readings and Resources

     Check out these resources for additional information on the topic of using visual and graphic elements while
     designing instructional activities:

           233 Tips on Graphics and Visual Design [PDF eBook]
           Instructional Design and Visual Design: The Pillars of Great eLearning
           10 Types of Visual Content You Should Use to Increase Learner Engagement
           Accessible U: Instructional Graphics
           Do Learners Understand Your Instructional Graphics? [Podcast]

References

Carney, R. N., & Levin, J. R. (2002). Pictorial illustrations still improve students' learning from text. Educational
         Psychology Review, 14(1), 5-26.

Knowlton, J. Q. (1966). On the definition of `picture'. AV Communication Review, 14(2), 157-183.
Levin, J. R., Anglin, G. J., & Carney, R. N. (1987). On empirically validating functions of pictures in prose. In D. M. Willows

         & H. A. Houghton (Eds.), The Psychology of Illustration: I. Basic Research (pp. 51-85). New York: Springer.
Peeck, J. (1993). Increasing picture effects in learning from illustrated text. Learning and Instruction, 3(3), 227-238.
Sweller, J., van Merriënboer, J. J. G., & Paas, F. (2019). Cognitive architecture and instructional design: 20 years later.

         Educational Psychology Review, 31(2), 261-292.

                                                                                       386
This content is provided to you freely by EdTech Books.
Access it online or download it at https://edtechbooks.org/id/using_visual_and_graphic_elements.

                                                        387
388
  31

Simulations and Games

Jeff Batt

    Using simulation games, learners can explore real-world scenarios in a safe environment. Simulation try,
    simulation watch, simulation evaluate, and simulation play are examples of simulation game scenarios that can
    help students learn. It is critical to establish a theme for your game in order to bring balance between the tone,
    visuals, audio, video, text, and other elements involved in its development. In addition, learners should be
    presented with a variety of challenges of varying levels of difficulty. Finally, you should consider how to manage
    interactions in simulations and games. There are three common ways: Variables, Triggers, and Conditions.
    Learning through simulation games could help learners to comprehend new concepts and then apply what they
    learn in a safe and controlled environment

Simulations present the learner with real-world scenarios and allow them to explore the scenario in a "safe"
environment. A basic pattern for this is to (a) present or show the desired end result; (b) allow students to safely try the
result out; (c) then evaluate if the student is able to complete the task; (d) and allow them to play around with the
concepts in an engaging way to deepen their learning. Let's call these: present, try, evaluate, and play.

Present: Presenting starts by showing the learner how to perform a certain action. This could be by simply showing
them a video or having them click through a series of slides or steps to see how to accomplish a task.

Try: Trying happens as the learner is placed in an environment that is reminiscent of the real-world environment, but this
environment has been simplified, altered to minimize or eliminate risks, or has been otherwise modified to draw out the
material to be learned. This is what we mean when we say a simulation is a "safe environment." For instance, in a
simulated Information Technology environment, the learner can't cause a system to crash or accidentally send out
secure user data as they try things out. You do want the simulated environment to be recognizable when compared to
the real-world scenario, however, so that learners get an authentic experience and can transfer what they learned back
into the real environment.

Evaluate: After learners have seen the desired outcome and tried it in a safe environment, you want to evaluate them:
can they do it in an environment with no extra help and with real consequences? Evaluation helps both solidify lessons
learned as well as providing the teacher/instructional designer insight into whether the learner can perform the task or
not.

Play: Simulations and games allow for exploration; learners don't have to just proceed through the instructional material
in a linear way. And even fun, exciting games can be educational; they create engagement that helps students learn the
concepts in a different manner through their simulated play. Games can even create a desire for the student to "try
again" to see if they can get a higher score or if they can master a concept. Gaming, then, could be a useful technique to
help solidify the concepts being taught.

Keeping these four principles in mind, let's consider how they could be applied in some common scenarios.

                                                                                       389
Simulation--Watch

One form of an instructional simulation asks learners to watch a procedure or skill. One of the more common forms
these simulations can take is the software simulation. A software simulation is essentially showing someone how to do
some action on a computer by recording your screen. In Video 1 you can see an example of how to create a Watch
simulation using the screen recording tool Camtasia.

   Video 1: How to Create a Watch Simulation

                                                                     Watch on Vimeo

Simulation--Try

The next kind of simulation is one that allows students to try a skill or procedure themselves. This allows the learner to
engage with the content and practice it in a safe environment. There are various applications that can be used for
creating a Try simulation; in Video 2 you can see an example of how to create a Try simulation using the tool Captivate.

                                                                                       390
   Video 2: How to Create a Try Simulation

                                                                     Watch on Vimeo
One last tip: when you create Try simulations, consider including ways that the student could possibly fail. Failing is part
of learning; it can help the learner see what happens if they select various alternatives, as well as help them consider
how they can recover from their mistakes.

Simulation--Evaluate

After the learner has watched a procedure and tried it out for themselves, you may need to ensure they know how to
perform certain tasks. This is where the role of Evaluate simulations come into play. Evaluate simulations help both you
and the learner judge if they are able to perform a task they have just learned. The most helpful evaluation simulations
are ones that allow the user to fail and learn from their mistakes. The key here is to try to make the simulations as close
to the real environment as possible. Video 3 shows you how to get started doing this.

                                                                                       391
   Video 3: An Example of How to Create an Evaluate Simulation

                                                                     Watch on Vimeo

Simulation--Play

The last type of simulation allows students to play with ideas or concepts associated with the instructional
environment. Playing helps learners work with the knowledge they have gained in different, engaging ways. The goal is
to help them take what they learn and apply it in novel ways so they are able to master it better. Let's walk through some
important parts of a game.
There are key factors that go into creating a learning game which enables this simulated play. I don't think anyone
expects you to create a World of Warcraft type game, but there are some parts you can use to make the game stand out
in an engaging and fun way for the learner. Some important considerations for Play simulations include: Theme,
Progression, and Challenge. Consider each of these principles using the extended example below.

Theme

A theme is a unifying core to your game that helps express its purpose, and bring a sense of harmony between that
purpose and the tone, visuals, audio, video, text, and other elements you create. To immerse learners into the game,
introduce a theme as soon as possible, perhaps expressed by using a clever or unique logo. This helps the learner know
they are exiting the standard instructional format and entering a gamifed environment.
Review this Jeopardy-style game. Notice how a theme is introduced when the learner first begins the game, as are
initially presented with a large logo that provides clues about what they will be doing.
Figure 1
Initial Logo of a Game

                                                                                       392
Providing a theme has a couple of results. It sets the tone of the game through the logo and visuals that complement
the logo. And the theme can help you tell the "story" of the game, or provide cues to the learners about how they should
interact with the environment.

Progression

Progression is how learners move from the beginning to the end of your game, and how they navigate through the steps
in between. Progression is a principle you could use in different ways. In the case of our Jeopardy game, the tool to
manage progression is the game board.
Figure 2
Progression Screen

                                                                                       393
As the learner moves throughout the game, they clearly see where they have been along with what levels or cards were
successful or unsuccessful.
Figure 3
Progression Screen Reflecting Progress

                                                                                       394
This type of progression tool is also helpful for the learner if they try the game again. They can use the progression
board to gauge how they are doing each time they play.

Challenge

Challenges are how you present instructional content and allow learners to interact with that content. In our game, when
the learner chooses options on our the progression board, they begin an individual challenge. These challenges can
come in many different forms with varying levels of challenge between the tasks. One way to challenge the learner is
through a standard question.
Figure 4
Standard Question

                                                                                       395
If the learner gets the answer incorrect, they will see some kind of visual indication, and perhaps some feedback.
Figure 5
Feedback on Incorrect Answer

                                                                                       396
If the learner gets the question correct, they will see correct feedback.
Figure 6
Feedback on Correct Answer

                                                                                       397
But you can present challenges in ways other than through questions. You can also add some more ambitious aspects
into each challenge, like having them try a procedure or a skill.
Also, since this is a game, you might want to have an overall score that is visible to the learner. When the learner gets
the challenge correct, the score increases. To make it even more challenging, points could be taken away when the
learner does not answer correctly. You could also add a timer or other sense of urgency for students to complete the
game.

Managing Interactions in Simulations and Games

Simulations and games require you to manage interactions that students have with the program, such as when you
have to pass information from one screen to another based on how students respond to a question. Three common
ways of managing interactions you should know about are Variables, Triggers, and Conditions.

Variables

Variables are storage locations. They hold information that can change or be updated later. The most common type of
variable for a game is the Number variable which will store a number value. This is perfect for scoring or being able to
calculate end results in a final interaction. For instance, if you create a game with a score, you want to create a variable
that holds the initial starting value (probably 0), but can then be changed depending on whether learners earn points or
have them taken away.
Let's explore how to create a variable in this video in a common instructional authoring tool.

                                                                                       398
   Video 4: Creating Variables

                                                                     Watch on Vimeo

Triggers

Triggers are events that happen in a simulation. For instance, when a button is clicked, what should happens next? In
many instructional authoring tools, you'll use triggers to show and hide different elements based on how learners
interact with a page.
You have a lot of flexibility with triggers, and the key to adding different types of interactive play is to try out different
types of triggers. Instead of only using standard questions in a game, for example, you can use drag and drop, timed
elements, and more. This creates the interaction and intensity of simulated play.

                                                                                       399
   Video 5: Using Triggers

                                                                     Watch on Vimeo
One key to using triggers is deciding when the trigger will happen. This is done under the "when" part of the triggers.
Figure 7 provides a list of instances when a trigger can fire.
Figure 7
Trigger Selection Screen

                                                                                       400
Conditions

Triggers are great, but there may be times you only want the trigger to happen if a certain condition is true. Consider the
following statement: "If you're happy and you know it, clap your hands."
This is a simple statement, but it reflects so much of what a condition is. It starts with the key word if. Meaning, we only
want this condition to happen if certain conditions are true, and the conditions are, "if you are happy and you know it."
We are checking for two conditions, then running the action if the condition is true.

                                                                                       401
   Video 6: Understanding Conditions

                                                                     Watch on Vimeo
Most of the time you will use conditions when you are checking a variable value. So, with the Variable option selected,
find the variable you are checking for and select the value. It will then ask you to select an operator. Let's use the score
variable and check if it is greater than or equal to 100.
Figure 8
Trigger Condition Screen

                                                                                       402
Now this trigger will only run if the value is 100 or greater. This is a great way for you to only have triggers run if a
condition is met.

Conclusion

The goal of instruction is to help the learner first understand and then be able to apply what they are learning in safe and
controlled environment. Simulations and games are great tools for doing this, allowing learners to test the new
concepts before entering the real world, practice mastery through fun and engaging games, and try scenarios in an
environment that allows them to fail and learn from their mistakes.

                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/simulations_and_games.

                                                                                       403
404
  32

Designing Informal Learning Environments

Seth-Aaron Martinez & Justin N. Whiting

For the past 30 years, the prevailing 70-20-10 industry model of employee development postulates that 70 percent of
individual learning and growth occurs through relevant but challenging experiences, 20 percent through relationships
and social interactions, and only 10 percent through formal learning activities (Center for Creative Leadership, 2020;
Watkins et al., 2014). What, then, is the implication for learning design given these trends? To assist in taking advantage
of such patterns, this chapter is dedicated to the design of informal learning environments.

Defining Formal and Informal Learning

Have you ever sat outside at night and gazed up at the stars? Perhaps you have done this with a child as you talk about
constellations. Now imagine you pick up a smartphone with an augmented reality app that can provide instant
information and feedback on the stars that you see. Without stepping foot in a classroom or reading a textbook,
personalized and on-demand informal learning has occurred. This scenario of real-time information and feedback not
connected with any formal setting is one example of informal learning.

According to TrainingIndustry.com, formal learning refers to "a type of learning program in which the goals and
objectives are defined by the training department, instructional designer, and/or instructor." Informal learning can be
defined as the pursuit of any knowledge, skill, or understanding that occurs outside a formal or non-formal learning
event, such as a classroom, training facility, or eLearning course (Dirksen, 2015). Informal learning includes family
discussions at home, Googling a topic on the Internet, seeking advice from a colleague, visits to museums, and other
everyday experiences (Livingstone, 1999; Bell, 2009). Informal learning has shown to be effective across many contexts
(Allen, 2004; Bell, 2009; Miller et al., 2008), especially in work environments (Carliner 2012). Examples in the workplace
include "brown bag" learning, like Talks at Google; the Boeing Leadership Center, which devotes an entire portion of the
learning path to an open-ended, unstructured mentoring program; and GE's Crotenville training, which is famous for the
shadowing and rotation program that was created precisely to take advantage of the informal learning that occurs
between a novice and an expert.

In trying to delineate formal and informal learning, it may also be helpful to consider the formality of instruction on a
scale. Sefton-Green (2004) stated that informal learning is used quite loosely to describe many kinds of learning that
occur outside of schools or other formal settings. Rather than pointing to a specific definition for informal learning, he
proposed that learning environments be evaluated on a scale from informal to formal on two criteria: (1) organization of
the curricula and (2) the setting, as seen in Figure 1.

Figure 1

Evaluating Learning Environments

                                                                                       405
If we use this as a guide to identify informal learning spaces, we can determine not only if a learning environment is
formal or informal, but we can see where it might sit on this spectrum. For example, schools have traditionally been
highly formal on both the setting and the curricula. But recent growth of online/virtual schools may still be formal and
highly structured in terms of the curricula, while the setting may be in someone's own home and on their own time
schedule. Similarly, workplaces are using more informal newsletters, podcasts, wikis, or informal professional
development, rather than formal, in-person training meetings, or assigned computer-based training.

Designing for Informal Learning

With informal learning now defined, the focus of this chapter shifts to four principles that are effective guides to
consider when designing environments conducive to informal learning.

Principle 1: Provide Learners a Choice in Their Learning

Instructional designers should deeply consider how learners are going to interact with content. For teachers and
professors developing a syllabus, allowing the learner a choice in their learning experience can have a direct link to
intrinsic motivation (Cordova & Lepper, 1996). Consider providing several options for a midterm assignment and allow
the learner to decide which assignment looks most relevant to them. Or, rather than assign a topic for a project, allow
the learner to submit a proposal of a topic of personal interest.

In a corporate setting, developers can provide a list of possible related topics, and only require mastery of one. Consider
curating a library of resources in various media such as video, image, text, audio (podcast), self-guided modules, and
then allow learners to decide how to gain mastery of the topic. Even in instances where there is a more strict set of
regulations (e.g. compliance) that must be covered, there are options to allow the learner some control over their
learning. For example, consider presenting the content not in a linear way, where the learner proceeds from topic A to

                                                                                       406
topic Z, but instead allow them to choose the order of topics they may want to go through first. Instead of a bulleted list
or outline in a computer module, consider a more visual layout where they can choose the order from a group of topics.

Museums have long been an excellent example of providing learners with a choice in their learning experience. While
museums may provide courses, scripted tours, and other formal learning experiences, for many, museums are open
spaces that encourage exploration and even learning through failure (Simpson et al., 2019). Museum educators
carefully plan out displays, flow of the rooms, and many other factors to create fun and engaging environments.
Learners in a museum choose where they go, how long they interact with a specific exhibit or section, and generally do
not have many constraints.

Principle 2: Design for Collaboration, Idea Sharing, and Peer Interaction

While teamwork, collaboration, and group projects are all components of formal learning, informal learning is also well
suited to benefit from social interaction. Albert Bandura argued that "most human behavior is learned observationally
through modeling: from observing others, one forms an idea of how new behaviors are performed, and on later
occasions this coded information serves as a guide for action" (p. 22, 1977). Further, Lev Vygotsky maintained that
social learning consists of learning through social interaction (1978).

When more specific knowledge is needed by an individual, a personalized informal experience in the form of coaching
or mentoring will allow a novice to learn directly from an expert. A learning experience that centers on group work and
peer interaction will invite informal learning to take place as individuals pose questions to and receive answers from
their teammates and colleagues. To do this, consider assigning group tasks or projects, or posing problems that require
multiple individuals to complete. A technique to take advantage of the unscheduled nature of informal social learning is
to design a wiki or forum system that leverages the knowledge of the community in a socially constructed manner.

Lastly, an emerging trend in informal learning is microlearningsmall learning units such as text phrases, photos, or
audio snippetsand its associated affordances for instructional designers (Giurgiu, 2017). For the specific principles of

microlearning design, see Zhang and West (2020).

Principle 3: Leverage the Benefits of Constructionism and Project-Based
Learning

Constructionism and project-based learning are two additional areas where we can design informal learning
opportunities. With constructionism and project-based learning, learners typically become active participants in the
experience and learn organically as they work through a challenge. Designers working to implement informal learning
principles can include opportunities for exploration and creation. Many businesses have implemented innovation
centers or creativity labs where new ideas can be tested and prototyped. For example, Pixar fosters innovation and
encourages their employees to develop new skills through their Pixar University. As part of the "university," employees
can sign up for free classes in painting, ballet, or sculpting. Additionally, Lowe's Home Improvement company has
created innovation labs where employees reimagine how to help customers using augmented reality to plan virtual
home renovation projects. Allowing learning through experimentation and failure can lead to better quality products and
services (see April 2011 issue of Harvard Business Review).

A recent trend in schools, communities, museums, and even healthcare, is the rise of makerspaces. Makerspaces
provide a physical space where many forms of constructionist and project-based learning may occur (Peppler et al.,
2016). Maker Faires have been promoted and popularized by Make Magazine and other organizations around the globe.
The effort to overcome constraints can often lead to creativity in these spaces (Stokes, 2005). In a makerspace,
learners may be mentored by another individual on how to use a machine such as a 3D printer or laser cutter. Often,
small groups may work together to prototype and test ideas.

                                                                                       407
Principle 4: Leverage the Benefits of Gamification and Playful
Competition

Playful competition and gaming are proven mechanisms for increasing learner engagement (Steinkuehler & Squire,
2014). While the principle holds for formal learning, it is particularly effective in designing informal learning
environments. Gamification is "a relatively recent term that describes using game thinking and game mechanisms in
nongame contexts to engage users (Deterding, 2013)" (Steinkuehler & Squire, 2014, p. 389). The key criteria for an
effective gamified design are exploration, immersion, socialization, and competition.

To design for exploration, it is important to design an experience that does not necessarily have a single path toward a
solution. This allows the learners to truly discover, explore, make mistakes, and iterate. To design for immersion, it is
important that the challenge be such that the only way to solve the problem is with dedicated, focused time, over an
extended period. For example, learners can (a) build or produce an artifact from scratch, (b) solve a problem that
requires the synthesis and application of multiple concepts simultaneously, (c) give or receive feedback to or from
peers or experts where they must refine their work as part of the final solution, or (d) some combination of all three.
(Socialization is discussed in Principle 2 above.) To design for competition, it is helpful to consider something that
determines one or multiple winners. Challenges with levels, simulations, scoring, badges, and leaderboards are features
of competition that you can incorporate into your design.

Designing for Informal Learning at Adobe

In late 2016, the Adobe CTO set forth a new charge: to upskill all software engineers (SWE), of which there were
approximately 6,000 globally, in the machine learning (ML) discipline. To accomplish this, an ML Training Program was
launched to begin the process of mass upskilling. After one year, an initial cohort of 1,000 individuals had completed
the five-month e-learning courses, bringing with them many lessons learned. One consistent point of feedback across
multiple stakeholders was that an opportunity for more in-person, hands-on learning was needed compared to the one-
hour-per-week classroom sessions that had been available for the participants over the five months. In this instance,
the challenge loomed large to introduce a more intensive in-person, hands-on experience to 1,000 people who were
scattered across the globe.

To address the feedback, the training team designed and implemented a bootcamp model that incorporated a project-
based competition in the form of a hackathon for the second cohort of 1,000. The bootcamps would be three
consecutive days each. Over those three days, the team would spend three-fourths of one day delivering company-
specific material related to the company's AI/ML platform. The remaining two and one-fourth days for the learners
would be spent in the hackathon competition, thus allowing for the immersive and dedicated time-on-task.

To start each hackathon, the training team provided the basic rules and constraints. Bootcamp attendees would divide
into small teams of two to six people, organized by their shared ML interests. To gamify the hackathon, each team was
asked to prepare a brief "Venture Capital Pitch" to present the ML feature they each worked on over the two or more
days. There were four categories that would be awarded winners: most interesting feature, most likely to become an
actual Adobe product feature, most whimsical, and best overall feature. During the course of the entire hackathon, a
small number of ML experts would roam the large room and answer any questions a team may have.

By creating this environment, the learners spent two or more days laboring to develop their ML feature. The ID team
observed that the competition fueled their natural interest by increasing the participants' drive to ask project-relevant
questions and to consume publicly available ML content on the internet. Numerous teams arrived hours before the days
started and stayed hours past the day's allotted block of time. Participants would extend what they had learned from
the e-Learning courses by scouring publicly available websites to find help for their unique scenarios. Some would
watch brief tutorials and, if relevant, immediately teach their teammates what they learned; other teams would divide
and test out open-source code and, once successful, exclaim "I got it, it worked!" for the others on the team to know
they need not search further.

                                                                                       408
This pattern repeated itself constantly over the approximately two days. Ultimately, the hackathon experience fostered
an environment in which participants would seek out anything and everything to assist them in overcoming an obstacle
and therefore advance the ML feature they were designing in hopes of building a competitive feature for the
competition. In the end, knowing (and hearing) the effort put forth by all teams, the participants reported taking great
pleasure in listening to the presentations and viewing the demos of each team's ML solution developed over the
hackathon.

Reflection

Table 1 below lists several informal learning activities with their associated strengths and limitations. The application
exercise that follows Table 1 will assist in your thinking of when and how to apply the informal learning design
principles discussed in this chapter.

Table 1

Strengths and Limitations of Informal Learning Activities

  Learning        Description                          Strengths               Limitations
Activity/Tool

Project- or       Projects/problems allow         Can address multiple         Cost: potentially very
problem-based     learners to appropriately       learning objectives with a   expensive to execute
scenarios         struggle through arriving at a  single problem scenario      Time: potentially very time-
                  solution. The struggle area is  Can be accomplished in       consuming to design,
                  essentially the zone of         small groups, thus           execute, and evaluate
                  proximal development, where     alleviating the need for a
                  informal learning occurs.       high number of experts to
                                                  provide guidance or help

Coaching or       An example of social            Close proximity to relevant  Time: often difficult to
mentoring         learning, where a novice can    expertise                    design a program that
programs          learn directly from someone     Feedback that is very        requires so much time of
                  more expert. The cycle of       specific, timely, and        someone senior in
                  practicing/trying, receiving    personalized                 experience
                  feedback, and trying again is   Often 1:1                    Challenging to find one (let
                  strongly supported by an                                     alone many) individuals
                  expert who serves as a                                       with seniority willing to
                  guide.                                                       participate

Job aids, wikis,  A strong collection of          Cost: minimal                Potential lack of
tutorials, or     relevant resources is           Speed: utility is all that   monitoring/governance
internal forums   generally superior to formal    matters, so they can be      Because utility is most
                  instruction. In contrast to     created quickly              important, they can
                  formal instruction, these       Crowd-sourced information    sometimes be poorly
                  resources require little cost,  is often highly actionable   designed visually
                  take little time to design and
                  produce, and are extremely
                  tactical in nature.

                                                  409
Communities of   A community of practice is     Cost: typically free or      Can be overly general (i.e.,
practice,        designed for, and by, the      inexpensive                  not applicable enough)
learning         interested participants        Typically more broadly
networks, or     themselves.                    applicable than information  Cost: entry can be
external forums                                 in an internal forum         potentially quite expensive
                                                Maintenance & governance     Time: to maximize learning
Exhibits,        Exhibits and museums are                                    at a performance or
museums, or      open spaces that encourage     Exhibits and displays can    museum often requires
performances     exploration and even learning  be highly engaging           multiple hours, minimum.
                 through failure. At a          Socializing with other
                 performance, an individual     attendees/participants is
                 learns through observation.    common, thus inviting the
                                                social component of
                                                informal learning
                                                Observational learning is
                                                also very effective
                                                (Bandura, 2003)

Application Exercise

For your reflection:

  1. What is the relative amount of information intended to be taught?
        1. If it is a considerable amount, consider a formal learning approach.
        2. If it is a small amount, formal learning could simply be excessive. Consider a simpler solution like a
            wiki or job-aid that would require much less time/cost to create.

  2. What needs to be learned? Is it explicit or tacit?
        1. If it is explicit information (i.e., codified or written down), then consider formal instruction.
        2. If it is tacit information (i.e., not codified or recorded), consider leveraging an informal learning
            environment.

  3. How complex is the material?
        1. If it is highly complex, consider formal instruction.
        2. If it is not highly complex, consider leveraging a social learning component, where participants must
            work together or share knowledge to achieve a common goal.

  4. How much/often is the content subject to change?
        1. If it is not often, consider formal instruction. Taking the time, cost, and effort to design a formal
            experience is justified when it will have a long shelf-life.
        2. If the content will change fairly often (or more), then consider an informal learning design which, by
            definition, embraces a changing landscape and seeks up-to-date, accurate information--regardless of
            the source.

                                                410
Conclusion

The evidence is clear that often, learners gain knowledge predominantly outside of formal settings. As a result,
instructional and learning experience designers should be intentional about taking advantage of the affordances of
informal learning. To that end, as you follow the four design principles shared above, you will be able to design informal
learning experiences that take advantage of the natural interests and curiosity of your learners.

References

Allen, S. (2004). Designs for learning: Studying science museum exhibits that do more than entertain. Science
        Education, 88(S1), S17-S33.

Bandura, A., & Walters, R. H. (1977). Social learning theory (Vol. 1). Englewood Cliffs, NJ: Prentice-hall.

Bandura, A. (2003). Observational learning. In J. H. Byrne (Ed.), Encyclopedia of learning and memory, 2nd ed., p. 482-
         484. New York, NY: Macmillan.

Bell, P. (Ed.). (2009). Learning science in informal environments: People, places, and pursuits. National Academy Press.

Carliner, S. (2012). How to evaluate informal learning. Newsletters published by the Association for Talent Development.
         Article retrieved from http://bit.ly/1tBwXUk.

Center for Creative Leadership. (2020, August 5). The 70-20-10 rule for leadership development.
         https://www.ccl.org/articles/leading-effectively-articles/70-20-10-rule/

Cordova, D. I., & Lepper, M. R. (1996). Intrinsic motivation and the process of learning: Beneficial effects of
        contextualization, personalization, and choice. Journal of Educational Psychology, 88(4), 715.

Dirksen, J. (2015). Design for how people learn. New Riders.

Evans, J. R., Karlsven, M., & Perry, S. B. (2018). Informal Learning. In R. Kimmons (Ed.), The students' guide to learning
        design and research. EdTech Books. Retrieved from https://edtechbooks.org/studentguide/informal_learning

Galanis, N., Mayol, E., Alier, M., & Garcia-Peñalvo, F. J. (2015). Designing an informal learning support framework. In
         Proceedings of the 3rd International Conference on Technological Ecosystems for Enhancing Multiculturality (pp.
         461-466).

Giurgiu, L. (2017). Microlearning an evolving elearning trend. Scientific Bulletin, 22(1), 18-23.

Livingstone, D.W. (1999). Exploring the icebergs of adult learning: Findings of the first Canadian survey of informal
        learning practices. Canadian Journal for the Study of Adult Education. 13,2: 49-72.

Miller, C., Veletsianos, G., & Doering, A. (2008). Curriculum at forty below: A phenomenological inquiry of an
        educator/explorer's experience with adventure learning in the Arctic. Distance Education, 29(3), 253-267.

Peppler, K., Halverson, E., & Kafai, Y. B. (Eds.). (2016). Makeology: Makerspaces as learning environments (Volume 1)
         (Vol. 1). New York, NY: Routledge.

Sefton-Green, J. (2004). Literature review in informal learning with technology outside school. Futurelab, Report 7.
         Available at: www.futurelab.org.uk/research/lit_reviews.htm

Sefton-Green, J. (2012). Learning at not-school: A review of study, theory, and advocacy for education in non-formal
        settings. Cambridge, MA: MIT Press.

                                                                                       411
Simpson, A., Anderson, A., & Maltese, A. V. (2019). Caught on camera: Youth and educators' noticing of and responding
        to failure within making contexts. Journal of Science Education and Technology, 28(5), 480-492.

Steinkuehler, C., & Squire, K. (2014). Videogames and learning. Cambridge Handbook of the Learning Sciences, 377-
         396.

Stokes, P. D. (2005). Creativity from constraints: The psychology of breakthrough. London, UK: Springer Publishing
         Company.

Vygotsky, L. S. (1978). Mind in society: The development of higher psychological processes. Harvard University Press.
Watkins, K. E., Marsick, V. J., & Fernández de Álava, M. (2014). Evaluating informal learning in the workplace. In T.

        Halttenen, M. Koivsto, & S. Billett (Eds.), Promoting, assessing, recognizing and certifying lifelong learning (pp.
         59-77). London, UK: Springer.
Zhang, J., & West, R. E. (2020). Designing microlearning instruction for professional development through a competency
        based approach. TechTrends, 64(2), 310-318.

                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/designing_informal.

                                                                                       412
  33

The Design of Holistic Learning Environments

Jason K. McDonald

One of the factors that makes a design compelling is when it has a sense of harmony and completeness. When we
experience the design, it does not feel like a collection of individual parts that just happen to be together. Instead, they
"fit" together. In fact, we likely do not stop to consider the discrete components making up the design at all. But if we do
notice the individual parts, we typically can sense how each belongs. There is a sense of balance and resonance that
emerges from the precise configuration we experience. We see the design as a whole, meant to be experienced as a
whole. And in the best cases, the sense of completeness and balance somehow extends into us--we feel more
complete and more in balance because we have encountered something as complete and in balance as this design.
Nelson and Stolterman (2012), in their book, The Design Way: Intentional Change in an Unpredictable World, call this
type of experience holistic design. In this chapter, I consider some of the conditions that lead to holistic designs, along
with what these conditions could mean in the context of instructional design.
Figure 1 presents a diagram from The Design Way that highlights the major conditions of holistic design. Discussion of
the entire diagram is more complex than we need to consider here, but if you are interested in the topic, I encourage you
to review Nelson and Stolterman's complete treatment in their book (Nelson & Stolterman, 2012, pp. 93-102).
Figure 1
Dimensions of Emergent Wholes

                                                                                       413
Note. Reprinted from Nelson, H. G., & Stolterman, E. (2012). The design way: Intentional change in an unpredictable
world (2nd ed.). The MIT Press, p. 94. Used by permission; all rights reserved.

The diagram illustrates how we can design objects or services in a way that transcends the individual parts from which
they are assembled to create something holistic. As Nelson and Stolterman put it, we rely on "those unifying forces that
cause things to stand together . . . thus forming meaning for individuals who are part of the whole or served by the
whole" (p. 94). When something is holistic, it has properties that cannot be predicted when we examine each of the
pieces individually. But that does not mean the individual parts are not important. Quite the opposite, in fact. Each
component contributes something to the overall sense of the whole and is necessary to achieve the effect of the whole.
Removing or changing the pieces, then, could lead to a design with a completely different effect.

Holism is not often addressed in instructional design. Perhaps the closest we come is when we consider the graphic
design of our instruction. In this case we do frequently consider what effect the visual components of our instruction
are having, and if they are contributing to an overall pleasing visual sense. (For tips on how to create a pleasing visual
design, see the articles The Building Blocks of Visual Design or 10 Basic Principles of Graphic Design). Holism is
important to consider in other aspects of instruction as well. Yet despite its importance, holistic design can also be
difficult to talk about explicitly. The effects it has are subtle. But using Nelson and Stolterman (2012) as our guide, let's
explore some ways that the instruction you design can inspire a sense of holistic completeness.

Connection

First, consider the effects of the connections between individual elements in your design. There is an analogy here to
connections between physical objects: when joining together two pieces of wood, we connect them with a nail or a
screw. When joining together pieces of metal, we connect them with a weld. We can also consider more sophisticated
methods of connecting when we include the idea of an intermediary fastener. Nails and screws are a direct connector
between two pieces of wood, and the result is a rigid link. But we could connect our wood using an intermediary: a

                                                                                       414
hinge. We screw the hinge into adjacent pieces of wood and the result is a connection between the wood that is more
flexible. When building an object, then, we need to consider what materials we are working with, and this will help us
decide what kinds of connections we can make. Then we consider what we want the connection between the elements
to be, and this will help us further choose an appropriate link to achieve our objective. A holistic design will choose
connections that are both appropriate for the material being used as well as the type of connection that is desired.

There are at least two applications of this analogy to instructional design. The first is between different elements of an
instructional product that students experience. What types of connections are possible between the different pages of
an online educational activity, for instance? Or between elements on the same page? Or between different units of the
same course? The types of connections that are possible will be partly a function of the material the designer is
working with (images, text, web pages, etc.), and partly a function of the effect the designer wants to have (the student
can choose between these three pages; or the student must go to this page, etc.). Attentive designers will consider the
connections between these elements as much as a carpenter will consider the connection between wood beams
supporting the structure they are creating.

Another type of connection instructional designers can consider is between the different layers of their instruction.
Gibbons (2013) proposed that all instructional products or learning systems are composed of different layers that
perform different functions in a design. For example, one of the layers is the representations that students experience
(what they see, hear, touch, etc.). Another layer is the controls that students use to input information back into the
instruction (typing into a text box, submitting a form, or answering a teacher's question). There must be some kind of
connection between these layers for the instruction to have its effect. If instructional designers pay attention to the
effects they want each layer to have, they can find connections between the layers they can intentionally design to help
lead to that effect. Similar to connections between individual elements, designers should both pay attention to the
material each layer is made of (physical or conceptual) as well as the type of connections that are appropriate for the
intended effect.

   Application Exercise

     Find an example of an instructional product or service (perhaps an online training module, a face-to-face
     classroom lesson, or a museum-type experience). Ask yourself:

           What are the individual elements of which the product is composed? (e.g. different pages in the module;
           different activities in the lesson)
           What is connecting those elements together?
           Why do you think they were connected in that way?
           Can you imagine alternative ways of connecting these elements?

Relationships

The second condition of holistic design is the relationships between elements in a design. Relationships are similar to
the idea of connection, since every relationship connects different entities in some way. So everything just described
about connections applies to relationships as well. But relationship implies more than the fact that elements are
connected. The idea of relationship implies there is a structure to the connection, one that suggests an effect that
transcends what the individual elements provide on their own. When two (or more) things are in relationship with each
other, we can see that they belong together. Returning to our previous example of making something out of wood, we
can easily nail together wood of any shape or size. But a relationship between different pieces of wood implies that we
have done more. We also consider how our joint between the pieces fits together harmoniously. We might cut one board
so it fits into an existing grove in the other. Or we apply stain or paint so the coloring of the wood produces a pleasing

                                                                                       415
effect when placed next to each other. We can also consider the relationship of what we build with something larger
than itself. For instance, when we ask whether a chair fits in a room, we usually aren't talking about if we can actually
squeeze it into the space. Rather, what we usually mean is does the chair feel like it belongs? Is the relationship
between the chair and the rest of the furniture harmonious? Or does it feel like the chair came from a different family
than everything else in the room?

Parrish (2005) described some ways that instructional designers can pay attention to the relationship between
elements in their designs. He encouraged designers to pay attention to the "rhythms of instructional activities" in their
products, to find "methods for creating dynamic tension and revealing unity within content sequences," or to develop
"strategies that provide memorable closure to learning experiences" (p. 17). In each of these cases, elements in an
instructional product would not only be connected in some way, but the structure of that connection would produce an
aesthetic effect. This effect transcends the actual material being interacted with in a way that communicates messages
that often cannot be spoken (e.g. why a subject matters, what is beautiful about it, or how might I [the student] be
changed by it).

   Application Exercise

     Using the example you found earlier, try identifying the relationships between elements in the instruction. Ask:

           What are the structures of the connections identified in this instruction?
           What kind of effects do those relational structures suggest?
           Can you imagine alternative relationships that can connect these elements?

Unity

The last factor to consider is unity, or the overall effect the connections and relationships have in a complete design. In
considering unity we should first recognize that there will always be connections and relationships between design
elements. If designers do not consider them intentionally (leaving them to chance), people will look for some kind of
connection, and there is no guarantee the designer will be happy with what they find. When designers do not
intentionally plan for unity between connections/relationships, often this leads to the design being experienced as
disjointed. People may not be able to identify what about it is dissatisfying, but they will sense something about it that
is harsh or jarring. But worse is when the connections and relationships that people find generate a sense of
dissonance or incongruity, an active sense that these elements do not belong together. And more than being slightly
displeased with the design, people actively dislike it, again often without knowing exactly why.

But if connections and relationships are intentionally considered, they can generate an overall, unifying effect that is
pleasing and pleasurable to experience. People feel comfortable with these types of design. Wilson (2013) described
this as "how elements hang together" for the person experiencing it, "and support [them having] a coherent experience"
(p. 40). The word coherent is the key. Unity is a result of everything in the design seeming to belong, to be in its proper
place, and be in that place for a proper reason. Let's assume we designed and built a beautiful, ornate chair, with
intricate patterns in the legs and a soft, luxurious fabric on the back and seat. If we place the chair in an elementary
school cafeteria, it will stick out. Any sense of unity in the room (assuming there was one before!) would be lost. But if
we place our chair in a university library, perhaps in a special collections reading room, it could contribute to a sense of
unity that people experience in the room as being a place of learning.

How does unity apply in instructional design? Parrish (2009) described it as the designer's care for experiences that are
"infused with meaning, and felt as coherent and complete" (p. 511). While there might be multiple ways to do this,
Parrish proposed that designers can pursue unity by intentionally considering connections and relationships between

                                                                                       416
instructional elements that (a) create distinct beginnings, middles, and endings for the instruction; (b) set students in
the role of being the protagonist of their own learning; (c) set a theme for the instruction through the choice of learning
activities; and (d) create a context that immerses students in the instructional situation.

   Application Exercise

     Using the same example as before, consider the sense of unity you experience with the instruction. Ask:
           Do the connections and relationships in the instruction contribute towards an overall effect?
           How would you characterize this effect?
           Is there any evidence to suggest this effect was intentionally considered by designers? Or did they seem to
           leave it to chance?
           What might you change about the instruction to generate a stronger sense of unity?

Conclusion

Nelson and Stolterman (2012) concluded that a holistic design creates emergent qualities, or qualities that cannot be
experienced when only considering the individual elements that are connected together in intentionally considered
relationships of unity. They also stated these emergent qualities have "significance" for the people using a design. They
mean something to people, and "embody [some] essence of human potential more fully" (p. 101). This seems to be
sufficient justification for considering holistic design as part of the instructional design process. Instructional design is
about helping people learn, or, in other words, unlocking some aspect of their human potential. And it is more than the
educational content and instructional strategies that do this. To create designs that are truly remarkable and uncover at
least some aspects of human potential, people need to experience instruction with emergent, holistic qualities. These
are generated as designers consider the connections between individual elements of their instruction, form those
connections into structured relationships, and align both into a unified whole that can produce an aesthetic,
transcendent effect.

References

Gibbons, A. S. (2013). An architectural approach to instructional design. Routledge.
Nelson, H. G., & Stolterman, E. (2012). The design way: Intentional change in an unpredictable world (2nd ed.). The MIT

         Press.
Parrish, P. (2005). Embracing the aesthetics of instructional design. Educational Technology, 45(2), 16-25.
Parrish, P. (2009). Aesthetics principles for instructional design. Educational Technology Research and Development,

        57(4), 511-528.
Wilson, B. G. (2013). A practice-centered approach to instructional design. In J. M. Spector, B. B. Lockee, S. E. Smaldino,

        & M. Herring (Eds.), Learning, problem solving, and mind tools: Essays in honor of David H. Jonassen (pp. 35-
         54). Routledge.

                                                                                       417
This content is provided to you freely by EdTech Books.
Access it online or download it at https://edtechbooks.org/id/the_design_of_holistic.

                                                        418
  34

Measuring Student Learning

Lisa Harris & Marshall G. Jones

Measuring student learning is critical in the teaching and learning processes and can serve many purposes. Instructors
can use assessment results to plan future instruction, adapt current instruction, communicate levels of understanding
to students, and examine the overall effectiveness of instruction and course design. The measurement of student
learning can take place before, during, or after instruction. Before lessons are even developed, instructors need to know
what students already know and can do related to the content. There is no point in wasting time teaching something
students already know, or in starting at a level that is so advanced students don't have the prerequisite knowledge
necessary to be successful. To that end, the learner analysis in instructional design could be considered a type of
assessment. Giving a pre-assessment, also called diagnostic assessment, can provide instructors with this valuable
information. Measuring student learning during instruction, a formative assessment, provides instructors with
important information about how students are progressing towards the learning objectives while there is still time to
adjust instruction. Instructor may ask questions such as:

      Are students getting it?
      Are they confused about something that needs to be retaught?
      Is it time to move on with new material?

Finally, measuring student learning at the end of instruction, a summative assessment, provides information about the
degree to which students mastered the learning objectives.

This chapter outlines practical strategies instructional designers can use to develop high-quality assessments to
measure student learning. Best practices are the same for constructing diagnostic, formative, and summative
assessments. Links to additional tools and resources are also provided.

Constructing High-Quality Assessments

High-quality assessments are those that lead to valid, reliable and fair assessment results. Validity refers to the
trustworthiness of the assessment results. For instance, if a student gets 80% of test items correct, does that mean
they understand 80% of the material taught? Does the assessment measure what it purports to measure, or is the final
score polluted by other factors? For example, consider a test that assesses mathematical ability and is made up of
word problems. When taken by an English language learner or by an emerging reader, does the test assess math,
reading, or a combination of both? The reliability of an assessment refers to the consistency of the measure. Multiple-
choice test items, when properly constructed, are highly reliable. There should be only one correct answer and it is easy
to grade. Essay items or performance assessments, on the other hand, are more subjective to grade. Finally, the extent
to which an assessment is fair is a characteristic of a high-quality assessment. Fairness is the degree to which an
assessment provides all learners an equal opportunity to learn and demonstrate achievement. While some aspects of
validity and reliability can be measured through statistical analysis, it is uncommon that such complex measurement
procedures are used for typical classroom assessments. Attending to best practices in assessment alignment and test

                                                                                       419
item and assessment construction helps instructional designers increase the validity, reliability, and fairness of
assessment instruments.

Assessment Alignment

One of the most important concepts in assessment is alignment. It is critical that assessments and assessment items
are aligned with goals and objectives. It is impossible to determine the extent to which learners have met course or
workshop goals and objectives if their knowledge and skills have not been assessed. Assessment alignment tables and
test blueprints are two tools instructional designers can use to align assessments and assessment items with learning
objectives.

Learning Taxonomies and Learning Objectives

Learning taxonomies assists instructional designers in constructing both learning objectives and assessment items.
Bloom's Revised Taxonomy and Webb's Depth of Knowledge (DOK) are two frameworks commonly used by educators
to categorize the academic rigor of an assessment as a whole or individual assessment items. To increase the content
validity of an assessment, the complexity of the individual test questions should align with the level of knowledge or
skill specified in the learning goal. If a learning objective states that a student compares and contrasts information, it is
not appropriate for test items to simply ask students to recall information. Likewise, if the learning goal states that
students will be able to synthesize information, a paper-and-pencil test will likely not be a sufficient measure of that
skill.

Bloom's Revised Taxonomy divides learning into three domains: cognitive, affective, and psychomotor (Anderson et al.,
2001). This chapter focuses on the cognitive domain which consists of six levels that vary in complexity. The three
lower levels (remembering, understanding, and applying) are referred to as lower order thinking skills also called LOTS.
The top three (analyzing, evaluating, and creating) are referred to as higher order thinking skills, or HOTS. Lists of verbs
associated with each of these levels are readily available on the web and are very instrumental in helping instructional
designers write measurable learning objectives and test questions that go beyond recalling definitions. (For an example,
see: https://edtechbooks.org/-EZbp.)

Similar to Bloom, Webb divides levels of knowledge into increasingly complex categories. These include recall and
reproduction, skills and concepts, strategic thinking, and extended thinking (Webb, 1999). Student tasks range from a
student being able to recall facts to synthesizing information from a variety of sources. A description of tasks at each
level can be found online at https://edtechbooks.org/-bVW. These descriptions can help instructional designers design
assessment tasks that range in complexity.

Assessment Alignment Tables

Regardless of the assessment method, instructional designers can ensure that learning goals, objectives, and
assessments align by creating an alignment table. In the example below, course goals, student learning outcomes, and
assessments are aligned in a table. This example is from a college level course on teaching with technology for pre-
service teachers. This table indicates there is at least one learning objective aligned with each course goal and at least
one assessment method aligned with each objective. If you find that a particular learning objective isn't being assessed,
you can go back and develop an assessment to measure the learner's progress. A link to an Assessment Alignment
Table Template is provided at the end of this chapter in the Additional Resources list.

Table 1

Example Assessment Alignment Table

Course Goal  Student Learning Objective (SLO)  Assessment(s)

             420
Plan and implement meaningful learning       SLO1. Develop a technology integrated activity       Technology
opportunities that engage learners in the    plan that meets the needs of diverse learners        Integration
appropriate use of technology to meet        (e.g. ELL, at-risk, gifted, learners with learning   Portfolio
learning outcomes.                           disabilities).

Use technology to implement Universal        SLO2. Explain how and why to use technology          Technology
Design for Learning.                         to meets the needs of diverse learners (e.g.         Integration
                                             ELL, at-risk, gifted, students with learning         Portfolio
                                             disabilities).
                                                                                                  Midterm

                                             SLO3. Describe the elements of UDL included          Technology
                                             in the technology integrated activity.               Integration
                                                                                                  Portfolio

Model and require safe, legal, ethical, and  SLO4. Describe legal, ethical, cultural, and         Midterm
appropriate use of digital information and   societal issues related to technology.               Final
technology.

Table of Specifications

In addition to creating an alignment table for all assessments in the entire course, instructional designers can also
create a table of specifications, or test blueprint, to align individual test items to course objectives. A table of
specifications aligns the learning objective, all items on a single test, and the level of knowledge being assessed. This is
evidence of content validity. This also helps the instructional designer see if the test includes items related to all the
learning goals, and if the assessment items are written to elicit knowledge at the appropriate level of complexity. If you
find that you have too many questions about one topic or not enough about another, or that you are only asking lower
level questions when the learning objective is focused on higher order thinking skills, the test can be edited accordingly.
The figure below shows a test blueprint for a 12-item test about assessment. Each number represents the question
number on the test. A link to a Table of Specifications Template is provided at the end of this chapter in the Additional
Resources list.

Table 2

Sample Test Blueprint for a 12 Item Test

Learning objective                                                                         Level of Knowledge

                                                                                           Lower  Higher
                                                                                           Order  Order

Analyze learning objectives in terms of format, specificity, reasonableness, and           1, 2   8, 12

alignment.

                                             421
Explain the importance of alignment when designing lessons and assessments.         3, 5     10
Compare and contrast reliability and validity of classroom assessment               4, 6, 7  11, 9

Assessment Formats

Common assessment formats include multiple-choice and essay questions, observation, oral-questioning, and
performance-based assessments. This chapter focuses on paper-and-pencil tests and performance assessments. Best
practices in constructing each are described below. These guidelines help increase the validity, reliability, and fairness
of assessments.

Multiple-Choice Best Practice Guidelines

Multiple-choice items are very easy to grade (assuming there is only one correct answer) but very difficult to write.
Coming up with plausible distractors, or the incorrect responses, is the hardest part. If some answer choices aren't
plausible (ones that are meant to be funny, for example), the probability that a student will be able to guess the correct
answer increases. It is also difficult, but not impossible, to write multiple-choice questions that assess higher-order
thinking skills. Tips for constructing multiple-choice test questions that assess HOTS are provided below.

  1. All answer choices should be similar in length and grammatically correct in relation to the item stem.
  2. Avoid "all of the above", and "none of the above" answer choices.
  3. Avoid confusing combinations of answer choices such as "A and B"; "B and C"; "A, B and C but not D".
  4. Avoid negatively stated stems. If you must use them, bold the negative word to make it what you are asking clearer

      to the learner.
  5. Avoid overlapping answer choices. (This most commonly occurs with number choices.)
  6. The item stem should make sense on its own and not contain any extraneous information.
  7. Don't include any clues in the item stem that would give the answer away.
  8. Don't include too many answer choices. Typically, multiple choice questions contain four options.
  9. Ensure the correct answer is the best answer.
 10. Randomize the order of the correct answers.

Table 3

Examples of Poor and Improved Items

Poor Item                              Improved Item                   Explanation

If a boy is swimming two miles an      A boy is swimming two           The poor item contains extraneous information
hour down a river that is polluted     miles per hour down a river     and a confusing sentence structure. In the
and contains no fish and the river is  relative to the water. The      improved item, the extraneous information was
flowing at the rate of three miles     water is flowing at the rate    removed. In addition, the prompt was broken up
per hour in the same direction as      of three miles per hour. How    into several sentences and the actual question
the boy is swimming, how far will      far will the boy travel in two  stands on its own.
the boy travel in two hours?           hours?

a. four miles                          a. four miles

b. six miles                           b. six miles

c. ten miles                           c. ten miles

                                                      422
d. twelve miles                          d. twelve miles

Which one of the following is not a      All of the following are safe  When reading the poor item, a test taker may
safe driving practice on icy roads?      driving practices on icy       not recognize that they are being asked to pick a
a. accelerating slowly                   roads EXCEPT                   non-example of a safe driving practice. In the
b. jamming on the brakes                                                improved item, the word "except" is in all caps
c. holding the wheel firmly              a. accelerating slowly.        and underlined to call attention to what is being
d. slowing down gradually                                               asked.
                                         b. jamming on the brakes.

                                         c. holding the wheel firmly.

                                         d. slowing down gradually.

In most commercial publishing of a       In publishing a book, galley   In the poor item, each answer choice is not
book, galley proofs are most often       proofs are most often used     grammatically correct in relation to the item
used _________ .                         to                             stem. Often, a test taker can pick out the correct
                                                                        answer choice because it is the only one that is
  1. page proofs precede galley            1. aid in minor editing      grammatically correct and not because they
      proofs for minor editing.                after page proofs.       actually knew the answer. In the improved item,
                                                                        the item stem and answer choices have been
  2. to help isolate minor defects         2. isolate minor defects     edited so that they are all grammatically correct.
      prior to printing of page proofs.        prior to page proofs.

  3. they can be useful for major          3. assist in major editing
      editing or rewriting.                    or rewriting.

  4. publishers decide whether             4. validate menus on large
      book is worth publishing.                ships.

   Tips for Writing Higher Order Thinking Multiple-Choice Questions

     Tip 1: Use scenarios or provide examples that are new to learners. This allows you to ask learners to do more
     than simply recognize the correct answer. (Note that this can be problematic if you are assessing struggling
     readers or ESL learners. Know your audience!)

     Tip 2: Develop multiple-choice questions around a stimulus you provide such as a map, graph, diagram, or
     reading passage. These are called interpretive exercises. Interpretive exercises include a set of data or
     information and a series of multiple-choice questions having answers that are dependent upon the information
     given.

Best Practice Guidelines for Writing Essay Items

Essay questions are a good way to assess deep understanding and reasoning skills. Students can provide more in-
depth answers in essay questions. Essay questions are also much easier to write than multiple-choice items. They are,
however, harder to grade. Below are best practice guidelines for constructing and grading essay items and some real-
world examples.

                                                          423
      Select the most important content in the workshop or unit to assess with essay times. Using essay items limits the
      amount of content you can cover on any one test because they take more time for a learner to answer. If one topic
      is less important than another, consider only asking multiple-choice questions about it.
      Write the prompt to focus learners on the key ideas they should address in their response. For example, tell learners
      how many reasons should they give, or how many examples should they provide. Stating directly what you want
      means that the learner doesn't have to try to interpret how much is enough.
      Break multi-faceted questions up into individual items. If the question is very long, make it more than one essay
      question on the test. This helps focus both the test taker and the grader.
      Include scoring criteria with the prompt and assign appropriate point values. If you want someone to provide three
      reasons why the Renaissance began in Italy, decide how many points each reason should count and make that
      clear to the learner. It is very difficult to objectively grade an essay question worth 10 or 20 points without first
      determining the grading criteria.
      Only include essay items that require higher-order thinking. Essay questions are too time consuming to grade. If it
      can be assessed with a multiple-choice question instead, don't waste valuable time reading essay answers.
      Avoid allowing learners to select which essay items they answer. This keeps learner scores comparable. If learners
      can choose which essay questions to answer, the test is not assessing the same thing for all students.

Note: Essay items can also be assessed with rubrics. See Performance Assessments and Rubric Development for more
information on how to construct a rubric.

Essay Item Examples

Below are examples of high- and low-quality essay items. Note that the high quality examples include explicit
instructions about what needs to be included in the answer. In addition, how the points will be allocated is clear. The low
quality essay items are both very broad in scope. A test taker could easily answer the question without touching on any
of the topics the instructor wanted them to include in their answer. In addition, it isn't clear to the test taker or the
instructor how the points are allocated. This can lead to inconsistencies in grading.

High-Quality Examples

  1. Proof 1: Given ABC is equilateral, and BD is the angle bisector of angle ABC. Prove that the measure of angle ADB
      and angle CDB is equal to 90 degrees. Provide the statement and reason for each step using the two-column proof
      format. (1/2 point for each correct statement and 1/2 point for each correct reason given. 8 total points.)

  2. Compare and contrast large-scale assessment and classroom assessment on the dimensions of frequency and
      nature of feedback. (2 points frequency, 2 points feedback. 4 total points)

Low-Quality Examples

  1. Explain weather and climate. (20 points)
  2. Describe the three principles of Universal Design for Learning. Do you believe they should be used to guide

      instruction? Why or why not? (10 points)

Best Practice Guidelines in Developing Performance-Based Assessments

Performance-based assessment allows learners to apply knowledge and skills in authentic situations. Performance-
based assessment results in the creation of a performance or a product. Performance examples include public
speaking, inventing something to solve a problem, putting on a play, or playing in a basketball game. Public service

                                                                                       424
announcements, digital videos, and infographics created by learners are examples of products. Consider the following
guidelines when constructing performance assessments:

  1. Design a task that applies to real-world situations. The more authentic a performance-based assessment can be
      the more meaningful it will be to the learner, although access to resources and time will certainly impose project
      limitations. For example, writing a paper on gardening, designing a garden, and creating a garden are all examples
      of performance tasks with varying degrees of authenticity.

  2. Develop a task description that includes the following:
        a. Purpose/learning objectives. Why are the learners completing this task? Write the learning objectives in learner
            friendly language.
        b. Clear directions. Break down the task into its component parts. Don't assume learners know how to jump
            immediately into creating the final performance or product.
        c. Perimeters and constraints. How much time do the learners have to complete the project? What resources are
            they allowed to use? Is it a group or individual project? Who are they allowed to ask for help?
        d. Assessment criteria. How will the performance or product be graded? This is discussed in more detail below in
            the Rubrics section.

  3. Develop any job aides learners will need in order to complete the task. Do you need to teach any additional skills
      such as how to locate articles in a database, how to measure volume, or how to use a particular piece of software?

  4. If at all possible, provide learners with an example.

Rubrics

As discussed earlier in the chapter, reliability is related to scoring consistency. One way to help ensure scoring
consistency is to use rubrics for grading subjective assessment items, including essay questions and performance
assessments. Rubrics focus the attention of a grader on what is most important about the assignment. Rubrics include
topics or elements and descriptions of levels of performance. This provides a roadmap for how to assess an
assignment that is more subjective than a multiple-choice question. Without a rubric, it is easy for a grader to grade for
one thing for the first 10 papers and grade for something else the last 10 papers. This occurs when an instructor has a
lot of papers to grade, grading takes place over several days, and if more than one instructor is grading the same
assignment. Providing a rubric up front is also beneficial to the student. They communicate to the student from the
beginning what is important, on what to focus, and where to spend time and energy.
There are three types of rubrics: holistic, analytic, and single-point. This section will focus on analytic rubrics, because
they allow instructors to assess the component parts of the performance assessment individually and provide the
clearest grading criteria. Several additional resources about the different types of rubrics are provided below.
An analytic rubric consists of criteria, levels of performance, and descriptors.
Figure 1
Example of an Analytic Rubric

                                                                                       425
Best Practice Guidelines for Creating Rubrics

  1. Determine the criteria. Criteria can be written as a learning objective or category. Criteria should be measurable,
      important to the performance task, and taught. For example, creativity is often assessed in performance-based
      assessments. If creativity was not explicitly taught, it shouldn't be measured.

  2. Determine the weight of each criteria. Will they all be worth the same amount of points or will some count for more
      than others?

  3. Determine the number of performance levels. How many levels of the rating scale will be delineated on the rubric?
      Will they be numbers such as 4, 3, 2, 1 or descriptive such as developing, meets expectations, and exceeds
      expectations. Typically, analytic rubrics contain three to five performance levels.

  4. Write descriptors for each of the performance levels. This is the hardest part! Descriptors should address the
      quality of the product. It is okay to count project elements for some of your criteria (i.e. number of references,
      number of graphs), but not for all of them. See examples of quality and numerical descriptors below.

Numerical Descriptors vs Quality Descriptors Example

Table 4
Numerical Descriptors in an Annotated Bibliography Rubric

                4                      3                        2                     1

Quality /       All sources cited are  At least 80% of sources  At least 50% of       Less than 50% of
Reliability of  reliable and           cited are reliable and   sources are reliable  sources cited are
Sources         trustworthy.           trustworthy.             and trustworthy.      reliable and trustworthy.

                                          426
5 points                                 4-3 points            2 points                     0-1 point

Table 5
Quality Descriptors in a Technology Lesson Plan Rubric

                               Exceeds Expectations (A)        Meets Expectations (B to C)    Below Expectations
                                                                                              (C- and below)

Teacher candidate develops a   Activity promotes significant   Activity promotes creatively,  Activity focuses on
learner-centered,              learner engagement through      collaboration, or              teacher-use of
technology-integrated          creativity, collaboration, and  communication and focuses      technology but lacks
activity that promotes         communication. Actively         on learner engagement with     opportunities for
creativity, collaboration, or  includes opportunity for        technology. Actively includes  learner engagement
communication, and results     learner to create a product.    opportunity for learner to     and/or product
in a learner-created product.                                  create a product.              creation

                               5 points                        2-4 points                     1 point

Note also that the rubric element directly above is written as a learning objective rather than simply a category.

Conclusion

Aligning test items and performance assessments to learning objectives, using best practice guidelines to create
assessments, and using rubrics to grade complex tasks, are strategies instructional designers can use to develop high-
quality assessments. High-quality assessments provide instructors with accurate information regarding the extent to
which learners met the learning objectives, a critical component of the teaching and learning process. Accurate
assessment results help instructional designers plan future instruction, adapt current instruction, communicate levels
of understanding to students, and examine the overall effectiveness of instruction and course design.

References

Anderson, L.W. (Ed.), Krathwohl, D.R. (Ed.), Airasian, P.W., Cruikshank, K.A., Mayer, R.E., Pintrich, P.R., Raths, J., &
         Wittrock, M.C. (2001). A taxonomy for learning, teaching, and assessing: A revision of Bloom's Taxonomy of
         Educational Objectives (Complete edition). New York: Longman.

Webb, N. (1999). Alignment of science and mathematics standards and assessments in four states (Research
         Monograph No. 18). Washington, DC: CCSSO.

Additional Readings and Resources

      Assessment Alignment Table Template (also included at the end of this chapter)
      Making Data Driven Decisions using Assessment Data
      Single-point, Analytic, and Holistic Rubrics
      Rubric Wordsmith
      Table of Specifications Template (also included at the end of this chapter)

                                                        427
Templates

Assessment Alignment Tables

Use this template to align your course goals, student learning objectives and assessments. This table helps
instructional designers ensure that they have assessed all course goals and objectives. Delete the sample goals,
objectives, and assessments in the blue font below and insert your own.

Table 6

Example Alignment Table

Course Goal                                  Student Learning Objective (SLO)                    Assessment(s)

Plan and implement meaningful learning       SLO1. Develop a technology integrated activity      Technology
opportunities that engage learners in the    plan that meets the needs of diverse learners       Integration
appropriate use of technology to meet        (e.g. ELL, at-risk, gifted, learners with learning  Portfolio
learning outcomes.                           disabilities).

                                             SLO2. Explain how and why to use technology         Technology
                                             to meets the needs of diverse learners (e.g.        Integration
                                             ELL, at-risk, gifted, students with learning        Portfolio
                                             disabilities).
                                                                                                 Midterm

Use technology to implement Universal        SLO3. Describe the elements of UDL included         Technology
Design for Learning.                         in the technology integrated activity.              Integration
                                                                                                 Portfolio

Model and require safe, legal, ethical, and  SLO4. Describe legal, ethical, cultural, and        Midterm
appropriate use of digital information and   societal issues related to technology.              Final
technology.

Table of Specifications Template

Use this template to create a table of specifications for a test. This table helps you align the learning objective, all items
on a single test, and the level of knowledge being assessed. Using this table helps you ensure that each item on the test
is related to the learning goals and that you are asking higher order questions about the topics. Add rows to this table
as necessary.

Table 7

Test Blueprint Template

Learning objective     Level of Knowledge

                       Lower Order Test Question #s  Higher Order Test Question #s

Learning Objective #1

                                             428
Learning Objective #2
Learning Objective #3
Learning Objective #4
Learning Objective #5

                          This content is provided to you freely by EdTech Books.
                          Access it online or download it at https://edtechbooks.org/id/measuring_student_learning.

                                                                                      429
430
Design Relationships

Instructional design often takes place in team contexts. Designers can draw upon a rigorous knowledge base to
develop or maintain the relationships important for these teams. This both improves their effectiveness in their current
project settings, as well as encourages long-term associations that can provide them new opportunities in the future.

     Working With Stakeholders and Clients
     Leading Project Teams
     Implementation and Instructional Design

                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/design_relationships.

                                                                                       431
432
  35

Working With Stakeholders and Clients

Lee Tran, Kathy Sindt, Rudy Rico, & Benjamin Kohntopp

Throughout your experiences as an instructional designer, you may form many different relationships with your
colleagues. However, one of your most important relationships will be the one you have with your stakeholders or
clients. It is important to recognize that the relationship with your stakeholders or clients is not solely based on
transaction, but is also one of collaboration. In any instructional development process, there will be many different roles
that each collaborator plays, as each brings a different set of expertise.

Remember, as an instructional designer, the communication style you choose to use will involve feedback from both
parties. Your stakeholders or clients are looking to you for guidance in instructional design and content delivery.
However, part of your work will be reliant on the content that your stakeholders or clients are giving you.

As instructional designers, we want to build trust with those we work with to better collaborate and deliver an end-
product that meets the goals of a project. By building a stakeholder or client relationship, we can better understand who
our target audience is, the project needs, and what our learning outcomes are.

For example, let's say you received a set of instructional materials on how to make toast. The instructional material
provided may be simple to follow, but there may be details missing needed for you to start your work, such as knowing
if your target audience has access to a toaster. This detail would be important in your design to ensure that learners
have access to all of the materials needed to successfully complete the course. As we continue this chapter, think
about what kind of details you would need to start a course development and write them down.

Throughout this chapter, we will be looking at different aspects of the client relationship, including the process,
guidance and communication, scope of work, collaborative workspaces, challenges, ethical concerns, and reviewing
content.

The Process

Every instructional design project should follow an instructional design model. The most familiar model is the ADDIE
model. This model includes the following components: Analyze, Design, Develop, Implement, and Evaluate (Kurt, 2017).

Another popular model and the one we use at Colorado Community Colleges Online (CCCOnline) is the backward
design model (Wiggins and McTighe, 1998). In this model, the focus is on the result of the instruction, while also asking
what the students should be able to understand and do after the instruction has been provided. All instruction, learning
activities, and assessments direct the students toward achieving the result.

Whether you are following the ADDIE model, the backward design model, or another process to design your instruction,
it is important that your stakeholders understand your process and the reasons you are using that process. It's also
important to be sure your stakeholders know what you expect of them as part of the process. Getting buy-in on your

                                                                                       433
process at the start will eliminate problems later. If the stakeholders understand what you expect, and the reasons for
the expectations, they are better equipped to follow your procedures and processes.

There are different ways to ensure your stakeholders understand your process. One excellent option is to have initial
meetings with all the stakeholders where you provide the stakeholders with information about the process and your
expectations of each of them.

At CCCOnline, all of our stakeholders are required to take an orientation course that describes our processes and
expectations. Once all stakeholders have completed the orientation, an initial vision meeting is held to discuss the
scope of the project, to clarify the expectations from the stakeholder perspective, and to establish the duties and roles
of all members of the team. After the vision meeting is completed, a kickoff meeting is held a couple of weeks later to
review and finalize the project outline and scope, to set the timeline for the project, including deliverable due dates,
review dates, and the final project deliverable due dates. The kickoff meeting is the beginning of the design phase of the
project.

Guidance and Communication

Setting Communication Standards

As you work with your clients in your course development, you will want to ensure there is a standard of communication
in place. A communication standard may include preferred methods of communication, frequency, and availability. By
setting communication standards, you and your client can follow the expectations of each party in the development and
promote a steady workflow. Remember that even though you are the instructional designer, your client is very much
your partner throughout the development to ensure content validity and that the end product meets the needs of the
target audience.

Becoming a Learning Coach

Your client will be looking at you for guidance in your expertise in instructional design. This expertise makes you what
we will be calling a Learning Coach. A client may be an expert in their particular field, but may not have the same
expertise with learning theories and applications to deliver their content to a mass audience. By understanding your role
as not only the instructional designer, but as a Learning Coach to your clients, you are there to help guide your clients in
their instruction development journey. Some clients may come to you with anxieties or questions like, "How do we
engage the audience within different learning environments?" or "How do we measure the appropriate outcomes?". Your
coaching is meant to put your client at ease. As you coach your client through their concerns, you may notice your client
becoming more confident in what your instructional product will be and in turn providing content that is better suited for
the learning environment. This mutual understanding can ensure success.

Flexibility

Always remember that your client is human. Much like you, certain circumstances in their lives may affect the delivery
of content. We want to ensure that the proper expectations are set in place, but also be flexible enough to understand
that certain circumstances may get in the way. By being flexible and empathetic, you ensure that neither you nor your
clients lose motivation or energy throughout the development process.

Scope of Work

When beginning work on an instructional design project, it is important to ensure that all the stakeholders agree on the
scope of work (SOW) for the project. The project scope determines the goals/objectives, deliverables, and deadlines of
the project.

                                                                                       434
At the start of any project, define the goals and objectives so you understand what the stakeholders are expecting. We
have included some templates that can aid in defining your goals and setting the scope of your project. These include a
PreMeeting and vision Meeting Guide, a Vision Scope Template, a Kickoff Call script, and a Course Map (outline of the
project).

In addition to the goals and objectives, determine what deliverables you will provide as part of the project. Will you be
creating a large, full-scale curriculum project, with multiple courses, or are you developing a single course? You need to
know what kinds of media you will be developing. Are you expected to create video or interactive content, or will you be
developing more static content? If you are developing any multimedia, be sure to determine the length/amount of this
content before beginning. The more multimedia and interactive content you will be developing, the more resources your
project will take. You need to be in agreement with your stakeholders on all aspects related to the scope of the work
before the start of the project.

Finally, you need to determine the timeline of the project. Decide upfront when each deliverable is due, how long the
stakeholders have to review the content, and how long you will need to make any revisions requested by the
stakeholders. Agreement on these issues avoids conflict later in the process.

In addition to having the scope clearly defined at the start of the project, it is important that you and the stakeholders
have clearly defined expectations of all members of the team. Are the stakeholders expected to write content? Are they
expected to review content, and if so, at what stages of the project? Some stakeholders may only be directly involved at
the beginning and end of a design project, while others may be involved during the entire process. Be sure that each
stakeholder, including you, understands the expectations of them during the development process.

A major reason for clearly defining the scope of the work and your expectations of the stakeholders is to help eliminate
scope creep. Scope creep occurs when a part of the project takes longer or more work than originally determined. This
usually happens when one of the stakeholders expects or asks for additional work beyond the original agreement or
statement of work. The best way to avoid scope creep is to have clearly defined and agreed upon scope and
expectations before the project starts.

Setting up a Collaborative Workspace

In this section, we will focus on collaborating with your design team and setting up a workspace that allows each
member to contribute. Depending on your situation, a collaborative workspace can include both physical and virtual
spaces. Setting up a collaborative workspace is key to ensuring that all stakeholders can contribute during the design
process and questions about content can be addressed before developing course materials.

The first step to consider when setting up a collaborative workspace is the types of materials that will be delivered. If
the instructor or subject matter expert you are working with is delivering large files, such as MP4 video files or large text
files, then a cloud-based file hosting service like Dropbox, Microsoft's One Drive, or Google's G Drive may be a solution.
File hosting services allow the user to upload large files and share the uploaded content with members in your
organization.

Once you agree on a file hosting service, set up a folder, and share the folder with the stakeholders who will be
delivering content. Make sure you provide the right type of access so that the stakeholders have permission to edit and
add content.

In addition to setting up a file-sharing collaborative workspace, you should set a schedule for delivering content, and
schedule regular meetings to check in with your stakeholders. Having a regular meeting scheduled can help prevent any
communication issues or identify issues that come up as content is delivered.

                                                                                       435
   Collaboration Tools

   Tools for Meeting With Stakeholders

     Web conferencing software - ex. Zoom, Skype

   Tools for Project Planning

     Spreadsheets, Shared Calendars - ex. MS Excel, Google Calendar

   Tools for Content Delivery

     Cloud-based services - ex. Dropbox, MS One Drive

Depending on your institution, a face-to-face meeting can be held at the start of the project and then transition to online
meetings or conference calls. Meeting with all your stakeholders face-to-face at the beginning of course development
can help determine which members of the development team are essential to future meetings and which content to
assign for development to each member.

Challenges

Communication with stakeholders, as stated in our section on setting up a collaboration space, is key to ensuring
completion of the course development on deadline. One common issue that occurs when developing online courses is
lack of communication leading to confusion on how content is delivered, when content is to be delivered, and how
content is reviewed for quality. For example, while working on a teacher education course last summer, I encountered
an issue with the subject matter expert's schedule. At the initial meeting, the subject matter expert indicated she was
familiar with the content from previously teaching the course and would have no issues making the content updates.
However, the subject matter expert also indicated during the meeting that she would be on vacation abroad and would
not be able to deliver content until after she returned. Since the subject matter expert indicated she was familiar with
the content as an instructor, I recommended that she complete an initial review and submission of new content for the
course's first two modules prior to going on vacation. Knowing that the subject matter expert would be unavailable
during the first phase of development prompted me to update the content delivery schedule. Therefore, setting up
expectations early on is essential to catching possible scheduling conflicts and avoiding confusion later in the content
delivery stage of course development.

To avoid communication issues, also speak with your stakeholders regularly. We emphasize "speak," because long
emails can lead to more confusion. Email communication is good for quick updates, but long emails chains can be
more time consuming than simply talking on the phone for 5 minutes to clarify an issue. Therefore, set up a regular
meeting time each week and check in with your stakeholders often by phone or web conference. After all the
stakeholders are comfortable with the development process, you can hold meetings less frequently, but at the
beginning stages of development avoid going more than a week between meetings.

Not communicating expectations early on with all of your stakeholders can lead to missed deadlines and content
delivery falling behind schedule. Therefore, make deadlines clear and use a project plan to keep track of all the major
milestones during the content delivery phase. If a deadline is missed, communicate with your stakeholders immediately
and identify the issue that caused the delay. However, sometimes the stakeholder in charge of delivering the content
may have fallen behind and need additional support to create the content. Courses that incorporate Open Educational
Resources (OER) may be more challenging to develop content for and, therefore, may require more time. This is due to
the "open" nature of OER content. While there are many free resources available to educators, not all OER content is high
quality, or accessible.

                                                                                       436
Technical issues may also prevent the delivery of content; checking with your stakeholders when they miss deadlines
can help identify if it is a technology issue or a content issue. Depending on the file-sharing system you selected, there
may be issues updating content in the online workspace, and you may need to coach your stakeholders as to how to
properly upload and share content with the design team.

When content is not delivered, and several deadlines are missed, set up a meeting with the key stakeholders, and
develop a plan to get content delivery back on schedule. For this reason, it is often a good idea to set up a buffer
between the end of content delivery and the start of the course launch. I typically set an early content delivery date of
about 3 weeks before content is due for review.

Ethical Concerns

On some days during your course development cycle, you may feel like teacher dealing with a student. You know that
the student is very skilled, but at times they may need your guidance. This is especially true when it comes to Ethical
Concerns that might arise during the course development process. While a subject matter expert (SME) is exactly that,
an expert in their chosen subject, they aren't expected to know everything. This means that, regardless of the type of
development (OER or otherwise), your subject matter expert will be looking for outside sources to supplement their
material.

Plagiarism

Although some might think of plagiarism as a concern reserved for students, it is a reality for the individuals creating
the courses as well. Any time a subject matter expert looks for material, they run the risk of plagiarizing content. In
most environments, this is very problematic. Many places will take ownership of a SME's work upon its completion;
therefore, having plagiarized or stolen content can cause problems for that institution or place of business. Here a few
strategies you can use when working with your SME:

  1. When the content first comes in, be sure to read it thoroughly. Reading your content is the simplest way to tell if a
      SME has been plagiarizing. You should have a feeling for how a SME writes by now, from emails to course maps, so
      if anything in their content seems suspect to you, it might be time to raise a red flag and ask them about it,
      especially if they are missing citations for their material.

  2. If you're able, run the content through a plagiarism checker, such as Turnitin, Quetext, or Prepostseo (those last two
      are free). Keep in mind that while the plagiarism checker will give you a better idea of where an SME's content
      came from, it doesn't necessarily mean plagiarism has taken place.

  3. As you read over the content and suspect plagiarism in a particular passage, highlight it, and paste the suspected
      content into a Google search. Believe it or not, the search results that come back may be bolded portions of a
      website where the content is from. If it is, you need to discuss this with your SME.

Catching plagiarism early is vital. SMEs may not be aware that what they're doing is plagiarism and may continue to do
it throughout the process. It might be helpful to discuss Creative Commons licenses with them to elucidate what they
can and cannot do. Reading through a basic overview of the licenses (https://edtechbooks.org/-JMt) might save you
from future issues.

Conflict

Unfortunately, sometimes, conflicts between you and your SME arise. Remember that during development,
communication is key. More often than not, SMEs are happy to dispense their knowledge, but they also must be heard.
They are not a tool to be used and discarded. Keep this in mind to save your developments from falling apart. Here is
one example:

During the development of a course, the SME, who was writing an entire OER eBook, decided that she wanted links to
the eBook placed in every page of the course so students could readily access it everywhere. While I immediately

                                                                                       437
disagreed with her, I allowed her to finish her reasoning. Once she concluded, I explained that from a design perspective,
this could cause confusion for students, regardless of her good intentions. I told her I appreciated her input and told her
that if she disagreed, we could have a meeting involving the dean (her boss) and we could talk things out with him. She
decided to do so, we talked it out and we came to an agreement: the links to the eBook would be placed in only the
most relevant and useful places. We both walked away from the conflict satisfied with our agreement.

Reviewing the Content

Quality Assurance

We all want our students to have the best quality courses. One of the most important components of a course
development comes during the quality assurance (QA) check. Whether you as the ID do it alone or you're lucky to have
someone there to help you with it, quality assurance is paramount. CCCOnline implements QA via a two-fold approach:
we have a designated QA person checking the course throughout the entirety of the build. When the content first comes
in, they go over all the essential components thoroughly and write feedback and recommendations. Then, once the
course is in place in the LMS, they review it again. Throughout the entire process, the QA person is viewing the course
as a student would and ensuring everything makes sense. As Instructional Designers, we should never forget the end
user: our students.

Approval

Not only should a QA person sign off on the content, but the SME and the program leader(s) should also have a say in
approving the content. Essentially, when the content is in, and before it is placed in the LMS all parties should have their
voices heard:

  1. The QA person should be viewing the course from the student's perspective, giving valuable insight that might go
      unnoticed otherwise.

  2. The SME and program leader(s) should have the best understanding of the content and should, therefore, ensure
      the course aligns with all objectives and hits all of its necessary deliverables.

  3. As the ID, you must do some of both: ensure the content aligns with the outcomes that have been set and ensure
      the course will make sense from a student perspective.

Conclusion

In this chapter we discussed some strategies for collaborating with various stakeholders during a course development
and provided some recommendations for solving some common issues instructional designers encounter during the
collaboration phase. As the instructional designer, having a well-developed project plan, that includes deadlines for
content delivery and dates for meetings with stakeholders, is essential for a successful course development. Therefore,
when developing your project plan remember some of the issues we presented here and the types of challenges your
stakeholders may encounter during content delivery. What can you do as the instructional designer to help your
stakeholders meet the deadlines? Consider the following:

  1. What type of content are your stakeholders expected to deliver?
  2. When is the course expected to launch? Consider potential time constraints for stakeholders.
  3. How much time will the quality assurance process take?
  4. Will stakeholders be asked to review content on multiple occasions? How will reviews and feedback be managed?

                                                                                       438
   Templates

           Kickoff Call Script
           Pre-Meeting and Vision Meeting Guide
           Vision Scope Template
           Course Map (.xlsx)

References

Kurt, S. (2017). ADDIE model: Instructional design. Retrieved from https://edtechbooks.org/-vDu
Mochal, T. (n.d). Defining project goals and objectives. Retrieved from https://edtechbooks.org/-sne.
Wiggins, G., & McTighe, J. (1998). Understanding by design. Alexandria, VA: Association for Supervision and Curriculum

         Development.

                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/working_with_stakeholders.

                                                                                       439
440
  36

Leading Project Teams

Ashley Smith

My palms were sweaty. The conference room was warm. Seated directly across the table was the Vice President of
Asset Protection of Walmart US. The company had flown me from Idaho to Bentonville, Arkansas, for this moment, my
first big break, and my chance to make a name for myself. I was leading a team tasked with developing a curriculum
that would be implemented everywhere in the United States, and I was ready--ready to spring into action and dazzle the
high-level executives with my instructional design prowess.

Finally, the VP turned to me and asked, "When do you think we can roll this out?" I gave an estimate, but that wasn't
what I thought was the most crucial part of my presentation. Then came the follow-up question. "Do you have a
breakdown of who will pay for what?" I did not. Reality smacked me square across my overly confident face. I was not
there to present a proposal for my innovative, custom-designed, and beautifully-built curriculum. I mean, yes, I was, but
that was only a small part of my role. As this development progressed, it became more evident that the senior
leadership was more concerned about milestones and deadlines than my knowledge of ADDIE. I was not only an
instructional designer; I was the project manager, the bottom line. It was my responsibility to handle timelines, budgets,
and all the not-so-fun, not-so-creative details that would enable my team to pull off this massive project and produce an
excellent product.

When I left the corporate world behind and started to design curriculum for higher education, my project management
skills were invaluable. As you join the world of instructional designers, you will often find yourself in the role of a project
manager. Over the years, I have learned, leading project teams is an essential skill for an instructional designer to
master. It is something to embrace and not fear.

Basics of Project Management

From the Project Management Body of Knowledge Guide (PMBOK Guide), a project is "a temporary endeavor
undertaken to create a unique product, service, or result" (Project Management Institute, 4). The keyword in the
definition of project is temporary. The permanent, everyday production of products and services is called operations.
Project management is "the application of knowledge, skills, tools, and techniques to project activities to meet the
project requirements" (Project Management Institute, 10).

Now let's explore several basic concepts of Project Management: Project Phases, Project Constraints, Project
Management Triangle, and Ethics of Working with Teams.

Project Phases

The textbook, Project Management for Instructional Designers, describes four typical lifecycle phases of a project:
Initiation, Planning, Execution, and Closeout (or Closing) (see Figure 1).

Figure 1

                                                                                       441
Four Phases of Project Management

The Initiation phase encompasses all the assignments and actions that need to occur before project planning. The
Initiation phase starts "with the assignment of the project manager and ends when the project team has sufficient
information to begin developing a detailed schedule and budget" (Wiley et al., p. 3.1). The Planning phase centers on
developing "an understanding of how the project will be executed and a plan for acquiring the resources needed to
execute it" (Wiley et al., p. 3.1). The Execution phase emphasizes "the major activities needed to accomplish the work of
the project" (Wiley et al., p. 3.1). The Closeout (or Closing) phase signifies the last stage of a project and where "project
staff is transferred off the project, project documents are archived, and the final few items or punch list is completed"
(Wiley et al., p. 3.1). I would recommend exploring more about each of these phases in this textbook, Project
Management for Instructional Designers.

   Additional Videos

           4 Phases of the Project Life Cycle
           The Typical Phases in Project Management

Project Constraints

Scope

With every project, we must consider scope. The definition of project scope is "what tasks the project team is expected
to accomplish and, just as importantly, what is not part of the project" (Wiley et al, p. 7.2).
Every project, being temporary in nature, has a beginning and ending. As the project manager, you need a clear vision of
where the project needs to go. To help understand this concept, we will explore the second habit of 7 Habits of Highly
Effective People, "Begin with the end in mind." Stephen Covey (2013) states,
"`Begin with the end in mind' is based on the principle that all things are created twice. There's a mental or first creation,
and a physical or second creation, to all things. Take the construction of a home, for example. You create it in every
detail before you ever hammer the first nail into place. You try to get a very clear sense of what kind of house you want...
The carpenter's rule is 'measure twice, cut once.' You have to make sure that the blueprint, the first creation, is really
what you want, that you've thought everything through. Then you put it into bricks and mortar. Each day you go to the
construction shed and pull out the blueprint to get marching orders for the day. You begin with the end in mind" (p. 95).
Like the construction of a house, you need to plan and create a "blueprint" for your project before work can begin. After
you understand the project, you will need to articulate this vision to your team. Your team will never have a better
understanding of the project scope than you. As the project manager, if you are unclear about the project's expectations,
your team will also be unclear on them. This path leads to scope creep.

Dangers of Scope Creep

Scope creep happens anytime the project's requirements change after you start the project. This creep is a subtle
phenomenon. Threats to time and cost loom as you allow small changes to occur based on last-minute suggestions.

                                                                                       442
These change requests are often coupled with the rationale that, "We are already adjusting the curriculum anyway." Or
"It's only one small change." One small change can have an enormous impact on the project. In many cases, as the
project manager, you will underestimate the full effect of the change. You never want to find yourself in the position
where you overpromise and underdeliver. Scope creep will make you unable to fill your commitments and leave your
stakeholders disappointed.

Cost

As a project manager, you have an essential role to manage budgets and cost. You will need know your budgetary
constraints and work to minimize costs. As illustrated in my Walmart example, knowing your budget and costs are of
crucial consideration and interest to your stakeholders.

Time

Your stakeholders also want to know whether the project is on schedule. Stakeholders' satisfaction is often tied to
project performance expectations. You need to know the timeframe you have to accomplish the project. You will need
to estimate how long every stage of the project will take and find the project's critical path.
Rapid prototyping reduces time and cost in a project. (Rapid prototyping was explored in earlier chapters.) By
prototyping, you and your stakeholders can make decisions that will save time and money. This process results in
higher overall quality and allows you to influence the decision-making process along the way. Prototyping enables you
to make the best use of our next concept, the project management triangle.

Project Management Triangle

A common adage about project management is, "You can have it good, fast, or cheap. Pick two." This statement derives
from the concept called the "project management triangle," or sometimes referred to as the "iron triangle." There are
several variations of the triangle. One of the most common variations is the constraints of cost, time, and scope make
the sides of the triangle with the center representing quality (see Figure 2).
Figure 2
Project Management Triangle

                                                                                       443
Let's say you are creating a training course, and you want it to be done quickly (time constraint) and at a low cost (cost
constraint.) It becomes evident that the scope needs to be limited (scope constraint). You will not be able to add a lot of
extras to the course (e-learning or VR experience). If you try to do all three, the quality of the course will suffer. If the
clients want the VR experience, then the costs will rise. There are always tradeoffs for every adjustment made to a
project because you cannot improve all three constraints simultaneously. As the project manager, it is critical to
understand the consequences of every decision made and how it affects other decisions.

   Application Exercises

           Create a chart that explains what happens at each phase of the project management lifecycle.
           How can effectively managing project constraints (scope, time, and cost) help a project to be successful
           and prevent scope creep?

Working With Teams

Besides knowing the constraints, understanding the different teams that you will be working with on a project is
imperative. There are several different types of teams that you may work with on any project. In this chapter, we will only
highlight a few of those teams.
Figure 3
Relationship Between Different Teams on a Project

                                                                                       444
Working With Your Team

Whether you were given your team or you selected your team, your leadership will determine the team effectiveness and
ultimately the success of the project. Your ability to manage and inspire people will be one of the most critical factors to
the success of the project. You will need to assess what skills you will need for the project and match them with your
team members.

As you identify strengths and skills of your team members, you need to allow people to play off their strengths. In the
book, Strengths Finder 2.0, Gallup scientists "discovered that people have several times more potential for growth when
they invest energy to developing their strengths instead of correcting their deficiencies" (Rath, 2007, p. i). I have found
when people are leveraging their strengths, they are happier and more productive.

Working With SME

A subject-matter expert (SME) can be a blessing and a curse at the same time. A SME offers much-needed content
knowledge; however, the project can quickly become off schedule if the SME is unavailable or unable to meet critical
deadlines. Developing a relationship with an SME is explored more in the chapter called, "Working With Stakeholders
and Clients." Maintaining healthy relationships with key stakeholders, like the SME, will dramatically impact the project.

Working With the Delivery Team (Instructors, Faculty, Trainers, etc.)

As an instructional designer, rarely are you the one delivering or teaching the curriculum. Because of that, it is critical to
keep the instructor/trainer in mind with the design of the materials, and how will it be implemented by the delivery team.
With the excellent materials your team created, you will want the instructor training performed at the same high level. It
may be necessary for you to work with your organization's delivery arm to ensure that the Train the Trainer (T3) sessions
or teacher education programs happen according to plan. With this coordination, a successful implementation will allow

                                                                                       445
the participants to experience the curriculum the way you design it. To explore more about implementation, read the
chapter in this textbook titled, "Implementation and Instructional Design."

Working With the Stakeholders and Clients

Your underlying goal of the project is to deliver what your stakeholders and clients want. To effectively do that, you need
to answer these questions: what is the goal and how will success be assessed? You need to know the goal and design
in the assessment.
When I created my first workshop for Walmart, I built it solely around the content given to me by the subject matter
experts. I created beautiful workbooks and presentations. I even had the chance to attend the workshop as the material
was delivered. At the end of the workshop, all the participants were ready to change the world because of my class. I
thought to myself, "#LearningWasAchieved." A few weeks later, I was asked by a senior leader what type of return on
investment (ROI) metrics I had and how well did the workshop achieve the outcomes? After scrambling to pull
information together about the success of the class, I was only able to generate a pitiful report. I realized the failure was
not in the workshop, but that I was unprepared to report on its success. From this experience, I learned the importance
of backward design. In backward design, you start with the learning outcomes or goals, create assessments to measure
those outcomes, and then create the content that will enable the learners to complete the assessments successfully.
The next time I developed a class, I started the conversation by asking questions such as: "What do we want to
measure?" and "What do the students need to know?" I did this questioning on the front side of the project. I designed in
key outcome metrics with how they would be assessed in the curriculum. This time, I had the reports prepared, showing
ROI. Of all the leadership and management skills I have learned, the ability to show value (ROI) to my stakeholders has
had the greatest impact on my professional success as a designer. Another chapter, "Working With Stakeholders and
Clients," reviews more on these interactions.

Effective Strategies in Leading a Team

Figure 4 shows the strategies to use to help you successfully lead your team through a project.

Figure 4
Effective Strategies in Leading a Team

                                                                                       446
Clarifying Roles and Responsibilities

Define the roles of your team. You need to understand who is ultimately responsible. Hyman G. Rickover, the "Father of
the Nuclear Navy," led a team to build the world's first nuclear-powered submarine, the USS Nautilus. He had a deep
understanding of responsibility. He said, "Responsibility is a unique concept... You may share it with others, but your
portion is not diminished. You may delegate it, but it is still with you... If responsibility is rightfully yours, no evasion, or
ignorance or passing the blame can shift the burden to someone else. Unless you can point your finger at the man who
is responsible when something goes wrong, then you have never had anyone really responsible." (United States
Congress, 1965, p. 87). Although you may have delegated tasks, the shared successes and failures of your project are
yours to bear.

   Additional Videos

           Key Project Team Roles

Streamlining Workflow

Find bottlenecks. As a leader, you can't fully delegate responsibility away for a project. So, if tasks aren't happening, you
need to ask, "Why?" You will need to find solutions. In the book, The Goal: A Process of Ongoing Improvement, the main
character receives a critical insight from one of his professors, "What you have learned is that the capacity of the
[project] is equal to the capacity of its bottlenecks" (Goldratt, 1992, p. 158). This capacity of any given bottleneck on
your team or as part of your project affects the amount of work accomplished during a particular timeframe.

                                                                                       447
The speed of the project will depend on the number of bottlenecks. It would be best if you gave time and attention to
improving those bottlenecks each day. The priority has to be what problems your team is having and helping them solve
the issues quickly. These problems or bottlenecks will determine whether you will meet your deadline.

Communicating Effectively

Don't keep your people in the dark. As the project manager, you will need to develop strong communication channels
with your team and stakeholders. B.G. Zulch discovered that "The single most significant factor affecting the success of
a project is the communication ability of the project manager. If it seems true that everything rises and falls on
communication and leadership, it stands to reason that leadership communication ability is the foundational skill that
must be attained for a project manager to be effective...Communication is so important to project success that it has
been referred to as the lifeblood of a project..." (2014, p. 1001).
Don't allow blind spots to develop. The responsibility of communication does not rest solely on the project manager.
The entire team needs to be communicating regularly with updates on progress. Without regular updates, blind spots
will occur.
Have regular meetings with your team. Many meetings are considered boring and a waste of time because the leader
conducting the meeting does not know how to have a productive meeting. From the book, Death by Meeting, Lencioni
(2004) asserts two ideas for more productive meetings. First, meetings need more drama. The meetings need to be
centered around conflict by addressing difficult questions. Let your people be passionate about what they do. Second,
meetings need contextual structure. Not every meeting is the same type. Have the right kind of meeting that addresses
your needs from a daily check-in to a monthly strategic meeting.

   Additional Videos

           Improving Your Project Management Communication
           How to Run Team Meetings

Creating a Feedback Culture

Foster a culture of offering and receiving feedback. Feedback creates accountability. In the book, The Oz Principle,
Connors et al. (2004) state, "You can gain great insight from frequent, regular, and ongoing feedback from other people.
Although it can cause a great deal of pain and embarrassment at times, honest input helps create the accurate picture
of reality that lies at the core of accountability" (p. 81). With any project, you need people to be accountable for their
assigned tasks. With this loop of giving and receiving feedback, you, as the manager, can see the status of each part of
the project. When you see epic failures with projects, many times, the lack of feedback is the underlying reason for it.

   Additional Videos

           Managing Teams & Giving Feedback - Project Management

Be in the Details

Spend the majority of your time in the details of the project. Admiral Rickover, the manager who led the team to build the
world's first nuclear-powered submarine, drove this point home by stating, "The man in charge must concern himself

                                                                                       448
with details. If he does not consider them important, neither will his subordinates. Yet 'the devil is in the details.' It is
hard and monotonous to pay attention to seemingly minor matters. In my work, I probably spend about ninety-nine
percent of my time on what others may call petty details. Most managers would rather focus on lofty policy matters.
But when the details are ignored, the project fails. No infusion of policy or lofty ideals can then correct the situation"
(Rickover, 1982). Success is through the execution of small tasks.

Managing Conflicts

Have crucial conversations. Anytime you are dealing with people under time and money constraints, you are going to
have conflict. As a project manager, you will be required to have crucial conversations. From the book, Crucial
Conversations, Patterson et al. (2002) define a crucial conversation as "a discussion between two or more people where
stakes are high, opinions vary, and emotions run strong" (p. 3). When leading a crucial conversation, it is imperative to
stay focused on the facts and issues. Prepare ahead of time so you can stay calm and not be distracted by strong
feelings and emotional appeals. A leader who approaches these difficult conversations in a non-emotional way serves
his people more effectively. You can create a safe environment for a crucial conversation by validating the other
person's position. Patterson et al. (2002) recommend the STATE method to manage conflict. The STATE method is an
acronym of the tools you can use: Share your facts, Tell your story, Ask for others' paths, Talk tentatively (or speaking
gently and respectfully), and Encourage testing (or invite others to talk) (p. 124).

   Additional Videos

           Conflict Management - Key Concepts in Project Management

   Application Exercises

           Identify the type of support you will need to give for each of the different teams with which you are
           interacting.
           Which strategies do you feel are the most important and how would you apply those strategies to your
           situation?

Conclusion

In this chapter, we discussed that although you may be an instructional designer, you might often find yourself in the
project manager role. We explored the four phases of the project management lifecycle: initiation, planning, execution,
and closeout. We also studied how the project management triangle showed how the three constraints of scope, time,
and cost can impact quality of a project. We reviewed the different teams you will work with on a project and provided
effective strategies in leading a team.
Even if project management is new to you, becoming an effective project manager can be learned. As you master those
skills, you will find yourself leading effective teams. A well-managed project will cause a positive outlook for everyone
from your team down to the stakeholders and clients. Your finely-tuned project management skills will lead you to future
success.

                                                                                       449
   Resources

           Project Management for Instructional Designers Textbook (PM4ID)
           Project Management Institute - pmi.org
           A Guide to the Project Management Body of Knowledge ( PMBOK® Guide)

References

Connors, R., Smith, T., & Hickman, C. (2004). The Oz Principle: Getting results through individual and organizational
         accountability. New York, NY: Portfolio.

Covey, S. (2013). The 7 habits of highly effective people: Powerful lessons in personal change (25th anniversary
         edition). Retrieved from https://ebookcentral.proquest.com

Goldratt, E.M. & Cox, J. (1992). The Goal: A process of ongoing improvement (Second revised edition). Great Barrington,
         MA: North River Press.

Lencioni, P. (2004). Death by meeting: A leadership fable...about solving the most painful problem in business (Vol. 1st
         ed). San Francisco, CA: Jossey-Bass. Retrieved from https://ebookcentral.proquest.com

Patterson, K., Grenny, J., McMillan, R., Switzler, A. (2002). Crucial conversations. New York, NY: McGraw-Hill
         Professional Publishing.

Project Management Institute. (2017). A Guide to the Project Management Body of Knowledge (PMBOK® Guide)-Sixth
         Edition. Newtown Square, Pennsylvania: Project Management Institute.

Rath, T. (2007). Strengths Finder 2.0. New York, NY: Gallup Press.
Rickover, H. G. (1982). Doing a Job. Retrieved from https://edtechbooks.org/-Nyt
United States Congress. Joint Committee on Atomic Energy. (1965). Loss of the U.S.S. "Thresher.": Hearings, Eighty-

        eigth Congress, First and Second Sessions. Washington D. C.: U.S. Government Publishing Office
Wiley et al. (n.d.). Project management for instructional designers. Retrieved from http://pm4id.org/
Zulch, B. (2014). Communication: The foundation of project management. Procedia Technology, 16, 1000-1009.

         https://doi.org/10.1016/j.protcy.2014.10.054

                                                                                       450
This content is provided to you freely by EdTech Books.
Access it online or download it at https://edtechbooks.org/id/leading_project_team.

                                                        451
452
  37

Implementation and Instructional Design

Brittany Eichler & Jason K. McDonald

   Editor's Note

     This is a remixed version of an earlier chapter on implementation in instructional design that can be found at
     the ADDIE Explained website, and is printed here under the same license as the original.

Instruction is designed to be used. This seemingly obvious statement carries a rather significant implication: the work
of an instructional designer should not end upon the final development of the product, but must include considerations
for when, where, and how the instruction will be used by real learners in actual situations. This work is called
implementation. It requires planning and attention to detail--the same as found throughout the rest of the instructional
design process, in fact--to complete successfully. Without implementing an instructional design, all the design work
would, in large measure, be wasted.
Implementation is a frequently-skipped step of the instructional design process, however. Designers are often
(understandably) ready for their next exciting assignment, and often the client or other stakeholders want to be the
primary actors during implementation. The organization the designer works for may also not consider it within their
scope to assign instructional designers to help in the implementation phase.
But even when someone else has the actual responsibility to implement an instructional design, the designer can (and
should) still be involved, at least in some fashion. Often he or she will have information that no one else has about the
design (what certain components are meant for, or how certain features behave), and that information is crucial to
ensure it can be implemented successfully. Few people know the entire project as well as the designer does, and this
expertise should be drawn upon during the implementation process.
The purpose of this chapter is to introduce considerations that need to be made during the implementation phase of the
instructional design process. To organize our discussion we rely on the five stages of introducing a new design as
described by Everett M. Rogers (2003). Additionally, it is imperative that instructional designers (or other change agents
like teachers or stakeholders) are aware of how people typically use products or services as they are being
implemented. So we also describe how adopters of new products or services commonly move through Rogers's stages.

Adopting New Designs

Gibbons (2013) described the importance of implementation as follows:

                                                                                       453
Implementation is a period of intense and important change. In addition, it is a period of high-stakes decisions that
affect the judgment of continued use of your product. Your product is not only making its first impression on people
during implementation, but it is gathering either support or censure from those most likely to determine its viability--
students, instructors, and administrators. A careful implementation plan can help your product to be introduced with the
best possible chances of success (p. 410).
Similarly, Rogers (2003) suggested that, "the perceived newness of an innovation, and the uncertainty associated with
this newness, is a distinctive aspect of innovation decision making" (p.161). As a result of this "uncertainty,"
understanding the design adoption process can help designers plan an instructional design implementation to
maximize the chances it can have its intended effect with learners. To help instructional designers create a complete
implementation plan, we recommend considering the phases of innovation adoption as a framework for creating their
implementation plans (see Figure 1). The five stages in Rogers's model that will be discussed in this chapter are:

      Knowledge
      Persuasion
      Decision
      Use
      Confirmation
Note that the stage when people actually use the new material is stage four of this model! This should be evidence of
how important it is to consider many factors that affect how someone will successfully use an instructional design, and
encourage designers to not just complete the project and walk away.
Figure 1
The Stages of Roger's Implementation Model

Knowledge

The expectation within the knowledge stage is that the adopter becomes aware of the design to be implemented, and
determines if a need for adopting (or implementing) the design is actually present. In the context of instructional design,
this could mean the designer prepares (or helps prepare) material that is useful to decision-makers about why they

                                                                                       454
should use the instruction. This could take the form of an information sheet, or be more sophisticated like a full
marketing campaign. It can also be directed to the students themselves, or others who might be the primary adopter of
the design who will then introduce it to students (like a teacher or a school district).

Persuasion

The persuasion stage occurs when the adopter begins to decide if they find the new design acceptable. During this
process, the adopter "actively seeks information about the new idea, decides what messages he or she regards as
credible, and decides how he or she interprets the information that is received" (Rogers, 2003). It is through this process
that an adopter begins to decide if the design will be accepted. Instructional designers can facilitate the persuasion
stage at the same time they provide knowledge about it. Why is it compelling? How does it fulfill real needs? What can
be said about it that adopters will feel emotionally attracted to? (Do more than just provide the facts!) Like before,
persuasion can be directed to both the student or other decision-makers.

Decision

The decision stage includes the adopter actively participating in tests that will assist them in determining if the design
will be adopted or rejected. It is important to note that this process can justifiably lead to either of these results:
adoption or rejection. If the design is adopted, it is evidence that it is seen as a solution to the problem or issue the
adopter initially defined. If the design is rejected, it can be classified as either active or passive rejection. According to
Rogers (2003), active rejection consists of considering adoption of an innovation and then actively deciding not to
adopt it. Passive rejection is when no identifiable decision is made, but due to inaction the innovation is effectively
rejected. Instructional designers can help with the decision phase by making it as easy as possible for students or
decision-makers to try out the instruction before committing to it. Can the designer be on-site for a test of the
materials? Can they demonstrate to students or decision-makers what it actually looks like when the instruction is being
used? Can they give away a component for free that people can test?

Use

The next stage in this model is the actual usage of the new design. Using a new product is generally not a one-time
endeavor. New design usage is generally considered a long-term process. While the definition of "long-term" can be
ambiguous and is heavily determined by the context, it is important to know the use of a new innovation within
instructional design is usually not simply "plug and play." There is generally a period of continued education and
professional development associated with the adoption. The instructional designer might provide getting started
materials so people begin using the materials successfully, or technical support to make sure problems can be solved
as soon as they are apparent. They might have to train the person leading the instruction, or at the very least show
students how to use all of the features found in the instruction.

As the design is implemented, it is likely that an event referred to as re-invention may occur. Re-invention is defined in
this context "as the degree to which an innovation is changed or modified by a user in the process of its adoption and
implementation" (Rogers, 2003, p. 180). It is important to note that re-invention is not necessarily a negative, as it can
lead to improved results. For instance, an instructional designer may have intended that students complete an online
module individually, but as it begins to be used throughout a company, the employees start to gather together in groups
and complete the assignments together. Even though the designer did not intend for this kind of use, evaluations could
show that it is more effective--students learn more and have deeper insights as they work together. An implication of
this is that designers should make their designs flexible, so they don't break down during re-invention. They should also
watch for re-invention because it might give them ideas for how they can design better in the future.

Confirmation

Confirmation occurs as the adopter evaluates the decision to adopt and implement the design. Are they satisfied with
what they chose? During this stage it is possible that the design will be subsequently discontinued. The evaluation can
be based on many measures: learner performance, ease of use, satisfaction, cost to maintain, etc. If discontinuance

                                                                                       455
occurs, it is often a result of some kind of dissonance, or the gap adopters experience between what they expected to
happen and what actually happened. It is important, then, for continued use of the design, that the instructional
designer seeks methods to reduce or eliminate dissonance. Some methods to achieve reduction of elimination include
helping adopters understand how to incorporate the design into their existing practices, continued support and training,
and fixing problems the adopter may be experiencing with the instruction that interfere with its ability to achieve its
intended outcomes.

   Application Exercise

     Consider an instructional design project you are either currently involved in, or one you are familiar with. Write a
     brief implementation plan for this project that uses all five of Rogers's implementation phases.

     Prepare a brief presentation about this implementation plan, as if you were assigned to explain to your client
     why each phase is important to successfully implement the project.

Attributes of Designs That Lead to Successful Implementation

In addition to the innovation-decision process, it is important for the instructional designer to consider factors in the
design itself that contribute to rates of adoption. Rogers (2003) identified five such attributes: relative advantage,
compatibility, complexity, trialability and observability.

Relative Advantage

The concept of relative advantage refers to whether the design is actually an improvement over the current product or
service the adopter has been using. If the adopter perceives that the design's value does not exceed that of the current
product used, the design is much less desirable and unlikely to be adopted. In contrast, a design that is determined to
be of greater value is more likely to be adopted. Instructional designers should be considering the relative advantage of
their instruction throughout the design process. How is what they are designing better than the status quo?

Compatibility

Compatibility is in reference to how well the design aligns with other aspects of the adopter's life and circumstances.
This could include the adopter's professional, pedagogical, and sociocultural ideologies. Conflict with any of these
schemas, whether directly impacting the design's actual use, could threaten adoption. As indicated by Rogers (2003),
"any new idea is evaluated in comparison to existing practice. Thus compatibility is, not surprisingly, related to the rate
of adoption of an innovation" (p. 249). Through careful attention to the adopter's (students or other decision-makers)
beliefs, interests, needs, and concerns throughout the design process, designers can help prepare their instruction so it
is more compatible with what adopters expect and need.

Complexity

Complexity is how difficult it is to comprehend, incorporate, and actually use the design. While complexity does not
impact the rate of adoption to the same degree as relative advantage and compatibility, the complexity of a design can
negatively impact how likely it is for adopters to use (or want to use) it. If a design is perceived to be too difficult to
incorporate or use, it is less likely to be adopted in the first place or more likely to be discontinued if it is adopted. Good
evaluation and testing of prototypes throughout the instructional design process can help minimize the complexity of
their instruction. Designers, in fact, can consider how they can specifically test prototypes to help minimize complexity
(such as through a usability test).

                                                                                       456
Trialability

Trialability refers to how readily a design can be tested or used with a limited commitment. For example, software is
often introduced in stages, or "betas." These stages of progressively more complete versions of a product permit its
testing on a limited basis. Such testing permits users to identify issues and helps increase adoption. Trialability has a
positive impact on the rate of adoption for early adopters, but is less impactful on the rate of adoption for later adopters
(Rogers, 2003). As is hopefully clear, the trialability of instruction is closely associated with the decision phase
described above. Designers should prepare for the trialability of their instruction as early as possible in their design
process. High fidelity prototypes might be an easy and low-cost way of doing this.

Observability

Observability refers to "the degree to which the results of an innovation are visible to others" (Rogers, 2003, p. 16).
Designs that are more difficult to observe or difficult to explain and operationalize are less likely to be adopted. This can
be especially difficult for instructional designers because so much of the learning process is invisible or hard to
observe. It helps to make sure the learning goals of the instruction are as measurable and observable as possible.
Regularly reporting the results of assessments of student learning can also help. While important, however,
observability is the least impactful of the attributes Rogers identified.

   Application Exercise

     You are an instructional designer implementing a new computer-based learning tool in a K-12 classroom. The
     teacher is not technologically savvy and is hesitant to use this new tool. Explain what steps might be taken to
     support the teacher and mitigate their concerns.
     Considering Rogers' five attributes that impact the rate of adoption of innovations, please explain how these
     attributes would affect implementation decisions that you, as an instructional designer, would make, for this
     teacher.

Conclusion

In this chapter, we discussed the implementation phase of the instructional design process. We described important
factors of implementation using the five stages of the diffusion of innovations: knowledge, persuasion, decision,
implementation, and confirmation. We also reviewed characteristics of a design itself that can impact rates of
implementation: relative advantage, compatibility, complexity, trialability and observability.
Implementation is a phase instructional designers should begin planning for at the beginning of their project. By
carefully reviewing the material we provide here, designers--and those they support--will be able to ensure the
instruction they create is actually used by those it is intended for so the desired changes that led to its creation can be
brought about.

References

Gibbons, A. S. (2013). An architectural approach to instructional design. Routledge.
Rogers, E. M. (2003). Diffusion of innovations. Simon and Schuster.

                                                                                       457
This content is provided to you freely by EdTech Books.
Access it online or download it at https://edtechbooks.org/id/implementation_and_i.

                                                        458
Appendices

     Author Biographies
                           This content is provided to you freely by EdTech Books.
                           Access it online or download it at https://edtechbooks.org/id/appendices.

                                                                                       459
460
Author Biographies                                                                          Bohdana Allman

                Bert Aertgeerts                                                             Bohdana Allman is an educator,
                                                                                            researcher, and learning experience
                    KU Leuven                                                               designer. Her work focuses on designing
                                                                                            and implementing online collaborative
                Tutaleni I. Asino                                                           teacher professional development
                                                                                            grounded in sociocultu...
                    Oklahoma State University
                    Tutaleni I. Asino, PhD, is an Associate                                 Jeff Batt
                    Professor of Educational Technology and
                    Director of the Emerging Technology and                                 Learning Dojo
                    Creativity Research Lab in the College of
                    Education and Human Sciences at                                         Jeff has 10+ years experience in the
                    Oklaho...                                                               digital learning and media industry.
                                                                                            Currently he is Founder and Trainer at
                Robert Bodily                                                               Learning Dojo, a company dedicated to
                                                                                            training you to become a software ninja in
                    Lumen Learning                                                          a vari...
                    Dr. Bob Bodily is a Senior Data Scientist
                    and Educational Researcher at Lumen                                     Elizabeth Boling
                    Learning. He focuses on building
                    educational data pipelines, creating                                    Indiana University
                    actionable reports, and generating insights
                    to impro...                                                             Elizabeth Boling is professor of
                                                                                            instructional systems technology in the
                Cheryl Calhoun                                                              School of Education at Indiana University.
                                                                                            Prior experience includes 10 years in
                    Santa Fe College                                                        design practice, five with Apple Computer,
                    Cheryl Calhoun is the Dean of Access &                                  Inc...
                    Inclusion at Santa Fe College where she
                    taught information technology for over 20                               Trudy K. Christensen
                    years. Her experience spans higher
                    education, non-profit administration, and                               Utah Valley University
                    se...
                                                                                            Trudy K. Christensen, PhD, is an assistant
                                                                                       461  professor in the Digital Media Department
                                                                                            at Utah Valley State College and founder
                                                                                            and president of Learning Connections, a
                                                                                            consulting company specializing i...
Sheri Conklin                                      Theresa A. Cullen

University of North Carolina Wilmington            Arkansas Tech University
Dr. Sheri Conklin is an Assistant Professor
at the University of North Carolina                Dr. Cullen is a professor and the
Wilmington. Prior to moving into this role,        Department Head of Curriculum and
she worked as the Director of eLearning            Instruction at Arkansas Tech University.
over a team of Instructional Designe...            Previously she was the Director of Digital
                                                   Strategy for the Jeannine Rainbolt College
John H. Curry                                      of Ed...

Idaho State University                             Bucky J. Dodd
John H. Curry is the Chair of the
Organizational Learning and Performance            University of Central Oklahoma
Department and a Professor of
Instructional Design and Technology at             Bucky Dodd, Ph.D. is the Chief Learning
Idaho State University. He teaches in the          Innovation Officer and Director of the
Instructional D...                                 Institute for Learning Environment Design
                                                   at the University of Central Oklahoma. Dr.
Brittany Eichler                                   Dodd is an innovator, master teacher...

José Fulgencio                                     Jimmy Frerejean

Roosevelt University                               Maastricht University
José "Jay" Fulgencio, Ph.D. is an instructor
at Roosevelt University for the Heller             Shanali C. Govender
College of Business, Organizational
Leadership. Dr. Jay earned his Ph.D. from          University of Cape Town
Oklahoma State University. Dr. Jay ha...
                                                   Shanali is a lecturer within the Academic
Susie L. Gronseth                                  Staff Development unit at the University of
                                                   Cape Town's Centre for Innovation in
University of Houston                              Learning and Teaching. Her particular brief
Dr. Susie Gronseth is a Clinical Associate         in the staff development team i...
Professor in the Learning, Design, and
Technology program area in the College of          Lisa Harris
Education at the University of Houston
(Houston, Texas, USA). She specialize...           Winthrop University

                                                   Lisa Harris is an associate professor in the
                                                   Education Core Department at Winthrop
                                                   University and is the program coordinator
                                                   for the Master of Arts in Teaching degree.
                                                   She teaches courses in educatio...

                                              462
Joshua Hill                                     Brad Hokanson

Peter C. Honebein                               University of Minnesota

Customer Performance Group                      Brad Hokanson is the Mertie Buckman
Dr. Peter C. Honebein, co-founder and           Professor of Design Education in the
managing director of the Customer               College of Design at the University of
Performance Group, focuses his career on        Minnesota. He has a diverse academic
researching, designing, and developing          record, including degrees in art,
innovative employee and customer                architecture, and...
performance imp...
                                                Tasneem Jaffer
Jacquelyn Claire Johnson
                                                University of Cape Town
Dr. Jacquelyn Johnson is a product
development and user experience                 Tasneem has worked as a senior project
researcher. She graduated with degrees in       coordinator and learning designer at the
Instructional Psychology and Technology         University of Cape Town in South Africa.
from Brigham Young University. She has          She has completed an MEd in Educational
experience d...                                 Technology and an MBA from the
                                                Universi...
Marshall G. Jones
                                                Sacha Johnson
Winthrop University
                                                Idaho State University
Paul A. Kirschner
                                                Dr. Sacha Johnson is a Senior
Open University of the Netherlands and          Instructional Technologist at Idaho State
the University of Oulu                          University working in the Instructional
                                                Technology Resource Center (ITRC). She
                                                enjoys helping faculty with the learning
                                                manageme...

                                                Linda Jordan

                                                Benjamin Kohntopp

                                                Colorado Community Colleges Online

                                                Ben Kohntopp is an ID at CCCOnline and
                                                has 6 years of instructional design
                                                experience. He started his career at
                                                Colorado Christian University before
                                                moving to CCCOnline, where he has
                                                designed courses ...

                                           463
Ahmed Lachheb                                                        Marco Marcellis

University of Michigan                                               Amsterdam University of Applied Sciences

Dr. Ahmed Lachheb is a design scholar, a                             Seth-Aaron Martinez
design practitioner, and a design educator.
He serves as a Senior Learning Experience                            Boise State University
Designer at the University of Michigan's                             Seth Martinez, PhD, is an assistant
Center for Academic Innovation. ...                                  professor in the Organizational
                                                                     Performance & Workplace Learning
Florence Martin                                                      department at Boise State University. His
                                                                     scholarship focuses on expertise
University of North Carolina Charlotte                               development, including t...

Dr. Florence Martin is a Professor in the                            Esther Michela
Learning, Design and Technology program
at University of North Carolina Charlotte.                           University of Tennessee
She received her Doctoral and Master's                               Currently a PhD student at the University
degrees in Educational Technology fr...                              of Tennessee Knoxville, Esther is exploring
                                                                     issues of accessibility, empathy, Universal
Jason K. McDonald                                                    Design for Learning, and student
                                                                     interaction in the context of instruc...
Brigham Young University
                                                                     Rebeca Peacock
Dr. Jason K. McDonald is a Professor of
Instructional Psychology & Technology at                             Boise State University
Brigham Young University. He brings                                  Rebeca Peacock is an Instructional
twenty-five years of experience in industry                          Designer and Assistant Professor,
and academia, with a career spanning a ...                           Librarian at Boise State University. She has
                                                                     an MEd in Instructional Design and
Beth Oyarzun                                                         Technology from Wayne State University,
                                                                     an MSLIS fr...
University of North Carolina at Charlotte
                                                                     Rudy Rico
Beth Oyarzun is a Clinical Associate
Professor of Learning, Design and
Technology (LDT) at the University of
North Carolina at Charlotte. She teaches
fully online LDT technology courses and
her resear...

Charles M. Reigeluth

Indiana University

Charles M. Reigeluth is a distinguished
educational researcher and consultant
who focuses on paradigm change in
education. He has a B.A. in economics
from Harvard University, and a Ph.D. in
instructio...

                                                                464
Albert D. Ritzhaupt                                                  Ann Roex

University of Florida                                                Vrije Universiteit Brussel
Dr. Albert D. Ritzhaupt is a Professor of
Educational Technology and Computer                                  Justin Sentz
Science Education, and the Associate
Director for Graduate Studies in the School                          Shippensburg University
of Teaching and Learning at the Universit...                         Justin Sentz is the Deputy Chief
                                                                     Information Officer & Executive Director of
Shilpa Sahay                                                         Academic Technology and User Services
                                                                     at Shippensburg University. His research
Kathy Sindt                                                          interests focus on the management of
                                                                     cogniti...
Colorado Community Colleges Online
Kathy Sindt is a Senior Instructional                                Ashley Smith
Designer at Colorado Community Colleges
Online, where she designs online courses                             Brigham Young University Idaho
for the Colorado Community College                                   Ashley Smith is an online Curriculum
System. She brings 30 years of experience                            Designer for Brigham Young University-
to t...                                                              Idaho where he designs and develops
                                                                     online university courses for a worldwide
Jill E. Stefaniak                                                    audience. He also teaches an online
                                                                     business co...
University of Georgia
Jill Stefaniak is an Associate Professor in                          Ross Strader
the Learning, Design, and Technology
program in the Department of Workforce                               Lumen Learning
Education and Instructional Technology at
the University of Georgia. Her researc...                            Lee Tran

Vanessa Svihla                                                       Colorado Community Colleges Online
                                                                     Lee Tran is an e-Learning Technologist at
University of New Mexico                                             Colorado Community Colleges online,
Dr. Vanessa Svihla is an assistant                                   where he designs online courses, creates
professor at the University of New Mexico                            digital assets, and provides support to the
with appointments in the learning                                    instructional design team for the Col...
sciences and engineering, and she directs
the Interaction and Disciplinary Design in
Educ...

                                                                465
Lydia Oluchi Ugwu                                 Jeroen J.G. van Merriënboer

University of Houston                             Maastricht University

Lydia Oluchi Ugwu is a PhD student of             Richard E. West
curriculum and instruction at the
University of Houston. Her doctoral               Brigham Young University
research focuses on teaching strategies           Dr. Richard E. West is an associate
for diverse learners, teaching skills and         professor of Instructional Psychology and
competenci...                                     Technology at Brigham Young University.
                                                  He teaches courses in instructional
Ellen D. Wagner                                   design, academic writing, qualitative
                                                  research...
University of Central Florida
                                                  David Wiley
Dr. Ellen Wagner is a co-founder and chief
research officer at PAR Framework. She            Lumen Learning
also serves as vice president of research         Dr. David Wiley is the chief academic
at Hobsons. In the past, Dr. Wagner has           officer of Lumen Learning, an organization
been a partner at Sage Roads Solutio...           offering open educational resources
                                                  designed to increase student access and
Justin N. Whiting                                 success. Dr. Wiley has founded or co-
                                                  founde...
Justin Whiting is an Instructional Designer
for Intermountain Healthcare and Primary          Daniel R. Winder
Children's Hospital, and a Doctoral
Candidate at Indiana University. Justin has       Daniel R. Winder has conducted research
more than 15 years of industry an...              for business, industry, government, and
                                                  education for over 20 years. His PhD from
Matthew L. Wilson                                 Utah State University was in Instructional
                                                  Technology and Learning Sciences, w...
Kennesaw State University

Dr. Matthew L. Wilson is an Assistant
Professor of Instructional Technology in
the School of Instructional Technology and
Innovation for the Bagwell College of
Education at Kennesaw State University. ...

                                             466
This content is provided to you freely by EdTech Books.
Access it online or download it at https://edtechbooks.org/id/author_biographies.

                                                        467
468
