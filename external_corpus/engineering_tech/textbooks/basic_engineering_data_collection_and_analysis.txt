MD DALIM 8/14/00 #538341 PMS321 BLK


         Basic Engineering
         Data Collection
         and Analysis

                  Stephen B. Vardeman

                             Iowa State University

                  J. Marcus Jobe

                             Miami University

                                Ames, Iowa
Basic Engineering Data Collection and       © 2001 Stephen B. Vardeman and J. Marcus Jobe
Analysis                                    Basic Engineering Data Collection and Analysis is available under a Creative
Stephen B. Vardeman and J. Marcus Jobe      Commons Attribution NonCommercial ShareAlike 4.0 International license.

Sponsoring Editor: Carolyn Crockett         You may share and adapt the material, so long as you provide appropriate credit
Marketing: Chris Kelly                      to the original authors, do not use the material for commercial purposes, and
Editorial Assistant: Ann Day                any adaptations or remixes of the material which you create are distributed
Production Editor: Janet Hill               under the same license as the original.
Production Service: Martha Emry
Permissions: Sue Ewing                      Originally published by Brooks/Cole Cengage Learning in 2001.
Cover Design/Illustration: Denise Davidson  Published online by Iowa State University Digital Press in 2023.
Interior Design: John Edeen
Interior Illustration: Bob Cordes           Library of Congress Catalog Number: 00-040358
Print Buyer: Kristina Waller                ISBN-13: 978-0-534-36957-6 (print)
Typesetting: Eigentype Compositors          ISBN-10: 0-534-36957-X (print)
                                            ISBN: 978-1-958291-03-0 (PDF)
                                            https://doi.org/10.31274/isudp.2023.127

                                            Iowa State University Digital Press
                                            199 Parks Library
                                            701 Morrill Rd
                                            Ames, IA 50011-2102
                                            United States

                                            www.iastatedigitalpress.com

                                            Iowa State University is located on the ancestral lands and territory of the Baxoje
                                            (bah-kho-dzhe), or Ioway Nation. The United States obtained the land from the
                                            Meskwaki and Sauk nations in the Treaty of 1842. We wish to recognize our
                                            obligations to this land and to the people who took care of it, as well as to the
                                            17,000 Native people who live in Iowa today.
Soli Deo Gloria
qqqqqqqqqqqqqqqqqqq

               Preface

               This book is an abridgment and modernization of Statistics for Engineering Prob-

                                  lem Solving by Stephen Vardeman, which was published in 1994 by PWS Publishing
                                  and awarded the (biennial) 1994 Merriam-Wiley Distinguished Author Award by
                                  the American Society for Engineering Education recognizing an outstanding new
                                  engineering text. The present book preserves the best features of the earlier one,
                                  while improving readability and accessibility for engineering students and working
                                  engineers, and providing the most essential material in a more compact text.

                                       Basic Engineering Data Collection and Analysis emphasizes real application
                                  and implications of statistics in engineering practice. Without compromising math-
                                  ematical precision, the presentation is carried almost exclusively with references to
                                  real cases. Many of these real cases come from student projects from Iowa State
                                  University statistics and industrial engineering courses. Others are from our consult-
                                  ing experiences, and some are from engineering journal articles. (Examples bearing
                                  only name citations are based on student projects, and we are grateful to those
                                  students for the use of their data sets and scenarios.)

                                       We feature the well-proven order and emphasis of presentation from Statistics
                                  for Engineering Problem Solving. Practical issues of engineering data collection
                                  receive early and serious consideration, as do descriptive and graphical methods
                                  and the ideas of least squares curve- and surface-fitting and factorial analysis.
                                  More emphasis is given to the making of statistical intervals (including prediction
                                  and tolerance intervals) than to significance testing. Topics important to engineering
                                  practice, such as propagation of error, Shewhart control charts, 2p factorials and 2p-q
                                  fractional factorials are treated thoroughly, instead of being included as supplemental
                                  topics intended to make a general statistics text into an "engineering" statistics book.
                                  Topics that seem to us less central to common engineering practice (like axiomatic
                                  probability and counting) and some slightly more advanced matters (reliability
                                  concepts and maximum likelihood model fitting) have been placed in an appendix,
                                  where they are available for those instructors who have time to present them but do
                                  not interrupt the book's main story line.

                                                                                                                                            v
vi Preface

                                                qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

         Pedagogical Features

                                  Pedagogical and practical features include:
                                  s Precise exposition
                                  s A logical two-color layout, with examples delineated by a color rule

Example 1           Heat Treating Gears

                    The article "Statistical Analysis: Mack Truck Gear Heat Treating Experiments"
                    by P. Brezler (Heat Treating, November, 1986) describes a simple application
                    of engineering statistics. A process engineer was faced with the question, "How
                    should gears be loaded into a continuous carburizing furnace in order to mini-
                    mize distortion during heat treating?" Various people had various semi-informed
                    opinions about how it should be done--in particular, about whether the gears
                    should be laid flat in stacks or hung on rods passing through the gear bores. But
                    no one really knew the consequences of laying versus hanging.

s Use of computer output

WWW                 Printout 6 Computations for the Joint Strength Data

                    General Linear Model

                    Factor  Type Levels Values
                    joint
                    wood    fixed  3 beveled butt lap

                            fixed  3  oak pine walnut

s Boxing of those formulas students will need to use in exercises

Index (i) of the         Definition 1 identifies Q( p) for all p between .5/n and (n - .5)/n. To find
  ordered data      Q( p) for such a value of p, one may solve the equation p = (i - .5)/n for i,
   point that is    yielding
              Q(p)
                                                              i = np + .5

                    and locate the "(np + .5)th ordered data point."
                                                                    Teaching from the Text vii

s Margin notes naming formulas and calling attention to some main issues of
   discussion

Purposes of        The idea of replication is fundamental in experimentation. Reproducibility of
 replication  results is important in both science and engineering practice. Replication helps
              establish this, protecting the investigator from unconscious blunders and validating
              or confirming experimental conclusions.

s Identification of important calculations and final results in Examples

              To illustrate convention (2) of Definition 1, consider finding the .5 and .93
                                                                          .5-.45
              quantiles  of  the  strength  distribution.  Since  .5  is  .55-.45  =  .5  of  the  way  from  .45

              to .55, linear interpolation gives

              Q(.5) = (1 - .5) Q(.45) + .5 Q(.55) = .5(9,011) + .5(9,165) = 9,088 g

qqqqqqqqqqqqqqqqqqqqq

The Exercises

                    There are far more exercises in this text than could ever be assigned over several
                    semesters of teaching from this book. Exercises involving direct application of
                    section material appear at the end of each section, and answers for most of them
                    appear at the end of the book. These give the reader immediate reinforcement that
                    the mechanics and main points of the exposition have been mastered. The rich sets of
                    Chapter Exercises provide more. Beyond additional practice with the computations
                    of the chapter, they add significant insight into how engineering statistics is done
                    and into the engineering implications of the chapter material. These often probe
                    what kinds of analyses might elucidate the main features of a scenario and facilitate
                    substantive engineering progress, and ponder what else might be needed. In most
                    cases, these exercises were written after we had analyzed the data and seriously
                    considered what they show in the engineering context. These come from a variety
                    of engineering disciplines, and we expect that instructors will find them to be not
                    only useful for class assignments but also for lecture examples to many different
                    engineering audiences.

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

Teaching from the Text

                    A successful ISU classroom-tested, fast-paced introduction to applied engineering
                    statistics can be made by covering most of Chapters 1 through 9 in a single, three-
                    semester hour course (not including those topics designated as "optional" in section
viii Preface

              or subsection titles). More leisurely single-semester courses can be made, either by
              skipping the factorial analysis material in Section 4.3 and Chapter 8 altogether, or
              by covering only Chapters 1 through 6 and Sections 7.5 and 7.6, leaving the rest of
              the book for self-study as a working engineer finds need of the material.

                   Instructors who are more comfortable with a traditional "do more probability
              and do it first, and do factorials last" syllabus will find the additional traditional
              topics covered with engineering motivation (rather than appeal to cards, coins,
              and dice!) in Appendix A. For those instructors, an effective order of presentation
              is the following: Chapters 1 through 3, Appendices A.1 through A.3, Chapter 5,
              Chapter 6, Section 4.1, Section 9.1, Section 4.2, Section 9.2, Chapter 7, Section 4.3,
              and Chapter 8.



Ancillaries

                    Several types of ancillary material are available to support this text.

                          The CD packaged with the book provides PowerPointTM visuals and audio
                             presenting solutions for selected Section Exercises.

                          For instructors only, a complete solutions manual is available through the
                             local sales representative.

                          The publisher also maintains a web site supporting instruction using Basic
                             Engineering Data Collection and Analysis at www.brookscole.com.

                         At www.brookscole.com, using the Book Companions and Data Library links,
                    can be found the following:

                          Data sets for all exercises
                          MINITAB, JMP, and Microsoft Excel help for selected examples from

                             the book
                          Formula sheets in PDF and LaTeX formats
                          Lists of known errata



Acknowledgments

                    There are many who deserve thanks for their kind help with this project. People at
                    Duxbury Thomson Learning have been great. We especially thank Carolyn Crockett
                    for her encouragement and vision in putting this project together. Janet Hill has
                    been an excellent Production Editor. We appreciate the help of Seema Atwal with
                    the book's ancillaries, and are truly pleased with the design work overseen by
                    Vernon Boes.
                      Acknowledgments ix

     First class help has also come from outside of Duxbury Thomson Learning.
Martha Emry of Martha Emry Production Services has simply been dynamite to
work with. She is thorough, knowledgeable, possessed of excellent judgment and
unbelievably patient. Thanks Martha! And although he didn't work directly on this
project, we gratefully acknowledge the meticulous work of Chuck Lerch, who wrote
the solutions manual and provided the answer section for Statistics for Engineering
Problem Solving. We have borrowed liberally from his essentially flawless efforts
for answers and solutions carried over to this project. We are also grateful to Jimmy
Wright and Victor Chan for their careful work as error checkers. We thank Tom
Andrika for his important contributions to the development of the PowerPoint/audio
CD supplement. We thank Tiffany Lynn Hagemeyer for her help in preparing the
MINITAB, JMP, and Excel data files for download. Andrew Vardeman developed
the web site, providing JMP, MINITAB, and Excel help for the text, and we aprreci-
ate his contributions to this effort. John Ramberg, University of Arizona; V. A.
Samaranayake, University of Missouri at Rolla; Paul Joyce, University of Idaho;
James W. Hardin, Texas A & M; and Jagdish K. Patel, University of Missouri at
Rolla provided helpful reviews of this book at various stages of completion, and we
thank them.

     It is our hope that this book proves to be genuinely useful to both engineering
students and working engineers, and one that instructors find easy to build their
courses around. We'll be glad to receive comments and suggestions at our e-mail
addresses.

Steve Vardeman               J. Marcus Jobe
vardeman@iastate.edu  jobejm@muohio.edu
qqqqqqqqqqqqqqqqqqqqq

               Contents

                                                qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

          1 Introduction 1

                            1.1 Engineering Statistics: What and Why 1
                            1.2 Basic Terminology 5

                                  1.2.1 Types of Statistical Studies 5
                                  1.2.2 Types of Data 8
                                  1.2.3 Types of Data Structures 11
                            1.3 Measurement: Its Importance and Difficulty 14
                            1.4 Mathematical Models, Reality, and Data Analysis 19

                                                qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

          2 Data Collection 26

                            2.1 General Principles in the Collection of Engineering Data 26
                                  2.1.1 Measurement 26
                                  2.1.2 Sampling 28
                                  2.1.3 Recording 30

                            2.2 Sampling in Enumerative Studies 33
                            2.3 Principles for Effective Experimentation 38

                                  2.3.1 Taxonomy of Variables 38
                                  2.3.2 Handling Extraneous Variables 40
                                  2.3.3 Comparative Study 43
                                  2.3.4 Replication 44
                                  2.3.5 Allocation of Resources 46
                            2.4 Some Common Experimental Plans 47
                                  2.4.1 Completely Randomized Experiments 47
x
                                                                                                            Contents xi

                    2.4.2 Randomized Complete Block Experiments 50
                    2.4.3 Incomplete Block Experiments (Optional ) 53

             2.5 Preparing to Collect Engineering Data 56
                    2.5.1 A Series of Steps to Follow 56
                    2.5.2 Problem Definition 57
                    2.5.3 Study Definition 60
                    2.5.4 Physical Preparation 63

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

3 Elementary Descriptive Statistics 66

             3.1 Elementary Graphical and Tabular Treatment of
                     Quantitative Data 66
                    3.1.1 Dot Diagrams and Stem-and-Leaf Plots 66
                    3.1.2 Frequency Tables and Histograms 70
                    3.1.3 Scatterplots and Run Charts 74

             3.2 Quantiles and Related Graphical Tools 77
                    3.2.1 Quantiles and Quantile Plots 78
                    3.2.2 Boxplots 81
                    3.2.3 Q-Q Plots and Comparing Distributional Shapes 85

             3.3 Standard Numerical Summary Measures 92
                    3.3.1 Measures of Location 92
                    3.3.2 Measures of Spread 95
                    3.3.3 Statistics and Parameters 98
                    3.3.4 Plots of Summary Statistics 99
                    3.3.5 Summary Statistics and Personal Computer Software 102

             3.4 Descriptive Statistics for Qualitative and Count Data
                     (Optional ) 104
                    3.4.1 Numerical Summarization of Qualitative and Count Data 104
                    3.4.2 Bar Charts and Plots for Qualitative and Count Data 107

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

4 Describing Relationships Between Variables 123

             4.1 Fitting a Line by Least Squares 123
                    4.1.1 Applying the Least Squares Principle 124
                    4.1.2 The Sample Correlation and Coefficient of Determination 129
                    4.1.3 Computing and Using Residuals 132
                    4.1.4 Some Cautions 137
                    4.1.5 Computing 138

             4.2 Fitting Curves and Surfaces by Least Squares 141
                    4.2.1 Curve Fitting by Least Squares 141
                    4.2.2 Surface Fitting by Least Squares 149
                    4.2.3 Some Additional Cautions 158
xii Contents

              4.3 Fitted Effects for Factorial Data 162

              4.3.1  Fitted Effects for 2-Factor Studies 163               171
              4.3.2  Simpler Descriptions for Some Two-Way Data Sets        178
              4.3.3  Fitted Effects for Three-Way (and Higher) Factorials  184
              4.3.4  Simpler Descriptions of Some Three-Way Data Sets
              4.3.5  Special Devices for 2p Studies 187

              4.4 Transformations and Choice of Measurement Scale
                     (Optional ) 191

                     4.4.1 Transformations and Single Samples 192
                     4.4.2 Transformations and Multiple Samples 193
                     4.4.3 Transformations and Simple Structure in Multifactor Studies 194

              4.5 Beyond Descriptive Statistics 202

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

5 Probability: The Mathematics of Randomness 221

             5.1 (Discrete) Random Variables 221
                    5.1.1 Random Variables and Their Distributions 221
                    5.1.2 Discrete Probability Functions and Cumulative Probability
                             Functions 223
                    5.1.3 Summarization of Discrete Probability Distributions 228
                    5.1.4 The Binomial and Geometric Distributions 232
                    5.1.5 The Poisson Distributions 240

             5.2 Continuous Random Variables 244
                    5.2.1 Probability Density Functions and Cumulative Probability
                             Functions 245
                    5.2.2 Means and Variances for Continuous Distributions 249
                    5.2.3 The Normal Probability Distributions 251
                    5.2.4 The Exponential Distributions (Optional ) 257
                    5.2.5 The Weibull Distributions (Optional ) 260

             5.3 Probability Plotting (Optional ) 264
                    5.3.1 More on Normal Probability Plots 264
                    5.3.2 Probability Plots for Exponential and Weibull Distributions 270

             5.4 Joint Distributions and Independence 278
                    5.4.1 Describing Jointly Discrete Random Variables 279
                    5.4.2 Conditional Distributions and Independence for Discrete Random
                             Variables 283
                    5.4.3 Describing Jointly Continuous Random Variables (Optional ) 292
                    5.4.4 Conditional Distributions and Independence for Continuous Random
                             Variables (Optional ) 297

             5.5 Functions of Several Random Variables 302
                    5.5.1 The Distribution of a Function of Random Variables 302
                    5.5.2 Simulations to Approximate the Distribution of
                             U = g(X, Y, . . . , Z ) 304
                                                                                                          Contents xiii

                    5.5.3 Means and Variances for Linear Combinations of
                             Random Variables 307

                    5.5.4 The Propagation of Error Formulas 310
                    5.5.5 The Central Limit Effect 316

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

6 Introduction to Formal Statistical Inference 334

             6.1 Large-Sample Confidence Intervals for a Mean 334
                    6.1.1 A Large-n Confidence Interval for µ Involving  335
                    6.1.2 A Generally Applicable Large-n Confidence Interval for µ 338
                    6.1.3 Some Additional Comments Concerning Confidence Intervals 340

             6.2 Large-Sample Significance Tests for a Mean 345
                    6.2.1 Large-n Significance Tests for µ Involving  345
                    6.2.2 A Five-Step Format for Summarizing Significance Tests 350
                    6.2.3 Generally Applicable Large-n Significance Tests for µ 351
                    6.2.4 Significance Testing and Formal Statistical Decision
                             Making (Optional ) 353
                    6.2.5 Some Comments Concerning Significance Testing and Estimation 358

             6.3 One- and Two-Sample Inference for Means 361
                    6.3.1 Small-Sample Inference for a Single Mean 362
                    6.3.2 Inference for a Mean of Paired Differences 368
                    6.3.3 Large-Sample Comparisons of Two Means (Based on
                             Independent Samples) 374
                    6.3.4 Small-Sample Comparisons of Two Means (Based on Independent
                             Samples from Normal Distributions) 378

             6.4 One- and Two-Sample Inference for Variances 386
                    6.4.1 Inference for the Variance of a Normal Distribution 386
                    6.4.2 Inference for the Ratio of Two Variances (Based on Independent Samples
                             from Normal Distributions) 391

             6.5 One- and Two-Sample Inference for Proportions 399
                    6.5.1 Inference for a Single Proportion 400
                    6.5.2 Inference for the Difference Between Two Proportions (Based on
                             Independent Samples) 407

             6.6 Prediction and Tolerance Intervals 414
                    6.6.1 Prediction Intervals for a Normal Distribution 414
                    6.6.2 Tolerance Intervals for a Normal Distribution 420
                    6.6.3 Prediction and Tolerance Intervals Based on Minimum and/or Maximum
                             Values in a Sample 422

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

7 Inference for Unstructured Multisample Studies 443

             7.1 The One-Way Normal Model 443
                    7.1.1 Graphical Comparison of Several Samples of Measurement Data 444
xiv Contents

                                  7.1.2 The One-Way (Normal) Multisample Model, Fitted Values,
                                           and Residuals 447

                                  7.1.3 A Pooled Estimate of Variance for Multisample Studies 455
                                  7.1.4 Standardized Residuals 457

                            7.2 Simple Confidence Intervals in Multisample Studies 461
                                  7.2.1 Intervals for Means and for Comparing Means 461
                                  7.2.2 Intervals for General Linear Combinations of Means 464
                                  7.2.3 Individual and Simultaneous Confidence Levels 469

                            7.3 Two Simultaneous Confidence Interval Methods 471
                                  7.3.1 The Pillai-Ramachandran Method 471
                                  7.3.2 Tukey's Method 474

                            7.4 One-Way Analysis of Variance (ANOVA) 478
                                  7.4.1 Significance Testing and Multisample Studies 478
                                  7.4.2 The One-Way ANOVA F Test 479
                                  7.4.3 The One-Way ANOVA Identity and Table 482
                                  7.4.4 Random Effects Models and Analyses (Optional ) 487
                                  7.4.5 ANOVA-Based Inference for Variance Components (Optional ) 491

                            7.5 Shewhart Control Charts for Measurement Data 496
                                  7.5.1 Generalities about Shewhart Control Charts 496
                                  7.5.2 "Standards Given" x¯ Control Charts 500
                                  7.5.3 Retrospective x¯ Control Charts 504
                                  7.5.4 Control Charts for Ranges 509
                                  7.5.5 Control Charts for Standard Deviations 512
                                  7.5.6 Control Charts for Measurements and Industrial Process
                                           Improvement 515

                            7.6 Shewhart Control Charts for Qualitative and Count Data 518
                                  7.6.1 p Charts 518
                                  7.6.2 u Charts 523
                                  7.6.3 Common Control Chart Patterns and Special Checks 527

                                                qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

          8 Inference for Full and Fractional Factorial Studies 546

                            8.1 Basic Inference in Two-Way Factorials with
                                    Some Replication 546
                                  8.1.1 One-Way Methods in Two-Way Factorials 547
                                  8.1.2 Two-Way Factorial Notation and Definitions of Effects 551
                                  8.1.3 Individual Confidence Intervals for Factorial Effects 554
                                  8.1.4 Tukey's Method for Comparing Main Effects (Optional ) 562

                            8.2 p-Factor Studies with Two Levels for Each Factor 568
                                  8.2.1 One-Way Methods in p-Way Factorials 569
                                  8.2.2 p-Way Factorial Notation, Definitions of Effects, and Related Confidence
                                           Interval Methods 571
                                                                     Contents xv

8.2.3      2p Studies Without Replication and the Normal-Plotting of Fitted
8.2.4      Effects 577
8.2.5      Fitting and Checking Simplified Models in Balanced 2p Factorial Studies
           and a Corresponding Variance Estimate (Optional ) 580
           Confidence Intervals for Balanced 2p Studies under Few-Effects Models
           (Optional ) 587

8.3 Standard Fractions of Two-Level Factorials,

Part   I:  1  Fractions       591
           2

8.3.1 General Observations about Fractional Factorial Studies 592
                                 1                 2p
8.3.2      Choice  of  Standard  2  Fractions  of      Studies  596

8.3.3      Aliasing  in  the  Standard  1  Fractions   600
                                        2
8.3.4 Data Analysis for 2p-1 Fractional Factorials 603

8.3.5 Some Additional Comments 607

8.4 Standard Fractions of Two-Level Factorials Part II: General 2p-q

       Studies 612
       8.4.1 Using 2p-q Fractional Factorials 612
       8.4.2 Design Resolution 620
       8.4.3 Two-Level Factorials and Fractional Factorials in Blocks

                (Optional ) 625
       8.4.4 Some Additional Comments 631

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

9 Regression Analysis--Inference for Curve- and
    Surface-Fitting 650

9.1 Inference Methods Related to the Least Squares Fitting of a Line

       (Simple Linear Regression) 651

       9.1.1 The Simple Linear Regression Model, Corresponding Variance Estimate,
                and Standardized Residuals 651

       9.1.2 Inference for the Slope Parameter 658
       9.1.3 Inference for the Mean System Response for a Particular

                Value of x 661
       9.1.4 Prediction and Tolerance Intervals (Optional ) 666
       9.1.5 Simple Linear Regression and ANOVA 669
       9.1.6 Simple Linear Regression and Statistical Software 672

9.2 Inference Methods for General Least Squares Curve- and

Surface-Fitting (Multiple Linear Regression) 675

9.2.1      The Multiple Linear Regression Model, Corresponding Variance Estimate,
           and Standardized Residuals 675
9.2.2      Inference for the Parameters 0, 1, 2, . . . , k 682
9.2.3      Inference for the Mean System Response for a Particular Set of
           Values for x1, x2, . . . , xk 685
9.2.4      Prediction and Tolerance Intervals (Optional ) 689
9.2.5      Multiple Regression and ANOVA 691
xvi Contents

              9.3 Application of Multiple Regression in Response Surface
                     Problems and Factorial Analyses 697

                     9.3.1 Surface-Fitting and Response Surface Studies 698
                     9.3.2 Regression and Factorial Analyses 705

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

A More on Probability and Model Fitting 728

              A.1 More Elementary Probability 728

                     A.1.1 Basic Definitions and Axioms 729
                     A.1.2 Simple Theorems of Probability Theory 736
                     A.1.3 Conditional Probability and the Independence of Events 739

              A.2 Applications of Simple Probability to System Reliability
                      Prediction 746

                     A.2.1 Series Systems 746
                     A.2.2 Parallel Systems 747
                     A.2.3 Combination Series-Parallel Systems 749

              A.3 Counting 751
                     A.3.1 A Multiplication Principle, Permutations, and Combinations 751

              A.4 Probabilistic Concepts Useful in Survival Analysis 758
                     A.4.1 Survivorship and Force-of-Mortality Functions 759

              A.5 Maximum Likelihood Fitting of Probability Models and Related

              Inference Methods 765

              A.5.1  Likelihood Functions for Discrete Data and Maximum Likelihood
              A.5.2  Model Fitting 765
              A.5.3  Likelihood Functions for Continuous and Mixed Data and Maximum
                     Likelihood Model Fitting 774
                     Likelihood-Based Large-Sample Inference Methods 781

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

B Tables 785

                     Answers to Section Exercises 806
                     Index 825
1q q q q q q q q q q q q q q q q q q q q

          Introduction

          This chapter lays a foundation for all that follows: It contains a road map for the

                      study of engineering statistics. The subject is defined, its importance is described,
                      some basic terminology is introduced, and the important issue of measurement is
                      discussed. Finally, the role of mathematical models in achieving the objectives of
                      engineering statistics is investigated.

       qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

 1.1 Engineering Statistics: What and Why

                      In general terms, what a working engineer does is to design, build, operate, and/or
                      improve physical systems and products. This work is guided by basic mathematical
                      and physical theories learned in an undergraduate engineering curriculum. As the
                      engineer's experience grows, these quantitative and scientific principles work along-
                      side sound engineering judgment. But as technology advances and new systems and
                      products are encountered, the working engineer is inevitably faced with questions
                      for which theory and experience provide little help. When this happens, what is to
                      be done?

                            On occasion, consultants can be called in, but most often an engineer must
                      independently find out "what makes things tick." It is necessary to collect and
                      interpret data that will help in understanding how the new system or product works.
                      Without specific training in data collection and analysis, the engineer's attempts can
                      be haphazard and poorly conceived. Valuable time and resources are then wasted, and
                      sometimes erroneous (or at least unnecessarily ambiguous) conclusions are reached.
                      To avoid this, it is vital for a working engineer to have a toolkit that includes the
                      best possible principles and methods for gathering and interpreting data.

                            The goal of engineering statistics is to provide the concepts and methods needed
                      by an engineer who faces a problem for which his or her background does not serve
                      as a completely adequate guide. It supplies principles for how to efficiently acquire
                      and process empirical information needed to understand and manipulate engineering
                      systems.

                                                                                                                               1
2 Chapter 1 Introduction

Definition 1              Engineering statistics is the study of how best to

                               1. collect engineering data,
                               2. summarize or describe engineering data, and
                               3. draw formal inferences and practical conclusions on the basis of engi-

                                   neering data,

                          all the while recognizing the reality of variation.

                         To better understand the definition, it is helpful to consider how the elements of
                    engineering statistics enter into a real problem.

    Example 1             Heat Treating Gears

              Data        The article "Statistical Analysis: Mack Truck Gear Heat Treating Experiments"
       collection         by P. Brezler (Heat Treating, November, 1986) describes a simple application
                          of engineering statistics. A process engineer was faced with the question, "How
              Data        should gears be loaded into a continuous carburizing furnace in order to mini-
summarization             mize distortion during heat treating?" Various people had various semi-informed
                          opinions about how it should be done--in particular, about whether the gears
                          should be laid flat in stacks or hung on rods passing through the gear bores. But
                          no one really knew the consequences of laying versus hanging.

                               In order to settle the question, the engineer decided to get the facts--to
                          collect some data on "thrust face runout" (a measure of gear distortion) for gears
                          laid and gears hung. Deciding exactly how this data collection should be done
                          required careful thought. There were possible differences in gear raw material lots,
                          machinists and machines that produced the gears, furnace conditions at different
                          times and positions within the furnace, technicians and measurement devices that
                          would produce the final runout measurements, etc. The engineer did not want
                          these differences either to be mistaken for differences between the two loading
                          techniques or to unnecessarily cloud the picture. Avoiding this required care.

                               In fact, the engineer conducted a well-thought-out and executed study.
                          Table 1.1 shows the runout values obtained for 38 gears laid and 39 gears hung
                          after heat treating. In raw form, the runout values are hardly understandable.
                          They lack organization; it is not possible to simply look at Table 1.1 and tell
                          what is going on. The data needed to be summarized. One thing that was done
                          was to compute some numerical summaries of the data. For example, the process
                          engineer found

                                                            Mean laid runout = 12.6

                                                          Mean hung runout = 17.9
                       1.1 Engineering Statistics: What and Why 3

           Table 1.1
           Thrust Face Runouts (.0001 in.)

           Gears Laid               Gears Hung

           5, 8, 8, 9, 9,           7, 8, 8, 10, 10,
           9, 9, 10, 10, 10,        10, 10, 11, 11, 11,
           11, 11, 11, 11, 11,      12, 13, 13, 13, 15,
           11, 11, 12, 12, 12,      17, 17, 17, 17, 18,
           12, 13, 13, 13, 13,      19, 19, 20, 21, 21,
           14, 14, 14, 15, 15,      21, 22, 22, 22, 23,
           15, 15, 16, 17, 17,      23, 23, 23, 24, 27,
           18, 19, 27               27, 28, 31, 36

Variation  Further, a simple graphical summarization was made, as shown in Figure 1.1.
                From these summaries of the runouts, several points are obvious. One is that

           there is variation in the runout values, even within a particular loading method.
           Variability is an omnipresent fact of life, and all statistical methodology explicitly
           recognizes this. In the case of the gears, it appears from Figure 1.1 that there is
           somewhat more variation in the hung values than in the laid values.

                But in spite of the variability that complicates comparison between the load-
           ing methods, Figure 1.1 and the two group means also carry the message that the
           laid runouts are on the whole smaller than the hung runouts. By how much? One
           answer is

                                Mean hung runout - Mean laid runout = 5.3

                       Gears laid

           0  10                20  30          40

                       Runout (.0001 in.)

                       Gears hung

           0  10                20  30          40

                       Runout (.0001 in.)

              Figure 1.1 Dot diagrams of runouts
4 Chapter 1 Introduction

Example 1                 But how "precise" is this figure? Runout values are variable. So is there any
(continued )              assurance that the difference seen in the present means would reappear in further
                          testing? Or is it possibly explainable as simply "stray background noise"? Lay-
    Drawing               ing gears is more expensive than hanging them. Can one know whether the extra
  inferences              expense is justified?
  from data
                               These questions point to the need for methods of formal statistical inference
                          from data and translation of those inferences into practical conclusions. Meth-
                          ods presented in this text can, for example, be used to support the following
                          statements about hanging and laying gears:

                               1. One can be roughly 90% sure that the difference in long-run mean runouts
                                   produced under conditions like those of the engineer's study is in the range

                                                                         3.2 to 7.4

                               2. One can be roughly 95% sure that 95% of runouts for gears laid under
                                   conditions like those of the engineer's study would fall in the range

                                                                        3.0 to 22.2

                               3. One can be roughly 95% sure that 95% of runouts for gears hung under
                                   conditions like those of the engineer's study would fall in the range

                                                                         .8 to 35.0

                               These are formal quantifications of what was learned from the study of laid
                          and hung gears. To derive practical benefit from statements like these, the process
                          engineer had to combine them with other information, such as the consequences
                          of a given amount of runout and the costs for hanging and laying gears, and had to
                          apply sound engineering judgment. Ultimately, the runout improvement was great
                          enough to justify some extra expense, and the laying method was implemented.

                   The example shows how the elements of statistics were helpful in solving an
              engineer's problem. Throughout this text, the intention is to emphasize that the
              topics discussed are not ends in themselves, but rather tools that engineers can use
              to help them do their jobs effectively.

Section 1 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Explain why engineering practice is an inherently     3. Describe the difference between descriptive and
   statistical enterprise.                                  (formal) inferential statistics.

2. Explain why the concept of variability has a central
   place in the subject of engineering statistics.
              1.2 Basic Terminology 5

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

1.2 Basic Terminology

                    Engineering statistics requires learning both new words and new technical mean-
                    ings for familiar words. This section introduces some common jargon for types of
                    statistical studies, types of data that can arise in those studies, and types of structures
                    those data can have.

1.2.1         Types of Statistical Studies

              When an engineer sets about to gather data, he or she must decide how active to be.
              Will the engineer turn knobs and manipulate process variables or simply let things
              happen and try to record the salient features?

Definition 2  An observational study is one in which the investigator's role is basically
              passive. A process or phenomenon is watched and data are recorded, but there
              is no intervention on the part of the person conducting the study.

Definition 3  An experimental study (or, more simply, an experiment) is one in which the
              investigator's role is active. Process variables are manipulated, and the study
              environment is regulated.

Inferring          Most real statistical studies have both observational and experimental features,
causality     and these two definitions should be thought of as representing idealized opposite
              ends of a continuum. On this continuum, the experimental end usually provides
              the most efficient and reliable ways to collect engineering data. It is typically
              much quicker to manipulate process variables and watch how a system responds
              to the changes than to passively observe, hoping to notice something interesting or
              revealing.

                   In addition, it is far easier and safer to infer causality from an experiment than
              from an observational study. Real systems are complex. One may observe several
              instances of good process performance and note that they were all surrounded by
              circumstances X without being safe in assuming that circumstances X cause good
              process performance. There may be important variables in the background that are
              changing and are the true reason for instances of favorable system behavior. These
              so-called lurking variables may govern both process performance and circum-
              stances X. Or it may simply be that many variables change haphazardly without
              appreciable impact on the system and that by chance, during a limited period of
              observation, some of these happen to produce X at the same time that good perfor-
              mance occurs. In either case, an engineer's efforts to create X as a means of making
              things work well will be wasted effort.
6 Chapter 1 Introduction

                                       On the other hand, in an experiment where the environment is largely regulated
                                  except for a few variables the engineer changes in a purposeful way, an inference
                                  of causality is much stronger. If circumstances created by the investigator are con-
                                  sistently accompanied by favorable results, one can be reasonably sure that they
                                  caused the favorable results.

Example 2     Pelletizing Hexamine Powder

              Cyr, Ellson, and Rickard attacked the problem of reducing the fraction of non-
              conforming fuel pellets produced in the compression of a raw hexamine powder
              in a pelletizing machine. There were many factors potentially influencing the
              percentage of nonconforming pellets: among others, Machine Speed, Die Fill
              Level, Percent Paraffin added to the hexamine, Room Temperature, Humidity
              at manufacture, Moisture Content, "new" versus "reground" Composition of the
              mixture being pelletized, and the Roughness of the chute entered by the freshly
              stamped pellets. Correlating these many factors to process performance through
              passive observation was hopeless.

                   The students were, however, able to make significant progress by conducting
              an experiment. They chose three of the factors that seemed most likely to be
              important and purposely changed their levels while holding the levels of other
              factors as close to constant as possible. The important changes they observed
              in the percentage of acceptable fuel pellets were appropriately attributed to the
              influence of the system variables they had manipulated.

                   Besides the distinction between observational and experimental statistical stud-
              ies, it is helpful to distinguish between studies on the basis of the intended breadth
              of application of the results. Two relevant terms, popularized by the late W. E.
              Deming, are defined next:

Definition 4  An enumerative study is one in which there is a particular, well-defined,
              finite group of objects under study. Data are collected on some or all of these
              objects, and conclusions are intended to apply only to these objects.

Definition 5  An analytical study is one in which a process or phenomenon is investigated
              at one point in space and time with the hope that the data collected will
              be representative of system behavior at other places and times under similar
              conditions. In this kind of study, there is rarely, if ever, a particular well-defined
              group of objects to which conclusions are thought to be limited.

                   Most engineering studies tend to be of the second type, although some important
              engineering applications do involve enumerative work. One such example is the
              1.2 Basic Terminology 7

              reliability testing of critical components--e.g., for use in a space shuttle. The interest
              is in the components actually in hand and how well they can be expected to perform
              rather than on any broader problem like "the behavior of all components of this
              type." Acceptance sampling (where incoming lots are checked before taking formal
              receipt) is another important kind of enumerative study. But as indicated, most
              engineering studies are analytical in nature.

Example 2     The students working on the pelletizing machine were not interested in any partic-
(continued )  ular batch of pellets, but rather in the question of how to make the machine work
              effectively. They hoped (or tacitly assumed) that what they learned about making
              fuel pellets would remain valid at later times, at least under shop conditions like
              those they were facing. Their experimental study was analytical in nature.

                   Particularly when discussing enumerative studies, the next two definitions are
              helpful.

Definition 6  A population is the entire group of objects about which one wishes to gather
              information in a statistical study.

Definition 7  A sample is the group of objects on which one actually gathers data. In the
              case of an enumerative investigation, the sample is a subset of the population
              (and can in some cases include the entire population).

              Figure 1.2 shows the relationship between a population and a sample. If a crate of
              100 machine parts is delivered to a loading dock and 5 are examined in order to
              verify the acceptability of the lot, the 100 parts constitute the population of interest,
              and the 5 parts make up a (single) sample of size 5 from the population. (Notice the
              word usage here: There is one sample, not five samples.)

              Sample

                            Population
              Figure 1.2 Population and sample
8 Chapter 1 Introduction

                   There are several ways in which the meanings of the words population and
              sample are often extended. For one, it is common to use them to refer to not only
              objects under study but also data values associated with those objects. For example,
              if one thinks of Rockwell hardness values associated with 100 crated machine parts,
              the 100 hardness values might be called a population (of numbers). Five hardness
              values corresponding to the parts examined in acceptance sampling could be termed
              a sample from that population.

Example 2                 Cyr, Ellson, and Rickard identified eight different sets of experimental conditions
(continued )              under which to run the pelletizing machine. Several production runs of fuel pellets
                          were made under each set of conditions, and each of these produced its own
                          percentage of conforming pellets. These eight sets of percentages can be referred
                          to as eight different samples (of numbers).

                   Also, although strictly speaking there is no concrete population being investi-
              gated in an analytical study, it is common to talk in terms of a conceptual population
              in such cases. Phrases like "the population consisting of all widgets that could be
              produced under these conditions" are sometimes used. We dislike this kind of lan-
              guage, believing that it encourages fuzzy thinking. But it is a common usage, and it
              is supported by the fact that typically the same mathematics is used when drawing
              inferences in enumerative and analytical contexts.

1.2.2         Types of Data

              Engineers encounter many types of data. One useful distinction concerns the degree
              to which engineering data are intrinsically numerical.

Definition 8              Qualitative or categorical data are the values of basically nonnumerical char-
                          acteristics associated with items in a sample. There can be an order associated
                          with qualitative data, but aggregation and counting are required to produce
                          any meaningful numerical values from such data.

              Consider again 5 machine parts constituting a sample from 100 crated parts. If each
              part can be classified into one of the (ordered) categories (1) conforming, (2) rework,
              and (3) scrap, and one knows the classifications of the 5 parts, one has 5 qualitative
              data points. If one aggregates across the 5 and finds 3 conforming, 1 reworkable, and
              1 scrap, then numerical summaries have been derived from the original categorical
              data by counting.

                   In contrast to categorical data are numerical data.
              1.2 Basic Terminology 9

Definition 9  Quantitative or numerical data are the values of numerical characteristics
              associated with items in a sample. These are typically either counts of the
              number of occurrences of a phenomenon of interest or measurements of
              some physical property of the items.

              Returning to the crated machine parts, Rockwell hardness values for 5 selected
              parts would constitute a set of quantitative measurement data. Counts of visible
              blemishes on a machined surface for each of the 5 selected parts would make up a
              set of quantitative count data.

                   It is sometimes convenient to act as if infinitely precise measurement were
              possible. From that perspective, measured variables are continuous in the sense
              that their sets of possible values are whole (continuous) intervals of numbers. For
              example, a convenient idealization might be that the Rockwell hardness of a ma-
              chine part can lie anywhere in the interval (0, ). But of course this is only an
              idealization. All real measurements are to the nearest unit (whatever that unit may
              be). This is becoming especially obvious as measurement instruments are increas-
              ingly equipped with digital displays. So in reality, when looked at under a strong
              enough magnifying glass, all numerical data (both measured and count alike) are
              discrete in the sense that they have isolated possible values rather than a continuum
              of available outcomes. Although (0, ) may be mathematically convenient and
              completely adequate for practical purposes, the real set of possible values for the
              measured Rockwell hardness of a machine part may be more like {.1, .2, .3, . . .}
              than like (0, ).

                   Well-known conventional wisdom is that measurement data are preferable to
              categorical and count data. Statistical methods for measurements are simpler and
              more informative than methods for qualitative data and counts. Further, there is
              typically far more to be learned from appropriate measurements than from qualitative
              data taken on the same physical objects. However, this must sometimes be balanced
              against the fact that measurement can be more time-consuming (and thus expensive)
              than the gathering of qualitative data.

Example 3     Pellet Mass Measurements

              As a preliminary to their experimental study on the pelletizing process (discussed
              in Example 2), Cyr, Ellson, and Rickard collected data on a number of aspects
              of machine behavior. Included was the mass of pellets produced under standard
              operating conditions. Because a nonconforming pellet is typically one from which
              some material has broken off during production, pellet mass is indicative of
              system performance. Informal requirements for (specifications on) pellet mass
              were from 6.2 to 7.0 grams.
10 Chapter 1 Introduction

Example 3                       Information on 200 pellets was collected. The students could have simply
(continued )               observed and recorded whether or not a given pellet had mass within the specifi-
                           cations, thereby producing qualitative data. Instead, they took the time necessary
                           to actually measure pellet mass to the nearest .1 gram--thereby collecting mea-
                           surement data. A graphical summary of their findings is shown in Figure 1.3.

                           Frequency  20
                                      10

                                          3  4  5         6  7                    8

                                                Mass (g)

                                             Figure 1.3 Pellet mass measurements

                                Notice that one can recover from the measurements the conformity/noncon-
                           formity information--about 28.5% (57 out of 200) of the pellets had masses that
                           did not meet specifications. But there is much more in Figure 1.3 besides this.
                           The shape of the display can give insights into how the machine is operating and
                           the likely consequences of simple modifications to the pelletizing process. For
                           example, note the truncated or chopped-off appearance of the figure. Masses do
                           not trail off on the high side as they do on the low side. The students reasoned that
                           this feature of their data had its origin in the fact that after powder is dispensed
                           into a die, it passes under a paddle that wipes off excess material before a cylinder
                           compresses the powder in the die. The amount initially dispensed to a given die
                           may have a fairly symmetric mound-shaped distribution, but the paddle probably
                           introduces the truncated feature of the display.

                                Also, from the numerical data displayed in Figure 1.3, one can find a per-
                           centage of pellet masses in any interval of interest, not just the interval [6.2, 7.0].
                           And by mentally sliding the figure to the right, it is even possible to project the
                           likely effects of increasing die size by various amounts.

                   It is typical in engineering studies to have several response variables of interest.
              The next definitions present some jargon that is useful in specifying how many
              variables are involved and how they are related.
               1.2 Basic Terminology 11

Definition 10  Univariate data arise when only a single characteristic of each sampled item
               is observed.

Definition 11  Multivariate data arise when observations are made on more than one
               characteristic of each sampled item. A special case of this involves two
               characteristics--bivariate data.

Definition 12  When multivariate data consist of several determinations of basically the same
               characteristic (e.g., made with different instruments or at different times),
               the data are called repeated measures data. In the special case of bivariate
               responses, the term paired data is used.

               It is important to recognize the multivariate character of data when it is present. Hav-
               ing Rockwell hardness values for 5 of 100 crated machine parts and determinations
               of the percentage of carbon for 5 other parts is not at all equivalent to having both
               hardness and carbon content values for a single sample of 5 parts. There are two
               samples of 5 univariate data points in the first case and a single sample of 5 bivariate
               data points in the second. The second situation is preferable to the first, because it
               allows analysis and exploitation of any relationships that might exist between the
               variables Hardness and Percent Carbon.

Example 4      Paired Distortion Measurements

               In the furnace-loading scenario discussed in Example 1, radial runout measure-
               ments were actually made on all 38 + 39 = 77 gears both before and after heat
               treating. (Only after-treatment values were given in Table 1.1.) Therefore, the
               process engineer had two samples (of respective sizes 38 and 39) of paired data.
               Because of the pairing, the engineer was in the position of being able (if de-
               sired) to analyze how post-treatment distortion was correlated with pretreatment
               distortion.

1.2.3          Types of Data Structures

               Statistical engineering studies are sometimes conducted to compare process perfor-
               mance at one set of conditions to a stated standard. Such investigations involve only
               one sample. But it is far more common for several sets of conditions to be compared
               with each other, in which case several samples are involved. There are a variety of
12 Chapter 1 Introduction

                                  standard notions of structure or organization for multisample studies. Two of these
                                  are briefly discussed in the remainder of this section.

Definition 13  A (complete) factorial study is one in which several process variables (and
               settings of each) are identified as being of interest, and data are collected under
               each possible combination of settings of the process variables. The process
               variables are usually called factors, and the settings of each variable that are
               studied are termed levels of the factor.

               For example, suppose there are four factors of interest--call them A, B, C, and D for

               convenience. If A has 3 levels, B has 2, C has 2, and D has 4, a study that includes
               samples collected under each of the 3 × 2 × 2 × 4 = 48 different possible sets of
               conditions would be called a 3 × 2 × 2 × 4 factorial study.

Example 2      Experimentation with the pelletizing machine produced data with a 2 × 2 × 2
(continued )   (or 23) factorial structure. The factors and respective levels studied were

                    Die Volume low volume vs. high volume
                    Material Flow current method vs. manual filling
                    Mixture Type no binding agent vs. with binder

               Combining these then produced eight sets of conditions under which data were
               collected (see Table 1.2).

               Table 1.2
               Combinations in a 23 Factorial Study

               Condition Number Volume Flow          Mixture

               1  low   current                      no binder
                                                     no binder
               2  high  current                      no binder
                                                     no binder
               3  low   manual                       binder
                                                     binder
               4  high  manual                       binder
                                                     binder
               5  low   current

               6  high  current

               7  low   manual

               8  high  manual

                    When many factors and/or levels are involved, the number of samples in a
               full factorial study quickly reaches an impractical size. Engineers often find that
                                                                                   1.2 Basic Terminology 13

               they want to collect data for only some of the combinations that would make up a
               complete factorial study.

Definition 14  A fractional factorial study is one in which data are collected for only some
               of the combinations that would make up a complete factorial study.

                    One cannot hope to learn as much about how a response is related to a given set
               of factors from a fractional factorial study as from the corresponding full factorial
               study. Some information must be lost when only part of all possible sets of conditions
               are studied. However, some fractional factorial studies will be potentially more
               informative than others. If only a fixed number of samples can be taken, which
               samples to take is an issue that needs careful consideration. Sections 8.3 and 8.4
               discuss fractional factorials in detail, including how to choose good ones, taking
               into account what part of the potential information from a full factorial study they
               can provide.

Example 2      The experiment actually carried out on the pelletizing process was, as indicated
(continued )   in Table 1.2, a full factorial study. Table 1.3 lists four experimental combinations,
               forming a well-chosen half of the eight possible combinations. (These are the
               combinations numbered 2, 3, 5, and 8 in Table 1.2.)

                                                          Table 1.3
                                                          Half of the 23 Factorial

                                                          Volume Flow Mixture

                                                          high  current no binder

                                                          low   manual no binder

                                                          low   current binder

                                                          high  manual binder

Section 2 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Describe a situation in your field where an observa-        3. What kind of information can be derived from
   tional study might be used to answer a question of             a single sample of n bivariate data points (x, y)
   real importance. Describe another situation where              that can't be derived from two separate sam-
   an experiment might be used.                                   ples of, respectively, n data points x and n data
                                                                  points y?
2. Describe two different contexts in your field where,
   respectively, qualitative and quantitative data might       4. Describe a situation in your field where paired data
   arise.                                                         might arise.
14 Chapter 1 Introduction

5. Consider a study of making paper airplanes, where         a full factorial and then a fractional factorial data
   two different Designs (say, delta versus t wing), two     structure that might arise from such a study.
   different Papers (say, construction versus typing),
   and two different Loading Conditions (with a paper     6. Explain why it is safer to infer causality from an
   clip versus without a paper clip) are of interest in      experiment than from an observational study.
   terms of their effects on flight distance. Describe

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

1.3 Measurement: Its Importance and Difficulty

                    Success in statistical engineering studies requires the ability to measure. For some
                    physical properties like length, mass, temperature, and so on, methods of measure-
                    ment are commonplace and obvious. Often, the behavior of an engineering system
                    can be adequately characterized in terms of such properties. But when it cannot,
                    engineers must carefully define what it is about the system that needs observing and
                    then apply ingenuity to create a suitable method of measurement.

Example 5                  Measuring Brittleness

                           A senior design class in metallurgical engineering took on the project of helping
                           a manufacturer improve the performance of a spike-shaped metal part. In its
                           intended application, this part needed to be strong but very brittle. When meeting
                           an obstruction in its path, it had to break off rather than bend, because bending
                           would in turn cause other damage to the machine in which the part functions.

                                As the class planned a statistical study aimed at finding what variables
                           of manufacture affect part performance, the students came to realize that the
                           company didn't have a good way of assessing part performance. As a necessary
                           step in their study, they developed a measuring device. It looked roughly as
                           in Figure 1.4. A swinging arm with a large mass at its end was brought to a

                               60°                        Metal part
                                       40°

                           Angle
                           past vertical 20°

                           Figure 1.4 A device for measuring brittleness
                                                   1.3 Measurement: Its Importance and Difficulty 15

                   horizontal position, released, and allowed to swing through a test part firmly
                   fixed in a vertical position at the bottom of its arc of motion. The number of
                   degrees past vertical that the arm traversed after impact with the part provided an
                   effective measure of brittleness.

Example 6          Measuring Wood Joint Strength

                   Dimond and Dix wanted to conduct a factorial study comparing joint strengths
                   for combinations of three different woods and three glues. They didn't have
                   access to strength-testing equipment and so invented their own. To test a joint,
                   they suspended a large container from one of the pieces of wood involved and
                   poured water into it until the weight was sufficient to break the joint. Knowing
                   the volume of water poured into the container and the density of water, they could
                   determine the force required to break the joint.

                        Regardless of whether an engineer uses off-the-shelf technology or must fabri-
                   cate a new device, a number of issues concerning measurement must be considered.
                   These include validity, measurement variation/error, accuracy, and precision.

Definition 15      A measurement or measuring method is called valid if it usefully or appro-
         Validity  priately represents the feature of an object or system that is of engineering
                   importance.

Measurement        It is impossible to overstate the importance of facing the question of measurement
            error  validity before plunging ahead in a statistical engineering study. Collecting engi-
                   neering data costs money. Expending substantial resources collecting data, only to
                   later decide they don't really help address the problem at hand, is unfortunately all
                   too common.

                        The point was made in Section 1.1 that when using data, one is quickly faced
                   with the fact that variation is omnipresent. Some of that variation comes about
                   because the objects studied are never exactly alike. But some of it is due to the fact
                   that measurement processes also have their own inherent variability. Given a fine
                   enough scale of measurement, no amount of care will produce exactly the same
                   value over and over in repeated measurement of even a single object. And it is naive
                   to attribute all variation in repeat measurements to bad technique or sloppiness. (Of
                   course, bad technique and sloppiness can increase measurement variation beyond
                   that which is unavoidable.)

                        An exercise suggested by W. J. Youden in his book Experimentation and Mea-
                   surement is helpful in making clear the reality of measurement error. Consider
                   measuring the thickness of the paper in this book. The technique to be used is as
16 Chapter 1 Introduction

                                  follows. The book is to be opened to a page somewhere near the beginning and one
                                  somewhere near the end. The stack between the two pages is to be grasped firmly
                                  between the thumb and index finger and stack thickness read to the nearest .1 mm
                                  using an ordinary ruler. Dividing the stack thickness by the number of sheets in the
                                  stack and recording the result to the nearest .0001 mm will then produce a thickness
                                  measurement.

Example 7          Book Paper Thickness Measurements

                   Presented below are ten measurements of the thickness of the paper in Box,
                   Hunter, and Hunter's Statistics for Experimenters made one semester by engi-
                   neering students Wendel and Gulliver.

                         Wendel:    .0807, .0826, .0854, .0817, .0824,
                         Gulliver:  .0799, .0812, .0807, .0816, .0804

                                    .0972, .0964, .0978, .0971, .0960,
                                    .0947, .1200, .0991, .0980, .1033

                   Figure 1.5 shows a graph of these data and clearly reveals that even repeated
                   measurements by one person on one book will vary and also that the patterns of
                   variation for two different individuals can be quite different. (Wendel's values
                   are both smaller and more consistent than Gulliver's.)

                                    Wendel

                   .080  .090       .100            .110                          .120

                                    Thickness (mm)

                                    Gulliver

                   .080  .090       .100            .110                          .120

                                    Thickness (mm)

                         Figure 1.5 Dot diagrams of paper thickness measurements

                        The variability that is inevitable in measurement can be thought of as having
                   both internal and external components.

Definition 16      A measurement system is called precise if it produces small variation in
        Precision  repeated measurement of the same object.
                  1.3 Measurement: Its Importance and Difficulty 17

                  Precision is the internal consistency of a measurement system; typically, it can be
                  improved only with basic changes in the configuration of the system.

Example 7         Ignoring the possibility that some property of Gulliver's book was responsible for
(continued )      his values showing more spread than those of Wendel, it appears that Wendel's
                  measuring technique was more precise than Gulliver's.

                       The precision of both students' measurements could probably have been
                  improved by giving each a binder clip and a micrometer. The binder clip would
                  provide a relatively constant pressure on the stacks of pages being measured,
                  thereby eliminating the subjectivity and variation involved in grasping the stack
                  firmly between thumb and index finger. For obtaining stack thickness, a microm-
                  eter is clearly a more precise instrument than a ruler.

                       Precision of measurement is important, but for many purposes it alone is not
                  adequate.

Definition 17     A measurement system is called accurate (or sometimes, unbiased) if on
        Accuracy  average it produces the true or correct value of a quantity being measured.

                  Accuracy is the agreement of a measuring system with some external standard.
                  It is a property that can typically be changed without extensive physical change
                  in a measurement method. Calibration of a system against a standard (bringing
                  it in line with the standard) can be as simple as comparing system measurements
                  to a standard, developing an appropriate conversion scheme, and thereafter using
                  converted values in place of raw readings from the system.

Example 7         It is unknown what the industry-standard measuring methodology would have
(continued )      produced for paper thickness in Wendel's copy of the text. But for the sake of
                  example, suppose that a value of .0850 mm/sheet was appropriate. The fact that
                  Wendel's measurements averaged about .0817 mm/sheet suggests that her future
                  accuracy might be improved by proceeding as before but then multiplying any
                  figure obtained by the ratio of .0850 to .0817--i.e., multiplying by 1.04.

                       Maintaining the U.S. reference sets for physical measurement is the business of
                  the National Institute of Standards and Technology. It is important business. Poorly
                  calibrated measuring devices may be sufficient for local purposes of comparing
                  local conditions. But to establish the values of quantities in any absolute sense, or
                  to expect local values to have meaning at other places and other times, it is essential
                  to calibrate measurement systems against a constant standard. A millimeter must be
                  the same today in Iowa as it was last week in Alaska.

                       The possibility of bias or inaccuracy in measuring systems has at least two im-
                  portant implications for planning statistical engineering studies. First, the fact that
18 Chapter 1 Introduction

Accuracy and      measurement systems can lose accuracy over time demands that their performance
     statistical  be monitored over time and that they be recalibrated as needed. The well-known
        studies   phenomenon of instrument drift can ruin an otherwise flawless statistical study.
                  Second, whenever possible, a single system should be used to do all measuring. If
                  several measurement devices or technicians are used, it is hard to know whether the
                  differences observed originate with the variables under study or from differences in
                  devices or technician biases. If the use of several measurement systems is unavoid-
                  able, they must be calibrated against a standard (or at least against each other). The
                  following example illustrates the role that human differences can play.

Example 8                  Differences Between Technicians in Their Use of a Gauge

                           Cowan, Renk, Vander Leest, and Yakes worked with a company on the monitoring
                           of a critical dimension of a high-precision metal part produced on a computer-
                           controlled lathe. They encountered large, initially unexplainable variation in this
                           dimension between different shifts at the plant. This variation was eventually
                           traced not to any real shift-to-shift difference in the parts but to an instability
                           in the company's measuring system. A single gauge was in use on all shifts,
                           but different technicians used it quite differently when measuring the critical
                           dimension. The company needed to train the technicians in a single, standardized
                           method of using the gauge.

                       An analogy that is helpful in understanding the difference between precision
                  and accuracy involves comparing measurement to target shooting. In target shoot-
                  ing, one can be on or off target (accurate or inaccurate) with a small or large cluster
                  of shots (showing precision or imprecision). Figure 1.6 illustrates this analogy.

                           Not accurate,  Accurate,
                           not precise    not precise

                           Not accurate,  Accurate,
                           precise        precise

                           Figure 1.6 Measurement / Target shooting analogy
                                                1.4 Mathematical Models, Reality, and Data Analysis 19

                        Good measurement is hard work, but without it data collection is futile. To
                   make progress, engineers must obtain valid measurements, taken by methods whose
                   precision and accuracy are sufficient to let them see important changes in system
                   behavior. Usually, this means that measurement inaccuracy and imprecision must
                   be an order of magnitude smaller than the variation in measured response caused by
                   those changes.

Section 3 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Why might it be argued that in terms of producing       Explain which of the three aspects of measure-
   useful measurements, one must deal first with the       ment quality--validity, precision, and accuracy--
   issue of validity, then the issue of precision, and     this averaging of many measurements can be ex-
   only then the issue of accuracy?                        pected to improve and which it cannot.

2. Often, in order to evaluate a physical quantity      3. Explain the importance of the stability of the mea-
   (for example, the mean yield of a batch chemi-          surement system to the real-world success of a sta-
   cal process run according to some standard plant        tistical engineering study.
   operating procedures), a large number of measure-
   ments of the quantity are made and then averaged.

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

1.4 Mathematical Models,
       Reality, and Data Analysis

Mathematical       This is not a book on mathematics. Nevertheless, it contains a fair amount of
   models and      mathematics (that most readers will find to be reasonably elementary--if unfamiliar
          reality  and initially puzzling). Therefore, it seems wise to try to put the mathematical content
                   of the book in perspective early. In this section, the relationships of mathematics to
                   the physical world and to engineering statistics are discussed.

                        Mathematics is a construct of the human mind.While it is of interest to some
                   people in its own right, engineers generally approach mathematics from the point of
                   view that it can be useful in describing and predicting how physical systems behave.
                   Indeed, although they exist only in our minds, mathematical theories are guides in
                   every branch of modern engineering.

                        Throughout this text, we will frequently use the phrase mathematical model.

Definition 18      A mathematical model is a description or summarization of salient features of
                   a real-world system or phenomenon in terms of symbols, equations, numbers,
                   and the like.

                   Mathematical models are themselves not reality, but they can be extremely effective
                   descriptions of reality. This effectiveness hinges on two somewhat opposing prop-
                   erties of a mathematical model: (1) its degree of simplicity and (2) its predictive
20 Chapter 1 Introduction

                ability. The most powerful mathematical models are those that simultaneously are
                simple and generate good predictions. A model's simplicity allows one to maneuver
                within its framework, deriving mathematical consequences of basic assumptions that
                translate into predictions of process behavior. When these are empirically correct,
                one has an effective engineering tool.

                     The elementary "laws" of mechanics are an outstanding example of effective
                mathematical modeling. For example, the simple mathematical statement that the
                acceleration due to gravity is constant,

                                                              a=g

                yields, after one easy mathematical maneuver (an integration), the prediction that
                beginning with 0 velocity, after a time t in free fall an object will have velocity

                                                             v = gt

                And a second integration gives the prediction that beginning with 0 velocity, a time t
                in free fall produces displacement

                                                            d = 1 gt2
                                                                  2

                The beauty of this is that for most practical purposes, these easy predictions are quite

                adequate. They agree well with what is observed empirically and can be counted

                on as an engineer designs, builds, operates, and/or improves physical processes or

                products.

Mathematics                But then, how does the notion of mathematical modeling interact with the
and statistics
                subject of engineering statistics? There are several ways. For one, data collection

                and analysis are essential in fitting or estimating parameters of mathematical

                models. To understand this point, consider again the example of a body in free fall.

                If one postulates that the acceleration due to gravity is constant, there remains the

                question of what numerical value that constant should have. The parameter g must

                be evaluated before the model can be used for practical purposes. One does this by

                gathering data and using them to estimate the parameter.

                           A standard first college physics lab has traditionally been to empirically evalu-

                ate g. The method often used is to release a steel bob down a vertical wire running

                through a hole in its center and allowing 60-cycle current to arc from the bob through

                a paper tape to another vertical wire, burning the tape slightly with every arc. A

                schematic diagram of the apparatus used is shown in Figure 1.7. The vertical posi-

                tions      of  the  burn  marks  are  bob  positions  at  intervals  of  1   of  a  second.  Table  1.4
                                                                                         60
                gives measurements of such positions. (We are grateful to Dr. Frank Peterson of

                the ISU Physics and Astronomy Department for supplying the tape.) Plotting the

                bob positions in the table at equally spaced intervals produces the approximately

                quadratic plot shown in Figure 1.8. Picking a parabola to fit the plotted points in-

                volves identifying an appropriate value for g. A method of curve fitting (discussed
                in Chapter 4) called least squares produces a value for g of 9.79m/sec2, not far from
                the commonly quoted value of 9.8m/sec2.
    1.4 Mathematical Models, Reality, and Data Analysis 21

                                       Paper tape

              Sliding Arc
              metal
              bob

           Bare
           wire Bare

                                 wire

                             AC Generator
    Figure 1.7 A device for measuring g

Table 1.4
Measured Displacements of a Bob in Free Fall

Point Number Displacement (mm) Point Number       Displacement (mm)

1   .8                                        13          223.8
                                                          260.0
2   4.8                                       14          299.2
                                                          340.5
3   10.8                                      15          385.0
                                                          432.2
4   20.1                                      16          481.8
                                                          534.2
5   31.9                                      17          589.8
                                                          647.7
6   45.9                                      18          708.8

7   63.3                                      19

8   83.1                                      20

9   105.8                                     21

10  131.3                                     22

11  159.5                                     23

12  190.5

     Notice that (at least before Newton) the data in Table 1.4 might also have been
used in another way. The parabolic shape of the plot in Figure 1.8 could have
suggested the form of an appropriate model for the motion of a body in free fall.
That is, a careful observer viewing the plot of position versus time should conclude
that there is an approximately quadratic relationship between position and time (and
22 Chapter 1 Introduction

                           Displacement (mm)  700
                                              600
                                              500
                                              400
                                              300
                                              200
                                              100

                                                   Time  (  1   second)
                                                            60

                                                   Figure 1.8 Bob positions in free fall

               from that proceed via two differentiations to the conclusion that the acceleration
               due to gravity is roughly constant). This text is full of examples of how helpful it
               can be to use data both to identify potential forms for empirical models and to then
               estimate parameters of such models (preparing them for use in prediction).

                    This discussion has concentrated on the fact that statistics provides raw material
               for developing realistic mathematical models of real systems. But there is another
               important way in which statistics and mathematics interact. The mathematical theory
               of probability provides a framework for quantifying the uncertainty associated with
               inferences drawn from data.

Definition 19              Probability is the mathematical theory intended to describe situations and
                           phenomena that one would colloquially describe as involving chance.

               If, for example, five students arrive at the five different laboratory values of g,

                                               9.78, 9.82, 9.81, 9.78, 9.79

               questions naturally arise as to how to use them to state both a best value for g
               and some measure of precision for the value. The theory of probability provides
               guidance in addressing these issues. Material in Chapter 6 shows that probability
                                                                                          Chapter 1 Exercises 23

                  considerations support using the class average of 9.796 to estimate g and attaching
                  to it a precision on the order of plus or minus .02m/sec2.

                       We do not assume that the reader has studied the mathematics of probability,
                  so this text will supply a minimal introduction to the subject. But do not lose sight
                  of the fact that probability is not statistics--nor vice versa. Rather, probability is a
                  branch of mathematics and a useful subject in its own right. It is met in a statistics
                  course as a tool because the variation that one sees in real data is closely related
                  conceptually to the notion of chance modeled by the theory of probability.

Section 4 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q
1. Explain in your own words the importance of mathematical models to engineering practice.

Chapter 1 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Calibration of measurement equipment is most           5. Describe a situation from your field where a full
   clearly associated with which of the following            factorial study might be conducted (name at least
   concepts: validity, precision, or accuracy? Explain.      three factors, and the levels of each, that would
                                                             appear in the study).
2. If factor A has levels 1, 2, and 3, factor B has
   levels 1 and 2, and factor C has levels 1 and 2, list  6. Example 7 concerns the measurement of the thick-
   the combinations of A, B, and C that make up a            ness of book paper. Variation in measurements is
   full factorial arrangement.                               a fact of life. To observe this reality firsthand,
                                                             measure the thickness of the paper used in this
3. Explain how paired data might arise in a heat             book ten times. Use the method described imme-
   treating study aimed at determining the best way          diately before Example 7. For each determination,
   to heat treat parts made from a certain alloy.            record the measured stack thickness, the number
                                                             of sheets, and the quotient to four decimal places.
4. Losen, Cahoy, and Lewis purchased eight spanner           If you are using this book in a formal course,
   bushings of a particular type from a local machine        be prepared to hand in your results and compare
   shop and measured a number of characteristics of          them with the values obtained by others in your
   these bushings, including their outside diameters.        class.
   Each of the eight outside diameters was measured
   once by two student technicians, with the follow-      7. Exercise 6 illustrates the reality of variation in
   ing results. (The units are inches.) Considering          physical measurement. Another exercise that is
   both students' measurements, what type of data            similar in spirit, but leads to qualitative data, in-
   are given here? Explain.                                  volves the spinning of U.S. pennies. Spin a penny
                                                             on a hard surface 20 different times; for each trial,
Bushing      1      2      3      4                          record whether the penny comes to rest with heads
Student A  .3690  .3690  .3690  .3700                        or tails showing. Did all the trials have the same
Student B  .3690  .3695  .3695  .3695                        outcome? Is the pattern you observed the one you
                                                             expected to see? If not, do you have any possible
Bushing      5      6      7      8                          explanations?
Student A  .3695  .3700  .3695  .3690
Student B  .3695  .3700  .3700  .3690
24 Chapter 1 Introduction

 8. Consider a situation like that of Example 1 (in-                variables include such things as the hardnesses,
     volving the heat treating of gears). Suppose that              diameters and surface roughnesses of the pistons
     the original gears can be purchased from a variety             and the hardnesses, and inside diameters and sur-
     of vendors, they can be made out of a variety of               face roughnesses of the bores into which the pis-
     materials, they can be heated according to a va-               tons fit. Describe, in general terms, an observa-
     riety of regimens (involving different times and               tional study to try to determine how to improve
     temperatures), they can be cooled in a number of               life. Then describe an experimental study and say
     different ways, and the furnace atmosphere can                 why it might be preferable.
     be adjusted to a variety of different conditions. A
     number of features of the final gears are of interest,    12. In the context of Exercise 9, it might make sense
     including their flatness, their concentricity, their           to average the strengths you record. Would you
     hardness (both before and after heat treating), and            expect such an average to be more or less precise
     their surface finish.                                          than a single measurement as an estimate of the
     (a) What kind of data arise if, for a single set               average strength of this kind of dowel? Explain.
          of conditions, the Rockwell hardness of sev-              Argue that such averages can be no more (or less)
          eral gears is measured both before and after              accurate than the individual measurements that
          heat treating? (Use the terminology of Sec-               make them up.
          tion 1.2.) In the same context, suppose that
          engineering specifications on flatness require       13. A toy catapult launches golf balls. There are a
          that measured flatness not exceed .40 mm.                 number of things that can be altered on the con-
          If flatness is measured for several gears and             figuration of the catapult: The length of the arm
          each gear is simply marked Acceptable or Not              can be changed, the angle the arm makes when it
          Acceptable, what kind of data are generated?              hits the stop can be changed, the pull-back angle
     (b) Describe a three-factor full factorial study that          can be changed, the weight of the ball launched
          might be carried out in this situation. Name              can be changed, and the place the rubber cord
          the factors that will be used and describe the            (used to snap the arm forward) is attached to the
          levels of each. Write out a list of all the differ-       arm can be changed. An experiment is to be done
          ent combinations of levels of the factors that            to determine how these factors affect the distance
          will be studied.                                          a ball is launched.
                                                                    (a) Describe one three-factor full factorial study
 9. Suppose that you wish to determine "the" axial                       that might be carried out. Make out a data
     strength of a type of wooden dowel. Why might it                    collection form that could be used. For each
     be a good idea to test several such dowels in order                 launch, specify the level to be used of each of
     to arrive at a value for this "physical constant"?                  the three factors and leave a blank for record-
                                                                         ing the observed value of the response vari-
10. Give an example of a 2 × 3 full factorial data                       able. (Suppose two launches will be made for
     structure that might arise in a student study of the                each setup.)
     breaking strengths of wooden dowels. (Name the                 (b) If each of the five factors mentioned above is
     two factors involved, their levels, and write out all               included in a full factorial experiment, a min-
     six different combinations.) Then make up a data                    imum of how many different combinations of
     collection form for the study. Plan to record both                  levels of the five factors will be required? If
     the breaking strength and whether the break was                     there is time to make only 16 launches with
     clean or splintered for each dowel, supposing that                  the device during the available lab period, but
     three dowels of each type are to be tested.                         you want to vary all five factors, what kind of
                                                                         a data collection plan must you use?
11. You are a mechanical engineer charged with im-
     proving the life-length characteristics of a hydro-
     static transmission. You suspect that important
14. As a variation on Exercise 6, you could try using                                 Chapter 1 Exercises 25
     only pages in the first four chapters of the book.
     If there were to be a noticeable change in the ul-  by applying the method in Exercise 6 ten times
     timate precision of thickness measurement, what     to stacks of pages from only the first four chap-
     kind of a change would you expect? Try this out     ters. Is there a noticeable difference in precision
                                                         of measurement from what is obtained using the
                                                         whole book?
    2q q q q q q q q q q q q q q q q q q q q q q q

             Data Collection

           Data collection is arguably the most important activity of engineering statistics.

           Often, properly collected data will essentially speak for themselves, making formal
           inferences rather like frosting on the cake. On the other hand, no amount of cleverness
           in post-facto data processing will salvage a badly done study. So it makes sense to
           consider carefully how to go about gathering data.

                This chapter begins with a discussion of some general considerations in the col-
           lection of engineering data. It turns next to concepts and methods applicable specif-
           ically in enumerative contexts, followed by a discussion of both general principles
           and some specific plans for engineering experimentation. The chapter concludes
           with advice for the step-by-step planning of a statistical engineering study.

    qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

    2.1 General Principles in the Collection
           of Engineering Data

                        Regardless of the particulars of a statistical engineering study, a number of common
                        general considerations are relevant. Some of these are discussed in this section,
                        organized around the topics of measurement, sampling, and recording.

    2.1.1  Measurement

           Good measurement is indispensable in any statistical engineering study. An engi-
           neer planning a study ought to ensure that data on relevant variables will be col-
           lected by well-trained people using measurement equipment of known and adequate
           quality.

                When choosing variables to observe in a statistical study, the concepts of mea-
           surement validity and precision, discussed in Section 1.3, must be remembered. One
           practical point in this regard concerns how directly a measure represents a system
           property. When a direct measure exists, it is preferable to an indirect measure,
           because it will usually give much better precision.

26
           2.1 General Principles in the Collection of Engineering Data 27

Example 1  Exhaust Temperature Versus Weight Loss

           An engineer working on a drying process for a bulk material was having dif-
           ficulty determining when a target dryness had been reached. The method be-
           ing used was monitoring the temperature of hot air being exhausted from the
           dryer. Exhaust temperature was a valid but very imprecise indicator of moisture
           content.

                Someone suggested measuring the weight loss of the material instead of
           exhaust temperature. The engineer developed an ingenious method of doing this,
           at only slightly greater expense. This much more direct measurement greatly
           improved the quality of the engineer's information.

                It is often easier to identify appropriate measures than to carefully and unequiv-
           ocally define them so that they can be used. For example, suppose a metal cylinder
           is to be turned on a lathe, and it is agreed that cylinder diameter is of engineering
           importance. What is meant by the word diameter? Should it be measured on one
           end of the cylinder (and if so, which?) or in the center, or where? In practice, these
           locations will differ somewhat. Further, when a cylinder is gauged at some chosen
           location, should it be rolled in the gauge to get a maximum (or minimum) reading,
           or should it simply be measured as first put into the gauge? The cross sections of
           real-world cylinders are not exactly circular or uniform, and how the measurement
           is done will affect how the resulting data look.

                It is especially necessary--and difficult--to make careful operational defini-
           tions where qualitative and count variables are involved. Consider the case of a
           process engineer responsible for an injection-molding machine producing plastic
           auto grills. If the number of abrasions appearing on these is of concern and data
           are to be gathered, how is abrasion defined? There are certainly locations on a grill
           where a flaw is of no consequence. Should those areas be inspected? How big should
           an abrasion be in order to be included in a count? How (if at all) should an inspector
           distinguish between abrasions and other imperfections that might appear on a grill?
           All of these questions must be addressed in an operational definition of "abrasion"
           before consistent data collection can take place.

                Once developed, operational definitions and standard measurement procedures
           must be communicated to those who will use them. Training of technicians has to
           be taken seriously. Workers need to understand the importance of adhering to the
           standard definitions and methods in order to provide consistency. For example, if
           instructions call for zeroing an instrument before each measurement, it must always
           be done.

                The performance of any measuring equipment used in a study must be known
           to be adequate--both before beginning and throughout the study. Most large in-
           dustrial concerns have regular programs for both recalibrating and monitoring the
           precision of their measuring devices. The second of these activities sometimes goes
           under the name of gauge R and R studies--the two R's being repeatability and
           reproducibility. Repeatability is variation observed when a single operator uses the
28 Chapter 2 Data Collection

                                  gauge to measure and remeasure one item. Reproducibility is variation in measure-
                                  ment attributable to differences among operators. (A detailed discussion of such
                                  studies can be found in Section 2.2.2 of Statistical Quality Assurance Methods for
                                  Engineers by Vardeman and Jobe.)

                                       Calibration and precision studies should assure the engineer that instrumentation
                                  is adequate at the beginning of a statistical study. If the time span involved in the
                                  study is appreciable, the stability of the instrumentation must be maintained over
                                  the study period through checks on calibration and precision.

       2.1.2    Sampling

  How much      Once it is established how measurement/observation will proceed, the engineer can
         data?  consider how much to do, who is to do it, where and under what conditions it is
                to be done, etc. Sections 2.2, 2.3, and 2.4 consider the question of choosing what
Who should      observations to make, first in enumerative and then in experimental studies. But first,
collect data?   a few general comments about the issues of "How much?", "Who?", and "Where?".

                     The most common question engineers ask about data collection is "How many
                observations do I need?" Unfortunately, the proper answer to the question is typically
                "it depends." As you proceed through this book, you should begin to develop some
                intuition and some rough guides for choosing sample sizes. For the time being, we
                point out that the only factor on which the answer to the sample size question really
                depends is the variation in response that one expects (coming both from unit-to-unit
                variation and from measurement variation).

                     This makes sense. If objects to be observed were all alike and perfect measure-
                ment were possible, then a single observation would suffice for any purpose. But if
                there is increase either in the measurement noise or in the variation in the system
                or population under study, the sample size necessary to get a clear picture of reality
                becomes larger.

                     However, one feature of the matter of sample size sometimes catches people a bit
                off guard--the fact that in enumerative studies (provided the population size is large),
                sample size requirements do not depend on the population size. That is, sample size
                requirements are not relative to population size, but, rather, are absolute. If a sample
                size of 5 is adequate to characterize compressive strengths of a lot of 1,000 red
                clay bricks, then a sample of size 5 would be adequate to characterize compressive
                strengths for a lot of 100,000 bricks with similar brick-to-brick variability.

                     The "Who?" question of data collection cannot be effectively answered without
                reference to human nature and behavior. This is true even in a time when automatic
                data collection devices are proliferating. Humans will continue to supervise these
                and process the information they generate. Those who collect engineering data must
                not only be well trained; they must also be convinced that the data they collect will
                be used and in a way that is in their best interests. Good data must be seen as a help
                in doing a good job, benefiting an organization, and remaining employed, rather
                than as pointless or even threatening. If those charged with collecting or releasing
                data believe that the data will be used against them, it is unrealistic to expect them
                to produce useful data.
                 2.1 General Principles in the Collection of Engineering Data 29

Example 2        Data--An Aid or a Threat?

                 One of the authors once toured a facility with a company industrial statistician as
                 guide. That person proudly pointed out evidence that data were being collected
                 and effectively used. Upon entering a certain department, the tone of the con-
                 versation changed dramatically. Apparently, the workers in that department had
                 been asked to collect data on job errors. The data had pointed unmistakably to
                 poor performance by a particular individual, who was subsequently fired from the
                 company. Thereafter, convincing other workers that data collection is a helpful
                 activity was, needless to say, a challenge.

                      Perhaps all the alternatives in this situation (like retraining or assignment to a
                 different job) had already been exhausted. But the appropriateness of the firing is
                 not the point here. Rather, the point is that circumstances were allowed to create
                 an atmosphere that was not conducive to the collection and use of data.

Where should          Even where those who will gather data are convinced of its importance and are
        data be  eager to cooperate, care must be exercised. Personal biases (whether conscious or
                 subconscious) must not be allowed to enter the data collection process. Sometimes
     collected?  in a statistical study, hoped-for or predicted best conditions are deliberately or
                 unwittingly given preference over others. If this is a concern, measurements can be
                 made blind (i.e., without personnel knowing what set of conditions led to an item
                 being measured). Other techniques for ensuring fair play, having less to do with
                 human behavior, will be discussed in the next two sections.

                      The "Where?" question of engineering data collection can be answered in
                 general terms: "As close as possible in time and space to the phenomenon being
                 studied." The importance of this principle is most obvious in the routine monitoring
                 of complex manufacturing processes. The performance of one operation in such a
                 process is most effectively monitored at the operation rather than at some later point.
                 If items being produced turn out to be unsatisfactory at the end of the line, it is rarely
                 easy to backtrack and locate the operation responsible. Even if that is accomplished,
                 unnecessary waste has occurred during the time lag between the onset of operation
                 malfunction and its later discovery.

Example 3        IC Chip Manufacturing Process Improvement

                 The preceding point was illustrated during a visit to a "clean room" where
                 integrated circuit chips are manufactured. These are produced in groups of 50 or
                 so on so-called wafers. Wafers are made by successively putting down a number
                 of appropriately patterned, very thin layers of material on an inert background
                 disk. The person conducting the tour said that at one point, a huge fraction of
                 wafers produced in the room had been nonconforming. After a number of false
                 starts, it was discovered that by appropriate testing (data collection) at the point
                 of application of the second layer, a majority of the eventually nonconforming
30 Chapter 2 Data Collection

Example 3     wafers could be identified and eliminated, thus saving the considerable extra
(continued )  expense of further processing. What's more, the need for adjustments to the
              process was signaled in a timely manner.

2.1.3         Recording

              The object of engineering data collection is to get data used. How they are recorded
              has a major impact on whether this objective is met. A good data recording format
              can make the difference between success and failure.

Example 4     A Data Collection Disaster

              A group of students worked with a maker of molded plastic business signs in
              an effort to learn what factors affect the shrinkage a sign undergoes as it cools.
              They considered factors such as Operator, Heating Time, Mold Temperature,
              Mold Size, Ambient Temperature, and Humidity. Then they planned a partially
              observational and partially experimental study of the molding process. After
              spending two days collecting data, they set about to analyze them. The students
              discovered to their dismay that although they had recorded many features of
              what went on, they had neglected to record either the size of the plastic sheets
              before molding or the size of the finished signs. Their considerable effort was
              entirely wasted. It is likely that this mistake could have been prevented by careful
              precollection development of a data collection form.

                   When data are collected in a routine, ongoing, process-monitoring context (as
              opposed to a one-shot study of limited duration), it is important that they be used
              to provide effective, timely feedback of information. Increasingly, computer-made
              graphical displays of data, in real time, are used for this purpose. But it is often
              possible to achieve this much more cheaply through clever design of a manual data
              collection form, if the goal of making data recording convenient and immediately
              useful is kept in sight.

Example 5     Recording Bivariate Data on PVC Bottles

              Table 2.1 presents some bivariate data on bottle mass and width of bottom piece
              resulting from blow molding of PVC plastic bottles (taken from Modern Methods
              for Quality Control and Improvement by Wadsworth, Stephens, and Godfrey).
              Six consecutive samples of size 3 are represented.

                   Such bivariate data could be recorded in much the same way as they are listed
              in Table 2.1. But if it is important to have immediate feedback of information
              (say, to the operator of a machine), it would be much more effective to use a well-
              thought-out bivariate check sheet like the one in Figure 2.1. On such a sheet, it
             2.1 General Principles in the Collection of Engineering Data 31

Table 2.1
Mass and Bottom Piece Widths of PVC Bottles

Sample Item Mass (g) Width (mm) Sample Item Mass (g) Width (mm)

1  1  33.01  25.0                            4  10 32.80  26.5

1  2  33.08  24.0                            4  11 32.86  28.5

1  3  33.24  23.5                            4  12 32.89  25.5

2  4  32.93  26.0                            5  13 32.73  27.0

2  5  33.17  23.0                            5  14 32.57  28.0

2  6  33.07  25.0                            5  15 32.65  26.5

3  7  33.01  25.5                            6  16 32.43  30.0

3  8  32.82  27.0                            6  17 32.54  28.0

3  9  32.91  26.0                            6  18 32.61  26.0

                                                       First 3 samplesWidth (mm)
                                                       Last 3 samples
                              31
                              30
                              29
                              28
                              27
                              26
                              25
                              24

                                32.4 32.6 32.8 33.0 33.2 33.4
                                                        Mass (g)

                            Figure 2.1 Check sheet for the PVC bottle data

      is easy to see how the two variables are related. If, as in the figure, the recording
      symbol is varied over time, it is also easy to track changes in the characteristics
      over time. In the present case, width seems to be inversely related to mass, which
      appears to be decreasing over time.

           To be useful (regardless of whether data are recorded on a routine basis or
      in a one-shot mode, automatically or by hand), the recording must carry enough
      documentation that the important circumstances surrounding the study can be
      reconstructed. In a one-shot experimental study, someone must record responses
32 Chapter 2 Data CollectionMachineOperatorRaw Material Lot
        Operation
                                      Date
                                     TimeVariables Control ChartZero EqualsMeasurements

                                    RangeGageUnitsSpecifications
                                           1
                                           2
                                           3
                                           4
                                           5

                                      Sum
                                     Mean

Part and Drawing
        Dimension
                Production Lot(s)
                        Period Covered

                                                                                 Figure 2.2 Variables control chart form

                                                                      and values of the experimental variables, and it is also wise to keep track of other
                                                                      variables that might later prove to be of interest. In the context of routine process
                                                                      monitoring, data records will be useful in discovering differences in raw material
                                                                      lots, machines, operators, etc., only if information on these is recorded along with
                                                                      the responses of interest. Figure 2.2 shows a form commonly used for the routine
                                                                      collection of measurements for process monitoring. Notice how thoroughly the user
                                                                      is invited to document the data collection.

Section 1 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Consider the context of a study on making paper                    ditions (with a paper clip versus without a paper
   airplanes where two different Designs (say delta                   clip) are of interest with regard to their impact on
   versus t wing), two different Papers (say construc-                flight distance. Give an operational definition of
   tion versus typing), and two different Loading Con-                flight distance that you might use in such a study.
                                                      2.2 Sampling in Enumerative Studies 33

2. Explain how training operators in the proper use      our product--we take 5% samples of every outgo-
   of measurement equipment might affect both the        ing order, regardless of order size!"?
   repeatability and the reproducibility of measure-
   ments made by an organization.                     4. State briefly why it is critical to make careful oper-
                                                         ational definitions for response variables in statis-
3. What would be your response to another engi-          tical engineering studies.
   neer's comment, "We have great information on

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

2.2 Sampling in Enumerative Studies

                    An enumerative study has an identifiable, concrete population of items. This section
                    discusses selecting a sample of the items to include in a statistical investigation.

                         Using a sample to represent a (typically much larger) population has obvious
                    advantages. Measuring some characteristics of a sample of 30 electrical components
                    from an incoming lot of 10,000 can often be feasible in cases where it would not
                    be feasible to perform a census (a study that attempts to include every member
                    of the population). Sometimes testing is destructive, and studying an item renders
                    it unsuitable for subsequent use. Sometimes the timeliness and data quality of a
                    sampling investigation far surpass anything that could be achieved in a census.
                    Data collection technique can become lax or sloppy in a lengthy study. A moderate
                    amount of data, collected under close supervision and put to immediate use, can be
                    very valuable--often more valuable than data from a study that might appear more
                    complete but in fact takes too long.

                         If a sample is to be used to stand for a population, how that sample is chosen
                    becomes very important. The sample should somehow be representative of the
                    population. The question addressed here is how to achieve this.

                         Systematic and judgment-based methods can in some circumstances yield
                    samples that faithfully portray the important features of a population. If a lot of
                    items is manufactured in a known order, it may be reasonable to select, say, every
                    20th one for inclusion in a statistical engineering study. Or it may be effective to
                    force the sample to be balanced--in the sense that every operator, machine, and raw
                    material lot (for example) appears in the sample. Or an old hand may be able to look
                    at a physical population and fairly accurately pick out a representative sample.

                         But there are potential problems with such methods of sample selection. Humans
                    are subject to conscious and subconscious preconceptions and biases. Accordingly,
                    judgment-based samples can produce distorted pictures of populations. Systematic
                    methods can fail badly when unexpected cyclical patterns are present. (For example,
                    suppose one examines every 20th item in a lot according to the order in which
                    the items come off a production line. Suppose further that the items are at one
                    point processed on a machine having five similar heads, each performing the same
                    operation on every fifth item. Examining every 20th item only gives a picture of how
                    one of the heads is behaving. The other four heads could be terribly misadjusted,
                    and there would be no way to find this out.)

                         Even beyond these problems with judgment-based and systematic methods of
                    sampling, there is the additional difficulty that it is not possible to quantify their
34 Chapter 2 Data Collection

                                  properties in any useful way. There is no good way to take information from samples
                                  drawn via these methods and make reliable statements of likely margins of error. The
                                  method introduced next avoids the deficiencies of systematic and judgment-based
                                  sampling.

  Definition 1            A simple random sample of size n from a population is a sample selected
Simple random             in such a manner that every collection of n items in the population is a priori
                          equally likely to compose the sample.
        sampling

                               Probably the easiest way to think of simple random sampling is that it is
                          conceptually equivalent to drawing n slips of paper out of a hat containing one for
                          each member of the population.

Example 6                 Random Sampling Dorm Residents

                          C. Black did a partially enumerative and partially experimental study comparing
                          student reaction times under two different lighting conditions. He decided to
                          recruit subjects from his coed dorm floor, selecting a simple random sample of
                          20 of these students to recruit. In fact, the selection method he used involved
                          a table of so-called random digits. But he could have just as well written the
                          names of all those living on his floor on standard-sized slips of paper, put them in
                          a bowl, mixed thoroughly, closed his eyes, and selected 20 different slips from
                          the bowl.

Mechanical methods             Methods for actually carrying out the selection of a simple random sample
 and simple random        include mechanical methods and methods using "random digits." Mechanical
                sampling  methods rely for their effectiveness on symmetry and/or thorough mixing in a
                          physical randomizing device. So to speak, the slips of paper in the hat need to be of
                          the same size and well scrambled before sample selection begins.

                               The first Vietnam-era U.S. draft lottery was a famous case in which adequate
                          care was not taken to ensure appropriate operation of a mechanical randomizing
                          device. Birthdays were supposed to be assigned priority numbers 1 through 366 in a
                          "random" way. However, it was clear after the fact that balls representing birth dates
                          were placed into a bin by months, and the bin was poorly mixed. When the balls
                          were drawn out, birth dates near the end of the year received a disproportionately
                          large share of the low draft numbers. In the present terminology, the first five dates
                          out of the bin should not have been thought of as a simple random sample of size 5.
                          Those who operate games of chance more routinely make it their business to know
                          (via the collection of appropriate data) that their mechanical devices are operating
                          in a more random manner.
                                              2.2 Sampling in Enumerative Studies 35

                       Using random digits to do sampling implicitly relies for "randomness" on the
                  appropriateness of the method used to generate those digits. Physical random pro-
                  cesses like radioactive decay and pseudorandom number generators (complicated
                  recursive numerical algorithms) are the most common sources of random digits.
                  Until fairly recently, it was common to record such digits in printed tables. Table
                  B.1 consists of random digits (originally generated by a physical random pro-
                  cess). The first five rows of this table are reproduced in Table 2.2 for use in this
                  section.

                       In making a random digit table, the intention is to use a method guaranteeing
                  that a priori

                  1. each digit 0 through 9 has the same chance of appearing at any particular
                      location in the table one wants to consider, and

                  2. knowledge of which digit will occur at a given location provides no help in
                      predicting which one will appear at another.

 Random digit     In a random digit table, condition 1 should typically be reflected in roughly equal
      tables and  representation of the 10 digits, and condition 2 in the lack of obvious internal patterns
                  in the table.
simple random
        sampling       For populations that can easily be labeled with consecutive numbers, the fol-
                  lowing steps can be used to synthetically draw items out of a hat one at a time--to
                  draw a simple random sample using a table like Table 2.2.

                  Step 1  For a population of N objects, determine the number of digits in N
                  Step 2  (for example, N = 1291 is a four-digit number). Call this number M
                  Step 3  and assign each item in the population a different M-digit label.

                          Move through the table left to right, top to bottom, M digits at a time,
                          beginning from where you left off in last using the table, and choose
                          objects from the population by means of their associated labels until
                          n have been selected.

                          In moving through the table according to step 2, ignore labels that
                          have not been assigned to items in the population and any that would
                          indicate repeat selection of an item.

Table 2.2
Random Digits

12159  66144      05091  13446  45653  13684  66024  91410  51351  22772
30156  90519      95785  47544  66735  35754  11088  67310  19720  08379
59069  01722      53338  41942  65118  71236  01932  70343  25812  62275
54107  58081      82470  59407  13475  95872  16268  78436  39251  64247
99681  81295      06315  28212  45029  57701  96327  85436  33614  29070
36 Chapter 2 Data Collection

                            12159 66144 05091 13446 45653 13684 66024 91410 51351 22772
                            30156 90519 95785 47544 66735 35754 11088 67310 19720 08379

                                              Figure 2.3 Use of a random digit table

Statistical or spreadsheet       As an example of how this works, consider selecting a simple random sample
      software and simple   of 25 members of a hypothetical population of 80 objects. One first determines that
          random sampling   80 is an M = 2-digit number and therefore labels items in the population as 01, 02,
                            03, 04, . . . , 77, 78, 79, 80 (labels 00 and 81 through 99 are not assigned). Then, if
                            Table 2.2 is being used for the first time, begin in the upper left corner and proceed
                            as indicated in Figure 2.3. Circled numbers represent selected labels, Xs indicate
                            that the corresponding label has not been assigned, and slash marks indicate that
                            the corresponding item has already entered the sample. As the final item enters the
                            sample, the stopping point is marked with a penciled hash mark. Movement through
                            the table is resumed at that point the next time the table is used.

                                 Any predetermined systematic method of moving through the table could be
                            substituted in place of step 2. One could move down columns instead of across rows,
                            for example. It is useful to make the somewhat arbitrary choice of method in step 2
                            for the sake of classroom consistency.

                                 With the wide availability of personal computers, random digit tables have be-
                            come largely obsolete. That is, random numbers can be generated "on the spot"
                            using statistical or spreadsheet software. In fact, it is even easy to have such soft-
                            ware automatically do something equivalent to steps 1 through 3 above, selecting
                            a simple random sample of n of the numbers 1 to N . For example, Printout 1 was
                            produced using the MINITABTM statistical package. It illustrates the selection of
                            n = 25 members of a population of N = 80 objects. The numbers 1 through 80 are
                            placed into the first column of a worksheet (using the routine under the "Calc/Make
                            Patterned Data/Simple Set of Numbers" menu). Then 25 of them are selected us-
                            ing MINITAB's pseudorandom number generation capability (located under the
                            "Calc/Random Data/Sample from Columns" menu). Finally, those 25 values (the
                            results beginning with 56 and ending with 72) are printed out (using the routine
                            under the "Manip/Display Data" menu).

                            Printout 1 Random Selection of 25 Objects from a Population of 80 Objects

WWW                         MTB > Set C1
                            DATA> 1( 1 : 80 / 1 )1
                            DATA> End.
                            MTB > Sample 25 C1 C2.
                            MTB > Print C2.

                            Data Display

                            C2

                                56  74    43        61  80  22  30  67                35  7

                                10  69    19        49  8   45  3   37                21  17

                                2   12    9         14  72
                                                 2.2 Sampling in Enumerative Studies 37

     Regardless of how Definition 1 is implemented, several comments about the
method are in order. First, it must be admitted that simple random sampling meets
the original objective of providing representative samples only in some average or
long-run sense. It is possible for the method to produce particular realizations that
are horribly unrepresentative of the corresponding population. A simple random
sample of 20 out of 80 axles could turn out to consist of those with the smallest
diameters. But this doesn't happen often. On the average, a simple random sample
will faithfully portray the population. Definition 1 is a statement about a method,
not a guarantee of success on a particular application of the method.

     Second, it must also be admitted that there is no guarantee that it will be an
easy task to make the physical selection of a simple random sample. Imagine the
pain of retrieving 5 out of a production run of 1,000 microwave ovens stored in
a warehouse. It would probably be a most unpleasant job to locate and gather 5
ovens corresponding to randomly chosen serial numbers to, for example, carry to a
testing lab.

     But the virtues of simple random sampling usually outweigh its drawbacks. For
one thing, it is an objective method of sample selection. An engineer using it is
protected from conscious and subconscious human bias. In addition, the method
interjects probability into the selection process in what turns out to be a manage-
able fashion. As a result, the quality of information from a simple random sample
can be quantified. Methods of formal statistical inference, with their resulting con-
clusions ("I am 95% sure that . . ."), can be applied when simple random sampling
is used.

     It should be clear from this discussion that there is nothing mysterious or
magical about simple random sampling. We sometimes get the feeling while reading
student projects (and even some textbooks) that the phrase random sampling is
used (even in analytical rather than enumerative contexts) to mean "magically OK
sampling" or "sampling with magically universally applicable results." Instead,
simple random sampling is a concrete methodology for enumerative studies. It is
generally about the best one available without a priori having intimate knowledge
of the population.

Section 2 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. For the sake of exercise, treat the runout values for  2. Repeat Exercise 1 using statistical or spreadsheet
   38 laid gears (given in Table 1.1) as a population        software to do the random sampling.
   of interest, and using the random digit table (Ta-
   ble B.1), select a simple random sample of 5 of        3. Explain briefly why in an enumerative study, a sim-
   these runouts. Repeat this selection process a total      ple random sample is or is not guaranteed to be
   of four different times. (Begin the selection of the      representative of the population from which it is
   first sample at the upper left of the table and pro-      drawn.
   ceed left to right and top to bottom.) Are the four
   samples identical? Are they each what you would
   call "representative" of the population?
38 Chapter 2 Data Collection

                                                qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

         2.3 Principles for Effective Experimentation

                                  Purposely introducing changes into an engineering system and observing what
                                  happens as a result (i.e., experimentation) is a principal way of learning how the
                                  system works. Engineers meet such a variety of experimental situations that it is
                                  impossible to give advice that will be completely relevant in all cases. But it is
                                  possible to raise some general issues, which we do here. The discussion in this
                                  section is organized under the headings of

                                       1. taxonomy of variables,
                                       2. handling extraneous variables,
                                       3. comparative study,
                                       4. replication, and
                                       5. allocation of resources.

                                  Then Section 2.4 discusses a few generic experimental frameworks for planning a
                                  specific experiment.

2.3.1         Taxonomy of Variables

              One of the hard realities of experiment planning is the multidimensional nature
              of the world. There are typically many characteristics of system performance that
              the engineer would like to improve and many variables that might influence them.
              Some terminology is needed to facilitate clear thinking and discussion in light of
              this complexity.

Definition 2  A response variable in an experiment is one that is monitored as characterizing
              system performance/behavior.

              A response variable is a system output. Some variables that potentially affect a
              response of interest are managed by the experimenter.

Definition 3  A supervised (or managed) variable in an experiment is one over which
              an investigator exercises power, choosing a setting or settings for use in the
              study. When a supervised variable is held constant (has only one setting), it is
              called a controlled variable. And when a supervised variable is given several
              different settings in a study, it is called an experimental variable.
                           2.3 Principles for Effective Experimentation 39

                           Physical process

                                                           Response
                                                           variable

                           Managed variables

                                              Concomitant
                                                variables

                                                            Figure 2.4 Variables in an experiment

                           Some of the variables that are neither primary responses nor managed in an experi-
                           ment will nevertheless be observed.

Definition 4               A concomitant (or accompanying) variable in an experiment is one that is
                           observed but is neither a primary response variable nor a managed variable.
                           Such a variable can change in reaction to either experimental or unobserved
                           causes and may or may not itself have an impact on a response variable.

                           Figure 2.4 is an attempt to picture Definitions 2 through 4. In it, the physical process
                           somehow produces values of a response. "Knobs" on the process represent managed
                           variables. Concomitant variables are floating about as part of the experimental
                           environment without being its main focus.

              Example 7    Variables in a Wood Joint Strength Experiment
(Example 6, Chapter 1,
                           Dimond and Dix experimented with three different woods and three different
       revisited--p. 15 )  glues, investigating joint strength properties. Their primary interest was in the
                           effects of experimental variables Wood Type and Glue Type on two observed
                           response variables, joint strength in a tension test and joint strength in a shear
                           test.

                                In addition, they recognized that strengths were probably related to the
                           variables Drying Time and Pressure applied to the joints while drying. Their
                           method of treating the nine wood/glue combinations fairly with respect to the
                           Time and Pressure variables was to manage them as controlled variables, trying
                           to hold them essentially constant for all the joints produced.

                                Some of the variation the students observed in strengths could also have
                           originated in properties of the particular specimens glued, such as moisture
                           content. In fact, this variable was not observed in the study. But if the students
                           had had some way of measuring it, moisture content might have provided extra
                           insight into how the wood/glue combinations behave. It would have been a
                           potentially informative concomitant variable.
40 Chapter 2 Data Collection

      2.3.2   Handling Extraneous Variables

 Control of   In planning an experiment, there are always variables that could influence the re-
extraneous    sponses but which are not of practical interest to the experimenter. The investigator
              may recognize some of them as influential but not even think of others. Those that
   variables  are recognized may fail to be of primary interest because there is no realistic way
              of exercising control over them or compensating for their effects outside of the ex-
              perimental environment. So it is of little practical use to know exactly how changes
              in them affect the system.

                   But completely ignoring the existence of such extraneous variables in experi-
              ment planning can needlessly cloud the perception of the effects of factors that are
              of interest. Several methods can be used in an active attempt to avoid this loss of
              information. These are to manage them (for experimental purposes) as controlled
              variables (recall Definition 3) or as blocking variables, or to attempt to balance
              their effects among process conditions of interest through randomization.

                   When choosing to control an extraneous variable in an experiment, both the
              pluses and minuses of that choice should be recognized. On the one hand, the control
              produces a homogeneous environment in which to study the effects of the primary
              experimental variables. In some sense, a portion of the background noise has been
              eliminated, allowing a clearer view of how the system reacts to changes in factors
              of interest. On the other hand, system behavior at other values of the controlled
              variable cannot be projected on the firm basis of data. Instead, projections must be
              based on the basis of expert opinion that what is seen experimentally will prove
              true more generally. Engineering experience is replete with examples where what
              worked fine in a laboratory (or even a pilot plant) was much less dependable in
              subsequent experience with a full-scale facility.

Example 7     The choice Dimond and Dix made to control Drying Time and the Pressure
(continued )  provided a uniform environment for comparing the nine wood/glue combinations.
              But strictly speaking, they learned only about joint behavior under their particular
              experimental Time and Pressure conditions.

                   To make projections for other conditions, they had to rely on their expe-
              rience and knowledge of material science to decide how far the patterns they
              observed were likely to extend. For example, it may have been reasonable to
              expect what they observed to also hold up for any drying time at least as long
              as the experimental one, because of expert knowledge that the experimental time
              was sufficient for the joints to fully set. But such extrapolation is based on other
              than statistical grounds.

   Blocking        An alternative to controlling extraneous variables is to handle them as experi-
extraneous    mental variables, including them in study planning at several different levels. Notice
              that this really amounts to applying the notion of control locally, by creating not
   variables  one but several (possibly quite different) homogeneous environments in which to
              compare levels of the primary experimental variables. The term blocking is often
              used to refer to this technique.
                    2.3 Principles for Effective Experimentation 41

Definition 5        A block of experimental units, experimental times of observation, experimen-
                    tal conditions, etc. is a homogeneous group within which different levels of
                    primary experimental variables can be applied and compared in a relatively
                    uniform environment.

Example 7           Consider embellishing a bit on the gluing study of Dimond and Dix. Imagine
(continued )        that the students were uneasy about two issues, the first being the possibility that
                    surface roughness differences in the pieces to be glued might mask the wood/glue
                    combination differences of interest. Suppose also that because of constraints on
                    schedules, the strength testing was going to have to be done in two different
                    sessions a day apart. Measuring techniques or variables like ambient humidity
                    might vary somewhat between such periods. How might such potential problems
                    have been handled?

                         Blocking is one way. If the specimens of each wood type were separated into
                    relatively rough and relatively smooth groups, the factor Roughness could have
                    then served as an experimental factor. Each of the glues could have been used the
                    same number of times to join both rough and smooth specimens of each species.
                    This would set up comparison of wood/glue combinations separately for rough
                    and for smooth surfaces.

                         In a similar way, half the testing for each wood/glue/roughness combination
                    might have been done in each testing session. Then, any consistent differences
                    between sessions could be identified and prevented from clouding the comparison
                    of levels of the primary experimental variables. Thus, Testing Period could have
                    also served as a blocking variable in the study.

Randomization            Experimenters usually hope that by careful planning they can account for the
and extraneous      most important extraneous variables via control and blocking. But not all extraneous
                    variables can be supervised. There are an essentially infinite number, most of which
         variables  cannot even be named. And there is a way to take out insurance against the possibility
                    that major extraneous variables get overlooked and then produce effects that are
                    mistaken for those of the primary experimental variables.

Definition 6        Randomization is the use of a randomizing device or table of random dig-
                    its at some point where experimental protocol is not already dictated by the
                    specification of values of the supervised variables. Often this means that exper-
                    imental objects (or units) are divided up between the experimental conditions
                    at random. It can also mean that the order of experimental testing is randomly
                    determined.
42 Chapter 2 Data Collection

                                       The goal of randomization is to average between sets of experimental con-
                                  ditions the effects of all unsupervised extraneous variables. To put it differently,
                                  sets of experimental conditions are treated fairly, giving them equal opportunity to
                                  shine.

              Example 8    Randomization in a Heat Treating Study
(Example 1, Chapter 1,
                           P. Brezler, in his "Heat Treating" article, describes a very simple randomized
         revisited--p. 2)  experiment for comparing the effects on thrust face runout of laying versus hang-
                           ing gears. The variable Loading Method was the primary experimental variable.
                           Extraneous variables Steel Heat and Machining History were controlled by ex-
                           perimenting on 78 gears from the same heat code, machined as a lot. The 78
                           gears were broken at random into two groups of 39, one to be laid and the
                           other to be hung. (Note that Table 1.1 gives only 38 data points for the laid
                           group. For reasons not given in the article, one laid gear was dropped from
                           the study.)

                                Although there is no explicit mention of this in the article, the principle of
                           randomization could have been (and perhaps was) carried a step further by mak-
                           ing the runout measurements in a random order. (This means choosing gears 01
                           through 78 one at a time at random to measure.) The effect of this randomization
                           would have been to protect the investigator from clouding the comparison of
                           heat treating methods with possible unexpected and unintended changes in mea-
                           surement techniques. Failing to randomize and, for example, making all the laid
                           measurements before the hung measurements, would allow unintended changes
                           in measurement technique to appear in the data as differences between the two
                           loading methods. (Practice with measurement equipment might, for example,
                           increase precision and make later runouts appear to be more uniform than early
                           ones.)

Example 7                  Dimond and Dix took the notion of randomization to heart in their gluing study
(continued )               and, so to speak, randomized everything in sight. In the tension strength testing for
                           a given type of wood, they glued .5 × .5 × 3 blocks to a .75 × 3.5 × 31.5
                           board of the same wood type, as illustrated in Figure 2.5.

                                Each glue was used for three joints on each type of wood. In order to deal
                           with any unpredicted differences in material properties (e.g., over the extent of
                           the board) or unforeseen differences in loading by the steel strap used to provide
                           pressure on the joints, etc., the students randomized the order in which glue was
                           applied and the blocks placed along the base board. In addition, when it came
                           time to do the strength testing, that was carried out in a randomly determined
                           order.
       2.3 Principles for Effective Experimentation 43
                                         Metal strap

                                                              Wood block
       Block position

                        Wood board
       Figure 2.5 Gluing method for a single wood type

            Simple random sampling in enumerative studies is only guaranteed to be effec-
       tive in an average or long-run sense. Similarly, randomization in experiments will
       not prove effective in averaging the effects of extraneous variables between settings
       of experimental variables every time it is used. Sometimes an experimenter will
       be unlucky. But the methodology is objective, effective on the average, and about
       the best one can do in accounting for those extraneous variables that will not be
       managed.

2.3.3  Comparative Study

       Statistical engineering studies often involve more than a single sample. They usually
       involve comparison of a number of settings of process variables. This is true not
       only because there may be many options open to an engineer in a given situation,
       but for other reasons as well.

            Even in experiments where there is only a single new idea or variation on
       standard practice to be tried out, it is a good idea to make the study comparative
       (and therefore to involve more than one sample). Unless this is done, there is
       no really firm basis on which to say that any effects observed come from the
       new conditions under study rather than from unexpected extraneous sources. If
       standard yield for a chemical process is 63.2% and a few runs of the process with a
       supposedly improved catalyst produce a mean yield of 64.8%, it is not completely
       safe to attribute the difference to the catalyst. It could be caused by a number of
       things, including miscalibration of the measurement system. But suppose a few
       experimental runs are taken for both the standard and the new catalysts. If these
       produce two samples with small internal variation and (for example) a difference of
       1.6% in mean yields, that difference is more safely attributed to a difference in the
       catalysts.
44 Chapter 2 Data Collection

Example 8     In the gear loading study, hanging was the standard method in use at the time
(continued )  of the study. From its records, the company could probably have located some
              values for thrust face runout to use as a baseline for evaluating the laying method.
              But the choice to run a comparative study, including both laid and hung gears,
              put the engineer on firm ground for drawing conclusions about the new method.

A second           In a potentially confusing use of language, the word control is sometimes used
 usage of     to mean the practice of including a standard or no-change sample in an experiment
"control"     for comparison purposes. (Notice that this is not the usage in Definition 3.) When
              a control group is included in a medical study to verify the effectiveness of a new
              drug, that group is either a standard-treatment or no-treatment group, included to
              provide a solid basis of comparison for the new treatment.

2.3.4         Replication

              In much of what has been said so far, it has been implicit that having more than one
              observation for a given setting of experimental variables is a good idea.

Definition 7  Replication of a setting of experimental variables means carrying through the
              whole process of adjusting values for supervised variables, making an exper-
              imental "run," and observing the results of that run--more than once. Values
              of the responses from replications of a setting form the (single) sample corre-
              sponding to the setting, which one hopes represents typical process behavior
              at that setting.

Purposes of        The idea of replication is fundamental in experimentation. Reproducibility of
 replication  results is important in both science and engineering practice. Replication helps
              establish this, protecting the investigator from unconscious blunders and validating
              or confirming experimental conclusions.

                   But replication is not only important for establishing that experimental results
              are reproducible. It is also essential to quantifying the limits of that reproducibility--
              that is, for getting an idea of the size of experimental error. Even under a fixed setting
              of supervised variables, repeated experimental runs typically will not produce ex-
              actly the same observations. The effects of unsupervised variables and measurement
              errors produce a kind of baseline variation, or background noise. Establishing the
              magnitude of this variation is important. It is only against this background that one
              can judge whether an apparent effect of an experimental variable is big enough to
              establish it as clearly real, rather than explainable in terms of background noise.

                   When planning an experiment, the engineer must think carefully about what kind
              of repetition will be included. Definition 7 was written specifically to suggest that
              simply remeasuring an experimental unit does not amount to real replication. Such
              repetition will capture measurement error, but it ignores the effects of (potentially
                                                     2.3 Principles for Effective Experimentation 45

            changing) unsupervised variables. It is a common mistake in logic to seriously
            underestimate the size of experimental error by failing to adopt a broad enough
            view of what should be involved in replication, settling instead for what amounts to
            remeasurement.

Example 9   Replication and Steel Making

            A former colleague once related a consulting experience that went approximately
            as follows. In studying the possible usefulness of a new additive in a type of steel,
            a metallurgical engineer had one heat (batch) of steel made with the additive and
            one without. Each of these was poured into ingots. The metallurgist then selected
            some ingots from both heats, had them cut into pieces, and selected some pieces
            from the ingots, ultimately measuring a property of interest on these pieces and
            ending up with a reasonably large amount of data. The data from the heat with
            additive showed it to be clearly superior to the no-additive heat. As a result,
            the existing production process was altered (at significant expense) and the new
            additive incorporated. Unfortunately, it soon became apparent that the alteration
            to the process had actually degraded the properties of the steel.

                 The statistician was (only at this point) called in to help figure out what had
            gone wrong. After all, the experimental results, based on a large amount of data,
            had been quite convincing, hadn't they?

                 The key to understanding what had gone wrong was the issue of replication.
            In a sense, there was none. The metallurgist had essentially just remeasured the
            same two physical objects (the heats) many times. In the process, he had learned
            quite a bit about the two particular heats in the study but very little about all heats
            of the two types. Apparently, extraneous and uncontrolled foundry variables were
            producing large heat-to-heat variability. The metallurgist had mistaken an effect
            of this fluctuation for an improvement due to the new additive. The metallurgist
            had no notion of this possibility because he had not replicated the with-additive
            and without-additive settings of the experimental variable.

Example 10  Replication and Paper Airplane Testing

            Beer, Dusek, and Ehlers completed a project comparing the Kline-Fogelman and
            Polish Frisbee paper airplane designs on the basis of flight distance under a num-
            ber of different conditions. In general, it was a carefully done project. However,
            replication was a point on which their experimental plan was extremely weak.
            They made a number of trials for each plane under each set of experimental
            conditions, but only one Kline-Fogelman prototype and one Polish Frisbee pro-
            totype were used throughout the study. The students learned quite a bit about the
            prototypes in hand but possibly much less about the two designs. If their purpose
            was to pick a winner between the two prototypes, then perhaps the design of their
            study was appropriate. But if the purpose was to make conclusions about planes
46 Chapter 2 Data Collection

Example 10     "like" the two used in the study, they needed to make and test several prototypes
 (continued )  for each design.

                    ISU Professor Emeritus L. Wolins calls the problem of identifying what con-
               stitutes replication in an experiment the unit of analysis problem. There must be
               replication of the basic experimental unit or object. The agriculturalist who, in order
               to study pig blood chemistry, takes hundreds of measurements per hour on one pig,
               has a (highly multivariate) sample of size 1. The pig is the unit of analysis.

                    Without proper replication, one can only hope to be lucky. If experimental error
               is small, then accepting conclusions suggested by samples of size 1 will lead to
               correct conclusions. But the problem is that without replication, one usually has
               little idea of the size of that experimental error.

2.3.5          Allocation of Resources

               Experiments are done by people and organizations that have finite time and money.
               Allocating those resources and living within the constraints they impose is part of
               experiment planning. The rest of this section makes several points in this regard.

                    First, real-world investigations are often most effective when approached
               sequentially, the planning for each stage building upon what has been learned
               before. The classroom model of planning and/or executing a single experiment is
               more a result of constraints inherent in our methods of teaching than a realistic
               representation of how engineering problems are solved. The reality is most often
               iterative in nature, involving a series of related experiments.

                    This being the case, one can not use an entire experimental budget on the first
               pass of a statistical engineering study. Conventional wisdom on this matter is that no
               more than 20-25% of an experimental budget should be allocated to the first stage
               of an investigation. This leaves adequate resources for follow-up work built on what
               is learned initially.

                    Second, what is easy to do (and therefore usually cheap to do) should not dictate
               completely what is done in an experiment. In the context of the steel formula devel-
               opment study of Example 9, it seems almost certain that one reason the metallurgist
               chose to get his "large sample sizes" from pieces of ingots rather than from heats is
               that it was easy and cheap to get many measurements in that way. But in addition to
               failing to get absolutely crucial replication and thus botching the study, he probably
               also grossly overmeasured the two heats.

                    A final remark is an amplification of the discussion of sample size in Section 2.1.
               That is, minimum experimental resource requirements are dictated in large part by
               the magnitude of effects of engineering importance in comparison to the magnitude
               of experimental error. The larger the effects in comparison to the error (the larger
               the signal-to-noise ratio), the smaller the sample sizes required, and thus the fewer
               the resources needed.
                                                         2.4 Some Common Experimental Plans 47

Section 3 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Consider again the paper airplane study from Ex-      5. Continuing the paper airplane scenario of Exercise
   ercise 1 of Section 2.1. Describe some variables         1 of Section 2.1, discuss the pros and cons of Tom
   that you would want to control in such a study.          and Juanita flying each of their own eight planes
   What are the response and experimental variables         twice, as opposed to making and flying two planes
   that would be appropriate in this context? Name a        of each of the eight types, one time each.
   potential concomitant variable here.
                                                         6. Random number tables are sometimes used in the
2. In general terms, what is the trade-off that must        planning of both enumerative and analytical/ex-
   be weighed in deciding whether or not to control a       perimental studies. What are the two different ter-
   variable in a statistical engineering study?             minologies employed in these different contexts,
                                                            and what are the different purposes behind the use
3. In the paper airplane scenario of Exercise 1 of Sec-     of the tables?
   tion 2.1, if (because of schedule limitations, for
   example) two different team members will make         7. What is blocking supposed to accomplish in an
   the flight distance measurements, discuss how the        engineering experiment?
   notion of blocking might be used.
                                                         8. What are some purposes of replication in a statisti-
4. Again using the paper airplane scenario of Exer-         cal engineering study?
   cise 1 of Section 2.1, suppose that two students are
   each going to make and fly one airplane of each       9. Comment briefly on the notion that in order for
   of the 23 = 8 possible types once. Employ the no-        a statistical engineering study to be statistically
   tion of randomization and Table B.1 and develop          proper, one should know before beginning data col-
   schedules for Tom and Juanita to use in their flight     lection exactly how an entire experimental budget
   testing. Explain how the table was used.                 is to be spent. (Is this, in fact, a correct idea?)

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

2.4 Some Common Experimental Plans

                    In previous sections, experimentation has been discussed in general terms, and the
                    subtlety of considerations that enter the planning of an effective experiment has
                    been illustrated. It should be obvious that any exposition of standard experimental
                    "plans" can amount only to a discussion of standard "skeletons" around which real
                    plans can be built. Nevertheless, it is useful to know something about such skeletons.
                    In this section, so-called completely randomized, randomized complete block, and
                    incomplete block experimental plans are considered.

         2.4.1 Completely Randomized Experiments

Definition 8  A completely randomized experiment is one in which all experimental
              variables are of primary interest (i.e., none are included only for purposes of
              blocking), and randomization is used at every possible point of choosing the
              experimental protocol.
48 Chapter 2 Data Collection

 Paraphrase of   Notice that this definition says nothing about how the combinations of settings
 the definition  of experimental variables included in the study are structured. In fact, they may
                 be essentially unstructured or produce data with any of the structures discussed
   of complete   in Section 1.2. That is, there are completely randomized one-factor, factorial, and
randomization    fractional factorial experiments. The essential point in Definition 8 is that all else is
                 randomized except what is restricted by choice of which combinations of levels of
                 experimental variables are to be used in the study.

                      Although it doesn't really fit every situation (or perhaps even most) in which
                 the term complete randomization is appropriate, language like the following is
                 commonly used to capture the intent of Definition 8. "Experimental units (objects)
                 are allocated at random to the treatment combinations (settings of experimental
                 variables). Experimental runs are made in a randomly determined order. And any
                 post-facto measuring of experimental outcomes is also carried out in a random
                 order."

Example 11       Complete Randomization in a Glass Restrengthening Study

                 Bloyer, Millis, and Schibur studied the restrengthening of damaged glass through
                 etching. They investigated the effects of two experimental factors--the Con-
                 centration of hydrofluoric acid in an etching bath and the Time spent in the
                 etching bath--on the resulting strength of damaged glass rods. (The rods had
                 been purposely scratched in a 1 region near their centers by sandblasting.)
                 Strengths were measured using a three-point bending method on a 20 kip MTS
                 machine.

                      The students decided to run a 3 × 3 factorial experiment. The experimental
                 levels of Concentration were 50%, 75%, and 100% HF, and the levels of Time
                 employed were 30 sec, 60 sec, and 120 sec. There were thus nine treatment
                 combinations, as illustrated in Figure 2.6.

                              100%
                                 HF

                               75%
                                 HF

                               50%
                                 HF

                                            30 sec 60 sec 120 sec

                              Figure 2.6 Nine combinations of
                              three levels of concentration and three
                              levels of time
                                                          2.4 Some Common Experimental Plans 49

                    The students decided that 18 scratched rods would be allocated--two apiece
               to each of the nine treatment combinations--for testing. Notice that this could be
               done at random by labeling the rods 01-18, placing numbered slips of paper in
               a hat, mixing, drawing out two for 30 sec and 50% concentration, then drawing
               out two for 30 sec and 75% concentration, etc.

                    Having determined at random which rods would receive which experimental
               conditions, the students could again have used the slips of paper to randomly
               determine an etching order. And a third use of the slips of paper to determine an
               order of strength testing would have given the students what most people would
               call a completely randomized 3 × 3 factorial experiment.

Example 12     Complete Randomization and a Study of the Flight of Golf Balls

               G. Gronberg studied drive flight distances for 80, 90, and 100 compression
               golf balls, using 10 balls of each type in his experiment. Consider what com-
               plete randomization would entail in such a study (involving the single factor
               Compression).

                    Notice that the paraphrase of Definition 8 is not particularly appropriate to
               this experimental situation. The levels of the experimental factor are an intrinsic
               property of the experimental units (balls). There is no way to randomly divide
               the 30 test balls into three groups and "apply" the treatment levels 80, 90, and
               100 compression to them. In fact, about the only obvious point at which random-
               ization could be employed in this scenario is in the choice of an order for hitting
               the 30 test balls. If one numbered the test balls 01 through 30 and used a table
               of random digits to pick a hitting order (by choosing balls one at a time without
               replacement), most people would be willing to call the resulting test a completely
               randomized one-factor experiment.

                    Randomization is a good idea. Its virtues have been discussed at some length.
               So it would be wise to point out that using it can sometimes lead to practically
               unworkable experimental plans. Dogmatic insistence on complete randomization
               can in some cases be quite foolish and unrealistic. Changing experimental variables
               according to a completely randomly determined schedule can sometimes be exceed-
               ingly inconvenient (and therefore expensive). If the inconvenience is great and the
               fear of being misled by the effects of extraneous variables is relatively small, then
               backing off from complete to partial randomization may be the only reasonable
               course of action. But when choosing not to randomize, the implications of that
               choice must be carefully considered.

Example 11     Consider an embellishment on the glass strengthening scenario, where an exper-
 (continued )  imenter might have access to only a single container to use for a bath and/or have
               only a limited amount of hydrofluoric acid.
50 Chapter 2 Data Collection

Example 11          From the discussion of replication in the previous section and present con-
 (continued )  siderations of complete randomization, it would seem that the purest method of
               conducting the study would be to make a new dilution of HF for each of the rods
               as its turn comes for testing. But this would be time-consuming and might require
               more acid than was available.

                    If the investigator had three containers to use for baths but limited acid, an
               alternative possibility would be to prepare three different dilutions, one 100%,
               one 75%, and one 50% dilution. A given dilution could then be used in testing
               all rods assigned to that concentration. Notice that this alternative allows for a
               randomized order of testing, but it introduces some question as to whether there
               is "true" replication.

                    Taking the resource restriction idea one step further, notice that even if an
               investigator could afford only enough acid for making one bath, there is a way
               of proceeding. One could do all 100% concentration testing, then dilute the
               acid and do all 75% testing, then dilute the acid again and do all 50% testing.
               The resource restriction would not only affect the "purity" of replication but also
               prevent complete randomization of the experimental order. Thus, for example, any
               unintended effects of increased contamination of the acid (as more and more tests
               were made using it) would show up in the experimental data as indistinguishable
               from effects of differences in acid concentration.

                    To choose intelligently between complete randomization (with "true" repli-
               cation) and the two plans just discussed, the real severity of resource limitations
               would have to be weighed against the likelihood that extraneous factors would
               jeopardize the usefulness of experimental results.

2.4.2 Randomized Complete Block Experiments

Definition 9   A randomized complete block experiment is one in which at least one
               experimental variable is a blocking factor (not of primary interest to the in-
               vestigator); and within each block, every setting of the primary experimental
               variables appears at least once; and randomization is employed at all possible
               points where the exact experimental protocol is determined.

               A helpful way to think of a randomized complete block experiment is as a collection
               of completely randomized studies. Each of the blocks yields one of the component
               studies. Blocking provides the simultaneous advantages of homogeneous environ-
               ments for studying primary factors and breadth of applicability of the results.

                    Definition 9 (like Definition 8) says nothing about the structure of the settings
               of primary experimental variables included in the experiment. Nor does it say
               anything about the structure of the blocks. It is possible to design experiments
               where experimental combinations of primary variables have one-factor, factorial, or
               fractional factorial structure, and at the same time the experimental combinations of
                                                                       2.4 Some Common Experimental Plans 51

                         blocking variables also have one of these standard structures. The essential points
                         of Definition 9 are the completeness of each block (in the sense that it contains
                         each setting of the primary variables) and the randomization within each block. The
                         following two examples illustrate that depending upon the specifics of a scenario,
                         Definition 9 can describe a variety of experimental plans.

Example 12               As actually run, Gronberg's golf ball flight study amounted to a randomized
 (continued )            complete block experiment. This is because he hit and recorded flight distances
                         for all 30 balls on six different evenings (over a six-week period). Note that
                         this allowed him to have (six different) homogeneous conditions under which to
                         compare the flight distances of balls having 80, 90, and 100 compression. (The
                         blocks account for possible changes over time in his physical condition and skill
                         level as well as varied environmental conditions.)

                              Notice the structure of the data set that resulted from the study. The settings of
                         the single primary experimental variable Compression combined with the levels
                         of the single blocking factor Day to produce a 3 × 6 factorial structure for 18
                         samples of size 10, as pictured in Figure 2.7.

                              100
                         Compression

                               90
                         Compression

                               80
                         Compression

                                             Day 1 Day 2 Day 3 Day 4 Day 5 Day 6

                                  Figure 2.7 18 combinations of compression and day

             Example 13  Blocking in a Pelletizing Experiment
(Example 2, Chapter 1,
                         Near the end of Section 1.2, the notion of a fractional factorial study was il-
  revisited--pp. 6, 13)  lustrated in the context of a hypothetical experiment on a pelletizing machine.
                         The factors Volume, Flow, and Mixture were of primary interest. Table 1.3 is
                         reproduced here as Table 2.3, listing four (out of eight possible) combinations
                         of two levels each of the primary experimental variables, forming a fractional
                         factorial arrangement.

                              Consider a situation where two different operators can make four experi-
                         mental runs each on two consecutive days. Suppose further that Operator and
                         Day are blocking factors, their combinations giving four blocks, within which
                         the four combinations listed in Table 2.3 are run in a random order. This ends
52 Chapter 2 Data Collection   Table 2.3
                               Half of a 23 Factorial
                Example 13
                 (continued )

                               Volume Flow Mixture

                               high                current no binder

                               low                 manual no binder

                               low                 current binder

                               high                manual binder

up as a randomized complete block experiment in which the blocks have 2 × 2
factorial structure and the four combinations of primary experimental factors
have a fractional factorial structure.

     There are several ways to think of this plan. For one, by temporarily ignoring
the structure of the blocks and combinations of primary experimental factors,
it can be considered a 4 × 4 factorial arrangement of samples of size 1, as is
illustrated in Figure 2.8. But from another point of view, the combinations under
discussion (listed in Table 2.4) have fractional factorial structure of their own, rep-
resenting a (not particularly clever) choice of 16 out of 25 = 32 different possible
combinations of the two-level factors Operator, Day, Volume, Flow, and Mixture.
(The lines in Table 2.4 separate the four blocks.) A better use of 16 experimental
runs in this situation (at least from the perspective that the combinations in Table
2.4 have their own fractional factorial structure) will be discussed next.

                               Block 1 Operator 1  1 Run      1 Run      1 Run    1 Run
                                           Day 1

                               Block 2 Operator 2  1 Run      1 Run      1 Run    1 Run
                                           Day 1

                               Block 3 Operator 1  1 Run      1 Run      1 Run    1 Run
                                           Day 2

                               Block 4 Operator 2  1 Run      1 Run      1 Run    1 Run
                                           Day 2

                                                      High       Low      Low      High
                                                    Current    Manual    Current  Manual
                                                   No binder  No binder  Binder   Binder

                                                   Combination Combination Combination Combination

                                                   1          2          3        4

                               Figure 2.8 16 combinations of blocks and treatments
                  2.4 Some Common Experimental Plans 53

               Table 2.4
               Half of a 23 Factorial Run Once in Each of Four Blocks

               Operator Day Volume Flow Mixture

               1  1 high  current no binder

               1  1 low   manual no binder

               1  1 low   current binder

               1  1 high  manual binder

               2  1 high  current no binder

               2  1 low   manual no binder

               2  1 low   current binder

               2  1 high  manual binder

               1  2 high  current no binder

               1  2 low   manual no binder

               1  2 low   current binder

               1  2 high  manual binder

               2  2 high  current no binder

               2  2 low   manual no binder

               2  2 low   current binder

               2  2 high  manual binder

2.4.3          Incomplete Block Experiments (Optional )

               In many experimental situations where blocking seems attractive, physical con-
               straints make it impossible to satisfy Definition 9. This leads to the notion of
               incomplete blocks.

Definition 10  An incomplete (usually randomized) block experiment is one in which at
               least one experimental variable is a blocking factor and the assignment of
               combinations of levels of primary experimental factors to blocks is such that
               not every combination appears in every block.

Example 13     In Section 1.2, the pelletizing machine study examined all eight possible com-
 (continued )  binations of Volume, Flow, and Mixture. These are listed in Table 2.5. Imagine
               that only half of these eight combinations can be run on a given day, and there
               is some fear that daily environmental conditions might strongly affect process
               performance. How might one proceed?

                    There are then two blocks (days), each of which will accommodate four
               runs. Some possibilities for assigning runs to blocks would clearly be poor. For
               example, running combinations 1 through 4 on the first day and 5 through 8 on
54 Chapter 2 Data Collection   Table 2.5
                               Combinations in a 23 Factorial Study
                Example 13
                 (continued )

                               Combination Number Volume Flow                 Mixture

                               1       low                           current  no binder
                                                                              no binder
                               2       high                          current  no binder
                                                                              no binder
                               3       low                           manual   binder
                                                                              binder
                               4       high                          manual   binder
                                                                              binder
                               5       low                           current

                               6       high                          current

                               7       low                           manual

                               8       high                          manual

the second would make it impossible to distinguish the effects of Mixture from
any important environmental effects.

     What turns out to be a far better possibility is to run, say, the four combinations
listed in Table 2.3 (combinations 2, 3, 5, and 8) on one day and the others on
the next. This is illustrated in Table 2.6. In a well-defined sense (explained in
Chapter 8), this choice of an incomplete block plan minimizes the unavoidable
clouding of inferences caused by the fact all eight combinations of levels of
Volume, Flow, and Mixture cannot be run on a single day.

     As one final variation on the pelletizing scenario, consider an alternative
that is superior to the experimental plan outlined in Table 2.4: one that involves
incomplete blocks. That is, once again suppose that the two-level primary factors
Volume, Flow, and Mixture are to be studied in four blocks of four observations,
created by combinations of the two-level blocking factors Operator and Day.

     Since a total of 16 experimental runs can be made, all eight combinations
of primary experimental factors can be included in the study twice (instead of

                               Table 2.6
                               A 23 Factorial Run in Two Incomplete Blocks

                               Day Volume Flow Mixture

                               2 low   current                       no binder
                               1 high  current                       no binder
                               1 low   manual                        no binder
                               2 high  manual                        no binder
                               1 low   current                       binder
                               2 high  current                       binder
                               2 low   manual                        binder
                               1 high  manual                        binder
   2.4 Some Common Experimental Plans 55

Table 2.7
A Once-Replicated 23 Factorial Run in Four Incomplete

Blocks

Operator Day Volume Flow Mixture

1  1 high  current no binder

1  1 low   manual no binder

1  1 low   current binder

1  1 high  manual binder

2  1 low   current no binder

2  1 high  manual no binder

2  1 high  current binder

2  1 low   manual binder

1  2 low   current no binder

1  2 high  manual no binder

1  2 high  current binder

1  2 low   manual binder

2  2 high  current no binder

2  2 low   manual no binder

2  2 low   current binder

2  2 high  manual binder

including only four combinations four times apiece). To do this, incomplete
blocks are required, but Table 2.7 shows a good incomplete block plan. (Again,
blocks are separated by lines.)

     Notice the symmetry present in this choice of half of the 25 = 32 different
possible combinations of the five experimental factors. For example, a full facto-
rial in Volume, Flow, and Mixture is run on each day, and similarly, each operator
runs a full factorial in the primary experimental variables.

     It turns out that the study outlined in Table 2.7 gives far more potential
for learning about the behavior of the pelletizing process than the one out-
lined in Table 2.4. But again, a complete discussion of this must wait until
Chapter 8.

     There may be some reader uneasiness and frustration with the "rabbit out of a
hat" nature of the examples of incomplete block experiments, since there has been
no discussion of how to go about making up a good incomplete block plan. Both
the choosing of an incomplete block plan and corresponding techniques of data
analysis are advanced topics that will not be developed until Chapter 8. The purpose
here is to simply introduce the possibility of incomplete blocks as a useful option in
experimental planning.
56 Chapter 2 Data Collection

Section 4 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. What standard name might be applied to the ex-                  How does the method you used here differ from
   perimental plan you developed for Exercise 4 of                 what you did in part (a)?
   Section 2.3?
                                                           3. Once more referring to the paper airplane scenario
2. Consider an experimental situation where the three         of Exercise 1 of Section 2.1, suppose that only the
   factors A, B, and C each have two levels, and it           factors Design and Paper are of interest (all planes
   is desirable to make three experimental runs for           will be made without paper clips) but that Tom and
   each of the possible combinations of levels of the         Juanita can make and test only two planes apiece.
   factors.                                                   Devise an incomplete block plan for this study that
                                                              gives each student experience with both designs
    (a) Select a completely random order of experi-           and both papers. (Which two planes will each make
        mentation. Carefully describe how you use Ta-         and test?)
        ble B.1 or statistical software to do this. Make
        an ordered list of combinations of levels of the   4. Again in the paper airplane scenario of Exercise 1
        three factors, prescribing which combination          of Section 2.1, suppose that Tom and Juanita each
        should be run first, second, etc.                     have time to make and test only four airplanes
                                                              apiece, but that in toto they still wish to test all eight
   (b) Suppose that because of physical constraints,          possible types of planes. Develop a sensible plan
        only eight runs can be made on a given day.           for doing this. (Which planes should each person
        Carefully discuss how the concept of blocking         test?) You will probably want to be careful to make
        could be used in this situation when planning         sure that each person tests two delta wing planes,
        which experimental runs to make on each of            two construction paper planes, and two paper clip
        three consecutive days. What possible purpose         planes. Why is this? Can you arrange your plan so
        would blocking serve?                                 that each person tests each Design/Paper combina-
                                                              tion, each Design/Loading combination, and each
    (c) Use Table B.1 or statistical software to ran-         Paper/Loading combination once?
        domize the order of experimentation within
        the blocks you described in part (b). (Make        5. What standard name might be applied to the plan
        a list of what combinations of levels of the fac-     you developed in Exercise 4?
        tors are to be run on each day, in what order.)

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

2.5 Preparing to Collect Engineering Data

                    This chapter has raised many of the issues that engineers must consider when
                    planning a statistical study. What is still lacking, however, is a discussion of how to
                    get started. This section first lists and then briefly discusses a series of steps that can
                    be followed in preparing for engineering data collection.

2.5.1  A Series of Steps to Follow

       The following is a list of steps that can be used to organize the planning of a statistical
       engineering study.
        2.5 Preparing to Collect Engineering Data 57

              PROBLEM DEFINITION

              Step 1 Identify the problem to be addressed in general terms.
              Step 2 Understand the context of the problem.
              Step 3 State in precise terms the objective and scope of the study. (State the

                           questions to be answered.)

              STUDY DEFINITION

              Step 4 Identify the response variable(s) and appropriate instrumentation.
              Step 5 Identify possible factors influencing responses.
              Step 6 Decide whether (and if so how) to manage factors that are likely to

                           have effects on the response(s).
              Step 7 Develop a detailed data collection protocol and timetable for the

                           first phase of the study.

              PHYSICAL PREPARATION

              Step 8 Assign responsibility for careful supervision.
              Step 9 Identify technicians and provide necessary instruction in the study

                           objectives and methods to be used.
             Step 10 Prepare data collection forms and/or equipment.
             Step 11 Do a dry run analysis on fictitious data.
             Step 12 Write up a "best guess" prediction of the results of the actual study.

             These 12 points are listed in a reasonably rational order, but planning any
        real study may involve departures from the listed order as well as a fair amount
        of iterating among the steps before they are all accomplished. The need for other
        steps (like finding funds to pay for a proposed study) will also be apparent in some
        contexts. Nevertheless, steps 1 through 12 form a framework for getting started.

2.5.2   Problem Definition

Step 1  Identifying the general problem to work on is, for the working engineer, largely a
        matter of prioritization. An individual engineer's job description and place in an
        organization usually dictate what problem areas need attention. And far more things
        could always be done than resources of time and money will permit. So some choice
        has to be made among the different possibilities.

             It is only natural to choose a general topic on the basis of the perceived impor-
        tance of a problem and the likelihood of solving it (given the available resources).
        These criteria are somewhat subjective. So, particularly when a project team or
        other working group must come to consensus before proceeding, even this initial
58 Chapter 2 Data Collection

                                  planning step is a nontrivial task. Sometimes it is possible to remove part of the
                                  subjectivity and reliance on personal impressions by either examining existing data
                                  or commissioning a statistical study of the current state of affairs. For example,
                                  suppose members of an engineering project team can name several types of flaws
                                  that occur in a mechanical part but disagree about the frequencies or dollar impacts
                                  of the flaws. The natural place to begin is to search company records or collect some
                                  new data aimed at determining the occurrence rates and/or dollar impacts.

                                       An effective and popular way of summarizing the findings of such a preliminary
                                  look at the current situation is through a Pareto diagram. This is a bar chart whose
                                  vertical axis delineates frequency (or some other measure of impact of system
                                  misbehavior) and whose bars, representing problems of various types, have been
                                  placed left to right in decreasing order of importance.

Example 14  Maintenance Hours for a Flexible Manufacturing System

            Figure 2.9 is an example of a Pareto diagram that represents a breakdown (by
            craft classification) of the total maintenance hours required in one year on four
            particular machines in a company's flexible manufacturing system. (This infor-
            mation is excerpted from the ISU M.S. thesis work of M. Patel.) A diagram like
            Figure 2.9 can be an effective tool for helping to focus attention on the most
            important problems in an engineering system. Figure 2.9 highlights the fact that
            (in terms of maintenance hours required) mechanical problems required the most
            attention, followed by electrical problems.

            Maintenance hours  1000
                                500

                        Mechanic Electrical Toolmaker Oiler
                                                Craft

            Figure 2.9 Pareto diagram of maintenance hours by
            craft classification

Step 2           In a statistical engineering study, it is essential to understand the context of the
            problem. Statistics is no magic substitute for good, hard work learning how a process
            is configured; what its inputs and environment are; what applicable engineering,
            scientific, and mathematical theory has to say about its likely behavior; etc. A
            statistical study is an engineering tool, not a crystal ball. Only when an engineer
                                                       2.5 Preparing to Collect Engineering Data 59

            has studied and asked questions in order to gain expert knowledge about a system
            is he or she then in a position to decide intelligently what is not known about the
            system--and thus what data will be of help.

                 It is often helpful at step 2 to make flowcharts describing an ideal process and/or
            the process as it is currently operating. (Sometimes the comparison of the two is
            enough in itself to show an engineer how a process should be modified.) During the
            construction of such a chart, data needs and variables of potential interest can be
            identified in an organized manner.

Example 15  Work Flow in a Printing Shop

            Drake, Lach, and Shadle worked with a printing shop. Before collecting any data,
            they set about to understand the flow of work through the shop. They made a
            flowchart similar to Figure 2.10. The flowchart facilitated clear thinking about

            Work
            order

            Typesetting  Yes Typesetting
              needed?

                    No
            Makeready Yes Makeready

                                           needed?

                                                No
            Photo lab

            Masking

            Plating

            Printing

                                       Cutting Yes  Cutting
                                       needed?

                                             No
            Folding Yes Folding

                                       needed?
                                             No

                                         Ship

            Figure 2.10 Flowchart of a printing process
60 Chapter 2 Data Collection

Example 15     what might go wrong in the printing process and at what points what data could
 (continued )  be gathered in order to monitor and improve process performance.

Step 3              After determining the general arena and physical context of a statistical engi-
               neering study, it is necessary to agree on a statement of purpose and scope for the
               study. An engineering project team assigned to work on a wave soldering process
               for printed circuit boards must understand the steps in that process and then begin to
               define what part(s) of the process will be included in the study and what the goal(s)
               of the study will be. Will flux formulation and application, the actual soldering,
               subsequent cleaning and inspection, and touch-up all be studied? Or will only some
               part of this list be investigated? Is system throughput the primary concern, or is it
               instead some aspect of quality or cost? The sharper a statement of purpose and scope
               can be made at this point, the easier subsequent planning steps will be.

2.5.3          Study Definition

Step 4         Once one has defined in qualitative terms what it is about an engineering system that
               is of interest, one must decide how to represent that property (or those properties)
Step 5         in precise terms. That is, one must choose a well-defined response variable (or vari-
               ables) and decide how to measure it (or them). For example, in a manufacturing con-
               text, if "throughput" of a system is of interest, should it be measured in pieces/hour,
               or conforming pieces/hour, or net profit/hour, or net profit/hour/machine, or in some
               other way?

                    Sections 1.3 and 2.1 have already discussed issues that arise in measurement
               and the formation of operational definitions. All that needs to be added here is that
               these issues must be faced early in the planning of a statistical engineering study.
               It does little good to carefully plan a study assuming the existence of an adequate
               piece of measuring equipment, only to later determine that the organization doesn't
               own a device with adequate precision and that the purchase of one would cost more
               than the entire project budget.

                    Identification of variables that may affect system response requires expert
               knowledge of the process under study. Engineers who do not have hands-on ex-
               perience with a system can sometimes contribute insights gained from experience
               with similar systems and from basic theory. But it is also wise (in most cases, essen-
               tial) to include on a project team several people who have first-hand knowledge of
               the particular process and to talk extensively with those who work with the system
               on a regular basis.

                    Typically, the job of identifying factors of potential importance in a statistical
               engineering study is a group activity, carried out in brainstorming sessions. It is
               therefore helpful to have tools for lending order to what might otherwise be an
               inefficient and disorganized process. One tool that has proved effective is variously
               known as a cause-and-effect diagram, or fishbone diagram, or Ishikawa diagram.
                                                                                   2.5 Preparing to Collect Engineering Data 61

Example 16                           Identifying Potentially Important Variables in a Molding Process

                                     Figure 2.11 shows a cause-and-effect diagram from a study of a molding process
                                     for polyurethane automobile steering wheels. It is taken from the paper "Fine
                                     Tuning of the Foam System and Optimization of the Process Parameters for
                                     the Manufacturing of Polyurethane Steering Wheels Using Reaction Injection
                                     Molding by Applying Dr. Taguchi's Method of Design of Experiments" by Vimal
                                     Khanna, which appeared in 1985 in the Third Supplier Symposium on Taguchi
                                     Methods, published by the American Supplier Institute, Inc. Notice how the
                                     diagram in Figure 2.11 organizes the huge number of factors possibly affecting

                                                   Ishikawa Diagram for Molding

                     Machine                       Environment                     Man

Cure Time            Throughput
Shot Time
                              Ratio                      Temp. Training                  Knowledge of    Stds.
Mix Pressures                 Nucleation                      Airflow                        Shift
Colour                                                                                            Cycle  Time -°Open Mold
Mix                                         Cleanliness
Viscosity                        Mixing                                                                       Mold Cleanliness
Material                                                                                                             Vents
Qty. in Unit                         Mold Release               Which Operator                                       Flash
Cycletime                            Spray System                                                                    Mold Release

Colour Dosing Unit                            Humidity

High & Low Pressure  Recirc. System  Temp.                               Consistency
                                        Pumps
                                           Holding Tank        Pressure      Mold Release
                                                                             Application
                                              Day Tank
                                                    Clamp                        Flash Off Time
                                                                                  Gun Setting
                                                                                   Where

                                                                                     How Much
                                                                                       How Long
                                                                                        How

                                                                                                                                          Molded Wheel

                                                                                       OH. NO                    Cleanliness - Phosphate Coating
                                                                         Runner Water
                                                         Gate                                                    Geometric Shape
                                                                                        Freon                    Flatness of Hub
                                                                         Sealoff
                                                                                   Reactivity                    Dimensional Consistency
                                     Mold Volume
                                     Aftermixer                 Geometry                FRD
                                     Texture
                                     Dual                                               Polyol           Insert
                                     Temperature
                     Single                                      Alignment                                  Water Content
                     Mold                                                          FNCO                     Conc.
                                                                                                            Viscosity
                     Runner                                   Wear                                          Colour
                                                                        Reactivity                          Blend
                     Mold
                                                         Vents                                   Pigment

                                                             Size                                    Solvent
                                                                                                     Water Content
                                                             Location                                Conc.
                                                             Quantity                                Mold Release

                                                   ISO Choice of Foam System       Handling

                                                         Storage Temp.
                                                          Shelf Life
                                                       Temp.

                                     Tooling                             Material

Figure 2.11 Cause and effect diagram for a molding process. From the Third Symposium
on Taguchi Methods. c Copyright, American Supplier Institute, Dearborn, Michigan
(U.S.A.). Reproduced by permission under License No. 930403.
62 Chapter 2 Data Collection

Example 16     wheel quality. Without some kind of organization, it would be all but impossible
 (continued )  to develop anything like a complete list of important factors in a complex situation
               like this.

Step 6              Armed with (1) a list of variables that might influence the response(s) of interest
Step 7         and some guesses at their relative importance, (2) a solid understanding of the issues
               raised in Section 2.3, and (3) knowledge of resource and physical constraints and
               time-frame requirements, one can begin to make decisions about which (if any)
               variables are to be managed. Experiments have some real advantages over purely
               observational studies (see Section 1.2). Those must be weighed against possible extra
               costs and difficulties associated with managing both variables that are of interest
               and those that are not. The hope is to choose a physically and financially workable
               set of managed variables in such a way that the aggregate effects of variables not of
               interest and not managed are not so large as to mask the effects of those variables
               that are of interest.

                    Choosing experimental levels and then combinations for managed variables
               is part of the task of deciding on a detailed data collection protocol. Levels of
               controlled and block variables should usually be chosen to be representative of
               the values that will be met in routine system operation. For example, suppose the
               amount of contamination in a transmission's hydraulic fluid is thought to affect
               time to failure when the transmission is subjected to stress testing, where Operating
               Speed and Pressure are the primary experimental variables. It only makes sense to
               see that the contamination level(s) during testing are representative of the level(s)
               that will be typical when the transmission is used in the field.

                    With regard to primary experimental variables, one should also choose typical
               levels--with a couple of provisos. Sometimes the goal in an engineering experiment
               is to compare an innovative, nonstandard way of doing things to current practice.
               In such cases, it is not good enough simply to look at system behavior with typical
               settings for primary experimental variables. Also, where primary experimental vari-
               ables are believed to have relatively small effects on a response, it may be necessary
               to choose ranges for the primary variables that are wider than normal, to see clearly
               how they act on the response.

                    Other physical realities and constraints on data collection may also make it
               appropriate to use atypical values of managed variables and subsequently extrapolate
               experimental results to "standard" circumstances. For example, it is costly enough to
               run studies on pilot plants using small quantities of chemical reagents and miniature
               equipment but much cheaper than experimentation on a full-scale facility. Another
               kind of engineering study in which levels of primary experimental variables are
               purposely chosen outside normal ranges is the accelerated life test. Such studies
               are done to predict the life-length properties of products that in normal usage would
               far outlast any study of feasible length. All that can then be done is to turn up
               the stress on sample units beyond normal levels, observe performance, and try to
               extrapolate back to a prediction for behavior under normal usage. (For example, if
               sensitive electronic equipment performs well under abnormally high temperature
                  2.5 Preparing to Collect Engineering Data 63

                  and humidity, this could well be expected to imply long useful life under normal
                  temperature and humidity conditions.)

                       After the experimental levels of individual manipulated variables are chosen,
                  they must be combined to form the experimental patterns (combinations) of man-
                  aged variables. The range of choices is wide: factorial structures, fractional factorial
                  structures, other standard structures, and patterns tailor-made for a particular prob-
                  lem. (Tailor-made plans will, for example, be needed in situations where particular
                  combinations of factor levels prescribed by standard structures are a priori clearly
                  unsafe or destructive of company property.)

                       But developing a detailed data collection protocol requires more than even
                  choices of experimental combinations. Experimental order must be decided. Explicit
                  instructions for actually carrying out the testing must be agreed upon and written
                  down in such a way that someone who was not involved in study planning can carry
                  out the data collection. A timetable for initial data collection must be developed.
                  In all of this, it must be remembered that several iterations of data collection and
                  analysis (all within given budget constraints) may be required in order to find a
                  solution to the original engineering problem.

        2.5.4     Physical Preparation

          Step 8  After a project team has agreed on exactly what is to be done in a statistical
          Step 9  study, it can address the details of how to accomplish it and assign responsibility for
Steps 10 & 11     completion. One team member should be given responsibility for the direct oversight
                  of actual data collection. It is all too common for people who collect the data to say,
        Step 12   after the fact, "Oh, I did it the other way . . . I couldn't figure out exactly what you
                  meant here . . . and besides, it was easier the way I did it."

                       Again, technicians who carry out a study planned by an engineering project
                  group often need training in the study objectives and the methods to be used. As
                  discussed in Section 2.1, when people know why they are collecting data and have
                  been carefully shown how to collect them, they will produce better information.
                  Overseeing the data collection process includes making sure that this necessary
                  training takes place.

                       The discipline involved in carefully preparing complete data collection forms
                  and doing a dry run data analysis on fictitious values provides opportunities to refine
                  (and even salvage) a study before the expense of data collection is incurred. When
                  carrying out steps 10 and 11, each individual on the team gets a chance to ask, "Will
                  the data be adequate to answer the question at hand? Or are other data needed?" The
                  students referred to in Example 4 (page 30), who failed to measure their primary
                  response variables, learned the importance of these steps the hard way.

                       The final step in this list is writing up a best guess at what the study will show.
                  We first came across this idea in Statistics for Experimenters by Box, Hunter, and
                  Hunter. The motivation for it is sound. After a study is complete, it is only human to
                  say, "Of course that's the way things are. We knew that all along." When a careful
                  before-data statement is available to compare to an after-data summarization of
                  findings, it is much easier to see what has been learned and appreciate the value of
                  that learning.
64 Chapter 2 Data Collection

Section 5 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Either take an engineering system and response         tomer Satisfaction and make a cause-and-effect di-
   variable that you are familiar with from your field    agram showing a variety of variables that may po-
   or consider, for example, the United Airlines pas-     tentially affect the response. How might such a
   senger flight system and the response variable Cus-    diagram be practically useful?

Chapter 2 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Use Table B.1 and choose a simple random sam-                  carefully how you do this. If you use the table,
   ple of n = 8 out of N = 491 widgets. Describe                  begin in the upper left corner.
   carefully how you label the widgets. Begin in the         (b) What are some purposes of the randomization
   upper left corner of the table. Then use spread-               used in part (a)?
   sheet or statistical software to redo the selection.
                                                          4. A sanitary engineer wishes to compare two meth-
2. Consider a potential student project concerning           ods for determining chlorine content of Cl2-
   the making of popcorn. Possible factors affecting         demand-free water. To do this, eight quite dif-
   the outcome of popcorn making include at least            ferent water samples are split in half, and one
   the following: Brand of corn, Temperature of corn         determination is made using the MSI method and
   at beginning of cooking, Popping Method (e.g.,            another using the SIB method. Explain why it
   frying versus hot air popping), Type of Oil used          could be said that the principle of blocking was
   (if frying), Amount of Oil used (if frying), Batch        used in the engineer's study. Also argue that the
   Size, initial Moisture Content of corn, and Person        resulting data set could be described as consisting
   doing the evaluation of a single batch. Using these       of paired measurement data.
   factors and/or any others that you can think of, an-
   swer the following questions about such a project:     5. A research group is testing three different meth-
    (a) What is a possible response variable in a pop-       ods of electroplating widgets (say, methods A, B,
        corn project?                                        and C). On a particular day, 18 widgets are avail-
   (b) Pick two possible experimental factors in this        able for testing. The effectiveness of electroplat-
        context and describe a 2 × 2 factorial data          ing may be strongly affected by the surface texture
        structure in those variables that might arise in     of the widgets. The engineer running the exper-
        such a study.                                        iment is able to divide the 18 available widgets
    (c) Describe how the concept of randomization            into three groups of 6 on the basis of surface tex-
        might be employed.                                   ture. (Assume that widgets 1-6 are rough, widgets
   (d) Describe how the concept of blocking might            7-12 are normal, and widgets 13-18 are smooth.)
        be employed.                                          (a) Use Table B.1 or statistical software in an
                                                                  appropriate way and assign each of the treat-
3. An experiment is to be performed to compare the                ments to 6 widgets. Carefully explain exactly
   effects of two different methods for loading gears             how you do the assignment of levels of treat-
   in a carburizing furnace on the amount of distor-              ments A, B, and C to the widgets.
   tion produced in a heat treating process. Thrust          (b) If equipment limitations are such that only
   face runout will be measured for gears laid and                one widget can be electroplated at once, but
   for gears hung while treating.                                 it is possible to complete the plating of all 18
    (a) 20 gears are to be used in the study. Randomly            widgets on a single day, in exactly what order
        divide the gears into a group (of 10) to be laid          would you have the widgets plated? Explain
        and a group (of 10) to be hung, using either              where you got this order.
        Table B.1 or statistical software. Describe           (c) If, in contrast to the situation in part (b), it is
                                                            Chapter 2 Exercises 65

        possible to plate only 9 widgets in a single                  observed values of the two responses could
        day, make up an appropriate plan for plating                  be entered for each experimental run.
        9 on each of two consecutive days.                       (b) Suppose that it is feasible to make the runs
   (d) If measurements of plating effectiveness are                   listed in your answer to part (a) in a com-
        made on each of the 18 widgets, what kind of                  pletely randomized order. Use a mechanical
        data structure will result from the scenario in               method (like slips of paper in a hat) to arrive at
        part (b)? From the scenario in part (c)?                      a random order of experimentation for your
                                                                      study. Carefully describe the physical steps
6. A company wishes to increase the light intensity                   you follow in developing this order for data
   of its photoflash cartridge. Two wall thicknesses                  collection.
   ( 161 and 18 ) and two ignition point placements are
   under study. Two batches of the basic formulation         9. Use Table B.1 and
   used in the cartridge are to be made up, each                 (a) Select a simple random sample of 7 widgets
   batch large enough to make 12 cartridges. Discuss                  from a production run of 619 widgets (begin
   how you would recommend running this initial                       at the upper left corner of the table and move
   phase of experimentation if all cartridges can be                  left to right, top to bottom). Tell how you la-
   made and tested in a short time period by a single                 beled the widgets and name which ones make
   technician. Be explicit about any randomization                    up your sample.
   and/or blocking you would employ. Say exactly                 (b) Beginning in the table where you left off in
   what kinds of cartridges you would make and test,                  (a), select a second simple random sample of
   in what order. Describe the structure of the data                  7 widgets. Is this sample the same as the first?
   that would result from your study.                                 Is there any overlap at all?

7. Use Table B.1 or statistical software and                10. Redo Exercise 9 using spreadsheet or statistical
    (a) Select a simple random sample of 5 widgets               software.
        from a production run of 354 such widgets.
        (If you use the table, begin at the upper left      11. Consider a study comparing the lifetimes (mea-
        corner and move left to right, top to bottom.)           sured in terms of numbers of holes drilled before
   (b) Select a random order of experimentation for              failure) of two different brands of 8-mm drills in
        a context where an experimental factor A has             drilling 1045 steel. Suppose that steel bars from
        two levels; a second factor, B, has three lev-           three different heats (batches) of steel are avail-
        els; and two experimental runs are going to              able for use in the study, and it is possible that the
        be made for each of the 2 × 3 = 6 different              different heats have differing physical properties.
        possible combinations of levels of the factors.          The lifetimes of a total of 15 drills of each brand
        Carefully describe how you do this.                      will be measured, and each of the bars available
                                                                 is large enough to accommodate as much drilling
8. Return to the situation of Exercise 8 of the Chap-            as will be done in the entire study.
   ter 1 Exercises.                                              (a) Describe how the concept of control could be
    (a) Name factors and levels that might be used in                 used to deal with the possibility that different
        a three-factor, full factorial study in this situ-            heats might have different physical properties
        ation. Also name two response variables for                   (such as hardnesses).
        the study. Suppose that in accord with good              (b) Name one advantage and one drawback to
        engineering data collection practice, you wish                controlling the heat.
        to include some replication in the study. Make           (c) Describe how one might use the concept of
        up a data collection sheet, listing all the com-              blocking to deal with the possibility that dif-
        binations of levels of the factors to be studied,             ferent heats might have different physical
        and include blanks where the corresponding                    properties.
    3q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

             Elementary
             Descriptive Statistics

           Engineering data are always variable. Given precise enough measurement, even

           supposedly constant process conditions produce differing responses. Therefore, it is
           not individual data values that demand an engineer's attention as much as the pattern
           or distribution of those responses. The task of summarizing data is to describe their
           important distributional characteristics. This chapter discusses simple methods that
           are helpful in this task.

                The chapter begins with some elementary graphical and tabular methods of
           data summarization. The notion of quantiles of a distribution is then introduced and
           used to make other useful graphical displays. Next, standard numerical summary
           measures of location and spread for quantitative data are discussed. Finally comes a
           brief look at some elementary methods for summarizing qualitative and count data.

    qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

    3.1 Elementary Graphical and Tabular
           Treatment of Quantitative Data

                        Almost always, the place to begin in data analysis is to make appropriate graphical
                        and/or tabular displays. Indeed, where only a few samples are involved, a good
                        picture or table can often tell most of the story about the data. This section discusses
                        the usefulness of dot diagrams, stem-and-leaf plots, frequency tables, histograms,
                        scatterplots, and run charts.

    3.1.1  Dot Diagrams and Stem-and-Leaf Plots

           When an engineering study produces a small or moderate amount of univariate
           quantitative data, a dot diagram, easily made with pencil and paper, is often quite
           revealing. A dot diagram shows each observation as a dot placed at a position
           corresponding to its numerical value along a number line.

66
                           3.1 Elementary Graphical and Tabular Treatment of Quantitative Data 67

              Example 1    Portraying Thrust Face Runouts
(Example 1, Chapter 1,
                           Section 1.1 considered a heat treating problem where distortion for gears laid
         revisited--p. 2)  and gears hung was studied. Figure 1.1 has been reproduced here as Figure 3.1.
                           It consists of two dot diagrams, one showing thrust face runout values for gears
                           laid and the other the corresponding values for gears hung, and shows clearly
                           that the laid values are both generally smaller and more consistent than the hung
                           values.

                                  Gears laid

                           0  10  20                       30  40

                                  Runout (.0001 in.)

                                  Gears hung

                           0  10  20                       30  40

                              Figure 3.1 Dot diagrams of runouts

Example 2                  Portraying Bullet Penetration Depths

                           Sale and Thom compared penetration depths for several types of .45 caliber bullets
                           fired into oak wood from a distance of 15 feet. Table 3.1 gives the penetration
                           depths (in mm from the target surface to the back of the bullets) for two bullet
                           types. Figure 3.2 presents a corresponding pair of dot diagrams.

                           Table 3.1                       200 Grain Jacketed Bullets
                           Bullet Penetration Depths (mm)
                                                           63.80, 64.65, 59.50, 60.70,
                           230 Grain Jacketed Bullets      61.30, 61.50, 59.80, 59.10,
                                                           62.95, 63.55, 58.65, 71.70,
                           40.50, 38.35, 56.00, 42.55,     63.30, 62.65, 67.75, 62.30,
                           38.35, 27.75, 49.85, 43.60,     70.40, 64.05, 65.00, 58.00
                           38.75, 51.25, 47.90, 48.15,
                           42.90, 43.85, 37.35, 47.30,
                           41.15, 51.60, 39.75, 41.00
68 Chapter 3 Elementary Descriptive Statistics      230 Grain jacketed bullets

                  Example 2
                 (continued )

20                                              30  40  50            60        70

                                                    Penetration (mm)

                                                    200 Grain jacketed bullets

20                                              30  40  50            60        70

                                                    Penetration (mm)

    Figure 3.2 Dot diagrams of penetration depths

     The dot diagrams show the penetrations of the 200 grain bullets to be both
larger and more consistent than those of the 230 grain bullets. (The students
had predicted larger penetrations for the lighter bullets on the basis of greater
muzzle velocity and smaller surface area on which friction can act. The different
consistencies of penetration were neither expected nor explained.)

     Dot diagrams give the general feel of a data set but do not always allow the
recovery of exactly the values used to make them. A stem-and-leaf plot carries
much the same visual information as a dot diagram while preserving the original
values exactly. A stem-and-leaf plot is made by using the last few digits of each data
point to indicate where it falls.

0 589999
1 00111111122223333444555567789
27
3

0
0 589999
1 00111111122223333444
1 55567789
2
27
3
3

                 Figure 3.3 Stem-and-leaf plots of laid gear runouts (Example 1)
              3.1 Elementary Graphical and Tabular Treatment of Quantitative Data 69

Example 1     Figure 3.3 gives two possible stem-and-leaf plots for the thrust face runouts
(continued )  of laid gears. In both, the first digit of each observation is represented by
              the number to the left of the vertical line or "stem" of the diagram. The
              numbers to the right of the vertical line make up the "leaves" and give the
              second digits of the observed runouts. The second display shows somewhat
              more detail than the first by providing "0-4" and "5-9" leaf positions for each
              possible leading digit, instead of only a single "0-9" leaf for each leading
              digit.

Example 2     Figure 3.4 gives two possible stem-and-leaf plots for the penetrations of 200 grain
(continued )  bullets in Table 3.1. On these, it was convenient to use two digits to the left of
              the decimal point to make the stem and the two following the decimal point to
              create the leaves. The first display was made by recording the leaf values directly
              from the table (from left to right and top to bottom). The second display is a
              better one, obtained by ordering the values that make up each leaf. Notice that
              both plots give essentially the same visual impression as the second dot diagram
              in Figure 3.2.

              58 .65, .00       58 .00, .65
              59 .50, .80, .10  59 .10, .50, .80
              60 .70            60 .70
              61 .30, .50       61 .30, .50
              62 .95, .65, .30  62 .30, .65, .95
              63 .80, .55, .30  63 .30, .55, .80
              64 .65, .05       64 .05, .65
              65 .00            65 .00
              66                66
              67 .75            67 .75
              68                68
              69                69
              70 .40            70 .40
              71 .70            71 .70

              Figure 3.4 Stem-and-leaf plots of
              the 200 grain penetration depths

                   When comparing two data sets, a useful way to use the stem-and-leaf idea is to
              make two plots back-to-back.
70 Chapter 3 Elementary Descriptive Statistics

              Laid runouts                                        Hung runouts

                                                        99998850 788
4443333222211111110001 00001112333

                                                9877655551 57777899
                                                                                     2 011122233334

                                                                                72 778
                                                                                     31
                                                                                     36

Figure 3.5 Back-to-back stem-and-leaf plots of runouts (Example 1)

Example 1     Figure 3.5 gives back-to-back stem-and-leaf plots for the data of Table 1.1 (pg. 3).
(continued )  It shows clearly the differences in location and spread of the two data sets.

3.1.2         Frequency Tables and Histograms

              Dot diagrams and stem-and-leaf plots are useful devices when mulling over a data
              set. But they are not commonly used in presentations and reports. In these more
              formal contexts, frequency tables and histograms are more often used.

                   A frequency table is made by first breaking an interval containing all the data
              into an appropriate number of smaller intervals of equal length. Then tally marks can
              be recorded to indicate the number of data points falling into each interval. Finally,
              frequencies, relative frequencies, and cumulative relative frequencies can be added.

Example 1     Table 3.2 gives one possible frequency table for the laid gear runouts. The relative
(continued )  frequency values are obtained by dividing the entries in the frequency column

              Table 3.2
              Frequency Table for Laid Gear Thrust Face Runouts

               Runout                           Tally  Frequency   Relative     Cumulative
              (.0001 in.)                                         Frequency       Relative
                                                             3                  Frequency
               5-8                                         18         .079
               9 -12                                       12         .474           .079
              13 -16                                         4        .316           .553
              17-20                                          0        .105           .868
              21-24                                          1         0             .974
              25-28                                                   .026           .974
                                                           38                      1.000
                                                                    1.000
                            3.1 Elementary Graphical and Tabular Treatment of Quantitative Data 71

                         by 38, the number of data points. The entries in the cumulative relative frequency
                         column are the ratios of the totals in a given class and all preceding classes to the
                         total number of data points. (Except for round-off, this is the sum of the relative
                         frequencies on the same row and above a given cumulative relative frequency.)
                         The tally column gives the same kind of information about distributional shape
                         that is provided by a dot diagram or a stem-and-leaf plot.

Choosing intervals            The choice of intervals to use in making a frequency table is a matter of
    for a frequency      judgment. Two people will not necessarily choose the same set of intervals. However,
                  table  there are a number of simple points to keep in mind when choosing them. First, in
                         order to avoid visual distortion when using the tally column of the table to gain an
                         impression of distributional shape, intervals of equal length should be employed.
                         Also, for aesthetic reasons, round numbers are preferable as interval endpoints. Since
                         there is usually aggregation (and therefore some loss of information) involved in the
                         reduction of raw data to tallies, the larger the number of intervals used, the more
                         detailed the information portrayed by the table. On the other hand, if a frequency
                         table is to have value as a summarization of data, it can't be cluttered with too many
                         intervals.

                              After making a frequency table, it is common to use the organization provided
                         by the table to create a histogram. A (frequency or relative frequency) histogram is
                         a kind of bar chart used to portray the shape of a distribution of data points.

Example 2                Table 3.3 is a frequency table for the 200 grain bullet penetration depths, and
(continued )             Figure 3.6 is a translation of that table into the form of a histogram.

                         Table 3.3
                         Frequency Table for 200 Grain Penetration Depths

                         Penetration    Tally  Frequency   Relative        Cumulative
                         Depth (mm)                       Frequency          Relative
                                                     5                     Frequency
                         58.00 -59.99                3         .25
                         60.00 - 61.99               6         .15              .25
                         62.00 - 63.99               3         .30              .40
                         64.00 - 65.99               1         .15              .70
                         66.00 - 67.99               0         .05              .85
                         68.00 - 69.99               2         0                .90
                         70.00 -71.99                          .10              .90
                                                   20                          1.00
                                                             1.00
72 Chapter 3 Elementary Descriptive Statistics

Example 2                                                  6
(continued )                                               5
                                                           4
                                                Frequency  3
                                                           2
                                                           1

                                                              60  70

                                                              Penetration depth (mm)

                                                Figure 3.6 Histogram of the 200 grain
                                                penetration depths

                     The vertical scale in Figure 3.6 is a frequency scale, and the histogram is a frequency
                     histogram. By changing to relative frequency on the vertical scale, one can produce
                     a relative frequency histogram. In making Figure 3.6, care was taken to

Guidelines for       1. (continue to) use intervals of equal length,
         making      2. show the entire vertical axis beginning at zero,
                     3. avoid breaking either axis,
    histograms       4. keep a uniform scale across a given axis, and
                     5. center bars of appropriate heights at the midpoints of the (penetration depth)

                         intervals.

        Examples of  Following these guidelines results in a display in which equal enclosed areas cor-
        engineering  respond to equal numbers of data points. Further, data point positioning is clearly
interpretations of   indicated by bar positioning on the horizontal axis. If these guidelines are not fol-
distribution shape   lowed, the resulting bar chart will in one way or another fail to faithfully represent
                     its data set.

                          Figure 3.7 shows terminology for common distributional shapes encountered
                     when making and using dot diagrams, stem-and-leaf plots, and histograms.

                          The graphical and tabular devices discussed to this point are deceptively simple
                     methods. When routinely and intelligently used, they are powerful engineering
                     tools. The information on location, spread, and shape that is portrayed so clearly on
                     a histogram can give strong hints as to the functioning of the physical process that
                     is generating the data. It can also help suggest physical mechanisms at work in the
                     process.

                          For example, if data on the diameters of machined metal cylinders purchased
                     from a vendor produce a histogram that is decidedly bimodal (or multimodal,
                     having several clear humps), this suggests that the machining of the parts was done
3.1 Elementary Graphical and Tabular Treatment of Quantitative Data 73

Bell-shaped  Right-skewed                      Left-skewed

Uniform                      Bimodal           Truncated
             Figure 3.7 Distributional shapes

on more than one machine, or by more than one operator, or at more than one
time. The practical consequence of such multichannel machining is a distribution
of diameters that has more variation than is typical of a production run of cylinders
from a single machine, operator, and setup. As another possibility, if the histogram
is truncated, this might suggest that the lot of cylinders has been 100% inspected
and sorted, removing all cylinders with excessive diameters. Or, upon marking
engineering specifications (requirements) for cylinder diameter on the histogram,
one may get a picture like that in Figure 3.8. It then becomes obvious that the lathe
turning the cylinders needs adjustment in order to increase the typical diameter.
But it also becomes clear that the basic process variation is so large that this
adjustment will fail to bring essentially all diameters into specifications. Armed
with this realization and a knowledge of the economic consequences of parts failing
to meet specifications, an engineer can intelligently weigh alternative courses of
action: sorting of all incoming parts, demanding that the vendor use more precise
equipment, seeking a new vendor, etc.

     Investigating the shape of a data set is useful not only because it can lend insight
into physical mechanisms but also because shape can be important when determining
the appropriateness of methods of formal statistical inference like those discussed
later in this book. A methodology appropriate for one distributional shape may not
be appropriate for another.

             Lower         Upper

             specification specification

                         Cylinder diameter

             Figure 3.8 Histogram marked with
             engineering specifications
74 Chapter 3 Elementary Descriptive Statistics

3.1.3      Scatterplots and Run Charts

           Dot diagrams, stem-and-leaf plots, frequency tables, and histograms are univari-
           ate tools. But engineering data are often multivariate and relationships between
           the variables are then usually of interest. The familiar device of making a two-
           dimensional scatterplot of data pairs is a simple and effective way of displaying
           potential relationships between two variables.

Example 3  Bolt Torques on a Face Plate

           Brenny, Christensen, and Schneider measured the torques required to loosen
           six distinguishable bolts holding the front plate on a type of heavy equipment
           component. Table 3.4 contains the torques (in ft lb) required for bolts number 3
           and 4, respectively, on 34 different components. Figure 3.9 is a scatterplot of the
           bivariate data from Table 3.4. In this figure, where several points must be plotted
           at a single location, the number of points occupying the location has been plotted
           instead of a single dot.

                The plot gives at least a weak indication that large torques at position 3 are
           accompanied by large torques at position 4. In practical terms, this is comforting;

           Table 3.4
           Torques Required to Loosen Two Bolts on Face Plates (ft lb)

           Component  Bolt 3                        Bolt 4  Component  Bolt 3  Bolt 4
                      Torque                        Torque             Torque  Torque

           1                                    16  16      18          15       14
                                                                                 17
           2                                    15  16      19          17       16
                                                                                 18
           3                                    15  17      20          14       16
                                                                                 18
           4                                    15  16      21          17       20
                                                                                 15
           5                                    20  20      22          19       15
                                                                                 20
           6                                    19  16      23          19       18
                                                                                 18
           7                                    19  20      24          19       18
                                                                                 14
           8                                    17  19      25          15       13
                                                                                 17
           9                                    15  15      26          12       16

           10                                   11  15      27          18

           11                                   17  19      28          13

           12                                   18  17      29          14

           13                                   18  14      30          18

           14                                   15  15      31          18

           15                                   18  17      32          15

           16                                   15  17      33          16

           17                                   18  20      34          16
           3.1 Elementary Graphical and Tabular Treatment of Quantitative Data 75

                                  20                         22

                                          2

           Bolt 4 torque (ft lb)      2                      2

                                      22                        2

                                  15  3

                                                             2

                                  10  15                           20

                                      Bolt 3 torque (ft lb)

                                  Figure 3.9 Scatterplot of bolt 3 and bolt 4
                                  torques

           otherwise, unwanted differential forces might act on the face plate. It is also quite
           reasonable that bolt 3 and bolt 4 torques be related, since the bolts were tightened
           by different heads of a single pneumatic wrench operating off a single source of
           compressed air. It stands to reason that variations in air pressure might affect the
           tightening of the bolts at the two positions similarly, producing the big-together,
           small-together pattern seen in Figure 3.9.

                The previous example illustrates the point that relationships seen on scatterplots
           suggest a common physical cause for the behavior of variables and can help reveal
           that cause.

                In the most common version of the scatterplot, the variable on the horizontal
           axis is a time variable. A scatterplot in which univariate data are plotted against time
           order of observation is called a run chart or trend chart. Making run charts is one
           of the most helpful statistical habits an engineer can develop. Seeing patterns on a
           run chart leads to thinking about what process variables were changing in concert
           with the pattern. This can help develop a keener understanding of how process
           behavior is affected by those variables that change over time.

Example 4  Diameters of Consecutive Parts Turned on a Lathe

           Williams and Markowski studied a process for rough turning of the outer diameter
           on the outer race of a constant velocity joint. Table 3.5 gives the diameters (in
           inches above nominal) for 30 consecutive joints turned on a particular automatic
76 Chapter 3 Elementary Descriptive Statistics

Example 4     Table 3.5
(continued )  30 Consecutive Outer Diameters Turned on a Lathe

                                Diameter                                        Diameter
              Joint (inches above nominal)                    Joint (inches above nominal)

              1                                 -.005         16  .015

              2                                 .000          17  .000

              3                                 -.010         18  .000

              4                                 -.030         19  -.015

              5                                 -.010         20  -.015

              6                                 -.025         21  -.005

              7                                 -.030         22  -.015

              8                                 -.035         23  -.015

              9                                 -.025         24  -.010

              10                                -.025         25  -.015

              11                                -.025         26  -.035

              12                                -.035         27  -.025

              13                                -.040         28  -.020

              14                                -.035         29  -.025

              15                                -.035         30  -.015

                  +.020                                +.020

              Diameter.000                             .000
                                         Diameter

                  -.020                                -.020

                  -.040                                -.040

                                                              5 10 15 20 25 30
                                                                    Time of manufacture

              Figure 3.10 Dot diagram and run chart of consecutive outer diameters

              lathe. Figure 3.10 gives both a dot diagram and a run chart for the data in the
              table. In keeping with standard practice, consecutive points on the run chart have
              been connected with line segments.

                   Here the dot diagram is not particularly suggestive of the physical mecha-
              nisms that generated the data. But the time information added in the run chart
              is revealing. Moving along in time, the outer diameters tend to get smaller until
                                       3.2 Quantiles and Related Graphical Tools 77

part 16, where there is a large jump, followed again by a pattern of diameter gen-
erally decreasing in time. In fact, upon checking production records, Williams
and Markowski found that the lathe had been turned off and allowed to cool down
between parts 15 and 16. The pattern seen on the run chart is likely related to the
behavior of the lathe's hydraulics. When cold, the hydraulics probably don't do
as good a job pushing the cutting tool into the part being turned as when they are
warm. Hence, the turned parts become smaller as the lathe warms up. In order
to get parts closer to nominal, the aimed-for diameter might be adjusted up by
about .020 in. and parts run only after warming up the lathe.

Section 1 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. The following are percent yields from 40 runs of       (b) A trick often employed in the analysis of paired
   a chemical process, taken from J. S. Hunter's arti-         data such as these is to reduce the pairs to dif-
   cle "The Technology of Quality" (RCA Engineer,              ferences by subtracting the values of one of the
   May/June 1985):                                             variables from the other. Compute differences
                                                               (top bolt-bottom bolt) here. Then make and
   65.6, 65.6, 66.2, 66.8, 67.2, 67.5, 67.8, 67.8, 68.0,       interpret a dot diagram for these values.
   68.0, 68.2, 68.3, 68.3, 68.4, 68.9, 69.0, 69.1, 69.2,
   69.3, 69.5, 69.5, 69.5, 69.8, 69.9, 70.0, 70.2, 70.4,  Piece  Top Bolt  Bottom Bolt
   70.6, 70.6, 70.7, 70.8, 70.9, 71.3, 71.7, 72.0, 72.6,
   72.7, 72.8, 73.5, 74.2                                    1      110         125
                                                             2      115         115
   Make a dot diagram, a stem-and-leaf plot, a fre-          3      105         125
   quency table, and a histogram of these data.              4      115         115
                                                             5      115         120
2. Make back-to-back stem-and-leaf plots for the two         6      120         120
   samples in Table 3.1.                                     7      110         115
                                                             8      125         125
3. Osborne, Bishop, and Klein collected manufactur-          9      105         110
   ing data on the torques required to loosen bolts        10       130         110
   holding an assembly on a piece of heavy machin-         11        95         120
   ery. The accompanying table shows part of their         12       110         115
   data concerning two particular bolts. The torques       13       110         120
   recorded (in ft lb) were taken from 15 different        14        95         115
   pieces of equipment as they were assembled.             15       105         105
    (a) Make a scatterplot of these paired data. Are
        there any obvious patterns in the plot?

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

3.2 Quantiles and Related Graphical Tools

                    Most readers will be familiar with the concept of a percentile. The notion is most
                    famous in the context of reporting scores on educational achievement tests. For
                    example, if a person has scored at the 80th percentile, roughly 80% of those taking
78 Chapter 3 Elementary Descriptive Statistics

                                  the test had worse scores, and roughly 20% had better scores. This concept is also
                                  useful in the description of engineering data. However, because it is often more
                                  convenient to work in terms of fractions between 0 and 1 rather than in percentages
                                  between 0 and 100, slightly different terminology will be used here: "Quantiles,"
                                  rather than percentiles, will be discussed. After the quantiles of a data set are carefully
                                  defined, they are used to create a number of useful tools of descriptive statistics:
                                  quantile plots, boxplots, Q-Q plots, and normal plots (a type of theoretical Q-Q
                                  plot).

3.2.1                Quantiles and Quantile Plots

                     Roughly speaking, for a number p between 0 and 1, the p quantile of a distribution
                     is a number such that a fraction p of the distribution lies to the left and a fraction
                     1 - p of the distribution lies to the right. However, because of the discreteness of
                     finite data sets, it is necessary to state exactly what will be meant by the terminology.
                     Definition 1 gives the precise convention that will be used in this text.

Definition 1         For a data set consisting of n values that when ordered are x1  x2  · · ·  xn,

                     1.  if  p  =  i -.5  for  a  positive  integer      i   n,  the  p  quantile  of    the  data
                                     n
                         set is

                                                        Q( p) = Q i - .5 = xi
                                                                          n

                         (The    ith  smallest    data  point  will  be  called  the  i -.5  quantile.)
                                                                                        n

                     2.  for any number        p between       .5  and  n-.5  that is not of the form    i -.5  for
                                                               n          n                                n
                         an integer i, the p quantile of the data set will be obtained by linear
                         interpolation between the two values of Q( i-.5 n ) with corresponding
                         i -.5
                           n    that  bracket   p.

                     In both cases, the notation Q( p) will be used to denote the p quantile.

Index (i ) of the         Definition 1 identifies Q( p) for all p between .5/n and (n - .5)/n. To find
  ordered data       Q( p) for such a value of p, one may solve the equation p = (i - .5)/n for i,
    point that is    yielding
              Q( p)
                                                               i = np + .5

                     and locate the "(np + .5)th ordered data point."
                         3.2 Quantiles and Related Graphical Tools 79

Example 5  Quantiles for Dry Breaking Strengths of Paper Towel

           Lee, Sebghati, and Straub did a study of the dry breaking strength of several brands
           of paper towel. Table 3.6 shows ten breaking strengths (in grams) reported by the
           students for a generic towel. By ordering the strength data and computing values
           of 10 i-.5 , one can easily find the .05, .15, .25, . . . , .85, and .95 quantiles of the
           breaking strength distribution, as shown in Table 3.7.

                Since there are n = 10 data points, each one accounts for 10% of the data set.
           Applying convention (1) in Definition 1 to find (for example) the .35 quantile,

                     Table 3.6
                     Ten Paper Towel Breaking
                     Strengths

                     Test Breaking Strength (g)

                     1   8,577

                     2   9,471

                     3   9,011

                     4   7,583

                     5   8,572

                     6   10,688

                     7   9,614

                     8   9,614

                     9   8,527

                     10  9,165

           Table 3.7
           Quantiles of the Paper Towel Breaking Strength
           Distribution

           i  i -.5  ith Smallest Data Point, xi = Q       i -.5
               10                                           10

            1 .05         7,583 = Q(.05)
            2 .15         8,527 = Q(.15)
            3 .25         8,572 = Q(.25)
            4 .35         8,577 = Q(.35)
            5 .45         9,011 = Q(.45)
            6 .55         9,165 = Q(.55)
            7 .65         9,471 = Q(.65)
            8 .75         9,614 = Q(.75)
            9 .85         9,614 = Q(.85)
           10 .95        10,688 = Q(.95)
80 Chapter 3 Elementary Descriptive Statistics

Example 5     the smallest 3 data points and half of the fourth smallest are counted as lying to
(continued )
              the left of the desired number, and the largest 6 data points and half of the seventh

              largest are counted as lying to the right. Thus, the fourth smallest data point must

              be the .35 quantile, as is shown in Table 3.7.

                    To illustrate convention (2) of Definition 1, consider finding the .5 and .93
                                                                                          .5-.45
              quantiles  of  the  strength                distribution.   Since   .5  is  .55-.45  =  .5   of  the  way  from  .45

              to .55, linear interpolation gives

              Q(.5) = (1 - .5) Q(.45) + .5 Q(.55) = .5(9,011) + .5(9,165) = 9,088 g

              Then,  observing      that           .93    is  .93-.85  =  .8  of  the  way  from      .85  to  .95,  linear  inter-
                                                              .95-.85
              polation gives

              Q(.93) = (1 - .8) Q(.85) + .8Q(.95) = .2(9,614) + .8(10,688) = 10,473.2 g

                   Particular round values of p give quantiles Q( p) that are known by special
              names.

Definition 2 Q(.5) is called the median of a distribution.

Definition 3  Q(.25) and Q(.75) are called the first (or lower) quartile and third (or
              upper) quartile of a distribution, respectively.

Example 5     Referring again to Table 3.7 and the value of Q(.5) previously computed, for the
(continued )  breaking strength distribution

                                               Median = Q(.5) = 9,088 g
                                          1st quartile = Q(.25) = 8,572 g
                                          3rd quartile = Q(.75) = 9,614 g

               A way of representing the quantile idea graphically is to make a quantile plot.

Definition 4  A quantile plot is a plot of Q( p) versus p. For an ordered data set of size

              n containing values x1  x2  · · ·  xn, such a display is made by first plot-
                                    i -.5
              ting  the  points  (    n         ,  xi  )  and  then    connecting     consecutive     plotted  points    with

              straight-line segments.

              It is because convention (2) in Definition 1 calls for linear interpolation that straight-
              line segments enter the picture in making a quantile plot.
                                                  3.2 Quantiles and Related Graphical Tools 81

Example 5       Referring  again  to  Table  3.7  for  the  i -.5  quantiles  of  the  breaking  strength  distri-
(continued )                                                 10
                bution, it is clear that a quantile plot for these data will involve plotting and then

                connecting consecutive ones of the following ordered pairs.

                                  (.05, 7,583)         (.15, 8,527)  (.25, 8,572)
                                  (.35, 8,577)         (.45, 9,011)  (.55, 9,165)
                                  (.65, 9,471)         (.75, 9,614)  (.85, 9,614)
                                  (.95, 10,688)

                Figure 3.11 gives such a plot.

                                      Q( p)

                                          10,000

                                            9,000

                                            8,000

                                            7,000
                                                        .1 .2 .3 .4 .5 .6 .7 .8 .9 p

                                              Figure 3.11 Quantile plot of paper towel
                                              strengths

                A quantile plot allows the user to do some informal visual smoothing of the plot to
                compensate for any jaggedness. (The tacit assumption is that the underlying data-
                generating mechanism would itself produce smoother and smoother quantile plots
                for larger and larger samples.)

3.2.2           Boxplots

                Familiarity with the quantile idea is the principal prerequisite for making boxplots,
                an alternative to dot diagrams or histograms. The boxplot carries somewhat less
                information, but it has the advantage that many can be placed side-by-side on a
                single page for comparison purposes.

                     There are several common conventions for making boxplots. The one that will
                be used here is illustrated in generic fashion in Figure 3.12. A box is made to extend
                from the first to the third quartiles and is divided by a line at the median. Then the
                interquartile range

Interquartile                                IQR= Q(.75) - Q(.25)
         range
82 Chapter 3 Elementary Descriptive Statistics

              1.5 IQR                           IQR            1.5 IQR

                                Q(.25)          Q(.5)  Q(.75)

              Smallest data                                    Largest data
              point bigger than                                point less than
              or equal to                                      or equal to
              Q(.25) - 1.5IQR                                  Q(.75) + 1.5IQR

              Any points not in the interval [Q(.25) - 1.5IQR, Q(.75) + 1.5IQR]
              are plotted separately

                                   Figure 3.12 Generic boxplot

              is calculated and the smallest data point within 1.5IQR of Q(.25) and the largest
              data point within 1.5IQR of Q(.75) are determined. Lines called whiskers are made
              to extend out from the box to these values. Typically, most data points will be within
              the interval [Q(.25) - 1.5IQR, Q(.75) + 1.5IQR]. Any that are not then get plotted
              individually and are thereby identified as outlying or unusual.

Example 5     Consider making a boxplot for the paper towel breaking strength data. To begin,
(continued )                                       Q(.25) = 8,572 g
                                                     Q(.5) = 9,088 g
                                                   Q(.75) = 9,614 g

              So
                             IQR = Q(.75) - Q(.25) = 9,614 - 8,572 = 1,042 g

              and
                                                   1.5IQR = 1,563 g

              Then
                                 Q(.75) + 1.5IQR = 9,614 + 1,563 = 11,177 g

              and
                                  Q(.25) - 1.5IQR = 8,572 - 1,563 = 7,009 g
                                                                   3.2 Quantiles and Related Graphical Tools 83

                            Since all the data points lie in the range 7,009 g to 11,177 g, the boxplot is as
                            shown in Figure 3.13.

                                                            9,088

                                                       7,583 8,572                 10,688
                                                                         9,614

                                                  7,000     8,000 9,000 10,000 11,000
                                                              Breaking strength (g)

                                                  Figure 3.13 Boxplot of the paper towel
                                                  strengths

                                 A boxplot shows distributional location through the placement of the box and
                            whiskers along a number line. It shows distributional spread through the extent of
                            the box and the whiskers, with the box enclosing the middle 50% of the distribution.
                            Some elements of distributional shape are indicated by the symmetry (or lack
                            thereof) of the box and of the whiskers. And a gap between the end of a whisker
                            and a separately plotted point serves as a reminder that no data values fall in that
                            interval.

                                 Two or more boxplots drawn to the same scale and side by side provide an
                            effective way of comparing samples.

            Example 6       More on Bullet Penetration Depths
(Example 2, page 67,
                            Table  3.8  contains  the  raw  information  needed  to  find  the  i -.5  quantiles  for  the
               revisited )                                                                       20
                            two distributions of bullet penetration depth introduced in the previous section.

                            For the 230 grain bullet penetration depths, interpolation yields

                                Q(.25) = .5Q(.225) + .5Q(.275) = .5(38.75) + .5(39.75) = 39.25 mm
                                 Q(.5) = .5Q(.475) + .5Q(.525) = .5(42.55) + .5(42.90) = 42.725 mm
                                Q(.75) = .5Q(.725) + .5Q(.775) = .5(47.90) + .5(48.15) = 48.025 mm

                            So

                                                        IQR = 48.025 - 39.25 = 8.775 mm
                                                    1.5IQR = 13.163 mm
                                        Q(.75) + 1.5IQR = 61.188 mm
                                        Q(.25) - 1.5IQR = 26.087 mm
84 Chapter 3 Elementary Descriptive Statistics

Example 6     Similar calculations for the 200 grain bullet penetration depths yield
(continued )
                                                        Q(.25) = 60.25 mm
                                                         Q(.5) = 62.80 mm
                                                        Q(.75) = 64.35 mm
                                           Q(.75) + 1.5IQR = 70.50 mm
                                           Q(.25) - 1.5IQR = 54.10 mm

              Table 3.8
              Quantiles of the Bullet Penetration Depth Distributions

                              ith Smallest 230 Grain ith Smallest 200 Grain
              i i-.5 20 Data Point = Q( i-.5 20 ) Data Point = Q( i-.5 20 )

               1 .025                           27.75  58.00
               2 .075                           37.35  58.65
               3 .125                           38.35  59.10
               4 .175                           38.35  59.50
               5 .225                           38.75  59.80
               6 .275                           39.75  60.70
               7 .325                           40.50  61.30
               8 .375                           41.00  61.50
               9 .425                           41.15  62.30
              10 .475                           42.55  62.65
              11 .525                           42.90  62.95
              12 .575                           43.60  63.30
              13 .625                           43.85  63.55
              14 .675                           47.30  63.80
              15 .725                           47.90  64.05
              16 .775                           48.15  64.65
              17 .825                           49.85  65.00
              18 .875                           51.25  67.75
              19 .925                           51.60  70.40
              20 .975                           56.00  71.70

                   Figure 3.14 then shows boxplots placed side by side on the same scale. The
              plots show the larger and more consistent penetration depths of the 200 grain
              bullets. They also show the existence of one particularly extreme data point in
              the 200 grain data set. Further, the relative lengths of the whiskers hint at some
              skewness (recall the terminology introduced with Figure 3.7) in the data. And
              all of this is done in a way that is quite uncluttered and compact. Many more of
                                         3.2 Quantiles and Related Graphical Tools 85

                                     70

             Penetration depth (mm)      200 Grain

                                     60                              bullets

                                     50

                                                          230 Grain
                                     40 bullets

                                     30

             Figure 3.14 Side-by-side boxplots for
             the bullet penetration depths

       these boxes could be added to Figure 3.14 (to compare other bullet types) without
       visual overload.

3.2.3  Q-Q Plots and Comparing Distributional Shapes

       It is often important to compare the shapes of two distributions. Comparing his-
       tograms is one rough way of doing this. A more sensitive way is to make a single
       plot based on the quantile functions for the two distributions and exploit the fact
       that "equal shape" is equivalent to "linearly related quantile functions." Such a plot
       is called a quantile-quantile plot or, more briefly, a Q-Q plot.

            Consider the two small artificial data sets given in Table 3.9. Dot diagrams of
       these two data sets are given in Figure 3.15. The two data sets have the same shape.
       But why is this so? One way to look at the equality of the shapes is to note that

       ith smallest value in data set 2 = 2 ith smallest value in data set 1 + 1 (3.1)

       Then, recognizing ordered data values as quantiles and letting Q1 and Q2 stand for
       the quantile functions of the two respective data sets, it is clear from display (3.1)

       that

                                         Q2(p) = 2Q1(p) + 1                   (3.2)

             Table 3.9
             Two Small Artificial Data Sets

             Data Set 1                  Data Set 2

             3, 5, 4, 7, 3               15, 7, 9, 7, 11
86 Chapter 3 Elementary Descriptive Statistics

                                   Data set 1                                             Q2( p)
                                                                                    15
                         345678                                                     14
                                                                                    13
                                   Data set 2                                       12
                                                                                    11
                         7 9 11 13 15 17                                            10
                   Figure 3.15 Dot diagrams for two                                   9
                   small data sets                                                    8
                                                                                      72

                                                                                            3 4 5 6 7 Q1( p)

                                                                                   Figure 3.16 Q- Q plot for the data
                                                                                   of Table 3.9

                   That is, the two data sets have quantile functions that are linearly related. Looking
                   at either display (3.1) or (3.2), it is obvious that a plot of the points

                                                      i - .5                  i - .5
                                                Q1 5 ,                  Q2 5

                   (for i = 1, 2, 3, 4, 5) should be exactly linear. Figure 3.16 illustrates this--in fact
                   Figure 3.16 is a Q-Q plot for the data sets of Table 3.9.

Definition 5       A Q-Q plot for two data sets with respective quantile functions Q1 and Q2 is

                   a plot of ordered pairs (Q1( p), Q2( p)) for appropriate values of p. When two
                   data sets of size n are involved, the values of p used to make the plot will be
                   i -.5
                     n    for  i  =  1, 2, . . . , n.  When  two  data  sets  of   unequal    sizes  are  involved,    the

                   values  of     p  used  to   make   the  plot  will  be  i -.5  for  i  =  1, 2, . . . , n,  where  n  is
                                                                              n
                   the size of the smaller set.

                   To make a Q-Q plot for two data sets of the same size,

Steps in making    1. order each from the smallest observation to the largest,
       a Q-Q plot
                   2. pair off corresponding values in the two data sets, and

                   3. plot ordered pairs, with the horizontal coordinates coming from the first data
                       set and the vertical ones from the second.

                   When data sets of unequal size are involved, the ordered values from the smaller
                   data set must be paired with quantiles of the larger data set obtained by interpolation.
                                                         3.2 Quantiles and Related Graphical Tools 87

                   A Q-Q plot that is reasonably linear indicates the two distributions involved have
              similar shapes. When there are significant departures from linearity, the character
              of those departures reveals the ways in which the shapes differ.

Example 6     Returning again to the bullet penetration depths, Table 3.8 (page 84) gives the
(continued )  raw material for making a Q-Q plot. The depths on each row of that table need
              only be paired and plotted in order to make the plot given in Figure 3.17.

                   The scatterplot in Figure 3.17 is not terribly linear when looked at as a whole.
              However, the points corresponding to the 2nd through 13th smallest values in
              each data set do look fairly linear, indicating that (except for the extreme lower
              ends) the lower ends of the two distributions have similar shapes.

                   The horizontal jog the plot takes between the 13th and 14th plotted points
              indicates that the gap between 43.85 mm and 47.30 mm (for the 230 grain data)
              is out of proportion to the gap between 63.55 and 63.80 mm (for the 200 grain
              data). This hints that there was some kind of basic physical difference in the
              mechanisms that produced the smaller and larger 230 grain penetration depths.
              Once this kind of indication is discovered, it is a task for ballistics experts or
              materials people to explain the phenomenon.

                   Because of the marked departure from linearity produced by the 1st plotted
              point (27.75, 58.00), there is also a drastic difference in the shapes of the extreme
              lower ends of the two distributions. In order to move that point back on line with
              the rest of the plotted points, it would need to be moved to the right or down
              (i.e., increase the smallest 230 grain observation or decrease the smallest 200
              grain observation). That is, relative to the 200 grain distribution, the 230 grain
              distribution is long-tailed to the low side. (Or to put it differently, relative to
              the 230 grain distribution, the 200 grain distribution is short-tailed to the low
              side.) Note that the difference in shapes was already evident in the boxplot in
              Figure 3.14. Again, it would remain for a specialist to explain this difference in
              distributional shapes.

              200 Grain pentration (mm)  70

                                         60

                                         50

                                         20  30  40  50                  60

                                             230 Grain penetration (mm)

              Figure 3.17 Q-Q plot for the bullet penetration depths
88 Chapter 3 Elementary Descriptive Statistics

                                       The Q-Q plotting idea is useful when applied to two data sets, and it is easiest to
                                  explain the notion in such an "empirical versus empirical" context. But its greatest
                                  usefulness is really when it is applied to one quantile function that represents a data
                                  set and a second that represents a theoretical distribution.

Definition 6             A theoretical Q-Q plot or probability plot for a data set of size n and a

                         theoretical distribution, with respective quantile functions Q1 and Q2, is a plot
                         of ordered pairs (Q1( p), Q2( p)) for appropriate values of p. In this text, the
                                                       i -.5          =
                         values  of  p  of  the  form    n    for  i     1, 2, . . . , n  will  be  used.

                              Recognizing Q1( n i-.5 ) as the ith smallest data point, one sees that a theoretical
                         Q-Q plot is a plot of points with horizontal plotting positions equal to observed data

                         and vertical plotting positions equal to quantiles of the theoretical distribution. That
                         is, with ordered data x1  x2  · · ·  xn, the points

  Ordered pairs                                                         i - .5
        making a                                              xi , Q2 n

probability plot

 Normal                  are plotted. Such a plot allows one to ask, "Does the data set have a shape similar to
plotting                 the theoretical distribution?"

                              The most famous version of the theoretical Q-Q plot occurs when quantiles for
                         the standard normal or Gaussian distribution are employed. This is the familiar
                         bell-shaped distribution. Table 3.10 gives some quantiles of this distribution. In
                         order to find Q( p) for p equal to one of the values .01, .02, . . . , .98, .99, locate the
                         entry in the row labeled by the first digit after the decimal place and in the column
                         labeled by the second digit after the decimal place. (For example, Q(.37) = -.33.)
                         A simple numerical approximation to the values given in Table 3.10 adequate for
                         most plotting purposes is

Approximate standard                                   Q( p)  4.9( p.14 - (1 - p).14)                      (3.3)
       normal quantiles

                              The origin of Table 3.10 is not obvious at this point. It will be explained in
                         Section 5.2, but for the time being consider the following crude argument to the
                         effect that the quantiles in the table correspond to a bell-shaped distribution. Imagine
                         that each entry in Table 3.10 corresponds to a data point in a set of size n = 99. A
                         possible frequency table for those 99 data points is given as Table 3.11. The tally
                         column in Table 3.11 shows clearly the bell shape.

                              The standard normal quantiles can be used to make a theoretical Q-Q plot as
                         a way of assessing how bell-shaped a data set looks. The resulting plot is called a
                         normal (probability) plot.
                                                      3.2 Quantiles and Related Graphical Tools 89

Table 3.10
Standard Normal Quantiles

    .00  .01               .02     .03     .04          .05     .06     .07     .08     .09

.0       -2.33 -2.05            -1.88   -1.75        -1.65   -1.55   -1.48   -1.41   -1.34
                                -1.13   -1.08        -1.04    -.99    -.95    -.92    -.88
.1 -1.28 -1.23 -1.18             -.74    -.71         -.67    -.64    -.61    -.58    -.55
                                 -.44    -.41         -.39    -.36    -.33    -.31    -.28
.2 -.84 -.81 -.77                -.18    -.15         -.13    -.10    -.08    -.05    -.03

.3 -.52 -.50 -.47                  .08     .10          .13     .15     .18     .20     .23
                                   .33     .36          .39     .41     .44     .47     .50
.4 -.25 -.23 -.20                  .61     .64          .67     .71     .74     .77     .81
                                   .95     .99         1.04    1.08    1.13    1.18    1.23
.5 0.00  .03               .05    1.48    1.55         1.65    1.75    1.88    2.05    2.33

.6  .25  .28               .31

.7  .52  .55               .58

.8  .84  .88               .92

.9 1.28 1.34 1.41

                                Table 3.11
                                A Frequency Table for the Standard Normal Quantiles

                                        Value                Tally   Frequency

                                -2.80 to -2.30                               1

                                -2.29 to -1.79                               2

                                -1.78 to -1.28                               7

                                -1.27 to -.77                                12

                                        -.76 to -.26                         17

                                        -.25 to .25                          21

                                        .26 to .76                           17

                                        .77 to 1.27                          12

                                        1.28 to 1.78                         7

                                        1.79 to 2.29                         2

                                        2.30 to 2.80                         1

    Example 5              Consider again the paper towel strength testing scenario and now the issue of
    (continued )           how bell-shaped the data set in Table 3.6 (page 79) is. Table 3.12 was made using
                           Tables 3.7 (page 79) and 3.10; it gives the information needed to produce the
                           theoretical Q-Q plot in Figure 3.18.

                                Considering the small size of the data set involved, the plot in Figure 3.18
                           is fairly linear, and so the data set is reasonably bell-shaped. As a practical
                           consequence of this judgment, it is then possible to use the normal probability
                           models discussed in Section 5.2 to describe breaking strength. These could be
                           employed to make breaking strength predictions, and methods of formal statistical
                           inference based on them could be used in the analysis of breaking strength data.
90 Chapter 3 Elementary Descriptive Statistics

Example 5                          Table 3.12
(continued )                       Breaking Strength and Standard Normal Quantiles

                                                  i -.5                           i-.5 10 Breaking              i-.5 10 Standard
                                                   10
                                         i                                        Strength Quantile             Normal Quantile

                                    1 .05                                                    7,583              -1.65
                                    2 .15                                                    8,527              -1.04
                                    3 .25                                                    8,572
                                    4 .35                                                    8,577               -.67
                                    5 .45                                                    9,011               -.39
                                    6 .55                                                    9,165               -.13
                                    7 .65                                                    9,471
                                    8 .75                                                    9,614                 .13
                                    9 .85                                                    9,614                 .39
                                   10 .95                                                   10,688                 .67
                                                                                                                  1.04
                                                                                                                  1.65

                                                  Standard normal quantile   2.0
                                                                             1.0

                                                                               0
                                                                            -1.0

                                                      7,000 8,000 9,000 10,000 11,000
                                                                Breaking strength quantile (g)

                                                  Figure 3.18 Theoretical Q-Q plot for the
                                                  paper towel strengths

                      Special graph paper, called normal probability paper (or just probability

              paper), is available as an alternative way of making normal plots. Instead of plotting

              points on regular graph paper using vertical plotting positions taken from Table 3.10,

              points are plotted on probability paper using vertical plotting positions of the form
                 -.5
              i  n    .  F  igure  3.19  i  s  a  normal                          plot  of  t  he  breaking  s  trength  data  from  Example  5  made

              on probability paper. Observe that this is virtually identical to the plot in Figure 3.18.

                      Normal plots are not the only kind of theoretical Q-Q plots useful to engineers.

              Many other types of theoretical distributions are of engineering importance, and

              each can be used to make theoretical Q-Q plots. This point is discussed in more
                                         3.2 Quantiles and Related Graphical Tools 91

2 1 0.5 0.2 0.1 0.05 0.01                                                                                     99.99

                                                                                                              99.8 99.9

                                                                                                              98 99

5                                                                                                             95

10                                                                                                            90

80 70 60 50 40 30 20                                                                                          20 30 40 50 60 70 80

90                                                                                                            10

95                                                                                                            5

99 98                                                                                                         0.01 0.05 0.1 0.2 0.5 1 2

99.9 99.8

99.99                      7,000  8,000  9,000  10,000

                           Figure 3.19 Normal plot for the paper towel strengths (made on probability paper,
                           used with permission of the Keuffel and Esser Company)

detail in Section 5.3, but the introduction of theoretical Q-Q plotting here makes it
possible to emphasize the relationship between probability plotting and (empirical)
Q-Q plotting.
92 Chapter 3 Elementary Descriptive Statistics

Section 2 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. The following are data (from Introduction to Con-         (a) Make quantile plots for these two samples.
   temporary Statistical Methods by L. H. Koopmans)              Find the medians, the quartiles, and the .37
   on the impact strength of sheets of insulating ma-            quantiles for the two data sets.
   terial cut in two different ways. (The values are in
   ft lb.)                                                  (b) Draw (to scale) carefully labeled side-by-side
                                                                 boxplots for comparing the two cutting meth-
Lengthwise Cuts  Crosswise Cuts                                  ods. Discuss what these show about the two
                                                                 methods.
       1.15             .89
         .84            .69                                  (c) Make and discuss the appearance of a Q-Q plot
         .88            .46                                      for comparing the shapes of these two data sets.
         .91            .85
         .86            .73                              2. Make a Q-Q plot for the two small samples in
         .88            .67                                 Table 3.13 in Section 3.3.
         .92            .78
         .87            .77                              3. Make and interpret a normal plot for the yield data
         .93            .80                                 of Exercise 1 of Section 3.1.
         .95            .79
                                                         4. Explain the usefulness of theoretical Q-Q plotting.

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

3.3 Standard Numerical Summary Measures

                    The smooth functioning of most modern technology depends on the reduction
                    of large amounts of data to a few informative numerical summary values. For
                    example, over the period of a month, a lab doing compressive strength testing for
                    a manufacturer's concrete blocks may make hundreds or even thousands of such
                    measurements. But for some purposes, it may be adequate to know that those
                    strengths average 4,618 psi with a range of 2,521 psi (from smallest to largest).

                         In this section, several standard summary measures for quantitative data are
                    discussed, including the mean, median, range, and standard deviation. Measures of
                    location are considered first, then measures of spread. There follows a discussion
                    of the difference between sample statistics and population parameters and then
                    illustrations of how numerical summaries can be effectively used in simple plots to
                    clarify the results of statistical engineering studies. Finally, there is a brief discussion
                    of the use of personal computer software in elementary data summarization.

3.3.1            Measures of Location

                 Most people are familiar with the concept of an "average" as being representative
                 of, or in the center of, a data set. Temperatures may vary between different locations
                 in a blast furnace, but an average temperature tells something about a middle or
                 representative temperature. Scores on an exam may vary, but one is relieved to score
                 at least above average.
                                                      3.3 Standard Numerical Summary Measures 93

                   The word average, as used in colloquial speech, has several potential technical
              meanings. One is the median, Q(.5), which was introduced in the last section. The
              median divides a data set in half. Roughly half of the area enclosed by the bars of a
              well-made histogram will lie to either side of the median. As a measure of center,
              it is completely insensitive to the effects of a few extreme or outlying observations.
              For example, the small set of data

                                                        2, 3, 6, 9, 10

              has median 6, and this remains true even if the value 10 is replaced by 10,000,000
              and/or the value 2 is replaced by -200,000.

                   The previous section used the median as a center value in the making of boxplots.
              But the median is not the technical meaning most often attached to the notion of
              average in statistical analyses. Instead, it is more common to employ the (arithmetic)
              mean.

Definition 7  The (arithmetic) mean of a sample of quantitative data (say, x1, x2, . . . , xn) is
                                                          1n

                                                    x¯ = n xi

                                                                                  i =1

              The mean is sometimes called the first moment or center of mass of a distribution,
              drawing on an analogy to mechanics. Think of placing a unit mass along the number
              line at the location of each value in a data set--the balance point of the mass
              distribution is at x¯.

Example 7     Waste on Bulk Paper Rolls

              Hall, Luethe, Pelszynski, and Ringhofer worked with a company that cuts paper
              from large rolls purchased in bulk from several suppliers. The company was
              interested in determining the amount of waste (by weight) on rolls obtained
              from the various sources. Table 3.13 gives percent waste data, which the students
              obtained for six and eight rolls, respectively, of paper purchased from two different
              sources.

                   The medians and means for the two data sets are easily obtained. For the
              supplier 1 data,

                                    Q(.5) = .5(.65) + .5(.92) = .785% waste

              and
                        x¯ = 1 (.37 + .52 + .65 + .92 + 2.89 + 3.62) = 1.495% waste
                              6
94 Chapter 3 Elementary Descriptive Statistics

Example 7        Table 3.13
(continued )     Percent Waste by Weight on Bulk Paper Rolls

                 Supplier 1                                   Supplier 2

                 .37, .52, .65,                               .89, .99, 1.45, 1.47,
                 .92, 2.89, 3.62                              1.58, 2.27, 2.63, 6.54

              For the supplier 2 data,

                                  Q(.5) = .5(1.47) + .5(1.58) = 1.525% waste

              and

                        x¯ = 1 (.89 + .99 + 1.45 + 1.47 + 1.58 + 2.27 + 2.63 + 6.54)
                              8

                          = 2.228% waste

              Figure 3.20 shows dot diagrams with the medians and means marked. Notice
              that a comparison of either medians or means for the two suppliers shows the
              supplier 2 waste to be larger than the supplier 1 waste. But there is a substan-
              tial difference between the median and mean values for a given supplier. In
              both cases, the mean is quite a bit larger than the corresponding median. This
              reflects the right-skewed nature of both data sets. In both cases, the center of
              mass of the distribution is pulled strongly to the right by a few extremely large
              values.

                                                           Supplier 1

                 Q(.5) = .785

              0  1                              2          3  4             5         6

                                                x = 1.495

                                                           Waste (percent)

                                                           Supplier 2

                    Q(.5) = 1.525

              0  1                              2          3  4             5         6

                                                x = 2.228
                                                       Waste (percent)

                 Figure 3.20 Dot diagrams for the waste percentages
                                                      3.3 Standard Numerical Summary Measures 95

                   Example 7 shows clearly that, in contrast to the median, the mean is a mea-
              sure of center that can be strongly affected by a few extreme data values. People
              sometimes say that because of this, one or the other of the two measures is "better."
              Such statements lack sense. Neither is better; they are simply measures with dif-
              ferent properties. And the difference is one that intelligent consumers of statistical
              information do well to keep in mind. The "average" income of employees at a com-
              pany paying nine workers each $10,000/year and a president $110,000/year can be
              described as $10,000/year or $20,000/year, depending upon whether the median or
              mean is being used.

3.3.2         Measures of Spread

              Quantifying the variation in a data set can be as important as measuring its location.
              In manufacturing, for example, if a characteristic of parts coming off a particular
              machine is being measured and recorded, the spread of the resulting data gives
              information about the intrinsic precision or capability of the machine. The location
              of the resulting data is often a function of machine setup or settings of adjustment
              knobs. Setups can fairly easily be changed, but improvement of intrinsic machine
              precision usually requires a capital expenditure for a new piece of equipment or
              overhaul of an existing one.

                   Although the point wasn't stressed in Section 3.2, the interquartile range,
              IQR = Q(.75) - Q(.25), is one possible measure of spread for a distribution. It
              measures the spread of the middle half of a distribution. Therefore, it is insensitive
              to the possibility of a few extreme values occurring in a data set. A related measure
              is the range, which indicates the spread of the entire distribution.

Definition 8  The range of a data set consisting of ordered values x1  x2  · · ·  xn is
                                                     R = xn - x1

              Notice the word usage here. The word range could be used as a verb to say, "The
              data range from 3 to 21." But to use the word as a noun, one says, "The range is
              (21 - 3) = 18." Since the range depends only on the values of the smallest and
              largest points in a data set, it is necessarily highly sensitive to extreme (or outlying)
              values. Because it is easily calculated, it has enjoyed long-standing popularity in
              industrial settings, particularly as a tool in statistical quality control.

                   However, most methods of formal statistical inference are based on another mea-
              sure of distributional spread. A notion of "mean squared deviation" or "root mean
              squared deviation" is employed to produce measures that are called the variance
              and the standard deviation, respectively.
96 Chapter 3 Elementary Descriptive Statistics

Definition 9  The sample variance of a data set consisting of values x1, x2, . . . , xn is

                                                1  n

                                                s2 = n - 1 (xi - x¯ )2

                                                   i =1

              The sample standard deviation, s, is the nonnegative square root of the
              sample variance.

                   Apart from an exchange of n - 1 for n in the divisor, s2 is an average squared
              distance of the data points from the central value x¯ . Thus, s2 is nonnegative and
              is 0 only when all data points are exactly alike. The units of s2 are the squares of
              the units in which the original data are expressed. Taking the square root of s2 to

              obtain s then produces a measure of spread expressed in the original units.

Example 7     The spreads in the two sets of percentage wastes recorded in Table 3.13 can be
(continued )  expressed in any of the preceding terms. For the supplier 1 data,

                                                Q(.25) = .52

                                                Q(.75) = 2.89

              and so

                                                IQR = 2.89 - .52 = 2.37% waste

              Also,

                                                R = 3.62 - .37 = 3.25% waste

              Further,

              s2 = 1 ((.37 - 1.495)2 + (.52 - 1.495)2 + (.65 - 1.495)2 + (.92 - 1.495)2
                    6-1
                     + (2.89 - 1.495)2 + (3.62 - 1.495)2)

                 = 1.945(% waste)2

              so that

                                                     
                                                s = 1.945 = 1.394% waste
                                                                   3.3 Standard Numerical Summary Measures 97

                              Similar calculations for the supplier 2 data yield the values

                                                                  IQR = 1.23% waste

                              and

                                                           R = 6.54 - .89 = 5.65% waste

                              Further,

                              s2 = 1 ((.89 - 2.228)2 + (.99 - 2.228)2 + (1.45 - 2.228)2 + (1.47 - 2.228)2
                                     8-1
                                     + (1.58 - 2.228)2 + (2.27 - 2.228)2 + (2.63 - 2.228)2 + (6.54 - 2.228)2)

                                 = 3.383(% waste)2

                              so

                                                                   s = 1.839% waste

                              Supplier 2 has the smaller IQR but the larger R and s. This is consistent with
                              Figure 3.20. The central portion of the supplier 2 distribution is tightly packed.
                              But the single extreme data point makes the overall variability larger for the
                              second supplier than for the first.

                                The calculation of sample variances just illustrated is meant simply to reinforce
                           the fact that s2 is a kind of mean squared deviation. Of course, the most sensible
                           way to find sample variances in practice is by using either a handheld electronic
                           calculator with a preprogrammed variance function or a statistical package on a
                           personal computer.

                                The measures of variation, IQR, R, and s, are not directly comparable. Although
                           it is somewhat out of the main flow of this discussion, it is worth interjecting at this
                           point that it is possible to "put R and s on the same scale." This is done by dividing
                           R by an appropriate conversion factor, known to quality control engineers as d2.
                           Table B.2 contains control chart constants and gives values of d2 for various sample
                           sizes n. For example, to get R and s on the same scale for the supplier 1 data,
                           division of R by 2.534 is in order, since n = 6.

                                Students often have some initial difficulty developing a feel for the meaning
                           of the standard deviation. One possible help in this effort is a famous theorem of a
                           Russian mathematician.

            Proposition 1  For any data set and any number k larger than 1, a fraction of at least 1 - (1/k2)
(Chebyschev's Theorem )    of the data are within ks of x¯ .
98 Chapter 3 Elementary Descriptive Statistics

               This  little   theorem      says,  for  example,      that   at  least  3  of  a  data  set  is  within  2  standard
                                                                                       4
                                                                  8
               deviations     of  its  mean.    And    at  least  9  of  a  data  set  is  within  3   standard   deviations      of

               its mean. So the theorem promises that if a data set has a small standard deviation,

               it will be tightly packed about its mean.

Example 7      Returning to the waste data, consider illustrating the meaning of Chebyschev's
(continued )
               theorem            with  the   supplier     1  values.    For    example,         taking  k  = 2,  at    least  3  =
                                                                                                                               4
                     -     1  )2
               1        (  2      of  the  6  data  points  (i.e.,   at  least  4.5  of   them)  must    be     within  2  standard

               deviations of x¯ . In fact

                                           x¯ - 2s = 1.495 - 2(1.394) = -1.294% waste

               and

                                              x¯ + 2s = 1.495 + 2(1.394) = 4.284% waste

               so simple counting shows that all (a fraction of 1.0) of the data are between these
               two values.

3.3.3          Statistics and Parameters

               At this point, it is important to introduce some more basic terminology. Jargon and
               notation for distributions of samples are somewhat different than for population
               distributions (and theoretical distributions).

Definition 10  Numerical summarizations of sample data are called (sample) statistics. Nu-
               merical summarizations of population and theoretical distributions are called
               (population or model) parameters. Typically, Roman letters are used as sym-
               bols for statistics, and Greek letters are used to stand for parameters.

               As an example, consider the mean. Definition 7 refers specifically to a calculation
               for a sample. If a data set represents an entire population, then it is common to use
               the lowercase Greek letter mu (µ) to stand for the population mean and to write

Population                                                           1N                                                        (3.4)
       mean                                                   µ = N xi

                                                                            i =1

               Comparing this expression to the one in Definition 7, not only is a different symbol
               used for the mean but also N is used in place of n. It is standard to denote a
               population size as N and a sample size as n. Chapter 5 gives a definition for the
                                                                  3.3 Standard Numerical Summary Measures 99

                          mean of a theoretical distribution. But it is worth saying now that the symbol µ will
                          be used in that context as well as in the context of equation (3.4).

                               As another example of the usage suggested by Definition 10, consider the vari-
                          ance and standard deviation. Definition 9 refers specifically to the sample variance
                          and standard deviation. If a data set represents an entire population, then it is com-
                          mon to use the lowercase Greek sigma squared ( 2) to stand for the population
                          variance and to define

Population                1         N
   variance
                          2 =             (xi - µ)2  (3.5)
                                 N
                                    i =1

                          The nonnegative square root of  2 is then called the population standard devia-
                          tion,  . (The division in equation (3.5) is by N , and not the N - 1 that might be
                          expected on the basis of Definition 9. There are reasons for this change, but they are
                          not accessible at this point.) Chapter 5 defines a variance and standard deviation for
                          theoretical distributions, and the symbols  2 and  will be used there as well as in
                          the context of equation (3.5).

                               On one point, this text will deviate from the Roman/Greek symbolism conven-
                          tion laid out in Definition 10: the notation for quantiles. Q( p) will stand for the pth
                          quantile of a distribution, whether it is from a sample, a population, or a theoretical
                          model.

       3.3.4              Plots of Summary Statistics

Plots against             Plotting numerical summary measures in various ways is often helpful in the early
           time           analysis of engineering data. For example, plots of summary statistics against time
                          are frequently revealing.

              Example 8   Monitoring a Critical Dimension of Machined Parts
(Example 8, Chapter 1,
                          Cowan, Renk, Vander Leest, and Yakes worked with a company that makes
       revisited--p. 18)  precision metal parts. A critical dimension of one such part was monitored by
                          occasionally selecting and measuring five consecutive pieces and then plotting the
                          sample mean and range. Table 3.14 gives the x¯ and R values for 25 consecutive
                          samples of five parts. The values reported are in .0001 in.

                               Figure 3.21 is a plot of both the means and ranges against order of observation.
                          Looking first at the plot of ranges, no strong trends are obvious, which suggests
                          that the basic short-term variation measured in this critical dimension is stable.
                          The combination of process and measurement precision is neither improving nor
                          degrading with time. The plot of means, however, suggests some kind of physical
                          change. The average dimensions from the second shift on October 27 (samples 9
                          through 15) are noticeably smaller than the rest of the means. As discussed in
                          Example 8, Chapter 1, it turned out to be the case that the parts produced on that
100 Chapter 3 Elementary Descriptive Statistics

Table 3.14
Means and Ranges for a Critical Dimension on Samples of n = 5 Parts

Sample Date       Time   x¯        R Sample Date                     Time   x¯  R

1 10/27 7:30 AM 3509.4 5 14                                          10:15  3504.4 4

2                 8:30   3509.2 2 15                                 11:15  3504.6 3

3                 9:30   3512.6 3 16 10/28 7:30 AM 3513.0 2

4                 10:30  3511.6 4 17                                 8:30   3512.4 1

5                 11:30  3512.0 4 18                                 9:30   3510.8 5

6                 12:30 PM 3513.6 6              19                  10:30  3511.8 4

7                 1:30   3511.8 3 20                                 6:15 PM 3512.4 3

8                 2:30   3512.2 2 21                                 7:15   3511.0 4

9                 4:15   3500.0 3 22                                 8:45   3510.6 1

10                5:45   3502.0 2 23                                 9:45   3510.2 5

11                6:45   3501.4 2 24                                 10:45  3510.4 2

12                8:15   3504.0 2 25                                 11:45  3510.8 3

13                9:15   3503.6 3

    Example 8
    (continued )

                                     x
                             3515

                             3510

                             3505

                             3500                    10 15           20 25
                                              5
                                                     Sample number
                                     R
                                 5

                             0                   5   10 15           20 25

                                                     Sample number

                                   Figure 3.21 Plots of x¯ and R over time
                                                              3.3 Standard Numerical Summary Measures 101

                           shift were not really systematically any different from the others. Instead, the
                           person making the measurements for samples 9 through 15 used the gauge in a
                           fundamentally different way than other employees. The pattern in the x¯ values
                           was caused by this change in measurement technique.

  Terminology and               Patterns revealed in the plotting of sample statistics against time ought to alert
causes for patterns        an engineer to look for a physical cause and (typically) a cure. Systematic vari-
                           ations or cycles in a plot of means can often be related to process variables that
    on plots against       come and go on a more or less regular basis. Examples include seasonal or daily
                   Time    variables like ambient temperature or those caused by rotation of gauges or fixtures.
                           Instability or variation in excess of that related to basic equipment precision can
        Plots against      sometimes be traced to mixed lots of raw material or overadjustment of equipment
   process variables       by operators. Changes in level of a process mean can originate in the introduction
                           of new machinery, raw materials, or employee training and (for example) tool wear.
                           Mixtures of several patterns of variation on a single plot of some summary statistic
                           against time can sometimes (as in Example 8) be traced to changes in measurement
                           calibration. They are also sometimes produced by consistent differences in machines
                           or streams of raw material.

                                Plots of summary statistics against time are not the only useful ones. Plots
                           against process variables can also be quite informative.

              Example 9    Plotting the Mean Shear Strength of Wood Joints
(Example 6, Chapter 1,
                           In their study of glued wood joint strength, Dimond and Dix obtained the values
       revisited--p. 15 )  given in Table 3.15 as mean strengths (over three shear tests) for each combination
                           of three woods and three glues. Figure 3.22 gives a revealing plot of these
                           3 × 3 = 9 different x¯ 's.

                           Table 3.15
                           Mean Joint Strengths for Nine Wood/Glue Combinations

                           Wood Glue                          x¯
                                             Mean Joint Shear Strength (lb)

                           pine white        131.7
                                             192.7
                           pine carpenter's  201.3
                                              92.0
                           pine cascamite    146.3
                                             156.7
                           fir  white        257.7
                                             234.3
                           fir  carpenter's  177.7

                           fir  cascamite

                           oak white

                           oak carpenter's

                           oak cascamite
102 Chapter 3 Elementary Descriptive Statistics                             Strength (lb)         Oak
                  Example 9                                                                Pine
                 (continued )
                                                                         250                    Fir

                                                                         200

                                                                         150

                                                                         100

                                             White Carpenter's Cascamite

                                Figure 3.22 Plot of mean joint strength vs.
                                glue type for three woods

            From the plot, it is obvious that the gluing properties of pine and fir are
       quite similar, with pine joints averaging around 40-45 lb stronger. For these
       two soft woods, cascamite appears slightly better than carpenter's glue, both of
       which make much better joints than white glue. The gluing properties of oak
       (a hardwood) are quite different from those of pine and fir. In fact, the glues
       perform in exactly the opposite ordering for the strength of oak joints. All of this
       is displayed quite clearly by the simple plot in Figure 3.22.

            The two previous examples have illustrated the usefulness of plotting sample
       statistics against time and against levels of an experimental variable. Other possi-
       bilities in specific engineering situations can potentially help the working engineer
       understand and manipulate the systems on which he or she works.

3.3.5  Summary Statistics and Personal Computer Software

       The numerical data summaries introduced in this chapter are relatively simple. For
       small data sets they can be computed quite easily using only a pocket calculator.
       However, for large data sets and in cases where subsequent additional calculations
       or plotting may occur, statistical or spreadsheet software can be convenient.

            Printout 1 illustrates the use of the MINITAB statistical package to produce
       summary statistics for the percent waste data sets in Table 3.13. (The appropriate
       MINITAB routine is found under the "Stat/Basic Statistics/Display Descriptive
       Statistics" menu.) The mean, median, and standard deviation values on the printout
       agree with those produced in Example 7. However, the first and third quartile
                           3.3 Standard Numerical Summary Measures 103

WWW  Printout 1 Descriptive Statistics for the Percent Waste Data of Table

     Descriptive Statistics

     Variable           N      Mean  Median    TrMean    StDev  SE Mean
     Supply 1           6     1.495   0.785     1.495    1.394     0.569
     Supply 2           8     2.228   1.525     2.228    1.839     0.650

     Variable  Minimum     Maximum         Q1        Q3
     Supply 1     0.370       3.620   0.483     3.073
     Supply 2     0.890       6.540   1.105     2.540

     figures on the printout do not match exactly those found earlier. MINITAB simply
     uses slightly different conventions for those quantities than the ones introduced in
     Section 3.2.

          High-quality statistical packages like MINITAB (and JMP, SAS, SPSS, SYS-
     TAT, SPLUS, etc.) are widely available. One of them should be on the electronic
     desktop of every working engineer. Unfortunately, this is not always the case, and
     engineers often assume that standard spreadsheet software (perhaps augmented with
     third party plug-ins) provides a workable substitute. Often this is true, but sometimes
     it is not.

          The primary potential problem with using a spreadsheet as a substitute for sta-
     tistical software concerns numerical accuracy. Spreadsheets can and do on occasion
     return catastrophically wrong values for even simple statistics. Established vendors
     of statistical software have many years of experience dealing with subtle numerical
     issues that arise in the computation of even simple summaries of even small data
     sets. Most vendors of spreadsheet software seem unaware of or indifferent to these
     matters. For example, consider the very small data set

                                                   0, 1, 2

     The sample variance of these data is easily seen to be 1.0, and essentially any
     statistical package or spreadsheet will reliably return this value. However, suppose
     100,000,000 is added to each of these n = 3 values, producing the data set

                                 100000000, 100000001, 100000002

     The actual sample variance is unchanged, and high-quality statistical software will
     reliably return the value 1.0. However, as of late 1999, the current version of the
     leading spreadsheet program returned the value 0 for this second sample variance.
     This is a badly wrong answer to an apparently very simple problem.

          So at least until vendors of spreadsheet software choose to integrate an es-
     tablished statistical package into their products, we advise extreme caution in the
     use of spreadsheets to do statistical computations. A good source of up-to-date
     information on this issue is the AP Statistics electronic bulletin board found at
     http://forum.swarthmore.edu/epigone/apstat-l.
104 Chapter 3 Elementary Descriptive Statistics

Section 3 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Calculate and compare the means, medians, ranges,          Exercise 1 of Section 3.2 and thereby check your
   interquartile ranges, and standard deviations of the       answers to Exercise 1 here.
   two data sets introduced in Exercise 1 of Section
   3.2. Discuss the interpretation of these values in the  4. Add 1.3 to each of the lengthwise cut impact
   context of comparing the two cutting methods.              strengths referred to in Exercise 1 and then re-
                                                              compute the values of the mean, median, range,
2. Are the numerical values you produced in Exercise          interquartile range, and standard deviation. How
   1 above most naturally thought of as statistics or as      do these compare with the values obtained earlier?
   parameters? Explain.                                       Repeat this exercise after multiplying each length-
                                                              wise cut impact strength by 2 (instead of adding
3. Use a statistical package to compute basic sum-            1.3).
   mary statistics for the two data sets introduced in

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

3.4 Descriptive Statistics for Qualitative
       and Count Data (Optional )

                    The techniques presented thus far in this chapter are primarily relevant to the analysis
                    of measurement data. As noted in Section 1.2, conventional wisdom is that where
                    they can be obtained, measurement data (or variables data) are generally preferable
                    to count and qualitative data (or attributes data). Nevertheless, qualitative or count
                    data will sometimes be the primary information available. It is therefore worthwhile
                    to consider their summarization.

                         This section will cover the reduction of qualitative and count data to per-item or
                    per-inspection-unit figures and the display of those ratios in simple bar charts and
                    plots.

3.4.1              Numerical Summarization of Qualitative and Count Data

                   Recall from Definitions 8 and 9 in Chapter 1 that aggregation and counting are
                   typically used to produce numerical values from qualitative data. Then, beginning
                   with counts, it is often helpful to calculate rates on a per-item or per-inspection-unit
                   basis.

                        When each item in a sample of n either does or does not have a characteristic
                   of interest, the notation

Sample fraction    p^ = The number of items in the sample with the characteristic (3.6)
 of items with a                                           n

   characteristic

                   will be used. A given sample can produce many such values of " p hat" if either a
                   single characteristic has many possible categories or many different characteristics
                   are being monitored simultaneously.
            3.4 Descriptive Statistics for Qualitative and Count Data 105

Example 10  Defect Classifications of Cable Connectors

            Delva, Lynch, and Stephany worked with a manufacturer of cable connectors.
            Daily samples of 100 connectors of a certain design were taken over 30 produc-
            tion days, and each sampled connector was inspected according to a well-defined
            (operational) set of rules. Using the information from the inspections, each in-
            spected connector could be classified as belonging to one of the following five
            mutually exclusive categories:

                 Category A: having "very serious" defects

                 Category B: having "serious" defects but no "very serious" defects

                 Category C: having "moderately serious" defects but no "serious" or "very
                                  serious" defects

                 Category D: having only "minor" defects

                 Category E: having no defects

                 Table 3.16 gives counts of sampled connectors falling into the first four
            categories (the four defect categories) over the 30-day period. Then, using the
            fact that 30 × 100 = 3,000 connectors were inspected over this period,

                                              p^ A = 3/3000 = .0010
                                              p^ B = 0/3000 = .0000
                                              p^ C = 11/3000 = .0037
                                              p^ D = 1/3000 = .0003

            Notice that here p^ E = 1 - ( p^ A + p^ B + p^ C + p^ D), because categories A through
            E represent a set of nonoverlapping and exhaustive classifications into which an
            individual connector must fall, so that the p^ 's must total to 1.

            Table 3.16
            Counts of Connectors Classified into Four Defect
            Categories

            Category Number of Sampled Connectors

            A  3

            B  0

            C  11

            D  1
106 Chapter 3 Elementary Descriptive Statistics

Example 11        Pneumatic Tool Manufacture

                  Kraber, Rucker, and Williams worked with a manufacturer of pneumatic tools.
                  Each tool produced is thoroughly inspected before shipping. The students col-
                  lected some data on several kinds of problems uncovered at final inspection.
                  Table 3.17 gives counts of tools having these problems in a particular production
                  run of 100 tools.

                  Table 3.17
                  Counts and Fractions of Tools with Various
                  Problems

                  Problem                        Number of Tools p^

                  Type 1 leak                    8  .08

                  Type 2 leak                    4  .04

                  Type 3 leak                    3  .03

                  Missing part 1                 2  .02

                  Missing part 2                 1  .01

                  Missing part 3                 2  .02

                  Bad part 4                     1  .01

                  Bad part 5                     2  .02

                  Bad part 6                     1  .01

                  Wrong part 7                   2  .02

                  Wrong part 8                   2  .02

                       Table 3.17 is a summarization of highly multivariate qualitative data. The

                  categories listed in Table 3.17 are not mutually exclusive; a given tool can fall

                  into more than one of them. Instead of representing different possible values of

                  a single categorical variable (as was the case with the connector categories in

                  Example 10), the categories listed above each amount to 1 (present) of 2 (present

                  and not present) possible values for a different categorical variable. For example,
                  for type 1 leaks, p^ = .08, so 1 - p^ = .92 for the fraction of tools without type 1
                  leaks. The p^ values do not necessarily total to the fraction of tools requiring rework
                  at final inspection. A given faulty tool could be counted in several p^ values.

                       Another kind of per-item ratio, also based on counts, is sometimes confused
                  with p^ . Such a ratio arises when every item in a sample provides an opportunity for
                  a phenomenon of interest to occur, but multiple occurrences are possible and counts

                  are kept of the total number of occurrences. In such cases, the notation

 Sample mean      u^ = The total number of occurrences (3.7)
occurences per          The total number of inspection units or sampled items

    unit or item
                                     3.4 Descriptive Statistics for Qualitative and Count Data 107

               is used. u^ is really closer in meaning to x¯ than to p^ , even though it can turn out to be
               a number between 0 and 1 and is sometimes expressed as a percentage and called a
               rate.

                    Although the counts totaled in the numerator of expression (3.7) must all be
               integers, the values totaled to create the denominator need not be. For instance,
               suppose vinyl floor tiles are being inspected for serious blemishes. If on one occasion
               inspection of 1 box yields a total of 2 blemishes, on another occasion .5 box yields
               0 blemishes, and on still another occasion 2.5 boxes yield a total of 1 blemish, then

                                         u^ = 2 + 0 + 1 = .75 blemishes/box
                                               1 + .5 + 2.5

               Depending on exactly how terms are defined, it may be appropriate to calculate
               either p^ values or u^ values or both in a single situation.

Example 10     It was possible for a single cable connector to have more than one defect of a
 (continued )  given severity and, in fact, defects of different severities. For example, Delva,
               Lynch, and Stephany's records indicate that in the 3,000 connectors inspected,
               1 connector had exactly 2 moderately serious defects (along with a single very
               serious defect), 11 connectors had exactly 1 moderately serious defect (and no
               others), and 2,988 had no moderately serious defects. So the observed rate of
               moderately serious defects could be reported as

                      u^ = 2 + 11 = .0043 moderately serious defects/connector
                            1 + 11 + 2988

               This is an occurrence rate for moderately serious defects( u^ ), but not a fraction
               of connectors having moderately serious defects ( p^ ).

                    The difference between the statistics p^ and u^ may seem trivial. But it is a point

               that constantly causes students confusion. Methods of formal statistical inference
               based on p^ are not the same as those based on u^ . The distinction between the two

               kinds of rates must be kept in mind if those methods are to be applied appropriately.

                    To carry this warning a step further, note that not every quantity called a
               percentage is even of the form p^ or u^ . In a laboratory analysis, a specimen may be
               declared to be "30% carbon." The 30% cannot be thought of as having the form of p^
               in equation (3.6) or u^ in equation (3.7). It is really a single continuous measurement,
               not a summary statistic. Statistical methods for p^ or u^ have nothing to say about

               such rates.

3.4.2          Bar Charts and Plots for Qualitative and Count Data

               Often, a study will produce several values of p^ or u^ that need to be compared. Bar
               charts and simple bivariate plots can be a great aid in summarizing these results.
108 Chapter 3 Elementary Descriptive Statistics

Example 10     Figure 3.23 is a bar chart of the fractions of connectors in the categories A through
 (continued )  D. It shows clearly that most connectors with defects fall into category C, having
               moderately serious defects but no serious or very serious defects. This bar chart
               is a presentation of the behavior of a single categorical variable.

                                  Fraction of connectors  .004

                                                          .003

                                                          .002

                                                          .001

                                                          0     A  B  C                D

                                                                   Connector category

                                                          Figure 3.23 Bar chart of connector defects

Example 11     Figure 3.24 is a bar chart of the information on tool problems in Table 3.17. It
 (continued )  shows leaks to be the most frequently occurring problems on this production run.

               Problem rate  .08
                             .07
                             .06                          Type 1 leak
                             .05                                 Type 2 leak
                             .04                                        Type 3 leak
                             .03                                               Missing part 1
                             .02                                                      Missing part 2
                             .01                                                              Missing part 3

                               0                                                                     Bad part 4
                                                                                                            Bad part 5
                                                                                                                   Bad part 6
                                                                                                                           Wrong part 7
                                                                                                                                  Wrong part 8

                                                          Figure 3.24 Bar chart for assembly problems
                                        3.4 Descriptive Statistics for Qualitative and Count Data 109

                       Figures 3.23 and 3.24 are both bar charts, but they differ considerably. The
                  first concerns the behavior of a single (ordered) categorical variable--namely, Con-
                  nector Class. The second concerns the behavior of 11 different present-not present
                  categorical variables, like Type 1 Leak, Missing Part 3, etc. There may be some
                  significance to the shape of Figure 3.23, since categories A through D are arranged
                  in decreasing order of defect severity, and this order was used in the making of
                  the figure. But the shape of Figure 3.24 is essentially arbitrary, since the particular
                  ordering of the tool problem categories used to make the figure is arbitrary. Other
                  equally sensible orderings would give quite different shapes.

                       The device of segmenting bars on a bar chart and letting the segments stand
                  for different categories of a single qualitative variable can be helpful, particularly
                  where several different samples are to be compared.

   Example 12     Scrap and Rework in a Turning Operation

                  The article "Statistical Analysis: Roadmap for Management Action" by H.
                  Rowe (Quality Progress, February 1985) describes a statistically based quality-
                  improvement project in the turning of steel shafts. Table 3.18 gives the percentages
                  of reworkable and scrap shafts produced in 18 production runs made during the
                  study.

                       Figure 3.25 is a corresponding segmented bar graph, with the jobs ordered
                  in time, showing the behavior of both the scrap and rework rates over time. (The
                  total height of any bar represents the sum of the two rates.) The sharp reduction in
                  both scrap and rework between jobs 10 and 11 was produced by overhauling one
                  of the company's lathes. That lathe was identified as needing attention through
                  engineering data analysis early in the plant project.

Table 3.18
Percents Scrap and Rework in a Turning Operation

Job Number Percent Scrap Percent Rework           Job Number  Percent Scrap  Percent Rework

1              2  25                                   10            3               18
                                                       11            0                3
2              3  11                                   12            1                5
                                                       13            0                0
3              0  5                                    14            0                0
                                                       15            0                3
4              0  0                                    16            0                2
                                                       17            0                2
5              0  20                                   18            1                5

6              2  23

7              0  6

8              0  5

9              2  8
110 Chapter 3 Elementary Descriptive Statistics                                                                       Scrap
                                                                                                                      Rework
                Example 12
                 (continued )                                Percent of production

                                                          25

                                                          20

                                                          15

                                                          10

                                                            5

                                                                                          1  5      10          15

                                                                                                    Job number

                                                                                    Figure 3.25 Segmented bar chart of scrap and rework rates

                        In many cases, the simple plotting of p^ or u^ values against time or process
                   variables can make clear the essential message in a set of qualitative or count data.

       Example 13  Defects per Truck Found at Final Inspection

                   In his text Engineering Statistics and Quality Control, I. W. Burr illustrates
                   the usefulness of plotting u^ versus time with a set of data on defects found at
                   final inspection at a truck assembly plant. From 95 to 130 trucks were produced
                   daily at the plant; Table 3.19 gives part of Burr's daily defects/truck values. These
                   statistics are plotted in Figure 3.26. The graph shows a marked decrease in quality
                   (increase in u^ ) over the third and fourth weeks of December, ending with a rate

Table 3.19
Defects Per Truck on 26 Production Days

Date u^ = Defects/Truck Date u^ = Defects/Truck Date u^ = Defects/Truck Date u^ = Defects/Truck

12/2   1.54        12/11                                                            1.18     12/20  2.32        1/3                            1.15

12/3   1.42        12/12                                                            1.39     12/23  1.23        1/6                            1.37

12/4   1.57        12/13                                                            1.42     12/24  2.91        1/7                            1.79

12/5   1.40        12/16                                                            2.08     12/26  1.77        1/8                            1.68

12/6   1.51        12/17                                                            1.85     12/27  1.61        1/9                            1.78

12/9   1.08        12/18                                                            1.82     12/30  1.25        1/10                           1.84

12/10  1.27        12/19                                                            2.07
                            3.4 Descriptive Statistics for Qualitative and Count Data 111

            Defects /Truck2.5
                          2.0
                          1.5
                          1.0
                           .5

                                                                    Date
                                      Figure 3.26 Plot of daily defects per truck

            of 2.91 defects/truck on Christmas Eve. Apparently, this situation was largely
            corrected with the passing of the holiday season.
                            12/2
                                       12/6
                                          12/9
                                                      12/13
                                                         12/16
                                                                    12/20
                                                                       12/23
                                                                          12/24
                                                                             12/26
                                                                                12/27
                                                                                   12/30
                                                                                     1/3
                                                                                        1/6
                                                                                                    1/10

                 Plots of p^ or u^ against levels of manipulated variables from an experiment are
            often helpful in understanding the results of that experiment.

Example 14  Plotting Fractions of Conforming Pellets

            Greiner, Grim, Larson, and Lukomski experimented with the same pelletizing
            machine studied by Cyr, Ellson, and Rickard (see Example 2 in Chapter 1). In
            one part of their study, they ran the machine at an elevated speed and varied the
            shot size (amount of powder injected into the dies) and the composition of that
            powder (in terms of the relative amounts of new and reground material). Table
            3.20 lists the numbers of conforming pellets produced in a sample of 100 at each
            of 2 × 2 = 4 sets of process conditions. A simple plot of p^ values versus shot
            size is given in Figure 3.27.

                 The figure indicates that increasing the shot size is somewhat harmful, but
            that a substantial improvement in process performance happens when the amount
            of reground material used in the pellet-making mixture is increased. This makes
            sense. Reground material had been previously compressed into (nonconforming)
            pellets. In the process, it had been allowed to absorb some ambient humidity.
            Both the prior compression and the increased moisture content were potential
            reasons why this material improved the ability of the process to produce solid,
            properly shaped pellets.
112 Chapter 3 Elementary Descriptive Statistics

Example 14     Table 3.20
 (continued )  Numbers of Conforming Pellets for Four Shot Size/Mixture
               Combinations

               Sample Shot Size Mixture                                       Number Conforming

               1  small                                     20% reground             38

               2  small                                     50% reground             66

               3  large                                     20% reground             29

               4  large                                     50% reground             53

                  Fraction of pellets conforming  .6               50% Reground powder

                                                  .4 20% Reground powder
                                                  .2

                                                            Small             Large

                                                                   Shot size

                  Figure 3.27 Plot of fraction conforming vs.
                  shot size

Section 4 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. From your field, give an example of a variable that         represented in Table 1.1, what would have been the
   is a rate (a) of the form p^ , (b) of the form u^ , and     sample fractions nonconforming p^ ? Give a practi-
   (c) of neither form.                                        cal reason why having the values in Table 1.1 might
                                                               be preferable to knowing only the corresponding p^
2. Because gauging is easier, it is sometimes tempting         values.
   to collect qualitative data related to measurements
   rather than the measurements themselves. For ex-         3. Consider the measurement of the percentage cop-
   ample, in the context of Example 1 in Chapter 1, if         per in brass specimens. The resulting data will be a
   gears with runouts exceeding 15 were considered             kind of rate data. Are the rates that will be obtained
   to be nonconforming, it would be possible to derive         of the type p^ , of the type u^ , or of neither type?
   fractions nonconforming, p^ , from simple "go-no            Explain.
   go" checking of gears. For the two sets of gears
                                                              Chapter 3 Exercises 113

Chapter 3 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. The accompanying values are gains measured on              which later can clog the filters in extruders when
   120 amplifiers designed to produce a 10 dB gain.           the recycled material is used. The following are the
   These data were originally from the Quality Im-            amounts (in ppm by weight of aluminum) found
   provement Tools workbook set (published by the             in bihourly samples of PET recovered at the plant
   Juran Institute). They were then used as an exam-          over roughly a two-day period.
   ple in the article "The Tools of Quality" (Quality
   Progress, September 1990).                                 291, 222, 125, 79, 145, 119, 244, 118, 182, 63,
                                                              30, 140, 101, 102, 87, 183, 60, 191, 119, 511,
   8.1, 10.4, 8.8, 9.7, 7.8, 9.9, 11.7, 8.0, 9.3, 9.0, 8.2,   120, 172, 70, 30, 90, 115
   8.9, 10.1, 9.4, 9.2, 7.9, 9.5, 10.9, 7.8, 8.3, 9.1, 8.4,
   9.6, 11.1, 7.9, 8.5, 8.7, 7.8, 10.5, 8.5, 11.5, 8.0, 7.9,  (Apparently, the data are recorded in the order in
   8.3, 8.7, 10.0, 9.4, 9.0, 9.2, 10.7, 9.3, 9.7, 8.7, 8.2,   which they were collected, reading left to right, top
   8.9, 8.6, 9.5, 9.4, 8.8, 8.3, 8.4, 9.1, 10.1, 7.8, 8.1,    to bottom.)
   8.8, 8.0, 9.2, 8.4, 7.8, 7.9, 8.5, 9.2, 8.7, 10.2, 7.9,    (a) Make a run chart for these data. Are there any
   9.8, 8.3, 9.0, 9.6, 9.9, 10.6, 8.6, 9.4, 8.8, 8.2, 10.5,
   9.7, 9.1, 8.0, 8.7, 9.8, 8.5, 8.9, 9.1, 8.4, 8.1, 9.5,          obvious time trends? What practical engineer-
   8.7, 9.3, 8.1, 10.1, 9.6, 8.3, 8.0, 9.8, 9.0, 8.9, 8.1,         ing reason is there for looking for such trends?
   9.7, 8.5, 8.2, 9.0, 10.2, 9.5, 8.3, 8.9, 9.1, 10.3, 8.4,   (b) Ignoring the time order information, make a
   8.6, 9.2, 8.5, 9.6, 9.0, 10.7, 8.6, 10.0, 8.8, 8.6              stem-and-leaf diagram. Use the hundreds digit
                                                                   to make the stem and the other two digits (sep-
   (a) Make a stem-and-leaf plot and a boxplot for                 arated by commas to indicate the different data
        these data. How would you describe the shape               points) to make the leaves. After making an
        of this data set? Does the shape of your stem-             initial stem-and-leaf diagram by recording the
        and-leaf plot (or a corresponding histogram)               data in the (time) order given above, make a
        give you any clue how a high fraction within               second one in which the values have been or-
        specifications was achieved?                               dered.
                                                              (c) How would you describe the shape of the stem-
   (b) Make a normal plot for these data and interpret             and-leaf diagram? Is the data set bell-shaped?
        its shape. (Standard normal quantiles for p =         (d) Find the median and the first and third quartiles
        .0042 and p = .9958 are approximately -2.64                for the aluminum contents and then find the .58
        and 2.64, respectively.)                                   quantile of the data set.
                                                              (e) Make a boxplot.
   (c) Although the nominal gain for these amplifiers         (f) Make a normal plot, using regular graph paper.
        was to be 10 dB, the design allowed gains from             List the coordinates of the 26 plotted points.
        7.75 dB to 12.2 dB to be considered acceptable.            Interpret the shape of the plot.
        About what fraction, p, of such amplifiers do         (g) Try transforming the data by taking natural log-
        you expect to meet these engineering specifi-              arithms and again assess the shape. Is the trans-
        cations?                                                   formed data set more bell-shaped than the raw
                                                                   data set?
2. The article "The Lognormal Distribution for Mod-           (h) Find the sample mean, the sample range, and
   eling Quality Data When the Mean is Near Zero"                  the sample standard deviation for both the
   by S. Albin (Journal of Quality Technology, April               original data and the log-transformed values
   1990) described the operation of a Rutgers Uni-                 from (g). Is the mean of the transformed val-
   versity plastics recycling pilot plant. The most im-            ues equal to the natural logarithm of the mean
   portant material reclaimed from beverage bottles                of the original data?
   is PET plastic. A serious impurity is aluminum,
114 Chapter 3 Elementary Descriptive Statistics

3. The accompanying data are three hypothetical sam-     4. Gaul, Phan, and Shimonek measured the resis-
   ples of size 10 that are supposed to represent mea-
   sured manganese contents in specimens of 1045         tances of 15 resistors of 2 × 5 = 10 different types.
   steel (the units are points, or .01%). Suppose that
   these measurements were made on standard speci-       Two different wattage ratings were involved, and
   mens having "true" manganese contents of 80, us-
   ing three different analytical methods. (Thirty dif-  five different nominal resistances were used. All
   ferent specimens were involved.)
                                                         measurements were reported to three significant
                         Method 1
                                                         digits. Their data follow.
                         87, 74, 78, 81, 78,
                         77, 84, 80, 85, 78              (a) Make back-to-back stem-and-leaf plots for

                         Method 2                             comparing     the   1  watt  and     1  watt  resistance
                                                                                  4                2
                         86, 85, 82, 87, 85,                  distributions for each nominal resistance. In a
                         84, 84, 82, 82, 85
                                                              few sentences, summarize what these show.
                         Method 3
                                                         (b)  Make  pairs   of    boxplots    for  comparing  the    1
                         84, 83, 78, 79, 85,                                                                         4
                         82, 82, 81, 82, 79                              1
                                                              watt  and  2  watt  resistance  distributions  for  each
    (a) Make (on the same coordinate system) side-
        by-side boxplots that you can use to compare          nominal resistance.
        the three analytical methods.
                                                         (c)  Make  normal     plots  for  the  1     watt  nominal  20
   (b) Discuss the apparent effectiveness of the three                                          2
        methods in terms of the appearance of your di-        ohm and nominal 200 ohm resistors. Interpret
        agram from (a) and in terms of the concepts
        of accuracy and precision discussed in Sec-           these in a sentence or two. From the appear-
        tion 1.3.
                                                              ance of the second plot, does it seem that if
    (c) An alternative method of comparing two such
        analytical methods is to use both methods of          the nominal 200 ohm resistances were treated
        analysis once on each of (say) 10 different
        specimens (10 specimens and 20 measure-               as if they had a bell-shaped distribution, the
        ments). In the terminology of Section 1.2, what
        kind of data would be generated by such a             tendency would be to overestimate or to un-
        plan? If one simply wishes to compare the
        average measurements produced by two ana-             derestimate the fraction of resistances near the
        lytical methods, which data collection plan (20
        specimens and 20 measurements, or 10 spec-            nominal value?
        imens and 20 measurements) seems to you
        most likely to provide the better comparison?                       1  Watt  Resistors
        Explain.                                                            4

                                                         20 ohm     75 ohm 100 ohm 150 ohm                  200 ohm

                                                          19.2      72.9          97.4          148           198
                                                          19.2                                                196
                                                          19.3      72.4          95.8          148           199
                                                          19.3                                                196
                                                          19.1      72.0          97.7          148           196
                                                          19.0                                                195
                                                          19.6      72.5          94.1          148           193
                                                          19.2                                                196
                                                          19.3      72.7          95.1          148           196
                                                          19.4                                                199
                                                          19.4      72.3          95.4          147           194
                                                          19.3                                                195
                                                          19.5      72.9          94.9          148           196
                                                          19.2                                                195
                                                          19.1      73.2          98.5          148           199

                                                                    71.8          94.8          148

                                                                    73.4          94.6          147

                                                                    70.9          98.3          147

                                                                    72.3          96.0          149

                                                                    72.5          97.3          148

                                                                    72.1          96.0          148

                                                                    72.6          94.8          148
                                                                                    Chapter 3 Exercises 115

              1  Watt  Resistors                                                   5-Gram Weighings
              2

20 ohm  75 ohm 100 ohm 150 ohm         200 ohm                       Scale 1        Scale 2          Scale 3

 20.1   73.9     97.2             152    207              Student 1  5.03, 5.02     5.07, 5.09       4.98, 4.98
 19.7                                    205              Student 2  5.03, 5.01     5.02, 5.07       4.99, 4.98
 20.2   74.2     97.9             151    214              Student 3  5.06, 5.00     5.10, 5.08       4.98, 4.98
 24.4                                    195
 20.2   74.6     96.8             155    202
 20.1                                    211
 20.0   72.1     99.2             146    197
 20.4                                    197
 20.3   73.8     98.5             148    199                                     20-Gram Weighings
 20.6                                    196
 19.9   74.8     95.5             154    207                         Scale 1        Scale 2          Scale 3
 19.7                                    210
 20.8   75.0     97.2             149    192
 20.4                                    201
 20.5   68.6     98.7             150    257              Student 1  20.04, 20.06   20.04, 20.04     19.94, 19.93
                                                          Student 2  20.02, 19.99   20.03, 19.93     19.95, 19.95
        74.0     96.6             153                     Student 3  20.03, 20.02   20.06, 20.03     19.91, 19.96

        71.7     102              149

        76.5     103              150                                            100-Gram Weighings

        76.2     102              149

        72.8     102              145                                Scale 1        Scale 2          Scale 3

        73.2     100              147

        76.7     100              149                     Student 1 100.06, 100.35  100.25, 100.08   99.87, 99.88
                                                          Student 2 100.05, 100.01  100.10, 100.02   99.87, 99.88
   (d) Compute the sample means and sample stan-          Student 3 100.00, 100.00  100.01, 100.02   99.88, 99.88
        dard deviations for all 10 samples. Do these
        values agree with your qualitative statements     6. The accompanying values are the lifetimes (in num-
        made in answer to part (a)?                          bers of 24 mm deep holes drilled in 1045 steel
                                                             before tool failure) for n = 12 D952-II (8 mm)
    (e) Make a plot of the 10 sample means computed          drills. These were read from a graph in "Computer-
        in part (d), similar to the plot in Figure 3.22.     assisted Prediction of Drill-failure Using In-process
        Comment on the appearance of this plot.              Measurements of Thrust Force" by A. Thangaraj
                                                             and P. K. Wright (Journal of Engineering for In-
5. Blomquist, Kennedy, and Reiter studied the prop-          dustry, May 1988).
   erties of three scales by each weighing a standard
   5 g weight, 20 g weight, and 100 g weight twice              47, 145, 172, 86, 122, 110, 172, 52, 194, 116,
   on each scale. Their data are presented in the ac-           149, 48
   companying table. Using whatever graphical and
   numerical data summary methods you find helpful,          Write a short report to your engineering manager
   make sense of these data. Write a several-page dis-       summarizing what these data indicate about the
   cussion of your findings. You will probably want          lifetimes of drills of this type in this kind of appli-
   to consider both accuracy and precision and (to the       cation. Use whatever graphical and numerical data
   extent possible) make comparisons between scales          summary tools make clear the main features of the
   and between students. Part of your discussion might       data set.
   deal with the concepts of repeatability and repro-
   ducibility introduced in Section 2.1. Are the pic-     7. Losen, Cahoy, and Lewis purchased eight spanner
   tures you get of the scale and student performances       bushings of a particular type from a local machine
   consistent across the different weights?                  shop and measured a number of characteristics of
                                                             these bushings, including their outside diameters.
                                                             Each of the eight outside diameters was measured
116 Chapter 3 Elementary Descriptive Statistics

once by each of two student technicians, with the              (a) Find the .84 quantile of the Compound 1 failure
following results (the units are inches):                          times.

Bushing      1      2      3         4                        (b) Give the coordinates of the two lower-left
Student A  .3690  .3690  .3690     .3700                           points that would appear on a normal plot of
Student B  .3690  .3695  .3695     .3695                           the Compound 1 data.

Bushing      5      6      7         8                         (c) Make back-to-back stem-and-leaf plots for
Student A  .3695  .3700  .3695     .3690                           comparing the life length properties of bear-
Student B  .3695  .3700  .3700     .3690                           ings made from Compounds 1 and 2.

   A common device when dealing with paired data              (d) Make (to scale) side-by-side boxplots for com-
   like these is to analyze the differences. Subtracting           paring the life lengths for the two compounds.
   B measurements from A measurements gives the                    Mark numbers on the plots indicating the loca-
   following eight values:                                         tions of their main features.

       .0000, -.0005, -.0005, .0005, .0000, .0000,             (e) Compute the sample means and standard devi-
       -.0005, .0000                                               ations of the two sets of lifetimes.

    (a) Find the first and third quartiles for these dif-      (f) Describe what your answers to parts (c), (d),
        ferences, and their median.                                and (e) above indicate about the life lengths of
                                                                   these turbine bearings.
   (b) Find the sample mean and standard deviation
        for the differences.                               9. Heyde, Kuebrick, and Swanson measured the
                                                              heights of 405 steel punches purchased by a com-
    (c) Your mean in part (b) should be negative. Inter-      pany from a single supplier. The stamping machine
        pret this in terms of the original measurement        in which these are used is designed to use .500 in.
        problem.                                              punches. Frequencies of the measurements they
                                                              obtained are shown in the accompanying table.
   (d) Suppose you want to make a normal plot of the
        differences on regular graph paper. Give the co-   Punch Height                  Punch Height  Frequency
        ordinates of the lower-left point on such a plot.    (.001 in.)  Frequency (.001 in.)

8. The accompanying data are the times to failure (in      482           1  496                        7
   millions of cycles) of high-speed turbine engine
   bearings made out of two different compounds.           483           0  497                        13
   These were taken from "Analysis of Single Classi-
   fication Experiments Based on Censored Samples          484           1  498                        24
   from the Two-parameter Weibull Distribution" by
   J. I. McCool (The Journal of Statistical Planning       485           1  499                        56
   and Inference, 1979).
                                                           486           0  500                        82

                                                           487           1  501                        97

                                                           488           0  502                        64

                                                           489           1  503                        43

                                                           490           0  504                        3

                                                           491           2  505                        1

Compound 1                                                 492           0  506                        0

3.03, 5.53, 5.60, 9.30, 9.92,                              493           0  507                        0
12.51, 12.95, 15.21, 16.04, 16.84
                                                           494           0  508                        0

                                                           495           6  509                        2

Compound 2

3.19, 4.26, 4.47, 4.53, 4.67,
4.69, 5.78, 6.79, 9.37, 12.75
                                                                                   Chapter 3 Exercises 117

     (a) Summarize these data, using appropriate                       purity. Describe the shape of the purity distri-
          graphical and numerical tools. How would                     bution.
          you describe the shape of the distribution of           (c) The author of the article found it useful to
          punch heights? The specifications for punch                  reexpress the purities by subtracting 99.30
          heights were in fact .500 in. to .505 in. Does               (remember that the preceding values are in
          this fact give you any insight as to the ori-                units of .01% above 99.00%) and then tak-
          gin of the distributional shape observed in                  ing natural logarithms. Do this with the raw
          the data? Does it appear that the supplier has               data and make a second stem-and-leaf dia-
          equipment capable of meeting the engineer-                   gram and a second histogram to portray the
          ing specifications on punch height?                          shape of the transformed data. Do these fig-
                                                                       ures look more bell-shaped than the ones you
     (b) In the manufacturing application of these                     made in part (b)?
          punches, several had to be placed side-by-side          (d) Make a normal plot for the transformed values
          on a drum to cut the same piece of material. In              from part (c). What does it indicate about the
          this context, why is having small variability                shape of the distribution of the transformed
          in punch height perhaps even more important                  values? (Standard normal quantiles for p =
          than having the correct mean punch height?                   .005 and p = .995 are approximately -2.58
                                                                       and 2.58, respectively.)
10. The article "Watch Out for Nonnormal Distri-
     butions" by D. C. Jacobs (Chemical Engineer-            11. The following are some data taken from the article
     ing Progress, November 1990) contains 100 mea-               "Confidence Limits for Weibull Regression with
     sured daily purities of oxygen delivered by a sin-           Censored Data" by J. I. McCool (IEEE Transac-
     gle supplier. These are as follows, listed in the time       tions on Reliability, 1980). They are the ordered
     order of their collection (read left to right, top to        failure times (the time units are not given in the
     bottom). The values given are in hundredths of               paper) for hardened steel specimens subjected to
     a percent purity above 99.00% (so 63 stands for              rolling contact fatigue tests at four different values
     99.63%).                                                     of contact stress.

     63, 61, 67, 58, 55, 50, 55, 56, 52, 64, 73, 57, 63,     .87 × 106  .99 × 106  1.09 × 106  1.18 × 106
     81, 64, 54, 57, 59, 60, 68, 58, 57, 67, 56, 66, 60,         psi        psi        psi          psi
     49, 79, 60, 62, 60, 49, 62, 56, 69, 75, 52, 56, 61,
     58, 66, 67, 56, 55, 66, 55, 69, 60, 69, 70, 65, 56,         1.67       .80        .012        .073
     73, 65, 68, 59, 62, 58, 62, 66, 57, 60, 66, 54, 64,         2.20      1.00        .18         .098
     62, 64, 64, 50, 50, 72, 85, 68, 58, 68, 80, 60, 60,         2.51      1.37        .20         .117
     53, 49, 55, 80, 64, 59, 53, 73, 55, 54, 60, 60, 58,         3.00      2.25        .24         .135
     50, 53, 48, 78, 72, 51, 60, 49, 67                          3.90      2.95        .26         .175
                                                                 4.70      3.70        .32         .262
     You will probably want to use a statistical analysis        7.53      6.07        .32         .270
     package to help you do the following:                     14.7        6.65        .42         .350
     (a) Make a run chart for these data. Are there any        27.8        7.05        .44         .386
                                                               37.4        7.37        .88         .456
          obvious time trends? What would be the prac-
          tical engineering usefulness of early detection    (a) Make side-by-side boxplots for these data.
          of any such time trend?                                Does it look as if the different stress levels
     (b) Now ignore the time order of data collection            produce life distributions of roughly the same
          and represent these data with a stem-and-leaf          shape? (Engineering experience suggests that
          plot and a histogram. (Use .02% class widths
          in making your histogram.) Mark on these the
          supplier's lower specification limit of 99.50%
118 Chapter 3 Elementary Descriptive Statistics

          different stress levels often change the scale          by R. Rossi (Solid State Technology, 1984). (The
          but not the basic shape of life distributions.)         units were not given in the article.)
     (b) Make Q-Q plots for comparing all six dif-
          ferent possible pairs of distributional shapes.           5.55, 5.52, 5.45, 5.53, 5.37, 5.22, 5.62, 5.69,
          Summarize in a few sentences what these in-               5.60, 5.58, 5.51, 5.53
          dicate about the shapes of the failure time
          distributions under the different stress levels.        (a) Make a dot diagram and a boxplot for these
                                                                       data and compute the statistics x¯ and s.
12. Riddle, Peterson, and Harper studied the perfor-
     mance of a rapid-cut industrial shear in a continu-          (b) Make a normal plot for these data. How bell-
     ous cut mode. They cut nominally 2-in. and 1-in.                  shaped does this data set look? If you were to
     strips of 14 gauge and 16 gauge steel sheet metal                 say that the shape departs from a perfect bell
     and measured the actual widths of the strips pro-                 shape, in what specific way does it? (Refer to
     duced by the shear. Their data follow, in units of                characteristics of the normal plot to support
     10-3 in. above nominal.                                           your answer.)

                             Material Thickness              14. The article "Thermal Endurance of Polyester
                                                                  Enameled Wires Using Twisted Wire Specimens"
                             14 Gauge       16 Gauge              by H. Goldenberg (IEEE Transactions on Electri-
                                                                  cal Insulation, 1965) contains some data on the
                      1 in.  2, 1, 1, 1,    -2, -6, -1, -2,       lifetimes (in weeks) of wire specimens tested for
Machine Setting              0, 0, -2,      -1, -2, -1,           thermal endurance according to AIEE Standard
                             -10, -5, 1     -1, -1, -5            57. Several different laboratories were used to
                      2 in.                                       make the tests, and the results from two of the
                             10, 10, 8, 8,  -4, -3, -4, -2,       laboratories, using a test temperature of 200C,
                             8, 8, 7,       -3, -3, -3,           follow:
                             7, 9, 11       -3, -4, -4
                                                             Laboratory 1         Laboratory 2
     (a) Compute sample means and standard devia-
          tions for the four samples. Plot the means in      14, 16, 17, 18, 20,  27, 28, 29, 29, 29,
          a manner similar to the plot in Figure 3.22.       22, 23, 25, 27, 28   30, 31, 31, 33, 34
          Make a separate plot of this kind for the stan-
          dard deviations.                                   Consider first only the Laboratory 1 data.
                                                             (a) Find the median and the first and third quar-
     (b) Write a short report to an engineering man-
          ager to summarize what these data and your              tiles for the lifetimes and then find the .64
          summary statistics and plots show about the             quantile of the data set.
          performance of the industrial shear. How do        (b) Make and interpret a normal plot for these
          you recommend that the shear be set up in               data. Would you describe this distribution as
          the future in order to get strips cut from these        bell-shaped? If not, in what way(s) does it
          materials with widths as close as possible to           depart from being bell-shaped? Give the co-
          specified dimensions?                                   ordinates of the 10 points you plot on regular
                                                                  graph paper.
13. The accompanying data are some measured resis-           (c) Find the sample mean, the sample range, and
     tivity values from in situ doped polysilicon spec-           the sample standard deviation for these data.
     imens taken from the article "LPCVD Process             Now consider comparing the work of the two dif-
     Equipment Evaluation Using Statistical Methods"         ferent laboratories (i.e., consider both data sets).
                                                          Chapter 3 Exercises 119

     (d) Make back-to-back stem-and-leaf plots for        16. The accompanying values are representative of
          these two data sets (use two leaves for obser-       data summarized in a histogram appearing in
          vations 10-19, two for observations 20-29,           the article "Influence of Final Recrystallization
          etc.)                                                Heat Treatment on Zircaloy-4 Strip Corrosion"
                                                               by Foster, Dougherty, Burke, Bates, and Worces-
     (e) Make side-by-side boxplots for these two data         ter (Journal of Nuclear Materials, 1990). Given
          sets. (Draw these on the same scale.)                are n = 20 particle diameters observed in a bright-
                                                               field TEM micrograph of a Zircaloy-4 specimen.
      (f) Based on your work in parts (d) and (e), which       The units are 10-2µm.
          of the two labs would you say produced the
          more precise results?                                  1.73, 2.47, 2.83, 3.20, 3.20, 3.57, 3.93, 4.30,
                                                                 4.67, 5.03, 5.03, 5.40, 5.77, 6.13, 6.50, 7.23,
     (g) Is it possible to tell from your plots in (d)           7.60, 8.33, 9.43, 11.27
          and (e) which lab produced the more accurate
          results? Why or why not?                             (a) Compute the mean and standard deviation of
                                                                    these particle diameters.
15. Agusalim, Ferry, and Hollowaty made some mea-
     surements on the thickness of wallboard during            (b) Make both a dot diagram and a boxplot for
     its manufacture. The accompanying table shows                  these data. Sketch the dot diagram on a ruled
     thicknesses (in inches) of 12 different 4 ft × 8 ft            scale and make the boxplot below it.
     boards (at a single location on the boards) both
     before and after drying in a kiln. (These boards          (c) Based on your work in (b), how would you
     were nominally .500 in. thick.)                                describe the shape of this data set?

Board           123456                                         (d) Make a normal plot of these data. In what
Before Drying  .514 .505 .500 .490 .503 .500                        specific way does the distribution depart from
After Drying   .510 .502 .493 .486 .497 .494                        being bell-shaped?

Board           7 8 9 10 11 12                                 (e) It is sometimes useful to find a scale of mea-
Before Drying  .510 .508 .500 .511 .505 .501                        surement on which a data set is reasonably
After Drying   .502 .505 .488 .486 .491 .498                        bell-shaped. To that end, take the natural loga-
                                                                    rithms of the raw particle diameters. Normal-
(a) Make a scatterplot of these data. Does there                    plot the log diameters. Does this plot appear
     appear to be a strong relationship between                     to be more linear than your plot in (d)?
     after-drying thickness and before-drying
     thickness? How might such a relationship             17. The data in the accompanying tables are measure-
     be of practical engineering importance in the             ments of the latent heat of fusion of ice taken from
     manufacture of wallboard?                                 Experimental Statistics (NBS Handbook 91) by
                                                               M. G. Natrella. The measurements were made (on
(b) Calculate the 12 before minus after differ-                specimens cooled to -.072C) using two differ-
     ences in thickness. Find the sample mean and              ent methods. The first was an electrical method,
     sample standard deviation of these values.                and the second was a method of mixtures. The
     How might the mean value be used in running               units are calories per gram of mass.
     the sheetrock manufacturing process? (Based               (a) Make side-by-side boxplots for comparing
     on the mean value, what is an ideal before-                    the two measurement methods. Does there
     drying thickness for the boards?) If some-                     appear to be any important difference in the
     how all variability in before-drying thickness                 precision of the two methods? Is it fair to
     could be eliminated, would substantial after-                  say that at least one of the methods must be
     drying variability in thickness remain? Ex-                    somewhat inaccurate? Explain.
     plain in terms of your calculations.
120 Chapter 3 Elementary Descriptive Statistics

        Method A (Electrical)                                        coordinates of the points you plot on regular
                                                                     graph paper.)
        79.98, 80.04, 80.02, 80.04, 80.03, 80.03, 80.04,        (c) Find the sample mean and sample standard
        79.97, 80.05, 80.03, 80.02, 80.00, 80.02                     deviation of the Heat 1 data.
                                                                (d) Make a stem-and-leaf plot for the Heat 1 data
                    Method B (Mixtures)                              using only the leading digits 0, 1, 2, 3, 4 and 5
                                                                     to the left of the stem (and pairs of final digits
                    80.02, 79.94, 79.98, 79.97,                      to the right).
                    79.97, 80.03, 79.95, 79.97                  (e) Now make back-to-back stem-and-leaf plots
                                                                     for the Heat 1 and Heat 2 data. How do the
     (b) Compute and compare the sample means and                    two distributions of fatigue lives compare?
          the sample standard deviations for the two             (f) Show the calculations necessary to make box-
          methods. How are the comparisons of these                  plots for each of the three data sets above.
          numerical quantities already evident on your               Then draw these side by side on the same
          plot in (a)?                                               scale to compare the three heats. How would
                                                                     you say that these three heats compare in
18. T. Babcock did some fatigue life testing on spec-                terms of uniformity of fatigue lives produced?
     imens of 1045 steel obtained from three different               Do you see any clear differences between
     heats produced by a single steel supplier. The lives            heats in terms of the average fatigue life pro-
     till failure of 30 specimens tested on a rotary fa-             duced?
     tigue strength machine (units are 100 cycles) are
                                                           19. Loveland, Rahardja, and Rainey studied a metal
                      Heat 1                                    turning process used to make some (cylindrical)
                                                                servo sleeves. Outside diameter measurements
                      313, 100, 235, 250, 457,                  made on ten of these sleeves are given here. (Units
                      11, 315, 584, 249, 204                    are 10-5 inch above nominal. The "notch" axis of
                                                                the sleeve was an identifiable axis and the non-
                      Heat 2                                    notch axis was perpendicular to the notch axis. A
                                                                dial bore gauge and an air spindler gauge were
                      349, 206, 163, 350, 189,                  used.)
                      216, 170, 359, 267, 196
                                                           Sleeve              1  2  3   4  5
                      Heat 3
                                                           Notch/Dial Bore     130 160 170 310 200
                      289, 279, 142, 334, 192,
                      339, 87, 185, 262, 194               Non-Notch/Dial Bore 150 150 210 160 160

     (a) Find the median and first and third quartiles     Notch/Air Spindler  40 60 45  0 30
          for the Heat 1 data. Then find the .62 quantile
          of the Heat 1 data set.                          Sleeve              6  7  8   9 10

     (b) Make and interpret a normal plot for the Heat     Notch/Dial Bore     130 200 150 200 140
          1 data. Would you describe this data set as
          bell-shaped? If not, in what specific way does   Non-Notch/Dial Bore 140 220 150 220 160
          the shape depart from the bell shape? (List the
                                                           Notch/Air Spindler     0 25 25 -40 65

                                                           (a) What can be learned from the dial bore data
                                                               that could not be learned from data consisting
                                                               of the given notch measurements above and
                                                               ten non-notch measurements on a different
                                                               ten servo sleeves?
                                                             Chapter 3 Exercises 121

     (b) The dial bore data might well be termed                  (c) Find the sample mean, the sample range, and
          "paired" data. A common method of anal-                      the sample standard deviation for the Laser
          ysis for such data is to take differences and                data.
          study those. Compute the ten "notch minus
          non-notch" differences for the dial bore val-           Now consider comparing the two different drilling
          ues. Make a dot diagram for these and then              methods.
          a boxplot. What physical interpretation does            (d) Make back-to-back stem-and-leaf plots for
          a nonzero mean for such differences have?
          What physical interpretation does a large vari-              the two data sets.
          ability in these differences have?                      (e) Make side-by-side boxplots for the two data

     (c) Make a scatterplot of the air spindler notch                  sets. (Draw these on the same scale.)
          measurements versus the dial bore notch mea-             (f) Based on your work in parts (d) and (e), which
          surements. Does it appear that the air spindler
          and dial bore measurements are strongly re-                  of the two processes would you say produced
          lated?                                                       the most consistent results? Which process
                                                                       produced an "average" angle closest to the
     (d) How would you suggest trying to determine                     nominal angle (45)?
          which of the two gauges is most precise?                As it turns out, each metal part actually had two
                                                                  holes drilled in it and their angles measured. Be-
20. Duren, Leng and Patterson studied the drilling of             low are the measured angles of the second hole
     holes in a miniature metal part using two different          drilled in each of the parts made using the Laser
     physical processes (laser drilling and electrical            process. (The data are listed in the same part order
     discharge machining). Blueprint specifications on            as earlier.)
     these holes called for them to be drilled at an angle
     of 45 to the top surface of the part in question.                    Laser (Hole B)
     The realized angles measured on 13 parts drilled
     using each process (26 parts in all) are                             43.1, 44.3, 44.5, 46.3, 43.9, 41.9,

             Laser (Hole A)                                               43.4, 49.0, 43.5, 47.2, 44.8 ,44.0, 43.9

             42.8, 42.2, 42.7, 43.1, 40.0, 43.5,                  (g) Taking together the two sets of Laser mea-
             42.3, 40.3, 41.3, 48.5, 39.5, 41.1, 42.1                  surements, how would you describe these val-
                                                                       ues using the terminology of Section 1.2?
             EDM (Hole A)
                                                                  (h) Make a scatterplot of the Hole A and Hole B
             46.1, 45.3, 45.3, 44.7, 44.2, 44.6,                       laser data. Does there appear to be a strong
             43.4, 44.6, 44.6, 45.5, 44.4, 44.0, 43.2                  relationship between the angles produced in
                                                                       a single part by this drilling method?
     (a) Find the median and the first and third quar-
          tiles for the Laser data. Then find the .37 quan-        (i) Calculate the 13 Hole A minus Hole B differ-
          tile of the Laser data set.                                  ences in measured angles produced using the
                                                                       Laser drilling process. Find the sample mean
     (b) Make and interpret a normal plot for the Laser                and sample standard deviation of these val-
          data. Would you describe this distribution as                ues. What do these quantities measure here?
          bell-shaped? If not, in what way(s) does it
          depart from being bell-shaped?                     21. Blad, Sobotka, and Zaug did some hardness test-
                                                                  ing of a metal specimen. They tested it on three
                                                                  different machines, a dial Rockwell tester, a dig-
                                                                  ital Rockwell tester, and a Brinell tester. They
                                                                  made ten measurements with each machine and
                                                                  the values they obtained for Brinell hardness (after
122 Chapter 3 Elementary Descriptive Statistics

conversion in the case of the Rockwell readings)              (g) Is it possible to tell from your plot (e) which
were                                                               machine produced the most accurate results?
                                                                   Why or why not?
                  Dial Rockwell
                                                         22. Ritchey, Bazan, and Buhman did an experiment
                  536.6, 539.2, 524.4,                        to compare flight times of several designs of pa-
                  536.6, 526.8, 531.6,                        per helicopters, dropping them from the first to
                  540.5, 534.0, 526.8,                        ground floors of the ISU Design Center. The flight
                  531.6                                       times that they reported for two different designs
                                                              were (the units are seconds)

                  Digital Rockwell                       Design 1                       Design 2

                  501.2, 522.0, 531.6,                   2.47, 2.45, 2.43, 2.67, 2.69,  3.42, 3.50, 3.29, 3.51, 3.53,
                  522.0, 519.4, 523.2,                   2.48, 2.44, 2.71, 2.84, 2.84   2.67, 2.69, 3.47, 3.40, 2.87
                  522.0, 514.2, 506.4,
                  518.1                                  (a) Find the median and the first and third quar-
                                                              tiles for the Design 1 data. Then find the .62
                  Brinell                                     quantile of the Design 1 data set.

                  542.6, 526.0, 520.5,                   (b) Make and interpret a normal plot for the De-
                  514.0, 546.6, 512.6,                        sign 1 data. Would you describe this distri-
                  516.0, 580.4, 600.0,                        bution as bell-shaped? If not, in what way(s)
                  601.0                                       does it depart from being bell-shaped?

Consider first only the Dial Rockwell data.              (c) Find the sample mean, the sample range, and
(a) Find the median and the first and third quar-             the sample standard deviation for the Design 1
                                                              data. Show some work.
     tiles for the hardness measurements. Then
     find the .27 quantile of the data set.              Now consider comparing the two different de-
(b) Make and interpret a normal plot for these           signs.
     data. Would you describe this distribution as       (d) Make back-to-back stem-and-leaf plots for
     bell-shaped? If not, in what way(s) does it
     depart from being bell-shaped?                           the two data sets.
(c) Find the sample mean, the sample range, and          (e) Make side-by-side boxplots for the two data
     the sample standard deviation for these data.
Now consider comparing the readings from the                  sets. (Draw these on the same scale.)
different testers (i.e., consider all three data sets.)  (f) Based on your work in parts (d) and (e), which
(d) Make back-to-back stem-and-leaf plots for
     the two Rockwell data sets. (Use two "leaves"            of the two designs would you say produced
     for observations 500-509, two for the obser-             the most consistent results? Which design
     vations 510-519, etc.)                                   produced the longest flight times?
(e) Make side-by-side boxplots for all three data        (g) It is not really clear from the students' report
     sets. (Draw these on the same scale.)                    whether the data came from the dropping of
(f) Based on your work in part (e), which of the              one helicopter of each design ten times, or
     three machines would you say produced the                from the dropping of ten helicopters of each
     most precise results?                                    design once. Briefly discuss which of these
                                                              possibilities is preferable if the object of the
                                                              study was to identify a superior design. (If
                                                              necessary, review Section 2.3.4.)
4q q q q q q q q q q q q q q q q q q q q q q q q q q q

         Describing
         Relationships
         Between Variables

The methods of Chapter 3 are really quite simple. They require little in the way of

calculations and are most obviously relevant to the analysis of a single engineering
variable. This chapter provides methods that address the more complicated prob-
lem of describing relationships between variables and are computationally more
demanding.

     The chapter begins with least squares fitting of a line to bivariate quantitative
data and the assessment of the goodness of that fit. Then the line-fitting ideas are
generalized to the fitting of curves to bivariate data and surfaces to multivariate
quantitative data. The next topic is the summarization of data from full factorial
studies in terms of so-called factorial effects. Next, the notion of data transforma-
tions is discussed. Finally, the chapter closes with a short transitional section that
argues that further progress in statistics requires some familiarity with the subject
of probability.

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

4.1 Fitting a Line by Least Squares

                    Bivariate data often arise because a quantitative experimental variable x has been
                    varied between several different settings, producing a number of samples of a
                    response variable y. For purposes of summarization, interpolation, limited extrap-
                    olation, and/or process optimization/adjustment, it is extremely helpful to have an
                    equation relating y to x. A linear (or straight line) equation

y  0 + 1x                                               (4.1)
                                                         123
124 Chapter 4 Describing Relationships Between Variables

                                  relating y to x is about the simplest potentially useful equation to consider after
                                  making a simple (x, y) scatterplot.

                                       In this section, the principle of least squares is used to fit a line to (x, y)
                                  data. The appropriateness of that fit is assessed using the sample correlation and
                                  the coefficient of determination. Plotting of residuals is introduced as an important
                                  method for further investigation of possible problems with the fitted equation. A
                                  discussion of some practical cautions and the use of statistical software in fitting
                                  equations to data follows.

4.1.1 Applying the Least Squares Principle

Example 1  Pressing Pressures and Specimen Densities for a Ceramic Compound

           Benson, Locher, and Watkins studied the effects of varying pressing pressures on
           the density of cylindrical specimens made by dry pressing a ceramic compound.
           A mixture of Al2O3, polyvinyl alcohol, and water was prepared, dried overnight,
           crushed, and sieved to obtain 100 mesh size grains. These were pressed into
           cylinders at pressures from 2,000 psi to 10,000 psi, and cylinder densities were
           calculated. Table 4.1 gives the data that were obtained, and a simple scatterplot
           of these data is given in Figure 4.1.

           Table 4.1
           Pressing Pressures and Resultant
           Specimen Densities

                  x,              y,
           Pressure (psi)  Density (g/cc)

            2,000                           2.486
            2,000                           2.479
            2,000                           2.472
            4,000                           2.558
            4,000                           2.570
            4,000                           2.580
            6,000                           2.646
            6,000                           2.657
            6,000                           2.653
            8,000                           2.724
            8,000                           2.774
            8,000                           2.808
           10,000                           2.861
           10,000                           2.879
           10,000                           2.858
                              4.1 Fitting a Line by Least Squares 125

                                          2.900Density (g/cc)

                                          2.800

                                          2.700

                                          2.600

                                          2.500

                                                        2,000 4,000 6,000 8,000 10,000
                                                                       Pressure (psi)

                                    Figure 4.1 Scatterplot of density vs. pressing pressure

                   It is very easy to imagine sketching a straight line through the plotted points in
              Figure 4.1. Such a line could then be used to summarize how density depends upon
              pressing pressure. The principle of least squares provides a method of choosing a
              "best" line to describe the data.

Definition 1  To apply the principle of least squares in the fitting of an equation for y to
              an n-point data set, values of the equation parameters are chosen to minimize

                               n                (4.2)

                                   yi - y^ i 2

                              i =1

              where y1, y2, . . . , yn are the observed responses and y^ 1, y^ 2, . . . , y^ n are corre-
              sponding responses predicted or fitted by the equation.

                   In the context of fitting a line to (x, y) data, the prescription offered by Def-
              inition 1 amounts to choosing a slope and intercept so as to minimize the sum of
              squared vertical distances from (x, y) data points to the line in question. This notion
              is shown in generic fashion in Figure 4.2 for a fictitious five-point data set. (It is the
              squares of the five indicated differences that must be added and minimized.)

                   Looking at the form of display (4.1), for the fitting of a line,

                                                       y^ = 0 + 1x
126 Chapter 4 Describing Relationships Between Variables

     y                                                                       A possible
                                                                             fitted line
                 y3 - y3                                    y4 - y4
               is positive                                is positive    y5 - y5
                                                                       is negative
       y1 - y1
     is positive

                              y2 - y2
                            is negative

                                                                         x

        Figure 4.2 Five data points (x, y) and a possible
        fitted line

Therefore, the expression to be minimized by choice of slope (1) and intercept
(0) is

                                n                         yi - (0 + 1xi ) 2               (4.3)

        S(0, 1) =

                              i =1

The minimization of the function of two variables S(0, 1) is an exercise in calculus.
The partial derivatives of S with respect to 0 and 1 may be set equal to zero, and the
two resulting equations may be solved simultaneously for 0 and 1. The equations
produced in this way are

        n0 +                                               n                  n           (4.4)

                                                              xi  1 = yi

                                                          i =1              i =1

and

         n      0 +                                        n                       n      (4.5)

            xi                                                xi2      1 = xi yi

        i =1                                              i =1                   i =1

For reasons that are not obvious, equations (4.4) and (4.5) are sometimes called
the normal (as in perpendicular) equations for fitting a line. They are two linear
equations in two unknowns and can be fairly easily solved for 0 and 1 (provided
                                                                       4.1 Fitting a Line by Least Squares 127
                    there are at least two different xi 's in the data set). Simultaneous solution of equations
                    (4.4) and (4.5) produces values of 1 and 0 given by

Slope of the             b1 =  xi - x¯ yi - y¯  (4.6)
least squares                      xi - x¯ 2

       line, b1

                    and

    Intercept of               b0 = y¯ - b1x¯   (4.7)
         the least

squares line, b0

                    Notice the notational convention here. The particular numerical slope and intercept
                    minimizing S(0, 1) are denoted (not as 's but) as b1 and b0.

                         In display (4.6), somewhat standard practice has been followed (and the sum-
                    mation notation abused) by not indicating the variable or range of summation (i,
                    from 1 to n).

Example 1           It is possible to verify that the data in Table 4.1 yield the following summary
(continued )        statistics:

                                         xi = 2,000 + 2,000 + · · · + 10,000 = 90,000,
                                       so x¯ = 90,000 = 6,000

                                                   15
                                  xi - x¯ 2 = (2,000 - 6,000)2 + (2,000 - 6,000)2 + · · · +

                                                (10,000 - 6,000)2 = 120,000,000
                                          yi = 2.486 + 2.479 + · · · + 2.858 = 40.005,
                                       so y¯ = 40.005 = 2.667

                                                   15
                                  yi - y¯ 2 = (2.486 - 2.667)2 + (2.479 - 2.667)2 + · · · +

                                                (2.858 - 2.667)2 = .289366
                         xi - x¯ yi - y¯ = (2,000 - 6,000)(2.486 - 2.667) + · · · +

                                                (10,000 - 6,000)(2.858 - 2.667) = 5,840
128 Chapter 4 Describing Relationships Between Variables

       Example 1        Then the least squares slope and intercept, b1 and b0, are given via equations
      (continued )      (4.6) and (4.7) as

Interpretation of                            b1 =   5,840        = .0000486 (g/cc)/psi
 the slope of the
     least squares                                  120,000,000
                  line
                        and
    Extrapolation
                                           b0 = 2.667 - (.0000486)(6,000) = 2.375 g/cc
                             Figure 4.3 shows the least squares line

                                                    y^ = 2.375 + .0000487x

                        sketched on a scatterplot of the (x, y) points from Table 4.1. Note that the slope on
                        this plot, b1  .0000487 (g/cc)/psi, has physical meaning as the (approximate)
                        increase in y (density) that accompanies a unit (1 psi) increase in x (pressure).
                        The intercept on the plot, b0 = 2.375 g/cc, positions the line vertically and is the
                        value at which the line cuts the y axis. But it should probably not be interpreted
                        as the density that would accompany a pressing pressure of x = 0 psi. The point
                        is that the reasonably linear-looking relation that the students found for pressures

                        between 2,000 psi and 10,000 psi could well break down at larger or smaller

                        pressures. Thinking of b0 as a 0 pressure density amounts to an extrapolation
                        outside the range of data used to fit the equation, something that ought always to

                        be approached with extreme caution.

                             Density (g/cc)  2.900
                                             2.800
                                             2.700                   Least squares line
                                             2.600               y = 2. 375 + .0000487x
                                             2.500

                                              2,000 4,000 6,000 8,000 10,000
                                                             Pressure (psi)

                             Figure 4.3 Scatterplot of the pressure/density data
                             and the least squares line
               4.1 Fitting a Line by Least Squares 129

                    As indicated in Definition 1, the value of y on the least squares line correspond-
               ing to a given x can be termed a fitted or predicted value. It can be used to represent
               likely y behavior at that x.

Example 1      Consider the problem of determining a typical density corresponding to a pressure
(continued )   of 4,000 psi and one corresponding to 5,000 psi.

                    First, looking at x = 4,000, a simple way of representing a typical y is to
               note that for the three data points having x = 4,000,

                                   y¯ = 1 (2.558 + 2.570 + 2.580) = 2.5693 g/cc
                                         3

               and so to use this as a representative value. But assuming that y is indeed
               approximately linearly related to x, the fitted value

               y^ = 2.375 + .0000486(4,000) = 2.5697 g/cc

Interpolation  might be even better for representing average density for 4,000 psi pressure.
                    Looking then at the situation for x = 5,000 psi, there are no data with this

               x value. The only thing one can do to represent density at that pressure is to ask

               whether interpolation is sensible from a physical viewpoint. If so, the fitted value

               y^ = 2.375 + .0000486(5,000) = 2.6183 g/cc

               can be used to represent density for 5,000 psi pressure.

4.1.2          The Sample Correlation and Coefficient of Determination

               Visually, the least squares line in Figure 4.3 seems to do a good job of fitting the
               plotted points. However, it would be helpful to have methods of quantifying the
               quality of that fit. One such measure is the sample correlation.

Definition 2   The sample (linear) correlation between x and y in a sample of n data pairs
               (xi , yi ) is

               r = xi - x¯ yi - y¯                                       (4.8)
                            xi - x¯ 2 · yi - y¯ 2

   Interpreting the The sample correlation always lies in the interval from -1 to 1. Further, it is -1
sample correlation or 1 only when all (x, y) data points fall on a single straight line. Comparison of
130 Chapter 4 Describing Relationships Between Variables

                   formulas (4.6) and (4.8) shows that r = b1  xi - x¯ 2 / yi - y¯ 2 so that 1/2

                   b1 and r have the same sign. So a sample correlation of -1 means that y decreases
                   linearly in increasing x, while a sample correlation of +1 means that y increases

                   linearly in increasing x.

                   Real data sets do not often exhibit perfect (+1 or -1) correlation. Instead r is

                   typically between -1 and 1. But drawing on the facts about how it behaves, people

                   take r as a measure of the strength of an apparent linear relationship: r near +1

                   or -1 is interpreted as indicating a relatively strong linear relationship; r near 0

                   is taken as indicating a lack of linear relationship. The sign of r is thought of as

                   indicating whether y tends to increase or decrease with increased x.

Example 1          For the pressure/density data, the summary statistics in the example following
(continued )       display (4.7) (page 127) produces

                                          r = 5,840 = .9911
                                                  (120,000,000)(.289366)

                   This value of r is near +1 and indicates clearly the strong positive linear rela-
                   tionship evident in Figures 4.1 and 4.3.

                        The coefficient of determination is another measure of the quality of a fitted
                   equation. It can be applied not only in the present case of the simple fitting of a line
                   to (x, y) data but more widely as well.

Definition 3       The coefficient of determination for an equation fitted to an n-point data set
                   via least squares and producing fitted y values y^ 1, y^ 2, . . . , y^ n is

                                              R2 =  yi - y¯ 2 - yi - y^ i 2              (4.9)
                                                             yi - y¯ 2

Interpretation     R2 may be interpreted as the fraction of the raw variation in y accounted for
            of R2
                   using the fitted equation. That is, provided the fitted equation includes a constant
                   term, (yi - y¯ )2  (yi - y^ i )2. Further, (yi - y¯ )2 is a measure of raw variabil-
                   ity in y, while (yi - y^ i )2 is a measure of variation in y remaining after fitting the
                   equation. So the nonnegative difference (yi - y¯ )2 - (yi - y^ i )2 is a measure of
                   the variability in y accounted for in the equation-fitting process. R2 then expresses

                   this difference as a fraction (of the total raw variation).
Example 1                                                         4.1 Fitting a Line by Least Squares 131
(continued )
                   Using the fitted line, one can find y^ values for all n = 15 data points in the original
                   data set. These are given in Table 4.2.

                   Table 4.2
                   Fitted Density Values

                   x, Pressure y^ , Fitted Density

                    2,000                               2.4723
                    4,000                               2.5697
                    6,000                               2.6670
                    8,000                               2.7643
                   10,000                               2.8617

                   Then, referring again to Table 4.1,

                        (yi - y^ i )2 = (2.486 - 2.4723)2 + (2.479 - 2.4723)2 + (2.472 - 2.4723)2
                                         + (2.558 - 2.5697)2 + · · · + (2.879 - 2.8617)2
                                         + (2.858 - 2.8617)2

                                     = .005153

                   Further, since (yi - y¯ )2 = .289366, from equation (4.9)

                                              R2 = .289366 - .005153 = .9822
                                                            .289366

                   and the fitted line accounts for over 98% of the raw variability in density, reducing
                   the "unexplained" variation from .289366 to .005153.

R2 as a squared         The coefficient of determination has a second useful interpretation. For equa-
      correlation
                   tions that are linear in the parameters (which are the only ones considered in this
                   text), R2 turns out to be a squared correlation. It is the squared correlation between
                   the observed values yi and the fitted values y^ i . (Since in the present situation of
                   fitting a line, the y^ i values are perfectly correlated with the xi values, R2 also turns
                   out to be the squared correlation between the yi and xi values.)

Example 1          For the pressure/density data, the correlation between x and y is
(continued )                                                 r = .9911
132 Chapter 4 Describing Relationships Between Variables

Example 1     Since y^ is perfectly correlated with x, this is also the correlation between y^ and y.
(continued )  But notice as well that

                                            r 2 = (.9911)2 = .9822 = R2

              so R2 is indeed the squared sample correlation between y and y^ .

4.1.3         Computing and Using Residuals

              When fitting an equation to a set of data, the hope is that the equation extracts the
              main message of the data, leaving behind (unpredicted by the fitted equation) only
              the variation in y that is uninterpretable. That is, one hopes that the yi 's will look like
              the y^ i 's except for small fluctuations explainable only as random variation. A way
              of assessing whether this view is sensible is through the computation and plotting
              of residuals.

Definition 4  If the fitting of an equation or model to a data set with responses y1, y2, . . . , yn
              produces fitted values y^ 1, y^ 2, . . . , y^ n, then the corresponding residuals are the
              values

                                                     ei = yi - y^ i

                   If a fitted equation is telling the whole story contained in a data set, then its
              residuals ought to be patternless. So when they're plotted against time order of
              observation, values of experimental variables, fitted values, or any other sensible
              quantities, the plots should look randomly scattered. When they don't, the patterns
              can themselves suggest what has gone unaccounted for in the fitting and/or how the
              data summary might be improved.

Example 2     Compressive Strength of Fly Ash Cylinders as a Function
              of Amount of Ammonium Phosphate Additive

              As an exaggerated example of the previous point, consider the naive fitting of a
              line to some data of B. Roth. Roth studied the compressive strength of concrete-
              like fly ash cylinders. These were made using varying amounts of ammonium
              phosphate as an additive. Part of Roth's data are given in Table 4.3. The ammo-
              nium phosphate values are expressed as a percentage by weight of the amount of
              fly ash used.
                                   4.1 Fitting a Line by Least Squares 133

Table 4.3
Additive Concentrations and Compressive Strengths for Fly Ash Cylinders

x, Ammonium        y, Compressive  x, Ammonium          y, Compressive
Phosphate (%)      Strength (psi)  Phosphate (%)        Strength (psi)

0                  1221            3                            1609

0                  1207            3                            1627

0                  1187            3                            1642

1                  1555            4                            1451

1                  1562            4                            1472

1                  1575            4                            1465

2                  1827            5                            1321

2                  1839            5                            1289

2                  1802            5                            1292

     Using formulas (4.6) and (4.7), it is possible to show that the least squares
line through the (x, y) data in Table 4.3 is

                   y^ = 1498.4 - .6381x                                  (4.10)

Then straightforward substitution into equation (4.10) produces fitted values y^ i
and residuals ei = yi - y^ i , as given in Table 4.4. The residuals for this straight-
line fit are plotted against x in Figure 4.4.

     The distinctly "up-then-back-down-again" curvilinear pattern of the plot
in Figure 4.4 is not typical of random scatter. Something has been missed in

Table 4.4
Residuals from a Straight-Line Fit to the Fly Ash Data

xy             y^  e = y - y^      xy                      y^   e = y - y^

0 1221 1498.4      -277.4          3 1609               1496.5      112.5
0 1207 1498.4      -291.4          3 1627               1496.5      130.5
0 1187 1498.4      -311.4          3 1642               1496.5      145.5
1 1555 1497.8                      4 1451               1495.8     -44.8
1 1562 1497.8         57.2         4 1472               1495.8     -23.8
1 1575 1497.8         64.2         4 1465               1495.8     -30.8
2 1827 1497.2         77.2         5 1321               1495.2   -174.2
2 1839 1497.2        329.8         5 1289               1495.2   -206.2
2 1802 1497.2        341.8         5 1292               1495.2   -203.2
                     304.8
134 Chapter 4 Describing Relationships Between Variables

Example 2                                 300
(continued )

                                          200

              Residual, ei                100

                                          0

                                          -100

                                          -200

                                          -300        1   2  3            4           5
                                                   0

                                                      Percent ammonium phosphate, xi

              Figure 4.4 Plot of residuals vs. x for a linear fit to
              the fly ash data

              the fitting of a line to Roth's data. Figure 4.5 is a simple scatterplot of Roth's
              data (which in practice should be made before fitting any curve to such data).
              It is obvious from the scatterplot that the relationship between the amount of
              ammonium phosphate and compressive strength is decidedly nonlinear. In fact,
              a quadratic function would come much closer to fitting the data in Table 4.3.

              Compressive strength (psi)  1800        Least squares line
                                          1700
                                          1600
                                          1500
                                          1400
                                          1300
                                          1200

                                                0     1   2  3            4           5

                                                      Percent ammonium phosphate

                                          Figure 4.5 Scatterplot of the fly ash data
                                     4.1 Fitting a Line by Least Squares 135

                             Plot 1         Plot 2                  Plot 3
                     ei              ei                 ei

                           Order of                 yi  1                   2 Technician
                     observation, i

                                     Figure 4.6 Patterns in residual plots

     Interpreting         Figure 4.6 shows several patterns that can occur in plots of residuals against
      patterns on    various variables. Plot 1 of Figure 4.6 shows a trend on a plot of residuals versus
    residual plots   time order of observation. The pattern suggests that some variable changing in time
                     is acting on y and has not been accounted for in fitting y^ values. For example,
Normal-plotting      instrument drift (where an instrument reads higher late in a study than it did early
          residuals  on) could produce a pattern like that in Plot 1. Plot 2 shows a fan-shaped pattern on
                     a plot of residuals versus fitted values. Such a pattern indicates that large responses
                     are fitted (and quite possibly produced and/or measured) less consistently than small
                     responses. Plot 3 shows residuals corresponding to observations made by Technician
                     1 that are on the whole smaller than those made by Technician 2. The suggestion is
                     that Technician 1's work is more precise than that of Technician 2.

                          Another useful way of plotting residuals is to normal-plot them. The idea is that
                     the normal distribution shape is typical of random variation and that normal-plotting
                     of residuals is a way to investigate whether such a distributional shape applies to
                     what is left in the data after fitting an equation or model.

Example 1            Table 4.5 gives residuals for the fitting of a line to the pressure/density data. The
(continued )         residuals ei were treated as a sample of 15 numbers and normal-plotted (using
                     the methods of Section 3.2) to produce Figure 4.7.

                          The central portion of the plot in Figure 4.7 is fairly linear, indicating a gen-
                     erally bell-shaped distribution of residuals. But the plotted point corresponding to
                     the largest residual, and probably the one corresponding to the smallest residual,
                     fail to conform to the linear pattern established by the others. Those residuals
                     seem big in absolute value compared to the others.

                          From Table 4.5 and the scatterplot in Figure 4.3, one sees that these large
                     residuals both arise from the 8,000 psi condition. And the spread for the three
                     densities at that pressure value does indeed look considerably larger than those at
                     the other pressure values. The normal plot suggests that the pattern of variation
                     at 8,000 psi is genuinely different from those at other pressures. It may be that
                     a different physical compaction mechanism was acting at 8,000 psi than at the
                     other pressures. But it is more likely that there was a problem with laboratory
                     technique, or recording, or the test equipment when the 8,000 psi tests were made.
136 Chapter 4 Describing Relationships Between Variables

Example 1     In any case, the normal plot of residuals helps draw attention to an idiosyncrasy
(continued )  in the data of Table 4.1 that merits further investigation, and perhaps some further
              data collection.

              Table 4.5
              Residuals from the Linear Fit to the Pressure/Density
              Data

              x, Pressure y, Density                      y^      e = y - y^

               2,000                    2.486             2.4723    .0137
               2,000                    2.479             2.4723    .0067
               2,000                    2.472             2.4723  -.0003
               4,000                    2.558             2.5697  -.0117
               4,000                    2.570             2.5697    .0003
               4,000                    2.580             2.5697
               6,000                    2.646             2.6670    .0103
               6,000                    2.657             2.6670  -.0210
               6,000                    2.653             2.6670  -.0100
               8,000                    2.724             2.7643  -.0140
               8,000                    2.774             2.7643  -.0403
               8,000                    2.808             2.7643
              10,000                    2.861             2.8617    .0097
              10,000                    2.879             2.8617    .0437
              10,000                    2.858             2.8617  -.0007
                                                                    .0173
                                                                  -.0037

              Standard normal quantile   2.0

                                         1.0

                                           0

                                       -1.0

                                       -2.0 -.04 -.02 0 .02 .04
                                                             Residual quantile

                                      Figure 4.7 Normal plot of residuals from a
                                      linear fit to the pressure/density data
                                                        4.1 Fitting a Line by Least Squares 137

             4.1.4   Some Cautions

  r Measures only    The methods of this section are extremely useful engineering tools when thoughtfully
linear association   applied. But a few additional comments are in order, warning against some errors
                     in logic that often accompany their use.
  Correlation and
          causation       The first warning regards the correlation. It must be remembered that r measures
                     only the linear relationship between x and y. It is perfectly possible to have a strong
     The influence   nonlinear relationship between x and y and yet have a value of r near 0. In fact,
        of extreme   Example 2 is an excellent example of this. Compressive strength is strongly related
                     to the ammonium phosphate content. But r = -.005, very nearly 0, for the data set
      observations   in Table 4.3.

                          The second warning is essentially a restatement of one implicit in the early part
                     of Section 1.2: Correlation is not necessarily causation. One may observe a large
                     correlation between x and y in an observational study without it being true that x
                     drives y or vice versa. It may be the case that another variable (say, z) drives the
                     system under study and causes simultaneous changes in both x and y.

                          The last warning is that both R2(r ) and least squares fitting can be drastically
                     affected by a few unusual data points. As an example of this, consider the ages and
                     heights of 36 students from an elementary statistics course plotted in Figure 4.8. By
                     the time people reach college age, there is little useful relationship between age and
                     height, but the correlation between ages and heights is .73. This fairly large value
                     is produced by essentially a single data point. If the data point corresponding to the
                     30-year-old student who happened to be 6 feet 8 inches tall is removed from the
                     data set, the correlation drops to .03.

                          An engineer's primary insurance against being misled by this kind of phe-
                     nomenon is the habit of plotting data in as many different ways as are necessary to
                     get a feel for how they are structured. Even a simple boxplot of the age data or height

                                   80                   A 30-year-old
                                                        6' 8" student
                                   75
                     Height (in.)
                                                     3

                                   70 2 3 2

                                               3

                                   65

                        60
                                    20 22 24 26 28 30
                                                  Age (years)

                     Figure 4.8 Scatterplot of ages and heights of 36
                     students
138 Chapter 4 Describing Relationships Between Variables

                                  data alone would have identified the 30-year-old student in Figure 4.8 as unusual.
                                  That would have raised the possibility of that data point strongly influencing both r
                                  and any curve that might be fitted via least squares.

4.1.5  Computing

       The examples in this section have no doubt left the impression that computations
       were done "by hand." In practice, such computations are almost always done with
       a statistical analysis package. The fitting of a line by least squares is done using a
       regression program. Such programs usually also compute R2 and have an option
       that allows the computing and plotting of residuals.

            It is not the purpose of this text to teach or recommend the use of any particular
       statistical package, but annotated printouts will occasionally be included to show
       how MINITAB formats its output. Printout 1 is such a printout for an analysis of
       the pressure/density data in Table 4.1, paralleling the discussion in this section.
       (MINITAB's regression routine is found under its "Stat/Regression/Regression"
       menu.) MINITAB gives its user much more in the way of analysis for least squares
       curve fitting than has been discussed to this point, so your understanding of Printout 1
       will be incomplete. But it should be possible to locate values of the major summary
       statistics discussed here. The printout shown doesn't include plots, but it's worth
       noting that the program has options for saving fitted values and residuals for later
       plotting.

WWW    Printout 1 Fitting the Least Squares Line to the Pressure/Density Data

       Regression Analysis

       The regression equation is
       density = 2.38 +0.000049 pressure

       Predictor            Coef         StDev          T        P
       Constant        2.37500        0.01206    197.01    0.000
       pressure    0.00004867     0.00000182               0.000
                                                  26.78

       S = 0.01991      R-Sq = 98.2%         R-Sq(adj) = 98.1%

       Analysis of Variance

       Source           DF               SS            MS         F        P
                                  0.28421       0.28421    717.06    0.000
       Regression       1         0.00515       0.00040
                                  0.28937
       Residual Error 13

       Total            14

       Obs pressure     density             Fit  StDev Fit      Residual      St Resid
                        2.48600       2.47233       0.00890      0.01367            0.77
       1          2000  2.47900       2.47233       0.00890      0.00667            0.37
                        2.47200       2.47233       0.00890
       2          2000  2.55800       2.56967       0.00630     -0.00033          -0.02
                        2.57000       2.56967       0.00630     -0.01167          -0.62
       3          2000  2.58000       2.56967       0.00630
                        2.64600       2.66700       0.00514      0.00033            0.02
       4          4000  2.65700       2.66700       0.00514      0.01033            0.55
                                                                -0.02100          -1.09
       5          4000                                          -0.01000          -0.52

       6          4000

       7          6000

       8          6000
                                                                      4.1 Fitting a Line by Least Squares 139

   9   6000 2.65300                                          2.66700  0.00514 -0.01400  -0.73
                                                                                        -2.14R
   10  8000 2.72400                                          2.76433  0.00630 -0.04033
                                                                                         0.51
   11  8000 2.77400                                          2.76433  0.00630  0.00967   2.31R
                                                                                        -0.04
   12  8000 2.80800                                          2.76433  0.00630  0.04367   0.97
                                                                                        -0.21
   13  10000 2.86100                                         2.86167  0.00890 -0.00067

   14  10000 2.87900                                         2.86167  0.00890  0.01733

   15  10000 2.85800                                         2.86167  0.00890 -0.00367

   R denotes an observation with a large standardized residual

        At the end of Section 3.3 we warned that using spreadsheet software in place of

   high-quality statistical software can, without warning, produce spectacularly wrong

   answers. The example provided at the end of Section 3.3 concerns a badly wrong

   sample variance of only three numbers. It is important to note that the potential

   for numerical inaccuracy shown in that example carries over to the rest of the

   statistical methods discussed in this book, including those of the present section.
   For example, consider the n = 6 hypothetical (x, y) pairs listed in Table 4.6. For
   fitting a line to these data via least squares, MINITAB correctly produces R2 = .997.
   But as recently as late 1999, the current version of the leading spreadsheet program
   returned the ridiculously wrong value, R2 = -.81648. (This data set comes from a
   posting by Mark Eakin on the "edstat" electronic bulletin board that can be found

   at http://jse.stat.ncsu.edu/archives/.)

          Table 4.6
          6 Hypothetical Data Pairs

                                                          x  y        x        y

          10,000,000.1 1.1 10,000,000.4 3.9
          10,000,000.2 1.9 10,000,000.5 4.9
          10,000,000.3 3.1 10,000,000.6 6.1

Section 1 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. The following is a small set of artificial data. Show         (c) Obtain the sample correlation between y and
   the hand calculations necessary to do the indicated                y^ for these data and compare it to your answer
   tasks.                                                            to part (b).

x  12345                                                        (d) Use the formula in Definition 3 and compute
                                                                      R2 for these data. Compare it to the square of
y  88664                                                             your answers to parts (b) and (c).

(a) Obtain the least squares line through these data.            (e) Find the five residuals from your fit in part (a).
     Make a scatterplot of the data and sketch this                  How are they portrayed geometrically on the
     line on that scatterplot.                                       scatterplot for (a)?

(b) Obtain the sample correlation between x and y            2. Use a computer package and redo the computations
     for these data.                                            and plotting required in Exercise 1. Annotate your
                                                                output, indicating where on the printout you can
140 Chapter 4 Describing Relationships Between Variables

   find the equation of the least squares line, the value      (e) Based on your analysis of these data, what
   of r , the value of R2, and the residuals.                      average molecular weight would you predict
                                                                   for an additional reaction run at 188C? At
3. The article "Polyglycol Modified Poly (Ethylene                 200C? Why would or wouldn't you be willing
   Ether Carbonate) Polyols by Molecular Weight Ad-                to make a similar prediction of average molec-
   vancement" by R. Harris (Journal of Applied Poly-               ular weight if the reaction is run at 70C?
   mer Science, 1990) contains some data on the effect
   of reaction temperature on the molecular weight of      4. Upon changing measurement scales, nonlinear re-
   resulting poly polyols. The data for eight experi-         lationships between two variables can sometimes
   mental runs at temperatures 165C and above are             be made linear. The article "The Effect of Experi-
   as follows:                                                mental Error on the Determination of the Optimum
                                                              Metal-Cutting Conditions" by Ermer and Wu (The
Pot Temperature, x (C)  Average Molecular Weight, y           Journal of Engineering for Industry, 1967) con-
                                                              tains a data set gathered in a study of tool life in
            165                         808                   a turning operation. The data here are part of that
            176                         940                   data set.
            188                       1183
            205                       1545                 Cutting Speed, x (sfpm)  Tool Life, y (min)
            220                       2012
            235                       2362                             800          1.00, 0.90, 0.74, 0.66
            250                       2742                             700          1.00, 1.20, 1.50, 1.60
            260                       2935                             600          2.35, 2.65, 3.00, 3.60
                                                                       500          6.40, 7.80, 9.80, 16.50
Use a statistical package to help you complete the                     400          21.50, 24.50, 26.00, 33.00
following (both the plotting and computations):
(a) What fraction of the observed raw variation in         (a) Plot y versus x and calculate R2 for fitting a
                                                                linear function of x to y. Does the relationship
     y is accounted for by a linear equation in x?              y  0 + 1x look like a reasonable explana-
(b) Fit a linear relationship y  0 + 1x to these                tion of tool life in terms of cutting speed?

     data via least squares. About what change in          (b) Take natural logs of both x and y and repeat
     average molecular weight seems to accompany                part (a) with these log cutting speeds and log
     a 1C increase in pot temperature (at least over            tool lives.
     the experimental range of temperatures)?
(c) Compute and plot residuals from the linear re-         (c) Using the logged variables as in (b), fit a lin-
     lationship fit in (b). Discuss what they suggest           ear relationship between the two variables us-
     about the appropriateness of that fitted equa-             ing least squares. Based on this fitted equation,
     tion. (Plot residuals versus x, residuals versus           what tool life would you predict for a cutting
     y^ , and make a normal plot of them.)                      speed of 550? What approximate relationship
(d) These data came from an experiment where the                between x and y is implied by a linear approx-
     investigator managed the value of x. There is              imate relationship between ln(x) and ln(y)?
     a fairly glaring weakness in the experimenter's            (Give an equation for this relationship.) By the
     data collection efforts. What is it?                       way, Taylor's equation for tool life is yx = C.
                                                      4.2 Fitting Curves and Surfaces by Least Squares 141

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

4.2 Fitting Curves and Surfaces by Least Squares

                    The basic ideas introduced in Section 4.1 generalize to produce a powerful engi-
                    neering tool: multiple linear regression, which is introduced in this section. (Since
                    the term regression may seem obscure, the more descriptive terms curve fitting and
                    surface fitting will be used here, at least initially.)

                         This section first covers fitting curves defined by polynomials and other func-
                    tions that are linear in their parameters to (x, y) data. Next comes the fitting of
                    surfaces to data where a response y depends upon the values of several variables
                    x1, x2, . . . , xk. In both cases, the discussion will stress how useful R2 and resid-
                    ual plotting are and will consider the question of choosing between possible fitted
                    equations. Lastly, we include some additional practical cautions.

4.2.1  Curve Fitting by Least Squares

       In the previous section, a straight line did a reasonable job of describing the pres-
       sure/density data. But in the fly ash study, the ammonium phosphate/compressive
       strength data were very poorly described by a straight line. This section first investi-
       gates the possibility of fitting curves more complicated than a straight line to (x, y)
       data. As an example, an attempt will be made to find a better equation for describing
       the fly ash data.

            A natural generalization of the linear equation

                                   y  0 + 1x  (4.11)

       is the polynomial equation

          y  0 + 1x + 2x2 + · · · + k xk      (4.12)

       The least squares fitting of equation (4.12) to a set of n pairs (xi , yi ) is conceptually
       only slightly more difficult than the task of fitting equation (4.11). The function of
       k + 1 variables

       n                           n

S(0, 1, 2, . . . , k ) = (yi - y^ i )2 = yi - (0 + 1xi + 2xi2 + · · · + k xik ) 2

       i =1                        i =1

       must be minimized. Upon setting the partial derivatives of S(0, 1, . . . , k) equal to
       0, the set of normal equations is obtained for this least squares problem, generaliz-
       ing the pair of equations (4.4) and (4.5). There are k + 1 linear equations in the k + 1
       unknowns 0, 1, . . . , k. And typically, they can be solved simultaneously for a
       single set of values, b0, b1, . . . , bk, minimizing S(0, 1, . . . , k). The mechanics
       of that solution are carried out using a multiple linear regression program.
142 Chapter 4 Describing Relationships Between Variables

              Example 3  More on the Fly Ash Data of Table 4.3
(Example 2 continued )
                         Return to the fly ash study of B. Roth. A quadratic equation might fit the data
                    WWW  better than the linear one. So consider fitting the k = 2 version of equation (4.12)

                                                         y  0 + 1x + 2x2                    (4.13)

                         to the data of Table 4.3. Printout 2 shows the MINITAB run. (After entering x
                         and y values from Table 4.3 into two columns of the worksheet, an additional
                         column was created by squaring the x values.)

                         Printout 2 Quadratic Fit to the Fly Ash Data

                         Regression Analysis

                         The regression equation is
                         y = 1243 + 383 x - 76.7 x**2

                         Predictor      Coef             StDev        T         P
                         Constant   1242.89              42.98  28.92     0.000
                         x                               40.43            0.000
                         x**2        382.67              7.762   9.46     0.000
                                    -76.661                     -9.88

                         S = 82.14       R-Sq = 86.7%        R-Sq(adj) = 84.9%

                         Analysis of Variance

                         Source          DF              SS           MS        F        P
                         Regression                             329115    48.78    0.000
                         Residual Error  2     658230
                         Total                                     6747
                                         15    101206

                                         17    759437

                         Source     DF         Seq SS
                                                     21
                         x          1
                                               658209
                         x**2       1

                              The fitted quadratic equation is

                                                      y^ = 1242.9 + 382.7x - 76.7x2

                         Figure 4.9 shows the fitted curve sketched on a scatterplot of the (x, y) data.
                         Although the quadratic curve is not an altogether satisfactory summary of Roth's
                         data, it does a much better job of following the trend of the data than the line
                         sketched in Figure 4.5.
                                                   4.2 Fitting Curves and Surfaces by Least Squares 143

                 Compressive strength (psi)  1800                 Least squares
                                             1700                    parabola
                                             1600
                                             1500
                                             1400
                                             1300
                                             1200

                                                   0     1  2  3  4                   5

                                                         Percent ammonium phosphate

                 Figure 4.9 Scatterplot and fitted quadratic for the fly
                 ash data

                      The previous section showed that when fitting a line to (x, y) data, it is helpful
                 to quantify the goodness of that fit using R2. The coefficient of determination can

                 also be used when fitting a polynomial of form (4.12). Recall once more from

                 Definition 3 that

 Coefficient of                                    R2 =  (yi - y¯ )2 - (yi - y^ i )2     (4.14)
determination                                                     (yi - y¯ )2

                 is the fraction of the raw variability in y accounted for by the fitted equation.

                 Calculation by hand from formula (4.14) is possible, but of course the easiest way
                 to obtain R2 is to use a computer package.

Example 3        Consulting Printout 2, it can be seen that the equation y^ = 1242.9 + 382.7x -
(continued )     76.7x2 produces R2 = .867. So 86.7% of the raw variability in compressive
                 strength is accounted for using the fitted quadratic. The sample correlation be-
                 tween the observed strengths yi and fitted strengths y^ i is + .867 = .93.

                      Comparing what has been done in the present section to what was done in
                 Section 4.1, it is interesting that for the fitting of a line to the fly ash data, R2

                 obtained there was only .000 (to three decimal places). The present quadratic is

                 a remarkable improvement over a linear equation for summarizing these data.

                      A natural question to raise is "What about a cubic version of equation (4.12)?"

                 Printout 3 shows some results of a MINITAB run made to investigate this possi-

                 bility, and Figure 4.10 shows a scatterplot of the data and a plot of the fitted cubic
144 Chapter 4 Describing Relationships Between Variables

Example 3     equation. (x values were squared and cubed to provide x, x2, and x3 for each y
(continued )  value to use in the fitting.)

              Printout 3 Cubic Fit to the Fly Ash Data

              Regression Analysis

              The regression equation is
              y = 1188 + 633 x - 214 x**2 + 18.3 x**3

              Predictor       Coef                             StDev              T         P
              Constant    1188.05                              28.79        41.27     0.000
              x                                                55.91        11.32     0.000
              x**2         633.11                              27.79        -7.69     0.000
              x**3        -213.77                              3.649                  0.000
                                                                             5.01
                           18.281

              S = 50.88                               R-Sq = 95.2%       R-Sq(adj) = 94.2%

              Analysis of Variance

              Source                                  DF             SS           MS        F            P
                                                               723197       241066    93.13        0.000
              Regression                              3
                                                                36240          2589
              Residual Error 14                                759437

              Total                                   17

                          Compressive strength (psi)  1800                                  Least squares
                                                      1700                                       cubic
                                                      1600
                                                      1500
                                                      1400
                                                      1300
                                                      1200

                                                            0       1    2  3         4         5

                                                                    Percent ammonium phosphate

                          Figure 4.10 Scatterplot and fitted cubic for the fly ash
                          data
                                           4.2 Fitting Curves and Surfaces by Least Squares 145

                   R2 for the cubic equation is .952, somewhat larger than for the quadratic.
              But it is fairly clear from Figure 4.10 that even a cubic polynomial is not totally
              satisfactory as a summary of these data. In particular, both the fitted quadratic in
              Figure 4.9 and the fitted cubic in Figure 4.10 fail to fit the data adequately near
              an ammonium phosphate level of 2%. Unfortunately, this is where compressive
              strength is greatest--precisely the area of greatest practical interest.

                The example illustrates that R2 is not the only consideration when it comes to
           judging the appropriateness of a fitted polynomial. The examination of plots is also
           important. Not only scatterplots of y versus x with superimposed fitted curves but
           plots of residuals can be helpful. This can be illustrated on a data set where y is
           expected to be nearly perfectly quadratic in x.

Example 4  Analysis of the Bob Drop Data of Section 1.4

           Consider again the experimental determination of the acceleration due to gravity

           (through the dropping of the steel bob) data given in Table 1.4 and reproduced here

           in the first two columns of Table 4.7. Recall that the positions y were recorded

           at  1   sec  intervals  beginning  at  some  unknown  time  t0  (less  than  1   sec)  after
               60                                                                       60
           the bob was released. Since Newtonian mechanics predicts the bob displacement

           to be

                                              displacement = gt2
                                                                   2

           one expects

                             11 2
                        y  g t0 + (x - 1)
                           2       60

                        =  g x2    + g t0 -             1   xg                    12
                                                                      + t0 -                (4.15)
                           2 60                         60  60 2                  60

                        = g x2 + g t0 - 1                        g 12
                                                            x + t0 -
                           7200 60                      60       2         60

           That is, y is expected to be approximately quadratic in x and, indeed, the plot of
           (x, y) points in Figure 1.8 (p. 22) appears to have that character.

                As a slight digression, note that expression () shows that if a quadratic is
           fitted to the data in Table 4.7 via least squares,

                                              y^ = b0 + b1x + b2x2                          (4.16)

           is obtained and an experimentally determined value of g (in mm/sec2) will be
146 Chapter 4 Describing Relationships Between Variables

Example 4     Table 4.7
(continued )  Data, Fitted Values, and Residuals for a Quadratic Fit to the Bob
              Displacement

              x, Point  y, Displacement                      y^ , Fitted  e, Residual
              Number                                      Displacement

              1         .8                                    .95         -.15
                                                             4.56           .24
              2         4.8                                10.89
                                                           19.93          -.09
              3         10.8                               31.70            .17
                                                           46.19            .20
              4         20.1                               63.39
                                                           83.31          -.29
              5         31.9                              105.96          -.09
                                                          131.32          -.21
              6         45.9                              159.40          -.16
                                                          190.21          -.02
              7         63.3                              223.73
                                                          259.97            .10
              8         83.1                              298.93            .29
                                                          340.61            .07
              9         105.8                             385.01            .03
                                                          432.13            .27
              10        131.3                             481.97          -.11
                                                          534.53          -.01
              11        159.5                             589.80            .07
                                                          647.80          -.17
              12        190.5                             708.52          -.33
                                                                            .00
              13        223.8                                             -.10
                                                                            .28
              14        260.0

              15        299.2

              16        340.5

              17        385.0

              18        432.2

              19        481.8

              20        534.2

              21        589.8

              22        647.7

              23        708.8

              7200b2. This is in fact how the value 9.79 m/sec2, quoted in Section 1.4, was
              obtained.

                   A multiple linear regression program fits equation (4.16) to the bob drop data
              giving

                                          y^ = .0645 - .4716x + 1.3597x2

              (from which g  9790 mm/sec2) with R2 that is 1.0 to 6 decimal places. Residuals
              for this fit can be calculated using Definition 4 and are also given in Table 4.7.
              Figure 4.11 is a normal plot of the residuals. It is reasonably linear and thus not
              remarkable (except for some small suggestion that the largest residual or two may
              not be as extreme as might be expected, a circumstance that suggests no obvious
              physical explanation).
Standard normal quantile     4.2 Fitting Curves and Surfaces by Least Squares 147

Residual                 2.0

                         1.0

                           0

                        -1.0

                        -2.0
                               -.3 -.2 -.1 0 .1 .2 .3
                                               Residual quantile

                      Figure 4.11 Normal plot of the residuals from a
                      quadratic fit to the bob drop data

                         .30

                           0

                        -.30
                                       5 10 15 20 25
                                                  Point number, x

                    Figure 4.12 Plot of the residuals from the bob drop
                    quadratic fit vs. x

     However, a plot of residuals versus x (the time variable) is interesting. Fig-
ure 4.12 is such a plot, where successive plotted points have been connected with
line segments. There is at least a hint in Figure 4.12 of a cyclical pattern in the
residuals. Observed displacements are alternately too big, too small, too big, etc.
It would be a good idea to look at several more tapes, to see if a cyclical pattern
appears consistently, before seriously thinking about its origin. But should the
148 Chapter 4 Describing Relationships Between Variables

Example 4     pattern suggested by Figure 4.12 reappear consistently, it would indicate that
(continued )
              something in the mechanism generating the 60 cycle current may cause cycles

              to  be  alternately  slightly  shorter      then  slightly  longer  than  1   sec.  The  practical
                                                                                        60
              implication of this would be that if a better determination of g were desired, the

              regularity of the AC current waveform is one matter to be addressed.

  What if a        Examples 3 and 4 (respectively) illustrate only partial success and then great
polynomial    success in describing an (x, y) data set by means of a polynomial equation. Situations
 doesn't fit  like Example 3 obviously do sometimes occur, and it is reasonable to wonder what
(x, y) data?  to do when they happen. There are two simple things to keep in mind.

                   For one, although a polynomial may be unsatisfactory as a global description
              of a relationship between x and y, it may be quite adequate locally--i.e., for
              a relatively restricted range of x values. For example, in the fly ash study, the
              quadratic representation of compressive strength as a function of percent ammonium
              phosphate is not appropriate over the range 0 to 5%. But having identified the region
              around 2% as being of practical interest, it would make good sense to conduct a
              follow-up study concentrating on (say) 1.5 to 2.5% ammonium phosphate. It is quite
              possible that a quadratic fit only to data with 1.5  x  2.5 would be both adequate
              and helpful as a summarization of the follow-up data.

                   The second observation is that the terms x, x2, x3, . . . , xk in equation (4.12) can
              be replaced by any (known) functions of x and what we have said here will remain
              essentially unchanged. The normal equations will still be k + 1 linear equations
              in 0, 1, . . . , k, and a multiple linear regression program will still produce least
              squares values b0, b1, . . . , bk. This can be quite useful when there are theoretical
              reasons to expect a particular (nonlinear but) simple functional relationship between
              x and y. For example, Taylor's equation for tool life is of the form

                                                          y  x

              for y tool life (e.g., in minutes) and x the cutting speed used (e.g., in sfpm). Taking
              logarithms,

                                             ln(y)  ln() +  ln(x)

              This is an equation for ln(y) that is linear in the parameters ln() and  involving
              the variable ln(x). So, presented with a set of (x, y) data, empirical values for  and
               could be determined by

              1. taking logs of both x's and y's,
              2. fitting the linear version of (4.12), and
              3. identifying ln() with 0 (and thus  with exp(0)) and  with 1.
       4.2 Fitting Curves and Surfaces by Least Squares 149

4.2.2  Surface Fitting by Least Squares

       It is a small step from the idea of fitting a line or a polynomial curve to realizing
       that essentially the same methods can be used to summarize the effects of several
       different quantitative variables x1, x2, . . . , xk on some response y. Geometrically
       the problem is fitting a surface described by an equation

       y  0 + 1x1 + 2x2 + · · · + k xk                         (4.17)

       to the data using the least squares principle. This is pictured for a k = 2 case in
       Figure 4.13, where six (x1, x2, y) data points are pictured in three dimensions, along
       with a possible fitted surface of the form (4.17). To fit a surface defined by equation
       (4.17) to a set of n data points (x1i , x2i , . . . , xki , yi ) via least squares, the function
       of k + 1 variables

       n     n

       S(0, 1, 2, . . . , k) = (yi - y^ i )2 = yi - (0 + 1x1i + · · · + k xki ) 2

       i =1  i =1

       must be minimized by choice of the coefficients 0, 1, . . . , k. Setting partial
       derivatives with respect to the 's equal to 0 gives normal equations generalizing
       equations (4.4) and (4.5). The solution of these k + 1 linear equations in the k + 1
       unknowns 0, 1, . . . , k is the first task of a multiple linear regression program. The
       fitted coefficients b0, b1, . . . , bk that it produces minimize S(0, 1, 2, . . . , k).

             y
                                Possible fitted surface

                                        x2

       x1

       Figure 4.13 Six data points (x1, x2, y) and a possible
       fitted plane
150 Chapter 4 Describing Relationships Between Variables

Example 5  Surface Fitting and Brownlee's Stack Loss Data

           Table 4.8 contains part of a set of data on the operation of a plant for the oxidation
           of ammonia to nitric acid that appeared first in Brownlee's Statistical Theory and
           Methodology in Science and Engineering. In plant operation, the nitric oxides
           produced are absorbed in a countercurrent absorption tower.

                The air flow variable, x1, represents the rate of operation of the plant. The
           acid concentration variable, x3, is the percent circulating minus 50 times 10. The
           response variable, y, is ten times the percentage of ingoing ammonia that escapes
           from the absorption column unabsorbed (i.e., an inverse measure of overall plant
           efficiency). For purposes of understanding, predicting, and possibly ultimately
           optimizing plant performance, it would be useful to have an equation describing
           how y depends on x1, x2, and x3. Surface fitting via least squares is a method of
           developing such an empirical equation.

                Printout 4 shows results from a MINITAB run made to obtain a fitted equation
           of the form

                                         y^ = b0 + b1x1 + b2x2 + b3x3

           Table 4.8
           Brownlee's Stack Loss Data

                 i,        x1i ,               x2i ,                x3i ,         yi ,
           Observation  Air Flow        Cooling Water              Acid      Stack Loss
                                                              Concentration
             Number                    Inlet Temperature                          37
                                                                     88           18
           1            80                                27         87           18
                                                                     87           19
           2            62                                22         93           20
                                                                     93           15
           3            62                                23         87           14
                                                                     80           14
           4            62                                24         89           13
                                                                     88           11
           5            62                                24         82           12
                                                                     93
           6            58                                23         89            8
                                                                     86            7
           7            58                                18         72            8
                                                                     79            8
           8            58                                18         80            9
                                                                     82           15
           9            58                                17

           10           58                                18

           11           58                                19

           12           50                                18

           13           50                                18

           14           50                                19

           15           50                                19

           16           50                                20

           17           56                                20
                                              4.2 Fitting Curves and Surfaces by Least Squares 151

                      The equation produced by the program is

                                            y^ = -37.65 + .80x1 + .58x2 - .07x3           (4.18)

       Interpreting   with R2 = .975. The coefficients in this equation can be thought of as rates of
fitted coefficients   change of stack loss with respect to the individual variables x1, x2, and x3, holding
                      the others fixed. For example, b1 = .80 can be interpreted as the increase in stack
  from a multiple     loss y that accompanies a one-unit increase in air flow x1 if inlet temperature x2
          regression  and acid concentration x3 are held fixed. The signs on the coefficients indicate
                      whether y tends to increase or decrease with increases in the corresponding x. For
             WWW
                      example, the fact that b1 is positive indicates that the higher the rate at which the
                      plant is run, the larger y tends to be (i.e., the less efficiently the plant operates).
                      The large value of R2 is a preliminary indicator that the equation (4.18) is an

                      effective summarization of the data.

                      Printout 4 Multiple Regression for the Stack Loss Data

                      Regression Analysis

                      The regression equation is
                      stack = - 37.7 + 0.798 air + 0.577 water - 0.0671 acid

                      Predictor         Coef         StDev         T         P
                      Constant     -37.652           4.732   -7.96     0.000
                      air          0.79769        0.06744    11.83     0.000
                      water                        0.1660              0.004
                      acid           0.5773       0.06160     3.48     0.296
                                  -0.06706                   -1.09

                      S = 1.253        R-Sq = 97.5%         R-Sq(adj) = 96.9%

                      Analysis of Variance

                      Source           DF               SS         MS         F        P
                                                  795.83     265.28    169.04    0.000
                      Regression       3
                                                   20.40        1.57
                      Residual Error 13           816.24

                      Total            16

                      Source      DF        Seq SS
                      air
                      water       1         775.48
                      acid
                                  1           18.49

                                  1           1.86

                      Unusual Observations

                      Obs         air      stack            Fit StDev Fit      Residual   St Resid
                                                                                  -2.506      -2.23R
                      10         58.0  11.000        13.506    0.552

                      R denotes an observation with a large standardized residual

                           Although the mechanics of fitting equations of the form (4.17) to multivariate
                      data are relatively straightforward, the choice and interpretation of appropriate
                      equations are not so clear-cut. Where many x variables are involved, the number
152 Chapter 4 Describing Relationships Between Variables

The goal of   of potential equations of form (4.17) is huge. To make matters worse, there is no
    multiple
              completely satisfactory way to plot multivariate (x1, x2, . . . , xk, y) data to "see"
 regression   how an equation is fitting. About all that we can do at this point is to (1) offer the

              broad advice that what is wanted is the simplest equation that adequately fits the
              data and then (2) provide examples of how R2 and residual plotting can be helpful

              tools in clearing up the difficulties that arise.

Example 5     In the context of the nitrogen plant, it is sensible to ask whether all three variables,
(continued )  x1, x2, and x3, are required to adequately account for the observed variation in
              y. For example, the behavior of stack loss might be adequately explained using
              only one or two of the three x variables. There would be several consequences
              of practical engineering importance if this were so. For one, in such a case, a
              simple or parsimonious version of equation (4.17) could be used in describing
              the oxidation process. And if a variable is not needed to predict y, then it is
              possible that the expense of measuring it might be saved. Or, if a variable doesn't
              seem to have much impact on y (because it doesn't seem to be essential to include
              it when writing an equation for y), it may be possible to choose its level on purely
              economic grounds, without fear of degrading process performance.

                   As a means of investigating whether indeed some subset of x1, x2, and x3
              is adequate to explain stack loss behavior, R2 values for equations based on all
              possible subsets of x1, x2, and x3 were obtained and placed in Table 4.9. This
              shows, for example, that 95% of the raw variability in y can be accounted for
              using a linear equation in only the air flow variable x1. Use of both x1 and the
              water temperature variable x2 can account for 97.3% of the raw variability in
              stack loss. Inclusion of x3, the acid concentration variable, in an equation already
              involving x1 and x2, increases R2 only from .973 to .975.

                   If identifying a simple equation for stack loss that seems to fit the data well
              is the goal, the message in Table 4.9 would seem to be "Consider an x1 term first,
              and then possibly an x2 term." On the basis of R2, including an x3 term in an
              equation for y seems unnecessary. And in retrospect, this is entirely consistent
              with the character of the fitted equation (4.18): x3 varies from 72 to 93 in the
              original data set, and this means that y^ changes only a total amount

              .07(93 - 72)  1.5

              based on changes in x3. (Remember that .07 = b3 = the fitted rate of change in
              y with respect to x3.) 1.5 is relatively small in comparison to the range in the
              observed y values.

                   Once R2 values have been used to identify potential simplifications of the

              equation

                                            y^ = b0 + b1x1 + b2x2 + b3x3

              these can and should go through thorough residual analyses before they are
              adopted as data summaries. As an example, consider a fitted equation involving
                         4.2 Fitting Curves and Surfaces by Least Squares 153

                         Table 4.9
                         R2's for Equations Predicting Stack Loss

                         Equation Fit                                   R2

                         y  0 + 1x1                                     .950

                         y  0 + 2x2                                     .695

                         y  0 + 3x3                                     .165

                         y  0 + 1x1 + 2x2                               .973

                         y  0 + 1x1 + 3x3                               .952

                         y  0 + 2x2 + 3x3                               .706

                         y  0 + 1x1 + 2x2 + 3x3 .975

                         x1 and x2. A multiple linear regression program can be used to produce the fitted
                         equation

                         y^ = -42.00 - .78x1 + .57x2                          (4.19)

 Dropping variables      (Notice that b0, b1, and b2 in equation (4.19) differ somewhat from the corre-
          from a fitted  sponding values in equation (4.18). That is, equation (4.19) was not obtained

   equation typically    from equation (4.18) by simply dropping the last term in the equation. In general,
changes coefficients
                         the values of the coefficients b will change depending on which x variables are

                         and are not included in the fitting.)

                              Residuals for equation (4.19) can be computed and plotted in any number

                         of potentially useful ways. Figure 4.14 shows a normal plot of the residuals and
                         three other plots of the residuals against, respectively, x1, x2, and y^ . There are
                         no really strong messages carried by the plots in Figure 4.14 except that the
                         data set contains one unusually large x1 value and one unusually large y^ (which
                         corresponds to the large x1). But there is enough of a curvilinear "up-then-down-
                         then-back-up-again" pattern in the plot of residuals against x1 to suggest the
                         possibility of adding an x12 term to the fitted equation (4.19).

                              You might want to verify that fitting the equation

                         y  0 + 1x1 + 2x2 + 3x12

                         to the data of Table 4.8 yields approximately

                         y^ = -15.409 - .069x1 + .528x2 + .007x12             (4.20)

                         with corresponding R2 = .980 and residuals that show even less of a pattern than

                         those for the fitted equation (4.19). In particular, the hint of curvature on the plot

                         of residuals versus x1 for equation (4.19) is not present in the corresponding plot
                         for equation (4.20). Interestingly, looking back over this example, one sees that
                         fitted equation (4.20) has a better R2 value than even fitted equation (4.18), in
154 Chapter 4 Describing Relationships Between Variables

Standard normal quantile   2.0                                          Residual   2.0
                           1.0                                                     1.0 2

                             0                                                             2
                          -1.0                                                       0
                          -2.0                                                    -1.0
                                                                                  -2.0
                                    -2.0 -1.0 0 1.0 2.0
                                          Residual quantile                       50                  60  70                80

                                                                                                      Air flow, x1

Residual                   2.0                                          Residual   2.0
                           1.0                                                     1.0

                                       2                                                           2
                                          2                                                2
                                                                                     0
                             0                                                    -1.0
                          -1.0                                                    -2.0
                          -2.0

                                             20  25                 30            10                  20            30

                                             Inlet temperature, x2                                    Fitted Stack Loss, y

                          Figure 4.14 Plots of residuals from a two-variable equation fit to the stack loss data
                          ( y^ = -42.00 - .78x1 + .57x2)

                          Example 5              spite of the fact that equation (4.18) involves the process variable x3 and equation
                          (continued )           (4.20) does not.

                                                      Equation (4.20) is somewhat more complicated than equation (4.19). But

                                                 because it still really only involves two different input x's and also eliminates the

                                                 slight pattern seen on the plot of residuals for equation (4.19) versus x1, it seems
                                                 an attractive choice for summarizing the stack loss data. A two-dimensional rep-

                                                 resentation of the fitted surface defined by equation (4.20) is given in Figure 4.15.
                                                 The slight curvature on the plotted curves is a result of the x12 term appearing in
                                                 equation (4.20). Since most of the data have x1 from 50 to 62 and x2 from 17 to
                                                 24, the curves carry the message that over these ranges, changes in x1 seem to
                                                 produce larger changes in stack loss than do changes in x2. This conclusion is
                                                 consistent with the discussion centered around Table 4.9.
                                            4.2 Fitting Curves and Surfaces by Least Squares 155

                      Fitted stack loss, y  35                            x2 = 16
                                            30                    x2 = 20
                                            25          x2 = 24
                                            20 x2 = 28
                                            15
                                            10

                           50 55 60 65 70 75
                                                   Air flow, x1

                      Figure 4.15 Plots of fitted stack loss from equation
                      (4.20)

Common residual       The plots of residuals used in Example 5 are typical. They are
 plots in multiple
          regression  1. normal plots of residuals,
                      2. plots of residuals against all x variables,
                      3. plots of residuals against y^ ,
                      4. plots of residuals against time order of observation, and
                      5. plots of residuals against variables (like machine number or operator) not

                          used in the fitted equation but potentially of importance.

                      All of these can be used to help assess the appropriateness of surfaces fit to multivari-
                      ate data, and they all have the potential to tell an engineer something not previously
                      discovered about a set of data and the process that generated them.

                           Earlier in this section, there was a discussion of the fact that an "x term" in
                      the equations fitted via least squares can be a known function (e.g., a logarithm)
                      of a basic process variable. In fact, it is frequently helpful to allow an "x term" in
                      equation (4.17) (page 149) to be a known function of several basic process variables.
                      The next example illustrates this point.
156 Chapter 4 Describing Relationships Between Variables

Example 6  Lift/Drag Ratio for a Three-Surface Configuration

           P. Burris studied the effects of the positions relative to the wing of a canard (a
           forward lifting surface) and tail on the lift/drag ratio for a three-surface configu-
           ration. Part of his data are given in Table 4.10, where

           x1 = canard placement in inches above the plane defined by the main wing
           x2 = tail placement in inches above the plane defined by the main wing

           (The front-to-rear positions of the three surfaces were constant throughout the
           study.)

                A straightforward least squares fitting of the equation

                                              y  0 + 1x1 + 2x2

           to these data produces R2 of only .394. Even the addition of squared terms in
           both x1 and x2, i.e., the fitting of

                                    y  0 + 1x1 + 2x2 + 3x12 + 4x22

           produces an increase in R2 to only .513. However, Printout 5 shows that fitting
           the equation

                                       y  0 + 1x1 + 2x2 + 3x1x2

           yields R2 = .641 and the fitted relationship

           y^ = 3.4284 + .5361x1 + .3201x2 - .5042x1x2                            (4.21)

           Table 4.10
           Lift/Drag Ratios for 9 Canard/Tail Position Combinations

                   x1,            x2,                                    y,
           Canard Position  Tail Position                        Lift/Drag Ratio

           -1.2                                           -1.2    .858
           -1.2                                             0.0  3.156
           -1.2                                             1.2  3.644
                                                                 4.281
             0.0                                          -1.2   3.481
             0.0                                            0.0  3.918
             0.0                                            1.2  4.136
             1.2                                                 3.364
             1.2                                          -1.2   4.018
             1.2                                            0.0
                                                            1.2
                                             4.2 Fitting Curves and Surfaces by Least Squares 157

Printout 5 Multiple Regression for the Lift/Drag Ratio Data

Regression Analysis

The regression equation is
y = 3.43 + 0.536 x1 + 0.320 x2 - 0.504 x1*x2

Predictor       Coef                                 StDev        T              P
Constant     3.4284                                 0.2613  13.12          0.000
x1           0.5361                                 0.2667                 0.101
x2           0.3201                                 0.2667   2.01          0.284
x1*x2       -0.5042                                 0.2722   1.20          0.123
                                                            -1.85

S = 0.7839      R-Sq = 64.1%                                R-Sq(adj) = 42.5%

Analysis of Variance

Source          DF                                      SS        MS           F          P
                                                  5.4771    1.8257         2.97     0.136
Regression      3                                 3.0724    0.6145
                                                  8.5495
Residual Error  5

Total           8

(After reading x1, x2, and y values from Table 4.10 into columns of MINITAB's
worksheet, x1x2 products were created and y fitted to the three predictor variables
x1, x2, and x1x2 in order to create this printout.)

     Figure 4.16 shows the nature of the fitted surface (4.21). Raising the canard

(increasing x1) has noticeably different predicted impacts on y, depending on the
value of x2 (the tail position). (It appears that the canard and tail should not be
lined up--i.e., x1 should not be near x2. For large predicted response, one wants
small x1 for large x2 and large x1 for small x2.) It is the cross-product term x1x2
in relationship (4.21) that allows the response curves to have different characters
for different x2 values. Without it, the slices of the fitted (x1, x2, y^ ) surface would
be parallel for various x2, much like the situation in Figure 4.15.

                Fitted lift / Drag ratio, y  4.0            x2 = 1.2

                                             3.0
                                                       x2 = 0

                                                                x2 = -1.2
                                             2.0

                                             1.0            0                  1.2
                                              -1.2

                                                    Canard position, x1

                Figure 4.16 Plots of fitted lift/drag from
                equation (4.21)
158 Chapter 4 Describing Relationships Between Variables

Example 6           Although the main new point of this example has by now been made, it
(continued )   probably should be mentioned that equation (4.21) is not the last word for fitting
               the data of Table 4.10. Figure 4.17 gives a plot of the residuals for relationship
               (4.21) versus canard position x1, and it shows a strong curvilinear pattern. In fact,
               the fitted equation

               y^ = 3.9833 + .5361x1 + .3201x2 - .4843x12 - .5042x1x2               (4.22)

               provides R2 = .754 and generally random-looking residuals. It can be verified
               by plotting y^ versus x1 curves for several x2 values that the fitted relationship
               (4.22) yields nonparallel parabolic slices of the fitted (x1, x2, y^ ) surface, instead
               of the nonparallel linear slices seen in Figure 4.16.

                         1.0

               Residual  0

                         -1.0

                               -1.2                       0                    1.2

                                                          Canard position, x1

               Figure 4.17 Plot of residuals from equation
               (4.21) vs. x1

        4.2.3  Some Additional Cautions

Extrapolation  Least squares fitting of curves and surfaces is of substantial engineering impor-
               tance--but it must be handled with care and thought. Before leaving the subject
               until Chapter 9, which explains methods of formal inference associated with it, a
               few more warnings must be given.

                    First, it is necessary to warn of the dangers of extrapolation substantially outside
               the "range" of the (x1, x2, . . . , xk, y) data. It is sensible to count on a fitted equation
               to describe the relation of y to a particular set of inputs x1, x2, . . . , xk only if they
               are like the sets used to create the equation. The challenge surface fitting affords is
                         4.2 Fitting Curves and Surfaces by Least Squares 159

                     x2  Dots show (x1, x2) locations
                         of fictitious data points
                20
                15              The region
                10
                 5       ×      with 1  x1  5

                                and 10  x2  20

                         (3,15) is unlike the
                         (x1, x2) pairs for the data

                         12345                         x1

                Figure 4.18 Hypothetical plot of (x1, x2) pairs

The influence   that when several different x variables are involved, it is difficult to tell whether a
   of outlying  particular (x1, x2, . . . , xk) vector is a large extrapolation. About all one can do is
  data vectors  check to see that it comes close to matching some single data point in the set on
                each coordinate x1, x2, . . . , xk. It is not sufficient that there be some point with x1
                value near the one of interest, another point with x2 value near the one of interest,
                etc. For example, having data with 1 x1  5 and 10 x2  20 doesn't mean that
                the (x1, x2) pair (3, 15) is necessarily like any of the pairs in the data set. This fact
                is illustrated in Figure 4.18 for a fictitious set of (x1, x2) values.

                     Another potential pitfall is that the fitting of curves and surfaces via least squares
                can be strongly affected by a few outlying or extreme data points. One can try to
                identify such points by examining plots and comparing fits made with and without
                the suspicious point(s).

Example 5       Figure 4.14 earlier called attention to the fact that the nitrogen plant data set
(continued )    contains one point with an extreme x1 value. Figure 4.19 is a scatterplot of
                (x1, x2) pairs for the data in Table 4.8 (page 150). It shows that by most qualitative
                standards, observation 1 in Table 4.8 is unusual or outlying.

                     If the fitting of equation (4.20) is redone using only the last 16 data points in
                Table 4.8, the equation

                y^ = -56.797 + 1.404x1 + .601x2 - .007x12        (4.23)

                and R2 = .942 are obtained. Using equation (4.23) as a description of stack loss
                and limiting attention to x1 in the range 50 to 62 could be considered. But it
                is possible to verify that though some of the coefficients (the b's) in equations
                (4.20) and (4.23) differ substantially, the two equations produce comparable y^
                values for the 16 data points with x1 between 50 and 62. In fact, the largest
                difference in fitted values is about .4. So, since point 1 in Table 4.8 doesn't
160 Chapter 4 Describing Relationships Between Variables

Example 5          Water temperature, x2  25 2
(continued )

                                          20 2
                                              2       3

                                          15

                                              50  55     60  65            70  75  80

                                                             Air flow, x1

                   Figure 4.19 Plot of (x1, x2) pairs for the stack loss data

                   radically change predictions made using the fitted equation, it makes sense to
                   leave it in consideration, adopt equation (4.20), and use it to describe stack loss
                   for (x1, x2) pairs interior to the pattern of scatter in Figure 4.19.

Replication and         A third warning has to do with the notion of replication (first discussed in
  surface fitting  Section 2.3). It is the fact that the fly ash data of Example 3 has several y's for
                   each x that makes it so clear that even the quadratic and cubic curves sketched
  The possibility  in Figures 4.9 and 4.10 are inadequate descriptions of the relationship between
   of overfitting  phosphate and strength. The fitted curves pass clearly outside the range of what look
                   like believable values of y for some values of x. Without such replication, what is
                   permissible variation about a fitted curve or surface can't be known with confidence.
                   For example, the structure of the lift/drag data set in Example 6 is weak from this
                   viewpoint. There is no replication represented in Table 4.10, so an external value for
                   typical experimental precision would be needed in order to identify a fitted value as
                   obviously incompatible with an observed one.

                        The nitrogen plant data set of Example 5 was presumably derived from a
                   primarily observational study, where no conscious attempt was made to replicate
                   (x1, x2, x3) settings. However, points number 4 and 5 in Table 4.8 (page 150) do
                   represent the replication of a single (x1, x2, x3) combination and show a difference
                   in observed stack loss of 1. And this makes the residuals for equation (4.20) (which
                   range from -2.0 to 2.3) seem at least not obviously out of line.

                        Section 9.2 discusses more formal and precise ways of using data from studies
                   with some replication to judge whether or not a fitted curve or surface misses some
                   observed y's too badly. For now, simply note that among replication's many virtues
                   is the fact that it allows more reliable judgments about the appropriateness of a fitted
                   equation than are otherwise possible.

                        The fourth caution is that the notion of equation simplicity ( parsimony) is
                   important for reasons in addition to simplicity of interpretation and reduced expense
                   involved in using the equation. It is also important from the point of view of typically
                   giving smooth interpolation and not overfitting a data set. As a hypothetical example,
                                                         4.2 Fitting Curves and Surfaces by Least Squares 161

                                                          y

                                                                   x

                  Figure 4.20 Scatterplot of 11 pairs
                  (x, y)

Empirical models  consider the artificial, generally linear (x, y) data plotted in Figure 4.20. It would be
and engineering   possible to run a (wiggly) k = 10 version of the polynomial (4.12) through each of
                  these points. But in most physical problems, such a curve would do a much worse
                  job of predicting y at values of x not represented by a data point than would a simple
                  fitted line. A tenth-order polynomial would overfit the data in hand.

                       As a final point in this section, consider how the methods discussed here fit
                  into the broad picture of using models for attacking engineering problems. It must
                  be said that physical theories of physics, chemistry, materials, etc. rarely produce
                  equations of the forms (4.12) or (4.17). Sometimes pertinent equations from those
                  theories can be rewritten in such forms, as was possible with Taylor's equation for
                  tool life earlier in this section. But the majority of engineering applications of the
                  methods in this section are to the large number of problems where no commonly
                  known and simple physical theory is available, and a simple empirical description
                  of the situation would be helpful. In such cases, the tool of least squares fitting of
                  curves and surfaces can function as a kind of "mathematical French curve," allowing
                  an engineer to develop approximate empirical descriptions of how a response y is
                  related to system inputs x1, x2, . . . , xk.

Section 2 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Return to Exercise 3 of Section 4.1. Fit a quadratic  2. Here are some data taken from the article "Chemi-
   relationship y  0 + 1x + 2x2 to the data via             thermomechanical Pulp from Mixed High Den-
   least squares. By appropriately plotting residuals       sity Hardwoods" by Miller, Shankar, and Peterson
   and examining R2 values, determine the advis-            (Tappi Journal, 1988). Given are the percent NaOH
                                                            used as a pretreatment chemical, x1, the pretreat-
   ability of using a quadratic rather than a linear        ment time in minutes, x2, and the resulting value
                                                            of a specific surface area variable, y (with units of
   equation to describe the relationship between x          cm3/g), for nine batches of pulp produced from a
                                                            mixture of hardwoods at a treatment temperature
   and y. If a quadratic fitted equation is used, how       of 75C in mechanical pulping.
   does the predicted mean molecular weight at 200C

   compare to that obtained in part (e) of the earlier

   exercise?
162 Chapter 4 Describing Relationships Between Variables

% NaOH, x1  Time, x2  Specific Surface Area, y           (d) What specific surface area would you predict
                                                              for an additional batch of pulp of this type
      3.0      30                 5.95                        produced using a 10% NaOH treatment for a
      3.0      60                 5.60                        time of 70 minutes? Would you be willing to
      3.0      90                 5.44                        make a similar prediction for 10% NaOH used
      9.0      30                 6.22                        for 120 minutes based on your fitted equation?
      9.0      60                 5.85                        Why or why not?
      9.0      90                 5.61
     15.0      30                 8.36                   (e) There are many other possible approximate re-
     15.0      60                 7.30                        lationships that might be fitted to these data via
     15.0      90                 6.43                        least squares, one of which is y  0 + 1x1 +
                                                              2x2 + 3x1x2. Fit this equation to the preced-
(a) Fit the approximate relationship y  0 +                   ing data and compare the resulting coefficient
     1x1 + 2x2 to these data via least squares.               of determination to the one found in (a). On the
     Interpret the coefficients b1 and b2 in the fit-         basis of these alone, does the use of the more
     ted equation. What fraction of the observed              complicated equation seem necessary?
     raw variation in y is accounted for using this
     equation?                                           (f) For the equation fit in part (e), repeat the steps
                                                              of part (c) and compare the plot made here to
(b) Compute and plot residuals for your fitted                the one made earlier.
     equation from (a). Discuss what these plots
     indicate about the adequacy of your fitted equa-    (g) What is an intrinsic weakness of this real pub-
     tion. (At a minimum, you should plot residuals           lished data set?
     against all of x1, x2, and y^ and normal-plot the
     residuals.)                                         (h) What terminology (for data structures) intro-
                                                              duced in Section 1.2 describes this data set? It
(c) Make a plot of y versus x1 for the nine data              turns out that since the data set has this special
     points and sketch on that plot the three different       structure and all nine sample sizes are the same
     linear functions of x1 produced by setting x2            (i.e., are all 1), some special relationships hold
     first at 30, then 60, and then 90 in your fitted         between the equation fit in (a) and what you get
     equation from (a). How well do fitted responses          by separately fitting linear equations in x1 and
     appear to match observed responses?                      then in x2 to the y data. Fit such one-variable
                                                              linear equations and compare coefficients and
                                                              R2 values to what you obtained in (a). What
                                                              relationships exist between these?

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

4.3 Fitted Effects for Factorial Data

                    The previous two sections have centered on the least squares fitting of equations
                    to data sets where a quantitative response y is presumed to depend on the lev-
                    els x1, x2, . . . , xk of quantitative factors. In many engineering applications, at least
                    some of the system "knobs" whose effects must be assessed are basically qualitative
                    rather than quantitative. When a data set has complete factorial structure (review the
                    meaning of this terminology in Section 1.2), it is still possible to describe it in terms
                    of an equation. This equation involves so-called fitted factorial effects. Sometimes,
                    when a few of these fitted effects dominate the rest, a parsimonious version of this
                                                                   4.3 Fitted Effects for Factorial Data 163

                 equation can adequately describe the data and have intuitively appealing and under-
                 standable interpretations. The use of simple plots and residuals will be discussed,
                 as tools helpful in assessing whether such a simple structure holds.

                      The discussion begins with the 2-factor case, then considers three (or, by anal-
                 ogy, more) factors. Finally, the special case where each factor has only two levels is
                 discussed.

4.3.1            Fitted Effects for 2-Factor Studies

                 Example 9 of Chapter 3 (page 101) illustrated how informative a plot of sample
                 means versus levels of one of the factors can be in a 2-factor study. Such plotting
                 is always the place to begin in understanding the story carried by two-way factorial
                 data. In addition, it is helpful to calculate the factor level (marginal) averages of the
                 sample means and the grand average of the sample means. For factor A having I
                 levels and factor B having J levels, the following notation will be used:

  Notation for   y¯ i j = the sample mean response when factor A is at
sample means            level i and factor B is at level j

      and their         1J
       averages  y¯ i. = J y¯ i j

                                j =1

                 = the average sample mean when factor A is at level i

                        1I
                 y¯ . j = I y¯ i j

                                i =1

                     = the average sample mean when factor B is at level j

                 y¯ .. =  1         y¯ i j

                          IJ  i, j

                 = the grand average sample mean

                 The y¯ i. and y¯ . j are row and column averages when one thinks of the y¯ i j laid out in
                 a two-dimensional format, as shown in Figure 4.21.

Example 7        Joint Strengths for Three Different Joint Types in Three Different Woods

                 Kotlers, MacFarland, and Tomlinson studied the tensile strength of three differ-
                 ent types of joints made on three different types of wood. Butt, lap, and beveled
                 joints were made in nominal 1 × 4 × 12 pine, oak, and walnut specimens
                 using a resin glue. The original intention was to test two specimens of each Joint
                 Type/Wood Type combination. But one operator error and one specimen failure
                 not related to its joint removed two of the original data points from consideration
                 and gave the data in Table 4.11. These data have complete 3 × 3 factorial struc-
164 Chapter 4 Describing Relationships Between Variables

                                                                  Factor B

                        Level 1 Level 2                                     Level J

                  Level 1 y11                             y12               y1J      y1.

                  Level 2 y21                             y22               y2J      y2.

              Factor A

                  Level I yI1                             yI2               yIJ      yI.

                             y.1                          y.2               y. J     y..

              Figure 4.21 Cell sample means and row, column, and
              grand average sample means for a two-way factorial

Example 7     Table 4.11
(continued )  Measured Strengths of 16 Wood Joints

              Specimen Joint                              Wood y, Stress at Failure (psi)

              1         beveled oak                                         1518
                                                                             829
              2         butt                              pine              2571
                                                                            1169
              3         beveled walnut                                      1927
                                                                            1348
              4         butt                              oak               1489
                                                                            2443
              5         beveled oak                                         1263
                                                                            1295
              6         beveled pine                                        1561
                                                                            1000
              7         lap                               walnut             596
                                                                             859
              8         beveled walnut                                      1029
                                                                            1207
              9         butt                              walnut

              10        lap                               oak

              11        lap                               oak

              12        lap                               pine

              13        butt                              pine

              14        lap                               pine

              15        butt                              walnut

              16        beveled pine
                                                                                          4.3 Fitted Effects for Factorial Data 165

Table 4.12
Sample Means for Nine Wood/Joint Combinations

                                                                             Wood

Joint  1 (Butt)          1 (Pine)                                          2 (Oak)          3 (Walnut)     y¯ 1. = 1009.17
       2 (Beveled)                                                                                         y¯ 2. = 1835.67
       3 (Lap)        y¯ 11 = 712.5                                    y¯ 12 = 1169.0     y¯ 13 = 1146.0   y¯ 3. = 1282.17
                      y¯ 21 = 1277.5                                   y¯ 22 = 1722.5     y¯ 23 = 2507.0
                      y¯ 31 = 929.5                                    y¯ 32 = 1428.0     y¯ 33 = 1489.0   y¯ .. = 1375.67

                      y¯ .1 = 973.17                                   y¯ .2 = 1439.83    y¯ .3 = 1714.00

       Interaction    ture. Collecting y's for the nine different combinations into separate samples and
                Plot  calculating means, the y¯ i j 's are as presented in tabular form in Table 4.12 and
                      plotted in Figure 4.22. This figure is a so-called interaction plot of these means.

                           The qualitative messages given by the plot are as follows:

                           1. Joint types ordered by strength are "beveled is stronger than lap, which
                               in turn is stronger than butt."

                                                                       2400

                                                                       2200

                                      Mean stress at failure, y (psi)  2000

                                                                       1800

                                                                       1600 Beveled Lap
                                                                       1400

                                                                       1200                     Butt

                                                                       1000

                                                                       800

                                                                       600                 Oak             Walnut
                                                                                    Pine  Wood

                                      Figure 4.22 Interaction plot of joint strength sample
                                      means
166 Chapter 4 Describing Relationships Between Variables

Example 7     2. Woods ordered by overall strength seem to be "walnut is stronger than
(continued )      oak, which in turn is stronger than pine."

              3. The strength pattern across woods is not consistent from joint type to joint
                  type (or equivalently, the strength pattern across joints is not consistent
                  from wood type to wood type).

              The idea of fitted effects is to invent a way of quantifying such qualitative
              summaries.

                   The row and column average means (y¯i·'s and y¯· j 's, respectively) might be
              taken as measures of average response behavior at different levels of the factors in

              question. If so, it then makes sense to use the differences between these and the
              grand average mean y¯.. as measures of the effects of those levels on mean response.
              This leads to Definition 5.

Definition 5  In a two-way complete factorial study with factors A and B, the fitted main
              effect of factor A at its ith level is

                                                    ai = y¯ i. - y¯ ..

              Similarly, the fitted main effect of factor B at its jth level is

                                                    bj = y¯ . j - y¯ ..

Example 7     Simple arithmetic and the y¯ 's in Table 4.12 yield the fitted main effects for the
(continued )  joint strength study of Kotlers, MacFarland, and Tomlinson. First for factor A
              (the Joint Type),

                            a1 = the Joint Type fitted main effect for butt joints
                                = 1009.17 - 1375.67
                                = -366.5 psi

                            a2 = the Joint Type fitted main effect for beveled joints
                                = 1835.67 - 1375.67
                                = 460.0 psi

                            a3 = the Joint Type fitted main effect for lap joints
                                = 1282.17 - 1375.67
                                = -93.5 psi
              4.3 Fitted Effects for Factorial Data 167

              Similarly for factor B (the Wood Type),

              b1 = the Wood Type fitted main effect for pine
                 = 973.17 - 1375.67
                 = -402.5 psi

              b2 = the Wood Type fitted main effect for oak
                 = 1439.83 - 1375.67
                 = 64.17 psi

              b3 = the Wood Type fitted main effect for walnut
                 = 1714.00 - 1375.67
                 = 338.33 psi

              These fitted main effects quantify the first two qualitative messages carried by
              the data and listed as (1) and (2) before Definition 5. For example,

                                                      a2 > a3 > a1

              says that beveled joints are strongest and butt joints the weakest. Further, the fact
              that the ai 's and bj 's are of roughly the same order of magnitude says that the
              Joint Type and Wood Type factors are of comparable importance in determining
              tensile strength.

                   A difference between fitted main effects for a factor amounts to a difference be-
              tween corresponding row or column averages and quantifies how different response
              behavior is for those two levels.

Example 7     For example, comparing pine and oak wood types,
(continued )
                                         b1 - b2 = (y¯ .1 - y¯ ..) - (y¯ .2 - y¯ ..)
                                                   = y¯ .1 - y¯ .2
                                                   = 973.17 - 1439.83
                                                   = -466.67 psi

              which indicates that pine joint average strength is about 467 psi less than oak
              joint average strength.
168 Chapter 4 Describing Relationships Between Variables

                   In some two-factor factorial studies, the fitted main effects as defined in Defini-
              tion 5 pretty much summarize the story told by the means y¯ i j , in the sense that

                                       y¯ i j  y¯ .. + ai + bj for every i and j                           (4.24)

              Display (4.24) implies, for example, that the pattern of mean responses for level 1

              of factor A is the same as for level 2 of A. That is, changing levels of factor B (from

              say j to j ) produces the same change in mean response for level 2 as for level 1
              (namely, bj - bj ). In fact, if relation (4.24) holds, there are parallel traces on an
              interaction plot of means.

Example 7     To illustrate the meaning of expression (4.24), the fitted effects for the Joint
(continued )
              Type/Wood Type data have been used to calculate 3 × 3 = 9 values of y¯ .. +
              ai + bj corresponding to the nine experimental combinations. These are given in
              Table 4.13.

                   For comparison purposes, the y¯ i j from Table 4.12 and the y¯ .. + ai + bj from
              Table 4.13 are plotted on the same sets of axes in Figure 4.23. Notice the parallel

              traces for the y¯ .. + ai + bj values for the three different joint types. The traces for

                                       2400               yij                     Beveled
                                       2200               y.. + ai + bj

                                       2000

              Stress at failure (psi)  1800

                                       1600                                       Lap

                                       1400
                                                                                                     Butt

                                       1200

                                       1000

                                       800

                                       600                 Oak           Walnut
                                                    Pine  Wood

              Figure 4.23 Plots of y¯ij and y¯.. + ai + bj vs. wood type for
              three joint types
                                                           4.3 Fitted Effects for Factorial Data 169

                    Table 4.13
                    Values of y¯.. + ai + bj for the Joint Strength Study

                                                                           Wood

                    Joint  1 (Butt)     1 (Pine)           2 (Oak)               3 (Walnut)
                           2 (Beveled)
                           3 (Lap)      y¯ .. + a1 + b1 =  y¯ .. + a1 + b2 =     y¯ .. + a1 + b3 =
                                        606.67             1073.33               1347.50
                                        y¯ .. + a2 + b1 =  y¯ .. + a2 + b2 =     y¯ .. + a2 + b3 =
                                        1433.17            1899.83               2174.00
                                        y¯ .. + a3 + b1 =  y¯ .. + a3 + b2 =     y¯ .. + a3 + b3 =
                                        879.67             1346.33               1620.50

                       the y¯ i j values for the three different joint types are not parallel (particularly when
                       walnut is considered), so there are apparently substantial differences between the
                       y¯ i j 's and the y¯ .. + ai + bj 's.

                         When relationship (4.24) fails to hold, the patterns in mean response across
                    levels of one factor depend on the levels of the second factor. In such cases, the
                    differences between the combination means y¯ i j and the values y¯ .. + ai + bj can
                    serve as useful measures of lack of parallelism on the plots of means, and this leads
                    to another definition.

Definition 6        In a two-way complete factorial study with factors A and B, the fitted inter-
                    action of factor A at its ith level and factor B at its jth level is

                                                 abi j = y¯ i j - (y¯ .. + ai + bj )

Interpretation of        The fitted interactions in some sense measure how much pattern the combination
 interactions in a  means y¯ i j carry that is not explainable in terms of the factors A and B acting
                    separately. Clearly, when relationship (4.24) holds, the fitted interactions abi j are all
           two-way  small (nearly 0), and system behavior can be thought of as depending separately on
   factorial study  level of A and level of B. In such cases, an important practical consequence is that it
                    is possible to develop recommendations for levels of the two factors independently
                    of each other. For example, one need not recommend one level of A if B is at its
                    level 1 and another if B is at its level 2.

                         Consider a study of the effects of factors Tool Type and Turning Speed on the
                    metal removal rate for a lathe. If the fitted interactions are small, turning speed
                    recommendations that remain valid for all tool types can be made. However, if
                    the fitted interactions are important, turning speed recommendations might vary
                    according to tool type.
170 Chapter 4 Describing Relationships Between Variables

Example 7       Again using the Joint Type/Wood Type data, consider calculating the fitted in-
(continued )    teractions. The raw material for these calculations already exists in Tables 4.12
                and 4.13. Simply taking differences between entries in these tables cell-by-cell
                yields the fitted interactions given in Table 4.14.

                     It is interesting to compare these fitted interactions to themselves and to
                the fitted main effects. The largest (in absolute value) fitted interaction (ab23)
                corresponds to beveled walnut joints. This is consistent with one visual message
                in Figures 4.22 and 4.23: This Joint Type/Wood Type combination is in some
                sense most responsible for destroying any nearly parallel structure that might
                otherwise appear. The fact that (on the whole) the abi j 's are not as large as the
                ai 's or bj 's is consistent with a second visual message in Figures 4.22 and 4.23:
                The lack of parallelism, while important, is not as important as differences in
                Joint Types or Wood Types.

                Table 4.14
                Fitted Interactions for the Joint Strength Study

                                                                  Wood

                                     1 (Pine)                     2 (Oak)         3 (Walnut)

                Joint   1 (Butt)     ab11 = 105.83                ab12 = 95.67    ab13 = -201.5
                        2 (Beveled)  ab21 = -155.66               ab22 = -177.33  ab23 = 333.0
                        3 (Lap)      ab31 = 49.83                 ab32 = 81.67    ab33 = -131.5

Fitted effects       Example 7 has proceeded "by hand." But using a statistical package can make
 sum to zero    the calculations painless. For example, Printout 6 illustrates that most of the results
                of Example 7 are readily available in MINITAB's "General Linear Model" routine
                (found under the "Stat/ANOVA/General Linear Model" menu). Comparing this
                printout to the example does bring up one point regarding the fitted effects defined
                in Definitions 5 and 6. Note that the printout provides values of only two (of three)
                Joint main effects, two (of three) Wood main effects, and four (of nine) Joint × Wood
                interactions. These are all that are needed, since it is a consequence of Definition 5
                that fitted main effects for a given factor must total to 0, and it is a consequence of
                Definition 6 that fitted interactions must sum to zero across any row or down any
                column of the two-way table of factor combinations. The fitted effects not provided
                by the printout are easily deduced from the ones that are given.

WWW             Printout 6 Computations for the Joint Strength Data

                General Linear Model

                Factor  Type Levels Values
                joint
                wood    fixed  3 beveled butt lap

                        fixed  3     oak pine walnut
                                                 4.3 Fitted Effects for Factorial Data 171

       Analysis of Variance for strength, using Adjusted SS for Tests

       Source         DF   Seq SS     Adj SS     Adj MS               F         P
                          2153879    1881650     940825         32.67     0.000
       joint          2   1641095    1481377     740689         25.72     0.001
                                                 117102                   0.052
       wood           2    468408     468408                     4.07
                           201614     201614      28802
       joint*wood 4       4464996

       Error          7

       Total          15

       Term                   Coef   StDev       T           P
       Constant           1375.67    44.22
                                                 31.11 0.000
          joint            460.00    59.63
       beveled            -366.50    63.95        7.71 0.000
       butt                                      -5.73 0.001
                             64.17   63.95
          wood            -402.50    59.63        1.00 0.349
       oak                                       -6.75 0.000
       pine               -177.33    85.38
                          -155.67    82.20       -2.08  0.076
          joint* wood                97.07       -1.89  0.100
       beveled oak           95.67   85.38              0.357
       beveled pine        105.83                 0.99  0.255
       butt oak                                   1.24
       butt pine

       Unusual Observations for strength

       Obs strength             Fit  StDev Fit   Residual    St Resid
          4 1169.00       1169.00        169.71        0.00           *X
          7 1489.00       1489.00        169.71        0.00           *X

       X denotes an observation whose X value gives it large influence.

       Least Squares Means for strength

       joint                 Mean     StDev
                          1835.7      69.28
       beveled            1009.2      80.00
                          1282.2      80.00
       butt
                          1439.8      80.00
       lap                 973.2      69.28
                                      80.00
       wood               1714.0
                                     120.00
       oak                1722.5     120.00
                          1277.5     120.00
       pine               2507.0     169.71
                          1169.0     120.00
       walnut                        120.00
                           712.5     120.00
       joint* wood        1146.0     120.00
                          1428.0     169.71
       beveled oak
                           929.5
       beveled pine       1489.0

       beveled walnut

       butt oak

       butt pine

       butt walnut

       lap      oak

       lap      pine

       lap      walnut

4.3.2  Simpler Descriptions for Some Two-Way Data Sets
       Rewriting the equation for abi j from Definition 6,

                                     y¯i j = y¯.. + ai + bj + abi j                (4.25)
172 Chapter 4 Describing Relationships Between Variables

That is, y¯.., the fitted main effects, and the fitted interactions provide a decomposition
or breakdown of the combination sample means into interpretable pieces. These
pieces correspond to an overall effect, the effects of factors acting separately, and
the effects of factors acting jointly.

     Taking a hint from the equation fitting done in the previous two sections, it
makes sense to think of (4.25) as a fitted version of an approximate relationship,

y  µ + i + j + i j                                                       (4.26)

where µ, 1, 2, . . . , I , 1, 2, . . . , J , 11, . . ., 1J , 21, . . . , IJ are some
constants and the levels of factors A and B associated with a particular response y

pick out which of the i 's, j 's, and i j 's are appropriate in equation (4.26). By
analogy with the previous two sections, the possibility should be considered that

a relationship even simpler than equation (4.26) might hold, perhaps not involving

i j 's or even i 's or perhaps j 's.
     It has already been said that when relationship (4.24) is in force, or equivalently

                               abi j  0 for every i and j

it is possible to understand an observed set of y¯ i j 's in simplified terms of the factors
acting separately. This possibility corresponds to the simplified version of equation
(4.26),

y  µ + i + j

and there are other simplified versions of equation (4.26) that also have appealing
interpretations. For example, the simplified version of equation (4.26),

                                                          y  µ + i

says that only factor A (not factor B) is important in determining response y.
(1, 2, . . . , I still allow for different response behavior for different levels of A.)

     Two questions naturally follow on this kind of reasoning: "How is a reduced or
simplified version of equation (4.26) fitted to a data set? And after fitting such an
equation, how is the appropriateness of the result determined?" General answers to
these questions are subtle. But there is one circumstance in which it is possible to
give fairly straightforward answers. That is the case where the data are balanced--
in the sense that all of the samples (leading to the y¯ i j 's) have the same size. With
balanced data, the fitted effects from Definitions 5 and 6 and simple addition produce
fitted responses. And based on such fitted values, the R2 and residual plotting ideas
from the last two sections can be applied here as well. That is, when working with
balanced data, least squares fitting of a simplified version of equation (4.26) can be
accomplished by

1. calculating fitted effects according to Definitions 5 and 6 and then
                                                           4.3 Fitted Effects for Factorial Data 173

                            2. adding those corresponding to terms in the reduced equation to compute
                                fitted responses, y^ .

              Residuals are then (as always)               e = y - y^

Residuals

                            (and should look like noise if the simplified equation is an adequate description of
                            the data set). Further, the fraction of raw variation in y accounted for in the fitting
                            process is (as always)

 Coefficient of             R2 =                           (y - y¯ )2 - (y - y^ )2                (4.27)
determination                                                      (y - y¯ )2

                            where the sums are over all observed y's. (Summation notation is being abused even
                            further than usual, by not even subscripting the y's and y^ 's.)

                Example 8   Simplified Description of Two-Way Factorial Golf Ball Flight Data
(Example 12, Chapter 2,
                            G. Gronberg tested drive flight distances for golf balls of several different com-
         revisited--p. 49)  pressions on several different evenings. Table 4.15 gives a small part of the data
                            that he collected, representing 80 and 100 compression flight distances (in yards)
                            from two different evenings. Notice that these data are balanced, all four sample
                            sizes being 10.

                            Table 4.15
                            Golf Ball Flight Distances for Four Compression/Evening Combinations

                                                                          Evening (B)

                                                                       1               2

                                                       80  180 192                  196 180
                            Compression (A)                193 190                  192 195
                                                           197 182                  191 197
                                                      100  189 192                  194 192
                                                           187 179                  186 193

                                                           180 175                  190 185
                                                           185 190                  195 167
                                                           167 185                  180 180
                                                           162 180                  170 180
                                                           170 185                  180 165
174 Chapter 4 Describing Relationships Between Variables

Example 8          These data have complete two-way factorial structure. The factor Evening is
(continued )  not really of primary interest. Rather, it is a blocking factor, its levels creating
              homogeneous environments in which to compare 80 and 100 compression flight
              distances. Figure 4.24 is a graphic using boxplots to represent the four samples
              and emphasizing the factorial structure.

                   Calculating sample means corresponding to the four cells in Table 4.15 and
              then finding fitted effects is straightforward. Table 4.16 displays cell, row, column,
              and grand average means. And based on those values,

                            a1 = 189.85 - 184.20 = 5.65 yards
                            a2 = 178.55 - 184.20 = -5.65 yards
                            b1 = 183.00 - 184.20 = -1.20 yards
                            b2 = 185.40 - 184.20 = 1.20 yards
                         ab11 = 188.1 - (184.20 + 5.65 + (-1.20)) = -.55 yards
                         ab12 = 191.6 - (184.20 + 5.65 + 1.20) = .55 yards
                         ab21 = 177.9 - (184.20 + (-5.65) + (-1.20)) = .55 yards
                         ab22 = 179.2 - (184.20 + (-5.65) + 1.20) = -.55 yards

              Flight distance (yd)                   80 Compression
                                    190
                                    180 100 Compression

                                    170

                                                          1              2

                                                             Evening

              Figure 4.24 Golf ball flight distance
              boxplots for four combinations of
              Compression and Evening

              Table 4.16
              Cell, Row, Column, and Grand Average Means for the Golf Ball Flight Data

                                                             Evening (B)

              Compression (A) 80                                1                 2        189.85
                                       100                                                 178.55
                                                          y¯ 11 = 188.1     y¯ 12 = 191.6
                                                          y¯ 21 = 177.9     y¯ 22 = 179.2  184.20

                                                             183.00            185.40
                            4.3 Fitted Effects for Factorial Data 175

Mean distance, yij (yd)  190
                                    80 Compression

                         180
                                    100 Compression

                         1           2

                            Evening

Figure 4.25 Interaction plot for the
golf ball flight data

The fitted effects indicate that most of the differences in the cell means in Ta-
ble 4.16 are understandable in terms of differences between 80 and 100 compres-
sion balls. The effect of differences between evenings appears to be on the order
of one-fourth the size of the effect of differences between ball compressions.
Further, the pattern of flight distances across the two compressions changed rela-
tively little from evening to evening. These facts are portrayed graphically in the
interaction plot of Figure 4.25.

     The story told by the fitted effects in this example probably agrees with most
readers' intuition. There is little reason a priori to expect the relative behaviors of
80 and 100 compression flight distances to change much from evening to evening.
But there is slightly more reason to expect the distances to be longer overall on
some nights than on others.

     It is worth investigating whether the data in Table 4.15 allow the simplest

                         "Compression effects only"

description, or require the somewhat more complicated

"Compression effects and Evening effects but no interactions"

description, or really demand to be described in terms of

"Compression, Evening, and interaction effects"

To do so, fitted responses are first calculated corresponding to the three different
possible corresponding relationships

                         y  µ + i                              (4.28)
                         y  µ + i + j                          (4.29)
                         y  µ + i + j + i j                    (4.30)
176 Chapter 4 Describing Relationships Between Variables

Example 8     Table 4.17
(continued )  Fitted Responses Corresponding to Equations (4.28), (4.29), and (4.30)

              Compression  Evening   For (4.28)            For (4.29)               For (4.30)
                                    y¯ .. + ai = y¯ i.    y¯ .. + ai + bj  y¯ .. + ai + bj + abi j = y¯ i j
                    80         1
                   100         1       189.85                188.65                   188.10
                               2       178.55                177.35                   177.90
                    80         2       189.85                191.05                   191.60
                   100                 178.55                179.75                   179.20

              These are generated using the fitted effects. They are collected in Table 4.17
              (not surprisingly, the first and third sets of fitted responses are, respectively, row
              average and cell means).

                   Residuals e = y - y^ for fitting the three equations (4.28), (4.29), and (4.30)
              are obtained by subtracting the appropriate entries in, respectively, the third,
              fourth, or fifth column of Table 4.17 from each of the data values listed in
              Table 4.15. For example, 40 residuals for the fitting of the "A main effects only"
              equation (4.28) would be obtained by subtracting 189.85 from every entry in the
              upper left cell of Table 4.15, subtracting 178.55 from every entry in the lower
              left cell, 189.85 from every entry in the upper right cell, and 178.55 from every
              entry in the lower right cell.

                   Figure 4.26 provides normal plots of the residuals from the fitting of the three
              equations (4.28), (4.29), and (4.30). None of the normal plots is especially linear,
              but at the same time, none of them is grossly nonlinear either. In particular, the
              first two, corresponding to simplified versions of relationship 4.26, are not signif-
              icantly worse than the last one, which corresponds to the use of all fitted effects
              (both main effects and interactions). From the limited viewpoint of producing
              residuals with an approximately bell-shaped distribution, the fitting of any of the
              three equations (4.28), (4.29), and (4.30) would appear approximately equally
              effective.

                   The calculation of R2 values for equations (4.28), (4.29), and (4.30) proceeds
              as follows. First, since the grand average of all 40 flight distances is y¯ = 184.2
              yards (which in this case also turns out to be y¯ ..) ,

                               (y - y¯ )2 = (180 - 184.2)2 + · · · + (179 - 184.2)2

                                              + (180 - 184.2)2 + · · · + (185 - 184.2)2

                                              + (196 - 184.2)2 + · · · + (193 - 184.2)2

                                              + (190 - 184.2)2 + · · · + (165 - 184.2)2

                                          = 3,492.4

              (This value can easily be obtained on a pocket calculator by using 39 (= 40 - 1 =
              n - 1) times the sample variance of all 40 flight distances.) Then (y - y^ )2
                                                              4.3 Fitted Effects for Factorial Data 177

2.0                          2.0                              2.0

1.0                          1.0                              1.0

0                            0                                0

-1.0                         -1.0                             -1.0

-2.0                         -2.0                             -2.0

      -10  10                      -10            10                         -10                         10

      Residual for y  µ + i        Residual for y  µ + i + j  Residual for y  µ + i + j + ij

           Figure 4.26 Normal plots of residuals from three different equations fitted to the golf data

                             values for the three equations are obtained as the sums of the squared residuals.
                             For example, using Tables 4.15 and 4.17, for equation (4.29),

                                            (y - y^ )2 = (180 - 188.65)2 + · · · + (179 - 188.65)2

                                                            + (180 - 177.35)2 + · · · + (185 - 177.35)2
                                                            + (196 - 191.05)2 + · · · + (193 - 191.05)2
                                                            + (190 - 179.75)2 + · · · + (165 - 179.75)2
                                                        = 2,157.90

                             Finally, equation (4.27) is used. Table 4.18 gives the three values of R2.
                                  The story told by the R2 values is consistent with everything else that's been

                             said in this example. None of the values is terribly big, which is consistent with
                             the large within-sample variation in flight distances evident in Figure 4.24. But

                                        Table 4.18
                                        R2 Values for Fitting Equations

                                        (4.28), (4.29), and (4.30) to

                                        Gronberg's Data

                                        Equation                         R2

                                        y  µ + i              .366

                                        y  µ + i + j          .382

                                        y  µ + i + j + i j .386
178 Chapter 4 Describing Relationships Between Variables

Example 8                     considering A (Compression) main effects does account for some of the observed
(continued )                  variation in flight distance, and the addition of B (Evening) main effects adds
                              slightly to the variation accounted for. Introducing interactions into consideration
                              adds little additional accounting power.

                                   The computations in Example 8 are straightforward but tedious. The kind of
                              software used to produce Printout 6 typically allows for the painless fitting of
                              simplified relationships like (4.28), (4.29), and (4.30) and computation (and later
                              plotting) of the associated residuals.

4.3.3                         Fitted Effects for Three-Way (and Higher) Factorials

                              The reasoning that has been applied to two-way factorial data is naturally general-
                              ized to complete factorial data structures that are three-way and higher. First, fitted
                              main effects and various kinds of interactions are computed. Then one hopes to
                              discover that a data set can be adequately described in terms of a few of these
                              that are interpretable when taken as a group. This subsection shows how this
                              is carried out for 3-factor situations. Once the pattern has been made clear, the
                              reader can carry it out for situations involving more than three factors, working by
                              analogy.

                                   In order to deal with three-way factorial data, yet more notation is needed.
                              Unfortunately, this involves triple subscripts. For factor A having I levels, factor B
                              having J levels, and factor C having K levels, the following notation will be used:

     Notation for sample      y¯ i jk = the sample mean response when factor A is at level i,
          means and their             factor B is at level j, and factor C is at level k

averages (for three-way       y¯ ... =   1            y¯ i jk
             factorial data)
                                         IJK
                                              i, j,k

                              = the grand average sample mean

                              y¯ i.. =   1         y¯ i jk

                                         JK   j,k

                              = the average sample mean when factor A is at level i

                              y¯ . j. =  1         y¯ i jk

                                         IK   i,k

                              = the average sample mean when factor B is at level j

                              y¯ ..k =   1         y¯ i jk

                                         IJ  i, j

                              = the average sample mean when factor C is at level k
                                     4.3 Fitted Effects for Factorial Data 179

y¯ i j. =  1        y¯ i jk

           K     k

= the average sample mean when factor A is at level i and factor B
   is at level j

y¯ i.k =   1        y¯ i jk

           J  j

= the average sample mean when factor A is at level i and factor C
   is at level k

y¯ . jk =  1        y¯ i jk

           I  i

= the average sample mean when factor B is at level j and factor C
   is at level k

In these expressions, where a subscript is used as an index of summation, the

summation is assumed to extend over all of its I, J , or K possible values.

     It is most natural to think of the means from a 3-factor study laid out in three
dimensions. Figure 4.27 illustrates this general situation, and the next example
employs another common three-dimensional display in a 23 context.

                             I

           Factor A level                                               K

                             2                                        level
                             1                          2 Factor C
                                                J1
                                 12

                                Factor B level

                    Figure 4.27 IJK cells in a three-dimensional table
180 Chapter 4 Describing Relationships Between Variables

  Example 9     A 23 Factorial Experiment on the Strength of a Composite Material

Cube plot for   In his article "Application of Two-Cubed Factorial Designs to Process Stud-
    displaying  ies" (ASQC Technical Supplement Experiments in Industry, 1985), G. Kinzer
     23 means   discusses a successful 3-factor industrial experiment.

                     The strength of a proprietary composite material was thought to be related to
                three process variables, as indicated in Table 4.19. Five specimens were produced
                under each of the 23 = 8 combinations of factor levels, and their moduli of rupture
                were measured (in psi) and averaged to produce the means in Table 4.20. (There
                were also apparently 10 specimens made with an autoclave temperature of 315F,
                an autoclave time of 8 hr, and a time span of 8 hr, but this will be ignored for
                present purposes.)

                     A helpful display of these means can be made using the corners of a cube,
                as in Figure 4.28. Using this three-dimensional picture, one can think of average
                sample means as averages of y¯ i jk's sharing a face or edge of the cube.

                Table 4.19
                Levels of Three Process Variables in a 23 Study of Material Strength

                Factor  Process Variable                     Level 1      Level 2

                A       Autoclave temperature                300F         330F
                B       Autoclave time                       4 hr         12 hr
                C       Time span (between product           4 hr         12 hr

                           formation and autoclaving)

                Table 4.20
                Sample Mean Strengths for 23 Treatment Combinations

                        i,              j,                       k,            y¯ i jk ,
                Factor A Level  Factor B Level            Factor C Level  Sample Mean

                                                                          Strength (psi)

                1               1                         1               1520

                2               1                         1               2450

                1               2                         1               2340

                2               2                         1               2900

                1               1                         2               1670

                2               1                         2               2540

                1               2                         2               2230

                2               2                         2               3230
                             4.3 Fitted Effects for Factorial Data 181

                   y212 = 2540        y222 = 3230

              2 y211 = 2450        y221 = 2900

Factor A level     y112 = 1670        y122 = 2230 2

                                   y121 = 2340 1 actor C  level

              1 y111 = 1520

                1                  2            F

                   Factor B level

Figure 4.28 23 sample mean strengths displayed on a
cube plot

For example,

                         1
               y¯ 1.. = (1520 + 2340 + 1670 + 2230) = 1940 psi

                       2·2
is the average mean on the bottom face, while

                                   1
                           y¯ 11. = (1520 + 1670) = 1595 psi

                                   2

is the average mean on the lower left edge. For future reference, all of the average
sample means are collected here:

              y¯ ... = 2360 psi    y¯ 2.. = 2780 psi
              y¯ 1.. = 1940 psi    y¯ .2. = 2675 psi
              y¯ .1. = 2045 psi    y¯ ..2 = 2417.5 psi
              y¯ ..1 = 2302.5 psi  y¯ 12. = 2285 psi
              y¯ 11. = 1595 psi    y¯ 22. = 3065 psi
              y¯ 21. = 2495 psi    y¯ 1.2 = 1950 psi
              y¯ 1.1 = 1930 psi    y¯ 2.2 = 2885 psi
              y¯ 2.1 = 2675 psi    y¯ .12 = 2105 psi
              y¯ .11 = 1985 psi    y¯ .22 = 2730 psi
              y¯ .21 = 2620 psi

     Analogy with Definition 5 provides definitions of fitted main effects in a 3-factor
study as the differences between factor-level average means and the grand average
mean.
182 Chapter 4 Describing Relationships Between Variables

Definition 7  In a three-way complete factorial study with factors A, B, and C, the fitted
              main effect of factor A at its ith level is

                                                    ai = y¯ i.. - y¯ ...
              The fitted main effect of factor B at its jth level is

                                                   bj = y¯ . j. - y¯ ...

              And the fitted main effect of factor C at its kth level is
                                                   ck = y¯ ..k - y¯ ...

              Using the geometrical representation of factor-level combinations given in Fig-
              ure 4.28, these fitted effects are averages of y¯i jk's along planes (parallel to one set
              of faces of the rectangular solid) minus the grand average sample mean.

                   Next, analogy with Definition 6 produces definitions of fitted two-way interac-

              tions in a 3-factor study.

Definition 8  In a three-way complete factorial study with factors A, B, and C, the fitted
              2-factor interaction of factor A at its ith level and factor B at its jth level is

                                           abi j = y¯ i j. - (y¯ ... + ai + bj )

              the fitted 2-factor interaction of factor A at its ith level and factor C at its
              kth level is

                                           acik = y¯ i.k - (y¯ ... + ai + ck )

              and the fitted 2-factor interaction of factor B at its jth level and factor C at
              its kth level is

                                          bcjk = y¯ . jk - (y¯ ... + bj + ck )
                                           4.3 Fitted Effects for Factorial Data 183

    Interpreting two-     These fitted 2-factor interactions can be thought of in two equivalent ways:
     way interactions
in a three-way study      1. as what one gets as fitted interactions upon averaging across all levels of
                              the factor that is not under consideration to obtain a single two-way table of
                              (average) means and then calculating as per Definition 6 (page 169);

                          2. as what one gets as averages, across all levels of the factor not under consid-
                              eration, of the fitted two-factor interactions calculated as per Definition 6,
                              one level of the excluded factor at a time.

Example 9                 To illustrate the meaning of Definitions 7 and 8, return to the composite material
(continued )              strength study. For example, the fitted A main effects are

                                               a1 = y¯ 1.. - y¯ ... = 1940 - 2360 = -420 psi
                                               a2 = y¯ 2.. - y¯ ... = 2780 - 2360 = 420 psi

                          And the fitted AB 2-factor interaction for levels 1 of A and 1 of B is

                          ab11 = y¯ 11. - (y¯ ... + a1 + b1) = 1595 - (2360 + (-420) + (2045 - 2360))
                                = -30 psi

                          The entire set of fitted effects for the means of Table 4.20 is as follows.

                            a1 = -420 psi    b1 = -315 psi     c1 = -57.5 psi
                            a2 = 420 psi     b2 = 315 psi      c2 = 57.5 psi
                          ab11 = -30 psi   ac11 = 47.5 psi   bc11 = -2.5 psi
                          ab12 = 30 psi    ac12 = -47.5 psi  bc12 = 2.5 psi
                          ab21 = 30 psi    ac21 = -47.5 psi  bc21 = 2.5 psi
                          ab22 = -30 psi   ac22 = 47.5 psi   bc22 = -2.5 psi

       Interpretation of       Remember equation (4.25) (page 171). It says that in 2-factor studies, the fitted
three-way interactions    grand mean, main effects, and two-factor interactions completely describe a factorial
                          set of sample means. Such is not the case in three-factor studies. Instead, a new pos-
                          sibility arises: 3-factor interaction. Roughly speaking, the fitted three-factor interac-
                          tions in a 3-factor study measure how much pattern the combination means carry that
                          is not explainable in terms of the factors A, B, and C acting separately and in pairs.

Definition 9              In a three-way complete factorial study with factors A, B, and C, the fitted
                          3-factor interaction of A at its ith level, B at its jth level, and C at its kth
                          level is

                                     abci jk = y¯ i jk - (y¯ ... + ai + bj + ck + abi j + acik + bcjk )
184 Chapter 4 Describing Relationships Between Variables

Example 9        To illustrate the meaning of Definition 9, consider again the composite ma-
(continued )     terial study. Using the previously calculated fitted main effects and 2-factor
                 interactions,

                 abc111 = 1520 - (2360 + (-420) + (-315) + (-57.5) + (-30)
                            + 47.5 + (-2.5)) = -62.5psi

                 Similar calculations can be made to verify that the entire set of 3-factor interac-
                 tions for the means of Table 4.20 is as follows:

                 abc111 = -62.5 psi                       abc211 = 62.5 psi
                 abc121 = 62.5 psi                        abc221 = -62.5 psi
                 abc112 = 62.5 psi                        abc212 = -62.5 psi
                 abc122 = -62.5 psi                       abc222 = 62.5 psi

       A second       Main effects and 2-factor interactions are more easily interpreted than 3-factor
interpretation   interactions. One insight into their meaning was given immediately before Defi-
                 nition 9. Another is the following. If at the different levels of (say) factor C, the
 of three-way    fitted AB interactions are calculated and the fitted AB interactions (the pattern of
   interactions  parallelism or nonparallelism) are essentially the same on all levels of C, then the
                 3-factor interactions are small (near 0). Otherwise, large 3-factor interactions allow
                 the pattern of AB interaction to change, from one level of C to another.

4.3.4 Simpler Descriptions of Some Three-Way Data Sets
          Rewriting the equation in Definition 9,

                 y¯ i jk = y¯ ... + ai + bj + ck + abi j + acik + bcjk + abci jk  (4.31)

                 This is a breakdown of the combination sample means into somewhat interpretable
                 pieces, corresponding to an overall effect, the factors acting separately, the factors
                 acting in pairs, and the factors acting jointly. Display (4.31) may be thought of as a
                 fitted version of an approximate relationship

                 y  µ + i + j + k + i j + ik + jk + i jk                          (4.32)

                 When beginning the analysis of three-way factorial data, one hopes to discover
                 a simplified version of equation (4.32) that is both interpretable and an adequate
                 description of the data. (Indeed, if it is not possible to do so, little is gained by
                 using the factorial breakdown rather than simply treating the data in question as IJK
                 unstructured samples.)

                      As was the case earlier with two-way factorial data, the process of fitting a
                 simplified version of display (4.32) via least squares is, in general, unfortunately
                 somewhat complicated. But when all sample sizes are equal (i.e., the data are
                        4.3 Fitted Effects for Factorial Data 185

              balanced), the fitting process can be accomplished by simply adding appropriate

              fitted effects defined in Definitions 7, 8, and 9. Then the fitted responses lead to
              residuals that can be used in residual plotting and the calculation of R2.

Example 9     Looking over the magnitudes of the fitted effects for Kinzer's composite material
(continued )  strength study, the A and B main effects clearly dwarf the others, suggesting the
              possibility that the relationship

                        y  µ + i + j                                     (4.33)

              could be used as a description of the physical system. This relationship doesn't
              involve factor C at all (either by itself or in combination with A or B) and indicates
              that responses for a particular AB combination will be comparable for both time
              spans studied. Further, the fact that display (4.33) doesn't include the i j term
              says that factors A and B act on product strength separately, so that their levels
              can be chosen independently. In geometrical terms corresponding to the cube plot
              in Figure 4.28, display (4.33) means that observations from the cube's back face
              will be comparable to corresponding ones on the front face and that parallelism
              will prevail on both the front and back faces.

                   Kinzer's article gives only y¯ i jk values, not raw data, so a residual analysis
              and calculation of R2 are not possible. But because of the balanced nature of the
              original data set, fitted values are easily obtained. For example, with factor A at
              level 1 and B at level 1, using the simplified relationship (4.33) and the fitted
              main effects found earlier produces the fitted value

              y^ = y¯ ... + a1 + b1 = 2360 + (-420) + (-315) = 1625 psi

                        Fitted y values, y

                        2465                3095

              2 2465              3095

              Factor A

                        1625            2255 2
                                         Factor C
              1 1625              2255  1
                   1                2
                        Factor B

              Figure 4.29 Eight fitted responses for
              relationship (4.33) and the composite
              strength study
186 Chapter 4 Describing Relationships Between Variables

Example 9     All eight fitted values corresponding to equation (4.33) are shown geometrically
(continued )  in Figure 4.29. The fitted values given in the figure might be combined with
              product requirements and cost information to allow a process engineer to make
              sound decisions about autoclave temperature, autoclave time, and time span.

                   In Example 9, the simplified version of display (4.32) was especially inter-
              pretable because it involved only main effects. But sometimes even versions of
              relation (4.32) involving interactions can draw attention to what is going on in a
              data set.

Example 10    Interactions in a 3-Factor Paper Airplane Experiment

              Schmittenberg and Riesterer studied the effects of three factors, each at two levels,

              on flight distance of paper airplanes. The factors were Plane Design (A) (design 1

              versus design 2), Plane Size (B) (large versus small), and Paper Type (C) (heavy

              versus light). The means of flight distances they obtained for 15 flights of each

              of the 8 = 2 × 2 × 2 types of planes are given in Figure 4.30.

                   Calculate the fitted effects corresponding to the y¯ i jk's given in Figure 4.30
              "by hand." (Printout 7 also gives the fitted effects.) By far the biggest fitted effects

              (more than three times the size of any others) are the AC interactions. This makes

              perfect sense. The strongest message in Figure 4.30 is that plane design 1 should

              be made with light paper and plane design 2 with heavy paper. This is a perfect

              example of a strong 2-factor interaction in a 3-factor study (where, incidentally,

              the  fitted  3-factor  interactions  are    roughly  1  the  size  of  any  other  fitted  effects).
                                                                   4
              Any simplified version of display (4.32) used to represent this situation would

              certainly have to include the ik term.

                                     Sample mean flight distances (ft)

                                                          18.3             14.0

                                     2 26.0                        24.0

                           Plane design 22.5                               ig 21.6 2 ht
                                                                                      L eight
                                     1 15.8                      18.4
                                                                           e1 av ap y er w
                                         1                        2        HP
                                      Large                     Small

                                                   Plane size

                           Figure 4.30 23 sample mean flight distances
                           displayed on the corners of a cube
                                                      4.3 Fitted Effects for Factorial Data 187

               Printout 7 Calculation of Fitted Effects for the Airplane Experiment

               General Linear Model

               Factor      Type Levels Values
               design
               size       fixed   2 12
               paper
                          fixed   2 12

                          fixed   2 12

               Analysis of Variance for mean dis, using Adjusted SS for Tests

               Source             DF  Seq SS              Adj SS     Adj MS  F       P

               design             1            2.000      2.000      2.000   **

               size               1            2.645      2.645      2.645   **

               paper              1            7.605      7.605      7.605   **

               design*size        1            8.000      8.000      8.000   **

               design*paper       1   95.220              95.220     95.220  **

               size*paper         1            4.205      4.205      4.205   **

               design*size*paper 1             0.180      0.180      0.180   **

               Error              0            0.000      0.000      0.000

               Total              7 119.855

               ** Denominator of F-test is zero.

               Term                      Coef      StDev          T  P
                                     20.0750      0.0000
               Constant           -0.500000    0.000000           *  *
                                   0.575000    0.000000
               design              0.975000    0.000000
                                   -1.00000     0.00000
               1                   -3.45000     0.00000           *  *
                                  -0.725000    0.000000
               size               -0.150000    0.000000

               1                                                  *  *

               paper

               1                                                  *  *

               design*size

               1       1                                          *  *

               design*paper

               1       1                                          *  *

               size*paper

               11                                                 *  *

               design*size*paper

               1       11                                         *  *

      4.3.5    Special Devices for 2p Studies

      Special  All of the discussion in this section has been general, in the sense that any value
2p factorial   has been permissible for the number of levels for a factor. In particular, all of the
               definitions of fitted effects in the section work as well for 3 × 5 × 7 studies as they
   notation    do for 2 × 2 × 2 studies. But from here on in the section, attention will be restricted
               to 2p data structures.

                    Restricting attention to two-level factors affords several conveniences. One is
               notational. It is possible to reduce the clutter caused by the multiple subscript "i jk"
               notation, as follows. One level of each factor is designated as a "high" (or "+")
               level and the other as a "low" (or "-") level. Then the 2p factorial combinations are
               labeled with letters corresponding to those factors appearing in the combination at
188 Chapter 4 Describing Relationships Between Variables

                         Table 4.21
                         Shorthand Names for the 23 Factorial Treatment

                         Combinations

                         Level of  Level of               Level of  Combination
                         Factor A  Factor B               Factor C      Name

                         1         1                      1         (1)

                         2         1                      1         a

                         1         2                      1         b

                         2         2                      1         ab

                         1         1                      2         c

                         2         1                      2         ac

                         1         2                      2         bc

                         2         2                      2         abc

Special relationship     their high levels. For example, if level 2 of each of factors A, B, and C is designated
between 2p effects       the high level, shorthand names for the 23 = 8 different ABC combinations are as
                         given in Table 4.21. Using these names, for example, y¯ a can stand for a sample mean
      of a given type    where factor A is at its high (or second) level and all other factors are at their low

                         (or first) levels.

                              A second convenience special to two-level factorial data structures is the fact

                         that all effects of a given type have the same absolute value. This has already been

                         illustrated in Example 9. For example, looking back, for the data of Table 4.20,

The Yates algorithm                                  a2 = 420 = -(-420) = -a1

for computing fitted     and
   2p factorial effects
                                                   bc22 = -2.5 = bc11 = -bc12 = -bc21

                         This is always the case for fitted effects in 2p factorials. In fact, if two fitted
                         effects of the same type are such that an even number of 1  2 or 2  1 subscript
                         changes are required to get the second from the first, the fitted effects are equal
                         (e.g., bc22 = bc11). If an odd number are required, then the second fitted effect is
                         -1 times the first (e.g., bc12 = -bc22). This fact is so useful because one needs only
                         to do the arithmetic necessary to find one fitted effect of each type and then choose
                         appropriate signs to get all others of that type.

                              A statistician named Frank Yates is credited with discovering an efficient,
                         mechanical way of generating one fitted effect of each type for a 2p study. His
                         method is easy to implement "by hand" and produces fitted effects with all "2"
                         subscripts (i.e., corresponding to the "all factors at their high level" combination).
                         The Yates algorithm consists of the following steps.

                              Step 1 Write down the 2p sample means in a column in what is called Yates
                                          standard order. Standard order is easily remembered by beginning
                                                4.3 Fitted Effects for Factorial Data 189

                            Step 2  with (1) and a, then multiplying these two names (algebraically) by
                            Step 3  b to get b and ab, then multiplying these four names by c to get c, ac,
                                    bc, abc, etc.

                                    Make up another column of numbers by first adding and then sub-
                                    tracting (first from second) the entries in the previous column in pairs.

                                    Follow step 2 a total of p times, and then make up a final column by
                                    dividing the entries in the last column by the value 2p.

                            The last column (made via step 3) gives fitted effects (all factors at level 2), again
                            in standard order.

Example 9                   Table 4.22 shows the use of the Yates algorithm to calculate fitted effects for the
(continued )                23 composite material study. The entries in the final column of this table are, of
                            course, exactly as listed earlier, and the rest of the fitted effects are easily obtained
                            via appropriate sign changes. This final column is an extremely concise summary
                            of the fitted effects, which quickly reveals which types of fitted effects are larger
                            than others.

                            Table 4.22
                            The Yates Algorithm Applied to the Means of Table 4.20

                            Combination y¯ Cycle 1 Cycle 2 Cycle 3                   Cycle 3 ÷ 8

                            (1)     1520  3970  9210 18,880                         2360 = y¯ ...
                                                                                     420 = a2
                            a       2450  5240  9670 3,360                           315 = b2
                                                                                    -30 = ab22
                            b       2340  4210  1490 2,520                          57.5 = c2
                                                                                    47.5 = ac22
                            ab      2900  5460  1870 -240                           -2.5 = bc22
                                                                                    62.5 = abc222
                            c       1670  930   1270  460

                            ac      2540  560   1250  380

                            bc      2230  870 -370    -20

                            abc     3230  1000  130   500

      The reverse Yates          The Yates algorithm is useful beyond finding fitted effects. For balanced data
   algorithm and easy       sets, it is also possible to modify it slightly to find fitted responses, y^ , correspond-
computation of fitted       ing to a simplified version of a relation like display (4.32). First, the desired (all
                            factors at their high level) fitted effects (using 0's for those types not considered)
                 responses  are written down in reverse standard order. Then, by applying p cycles of the
                            Yates additions and subtractions, the fitted values, y^ , are obtained, listed in re-
                            verse standard order. (Note that no final division is required in this reverse Yates
                            algorithm.)
190 Chapter 4 Describing Relationships Between Variables

Example 9            Consider fitting the relationship (4.33) to the balanced data set that led to the
(continued )         means of Table 4.20 via the reverse Yates algorithm. Table 4.23 gives the details.
                     The fitted values in the final column are exactly as shown earlier in Figure 4.29.

                     Table 4.23
                     The Reverse Yates Algorithm Applied to Fitting the "A and B
                     Main Effects Only" Equation (4.33) to the Data of Table 4.20

                     Fitted Effect Value Cycle 1 Cycle 2 Cycle 3 (y^ )

                     abc222                                        0      0      0  3095 = y^ abc
                     bc22                                          0      0  3095   2255 = y^ bc
                     ac22                                          0   315          2465 = y^ ac
                     c2                                            0  2780       0  1625 = y^ c
                     ab22                                          0      0  2255   3095 = y^ ab
                     b2                                         315       0         2255 = y^ b
                     a2                                         420    315       0  2465 = y^ a
                     y¯ ...                                    2360   1940   2465   1625 = y^ (1)

                                                                                 0
                                                                             1625

The importance            The restriction to two-level factors that makes these notational and computa-
     of two-level
         factorials  tional devices possible is not as specialized as it may at first seem. When an engineer
                     wishes to study the effects of a large number of factors, even 2p will be a large num-

                     ber of conditions to investigate. If more than two levels of factors are considered,

                     the sheer size of a complete factorial study quickly becomes unmanageable. Rec-

                     ognizing this, two-level studies are often used for screening to identify a few (from

                     many) process variables for subsequent study at more levels on the basis of their
                     large perceived effects in the screening study. So this 2p material is in fact quite

                     important to the practice of engineering statistics.

Section 3 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Since the data of Exercise 2 of Section 4.2 have                Plot these versus level of the NaOH variable,
   complete factorial structure, it is possible (at least          connecting fitted values having the same level
   temporarily) to ignore the fact that the two experi-            of the Time variable with line segments, as in
   mental factors are basically quantitative and make              Figure 4.23. Discuss how this plot compares
   a factorial analysis of the data.                               to the two plots of fitted y versus x1 made in
    (a) Compute all fitted factorial main effects and in-          Exercise 2 of Section 4.2.
        teractions for the data of Exercise 2 of Section       (c) Use the fitted values computed in (b) and find
        4.2. Interpret the relative sizes of these fitted ef-      a value of R2 appropriate to the "main effects
        fects, using a interaction plot like Figure 4.22           only" representation of y. How does it com-
        to facilitate your discussion.                             pare to the R2 values from multiple regres-
   (b) Compute nine fitted responses for the "main ef-             sions? Also use the fitted values to compute
        fects only" explanation of y, y  µ + i + j .
                                    4.4 Transformations and Choice of Measurement Scale 191

        residuals for this "main effects only" represen-     (b) The students actually had some physical the-
        tation. Plot these (versus level of NaOH, level           ory suggesting that the log of the drain time
        of Time, and y^ , and in normal plot form). What          might be a more convenient response variable
        do they indicate about the present "no interac-           than the raw time. Take the logs of the y's and
        tion" explanation of specific area?                       recompute the factorial effects. Does an inter-
                                                                  pretation of this system in terms of only main
2. Bachman, Herzberg, and Rich conducted a 23 fac-                effects seem more plausible on the log scale
   torial study of fluid flow through thin tubes. They            than on the original scale?
   measured the time required for the liquid level in
   a fluid holding tank to drop from 4 in. to 2 in. for       (c) Considering the logged drain times as the re-
   two drain tube diameters and two fluid types. Two              sponses, find fitted values and residuals for a
   different technicians did the measuring. Their data            "Diameter and Fluid main effects only" expla-
   are as follows:                                                nation of these data. Compute R2 appropriate
                                                                  to such a view and compare it to R2 that re-
              Diameter  Time (sec)                                sults from using all factorial effects to describe
Technician (in.) Fluid                                            log drain time. Make and interpret appropriate
                                                                  residual plots.
1  .188 water           21.12, 21.11, 20.80
                                                             (d) Based on the analysis from (c), what change in
2  .188 water           21.82, 21.87, 21.78                       log drain time seems to accompany a change
                                                                  from .188 in. diameter to .314 in. diameter?
1  .314 water           6.06, 6.04, 5.92                          What does this translate to in terms of raw drain
                                                                  time? Physical theory suggests that raw time is
2  .314 water           6.09, 5.91, 6.01                          inversely proportional to the fourth power of
                                                                  drain tube radius. Does your answer here seem
1  .188 ethylene glycol 51.25, 46.03, 46.09                       compatible with that theory? Why or why not?

2  .188 ethylene glycol 45.61, 47.00, 50.71               3. When analyzing a full factorial data set where the
                                                             factors involved are quantitative, either the surface-
1  .314 ethylene glycol 7.85, 7.91, 7.97                     fitting technology of Section 4.2 or the factorial
                                                             analysis material of Section 4.3 can be applied.
2  .314 ethylene glycol 7.73, 8.01, 8.32                     What practical engineering advantage does the first
                                                             offer over the second in such cases?
(a) Compute (using the Yates algorithm or other-
    wise) the values of all the fitted main effects,
    two-way interactions, and three-way interac-
    tions for these data. Do any simple interpreta-
    tions of these suggest themselves?

   qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

   4.4 Transformations and Choice of Measurement
          Scale (Optional )

                       Sections 4.2 and 4.3 are an introduction to one of the main themes of engineer-
                       ing statistical analysis: the discovery and use of simple structure in complicated
                       situations. Sometimes this can be done by reexpressing variables on some other
                       (nonlinear) scales of measurement besides the ones that first come to mind. That is,
                       sometimes simple structure may not be obvious on initial scales of measurement, but
                       may emerge after some or all variables have been transformed. This section presents
                       several examples where transformations are helpful. In the process, some comments
                       about commonly used types of transformations, and more specific reasons for using
                       them, are offered.
192 Chapter 4 Describing Relationships Between Variables

4.4.1       Transformations and Single Samples

            In Chapter 5, there are a number of standard theoretical distributions. When one of
            these standard models can be used to describe a response y, all that is known about
            the model can be brought to bear in making predictions and inferences regarding y.
            However, when no standard distributional shape can be found to describe y, it may
            nevertheless be possible to so describe g(y) for some function g(·).

Example 11  Discovery Times at an Auto Shop

            Elliot, Kibby, and Meyer studied operations at an auto repair shop. They collected
            some data on what they called the "discovery time" associated with diagnosing
            what repairs the mechanics were going to recommend to the car owners. Thirty
            such discovery times (in minutes) are given in Figure 4.31, in the form of a
            stem-and-leaf plot.

                 The stem-and-leaf plot shows these data to be somewhat skewed to the
            right. Many of the most common methods of statistical inference are based on
            an assumption that a data-generating mechanism will in the long run produce
            not skewed, but rather symmetrical and bell-shaped data. Therefore, using these
            methods to draw inferences and make predictions about discovery times at this
            shop is highly questionable. However, suppose that some transformation could
            be applied to produce a bell-shaped distribution of transformed discovery times.
            The standard methods could be used to draw inferences about transformed dis-
            covery times, which could then be translated (by undoing the transformation) to
            inferences about raw discovery times.

                 One common transformation that has the effect of shortening the right tail
            of a distribution is the logarithmic transformation, g(y) = ln(y). To illustrate its
            use in the present context, normal plots of both discovery times and log discovery
            times are given in Figure 4.32. These plots indicate that Elliot, Kibby, and Meyer
            could not have reasonably applied standard methods of inference to the discovery
            times, but they could have used the methods with log discovery times. The second
            normal plot is far more linear than the first.

               043
               06555668988
               14030
               17595695
               20
               29579
               32
               36

            Figure 4.31 Stem-and-leaf plot of
            discovery times
                          4.4 Transformations and Choice of Measurement Scale 193

                    2.0                                                                              2.0

                    Standard normal quantile1.0                                                      1.0
                                                                           Standard normal quantile
                    0                                                                                0

                    -1.0                                                                             -1.0

                    -2.0                                                                             -2.0

                            10 20 30                                                                            2.0 3.0 5.0
                          Discovery time (min)                                                             ln Discovery time (ln(min))

                    Figure 4.32 Normal plots for discovery times and log discovery times

                         The logarithmic transformation was useful in the preceding example in reducing
                    the skewness of a response distribution. Some other transformations commonly
                    employed to change the shape of a response distribution in statistical engineering
                    studies are the power transformations,

             Power        g(y) = (y -  )                                                                   (4.34)
transformations

                    In transformation (4.34), the number  is often taken as a threshold value, corre-
                    sponding to a minimum possible response. The number  governs the basic shape
                    of a plot of g(y) versus y. For  > 1, transformation (4.34) tends to lengthen the
                    right tail of a distribution for y. For 0 <  < 1, the transformation tends to shorten
                    the right tail of a distribution for y, the shortening becoming more drastic as  ap-
                    proaches 0 but not as pronounced as that caused by the logarithmic transformation

    Logarithmic           g(y) = ln(y -  )
transformation

4.4.2               Transformations and Multiple Samples

                    Comparing several sets of process conditions is one of the fundamental problems of
                    statistical engineering analysis. It is advantageous to do the comparison on a scale
                    where the samples have comparable variabilities, for at least two reasons. The first
                    is the obvious fact that comparisons then reduce simply to comparisons between
                    response means. Second, standard methods of statistical inference often have well-
                    understood properties only when response variability is comparable for the different
                    sets of conditions.
194 Chapter 4 Describing Relationships Between Variables

  Transformations           When response variability is not comparable under different sets of conditions,
         to stabilize  a transformation can sometimes be applied to all observations to remedy this. This
                       possibility of transforming to stabilize variance exists when response variance is
response variance      roughly a function of response mean. Some theoretical calculations suggest the fol-
                       lowing guidelines as a place to begin looking for an appropriate variance-stabilizing
                       transformation:

                            1. If response standard deviation is approximately proportional to response
                                mean, try a logarithmic transformation.

                            2. If response standard deviation is approximately proportional to the  power
                                of the response mean, try transformation (4.34) with  = 1 - .

                       Where several samples (and corresponding y¯ and s values) are involved, an empirical
                       way of investigating whether (1) or (2) above might be useful is to plot ln(s) versus
                       ln(y¯) and see if there is approximate linearity. If so, a slope of roughly 1 makes (1)
                       appropriate, while a slope of  = 1 signals what version of (2) might be helpful.

                            In addition to this empirical way of identifying a potentially variance-stabilizing
                       transformation, theoretical considerations can sometimes provide guidance. Stan-
                       dard theoretical distributions (like those introduced in Chapter 5) have their own
                       relationships between their (theoretical) means and variances, which can help pick
                       out an appropriate version of (1) or (2) above.

4.4.3                  Transformations and Simple Structure
                       in Multifactor Studies

                       In Section 4.2, Taylor's equation for tool life y in terms of cutting speed x was
                       advantageously reexpressed as a linear equation for ln(y) in terms of ln(x). This is
                       just one manifestation of the general fact that many approximate laws of physical
                       science and engineering are power laws, expressing one quantity as a product of a
                       constant and powers (some possibly negative) of other quantities. That is, they are
                       of the form

A power law            y  x11 x22 · · · xkk                                   (4.35)

                       Of course, upon taking logarithms in equation (4.35),

                       ln(y)  ln() + 1 ln(x1) + 2 ln(x2) + · · · + k ln(xk)   (4.36)

                       which immediately suggests the wide usefulness of the logarithmic transformation
                       for both y and x variables in surface-fitting applications involving power laws.

                            But there is something else in display (4.36) that bears examination: The k func-
                       tions of the fundamental x variables enter the equation additively. In the language of
                       the previous section, there are no interactions between the factors whose levels are
                       specified by the variables x1, x2, . . . , xk. This suggests that even in studies involving
                       only seemingly qualitative factors, if a power law for y is at work and the factors
                                   4.4 Transformations and Choice of Measurement Scale 195

            act on different fundamental variables x, a logarithmic transformation will tend to
            create a simple structure. It will do so by eliminating the need for interactions in
            describing the response.

Example 12  Daniel's Drill Advance Rate Study

            In his book Applications of Statistics to Industrial Experimentation, Cuthbert
            Daniel gives an extensive discussion of an unreplicated 24 factorial study of the
            behavior of a new piece of drilling equipment. The response y is a rate of advance
            of the drill (no units are given), and the experimental factors are Load on the small
            stone drill (A), Flow Rate through the drill (B), Rotational Speed (C), and Type
            of Mud used in drilling (D). Daniel's data are given in Table 4.24.

                 Application of the Yates algorithm to the data in Table 4.24 ( p = 4 cycles are
            required, as is division of the results of the last cycle by 24) gives the fitted effects:

                  y¯ .... = 6.1550       b2 = 1.6488       c2 = 3.2163         d2 = 1.1425
                   a2 = .4563         ac22 = .2975      ad22 = .4213      bcd222 = .0900
                 ab22 = .0750         bd22 = .2213      cd22 = .7987
                 bc22 = .7525       abd222 = .2950    acd222 = .3775
              abc222 = .0838
            abcd2222 = .2688

            Looking at the magnitudes of these fitted effects, the candidate relationships

                                    y  µ + j + k + l                      (4.37)

            and

                                    y  µ + j + k + l + jk +  kl           (4.38)

                 Table 4.24
                 Daniel's 24 Drill Advance Rate Data

                 Combination y                        Combination    y

                 (1)                1.68              d             2.07
                                                                    2.44
                 a                  1.98              ad            4.09
                                                                    4.53
                 b                  3.28              bd            7.77
                                                                    9.43
                 ab                 3.44              abd          11.75
                                                                   16.30
                 c                  4.98              cd

                 ac                 5.70              acd

                 bc                 9.97              bcd

                 abc                9.07              abcd
196 Chapter 4 Describing Relationships Between Variables

Example 12     are suggested. (The five largest fitted effects are, in order of decreasing magnitude,
 (continued )  the main effects of C, B, and D, and then the two-factor interactions of C with D
               and B with C.) Fitting equation (4.37) to the balanced data of Table 4.24 produces
               R2 = .875, and fitting relationship (4.38) produces R2 = .948. But upon closer
               examination, neither fitted equation turns out to be a very good description of
               these data.

                    Figure 4.33 shows a normal plot and a plot against y^ for residuals from
               a fitted version of equation (4.37). It shows that the fitted version of equation
               (4.37) produces several disturbingly large residuals and fitted values that are
               systematically too small for responses that are small and large, but too large for
               moderate responses. Such a curved plot of residuals versus y^ in general suggests
               that a nonlinear transformation of y may potentially be effective.

                    The reader is invited to verify that residual plots for equation (4.38) look even
               worse than those in Figure 4.33. In particular, it is the bigger responses that are

               Standard normal quantile   2.0

               Residual                   1.0

                                             0

                                         -1.0

                                         -2.0
                                                       -1.0 0.0 1.0 2.0 3.0 4.0
                                                                        Residual quantile

                                          4.0
                                          3.0
                                          2.0
                                          1.0

                                             0
                                         -1.0
                                         -2.0

                                                        2.0 4.0 6.0 8.0 10.0 12.0
                                                                        Fitted response, y

                                       Figure 4.33 Residual plots from fitting equation (4.37) to
                                       Daniel's data
                          4.4 Transformations and Choice of Measurement Scale 197

                          2.0

Standard normal quantile  1.0

                          0

                          -1.0

                          -2.0

                                -.2 -.1 0        .1      .2

                                Residual quantile

                          .2

                          .1

Residual                    0
                          -.1

                          -.2

                                1.0              2.0

                                Fitted response, ln( y)

Figure 4.34 Residual plots from fitting equation (4.39)
to Daniel's data

fitted relatively badly by relationship (4.38). This is an unfortunate circumstance,
since presumably one study goal is the optimization of response.

     But using y = ln(y) as a response variable, the situation is much different.
The Yates algorithm produces the following fitted effects.

     y .... = 1.5977                 b2 = .2900       c2 = .5772       d2 = .1633
       a2 = .0650                 ac22 = .0052     ad22 = .0334   bcd222 = -.0173
                                  bd22 = -.0075    cd22 = .0491
     ab22 = -.0172              abd222 = .0261   acd222 = .0266
     bc22 = -.0251
  abc222 = .0052
abcd2222 = .0193
198 Chapter 4 Describing Relationships Between Variables

Example 12     For the logged drill advance rates, the simple relationship
 (continued )                                  ln(y)  µ + j + k + l

                                                                            (4.39)

               yields R2 = .976 and absolutely unremarkable residuals. Figure 4.34 shows a
               normal plot of these and a plot of them against ln(y).

                    The point here is that the logarithmic scale appears to be the natural one on
               which to study drill advance rate. The data can be better described on the log
               scale without using interaction terms than is possible with interactions on the
               original scale.

                    There are sometimes other reasons to consider a logarithmic transformation of
               a response variable in a multifactor study, besides its potential to produce simple
               structure. In cases where responses vary over several orders of magnitude, simple
               curves and surfaces typically don't fit raw y values very well, but they can do a much
               better job of fitting ln(y) values (which will usually vary over less than a single order
               of magnitude). Another potentially helpful property of a log-transformed analysis
               is that it will never yield physically impossible negative fitted values for a positive
               variable y. In contrast, an analysis on an original scale of measurement can, rather
               embarrassingly, do so.

Example 13     A 32 Factorial Chemical Process Experiment
               The data in Table 4.25 are from an article by Hill and Demler ("More on Plan-
               ning Experiments to Increase Research Efficiency," Industrial and Engineering
               Chemistry, 1970). The data concern the running of a chemical process where
               the objective is to achieve high yield y1 and low filtration time y2 by choosing
               settings for Condensation Temperature, x1, and the Amount of B employed, x2.

                    For purposes of this example, consider the second response, filtration time.
               Fitting the approximate (quadratic) relationship

                                 y2  0 + 1x1 + 2x2 + 3x12 + 4x22 + 5x1x2

               to these data produces the equation

                y^ 2 = 5179.8 - 56.90x1 - 146.0x2 + .1733x12 + 1.222x22 + .6837x1x2 (4.40)

               and R2 = .866. Equation (4.40) defines a bowl-shaped surface in three dimen-
               sions, which has a minimum at about the set of conditions x1 = 103.2C and
               x2 = 30.88 cc. At first glance, it might seem that the development of equation
4.4 Transformations and Choice of Measurement Scale 199

Table 4.25
Yields and Filtration Times in a 32 Factorial Chemical

Process Study

        x1,         x2,      y1,       y2,
  Condensation   Amount     Yield  Filtration
Temperature (C)
                 of B (cc)   (g)   Time (sec)

90               24.4       21.1   150

90               29.3       23.7   10

90               34.2       20.7   8

100              24.4       21.1   35

100              29.3       24.1   8

100              34.2       22.2   7

110              24.4       18.4   18

110              29.3       23.4   8

110              34.2       21.9   10

(4.40) rates as a statistical engineering success story. But there is the embarrass-
ing fact that upon substituting x1 = 103.2 and x2 = 30.88 into equation (4.40),
one gets y^ 2 = -11 sec, hardly a possible filtration time.

     Looking again at the data, it is not hard to see what has gone wrong. The
largest response is more than 20 times the smallest. So in order to come close to
fitting both the extremely large and more moderate responses, the fitted quadratic
surface needs to be very steep--so steep that it is forced to dip below the (x1, x2)-
plane and produce negative y^ 2 values before it can "get turned around" and start
to climb again as it moves away from the point of minimum y^ 2 toward larger x1
and x2.

     One cure for the problem of negative predicted filtration times is to use ln(y2)
as a response variable. Values of ln(y2) are given in Table 4.26 to illustrate the
moderating effect the logarithm has on the factor of 20 disparity between the
largest and smallest filtration times.

     Fitting the approximate quadratic relationship

                ln(y2)  0 + 1x1 + 2x2 + 3x12 + 4x22 + 5x1x2

to the ln(y2) values produces the equation

 ln(y2) = 99.69 - .8869x1 - 3.348x2 + .002506x12 + .03375x22 + .01196x1x2
                                                                                         (4.41)

and R2 = .975. Equation (4.41) also represents a bowl-shaped surface in three
dimensions and has a minimum approximately at the set of conditions x1 =
101.5C and x2 = 31.6 cc. The minimum fitted log filtration time is ln(y2) =
1.7582 ln (sec), which translates to a filtration time of 5.8 sec, a far more sensible
value than the negative one given earlier.
200 Chapter 4 Describing Relationships Between Variables

Example 13     Table 4.26
 (continued )  Raw Filtration Times and Corresponding Logged Filtration
               Times

                         y2,                                           ln(y2),
               Filtration Time (sec)                      Log Filtration Time (ln(sec))

               150                                        5.0106

               10                                         2.3026

               8                                          2.0794

               35                                         3.5553

               8                                          2.0794

               7                                          1.9459

               18                                         2.8904

               8                                          2.0794

               10                                         2.3026

                       The taking of logs in this example had two beneficial effects. The first was to
                  cut the ratio of largest response to smallest down to about 2.5 (from over 20), al-
                  lowing a good fit (as measured by R2) for a fitted quadratic in two variables, x1 and
                  x2. The second was to ensure that minimum predicted filtration time was positive.

                    Of course, other transformations besides the logarithmic one are also useful in
               describing the structure of multifactor data sets. Sometimes they are applied to the
               responses and sometimes to other system variables. As an example of a situation
               where a power transformation like that specified by equation (4.34) is useful in
               understanding the structure of a sample of bivariate data, consider the following.

Example 14     Yield Strengths of Copper Deposits and Hall-Petch Theory

               In their article "Mechanical Property Testing of Copper Deposits for Printed Cir-
               cuit Boards" (Plating and Surface Finishing, 1988), Lin, Kim, and Weil present
               some data relating the yield strength of electroless copper deposits to the aver-
               age grain diameters measured for these deposits. The values in Table 4.27 were
               deduced from a scatterplot in their paper. These values are plotted in Figure
               4.35. They don't seem to promise a simple relationship between grain diameter
               and yield strength. But in fact, the so called Hall-Petch relationship says that
               yield strengths of most crystalline materials are proportional to the reciprocal
               square root of grain diameter. That is, Hall-Petch theory predicts a linear rela-
               tionship between y and x-.5 or between x and y-2. Thus, before trying to further
               detail the relationship between the two variables, application of transformation
               (4.34) with  = -.5 to x or transformation (4.34) with  = -2 to y seems in
               order. Figure 4.36 shows the partial effectiveness of the reciprocal square root
               transformation (applied to x) in producing a linear relationship in this context.
                           4.4 Transformations and Choice of Measurement Scale 201

Table 4.27
Average Grain Diameters and Yield Strengths for Copper Deposits

x, Average Grain                y,       x, Average Grain                        y,

Diameter (µm) Yield Strength (MPa) Diameter (µm) Yield Strength (MPa)

.22                             330          .48                                 236

.27                             370          .49                                 224

.33                             266          .51                                 236

.41                             270          .90                                 210

     Yield strength (MPa)  350

                           300

                           250

                           200  .2   .4  .6  .8

                                Average grain diameter (µm)

                           Figure 4.35 Scatterplot of yield strength versus
                           average grain diameter

     Yield strength (MPa)  350

                           300

                           250

                           200
                                1 1.2 1.4 1.6 1.8 2.0 2.2
                                        Reciprocal square root grain diameter

                           Figure 4.36 Scatterplot of yield strength versus the
                           reciprocal square root average grain diameter
202 Chapter 4 Describing Relationships Between Variables

The goal of data       In the preceding example, a directly applicable and well-known physical theory
  transformation  suggests a natural transformation. Sometimes physical or mathematical consider-
                  ations that are related to a problem, but do not directly address it, may also suggest
                  some things to try in looking for transformations to produce simple structure. For
                  example, suppose some other property besides yield strength were of interest and
                  thought to be related to grain size. If a relationship with diameter is not obvious,
                  quantifying grain size in terms of cross-sectional area or volume might be considered,
                  and this might lead to squaring or cubing a measured diameter. To take a different
                  example, if some handling characteristic of a car is thought to be related to its speed
                  and a relationship with velocity is not obvious, you might remember that kinetic
                  energy is related to velocity squared, thus being led to square the velocity.

                       To repeat the main point of this section, the search for appropriate transforma-
                  tions is a quest for measurement scales on which structure is transparent and simple.
                  If the original/untransformed scales are the most natural ones on which to report the
                  findings of a study, the data analysis should be done on the transformed scales but
                  then "untransformed" to state the final results.

Section 4 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. What are benefits that can sometimes be derived      will there be important interactions? (In order to
   from transforming data before applying standard
   statistical techniques?                              make this concrete, you may if you wish consider
                                                        the relationship y  kx12x2-3. Plot, for at least two
2. Suppose that a response variable, y, obeys an ap-    different values of x2, y as a function of x1. Then
   proximate power law in at least two quantitative     plot, for at least two different values of x2, ln(y) as
   variables (say, x1 and x2). Will there be important  a function of x1. What do these plots show in the
   interactions? If the log of y is analyzed instead,   way of parallelism?)

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

4.5 Beyond Descriptive Statistics

                    We hope that these first four chapters have made you genuinely ready to accept
                    the need for methods of formal statistical inference. Many real data sets have been
                    examined, and many instances of useful structure have been discovered--this in spite
                    of the fact that the structure is often obscured by what might be termed background
                    noise. Recognizing the existence of such variation, one realizes that the data in hand
                    are probably not a perfect representation of the population or process from which
                    they were taken. Thus, generalizing from the sample to a broader sphere will have
                    to be somehow hedged. To this point, the hedging has been largely verbal, specific
                    to the case, and qualitative. There is a need for ways to quantitatively express the
                    precision and reliability of any generalizations about a population or process that
                    are made from data in hand. For example, the chemical filtration time problem of
                    Example 13 produced the conclusion that with the temperature set at 101.5C and
                    using 31.6 cc of B, a predicted filtration time is 5.8 sec. But will the next time be
                    5.8 sec ± 3 sec or ± .05 sec? If you decide on ± somevalue, how sure can you be
                    of those tolerances?
                                                                                    Chapter 4 Exercises 203

                   In order to quantify precision and reliability for inferences based on samples,
              the mathematics of probability must be employed. Mathematical descriptions of
              data generation that are applicable to the original data collection (and any future
              collection) are necessary. Those mathematical models must explicitly allow for the
              kind of variation that has been faced in the last two chapters.

                   The models that are most familiar to engineers do not explicitly account for
              variation. Rather, they are deterministic. For example, Newtonian physics predicts
              that the displacement of a body in free fall in a time t is exactly 12 gt2. In this
              statement, there is no explicit allowance for variability. Any observed deviation
              from the Newtonian predictions is completely unaccounted for. Thus, there is really
              no logical framework in which to extrapolate from data that don't fit Newtonian
              predictions exactly.

                   Stochastic (or probabilistic) models do explicitly incorporate the feature that
              even measurements generated under the same set of conditions will exhibit variation.
              Therefore, they can function as descriptions of real-world data collection processes,
              where many small, unidentifiable causes act to produce the background noise seen
              in real data sets. Variation is predicted by stochastic or probabilistic models. So they
              provide a logical framework in which to quantify precision and reliability and to
              extrapolate from noisy data to contexts larger than the data set in hand.

                   In the next chapter, some fundamental concepts of probability will be introduced.
              Then Chapter 6 begins to use probability as a tool in statistical inference.

Section 5 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Read again Section 1.4 and the present one. Then        Give an example of a deterministic model that is
   describe in your own words the difference between       useful in your field.
   deterministic and stochastic/probabilistic models.

Chapter 4 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Nicholson and Bartle studied the effect of the wa-      (b) Compute the sample correlation between x and
   ter/cement ratio on 14-day compressive strength
   for Portland cement concrete. The water/cement          y by hand. Interpret this value.
   ratios (by volume) and compressive strengths of
   nine concrete specimens are given next.                 (c) What fraction of the raw variability in y is

                                                           accounted for in the fitting of a line to the data?

                                                           (d) Compute the residuals from your fitted line and

                                                           make a normal plot of them. Interpret this plot.

Water/Cement  14-Day Compressive                           (e) What compressive strength would you predict,
   Ratio, x   Strength, y (psi)
                                                           based on your calculations from (a), for speci-
      .45     2954, 2913, 2923
      .50     2743, 2779, 2739                             mens made using a .48 water/cement ratio?
      .55     2652, 2607, 2583
                                                           (f) Use a statistical package to find the least
                                                               squares line, the sample correlation, R2, and

                                                           the residuals for this data set.

                                                       2.  Griffith  and  Tesdall  studied  the  elapsed  time  in  1
                                                                                                                    4
(a) Fit a line to the data here via least squares,         mile runs of a Camaro Z-28 fitted with different
    showing the hand calculations.
204 Chapter 4 Describing Relationships Between Variables

sizes of carburetor jetting. Their data from six runs          (a) What type of data structure did the researchers
of the car follow:                                                 employ? (Use the terminology of Section 1.2.)
                                                                   What was an obvious weakness in their data
Jetting Size, x  Elapsed Time, y (sec)                             collection plan?

       66                 14.90                               (b) Use a regression program to fit the following
       68                 14.67                                    equations to these data:
       70                 14.50
       72                 14.53                                        y  0 + 1x1 + 2x2
       74                 14.79
       76                 15.02                                        y  0 + 1x1 + 2 ln(x2)

    (a) What is an obvious weakness in the students'                   y  0 + 1x1 + 2 ln(x2) + 3x1 ln(x2)
        data collection plan?
                                                                   What are the R2 values for the three differ-
   (b) Fit both a line and a quadratic equation (y                 ent fitted equations? Compare the three fitted
        0 + 1x + 2x2) to these data via least                      equations in terms of complexity and apparent
        squares. Plot both of these equations on a scat-           ability to predict y.
        terplot of the data.                                   (c) Compute the residuals for the third fitted equa-
                                                                   tion in (b). Plot them against x1, x2, and y^ .
    (c) What fractions of the raw variation in elapsed             Also normal-plot them. Do any of these plots
        time are accounted for by the two different                suggest that the third fitted equation is inade-
        fitted equations?                                          quate as summary of these data? What, if any,
                                                                   possible improvement over the third equation
   (d) Use your fitted quadratic equation to predict an            is suggested by these plots?
        optimal jetting size (allowing fractional sizes).     (d) As a means of understanding the nature of the
                                                                   third fitted equation in (b), make a scatterplot
3. The following are some data taken from "Kinet-                  of y vs. x2 using a logarithmic scale for x2. On
   ics of Grain Growth in Powder-formed IN-792: A                  this plot, plot three lines representing y^ as a
   Nickel-Base Super-alloy" by Huda and Ralph (Ma-                 function of x2 for the three different values of
   terials Characterization, September 1990). Three                 x1. Qualitatively, how would a similar plot for
   different Temperatures, x1 (K), and three different             the second equation differ from this one?
   Times, x2 (min), were used in the heat treating of          (e) Using the third equation in (b), what mean
   specimens of a material, and the response                       grain diameter would you predict for x1 =
                                                                   1500 and x2 = 500?
y = mean grain diameter (µm)                                   (f) It is possible to ignore the fact that the Tem-
                                                                   perature and Time factors are quantitative and
was measured.                                                      make a factorial analysis of these data. Do so.
                                                                   Begin by making an interaction plot similar
Temperature, x1  Time, x2  Grain Size, y                           to Figure 4.22 for these data. Based on that
                                                                   plot, discuss the apparent relative sizes of the
      1443            20           5                               Time and Temperature main effects and the
      1443          120            6                               Time × Temperature interactions. Then com-
      1443         1320            9                               pute the fitted factorial effects (the fitted main
      1493            20         14                                effects and interactions).
      1493          120          17
      1493         1320          25                        4. The article "Cyanoacetamide Accelerators for the
      1543            20         29                           Epoxide/Isocyanate Reaction" by Eldin and Ren-
      1543          120          38                           ner (Journal of Applied Polymer Science, 1990)
      1543         1320          60
                                                                    Chapter 4 Exercises 205

reports the results of a 23 factorial experiment. Us-          values of y^ and compute R2 for the "A main ef-
ing cyanoacetamides as catalysts for an epoxy/iso-             fects only" description of impact strength. (The
cyanate reaction, various mechanical properties of             formula in Definition 3 works in this context
a resulting polymer were studied. One of these was             as well as in regression.)
                                                          (d) Now recognize that the experimental factors
          y = impact strength (kJ/mm2)                         here are quantitative, so methods of curve and
                                                               surface fitting may be applicable. Fit the equa-
The three experimental factors employed and their              tion y  0 + 1(epoxy/isocyanate ratio) to
corresponding experimental levels were as follows:             the data. What eight values of y^ and value
                                                               of R2 accompany this fit?
Factor A  Initial Epoxy/Isocyanate Ratio
Factor B  0.4 (-) vs. 1.2 (+)                          5. Timp and M-Sidek studied the strength of mechan-
Factor C                                                  ical pencil lead. They taped pieces of lead to a desk,
          Flexibilizer Concentration                      with various lengths protruding over the edge of the
          10 mol % (-) vs. 40 mol % (+)                   desk. After fitting a small piece of tape on the free
                                                          end of a lead piece to act as a stop, they loaded it
          Accelerator Concentration                       with paper clips until failure. In one part of their
          1/240 mol % (-) vs. 1/30 mol% (+)               study, they tested leads of two different Diame-
                                                          ters, used two different Lengths protruding over
(The flexibilizer and accelerator concentrations are      the edge of the desk, and tested two different lead
relative to the amount of epoxy present initially.)       Hardnesses. That is, they ran a 23 factorial study.
The impact strength data obtained (one observation        Their factors and levels were as follows:
per combination of levels of the three factors) were
as follows:                                                Factor A Diameter .3 mm (-) vs. .7 mm (+)

Combination y   Combination y                              Factor B Length Protruding 3 cm (-) vs.
                                                                                                 4.5 cm (+)
(1)       6.7   c                       6.3
                                                           Factor C Hardness B (-) vs. 2H (+)
a         11.9  ac   15.1
                                                          and m = 2 trials were made at each of the 23 = 8
b         8.5   bc                      6.7               different sets of conditions. The data the students
                                                          obtained are given here.
ab        16.5  abc  16.4

(a) What is an obvious weakness in the researchers'    Combination  Number of Clips
     data collection plan?
                                                             (1)          13, 13
(b) Use the Yates algorithm and compute fitted fac-          a            74, 76
     torial effects corresponding to the "all high"          b            9, 10
     treatment combination (i.e., compute y¯ ..., a2,        ab           43, 42
     b2, etc.). Interpret these in the context of the        c            16, 15
     original study. (Describe in words which fac-           ac           89, 88
     tors and/or combinations of factors appear to           bc           10, 12
     have the largest effect(s) on impact strength           abc          54, 55
     and interpret the sign or signs.)
                                                       (a) It appears that analysis of these data in terms
(c) Suppose only factor A is judged to be of im-           of the natural logarithms of the numbers of
     portance in determining impact strength. What
     predicted/fitted impact strengths correspond to
     this judgment? (Find y^ values using the reverse
     Yates algorithm or otherwise.) Use these eight
206 Chapter 4 Describing Relationships Between Variables

        clips first causing failure is more straightfor-        (a) Compute the fitted 23 factorial effects (main
        ward than the analysis of the raw numbers of                effects, 2-factor interactions and 3-factor inter-
        clips. So take natural logs and compute the fit-            actions) corresponding to the following set of
        ted 23 factorial effects. Interpret these. In par-          conditions: 60 mesh, 500 cc, vibrated cylinder.
        ticular, what (in quantitative terms) does the
        size of the fitted A main effect say about lead        (b) If your arithmetic for part (a) is correct, you
        strength? Does lead hardness appear to play                 should have found that the largest of the fitted
        a dominant role in determining this kind of                 effects (in absolute value) are (respectively)
        breaking strength?                                          the C main effect, the A main effect, and then
   (b) Suppose only the main effects of Diameter are                the AC 2-factor interaction. (The next largest
        judged to be of importance in determining lead              fitted effect is only about half of the smallest
        strength. Find a predicted log breaking strength            of these, the AC interaction.) Now, suppose
        for .7 mm, 2H lead when the length protruding               you judge these three fitted effects to summa-
        is 4.5 cm. Use this to predict the number of                rize the main features of the data set. Interpret
        clips required to break such a piece of lead.               this data summary (A and C main effects and
    (c) What, if any, engineering reasons do you have               AC interactions) in the context of this 3-factor
        for expecting the analysis of breaking strength             study.
        to be more straightforward on the log scale than
        on the original scale?                                  (c) Using your fitted effects from (a) and the data
                                                                    summary from (b) (A and C main effects and
6. Ceramic engineering researchers Leigh and Taylor,                AC interactions), what fitted response would
   in their paper "Computer Generated Experimen-                    you have for these conditions: 60 mesh, 500
   tal Designs" (Ceramic Bulletin, 1990), studied the               cc, vibrated cylinder?
   packing properties of crushed T-61 tabular alumina
   powder. The densities of batches of the material            (d) Using your fitted effects from (a), what average
   were measured under a total of eight different sets              change in density would you say accompanies
   of conditions having a 23 factorial structure. The               the vibration of the graduated cylinder before
   following factors and levels were employed in the                density determination?
   study:
                                                            7. The article "An Analysis of Transformations" by
Factor A  Mesh Size of Powder Particles                        Box and Cox (Journal of the Royal Statistical So-
Factor B  6 mesh (-) vs. 60 mesh (+)                           ciety, Series B, 1964) contains a classical unrepli-
Factor C                                                       cated 33 factorial data set originally taken from an
          Volume of Graduated Cylinder                         unpublished technical report of Barella and Sust.
          100 cc (-) vs. 500 cc (+)                            These researchers studied the behavior of worsted
                                                               yarns under repeated loading. The response vari-
          Vibration of Cylinder                                able was
          no (-) vs. yes (+)
                                                                        y = the numbers of cycles till failure
The mean densities (in g/cc) obtained in m = 5
determinations for each set of conditions were as              for specimens tested with various values of
follows:
                                                                    x1 = length (mm)
y¯ (1) = 2.348  y¯ a = 2.080
y¯ b = 2.298    y¯ ab = 1.980                                       x2 = amplitude of the loading cycle (mm)
y¯ c = 2.354    y¯ ac = 2.314
y¯ bc = 2.404   y¯ abc = 2.374                                      x3 = load (g)
                                                        Chapter 4 Exercises 207

The researchers' data are given in the accompany-               to the data. What fraction of the observed vari-
ing table.                                                      ability in y = ln(y) does this equation account
                                                                for? What change in y seems to accompany a
x1 x2 x3   y  x1 x2 x3  y                                       unit (a 1 ln(g)) increase in x3?
                                                            (c) To carry the analysis one step further, note that
250 8 40 674 300 9 50 438                                       your fitted coefficients for x1 and x2 are nearly
                                                                the negatives of each other. That suggests that
250 8 45 370 300 10 40 442                                       y depends only on the difference between x1
                                                                and x2. To see how this works, fit the equation
250 8 50 292 300 10 45 332
                                                                          y  0 + 1(x1 - x2) + 2x3
250 9 40 338 300 10 50 220
                                                                to the data. Compute and plot residuals from
250 9 45 266 350 8 40 3,636                                     this relationship (still on the log scale). How
                                                                does this relationship appear to do as a data
250 9 50 210 350 8 45 3,184                                     summary? What power law for y (on the orig-
                                                                inal scale) in terms of x1, x2, and x3 (on their
250 10 40 170 350 8 50 2,000                                    original scales) is implied by this last fitted
                                                                equation? How does this equation compare to
250 10 45 118 350 9 40 1,568                                    the one from (a) in terms of parsimony?
                                                           (d) Use your equation from (c) to predict the life
250 10 50  90 350 9 45 1,070                                    of an additional specimen of length 300 mm, at
                                                                an amplitude of 9 mm, under a load of 45 g. Do
300 8 40 1,414 350 9 50 566                                     the same for an additional specimen of length
                                                                325 mm, at an amplitude of 9.5 mm, under
300 8 45 1,198 350 10 40 1,140                                  a load of 47 g. Why would or wouldn't you
                                                                be willing to make a similar projection for an
300 8 50 634 350 10 45 884                                      additional specimen of length 375 mm, at an
                                                                amplitude of 10.5 mm, under a load of 51 g?
300 9 40 1,022 350 10 50 360
                                                        8. Bauer, Dirks, Palkovic, and Wittmer fired tennis
300 9 45 620                                               balls out of a "Polish cannon" inclined at an angle
                                                           of 45, using three different Propellants and two
(a) To find an equation to represent these data,           different Charge Sizes of propellant. They observed
    you might first try to fit multivariable polyno-       the distances traveled in the air by the tennis balls.
    mials. Use a regression program and fit a full         Their data are given in the accompanying table.
    quadratic equation to these data. That is, fit         (Five trials were made for each Propellant/Charge
                                                           Size combination and the values given are in feet.)
     y  0 + 1x1 + 2x2 + 3x3 + 4x12 + 5x22

          +6x32 + 7x1x2 + 8x1x3 + 9x2x3

     to the data. What fraction of the observed vari-
     ation in y does it account for? In terms of par-
     simony (or providing a simple data summary),
     how does this quadratic equation do as a data
     summary?
(b) Notice the huge range of values of the response
     variable. In cases like this, where the response
     varies over an order of magnitude, taking log-
     arithms of the response often helps produce a
     simple fitted equation. Here, take (natural) log-
     arithms of all of x1, x2, x3, and y, producing
     (say) x1, x2, x3, and y , and fit the equation

              y  0 + 1x1 + 2x2 + 3x3
208 Chapter 4 Describing Relationships Between Variables

                        Propellant                       Combination Pull-Outs Combination Pull-Outs

             Lighter                Carburetor            (1)  9   c     13

             Fluid Gasoline Fluid                         a    70  ac    55

             58         76          90                    b    8   bc    7

             50         79          86                    ab   42  abc   19

             2.5 ml 53  84          79                    d    3   cd    5

             49         73          82                    ad   6   acd   28

             59         71          86                    bd   1   bcd   3

Charge Size                                               abd  7   abcd  6

             65         96          107                      (a) Use the Yates algorithm and identify dominant
                                                                 effects here.
             59         101         102
                                                            (b) Based on your analysis from (a), postulate a
             5.0 ml 61  94          91                           possible "few effects" explanation for these
                                                                 data. Use the reverse Yates algorithm to find
             68         91          95                           fitted responses for such a simplified descrip-
                                                                 tion of the system. Use the fitted values to com-
             67         87          97                           pute residuals. Normal-plot these and plot them
                                                                 against levels of each of the four factors, look-
   Complete a factorial analysis of these data, includ-          ing for obvious problems with your represen-
   ing a plot of sample means useful for judging the             tation of system behavior.
   size of Charge Size × Propellant interactions and
   the computing of fitted main effects and interac-         (c) Based on your "few effects" description of
   tions. Write a paragraph summarizing what these               bond strength, make a recommendation for the
   data seem to say about how these two variables                future making of these devices. (All else being
   affect flight distance.                                       equal, you should choose what appear to be the
                                                                 least expensive levels of factors.)
9. The following data are taken from the article "An
   Analysis of Means for Attribute Data Applied to       10. Exercise 5 of Chapter 3 concerns a replicated 33
   a 24 Factorial Design" by R. Zwickl (ASQC Elec-            factorial data set (weighings of three different
   tronics Division Technical Supplement, Fall 1985).         masses on three different scales by three differ-
   They represent numbers of bonds (out of 96) show-          ent students). Use a full-featured statistical pack-
   ing evidence of ceramic pull-out on an electronic          age that will compute fitted effects for such data
   device called a dual in-line package. (Low num-            and write a short summary report stating what
   bers are good.) Experimental factors and their lev-        those fitted effects reveal about the structure of
   els were:                                                  the weighings data.

    Factor A Ceramic Surface                             11. When it is an appropriate description of a two-
                 unglazed (-) vs. glazed (+)                  way factorial data set, what practical engineering
                                                              advantages does a "main effects only" descrip-
    Factor B Metal Film Thickness                             tion offer over a "main effects plus interactions"
                 normal (-) vs. 1.5 times normal (+)          description?

    Factor C Annealing Time                              12. The article referred to in Exercise 4 of Section
                 normal (-) vs. 4 times normal (+)            4.1 actually considers the effects of both cutting
                                                              speed and feed rate on tool life. The whole data
    Factor D Prebond Clean
                 normal clean (-) vs. no clean (+)

   The resultant numbers of pull-outs for the 24 treat-
   ment combinations are given next.
                                                                    Chapter 4 Exercises 209

set from the article follows. (The data in Section       13. K. Casali conducted a gas mileage study on his
4.1 are the x2 = .01725 data only.)                           well-used four-year-old economy car. He drove
                                                              a 107-mile course a total of eight different times
Cutting Speed,   Feed,    Tool Life, y (min)                  (in comparable weather conditions) at four differ-
   x1 (sfpm)    x2 (ipr)                                      ent speeds, using two different types of gasoline,
                          1.00, 0.90, 0.74, 0.66              and ended up with an unreplicated 4 × 2 factorial
      800       .01725    1.00, 1.20, 1.50, 1.60              study. His data are given in the table below.
      700       .01725    1.75, 1.85, 2.00, 2.20
      700       .01570    1.20, 1.50, 1.60, 1.60            Speed Gasoline Gallons Mileage
      600       .02200    2.35, 2.65, 3.00, 3.60
      600       .01725    6.40, 7.80, 9.80, 16.50        Test (mph) Octane  Used  (mpg)
      500       .01725    8.80, 11.00, 11.75, 19.00
      500       .01570    4.00, 4.70, 5.30, 6.00         1  65  87          3.2   33.4
      450       .02200    21.50, 24.50, 26.00, 33.00
      400       .01725                                   2  60  87          3.1   34.5

                                                         3  70  87          3.4   31.5

                                                         4  55  87          3.0   35.7

                                                         5  65  90          3.2   33.4

(a) Taylor's expanded tool life equation is              6  55  90          2.9   36.9
     yx11 x22 = C. This relationship suggests that
    ln(y) may well be approximately linear in            7  70  90          3.3   32.4

    both ln(x1) and ln(x2). Use a multiple linear        8  60  90          3.0   35.7
    regression program to fit the relationship
                                                         (a) Make a plot of the mileages that is useful for
           ln(y)  0 + 1 ln(x1) + 2 ln(x2)                     judging the size of Speed × Octane interac-
                                                              tions. Does it look as if the interactions are
     to these data. What fraction of the raw vari-            large in comparison to the main effects?
     ability in ln(y) is accounted for in the fitting
     process? What estimates of the parameters 1,        (b) Compute the fitted main effects and interac-
     2, and C follow from your fitted equation?               tions for the mileages, using the formulas of
(b) Compute and plot residuals (continuing to                 Section 4.3. Make a plot like Figure 4.23
     work on log scales) for the equation you fit             for comparing the observed mileages to fit-
     in part (a). Make at least plots of residuals            ted mileages computed supposing that there
     versus fitted ln(y) and both ln(x1) and ln(x2),          are no Speed × Octane interactions.
     and make a normal plot of these residuals.
     Do these plots reveal any particular problems       (c) Now fit the equation
     with the fitted equation?
(c) Use your fitted equation to predict first a log            Mileage  0 + 1(Speed) + 2(Octane)
     tool life and then a tool life, if in this machin-
     ing application a cutting speed of 550 and a             to the data and plot lines representing the pre-
     feed of .01650 is used.                                  dicted mileages versus Speed for both the 87
(d) Plot the ordered pairs appearing in the data              octane and the 90 octane gasolines on the
     set in the (x1, x2)-plane. Outline a region in           same set of axes.
     the plane where you would feel reasonably           (d) Now fit the equation Mileage  0 + 1
     safe using the equation you fit in part (a) to           (Speed) separately, first to the 87 octane data
     predict tool life.                                       and then to the 90 octane data. Plot the two
                                                              different lines on the same set of axes.
                                                         (e) Discuss the different appearances of the plots
                                                              you made in parts (a) through (d) of this exer-
                                                              cise in terms of how well they fit the original
210 Chapter 4 Describing Relationships Between Variables

          data and the different natures of the assump-             2, and C follow from your fitted equation?
          tions involved in producing them.                         Using your estimates of 1, 2, and C, plot on
      (f) What was the fundamental weakness in                      the same set of (x1, y) axes the functional re-
          Casali's data collection scheme? A weakness               lationships between x1 and y implied by your
          of secondary importance has to do with the                fitted equation for x2 equal to 3,000, 6,000,
          fact that tests 1-4 were made ten days ear-               and then 10,000 psi, respectively.
          lier than tests 5-8. Why is this a potential         (b) Compute and plot residuals (continuing to
          problem?                                                  work on log scales) for the equation you fit
                                                                    in part (a). Make at least plots of residuals
14. The article "Accelerated Testing of Solid Film                  versus fitted ln(y) and both ln(x1) and ln(x2),
     Lubricants" by Hopkins and Lavik (Lubrication                  and make a normal plot of these residuals.
     Engineering, 1972) contains a nice example of                  Do these plots reveal any particular problems
     the engineering use of multiple regression. In the             with the fitted equation?
     study, m = 3 sets of journal bearing tests were           (c) Use your fitted equation to predict first a log
     made on a Mil-L-8937 type film at each combi-                  wear life and then a wear life, if in this appli-
     nation of three different Loads and three different            cation a speed of 20 rpm and a load of 10,000
     Speeds. The wear lives of journal bearings, y,                 psi are used.
     in hours, are given next for the tests run by the         (d) (Accelerated life testing) As a means of
     authors.                                                       trying to make intelligent data-based predic-
                                                                    tions of wear life at low stress levels (and
 Speed,   Load,     Wear Life, y (hs)                               correspondingly large lifetimes that would be
x1 (rpm)  x2 (psi)                                                  impractical to observe directly), you might
                    300.2, 310.8, 333.0                             (fully recognizing the inherent dangers of the
    20     3,000    99.6, 136.2, 142.4                              practice) try to extrapolate using the fitted
    20     6,000    20.2, 28.2, 102.7                               equation. Use your fitted equation to predict
    20    10,000    67.3, 77.9, 93.9                                first a log wear life and then a wear life if
    60     3,000    43.0, 44.5, 65.9                                a speed of 15 rpm and load of 1,500 psi are
    60     6,000    10.7, 34.1, 39.1                                used in this application.
    60    10,000    26.5, 22.3, 34.8
  100      3,000    32.8, 25.6, 32.7                      15. The article "Statistical Methods for Controlling
  100      6,000    2.3, 4.4, 5.8                              the Brown Oxide Process in Multilayer Board
  100     10,000                                               Processing" by S. Imadi (Plating and Surface
                                                               Finishing, 1988) discusses an experiment con-
(a) The authors expected to be able to describe                ducted to help a circuit board manufacturer mea-
                                                               sure the concentration of important components
    wear life as roughly following the relationship            in a chemical bath. Various combinations of lev-
     yx1x2 = C, but they did not find this relation-           els of
    ship to be a completely satisfactory model. So
                                                               x1 = % by volume of component A (a proprietary
    instead, they tried using the more general rela-                  formulation, the major component of which
    tionship yx11 x22 = C. Use a multiple linear                      is sodium chlorite)
    regression program to fit the relationship
                                                               and
      ln(y)  0 + 1 ln(x1) + 2 ln(x2)
                                                               x2 = % by volume of component B (a proprietary
to these data. What fraction of the raw vari-                         formulation, the major component of which
ability in ln(y) is accounted for in the fitting                      is sodium hydroxide)
process? What estimates of the parameters 1,
                                                                      Chapter 4 Exercises 211

were set in the chemical bath, and the variables                   regression program. Is this equation the same
                                                                   one you found in part (b)?
y1 = ml of 1N H2SO4 used in the first phase                   (d) If you were to compare the equations for x2
       of a titration                                              derived in (b) and (c) in terms of the sum
                                                                   of squared differences between the predicted
and                                                                and observed values of x2, which is guaran-
y2 = ml of 1N H2SO4 used in the second phase                       teed to be the winner? Why?

       of a titration                                    16. The article "Nonbloated Burned Clay Aggregate
                                                              Concrete" by Martin, Ledbetter, Ahmad, and Brit-
were measured. Part of the original data col-                 ton (Journal of Materials, 1972) contains data
lected (corresponding to bath conditions free of              on both composition and resulting physical prop-
Na2CO3) follow:                                               erty test results for a number of different batches
                                                              of concrete made using burned clay aggregates.
                 x1 x2 y1 y2                                  The accompanying data are compressive strength
                                                              measurements, y (made according to ASTM C 39
                 15 25 3.3 .4                                 and recorded in psi), and splitting tensile strength
                 20 25 3.4 .4                                 measurements, x (made according to ASTM C
                 20 30 4.1 .4                                 496 and recorded in psi), for ten of the batches
                 25 30 4.3 .3                                 used in the study.
                 25 35 5.0 .5
                 30 35 5.0 .3                            Batch  1  2  3  4  5
                 30 40 5.7 .5                            y
                 35 40 5.8 .4                            x      1420 1950 2230 3070 3060

(a) Fit equations for both y1 and y2 linear in both      Batch  207 233 254 328 325
     of the variables x1 and x2. Does it appear          y
     that the variables y1 and y2 are adequately         x      6  7  8  9  10
     described as linear functions of x1 and x2?
                                                                3110 2650 3130 2960 2760
(b) Solve your two fitted equations from (a) for x2
     (the concentration of primary interest here) in            302 258 335 315 302
     terms of y1 and y2. (Eliminate x1 by solving
     the first for x1 in terms of the other three vari-  (a) Make a scatterplot of these data and comment
     ables and plugging that expression for x1 into           on how linear the relation between x and y
     the second equation.) How does this equa-                appears to be for concretes of this type.
     tion seem to do in terms of, so to speak, pre-
     dicting x2 from y1 and y2 for the original          (b) Compute the sample correlation between x
     data? Chemical theory in this situation indi-            and y by hand. Interpret this value.
     cated that x2  8(y1 - y2). Does your equa-
     tion seem to do better than the one from chem-      (c) Fit a line to these data using the least squares
     ical theory?                                             principle. Show the necessary hand calcula-
                                                              tions. Sketch this fitted line on your scatter-
(c) A possible alternative to the calculations in             plot from (a).
     (b) is to simply fit an equation for x2 in terms
     of y1 and y2 directly via least squares. Fit        (d) About what increase in compressive strength
     x2  0 + 1 y1 + 2 y2 to the data, using a                 appears to accompany an increase of 1 psi in
                                                              splitting tensile strength?

                                                         (e) What fraction of the raw variability in com-
                                                              pressive strength is accounted for in the fitting
                                                              of a line to the data?

                                                         (f) Based on your answer to (c), what measured
                                                              compressive strength would you predict for a
212 Chapter 4 Describing Relationships Between Variables

          batch of concrete of this type if you were to     polymer concentration, x2, on percent recoveries
          measure a splitting tensile strength of 245 psi?  of pyrite, y1, and kaolin, y2, from a step of an ore
     (g) Compute the residuals from your fitted line.       refining process. (High pyrite recovery and low
          Plot the residuals against x and against y^ .
          Then make a normal plot of the residuals.         kaolin recovery rates were desirable.) Data from
          What do these plots indicate about the linear-    one set of n = 9 experimental runs are given here.
          ity of the relationship between splitting ten-
          sile strength and compressive strength?           x1 (rpm)  x2 (ppm)  y1 (%)  y2 (%)
     (h) Use a statistical package to find the least
          squares line, the sample correlation, R2, and       1350        80      77      67
          the residuals for these data.                        950        80      83      54
      (i) Fit the quadratic relationship y  0 + 1x +           600        80      91      70
          2x2 to the data, using a statistical package.       1350       100      80      52
          Sketch this fitted parabola on your scatterplot      950       100      87      57
          from part (a). Does this fitted quadratic ap-        600       100      87      66
          pear to be an important improvement over the        1350       120      67      54
          line you fit in (c) in terms of describing the       950       120      80      52
          relationship of y to x?                              600       120      81      44
      (j) How do the R2 values from parts (h) and (i)
          compare? Does the increase in R2 in part (i)      (a) What type of data structure did the researcher
          speak strongly for the use of the quadratic (as        use? (Use the terminology of Section 1.2.)
          opposed to linear) description of the relation-        What was an obvious weakness in his data
          ship of y to x for concretes of this type?             collection plan?
     (k) If you use the fitted relationship from part
          (i) to predict y for x = 245, how does the        (b) Use a regression program to fit the following
          prediction compare to your answer for part             equations to these data:
          (f)?
      (l) What do the fitted relationships from parts                         y1  0 + 1x1
          (c) and (i) give for predicted compressive
          strengths when x = 400 psi? Do these com-                           y1  0 + 2x2
          pare to each other as well as your answers to
          parts (f) and (k)? Why would it be unwise to                        y1  0 + 1x1 + 2x2
          use either of these predictions without further
          data collection and analysis?                          What are the R2 values for the three differ-
                                                                 ent fitted equations? Compare the three fitted
17. In the previous exercise, both x and y were really           equations in terms of complexity and appar-
     response variables. As such, they were not subject          ent ability to predict y1.
     to direct manipulation by the experimenters. That      (c) Compute the residuals for the third fitted
     made it difficult to get several (x, y) pairs with          equation in part (b). Plot them against x1,
     a single x value into the data set. In experimen-           x2, and y^ 1. Also normal-plot them. Do any of
     tal situations where an engineer gets to choose             these plots suggest that the third fitted equa-
     values of an experimental variable x, why is it             tion is inadequate as a summary of these data?
     useful/important to get several y observations for     (d) As a means of understanding the nature of
     at least some x's?                                          the third fitted equation from part (b), make a
                                                                 scatterplot of y1 vs. x2. On this plot, plot three
18. Chemical engineering graduate student S. Osoka               lines representing y^ 1 as a function of x2 for
     studied the effects of an agitator speed, x1, and a         the three different values of x1 represented in
                                                                 the data set.
                                                                            Chapter 4 Exercises 213

     (e) Using the third equation from part (b), what        Factor A Plane Design
          pyrite recovery rate would you predict for                      straight wing (-) vs. t wing (+)
          x1 = 1000 rpm and x2 = 110 ppm?
                                                             Factor B Nose Weight
      (f) Consider also a multivariable quadratic de-                     none (-) vs. paper clip (+)
          scription of the dependence of y1 on x1 and
          x2. That is, fit the equation                      Factor C Paper Type
                                                                          notebook (-) vs. construction (+)
                  y1  0 + 1x1 + 2x2 + 3x12
                                                             Factor D Wing Tips
                         +4x22 + 5x1x2                                    straight (-) vs. bent up (+)

          to the data. How does the R2 value here com-       The mean flight distances, y (ft), recorded by Fel-
          pare with the ones in part (b)? As a means of      lows for two launches of each plane were as shown
          understanding this fitted equation, plot on a      in the accompanying table.
          single set of axes the three different quadratic   (a) Use the Yates algorithm and compute the fit-
          functions of x2 obtained by holding x1 at one
          of the values in the data set.                          ted factorial effects corresponding to the "all
     (g) It is possible to ignore the fact that the speed         high" treatment combination.
          and concentration factors are quantitative and     (b) Interpret the results of your calculations from
          to make a factorial analysis of these y1 data.          (a) in the context of the study. (Describe in
          Do so. Begin by making an interaction plot              words which factors and/or combinations of
          similar to Figure 4.22 for these data. Based            factors appear to have the largest effect(s) on
          on that plot, discuss the apparent relative sizes       flight distance. What are the practical impli-
          of the Speed and Concentration main effects             cations of these effects?)
          and the Speed × Concentration interactions.
          Then compute the fitted factorial effects (the     Combination y  Combination y
          fitted main effects and interactions).
     (h) If the third equation in part (b) governed y1,      (1)  6.25      d     7.00
          would it lead to Speed × Concentration inter-
          actions? What about the equation in part (f)?      a    15.50     ad    10.00
          Explain.
                                                             b    7.00      bd    10.00
19. The data given in the previous exercise concern
     both responses y1 and y2. The previous analysis         ab   16.50     abd   16.00
     dealt with only y1. Redo all parts of the problem,
     replacing the response y1 with y2 throughout.           c    4.75      cd    4.50

20. K. Fellows conducted a 4-factor experiment, with         ac   5.50      acd   6.00
     the response variable the flight distance of a pa-
     per airplane when propelled from a launcher fab-        bc   4.50      bcd   4.50
     ricated specially for the study. This exercise con-
     cerns part of the data he collected, constituting       abc  6.00      abcd  5.75
     a complete 24 factorial. The experimental factors
     involved and levels used were as given here.            (c) Suppose factors B and D are judged to be
                                                                 inert as far as determining flight distance is
                                                                 concerned. (The main effects of B and D and
                                                                 all interactions involving them are negligi-
                                                                 ble.) What fitted/predicted values correspond
                                                                 to this description of flight distance (A and
                                                                 C main effects and AC interactions only)?
                                                                 Use these 16 values of y^ to compute residu-
                                                                 als, y - y^ . Plot these against y^ , levels of A,
                                                                 levels of B, levels of C, and levels of D. Also
214 Chapter 4 Describing Relationships Between Variables

          normal-plot these residuals. Comment on any           (b) What is the correlation between x1 and y?
          interpretable patterns in your plots.                      The correlation between x2 and y?
     (d) Compute R2 corresponding to the descrip-
          tion of flight distance used in part (c). (The        (c) Based on (a) and (b), describe how strongly
          formula in Definition 3 works in this context              Thickness and Hardness appear to affect bal-
          as well as in regression. So does the represen-            listic limit. Review the raw data and specu-
          tation of R2 as the squared sample correlation             late as to why the variable with the smaller
          between y and y^ .) Does it seem that the grand            influence on y seems to be of only minor im-
          mean, A and C main effects, and AC 2-factor                portance in this data set (although logic says
          interactions provide an effective summary of               that it must in general have a sizable influence
          flight distance?                                           on y).

21. The data in the accompanying table appear in the            (d) Compute the residuals for the third fitted
     text Quality Control and Industrial Statistics by               equation from (a). Plot them against x1, x2,
     Duncan (and were from a paper of L. E. Simon).                  and y^ . Also normal-plot them. Do any of
     The data were collected in a study of the effec-                these plots suggest that the third fitted equa-
     tiveness of armor plate. Armor-piercing bullets                 tion is seriously deficient as a summary of
     were fired at an angle of 40 against armor plate                these data?
     of thickness x1 (in .001 in.) and Brinell hardness
     number x2, and the resulting so-called ballistic           (e) Plot the (x1, x2) pairs represented in the data
     limit, y (in ft/sec), was measured.                             set. Why would it be unwise to use any of the
                                                                     fitted equations to predict y for x1 = 265 and
x1  x2  y  x1  x2  y                                                 x2 = 440?

253 317 927 253 407 1393                                   22. Basgall, Dahl, and Warren experimented with
258 321 978 252 426 1401                                        smooth and treaded bicycle tires of different
259 341 1028 246 432 1436                                       widths. Tires were mounted on the same wheel,
247 350 906 250 469 1327                                        placed on a bicycle wind trainer, and accelerated
256 352 1159 242 257 950                                        to a velocity of 25 miles per hour. Then pedaling
246 363 1055 243 302 998                                        was stopped, and the time required for the wheel
257 365 1335 239 331 1144                                       to stop rolling was recorded. The sample means,
262 375 1392 242 355 1080                                       y, of five trials for each of six different tires were
255 373 1362 244 385 1276                                       as follows:
258 391 1374 234 426 1062
                                                           Tire Width  Tread    Time to Stop, y (sec)
(a) Use a regression program to fit the following
    equations to these data:                                700/19c    smooth             7.30
                                                            700/25c    smooth             8.44
             y  0 + 1x1                                     700/32c    smooth             9.27
             y  0 + 2x2                                     700/19c    treaded            6.63
             y  0 + 1x1 + 2x2                               700/25c    treaded            6.87
                                                            700/32c    treaded            7.07
What are the R2 values for the three differ-
ent fitted equations? Compare the three fitted             (a) Carefully make an interaction plot of times
equations in terms of complexity and appar-                    required to stop, useful for investigating the
ent ability to predict y.                                      sizes of Width and Tread main effects and
                                                               Width × Tread interactions here. Comment
                                                               briefly on what the plot shows about these
                                                               effects. Be sure to label the plot very clearly.
                                                          Chapter 4 Exercises 215

     (b) Compute the fitted main effects of Width,                  5.00 km/sec detonation velocity, what PETN
          the fitted main effects of Tread, and the fit-            density would you employ?
          ted Width × Tread interactions from the y's.         (g) Compute the residuals from your fitted line.
          Discuss how they quantify features that are               Plot them against x and against y^ . Then make
          evident in your plot from (a).                            a normal plot of the residuals. What do these
                                                                    indicate about the linearity of the relationship
23. Below are some data read from a graph in the ar-                between y and x?
     ticle "Chemical Explosives" by W. B. Sudweeks             (h) Use a statistical package and compute the
     that appears as Chapter 30 in Riegel's Handbook                least squares line, the sample correlation, R2,
     of Industrial Chemistry. The x values are densities            and the residuals from the least squares line
     (in g/cc) of pentaerythritol tetranitrate (PETN)               for these data.
     samples and the y values are corresponding deto-
     nation velocities (in km/sec).                       24. Some data collected in a study intended to reduce
                                                               a thread stripping problem in an assembly process
x  y  x  y  x  y                                               follow. Studs screwed into a metal block were
                                                               stripping out of the block when a nut holding
.19 2.65 .50 3.95 .91 5.29                                     another part on the block was tightened. It was
.20 2.71 .50 3.87 .91 5.11                                     thought that the depth the stud was screwed into
.24 2.79 .50 3.57 .95 5.33                                     the block (the thread engagement) might affect
.24 3.19 .55 3.84 .95 5.27                                     the torque at which the stud stripped out. In the
.25 2.83 .75 4.70 .97 5.30                                     table below, x is the depth (in 10-3 inches above
.30 3.52 .77 4.19 1.00 5.52                                    .400) and y is the torque at failure (in lbs/in.).
.30 3.41 .80 4.75 1.00 5.46
.32 3.51 .80 4.38 1.00 5.30                                         x yx yx yx y
.43 3.38 .85 4.83 1.03 5.59
.45 3.13 .85 5.32 1.04 5.71                                        80 15 40 70 75 70 20 70

(a) Make a scatterplot of these data and comment                   76 15 36 65 25 70 40 65
     on the apparent linearity (or the lack thereof)
     of the relationship between y and x.                          88 25 30 65 30 60 30 75

(b) Compute the sample correlation between y                       35 60 0 45 78 25 74 25
     and x. Interpret this value.
                                                                   75 35 44 50 60 45
(c) Show the "hand" calculations necessary to fit
     a line to these data by least squares. Then plot          (a) Use a regression program and fit both a linear
     your line on the graph from (a).                               equation and a quadratic equation to these
                                                                    data. Plot them on a scatterplot of the data.
(d) About what increase in detonation velocity                      What are the fractions of raw variability in y
     appears to accompany a unit (1 g/cc) increase                  accounted for by these two equations?
     in PETN density? What increase in detona-
     tion velocity would then accompany a .1 g/cc              (b) Redo part (a) after dropping the x = 0 and
     increase in PETN density?                                      y = 45 data point from consideration. Do
                                                                    your conclusions about how best to describe
(e) What fraction of the raw variability in detona-                 the relationship between x and y change ap-
     tion velocity is "accounted for" by the fitted                 preciably? What does this say about the ex-
     line from part (c)?                                            tent to which a single data point can affect a
                                                                    curve-fitting analysis?
(f) Based on your analysis, about what detona-
     tion velocity would you predict for a PETN                (c) Use your quadratic equation from part (a) and
     density of 0.65 g/cc? If it was your job to                    find a thread engagement that provides an op-
     produce a PETN explosive charge with a                         timal predicted failure torque. What would
216 Chapter 4 Describing Relationships Between Variables

          you probably want to do before recommend-              (e) About what increase in log grip force appears
          ing this depth for use in this assembly pro-                to accompany an increase in drag of 10% of
          cess?                                                       the total possible? This corresponds to what
                                                                      kind of change in raw grip force?
25. The textbook Introduction to Contemporary Sta-
     tistical Methods by L. H. Koopmans contains a                (f) What fraction of the raw variability in log grip
     data set from the testing of automobile tires. A tire            force is accounted for in the fitting of a line
     under study is mounted on a test trailer and pulled              to the data in part (d)?
     at a standard velocity. Using a braking mecha-
     nism, a standard amount of drag (measured in %)             (g) Based on your answer to (d), what log grip
     is applied to the tire and the force (in pounds)                 force would you predict for a tire of this type
     with which it grips the road is measured. The fol-               under these conditions using 40% of the pos-
     lowing data are from tests on 19 different tires                 sible drag? What raw grip force?
     of the same design made under the same set of
     road conditions. x = 0% indicates no braking and            (h) Compute the residuals from your fitted line.
     x = 100% indicates the brake is locked.                          Plot the residuals against x and against y^ .
                                                                      Then make a normal plot of the residuals.
Drag, x (%)  Grip Force, y (lb)                                       What do these plots indicate about the linear-
                                                                      ity of the relationship between drag and log
      10     550, 460, 610                                            grip force?
      20     510, 410, 580
      30     470, 360, 480                                        (i) Use a statistical package to find the least
      50     390, 310, 400                                            squares line, the sample correlation, R2, and
      70     300, 280, 340                                            the residuals for these (x, y ) data.
     100     250, 200, 200, 200
                                                            26. The article "Laboratory Testing of Asphalt Con-
(a) Make a scatterplot of these data and comment                 crete for Porous Pavements" by Woelfl, Wei, Faul-
     on "how linear" the relation between y and x                stich, and Litwack (Journal of Testing and Evalu-
     appears to be.                                              ation, 1981) studied the effect of asphalt content
                                                                 on the permeability of open-graded asphalt con-
In fact, physical theory can be called upon to pre-              crete. Four specimens were tested for each of
dict that instead of being linear, the relationship              six different asphalt contents, with the following
between y and x is of the form y   exp(x)                        results:
for suitable  and . Note that if natural loga-
rithms are taken of both sides of this expression,          Asphalt Content,  Permeability,
ln(y)  ln() + x. Calling ln() by the name                   x (% by weight)   y (in./hr water loss)
0 and  by the name 1, one then has a linear
relationship of the form used in Section 4.1.                        3        1189, 840, 1020, 980
(b) Make a scatterplot of y = ln(y) versus x.                        4        1440, 1227, 1022, 1293
                                                                     5        1227, 1180, 980, 1210
     Does this plot look more linear than the one                    6        707, 927, 1067, 822
     in (a)?                                                         7        835, 900, 733, 585
(c) Compute the sample correlation between y                         8        395, 270, 310, 208
     and x "by hand." Interpret this value.
(d) Fit a line to the drags and logged grip forces          (a) Make a scatterplot of these data and comment
     using the least squares principle. Show the                on how linear the relation between y and x
     necessary hand calculations. Sketch this line              appears to be. If you focus on asphalt con-
     on your scatterplot from (b).                              tents between, say, 5% and 7%, does linearity
                                                                seem to be an adequate description of the re-
                                                                lationship between y and x?
                                                                           Chapter 4 Exercises 217

Temporarily restrict your attention to the x = 5, 6,         (l) What do the fitted relationships from (c), (i)
and 7 data.                                                      and (j) give for predicted permeabilities when
(b) Compute the sample correlation between y                     x = 2%? Compare these to each other as well
                                                                 as your answers to (f) and (k). Why would
     and x "by hand." Interpret this value.                      it be unwise to use any of these predictions
(c) Fit a line to the asphalt contents and per-                  without further data collection?

     meabilities using the least squares principle.    27. Some data collected by Koh, Morden, and Og-
     Show the necessary hand calculations. Sketch           bourne in a study of axial breaking strengths (y)
     this fitted line on your scatterplot from (a).         for wooden dowel rods follow. The students tested
(d) About what increase in permeability appears             m = 4 different dowels for each of nine combi-
     to accompany a 1% (by weight) increase in              nations of three different diameters (x1) and three
     asphalt content?                                       different lengths (x2).
(e) What fraction of the raw variability in perme-
     ability is "accounted for" in the fitting of a    x1 (in.)  x2 (in.)  y (lb)
     line to the x = 5, 6, and 7 data in part (c)?
(f) Based on your answer to (c), what measured         .125          4     51.5, 37.4, 59.3, 58.5
     permeability would you predict for a speci-       .125          8     5.2, 6.4, 9.0, 6.3
     men of this material with an asphalt content      .125        12      2.5, 3.3, 2.6, 1.9
     of 5.5%?                                          .1875         4     225.3, 233.9, 211.2, 212.8
(g) Compute the residuals from your fitted line.       .1875         8     47.0, 79.2, 88.7, 70.2
     Plot the residuals against x and against y^ .     .1875       12      18.4, 22.4, 18.9, 16.6
     Then make a normal plot of the residuals.         .250          4     358.8, 309.6, 343.5, 357.8
     What do these plots indicate about the linear-    .250          8     127.1, 158.0, 194.0, 133.0
     ity of the relationship between asphalt content   .250        12      68.9, 40.5, 50.3, 65.6
     and permeability?
(h) Use a statistical package and values for x =       (a) Make a plot of the 3 × 3 means, y¯ , corre-
     5, 6, and 7 to find the least squares line, the        sponding to the different combinations of di-
     sample correlation, R2, and the residuals for          ameter and length used in the study, plotting
     these data.                                            y¯ vs. x2 and connecting the three means for
Now consider again the entire data set.                     a given diameter with line segments. What
(i) Fit the quadratic relationship y  0 + 1x +              does this plot suggest about how successful
     2x2 to the data using a statistical pack-              an equation for y that is linear in x2 for each
     age. Sketch this fitted parabola on your sec-          fixed x1 might be in explaining these data?
     ond scatterplot from part (a). Does this fit-
     ted quadratic appear to be an important im-       (b) Replace the strength values with their natural
     provement over the line you fit in (c) in terms        logarithms, y = ln(y), and redo the plotting
     of describing the relationship over the range          of part (a). Does this second plot suggest that
     3x 8?                                                  the logarithm of strength might be a linear
(j) Fit the linear relation y  0 + 1x to the en-            function of length for fixed diameter?
     tire data set. How do the R2 values for this fit
     and the one in (i) compare? Does the larger R2    (c) Fit the following three equations to the data
     in (i) speak strongly for the use of a quadratic       via least squares:
     (as opposed to a linear) description of the re-
     lationship of y to x in this situation?                     y  0 + 1x1,
(k) If one uses the fitted relationship from (i) to              y  0 + 2x2,
     predict y for x = 5.5, how does the prediction              y  0 + 1x1 + 2x2
     compare to your answer for (f)?
218 Chapter 4 Describing Relationships Between Variables

     What are the coefficients of determination for               analysis. Looking again at your plot from (a),
     the three fitted equations? Compare the equa-                does it seem that the interactions of Diameter
     tions in terms of their complexity and their                 and Length will be important in describing the
     apparent ability to predict y .                              raw strengths, y? Compute the fitted factorial
(d) Add three lines to your plot from part (b),                   effects and comment on the relative sizes of
     showing predicted log strength (from your                    the main effects and interactions.
     third fitted equation) as a function of x2 for          (h) Redo part (g), referring to the graph from part
     the three different values of x1 included in                 (b) and working with the logarithms of dowel
     the study. Use your third fitted equation to                 strength.
     predict first a log strength and then a strength
     for a dowel of diameter .20 in. and length 10      28. The paper "Design of a Metal-Cutting Drilling
     in. Why shouldn't you be willing to use your            Experiment--A Discrete Two-Variable Problem"
     equation to predict the strength of a rod with          by E. Mielnik (Quality Engineering, 1993-1994)
     diameter .50 in. and length 24 in.?                     reports a drilling study run on an aluminum al-
(e) Compute and plot residuals for the third equa-           loy (7075-T6). The thrust (or axial force), y1, and
     tion you fit in part (c). Make plots of residuals       torque, y2, required to rotate drills of various di-
     vs. fitted response and both x1 and x2, and             ameters x1 at various feeds (rates of drill penetra-
     normal-plot the residuals. Do these plots sug-          tion into the workpiece) x2, were measured with
     gest any potential inadequacies of the third            the following results:
     fitted equation? How might these be reme-
     died?                                              Diameter,  Feed Rate,    Thrust,  Torque,
(f) The students who did this study were strongly        x1 (in.)  x2 (in. rev)  y1 (lb)  y2 (ft-lb)
     suspicious that the ratio x3 = x12/x2 is the
     principal determiner of dowel strength. In           .250        .006        230        1.0
     fact, it is possible to empirically discover the     .406        .006        375        2.1
     importance of this quantity as follows. Try          .406        .013        570        3.8
     fitting the equation                                 .250        .013        375        2.1
                                                          .225        .009        280        1.0
               y  0 + 1 ln x1 + 2 ln x2                   .318        .005        225        1.1
                                                          .450        .009        580        3.8
     to these data and notice that the fitted coef-       .318        .017        565        3.4
     ficients of ln x1 and ln x2 are roughly in the       .318        .009        400        2.2
     ratio of 4 to -2, i.e., 2 to -1. (What does this     .318        .009        400        2.1
     fitted equation for ln(y) say about y?) Then         .318        .009        380        2.1
     plot y vs. x3 and fit the linear equation            .318        .009        380        1.9

                       y  0 + 3x3                       Drilling theory suggests that y1  1x1a x2b and
                                                        y2  2x1cx2d for appropriate constants 1, 2, a,
     to these data. Finally, add three curves to your   b, c, and d. (Note that upon taking natural log-
     plot from part (a) based on this fitted equation
     linear in x3, showing predicted strength as a      arithms, there are linear relationships between
     function of x2. Make one for each of the three     y1 = ln(y1) or y2 = ln(y2) and x1 = ln(x1) and
     different values of x1 included in the study.      x2 = ln(x2).)
(g) Since the students' data have a (replicated)
     3 × 3 factorial structure, you can do a facto-
     rial analysis as an alternative to the preceding
                                                                     Chapter 4 Exercises 219

(a) Use a regression program to fit the following               (f) Redo part (e), using y1 as the response vari-
     equations to these data:                                       able.

                  y1  0 + 1x1,                                 (g) Do your answers to parts (e) and (f) comple-
                                                                    ment those of part (d)? Explain.
                  y1  0 + 2x2,
                                                          29. The article "A Simple Method to Study Dispersion
                  y1  0 + 1x1 + 2x2                            Effects From Non-Necessarily Replicated Data in
                                                               Industrial Contexts" by Ferrer and Romero (Qual-
     What are the R2 values for the three differ-              ity Engineering, 1995) describes an unreplicated
     ent fitted equations? Compare the three fitted            24 experiment done to improve the adhesive force
     equations in terms of complexity and appar-               obtained when gluing on polyurethane sheets as
     ent ability to predict y1.                                the inner lining of some hollow metal parts. The
(b) Compute and plot residuals (continuing to                  factors studied were the amount of glue used (A),
     work on log scales) for the third equation you            the predrying temperature (B), the tunnel temper-
     fit in part (a). Make plots of residuals vs. fitted       ature (C), and the pressure applied (D). The exact
     y1 and both x1 and x2, and normal-plot these              levels of the variables employed were not given
     residuals. Do these plots reveal any particular           in the article (presumably for reasons of corporate
     problems with the fitted equation?                        security). The response variable was the adhesive
(c) Use your third equation from (a) to predict                force, y, in Newtons, and the data reported in the
     first a log thrust and then a thrust if a drill of        article follow:
     diameter .360 in. and a feed of .011 in./rev
     are used. Why would it be unwise to make             Combination y Combination y
     a similar prediction for x1 = .450 and x2 =
     .017? (Hint: Make a plot of the (x1, x2) pairs       (1)  3.80  d     3.29
     in the data set and locate this second set of
     conditions on that plot.)                            a    4.34  ad    2.82
(d) If the third equation fit in part (a) governed y1,
     would it lead to Diameter × Feed interactions        b    3.54  bd    4.59
     for y1 measured on the log scale? To help you
     answer this question, plot y1 vs. x2 (or x2) for     ab   4.59  abd   4.68
     each of x1 = .250, .318, and .406. Does this
     equation lead to Diameter × Feed interactions        c    3.95  cd    2.73
     for raw y1?
(e) The first four data points listed in the ta-          ac   4.83  acd   4.31
     ble constitute a very small complete factorial
     study (an unreplicated 2 × 2 factorial in the        bc   4.86  bcd   5.16
     factors Diameter and Feed). Considering only
     these data points, do a "factorial" analysis of      abc  5.28  abcd  6.06
     this part of the y1 data. Begin by making an in-
     teraction plot similar to Figure 4.22 for these      (a) Compute the fitted factorial effects corre-
     data. Based on that plot, discuss the apparent            sponding to the "all high" treatment com-
     relative sizes of the Diameter and Feed main              bination.
     effects on thrust. Then carry out the arith-
     metic necessary to compute the fitted factorial      (b) Interpret the results of your calculations in
     effects (the main effects and interactions).              the context of the study. Which factors and/or
                                                               combinations of factors appear to have the
                                                               largest effects on the adhesive force? Suppose
                                                               that only the A, B, and C main effects and
                                                               the B × D interactions were judged to be
                                                               of importance here. Make a corresponding
                                                               statement to your engineering manager about
                                                               how the factors impact adhesive force.
220 Chapter 4 Describing Relationships Between Variables

     (c) Using the reverse Yates algorithm or other-           Combination y Combination y
          wise, compute fitted/predicted values corre-
          sponding to an "A, B, and C main effects             (1)  646  d     666
          and BD interactions" description of adhesive
          force. Then use these 16 values to compute           a    623  ad    597
          residuals, e = y - y^ . Plot these against y^ , and
          against levels of A, B, C, and D. Also normal-       b    714  bd    718
          plot them. Comment on any interpretable pat-
          terns you see. Particularly in reference to the      ab   643  abd   661
          plot of residuals vs. level of D, what does this
          graph suggest if one is interested not only in       c    360  cd    304
          high mean adhesive force but in consistent
          adhesive force as well?                              ac   359  acd   309

     (d) Find and interpret a value of R2 correspond-          bc   325  bcd   360
          ing to the description of y used in part (c).
                                                               abc  318  abcd  318
30. The article "Chemical Vapor Deposition of Tung-
     sten Step Coverage and Thickness Uniformity Ex-           (a) Use the Yates algorithm and compute the fit-
     periments" by J. Chang (Thin Solid Films, 1992)                ted factorial effects corresponding to the "all
     describes an unreplicated 24 factorial experiment              high" treatment combination. (You will need
     aimed at understanding the effects of the factors              to employ four cycles in the calculations.)

Factor A  Chamber Pressure (Torr)                              (b) Interpret the results of your calculations from
Factor B  8 (-) vs. 9 (+)                                           (a) in the context of the study. (Describe in
Factor C  H2 Flow (standard cm3/min)                                words which factors and/or combinations of
Factor D  500 (-) vs. 1000 (+)                                      factors appear to have the largest effect(s) on
          SiH4 Flow (standard cm3/min)                              average sheet resistivity. What are the practi-
          15 (-) vs. 25 (+)                                         cal implications of these effects?)
          WF6 Flow (standard cm3/min)
          50 (-) vs. 60 (+)                                    (c) Suppose that you judge all factors except C
                                                                    to be "inert" as far as determining sheet resis-
on a number of response variables in the chemi-                     tivity is concerned (the main effects of A, B,
cal vapor deposition tungsten films. One response                   and D and all interactions involving them are
variable reported was the average sheet resistivity,                negligible). What fitted/predicted values cor-
y (m /cm) of the resultant film, and the values                     respond to this "C main effects only" descrip-
reported in the paper follow.                                       tion of average sheet resistivity? Use these 16
                                                                    values to compute residuals, e = y - y^ . Plot
                                                                    these against y^ , level of A, level of B, level
                                                                    of C, and level of D. Also normal-plot these
                                                                    residuals. Comment on any interpretable pat-
                                                                    terns in your plots.

                                                               (d) Compute an R2 value corresponding to the
                                                                    description of average sheet resistivity used in
                                                                    part (c). Does it seem that the grand mean and
                                                                    C main effects provide an effective summary
                                                                    of average sheet resistivity? Why?
5q q q q q q q q q q q q q q q q q q q q q q q q q

         Probability:
         The Mathematics
         of Randomness

       The theory of probability is the mathematician's description of random variation.

       This chapter introduces enough probability to serve as a minimum background for
       making formal statistical inferences.

            The chapter begins with a discussion of discrete random variables and their
       distributions. It next turns to continuous random variables and then probability
       plotting. Next, the simultaneous modeling of several random variables and the
       notion of independence are considered. Finally, there is a look at random variables
       that arise as functions of several others, and how randomness of the input variables
       is translated to the output variable.

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

5.1 (Discrete) Random Variables

                    The concept of a random (or chance) variable is introduced in general terms in this
                    section. Then specialization to discrete cases is considered. The specification of a
                    discrete probability distribution via a probability function or cumulative probability
                    function is discussed. Next, summarization of discrete distributions in terms of
                    (theoretical) means and variances is treated. Then the so-called binomial, geometric,
                    and Poisson distributions are introduced as examples of useful discrete probability
                    models.

5.1.1  Random Variables and Their Distributions

       It is usually appropriate to think of a data value as subject to chance influences.
       In enumerative contexts, chance is commonly introduced into the data collection
       process through random sampling techniques. Measurement error is nearly always a

                                                    221
222 Chapter 5 Probability: The Mathematics of Randomness

                                  factor in statistical engineering studies, and the many small, unnameable causes that
                                  work to produce it are conveniently thought of as chance phenomena. In analytical
                                  contexts, changes in system conditions work to make measured responses vary, and
                                  this is most often attributed to chance.

Definition 1  A random variable is a quantity that (prior to observation) can be thought
              of as dependent on chance phenomena. Capital letters near the end of the
              alphabet are typically used to stand for random variables.

                   Consider a situation (like that of Example 3 in Chapter 3) where the torques
              of bolts securing a machine component face plate are to be measured. The next
              measured value can be considered subject to chance influences and we thus term

                                              Z = the next torque recorded

              a random variable.
                   Following Definition 9 in Chapter 1, a distinction was made between discrete

              and continuous data. That terminology carries over to the present context and inspires
              two more definitions.

Definition 2  A discrete random variable is one that has isolated or separated possible
              values (rather than a continuum of available outcomes).

Definition 3  A continuous random variable is one that can be idealized as having an
              entire (continuous) interval of numbers as its set of possible values.

                   Random variables that are basically count variables clearly fall under Defi-
              nition 2 and are discrete. It could be argued that all measurement variables are
              discrete--on the basis that all measurements are "to the nearest unit." But it is often
              mathematically convenient, and adequate for practical purposes, to treat them as
              continuous.

                   A random variable is, to some extent, a priori unpredictable. Therefore, in
              describing or modeling it, the important thing is to specify its set of potential values
              and the likelihoods associated with those possible values.

Definition 4  To specify a probability distribution for a random variable is to give its set
              of possible values and (in one way or another) consistently assign numbers
                                                                          5.1 (Discrete) Random Variables 223

                        between 0 and 1--called probabilities--as measures of the likelihood that
                        the various numerical values will occur.

                             The methods used to specify discrete probability distributions are different
                        from those used to specify continuous probability distributions. So the implications
                        of Definition 4 are studied in two steps, beginning in this section with discrete
                        distributions.

5.1.2                   Discrete Probability Functions
                        and Cumulative Probability Functions

                        The tool most often used to describe a discrete probability distribution is the prob-
                        ability function.

Definition 5            A probability function for a discrete random variable X , having possible
                        values x1, x2, . . ., is a nonnegative function f (x), with f (xi ) giving the prob-
                        ability that X takes the value xi .

Probability function         This text will use the notational convention that a capital P followed by an
      for the discrete  expression or phrase enclosed by brackets will be read "the probability" of that
                        expression. In these terms, a probability function for X is a function f such that
 random variable X
                                                               f (x) = P[X = x]

                        That is, " f (x) is the probability that (the random variable) X takes the value x."

Example 1               A Torque Requirement Random Variable

                        Consider again Example 3 in Chapter 3, where Brenny, Christensen, and Schnei-
                        der measured bolt torques on the face plates of a heavy equipment component.
                        With

                            Z = the next measured torque for bolt 3 (recorded to the nearest integer)

                        consider treating Z as a discrete random variable and giving a plausible proba-
                        bility function for it.

                             The relative frequencies for the bolt 3 torque measurements recorded in
                        Table 3.4 on page 74 produce the relative frequency distribution in Table 5.1.
                        This table shows, for example, that over the period the students were collecting
                        data, about 15% of measured torques were 19 ft lb. If it is sensible to believe
                        that the same system of causes that produced the data in Table 3.4 will operate
224 Chapter 5 Probability: The Mathematics of Randomness

Example 1     to produce the next bolt 3 torque, then it also makes sense to base a probability
(continued )  function for Z on the relative frequencies in Table 5.1. That is, the probability
              distribution specified in Table 5.2 might be used. (In going from the relative
              frequencies in Table 5.1 to proposed values for f (z) in Table 5.2, there has been
              some slightly arbitrary rounding. This has been done so that probability values
              are expressed to two decimal places and now total to exactly 1.00.)

              Table 5.1
              Relative Frequency Distribution for Measured Bolt 3
              Torques

              z, Torque (ft lb) Frequency Relative Frequency

              11                                          1        1/34  .02941

              12                                          1        1/34  .02941

              13                                          1        1/34  .02941

              14                                          2        2/34  .05882

              15                                          9        9/34  .26471

              16                                          3        3/34  .08824

              17                                          4        4/34  .11765

              18                                          7        7/34  .20588

              19                                          5        5/34  .14706

              20                                          1        1/34  .02941

                                                          34       1

                  Table 5.2
                  A Probability Function
                  for Z

                  Torque                                      Probability
                     z                                            f (z)

                  11                                          .03

                  12                                          .03

                  13                                          .03

                  14                                          .06

                  15                                          .26

                  16                                          .09

                  17                                          .12

                  18                                          .20

                  19                                          .15

                  20                                          .03
                         5.1 (Discrete) Random Variables 225

                              The appropriateness of the probability function in Table 5.2 for describing Z
                         depends essentially on the physical stability of the bolt-tightening process. But
                         there is a second way in which relative frequencies can become obvious choices for
                         probabilities. For example, think of treating the 34 torques represented in Table 5.1
                         as a population, from which n = 1 item is to be sampled at random, and

                         Y = the torque value selected

       The probability   Then the probability function in Table 5.2 is also approximately appropriate for Y .
      distribution of a  This point is not so important in this specific example as it is in general: Where
single value selected    one value is to be selected at random from a population, an appropriate probability
      at random from     distribution is one that is equivalent to the population relative frequency distribution.

           a population       This text will usually express probabilities to two decimal places, as in Table 5.2.
                         Computations may be carried to several more decimal places, but final probabilities
        Properties of a  will typically be reported only to two places. This is because numbers expressed to
mathematically valid     more than two places tend to look too impressive and be taken too seriously by the
 probability function    uninitiated. Consider for example the statement "There is a .097328 probability of
                         booster engine failure" at a certain missile launch. This may represent the results of
                         some very careful mathematical manipulations and be correct to six decimal places
                         in the context of the mathematical model used to obtain the value. But it is doubtful
                         that the model used is a good enough description of physical reality to warrant that
                         much apparent precision. Two-decimal precision is about what is warranted in most
                         engineering applications of simple probability.

                              The probability function shown in Table 5.2 has two properties that are necessary
                         for the mathematical consistency of a discrete probability distribution. The f (z)
                         values are each in the interval [0, 1] and they total to 1. Negative probabilities or
                         ones larger than 1 would make no practical sense. A probability of 1 is taken as
                         indicating certainty of occurrence and a probability of 0 as indicating certainty of
                         nonoccurrence. Thus, according to the model specified in Table 5.2, since the values
                         of f (z) sum to 1, the occurrence of one of the values 11, 12, 13, 14, 15, 16, 17, 18,
                         19, and 20 ft lb is certain.

                              A probability function f (x) gives probabilities of occurrence for individual val-
                         ues. Adding the appropriate values gives probabilities associated with the occurrence
                         of one of a specified type of value for X .

Example 1                Consider using f (z) defined in Table 5.2 to find
(continued )                                  P[Z > 17] = P[the next torque exceeds 17]

                         Adding the f (z) entries corresponding to possible values larger than 17 ft lb,

                         P[Z > 17] = f (18) + f (19) + f (20) = .20 + .15 + .03 = .38

                         The likelihood of the next torque being more than 17 ft lb is about 38%.
226 Chapter 5 Probability: The Mathematics of Randomness

Example 1          If, for example, specifications for torques were 16 ft lb to 21 ft lb, then the
(continued )  likelihood that the next torque measured will be within specifications is

                  P[16  Z  21] = f (16) + f (17) + f (18) + f (19) + f (20) + f (21)
                                        = .09 + .12 + .20 + .15 + .03 + .00
                                        = .59

                   In the torque measurement example, the probability function is given in tabular
              form. In other cases, it is possible to give a formula for f (x).

Example 2     A Random Tool Serial Number

              The last step of the pneumatic tool assembly process studied by Kraber, Rucker,
              and Williams (see Example 11 in Chapter 3) was to apply a serial number plate
              to the completed tool. Imagine going to the end of the assembly line at exactly
              9:00 A.M. next Monday and observing the number plate first applied after 9:00.

                   Suppose that

              W = the last digit of the serial number observed

              Suppose further that tool serial numbers begin with some code special to the
              tool model and end with consecutively assigned numbers reflecting how many
              tools of the particular model have been produced. The symmetry of this situation
              suggests that each possible value of W (w = 0, 1, . . . , 9) is equally likely. That
              is, a plausible probability function for W is given by the formula

                           .1                             for w = 0, 1, 2, . . . , 9
              f (w) =                                     otherwise

                           0

                   Another way of specifying a discrete probability distribution is sometimes used.
              That is to specify its cumulative probability function.

Definition 6  The cumulative probability function for a random variable X is a function
              F(x) that for each number x gives the probability that X takes that value or a
              smaller one. In symbols,

                                                 F(x) = P[X  x]
                                                                                   5.1 (Discrete) Random Variables 227

                                  Since (for discrete distributions) probabilities are calculated by summing values
                             of f (x), for a discrete distribution,

Cumulative probability           F(x) = f (z)
 function for a discrete
                                               zx
                 variable X

                             (The sum is over possible values less than or equal to x.) In this discrete case, the
                             graph of F(x) will be a stair-step graph with jumps located at possible values and
                             equal in size to the probabilities associated with those possible values.

Example 1                    Values of both the probability function and the cumulative probability function
(continued )                 for the torque variable Z are given in Table 5.3. Values of F(z) for other z are
                             also easily obtained. For example,

                                                           F(10.7) = P[Z  10.7] = 0
                                            F(16.3) = P[Z  16.3] = P[Z  16] = F(16) = .50

                                                            F(32) = P[Z  32] = 1.00

                             A graph of the cumulative probability function for Z is given in Figure 5.1. It
                             shows the stair-step shape characteristic of cumulative probability functions for
                             discrete distributions.

                             Table 5.3
                             Values of the Probability Function and Cumulative
                             Probability Function for Z

                             z, Torque f (z) = P[Z = z] F(z) = P[Z  z]

                             11  .03               .03

                             12  .03               .06

                             13  .03               .09

                             14  .06               .15

                             15  .26               .41

                             16  .09               .50

                             17  .12               .62

                             18  .20               .82

                             19  .15               .97

                             20  .03               1.00
228 Chapter 5 Probability: The Mathematics of Randomness

Example 1           F(z)
(continued )  1.0

              .5

                  11 12 13 14 15 16 17 18 19 20              z

                  Figure 5.1 Graph of the cumulative
                  probability function for Z

                   The information about a discrete distribution carried by its cumulative probabil-
              ity function is equivalent to that carried by the corresponding probability function.
              The cumulative version is sometimes preferred for table making, because round-off
              problems are more severe when adding several f (x) terms than when taking the
              difference of two F(x) values to get a probability associated with a consecutive
              sequence of possible values.

5.1.3         Summarization of Discrete Probability Distributions

              Amost all of the devices for describing relative frequency (empirical) distributions
              in Chapter 3 have versions that can describe (theoretical) probability distributions.

                   For a discrete random variable with equally spaced possible values, a probabil-
              ity histogram gives a picture of the shape of the variable's distribution. It is made
              by centering a bar of height f (x) over each possible value x. Probability histograms
              for the random variables Z and W in Examples 1 and 2 are given in Figure 5.2.
              Interpreting such probability histograms is similar to interpreting relative frequency
              histograms, except that the areas on them represent (theoretical) probabilities instead
              of (empirical) fractions of data sets.

                   It is useful to have a notion of mean value for a discrete random variable (or its
              probability distribution).

Definition 7  The mean or expected value of a discrete random variable X (sometimes
              called the mean of its probability distribution) is

                          EX = x f (x)                          (5.1)

                                                          x

              EX is read as "the expected value of X ," and sometimes the notation µ is used
              in place of EX.
             Probability Distribution for Z                     5.1 (Discrete) Random Variables 229
    f (z)                                                 Probability Distribution for W
.2                                                f (w)
                                             .2
.1
                                             .1

11 12 13 14 15 16 17 18 19 20 z              0 1 2 3 4 5 6 7 8 9w

Figure 5.2 Probability histograms for Z and W (Examples 1 and 2)

              (Remember the warning in Section 3.3 that µ would stand for both the mean of a
              population and the mean of a probability distribution.)

Example 1     Returning to the bolt torque example, the expected (or theoretical mean) value of
(continued )  the next torque is

              EZ = z f (z)

                           z

                  = 11(.03) + 12(.03) + 13(.03) + 14(.06) + 15(.26)
                     + 16(.09) + 17(.12) + 18(.20) + 19(.15) + 20(.03)

                  = 16.35 ft lb

              This value is essentially the arithmetic mean of the bolt 3 torques listed in
              Table 3.4. (The slight disagreement in the third decimal place arises only because
              the relative frequencies in Table 5.1 were rounded slightly to produce Table 5.2.)
              This kind of agreement provides motivation for using the symbol µ, first seen in
              Section 3.3, as an alternative to EZ.

                   The mean of a discrete probability distribution has a balance point interpretation,
              much like that associated with the arithmetic mean of a data set. Placing (point)
              masses of sizes f (x) at points x along a number line, EX is the center of mass of
              that distribution.
230 Chapter 5 Probability: The Mathematics of Randomness

Example 2     Considering again the serial number example, and the second part of Figure 5.2,
(continued )  if a balance point interpretation of expected value is to hold, EW had better turn
              out to be 4.5. And indeed,

                      EW = 0(.1) + 1(.1) + 2(.1) + · · · + 8(.1) + 9(.1) = 45(.1) = 4.5

                   It was convenient to measure the spread of a data set (or its relative frequency
              distribution) with the variance and standard deviation. It is similarly useful to have
              notions of spread for a discrete probability distribution.

Definition 8  The variance of a discrete random variable X (or the variance of its distribu-
              tion) is

              Var X = (x - EX)2 f (x) = x2 f (x) - (EX)2    (5.2)

              The standard deviation of X is Var X. Often the notation  2 is used in
              place of Var X, and  is used in place of Var X .

                   The variance of a random variable is its expected (or mean) squared distance
              from the center of its probability distribution. The use of  2 to stand for both the

              variance of a population and the variance of a probability distribution is motivated

              on the same grounds as the double use of µ.

Example 1     The calculations necessary to produce the bolt torque standard deviation are
(continued )  organized in Table 5.4. So

                                                          
               = Var Z = 4.6275 = 2.15 ft lb

              Except for a small difference due to round-off associated with the creation of
              Table 5.2, this standard deviation of the random variable Z is numerically the
              same as the population standard deviation associated with the bolt 3 torques in
              Table 3.4. (Again, this is consistent with the equivalence between the population
              relative frequency distribution and the probability distribution for Z .)
                                      5.1 (Discrete) Random Variables 231

              Table 5.4
              Calculations for Var Z

              z f (z) (z - 16.35)2 (z - 16.35)2 f (z)

              11 .03  28.6225                  .8587
              12 .03  18.9225                  .5677
              13 .03  11.2225                  .3367
              14 .06   5.5225                  .3314
              15 .26   1.8225                  .4739
              16 .09                           .0110
              17 .12     .1225                 .0507
              18 .20     .4225                 .5445
              19 .15   2.7225                 1.0534
              20 .03   7.0225                  .3997
                      13.3225

                                        Var Z = 4.6275

Example 2     To illustrate the alternative for calculating a variance given in Definition 8, con-
(continued )
              sider finding the variance and standard deviation of the serial number variable W .
              Table 5.5 shows the calculation of w2 f (w).

                      Table 5.5         w2f (w)
                      Calculations for

                      w f (w) w2 f (w)

                      0 .1              0.0

                      1 .1              .1

                      2 .1              .4

                      3 .1              .9

                      4 .1              1.6

                      5 .1              2.5

                      6 .1              3.6

                      7 .1              4.9

                      8 .1              6.4

                      9 .1              8.1

                                        28.5
232 Chapter 5 Probability: The Mathematics of Randomness

Example 2             Then                w2 f (w) - (EW)2 = 28.5 - (4.5)2 = 8.25
(continued )                   Var W =

                      so that             
                                            Var W = 2.87

                      Comparing the two probability histograms in Figure 5.2, notice that the distribu-

                      tion of W appears to be more spread out than that of Z . Happily, this is reflected

                      in the fact that

                                                          
                                          Var W = 2.87 > 2.15 = Var Z

            5.1.4     The Binomial and Geometric Distributions

     Independent      Discrete probability distributions are sometimes developed from past experience
identical success-    with a particular physical phenomenon (as in Example 1). On the other hand, some-
                      times an easily manipulated set of mathematical assumptions having the potential
      failure trials  to describe a variety of real situations can be put together. When those can be ma-
                      nipulated to derive generic distributions, those distributions can be used to model
                      a number of different random phenomena. One such set of assumptions is that of
                      independent, identical success-failure trials.

                           Many engineering situations involve repetitions of essentially the same "go-no
                      go" (success-failure) scenario, where:

                      1. There is a constant chance of a go/success outcome on each repetition of the
                          scenario (call this probability p).

                      2. The repetitions are independent in the sense that knowing the outcome of
                          any one of them does not change assessments of chance related to any others.

Binomial              Examples of this kind include the testing of items manufactured consecutively,
 random               where each will be classified as either conforming or nonconforming; observing
variables             motorists as they pass a traffic checkpoint and noting whether each is traveling at a
                      legal speed or speeding; and measuring the performance of workers in two different
                      workspace configurations and noting whether the performance of each is better in
                      configuration A or configuration B.

                           In this context, there are two generic kinds of random variables for which
                      deriving appropriate probability distributions is straightforward. The first is the case
                      of a count of the repetitions out of n that yield a go/success result. That is, consider
                      a variable

                                 X = the number of go/success results in n independent identical
                                       success-failure trials

                      X has the binomial (n, p) distribution.
                                                       5.1 (Discrete) Random Variables 233

Definition 9  The binomial (n, p) distribution is a discrete probability distribution with
              probability function

                         
                          n! px (1 - p)n-x for x = 0, 1, . . . , n
              f (x) =  x! (n - x)! (5.3)
                           0                             otherwise

              for n a positive integer and 0 < p < 1.

              Equation (5.3) is completely plausible. In it there is one factor of p for each trial pro-
              ducing a go/success outcome and one factor of (1 - p) for each trial producing a no
              go/failure outcome. And the n!/x! (n - x)! term is a count of the number of patterns
              in which it would be possible to see x go/success outcomes in n trials. The name bi-
              nomial distribution derives from the fact that the values f (0), f (1), f (2), . . . , f (n)
              are the terms in the expansion of

                                                      ( p + (1 - p))n

              according to the binomial theorem.
                   Take the time to plot probability histograms for several different binomial

              distributions. It turns out that for p < .5, the resulting histogram is right-skewed.
              For p > .5, the resulting histogram is left-skewed. The skewness increases as p
              moves away from .5, and it decreases as n is increased. Four binomial probability
              histograms are pictured in Figure 5.3.

                  f (x)    n = 5          f (x)  n = 5       f (x)
                           p = .2                p = .5
              .4                      .4                 .4 n = 5 p = .8
              .3                      .3                 .3
              .2                      .2                 .2
              .1                      .1                 .1

              0 1 2 3 4 5x            0 1 2 3 4 5x       0 1 2 3 4 5x

                  f (x)       n = 10
              .3              p = .2
              .2
              .1

              0 1 2 3 4 5 6 7 8 9 10 x

                        Figure 5.3 Four binomial probability histograms
234 Chapter 5 Probability: The Mathematics of Randomness

Example 3          The Binomial Distribution and Counts of Reworkable Shafts
    WWW
                   Consider again the situation of Example 12 in Chapter 3 and a study of the
                   performance of a process for turning steel shafts. Early in that study, around 20%
                   of the shafts were typically classified as "reworkable." Suppose that p = .2 is
                   indeed a sensible figure for the chance that a given shaft will be reworkable.
                   Suppose further that n = 10 shafts will be inspected, and the probability that at
                   least two are classified as reworkable is to be evaluated.

                        Adopting a model of independent, identical success-failure trials for shaft
                   conditions,

                   U = the number of reworkable shafts in the sample of 10

                   is a binomial random variable with n = 10 and p = .2. So

                   P[at least two reworkable shafts] = P[U  2]

                                                          = f (2) + f (3) + · · · + f (10)

                                                          = 1 - ( f (0) + f (1))

                                                          = 1 - 10! (.2)0(.8)10 + 10! (.2)1(.8)9
                                                                 0! 10!           1! 9!

                                                          = .62

                   (The trick employed here, to avoid plugging into the binomial probability function
                   9 times by recognizing that the f (u)'s have to sum up to 1, is a common and
                   useful one.)

                        The .62 figure is only as good as the model assumptions that produced it.
                   If an independent, identical success-failure trials description of shaft production
                   fails to accurately portray physical reality, the .62 value is fine mathematics
                   but possibly a poor description of what will actually happen. For instance, say
                   that due to tool wear it is typical to see 40 shafts in specifications, then 10
                   reworkable shafts, a tool change, 40 shafts in specifications, and so on. In this
                   case, the binomial distribution would be a very poor description of U , and the
                   .62 figure largely irrelevant. (The independence-of-trials assumption would be
                   inappropriate in this situation.)

    The binomial        There is one important circumstance where a model of independent, identical
distribution and   success-failure trials is not exactly appropriate, but a binomial distribution can still be
                   adequate for practical purposes--that is, in describing the results of simple random
 simple random     sampling from a dichotomous population. Suppose a population of size N contains
         sampling
                                                                 5.1 (Discrete) Random Variables 235

           a fraction p of type A objects and a fraction (1 - p) of type B objects. If a simple
           random sample of n of these items is selected and

                                 X = the number of type A items in the sample

           strictly speaking, x is not a binomial random variable. But if n is a small fraction of
           N (say, less than 10%), and p is not too extreme (i.e., is not close to either 0 or 1),
           X is approximately binomial (n, p).

Example 4  Simple Random Sampling from a Lot of Hexamine Pellets
           In the pelletizing machine experiment described in Example 14 in Chapter 3,
           Greiner, Grimm, Larson, and Lukomski found a combination of machine settings
           that allowed them to produce 66 conforming pellets out of a batch of 100 pellets.
           Treat that batch of 100 pellets as a population of interest and consider selecting
           a simple random sample of size n = 2 from it.

                If one defines the random variable

                      V = the number of conforming pellets in the sample of size 2

           the most natural probability distribution for V is obtained as follows. Possible
           values for V are 0, 1, and 2.

                      f (0) = P[V = 0]
                            = P[first pellet selected is nonconforming and
                               subsequently the second pellet is also nonconforming]

                      f (2) = P[V = 2]
                            = P[first pellet selected is conforming and
                               subsequently the second pellet selected is conforming]

                      f (1) = 1 - ( f (0) + f (2))

           Then think, "In the long run, the first selection will yield a nonconforming pellet
           about 34 out of 100 times. Considering only cases where this occurs, in the long
           run the next selection will also yield a nonconforming pellet about 33 out of 99
           times." That is, a sensible evaluation of f (0) is

                                            f (0) = 34 · 33 = .1133
                                                     100 99
236 Chapter 5 Probability: The Mathematics of Randomness

Example 4          Similarly,
(continued )

                                                  f (2) = 66 · 65 = .4333
                                                           100 99

                   and thus

                               f (1) = 1 - (.1133 + .4333) = 1 - .5467 = .4533

                   Now, V cannot be thought of as arising from exactly independent trials. For

                   example, knowing that the first pellet selected was conforming would reduce most

                   people's  assessment  of  the  chance  that  the  second  is  also  conforming  from  66   to
                                                                                                         100
                   9965 . Nevertheless, for most practical purposes, V can be thought of as essentially
                   binomial with n = 2 and p = .66. To see this, note that

                                                2! (.34)2(.66)0 = .1156  f (0)
                                              0! 2!

                                                2! (.34)1(.66)1 = .4488  f (1)
                                              1! 1!

                                                2! (.34)0(.66)2 = .4356  f (2)
                                              2! 0!

                   Here, n is a small fraction of N , p is not too extreme, and a binomial distribution
                   is a decent description of a variable arising from simple random sampling.

                        Calculation of the mean and variance for binomial random variables is greatly
                   simplified by the fact that when the formulas (5.1) and (5.2) are used with the
                   expression for binomial probabilities in equation (5.3), simple formulas result. For
                   X a binomial (n, p) random variable,

   Mean of the                                    n       n!
binomial (n, p)
                                                                        x        n-x
    distribution               µ = E X = x x!(n - x)! p (1 - p) = np                                     (5.4)

                                                  x =0

                   Further, it is the case that

Variance of the                n                          n!
 binomial (n, p)
                                                                                 n-x
     distribution   2 = Var X = (x - np)2 x!(n - x)! p (1 - p) = np(1 - p)x                              (5.5)

                               x =0
                                                          5.1 (Discrete) Random Variables 237

Example 3                     Returning to the machining of steel shafts, suppose that a binomial distribution
(continued )                  with n = 10 and p = .2 is appropriate as a model for

                                           U = the number of reworkable shafts in the sample of 10

                              Then, by formulas (5.4) and (5.5),

                                                            EU = (10)(.2) = 2 shafts
                                                       

                                                          Var U = 10(.2)(.8) = 1.26 shafts

Geometric                          A second generic type of random variable associated with a series of indepen-
   random                     dent, identical success-failure trials is
  variables
                                       X = the number of trials required to first obtain a go/success result

                              X has the geometric ( p) distribution.

Definition 10                 The geometric ( p) distribution is a discrete probability distribution with
                              probability function

                                              f (x) =  p(1 - p)x-1 for x = 1, 2, . . .         (5.6)

                                                       0  otherwise

                              for 0 < p < 1.

                                   Formula (5.6) makes good intuitive sense. In order for X to take the value x,
                              there must be x - 1 consecutive no-go/failure results followed by a go/success. In
                              formula (5.6), there are x - 1 terms (1 - p) and one term p. Another way to see
                              that formula (5.6) is plausible is to reason that for X as above and x = 1, 2, . . .

                              That is,  1 - F(x) = 1 - P[X  x]
                                                    = P[X > x]
Simple relationship for                             = P[x no-go/failure outcomes in x trials]  (5.7)
       the geometric (p)
                                                          1 - F(x) = (1 - p)x
cumulative probability
                    function
238 Chapter 5 Probability: The Mathematics of Randomness

               f (x)  p = .5
           .5
           .4
           .3
           .2
           .1

           123456789 x

               f (x)  p = .25

           .3
           .2
           .1

             1 2 3 4 5 6 7 8 9 10 11 12 13 x
           Figure 5.4 Two geometric probability histograms

           by using the form of the binomial (x, p) probability function given in equation
           (5.3). Then for x = 2, 3, . . . , f (x) = F(x) - F(x - 1) = -(1 - F(x)) + (1 -
           F(x - 1)). This, combined with equation (5.7), gives equation (5.6).

                The name geometric derives from the fact that the values f (1), f (2), f (3), . . .
           are terms in the geometric infinite series for

                                                   p· 1
                                                       1 - (1 - p)

                The geometric distributions are discrete distributions with probability his-
           tograms exponentially decaying as x increases. Two different geometric probability
           histograms are pictured in Figure 5.4.

Example 5  The Geometric Distribution and Shorts in NiCad Batteries

           In "A Case Study of the Use of an Experimental Design in Preventing Shorts
           in Nickel-Cadmium Cells" (Journal of Quality Technology, 1988), Ophir, El-
           Gad, and Snyder describe a series of experiments conducted in order to reduce
           the proportion of cells being scrapped by a battery plant because of internal
           shorts. The experimental program was successful in reducing the percentage of
           manufactured cells with internal shorts to around 1%.
                                                                      5.1 (Discrete) Random Variables 239

                        Suppose that testing begins on a production run in this plant, and let

                                 T = the test number at which the first short is discovered

                   A model for T (appropriate if the independent, identical success-failure trials
                   description is apt) is geometric with p = .01. ( p is the probability that any
                   particular test yields a shorted cell.) Then, using equation (5.6),

                      P[the first or second cell tested has the first short] = P[T = 1 or T = 2]
                                                                                   = f (1) + f (2)
                                                                                   = (.01) + (.01)(1 - .01)
                                                                                   = .02

                   Or, using equation (5.7),

                           P[at least 50 cells are tested without finding a short] = P[T > 50]
                                                                                           = (1 - .01)50
                                                                                           = .61

                     Like the binomial distributions, the geometric distributions have means and
                variances that are simple functions of the parameter p. That is, if X is geometric ( p),

 Mean of the              µ = EX = x p(1 - p)x-1  = 1                                        (5.8)
geometric (p)                           x=1 p

  distribution

                     and                                  x- 1     x-1 2 1 - p               (5.9)
                                                                p    p(1 - p) = 2
Variance of the            2 = Var X =                                                    p
  geometric (p)
     distribution                                   x =1

Example 5       In the context of battery testing, with T as before,
(continued )

                          E T = 1 = 100 batteries
                                  .01

                                                          2 (1 - .01) = 99.5 batteries
                            Var T =                         (.01)
240 Chapter 5 Probability: The Mathematics of Randomness

Example 5      Formula (5.8) is an intuitively appealing result. If there is only 1 chance in 100 of
(continued )   encountering a shorted battery at each test, it is sensible to expect to wait through
               100 tests on average to encounter the first one.

5.1.5          The Poisson Distributions

               As discussed in Section 3.4, it is often important to keep track of the total number
               of occurrences of some relatively rare phenomenon, where the physical or time
               unit under observation has the potential to produce many such occurrences. A case
               of floor tiles has potentially many total blemishes. In a one-second interval, there
               are potentially a large number of messages that can arrive for routing through a
               switching center. And a 1 cc sample of glass potentially contains a large number of
               imperfections.

                    So probability distributions are needed to describe random counts of the number
               of occurrences of a relatively rare phenomenon across a specified interval of time
               or space. By far the most commonly used theoretical distributions in this context
               are the Poisson distributions.

Definition 11  The Poisson () distribution is a discrete probability distribution with prob-
               ability function

                                                          for x = 0, 1, 2, . . .                       (5.10)
                                             e-x          otherwise
                                   f (x) =  x!

                                                0

               for  > 0.

               The form of equation (5.10) may initially seem unappealing. But it is one that

               has sensible mathematical origins, is manageable, and has proved itself empirically

               useful in many different "rare events" circumstances. One way to arrive at equation

               (5.10) is to think of a very large number of independent trials (opportunities for

               occurrence), where the probability of success (occurrence) on any one is very small

               and the product of the number of trials and the success probability is . One is

               then  led  to  the  binomial  (n,     )  distribution.  In  fact,  for  large  n,  the  binomial  (n,     )
                                                  n                                                                   n
               probability function approximates the one specified in equation (5.10). So one

               might think of the Poisson distribution for counts as arising through a mechanism

               that would present many tiny similar opportunities for independent occurrence or

               nonoccurrence throughout an interval of time or space.

               The Poisson distributions are right-skewed distributions over the values x =

               0, 1, 2, . . . , whose probability histograms peak near their respective 's. Two dif-

               ferent Poisson probability histograms are shown in Figure 5.5.  is both the mean
                                             5.1 (Discrete) Random Variables 241

                              f (x)   = 1.5

                          .3
                          .2
                          .1

                          012345678 x

                              f (x)   = 3.0

                          .3
                          .2
                          .1

                              0 1 2 3 4 5 6 7 8 9 10 11 x
                          Figure 5.5 Two Poisson probability histograms

               and the variance for the Poisson () distribution. That is, if X has the Poisson ()
               distribution, then

Mean of the                                   e-x
  Poisson ()                         µ = EX = x       =                  (5.11)
                                             x=0 x !
 distribution

                     and              Var X = (x - )2 e-x =              (5.12)
                                                x=0 x !
Variance of the
      Poisson ()

     distribution

               Fact (5.11) is helpful in picking out which Poisson distribution might be useful in
               describing a particular "rare events" situation.

Example 6      The Poisson Distribution and Counts of -Particles
    WWW
               A classical data set of Rutherford and Geiger, reported in Philosophical Magazine
               in 1910, concerns the numbers of -particles emitted from a small bar of polonium
               and colliding with a screen placed near the bar in 2,608 periods of 8 minutes each.
               The Rutherford and Geiger relative frequency distribution has mean 3.87 and a
               shape remarkably similar to that of the Poisson probability distribution with mean
                = 3.87.
242 Chapter 5 Probability: The Mathematics of Randomness

Example 6          In a duplication of the Rutherford/Geiger experiment, a reasonable probabil-
(continued )  ity function for describing

                       S = the number of -particles striking the screen in an additional
                            8-minute period

              is then                -3.87

                                     e (3.87)                 s

                                f (s) =  s!                      for s = 0, 1, 2, . . .
                                             0                   otherwise

              Using such a model, one has (for example)

              P[at least 4 particles are recorded]

              = P[S  4]

              = f (4) + f (5) + f (6) + · · ·

              = 1 - ( f (0) + f (1) + f (2) + f (3))

              =1-      e-3.87(3.87)0 + e-3.87(3.87)1 + e-3.87(3.87)2 + e-3.87(3.87)3
                                0!                        1!     2!                      3!

              = .54

Example 7     Arrivals at a University Library

              Stork, Wohlsdorf, and McArthur collected data on numbers of students entering
              the ISU library during various periods over a week's time. Their data indicate
              that between 12:00 and 12:10 P.M. on Monday through Wednesday, an average
              of around 125 students entered. Consider modeling

              M = the number of students entering the ISU library between 12:00 and
                     12:01 next Tuesday

              Using a Poisson distribution to describe M, the reasonable choice of  would
              seem to be

                                 = 125 students (1 minute) = 12.5 students
                                      10 minutes

              For this choice,

                                    E M =  = 12.5 students
                                    
                                    Var M =  = 12.5 = 3.54 students
                                                              5.1 (Discrete) Random Variables 243

   and, for example, the probability that between 10 and 15 students (inclusive)
   arrive at the library between 12:00 and 12:01 would be evaluated as

   P[10  M  15] = f (10) + f (11) + f (12) + f (13) + f (14) + f (15)

                                      = e-12.5(12.5)10 + e-12.5(12.5)11 + e-12.5(12.5)12
                                                         10!  11!  12!

                                                         + e-12.5(12.5)13 + e-12.5(12.5)14 + e-12.5(12.5)15
                                                         13!  14!  15!

                                      = .60

Section 1 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. A discrete random variable X can be described            (b) If seven of the ten subjects correctly identify
   using the probability function                                the artificial sweetener, is this outcome strong
                                                                 evidence of a taste difference? Explain.
x  2 3 4 56
                                                         3. Suppose that a small population consists of the
f (x) .1 .2 .3 .3 .1                                         N = 6 values 2, 3, 4, 4, 5, and 6.
                                                             (a) Sketch a relative frequency histogram for this
    (a) Make a probability histogram for X. Also plot            population and compute the population mean,
         F(x), the cumulative probability function               µ, and standard deviation,  .
        for X .                                             (b) Now let X = the value of a single number se-
                                                                 lected at random from this population. Sketch
   (b) Find the mean and standard deviation of X .               a probability histogram for this variable X and
                                                                 compute EX and Var X .
2. In an experiment to evaluate a new artificial sweet-      (c) Now think of drawing a simple random sample
   ener, ten subjects are all asked to taste cola from           of size n = 2 from this small population. Make
   three unmarked glasses, two of which contain reg-             tables giving the probability distributions of the
   ular cola while the third contains cola made with             random variables
   the new sweetener. The subjects are asked to iden-
   tify the glass whose content is different from the                          X = the sample mean
   other two. If there is no difference between the
   taste of sugar and the taste of the new sweetener,                         S2 = the sample variance
   the subjects would be just guessing.
    (a) Make a table for a probability function for              (There are 15 different possible unordered sam-
                                                                 ples of 2 out of 6 items. Each of the 15 possible
X = the number of subjects correctly                             samples is equally likely to be chosen and has
      identifying the artificially                               its own corresponding x¯ and s2.) Use the tables
      sweetened cola                                             and make probability histograms for these ran-
                                                                 dom variables. Compute EX and Var X . How
under this hypothesis of no difference in taste.                 do these compare to µ and  2?
244 Chapter 5 Probability: The Mathematics of Randomness

4. Sketch probability histograms for the binomial dis-          each histogram, mark the location of the mean
   tributions with n = 5 and p = .1, .3, .5, .7, and .9.        and indicate the size of the standard deviation.
   On each histogram, mark the location of the mean
   and indicate the size of the standard deviation.         8. A process for making plate glass produces an av-
                                                                erage of four seeds (small bubbles) per 100 square
5. Suppose that an eddy current nondestructive eval-            feet. Use Poisson distributions and assess proba-
   uation technique for identifying cracks in critical          bilities that
   metal parts has a probability of around .20 of detect-       (a) a particular piece of glass 5 ft × 10 ft will
   ing a single crack of length .003 in. in a certain ma-            contain more than two seeds.
   terial. Suppose further that n = 8 specimens of this         (b) a particular piece of glass 5 ft × 5 ft will con-
   material, each containing a single crack of length                tain no seeds.
   .003 in., are inspected using this technique. Let W
   be the number of these cracks that are detected. Use     9. Transmission line interruptions in a telecommu-
   an appropriate probability model and evaluate the            nications network occur at an average rate of one
   following:                                                   per day.
    (a) P[W = 3]                                                (a) Use a Poisson distribution as a model for
   (b) P[W  2]
    (c) E W                                                           X = the number of interruptions in the next
   (d) Var W                                                                five-day work week
    (e) the standard deviation of W
                                                                     and assess P[X = 0].
6. In the situation described in Exercise 5, suppose            (b) Now consider the random variable
   that a series of specimens, each containing a sin-
   gle crack of length .003 in., are inspected. Let Y                  Y = the number of weeks in the next four
   be the number of specimens inspected in order to                          in which there are no interruptions
   obtain the first crack detection. Use an appropriate
   probability model and evaluate all of the following:              What is a reasonable probability model for
    (a) P[Y = 5]                                                     Y ? Assess P[Y = 2].
   (b) P[Y  4]                                             10. Distinguish clearly between the subjects of prob-
    (c) EY                                                      ability and statistics. Is one field a subfield of the
   (d) Var Y                                                    other?
    (e) the standard deviation of Y                        11. What is the difference between a relative fre-
                                                                quency distribution and a probability distribution?
7. Sketch probability histograms for the Poisson dis-
   tributions with means  = .5, 1.0, 2.0, and 4.0. On

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

5.2 Continuous Random Variables

                    It is often convenient to think of a random variable as not discrete but rather
                    continuous in the sense of having a whole (continuous) interval for its set of possible
                    values. The devices used to describe continuous probability distributions differ from
                    the tools studied in the last section. So the first tasks here are to introduce the
                    notion of a probability density function, to show its relationship to the cumulative
                    probability function for a continuous random variable, and to show how it is used to
                    find the mean and variance for a continuous distribution. After this, several standard
                                                                            5.2 Continuous Random Variables 245

                         continuous distributions useful in engineering applications of probability theory will
                         be discussed. That is, the normal (or Gaussian) exponential and Weibull distributions
                         are presented.

5.2.1                    Probability Density Functions
                         and Cumulative Probability Functions

                         The methods used to specify and describe probability distributions have parallels in
                         mechanics. When considering continuous probability distributions, the analogy to
                         mechanics becomes especially helpful. In mechanics, the properties of a continuous
                         mass distribution are related to the possibly varying density of the mass across its
                         region of location. Amounts of mass in particular regions are obtained from the
                         density by integration.

                              The concept in probability theory corresponding to mass density in mechanics
                         is probability density. To specify a continuous probability distribution, one needs
                         to describe "how thick" the probability is in the various parts of the set of possible
                         values. The formal definition is

Definition 12            A probability density function for a continuous random variable X is a
                         nonnegative function f (x) with

                                                               (5.13)

                              f (x) dx = 1

                         -

                         and such that for all a  b, one is willing to assign P[a  X  b] according
                         to

                                                            b  (5.14)

                         P[a  X  b] = f (x) dx

                                                          a

Mechanics analogy             A generic probability density function is pictured in Figure 5.6. In keeping with
      for probability    equations (5.13) and (5.14), the plot of f (x) does not dip below the x axis, the
                density  total area under the curve y = f (x) is 1, and areas under the curve above particular
                         intervals give probabilities corresponding to those intervals.

                              In direct analogy to what is done in mechanics, if f (x) is indeed the "density of
                         probability" around x, then the probability in an interval of small length dx around
                         x is approximately f (x) dx. (In mechanics, if f (x) is mass density around x, then
                         the mass in an interval of small length dx around x is approximately f (x) dx.) Then
                         to get a probability between a and b, one needs to sum up such f (x) dx values.
                          ab f (x) d x is exactly the limit of f (x) d x values as d x gets small. (In mechanics,
                          ab f (x) d x is the mass between a and b.) So the expression (5.14) is reasonable.
246 Chapter 5 Probability: The Mathematics of Randomness

                                                                               f (x)
                                                                                                Shaded area gives
                                                                                                P[2  X  6]

                                                                                               Total area under
                                                                                               the curve is 1

                                     0 2 6 10 14 18 22 x
                           Figure 5.6 A generic probability density function

Example 8  The Random Time Until a First Arc in the Bob Drop Experiment

           Consider once again the bob drop experiment first described in Section 1.4 and
           revisited in Example 4 in Chapter 4. In any use of the apparatus, the bob is almost
           certainly not released exactly "in sync" with the 60 cycle current that produces
           the arcs and marks on the paper tape. One could think of a random variable

               Y = the time elapsed (in seconds) from bob release until the first arc

           as  continuous  with  set  of   possible   values  (0,  1   ).
                                                                   60
               What is a plausible probability density function for Y ? The symmetry of this

           situation suggests that probability density should be constant over the interval

           (0, 160 ) and 0 outside the interval. That is, for any two values y1 and y2 in
           (0, 160 ), the probability that Y takes a value within a small interval around y1 of
           length dy (i.e., f (y1) dy approximately) should be the same as the probability
           that Y takes a value within a small interval around y2 of the same length dy (i.e.,
           f (y2) dy approximately). This forces f (y1) = f (y2), so there must be a constant
                                           1
           probability  density  on   (0,  60  ).

               Now if f (y) is to have the form

                                                   c  for     0    <       y  <                                    1
                                      f (y) =                                                                      60

                                                   0  otherwise

           for some constant c (i.e., is to be as pictured in Figure 5.7), in light of equation
           (5.13), it must be that

                0 1/60  1 = f (y) d y = 0 d y + c dy + 0 dy = c
                                                                                                                             60
                           -                       -          0                                                        1/60

           That is, c = 60, and thus,
                                             5.2 Continuous Random Variables 247

                               f (y)         Total area under the
                            c                graph of f (y) must be 1

                               0             1                    y

                                             60

                            Figure 5.7 Probability density function
                            for Y (time elapsed before arc)

                                         60  for  0  <  y  <  1                (5.15)
                            f (y) =                           60

                                         0   otherwise

                       If the function specified by equation (5.15) is adopted as a probability density for
                       Y , it is then (for example) possible to calculate that

                       P Y  1 1/100 0 1/100 = f (y) d y = 0 d y + 60 d y = .6
                       100
                            -                     -               0

For X a continuous          One point about continuous probability distributions that may at first seem coun-
  random variable,     terintuitive concerns the probability associated with a continuous random variable
         P[X = a] = 0  assuming a particular prespecified value (say, a). Just as the mass a continuous mass
                       distribution places at a single point is 0, so also is P[X = a] = 0 for a continuous
                       random variable X . This follows from equation (5.14), because

                                                               a

                            P[a  X  a] = f (x) dx = 0

                                                             a

                       One consequence of this mathematical curiosity is that when working with contin-
                       uous random variables, you don't need to worry about whether or not inequality
                       signs you write are strict inequality signs. That is, if X is continuous,

                       P[a  X  b] = P[a < X  b] = P[a  X < b] = P[a < X < b]

                            Definition 6 gave a perfectly general definition of the cumulative probability
                       function for a random variable (which was specialized in Section 5.1 to the case
                       of a discrete variable). Here equation (5.14) can be used to express the cumulative
248 Chapter 5 Probability: The Mathematics of Randomness

                            probability function for a continuous random variable in terms of an integral of its
                            probability density. That is, for X continuous with probability density f (x),

Cumulative probability                                                                         x                          (5.16)
            function for a
                                                     F(x) = P[X  x] = f (t) dt
    continuous variable
                                                                                             -

                            F(x) is obtained from f (x) by integration, and applying the fundamental theorem
                            of calculus to equation (5.16)

 Another relationship                                             d F(x) = f (x)                                          (5.17)
between F(x) and f(x)                                             dx

                            That is, f (x) is obtained from F(x) by differentiation.

Example 8                   The cumulative probability function for Y , the elapsed time from bob release
(continued )                until first arc, is easily obtained from equation (5.15). For y  0,

                                                                             y                       y

                                            F(y) = P[Y  y] =                       f (t) dt = 0 dt = 0

                                                                       -                          -

                            and for 0 < y  601 ,

                                                            y                   0                 y

                            F(y) = P[Y  y] =                   f (t) dt = 0 dt + 60 dt = 0 + 60y = 60y

                                                         -                      -              0

                            and for y > 601 ,

                                                            y                   0                 1/60              y

                            F(y) = P[Y  y] =                   f (t) dt = 0 dt +                        60 dt + 0 dt = 1

                                                         -                      -              0               1/60

                            That is,

                                                                                if y  0
                                                             0
                                                  F(y) = 60y                    if 0 < y  1/60
                                                            
                                                                                if   1   <  y
                                                                1                    60

                            A plot of F(y) is given in Figure 5.8. Comparing Figure 5.8 to Figure 5.7

                            shows  that  indeed   the  graph      of   F(y)     has  slope  0     for   y  <0  and  y  >  1   and
                                                                                                                          60
                                                     1
                            slope 60 for 0  <  y  <  60  .  That  is,  f (y) is the derivative of          F(y), as promised by

                            equation (5.17).
                           5.2 Continuous Random Variables 249
                  F ( y)
               1

               0  1             y

                  60

               Figure 5.8 Cumulative probability
               function for Y (time elapsed before arc)

                    Figure 5.8 is typical of cumulative probability functions for continuous distri-
               butions. The graphs of such cumulative probability functions are continuous in the
               sense that they are unbroken curves.

5.2.2          Means and Variances for Continuous Distributions

               A plot of the probability density f (x) is a kind of idealized histogram. It has the same
               kind of visual interpretations that have already been applied to relative frequency
               histograms and probability histograms. Further, it is possible to define a mean and
               variance for a continuous probability distribution. These numerical summaries are
               used in the same way that means and variances are used to describe data sets and
               discrete probability distributions.

Definition 13  The mean or expected value of a continuous random variable X (sometimes
               called the mean of its probability distribution) is

                                                         (5.18)

               EX = x f (x) dx

                            -

               As for discrete random variables, the notation µ is sometimes used in place of
               EX.

                    Formula (5.18) is perfectly plausible from at least two perspectives. First, the
               probability in a small interval around x of length dx is approximately f (x) dx.
               So multiplying this by x and summing as in Definition 7, one has x f (x) dx,
               and formula (5.18) is exactly the limit of such sums as dx gets small. And second,
               in mechanics the center of mass of a continuous mass distribution is of the form
               given in equation (5.18) except for division by a total mass, which for a probability
               distribution is 1.
250 Chapter 5 Probability: The Mathematics of Randomness

Example 8      Thinking of the probability density in Figure 5.7 as an idealized histogram and
(continued )
               thinking of the balance point interpretation of the mean, it is clear that EY had

               better  turn  out  to  be   1   for   the  elapsed  time  variable.  Happily,  equations  (5.18)
                                          120
               and (5.15) give

                                                           0              1/60                 

               µ = EY = y f (y) dy =                          y · 0dy +        y · 60 dy +         y · 0dy

                                      -                   -              0                    1/60

               = 30y2 1/60 = 1 sec
                             0        120

                    "Continuization" of the formula for the variance of a discrete random variable
               produces a definition of the variance of a continuous random variable.

Definition 14  The variance of a continuous random variable X (sometimes called the vari-
               ance of its probability distribution) is

                                                                                              (5.19)

               Var X = (x - EX)2 f (x) d x                     = x2 f (x) d x - (EX)2

                                 -                                   -

               The standard deviation of X is Var X. Often the notation  2 is used in
               place of Var X, and  is used in place of Var X .

Example 8      Return for a final time to the bob drop and the random variable Y . Using formula
(continued )   (5.19) and the form of Y 's probability density,

                       0  2 = Var Y = y - 1 2 1/60 · 0 d y +                        y- 1      2
                                                          120                            120
                                          -                              0                     · 60 dy
                                                                                    y- 1
                                                1 2 60                                   120  3 1/60
                                          +          y-            · 0dy =
                                                          120                          3         0
                                               1/60

                                      =1 1 2
                                         3 120

               So the standard deviation of Y is
                                       = Var Y = 1 1 2 = .0048 sec
                                                            3 120
                                                      5.2 Continuous Random Variables 251

5.2.3                The Normal Probability Distributions

                     Just as there are a number of standard discrete distributions commonly applied to
                     engineering problems, there are also a number of standard continuous probability
                     distributions. This text has already alluded to the normal or Gaussian distributions
                     and made use of their properties in producing normal plots. It is now time to introduce
                     them formally.

Definition 15        The normal or Gaussian (µ,  2) distribution is a continuous probability
                     distribution with probability density

                                f (x) =   1 e-(x-µ)2/2 2  for all x             (5.20)
                                         2  2

                     for  > 0.

                          It is not necessarily obvious, but formula (5.20) does yield a legitimate proba-
                     bility density, in that the total area under the curve y = f (x) is 1. Further, it is also
                     the case that

Normal distribution                               1 e-(x-µ)2/2 2 d x = µ
 mean and variance                               2  2
                                EX = x

                                             -

                     and

                                                       1 e-(x-µ)2/2 2 d x =  2
                                                      2  2
                                Var X = (x - µ)2

                                                  -

                     That is, the parameters µ and  2 used in Definition 15 are indeed, respectively, the
                     mean and variance (as defined in Definitions 13 and 14) of the distribution.

                          Figure 5.9 is a graph of the probability density specified by formula (5.20). The
                     bell-shaped curve shown there is symmetric about x = µ and has inflection points
                     at µ -  and µ +  . The exact form of formula (5.20) has a number of theoretical
                     origins. It is also a form that turns out to be empirically useful in a great variety of
                     applications.

                          In theory, probabilities for the normal distributions can be found directly by
                     integration using formula (5.20). Indeed, readers with pocket calculators that are
                     preprogrammed to do numerical integration may find it instructive to check some
                     of the calculations in the examples that follow, by straightforward use of formulas
                     (5.14) and (5.20). But the freshman calculus methods of evaluating integrals via
                     antidifferentiation will fail when it comes to the normal densities. They do not have
                     antiderivatives that are expressible in terms of elementary functions. Instead, special
                     normal probability tables are typically used.
252 Chapter 5 Probability: The Mathematics of Randomness
                                                                          f (x)

                                                            µ - 2 µ -  µ µ +  µ + 2        x

                      Figure 5.9 Graph of a normal probability density
                      function

                           The use of tables for evaluating normal probabilities depends on the following
                      relationship. If X is normally distributed with mean µ and variance  2,

                                                         b   1 e-(x-µ)2/2 2 d x =   (b-µ)/ 1 -z2/2  (5.21)
                                                            2  2                              e dz
                      P[a  X  b] =
                                                                                   (a-µ)/ 2
                                                       a

                      where the second inequality follows from the change of variable or substitution
                      z =  x-µ . Equation (5.21) involves an integral of the normal density with µ = 0
                      and  = 1. It says that evaluation of all normal probabilities can be reduced to the
                      evaluation of normal probabilities for that special case.

Definition 16         The normal distribution with µ = 0 and  = 1 is called the standard normal
                      distribution.

Relation between           The relationship between normal (µ,  2) and standard normal probabilities
     normal (µ,  2)   is illustrated in Figure 5.10. Once one realizes that probabilities for all normal
                      distributions can be had by tabulating probabilities for only the standard normal
 probabilities and    distribution, it is a relatively simple matter to use techniques of numerical integration
 standard normal      to produce a standard normal table. The one that will be used in this text (other forms
                      are possible) is given in Table B.3. It is a table of the standard normal cumulative
       probabilities  probability function. That is, for values z located on the table's margins, the entries
                      in the table body are

                                                                                z 1 -t2/2
                                                            (z) = F(z) =  e dt

                                                                               - 2

                      ( is routinely used to stand for the standard normal cumulative probability function,
                      instead of the more generic F.)
                                       5.2 Continuous Random Variables 253

           Normal                                 P[a  X  b]
           ( µ,  2)
           density

             µ - 2   µ- a              µ          µ+ b        µ + 2 x

           Standard           Equal areas!
           normal
           density                          a -µ  b -µ
                                       P  Z 

           -2 -1 0            1        2z

                        a -µ     b -µ
                         
                                 

           Figure 5.10 Illustration of the relationship between normal (µ,  2) and
           standard normal probabilities

Example 9  Standard Normal Probabilities
           Suppose that Z is a standard normal random variable. We will find some proba-
           bilities for Z using Table B.3.

                By a straight table look-up,

                                        P[Z < 1.76] = (1.76) = .96

           (The tabled value is .9608, but in keeping with the earlier promise to state final
           probabilities to only two decimal places, the tabled value was rounded to get .96.)
           After two table look-ups and a subtraction,

                            P[.57 < Z < 1.32] = P[Z < 1.32] - P[Z  .57]
                                                     = (1.32) - (.57)
                                                     = .9066 - .7157
                                                     = .19

           And a single table look-up and a subtraction yield a right-tail probability like

                          P[Z > -.89] = 1 - P[Z  -.89] = 1 - .1867 = .81

                As the table was used in these examples, probabilities for values z located
           on the table's margins were found in the table's body. The process can be run in
254 Chapter 5 Probability: The Mathematics of Randomness

Example 9                                                                       P[.57  Z  1.32] = .19
(continued )
                                               P[Z  1.76] = .96

                 -2 -1 0                       1 1.76 2 -2 -1                   0 .57 1 1.32 2
                                                P[Z > -.89] = .81                    P[Z > z] = .025

                 -2 -1             0           1      2 -2               -1     0         1    2
                             -.89                               -z                             z

                             Figure 5.11 Standard normal probabilities for Example 9

              reverse. Probabilities located in the table's body can be used to specify values z
              on the margins. For example, consider locating a value z such that

                                                  P[-z < Z < z] = .95

              z  will  then  put  probability  1-.95  =   .025  in  the  right  tail  of  the  standard  normal
                                                  2
              distribution--i.e., be such that (z) = .975. Locating .975 in the table body, one

              sees that z = 1.96.

                 Figure 5.11 illustrates all of the calculations for this example.

                   The last part of Example 9 amounts to finding the .975 quantile for the standard
              normal distribution. In fact, the reader is now in a position to understand the origin
              of Table 3.10 (see page 89). The standard normal quantiles there were found by
              looking in the body of Table B.3 for the relevant probabilities and then locating
              corresponding z's on the margins.

                   In mathematical symbols, for (z), the standard normal cumulative probability
              function, and Qz( p), the standard normal quantile function,

                                                   (Qz( p)) = p                                          (5.22)
                                                  Qz( (z)) = z

              Relationships (5.22) mean that Qz and are inverse functions. (In fact, the rela-
              tionship Q = F-1 is not just a standard normal phenomenon but is true in general

              for continuous distributions.)

                   Relationship (5.21) shows how to use the standard normal cumulative probabil-
              ity function to find general normal probabilities. For X normal (µ,  2) and a value
                       5.2 Continuous Random Variables 255

                       x associated with X , one converts to units of standard deviations above the mean
                       via

  z-value for a value  z = x-µ                                                            (5.23)
x of a normal (µ,  2)          

     random variable

                       and then consults the standard normal table using z instead of x.

Example 10             Net Weights of Jars of Baby Food
      WWW              J. Fisher, in his article "Computer Assisted Net Weight Control" (Quality
                       Progress, June 1983), discusses the filling of food containers by weight. In
                       the article, there is a reasonably bell-shaped histogram of individual net weights
                       of jars of strained plums with tapioca. The mean of the values portrayed is about
                       137.2 g, and the standard deviation is about 1.6 g. The declared (or label) weight
                       on jars of this product is 135.0 g.

                            Suppose that it is adequate to model

                                        W = the next strained plums and tapioca fill weight

                       with a normal distribution with µ = 137.2 and  = 1.6. And further suppose the
                       probability that the next jar filled is below declared weight (i.e., P[W < 135.0])
                       is of interest. Using formula (5.23), w = 135.0 is converted to units of standard
                       deviations above µ (converted to a z-value) as

                                                     z = 135.0 - 137.2 = -1.38
                                                                  1.6

                       Then, using Table B.3,

                                                  P[W < 135.0] = (-1.38) = .08

                       This model puts the chance of obtaining a below-nominal fill level at about 8%.
                            As a second example, consider the probability that W is within 1 gram of

                       nominal (i.e., P[134.0 < W < 136.0]). Using formula (5.23), both w1 = 134.0
                       and w2 = 136.0 are converted to z-values or units of standard deviations above
                       the mean as

                                                     z1 = 134.0 - 137.2 = -2.00
                                                                  1.6

                                                     z2 = 136.0 - 137.2 = -.75
                                                                  1.6
256 Chapter 5 Probability: The Mathematics of Randomness

Example 10     Normal µ = 137.2,  = 1.6 density           Standard normal density
 (continued )

               P[W < 135.0] = .08                         P[Z < -1.38] = .08

                134 136 138 140                           -2                  0  2
                     135 137.2                            -1.38

               Normal µ = 137.2,  = 1.6 density           Standard normal density

               P[134.0 < W < 136.0] = .20  P[-2.0 < Z < -.75] = .20

               134 136 138 140                            -2                  0  2
                                137.2
                                                                     -.75

               Figure 5.12 Normal probabilities for Example 10

               So then

                  P[134.0 < W < 136.0] = (-.75) - (-2.00) = .2266 - .0228 = .20

               The preceding two probabilities and their standard normal counterparts are shown
               in Figure 5.12.

                    The calculations for this example have consisted of starting with all of the
               quantities on the right of formula (5.23) and going from the margin of Table B.3
               to its body to find probabilities for W . An important variant on this process is to
               instead go from the body of the table to its margins to obtain z, and then--given
               only two of the three quantities on the right of formula (5.23)--to solve for the
               third.

                    For example, suppose that it is easy to adjust the aim of the filling process
               (i.e., the mean µ of W ) and one wants to decrease the probability that the next
               jar is below the declared weight of 135.0 to .01 by increasing µ. What is the
               minimum µ that will achieve this (assuming that  remains at 1.6 g)?

                    Figure 5.13 shows what to do. µ must be chosen in such a way that w =
               135.0 becomes the .01 quantile of the normal distribution with mean µ and
               standard deviation  = 1.6. Consulting either Table 3.10 or Table B.3, it is easy
               to determine that the .01 quantile of the standard normal distribution is

                                                  z = Qz(.01) = -2.33
                                    5.2 Continuous Random Variables 257
                   Normal density with mean = µ,  = 1.6
       P[W < 135.0] = .01

       135.0  µ                             w

       Figure 5.13 Normal distribution and
       P[W < 135.0] = .01

       So in light of equation (5.23) one wants

                                          -2.33 = 135.0 - µ
                                                           1.6

       i.e.,

                                               µ = 138.7 g

       An increase of about 138.7 - 137.2 = 1.5 g in fill level aim is required.
            In practical terms, the reduction in P[W < 135.0] is bought at the price

       of increasing the average give-away cost associated with filling jars so that on
       average they contain much more than the nominal contents. In some applications,
       this type of cost will be prohibitive. There is another approach open to a process
       engineer. That is to reduce the variation in fill level through acquiring more
       precise filling equipment. In terms of equation (5.23), instead of increasing µ
       one might consider paying the cost associated with reducing  . The reader is
       encouraged to verify that a reduction in  to about .94 g would also produce
       P[W < 135.0] = .01 without any change in µ.

            As Example 10 illustrates, equation (5.23) is the fundamental relationship used
       in problems involving normal distributions. One way or another, three of the four
       entries in equation (5.23) are specified, and the fourth must be obtained.

5.2.4  The Exponential Distributions (Optional )

       Section 5.1 discusses the fact that the Poisson distributions are often used as models
       for the number of occurrences of a relatively rare phenomenon in a specified interval
       of time. The same mathematical theory that suggests the appropriateness of the
       Poisson distributions in that context also suggests the usefulness of the exponential
       distributions for describing waiting times until occurrences.
258 Chapter 5 Probability: The Mathematics of Randomness

Definition 17                The exponential () distribution is a continuous probability distribution with
                             probability density

                                                                       for x > 0                    (5.24)
                                                               1 e-x/  otherwise
                                                     f (x) =  

                                                                  0

                             for  > 0.

                                  Figure 5.14 shows plots of f (x) for three different values of . Expression
                             (5.24) is extremely convenient, and it is not at all difficult to show that  is both the
                             mean and the standard deviation of the exponential () distribution. That is,

    Mean of the                                      µ = EX = x e  1 -x/ d x = 
exponential ()                                                      0

     distribution

                     and

Variance of the                              2 = Var X = (x - )2 e  1 -x/ d x = 2
exponential ()                                                         
                                                             0
     distribution

                             Further, the exponential () distribution has a simple cumulative probability
                             function,

          Exponential ()                             F(x) =  0           if x  0                    (5.25)
cumulative probability                                       1 - e-x/    if x > 0

                   function

                                              f (x)   = .5
                                        2.0
                                        1.5           = 1.0
                                        1.0           = 2.0

                                        .5

                                        0                                                        x

                                                     1.0     2.0  3.0    4.0       5.0

                                            Figure 5.14 Three exponential probability densities
                                                                  5.2 Continuous Random Variables 259

          Example 11    The Exponential Distribution and Arrivals at a University Library
(Example 7 revisited )
                        Recall that Stork, Wohlsdorf, and McArthur found the arrival rate of students at

                        the ISU library between 12:00 and 12:10 P.M. early in the week to be about 12.5

                        students  per  minute.  That  translates  to  a   1    = .08  min  average  waiting  time
                                                                         12.5
                        between student arrivals.

                        Consider observing the ISU library entrance beginning at exactly noon next

                        Tuesday and define the random variable

                        T = the waiting time (in minutes) until the first student passes through the door

                        A possible model for T is the exponential distribution with  = .08. Using it, the
                        probability of waiting more than 10 seconds ( 61 min) for the first arrival is

                                  P T > 1 = 1 - F 1 = 1 - 1 - e-1/6(.08) = .12
                                                6         6

                        This result is pictured in Figure 5.15.

                                             f (t)                             P[ T > 16 ] = .12
                                       10
                                        5

                                                      .1                 1 .2              t

                                                                         6

                                       Figure 5.15 Exponential probability for
                                       Example 11

Geometric and                The exponential distribution is the continuous analog of the geometric distribu-
    exponential         tion in several respects. For one thing, both the geometric probability function and
   distributions        the exponential probability density decline exponentially in their arguments x. For
                        another, they both possess a kind of memoryless property. If the first success in a
                        series of independent identical success-failure trials is known not to have occurred
                        through trial t0, then the additional number of trials (beyond t0) needed to produce
                        the first success is a geometric ( p) random variable (as was the total number of
                        trials required from the beginning). Similarly, if an exponential () waiting time is
                        known not to have been completed by time t0, then the additional waiting time to
260 Chapter 5 Probability: The Mathematics of Randomness

                                  completion is exponential (). This memoryless property is related to the force-of-
                                  mortality function of the distribution being constant. The force-of-mortality function
                                  for a distribution is a concept of reliability theory discussed briefly in Appendix A.4.

5.2.5            The Weibull Distributions (Optional )

                 The Weibull distributions generalize the exponential distributions and provide much
                 more flexibility in terms of distributional shape. They are extremely popular with
                 engineers for describing the strength properties of materials and the life lengths of
                 manufactured devices. The most natural way to specify these distributions is through
                 their cumulative probability functions.

Definition 18    The Weibull (, ) distribution is a continuous probability distribution with
                 cumulative probability function

                               F(x) =     0                   if x < 0          (5.26)
                                          1 - e-(x/)          if x  0

                 for parameters  > 0 and  > 0.

                      Beginning from formula (5.26), it is possible to determine properties of the
                 Weibull distributions. Differentiating formula (5.26) produces the Weibull (, )
                 probability density

Weibull (, )                              0                   if x < 0          (5.27)
   probability                               x -1e-(x/)       if x > 0
        density                f (x) =    

                 This in turn can be shown to yield the mean

Weibull (, )                   µ = E X =  1 + 1                                 (5.28)
          mean                                                                  (5.29)

                 and variance

Weibull (, )                    2 = Var X = 2   1 + 2 -                      2
       variance
                                                              1 + 1
                                            5.2 Continuous Random Variables 261

                     f (x)

                2.0           = .5

                                             = .5

                1.0  = 1.0
                                = 4.0

                0 1.0 2.0 3.0 4.0 5.0 x

                      f (x)   = .5           =1
                2.0           = 1.0
                              = 4.0
                1.0

                0            1.0       2.0  3.0    4.0     5.0  x

                      f (x)                  =4
                3.0
                                   = .5
                2.0

                                           = 1.0
                1.0  = 4.0

                0 1.0 2.0 3.0 4.0 5.0 x
                      Figure 5.16 Nine Weibull probability densities

                where (x) = 0 t x-1e-t dt is the gamma function of advanced calculus. (For
                integer values n, (n) = (n - 1)!.) These formulas for f (x), µ, and  2 are not par-
                ticularly illuminating. So it is probably most helpful to simply realize that  controls
                the shape of the Weibull distribution and that  controls the scale. Figure 5.16 shows
                plots of f (x) for several (, ) pairs.

                     Note that  = 1 gives the special case of the exponential distributions. For
                small , the distributions are decidedly right-skewed, but for  larger than about
                3.6, they actually become left-skewed. Regarding distribution location, the form of
                the distribution mean given in equation (5.28) is not terribly revealing. It is perhaps
                more helpful that the median for the Weibull (, ) distribution is

Weibull (, )                           Q(.5) = e-(.3665/)             (5.30)
        median
262 Chapter 5 Probability: The Mathematics of Randomness

                                  So, for example, for large shape parameter  the Weibull median is essentially .
                                  And formulas (5.28) through (5.30) show that for fixed  the Weibull mean, median,
                                  and standard deviation are all proportional to the scale parameter .

Example 12  The Weibull Distribution and the Strength of a Ceramic Material
            The report "Review of Workshop on Design, Analysis and Reliability Prediction
            for Ceramics--Part II" by E. Lenoe (Office of Naval Research Far East Scientific
            Bulletin, 1987) suggests that tensile strengths (MPa) of .95 mm rods of HIPped
            UBE SN-10 with 2.5% yttria material can be described by a Weibull distribution
            with  = 8.8 and median 428 MPa. Let

                         S = measured tensile strength of an additional rod (MPa)

            Under the assumption that S can be modeled using a Weibull distribution with
            the suggested characteristics, suppose that P[S  400] is needed. Using equation
            (5.30),

                                                428 = e-(.3665/8.8)

            Thus, the Weibull scale parameter is

                                                        = 446

            Then, using equation (5.26),

                                     P[S  400] = 1 - e-(400/446)8.8 = .32

            Figure 5.17 illustrates this probability calculation.

            f (s)
                  Weibull density

                   = 8.8,  = 446

            P[S  400] = .32

            300                    400  500            s

            Figure 5.17 Weibull density and P[S  400]
                                                         5.2 Continuous Random Variables 263

Section 2 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. The random number generator supplied on a cal-           (b) What adjustment to the grinding process (hold-
   culator is not terribly well chosen, in that values           ing the process standard deviation constant)
   it generates are not adequately described by a dis-           would increase the fraction of journal diam-
   tribution uniform on the interval (0, 1). Suppose             eters that will be in specifications? What ap-
   instead that a probability density                            pears to be the best possible fraction of jour-
                                                                 nal diameters inside ± .0005 in. specifications,
f (x) =  k(5 - x)            for 0 < x < 1                       given the  = .0004 in. apparent precision of
         0                   otherwise                           the grinder?

is a more appropriate model for X = the next value           (c) Suppose consideration was being given to pur-
produced by this random number generator.                        chasing a more expensive/newer grinder, capa-
(a) Find the value of k.                                         ble of holding tighter tolerances on the parts it
(b) Sketch the probability density involved here.                produces. What  would have to be associated
(c) Evaluate P[.25 < X < .75].                                   with the new machine in order to guarantee that
(d) Compute and graph the cumulative probability                 (when perfectly adjusted so that µ = 2.0000)
                                                                 the grinder would produce diameters with at
     function for X, F(x).                                       least 95% meeting 2.0000 in. ± .0005 in. spec-
(e) Calculate EX and the standard deviation of X .               ifications?

2. Suppose that Z is a standard normal random vari-      5. The mileage to first failure for a model of military
                                                            personnel carrier can be modeled as exponential
able. Evaluate the following probabilities involv-          with mean 1,000 miles.
                                                             (a) Evaluate the probability that a vehicle of this
ing Z :                                                          type gives less than 500 miles of service be-
                                                                 fore first failure. Evaluate the probability that
(a) P[Z < -.62]        (b) P[Z > 1.06]                           it gives at least 2,000 miles of service before
                                                                 first failure.
(c) P[-.37 < Z < .51] (d) P[|Z |  .47]                      (b) Find the .05 quantile of the distribution of
                                                                 mileage to first failure. Then find the .90 quan-
(e) P[|Z | > .93]      (f) P[-3.0< Z <3.0]                       tile of the distribution.

Now find numbers # such that the following state-        6. Some data analysis shows that lifetimes, x (in 106
                                                            revolutions before failure), of certain ball bearings
ments involving Z are true:                                 can be modeled as Weibull with  = 2.3 and  =
                                                            80.
(g) P[Z  #] = .90 (h) P[|Z | < #] = .90                      (a) Make a plot of the Weibull density (5.27)
                                                                 for this situation. (Plot for x between 0 and
(i) P[|Z | > #] = .03                                            200. Standard statistical software packages like
                                                                 MINITAB will have routines for evaluating this
3. Suppose that X is a normal random variable with               density. In MINITAB look under the "Calc/
                                                                 Probability Distributions/Weibull" menu.)
mean 43.0 and standard deviation 3.6. Evaluate the          (b) What is the median bearing life?
                                                             (c) Find the .05 and .95 quantiles of bearing life.
following probabilities involving X:

(a) P[X < 45.2]        (b) P[X  41.7]

(c) P[43.8 < X  47.0] (d) P[|X - 43.0|  2.0]

(e) P[|X- 43.0|>1.7]

Now find numbers # such that the following state-

ments involving X are true:

(f) P[X < #] = .95 (g) P[X  #] = .30

(h) P[|X - 43.0| > #] = .05

4. The diameters of bearing journals ground on a
   particular grinder can be described as normally dis-
   tributed with mean 2.0005 in. and standard devia-
   tion .0004 in.
    (a) If engineering specifications on these diame-
        ters are 2.0000 in. ± .0005 in., what fraction
        of these journals are in specifications?
264 Chapter 5 Probability: The Mathematics of Randomness

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

5.3 Probability Plotting (Optional )

                  Calculated probabilities are only as relevant in a given application as are the distri-
                  butions used to produce them. It is thus important to have data-based methods to
                  assess the relevance of a given continuous distribution to a given application. The
                  basic logic for making such tools was introduced in Section 3.2. Suppose you have
                  data consisting of n realizations of a random variable X , say x1  x2  · · ·  xn and
                  want to know whether a probability density with the same shape as f (x) might ade-
                  quately describe X. To investigate, it is possible to make and interpret a probability
                  plot consisting of n ordered pairs

  Ordered pairs                                     i - .5
        making a                           xi, Q n

probability plot

                  where xi is the ith smallest data value (the  i -.5  quantile of the data set) and
                                                                  n
                     i -.5          i -.5
                  Q    n    is the    n    quantile of the probability distribution specified by f (x).

                     This section will further discuss the importance of this method. First, some

                  additional points about probability plotting are made in the familiar context where

                  f (x) is the standard normal density (i.e., in the context of normal plotting). Then

                  the general applicability of the idea is illustrated by using it in assessing the appro-

                  priateness of exponential and Weibull models. In the course of the discussion, the

                  importance of probability plotting to process capability studies and life data analysis

                  will be indicated.

5.3.1             More on Normal Probability Plots

                  Definition 15 gives the form of the normal or Gaussian probability density with
                  mean µ and variance  2. The discussion that follows the definition shows that all
                  normal distributions have the same essential shape. Thus, a theoretical Q-Q plot
                  using standard normal quantiles can be used to judge whether or not there is any
                  normal probability distribution that seems a sensible model.

Example 13           Weights of Circulating U.S. Nickels
      WWW
                     Ash, Davison, and Miyagawa studied characteristics of U.S. nickels. They ob-
                     tained the weights of 100 nickels to the nearest .01 g. They found those to have
                     a mean of 5.002 g and a standard deviation of .055 g. Consider the weight of an-
                     other nickel taken from a pocket, say, U . It is sensible to think that EU  5.002 g
                     and Var U  .055 g. Further, it would be extremely convenient if a normal dis-
                     tribution could be used to describe U . Then, for example, normal distribution
                     calculations with µ = 5.002 g and  = .055 g could be used to assess

                                       P[U > 5.05] = P[the nickel weighs over 5.05 g]
                                                             5.3 Probability Plotting 265

     A way of determining whether or not the students' data support the use of
a normal model for U is to make a normal probability plot. Table 5.6 presents
the data collected by Ash, Davison, and Miyagawa. Table 5.7 shows some of the
calculations used to produce the normal probability plot in Figure 5.18.

Table 5.6
Weights of 100 U.S. Nickels

Weight (g) Frequency Weight (g)               Frequency

4.81            1                  5.00           12
                                                  10
4.86            1                  5.01            7
                                                   7
4.88            1                  5.02            5
                                                   4
4.89            1                  5.03            4
                                                   3
4.91            2                  5.04            2
                                                   3
4.92            2                  5.05            2
                                                   1
4.93            3                  5.06            1

4.94            2                  5.07

4.95            6                  5.08

4.96            4                  5.09

4.97            5                  5.10

4.98            4                  5.11

4.99            7                  5.13

Table 5.7
Example Calculations for a Normal Plot of
Nickel Weights

           i - .5                        i - .5
i 100 xi Qz 100

        1  .005              4.81        -2.576

        2  .015              4.86        -2.170

        3  .025              4.88        -1.960

        4  .035              4.89        -1.812

        5  .045              4.91        -1.695

        6  .055              4.91        -1.598

        7  .065              4.92        -1.514

      ...  ...               ...         ...

98         .975              5.10        1.960

99         .985              5.11        2.170

100        .995              5.13        2.576
266 Chapter 5 Probability: The Mathematics of Randomness

Example 13                                    Standard normal quantile                                                            2
 (continued )                                                                                                                 3

                                           2.0                                                                       43
                                                                                                            254
                                           0.0                                                        3 75

                                         -2.0                                                      97
                                                                                             4 73

                                                                                    3 44
                                                                               32 3
                                                                        22

                                                                        4.80 4.86 4.92 4.98 5.04 5.10
                                                                                            Nickel weight quantile (g)

                                                                            Figure 5.18 Normal plot of nickel weights

     At least up to the resolution provided by the graphics in Figure 5.18, the plot
is pretty linear for weights above, say, 4.90 g. However, there is some indication
that the shape of the lower end of the weight distribution differs from that of a
normal distribution. Real nickels seem to be more likely to be light than a normal
model would predict. Interestingly enough, the four nickels with weights under
4.90 g were all minted in 1970 or before (these data were collected in 1988). This
suggests the possibility that the shape of the lower end of the weight distribution
is related to wear patterns and unusual damage (particularly the extreme lower
tail represented by the single 1964 coin with weight 4.81 g).

     But whatever the origin of the shape in Figure 5.18, its message is clear. For
most practical purposes, a normal model for the random variable

                   U = the weight of a nickel taken from a pocket

will suffice. Bear in mind, though, that such a distribution will tend to slightly
overstate probabilities associated with larger weights and understate probabilities
associated with smaller weights.

     Much was made in Section 3.2 of the fact that linearity on a Q-Q plot indicates
equality of distribution shape. But to this point, no use has been made of the fact
that when there is near-linearity on a Q-Q plot, the nature of the linear relationship
gives information regarding the relative location and spread of the two distributions
involved. This can sometimes provide a way to choose sensible parameters of a
theoretical distribution for describing the data set.

     For example, a normal probability plot can be used not only to determine whether
some normal distribution might describe a random variable but also to graphically
pick out which one might be used. For a roughly linear normal plot,
Reading a mean                                                               5.3 Probability Plotting 267
    and standard
                  1. the horizontal coordinate corresponding to a vertical coordinate of 0 provides
 deviation from       a mean for a normal distribution fit to the data set, and
   a normal plot
                  2. the reciprocal of the slope provides a standard deviation (this is the differ-
                      ence between the horizontal coordinates of points with vertical coordinates
                      differing by 1).

Example 14        Normal Plotting and Thread Lengths of U-bolts

                  Table 5.8 gives thread lengths produced in the manufacture of some U-bolts for
                  the auto industry. The measurements are in units of .001 in. over nominal. The
                  particular bolts that gave the measurements in Table 5.8 were sampled from a
                  single machine over a 20-minute period.

                       Figure 5.19 gives a normal plot of the data. It indicates that (allowing for
                  the fact that the relatively crude measurement scale employed is responsible for
                  the discrete/rough appearance of the plot) a normal distribution might well have
                  been a sensible probability model for the random variable

                                   L = the actual thread length of an additional U-bolt
                                         manufactured in the same time period

                  The line eye-fit to the plot further suggests appropriate values for the mean and
                  standard deviation: µ  10.8 and   2.1. (Direct calculation with the data in
                  Table 5.8 gives a sample mean and standard deviation of, respectively, l¯  10.9
                  and s  1.9.)

                  Table 5.8
                  Measured Thread Lengths for 25 U-Bolts

                       Thread Length       Tally  Frequency
                  (.001 in. over Nominal)

                  6                                       1

                  7                                       0

                  8                                       3

                  9                                       0

                  10                                      4

                  11                                      10

                  12                                      0

                  13                                      6

                  14                                      1
268 Chapter 5 Probability: The Mathematics of Randomness

Example 14

(continued )                                 3.0

                                             2.0

                   Standard normal quantile  1.0
                                                     Intercept  10.8

                                               0

                        -1.0
                                                           Difference  2.1

                        -2.0
                                                     Line eye-fit to plot

                        -3.0
                                   6 7 8 9 10 11 12 13 14 15 16
                                      Thread length (.001 in. above nominal)

                   Figure 5.19 Normal plot of thread lengths and eye-fit line

                        In manufacturing contexts like the previous example, it is common to use the
                   fact that an approximate standard deviation can easily be read from the (reciprocal)
                   slope of a normal plot to obtain a graphical tool for assessing process potential. That
                   is, the primary limitation on the performance of an industrial machine or process
                   is typically the basic precision or short-term variation associated with it. Suppose
                   a dimension of the output of such a process or machine over a short period is
                   approximately normally distributed with standard deviation  . Then, since for any
                   normal random variable X with mean µ and standard deviation  ,

                                                  P[µ - 3 < X < µ + 3 ] > .99

6 as a process     it makes some sense to use 6 (= (µ + 3 ) - (µ - 3 )) as a measure of process
       capability  capability. And it is easy to read such a capability figure off a normal plot. Many
                   companies use specially prepared process capability analysis forms (which are in
  Example 14       essence pieces of normal probability paper) for this purpose.
   (continued )
                      Figure 5.20 is a plot of the thread length data from Table 5.8, made on a common
                      capability analysis sheet. Using the plot, it is very easy, even for someone with
                      limited quantitative background (and perhaps even lacking a basic understanding
                      of the concept of a standard deviation), to arrive at the figure

                                             Process capability  16 - 5 = 11(.001 in.)
                                                          5.3 Probability Plotting 269

              CAPABILITY ANALYSIS SHEET

              R-419-157

     Part/Dept./Supplier                 Date                                                   (0.003%)
                                                                                                    + 4
     Part Identity                       Spec.
     Operation Indentity                                                                       (0.135%)
     Person Performing Study             99.73%                                                     + 3
     Char. Measured                      (± 3 )
                                         99.994%                                           0.2
      % of 99.8                          (± 4 )
   Population
                                         Unit of Measure
                   99.5
                   99                                                                      0.5

                   98                                                                      1

                                                                                           2    (2.3%)

                                                                                                + 2

         95                                                                                5

         90                                                                                10

                                                                                                (15.9%)

                                                                                                + 

         80                                                                                20

         70                                                                                30

         60                                                                                40

         50                                                                                50 

         40                                                                                60

         30                                                                                70

         20                                                                                80

                                                                                                -

                                                                                                (15.9%)

         10                                                                                90

         5                                                                                 95
                                                                                                    -2 
         2
                                                                                           98 (2.3%)
         1                                                                                 99
                                                                                           99.5
         0.5                                                                               99.8 -3

S                                                                                               (0.135%)

T                                                                                                    -4
                                                                                                 (0.003%)
E        0.2

P

1 VALUE

2 FREQUENCY                           ++++++++++++++++++
                                  =+ =+ =+ =+ =+ =+ =+ =+ =+ =+ =+ =+ =+ =+ =+ =+ =+ =+ =
     Follow arrows and
     perform additions
     as shown (N  25)

3 EST. ACCUM. FREQ. (EAF)
4 PLOT POINTS (%) (EAF/2N) × 100

   Figure 5.20 Thread length data plotted on a capability analysis form (used with permission of
   Reynolds Metals Company)
270 Chapter 5 Probability: The Mathematics of Randomness

5.3.2       Probability Plots for Exponential and Weibull Distributions

            To illustrate the application of probability plotting to distributions that are not normal
            (Gaussian), the balance of this section considers its use with first exponential and
            then general Weibull models.

Example 15  Service Times at a Residence Hall Depot Counter
            and Exponential Probability Plotting

            Jenkins, Milbrath, and Worth studied service times at a residence hall "depot"
            counter. Figure 5.21 gives the times (in seconds) required to complete 65 different
            postage stamp sales at the counter.

                 The shape of the stem-and-leaf diagram is reminiscent of the shape of the
            exponential probability densities shown in Figure 5.14. So if one defines the
            random variable

                       T = the next time required to complete a postage stamp sale
                              at the depot counter

            an exponential distribution might somehow be used to describe T .

            0
            088899
            100000222233444444
            15677778899
            201222234
            2678999
            3022244
            36677
            423
            456788
            5
            5
            6
            6
            70
            7
            8
            87

               Figure 5.21 Stem-and-leaf plot of service times
                                             5.3 Probability Plotting 271

                            The exponential distributions introduced in Definition 17 all have the same
                       essential shape. Thus the exponential distribution with  = 1 is a convenient
                       representative of that shape. A plot of  = 1 exponential quantiles versus cor-
                       responding service time quantiles will give a tool for comparing the empirical
                       shape to the theoretical exponential shape.

                            For an exponential distribution with mean  = 1,

                                                    F(x) = 1 - e-x for x > 0

                       So for 0 < p < 1, setting F(x) = p and solving,

                                             x = - ln(1 - p)

                       That is, - ln(1 - p) = Q( p), the p quantile of this distribution. Thus, for data
                       x1  x2  · · ·  xn, an exponential probability plot can be made by plotting the
                       ordered pairs

       Points to plot                        xi , - ln 1 - i - .5                                (5.31)
for an exponential                                              n

    probability plot

                       Figure 5.22 is a plot of the points in display (5.31) for the service time data. It
                       shows remarkable linearity. Except for the fact that the third- and fourth-largest
                       service times (both 48 seconds) appear to be somewhat smaller than might be
                       predicted based on the shape of the exponential distribution, the empirical service
                       time distribution corresponds quite closely to the exponential distribution shape.

                       Exponential quantile   5

                                              4

                                              3

                                              2

                                              1

                                              0
                                                0 10 20 30 40 50 60 70 80 90

                                             About 7.5 About 24
                                                                       Data quantile

                                           Figure 5.22 Exponential probability plot and eye-fit
                                           line for the service times
272 Chapter 5 Probability: The Mathematics of Randomness

Example 15          As was the case in normal-plotting, the character of the linearity in Figure
 (continued )  5.22 also carries some valuable information that can be applied to the modeling
               of the random variable T . The positioning of the line sketched onto the plot
               indicates the appropriate location of an exponentially shaped distribution for T ,
               and the slope of the line indicates the appropriate spread for that distribution.

                    As introduced in Definition 17, the exponential distributions have positive
               density f (x) for positive x. One might term 0 a threshold value for the dis-
               tributions defined there. In Figure 5.22 the threshold value (0 = Q(0)) for the
               exponential distribution with  = 1 corresponds to a service time of roughly 7.5
               seconds. This means that to model a variable related to T with a distribution
               exactly of the form given in Definition 17, it is

                                                  S = T - 7.5

               that should be considered.
                    Further, a change of one unit on the vertical scale in the plot corresponds to

               a change on the horizontal scale of roughly

                                                  24 - 7.5 = 16.5 sec

               That is, an exponential model for S ought to have an associated spread that is
               16.5 times that of the exponential distribution with  = 1.

                    So ultimately, the data in Figure 5.21 lead via exponential probability plotting
               to the suggestion that

               S = T - 7.5

                 = the excess of the next time required to complete a postage stamp sale
                    over a threshold value of 7.5 seconds

               be described with the density        1 e-(s/16.5)  for s > 0         (5.32)
                                                  16.5            otherwise
                                                  0
                                                
                                      f (s) = 

               Probabilities involving T can be computed by first expressing them in terms of
               S and then using expression (5.32). If for some reason a density for T itself is
               desired, simply shift the density in equation (5.32) to the right 7.5 units to obtain
               the density

                           1 e-((t-7.5)/16.5)                          for t > 7.5
                         16.5                                          otherwise
               f (t) =   0

               Figure 5.23 shows probability densities for both S and T .
                                                                   5.3 Probability Plotting 273

                                     .06

                Probability density  .04
                                                                         Density for T

                                     .02
                                                  Density for
                                                  S = T - 7.5

                                          10  20               30  40                   50

                                              Service time (sec)

                                     Figure 5.23 Probability densities for both S and T

                     To summarize the preceding example: Because of the relatively simple form of
                the exponential  = 1 cumulative probability function, it is easy to find quantiles
                for this distribution. When these are plotted against corresponding quantiles of a
                data set, an exponential probability plot is obtained. On this plot, linearity indicates
                exponential shape, the horizontal intercept of a linear plot indicates an appropriate
                threshold value, and the reciprocal of the slope indicates an appropriate value for
                the exponential parameter .

                     Much the same story can be told for the Weibull distributions for any fixed .
                That is, using the form (5.26) of the Weibull cumulative probability function, it is
                straightforward to argue that for data x1  x2  · · ·  xn, a plot of the ordered pairs

Points to plot                                               i - .5 1/                      (5.33)
 for a fixed                              xi , - ln 1 -
 Weibull plot
                                                                n

                is a tool for investigating whether a variable might be described using a Weibull-
                shaped distribution for the particular  in question. On such a plot, linearity indicates
                Weibull shape , the horizontal intercept indicates an appropriate threshold value,
                and the reciprocal of the slope indicates an appropriate value for the parameter .

                     Although the kind of plot indicated by display (5.33) is easy to make and
                interpret, it is not the most common form of probability plotting associated with
                the Weibull distributions. In order to plot the points in display (5.33), a value of
                 is input (and a threshold and scale parameter are read off the graph). In most
                engineering applications of the Weibull distributions, what is needed (instead of a
                method that inputs  and can be used to identify a threshold and ) is a method that
                tacitly inputs the 0 threshold implicit in Definition 18 and can be used to identify
                 and . This is particularly true in applications to reliability, where the useful life
                or time to failure of some device is the variable of interest. It is similarly true in
                applications to material science, where intrinsically positive material properties like
                yield strength are under study.
274 Chapter 5 Probability: The Mathematics of Randomness

                           It is possible to develop a probability plotting method that allows identification
                      of values for both  and  in Definition 18. The trick is to work on a log scale. That
                      is, if X is a random variable with the Weibull (, ) distribution, then for x > 0,

                                                           F (x ) = 1 - e-(x/)

                      so that with Y = ln(X)

                                              P[Y  y] = P[X  ey]
                                                          = 1 - e-(ey /)

                      So for 0 < p < 1, setting p = P[Y  y] gives
                                                             p = 1 - e-(ey /)

                      After some algebra this implies

                                              y -  ln() = ln (- ln(1 - p))     (5.34)

                      Now y is (by design) the p quantile of the distribution of Y = ln(X ). So equation
                      (5.34) says that ln(- ln(1 - p)) is a linear function of ln(X)'s quantile function. The
                      slope of that relationship is . Further, equation (5.34) shows that when ln(- ln(1 -
                      p)) = 0, the quantile function of ln(X ) has the value ln(). So exponentiation of
                      the horizontal intercept gives . Thus, for data x1  x2  · · ·  xn, one is led to
                      consider a plot of ordered pairs

Points to plot for                            ln xi , ln - ln 1 - i - .5       (5.35)
     a 0-threshold                                                       n
      Weibull plot

   Reading  and       If data in hand are consistent with a (0-threshold) Weibull (, ) model, a reasonably
from a 0-threshold    linear plot with

        Weibull plot       1. slope  and
                           2. horizontal axis intercept equal to ln()

                      may be expected.

Example 16            Electrical Insulation Failure Voltages and Weibull Plotting
      WWW
                      The data given in the stem-and-leaf plot of Figure 5.24 are failure voltages (in
                      kv/mm) for a type of electrical cable insulation subjected to increasing voltage
                                                             5.3 Probability Plotting 275

                                     3
                                     3 9.4
                                     4 5.3
                                     4 9.2, 9.4
                                     5 1.3, 2.0, 3.2, 3.2, 4.9
                                     5 5.5, 7.1, 7.2, 7.5, 9.2
                                     6 1.0, 2.4, 3.8, 4.3
                                     6 7.3, 7.7

                               Figure 5.24 Stem-and-leaf plot of
                               insulation failure voltages

stress. They were taken from Statistical Models and Methods for Lifetime Data
by J. F. Lawless.

     Consider the Weibull modeling of

                 R = the voltage at which one additional specimen
                        of this insulation will fail

Table 5.9 shows some of the calculations needed to use display (5.35) to produce
Figure 5.25. The near-linearity of the plot in Figure 5.25 suggests that a (0-
threshold) Weibull distribution might indeed be used to describe R. A Weibull
shape parameter of roughly

                    slope of the fitted line  1 - (-4)  9.6
                                                       4.19 - 3.67

is indicated. Further, a scale parameter  with

                          ln()  horizontal intercept  4.08

and thus

                                              59

appears appropriate.
276 Chapter 5 Probability: The Mathematics of Randomness

Example 16     Table 5.9
 (continued )  Example Calculations for a 0-Threshold Weibull Plot of Failure Voltages

               i xi = ith Smallest Voltage ln(xi ) p = (i - .5)/20 ln(- ln(1 - p))

               1                    39.4       3.67                 .025                     -3.68
                                                                                             -2.55
               2                    45.3       3.81                 .075                     -2.01
                                                                                             -1.65
               3                    49.2       3.90                 .125
                                                                                                ..
               4                    49.4       3.90                 .175                        .

               ...                  ...                   ...        ...                        .95

               19                   67.3       4.21                 .925                       1.31

               20                   67.7       4.22                 .975

                    ln(-ln(1 - p))   2.0
                                                   Line eye-fit to plot

                                     1.0
                                                             About 3.67

                                       0

                                                                                 About 4.19
                                    -1.0 About 4.08

                                    -2.0

                                    -3.0

                                    -4.0

                                          3.5                  4.0        4.5

                                               ln (failure voltage)

                    Figure 5.25 0-threshold Weibull plot for insulation
                    failure voltages

                    Plotting form (5.35) is quite popular in reliability and materials applications. It is
               common to see such Weibull plots made on special Weibull paper (see Figure 5.26).
               This is graph paper whose scales are constructed so that instead of using plotting
               positions (5.35) on regular graph paper, one can use plotting positions

                                                               i - .5
                                                           xi , n
                                                                                         5.3 Probability Plotting 277

               Shape parameter ( )6.0             Weibull Probability Paper
  5.0
.99  4.0
.95
.90    3.0
.80         2.0

                                             .501.0
.70
.60
.50
.40
.30

.20

.10

.05
.04
.03

.02

.01    2 3 45                      10             20 30 40 50  100  200 300400500 1,000  10,000
    1

                                                  Figure 5.26 Weibull probability paper

                                   for data x1  x2  · · ·  xn. (The determination of  is even facilitated through
                                   the inclusion of the protractor in the upper left corner.) Further, standard statistical
                                   packages often have built-in facilities for Weibull plotting of this type.

                                        It should be emphasized that the idea of probability plotting is a quite general
                                   one. Its use has been illustrated here only with normal, exponential, and Weibull
                                   distributions. But remember that for any probability density f (x), theoretical Q-Q
                                   plotting provides a tool for assessing whether the distributional shape portrayed by
                                   f (x) might be used in the modeling of a random variable.

Section 3 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. What is the practical usefulness of the technique of        2. Explain how an approximate mean µ and standard
   probability plotting?                                          deviation  can be read off a plot of standard normal
                                                                  quantiles versus data quantiles.
278 Chapter 5 Probability: The Mathematics of Randomness

3. Exercise 3 of Section 3.2 refers to the chemical           (b) Use the method of display (5.35) and investi-
   process yield data of J. S. Hunter given in Exercise            gate whether the Weibull distribution might be
   1 of Section 3.1. There you were asked to make a                used to describe bearing load life. If a Weibull
   normal plot of those data.                                      description is sensible, read appropriate param-
    (a) If you have not already done so, use a computer            eter values from the plot. Then use the form
        package to make a version of the normal plot.              of the Weibull cumulative probability function
   (b) Use your plot to derive an approximate mean                 given in Section 5.2 to find the .05 quantile of
        and a standard deviation for the chemical pro-             the bearing load life distribution.
        cess yields.
                                                           5. The data here are from the article "Fiducial Bounds
4. The article "Statistical Investigation of the Fatigue      on Reliability for the Two-Parameter Negative Ex-
   Life of Deep Groove Ball Bearings" by J. Leiblein          ponential Distribution," by F. Grubbs (Technomet-
   and M. Zelen (Journal of Research of the National          rics, 1971). They are the mileages at first failure
   Bureau of Standards, 1956) contains the data given         for 19 military personnel carriers.
   below on the lifetimes of 23 ball bearings. The units
   are 106 revolutions before failure.                               162, 200, 271, 320, 393, 508, 539, 629,
                                                                     706, 777, 884, 1008, 1101, 1182, 1462,
         17.88, 28.92, 33.00, 41.52, 42.12, 45.60,                   1603, 1984, 2355, 2880
         48.40, 51.84, 51.96, 54.12, 55.56, 67.80,
         68.64, 68.64, 68.88, 84.12, 93.12, 98.64,             (a) Make a histogram of these data. How would
         105.12, 105.84, 127.92, 128.04, 173.40                    you describe its shape?

    (a) Use a normal plot to assess how well a normal         (b) Plot points (5.31) and make an exponential
        distribution fits these data. Then determine if            probability plot for these data. Does it appear
        bearing load life can be better represented by             that the exponential distribution can be used
        a normal distribution if life is expressed on the          to model the mileage to failure of this kind of
        log scale. (Take the natural logarithms of these           vehicle? In Example 15, a threshold service
        data and make a normal plot.) What mean and                time of 7.5 seconds was suggested by a similar
        standard deviation would you use in a normal               exponential probability plot. Does the present
        description of log load life? For these parame-            plot give a strong indication of the need for a
        ters, what are the .05 quantiles of ln(life) and           threshold mileage larger than 0 if an exponen-
        of life?                                                   tial distribution is to be used here?

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

5.4 Joint Distributions and Independence

                    Most applications of probability to engineering statistics involve not one but several
                    random variables. In some cases, the application is intrinsically multivariate. It
                    then makes sense to think of more than one process variable as subject to random
                    influences and to evaluate probabilities associated with them in combination. Take,
                    for example, the assembly of a ring bearing with nominal inside diameter 1.00 in.
                    on a rod with nominal diameter .99 in. If

                                                X = the ring bearing inside diameter
                                                Y = the rod diameter
                                                                  5.4 Joint Distributions and Independence 279

                        one might be interested in

                                          P[X < Y ] = P[there is an interference in assembly]

                        which involves both variables.
                             But even when a situation is univariate, samples larger than size 1 are essentially

                        always used in engineering applications. The n data values in a sample are usually
                        thought of as subject to chance causes and their simultaneous behavior must then
                        be modeled. The methods of Sections 5.1 and 5.2 are capable of dealing with only
                        a single random variable at a time. They must be generalized to create methods for
                        describing several random variables simultaneously.

                             Entire books are written on various aspects of the simultaneous modeling of
                        many random variables. This section can give only a brief introduction to the topic.
                        Considering first the comparatively simple case of jointly discrete random variables,
                        the topics of joint and marginal probability functions, conditional distributions,
                        and independence are discussed primarily through reference to simple bivariate
                        examples. Then the analogous concepts of joint and marginal probability density
                        functions, conditional distributions, and independence for jointly continuous random
                        variables are introduced. Again, the discussion is carried out primarily through
                        reference to a bivariate example.

5.4.1                   Describing Jointly Discrete Random Variables

                        For several discrete variables the device typically used to specify probabilities is a
                        joint probability function. The two-variable version of this is defined next.

Definition 19           A joint probability function for discrete random variables X and Y is a
                        nonnegative function f (x, y), giving the probability that (simultaneously) X
                        takes the value x and Y takes the value y. That is,

                                                   f (x, y) = P[X = x and Y = y]

          Example 17    The Joint Probability Distribution of Two Bolt Torques
(Example 1 revisited )  Return again to the situation of Brenny, Christensen, and Schneider and the
                        measuring of bolt torques on the face plates of a heavy equipment component to
                        the nearest integer. With

                                                X = the next torque recorded for bolt 3
                                                Y = the next torque recorded for bolt 4
280 Chapter 5 Probability: The Mathematics of Randomness

Example 17               the data displayed in Table 3.4 (see page 74) and Figure 3.9 suggest, for exam-
 (continued )            ple, that a sensible value for P[X = 18 and Y = 18] might be 341 , the relative
                         frequency of this pair in the data set. Similarly, the assignments

                                        P[X = 18 and Y = 17] = 2
                                                                         34

                                        P[X = 14 and Y = 9] = 0

                         also correspond to observed relative frequencies.
                              If one is willing to accept the whole set of relative frequencies defined by

                         the students' data as defining probabilities for X and Y , these can be collected
                         conveniently in a two-dimensional table specifying a joint probability function
                         for X and Y . This is illustrated in Table 5.10. (To avoid clutter, 0 entries in the
                         table have been left blank.)

                         Table 5.10
                         f (x, y) for the Bolt Torque Problem

                         y x 11 12 13 14 15 16 17 18 19 20

                         20                                                          2/34 2/34 1/34

                         19                                                    2/34

                         18             1/34 1/34                              1/34 1/34 1/34

                         17                                    2/34 1/34 1/34 2/34

                         16                                    1/34 2/34 2/34              2/34

                         15  1/34 1/34                         3/34

                         14                                    1/34                  2/34

                         13                                    1/34

        Properties of a       The probability function given in tabular form in Table 5.10 has two properties
      joint probability  that are necessary for mathematical consistency. These are that the f (x, y) values
function for X and Y     are each in the interval [0, 1] and that they total to 1. By summing up just some
                         of the f (x, y) values, probabilities associated with X and Y being configured in
                         patterns of interest are obtained.

Example 17               Consider using the joint distribution given in Table 5.10 to evaluate
 (continued )
                                                                      P[X  Y] ,
                                                              P[|X - Y |  1] ,

                                                                and P[X = 17]

                              Take first P[X  Y ], the probability that the measured bolt 3 torque is at least
                         as big as the measured bolt 4 torque. Figure 5.27 indicates with asterisks which
                         possible combinations of x and y lead to bolt 3 torque at least as large as the
    5.4 Joint Distributions and Independence 281

bolt 4 torque. Referring to Table 5.10 and adding up those entries corresponding
to the cells that contain asterisks,

P[X  Y ] = f (15, 13) + f (15, 14) + f (15, 15) + f (16, 16)
                + f (17, 17) + f (18, 14) + f (18, 17) + f (18, 18)
                + f (19, 16) + f (19, 18) + f (20, 20)

= 1 + 1 + 3 + 2 + · · · + 1 = 17
34 34 34 34       34 34

     Similar reasoning allows evaluation of P[|X - Y |  1]--the probability that
the bolt 3 and 4 torques are within 1 ft lb of each other. Figure 5.28 shows
combinations of x and y with an absolute difference of 0 or 1. Then, adding
probabilities corresponding to these combinations,

P[|X - Y |  1] = f (15, 14) + f (15, 15) + f (15, 16) + f (16, 16)

                       + f (16, 17) + f (17, 17) + f (17, 18) + f (18, 17)
                       + f (18, 18) + f (19, 18) + f (19, 20) + f (20, 20) = 18

                                                                                         34

y x 11 12 13 14 15 16 17 18 19 20

20                *

19             **

18             ***

17           ****

16        *****

15        ******

14  *******

13  ********

Figure 5.27 Combinations of bolt 3
and bolt 4 torques with x  y

y x 11 12 13 14 15 16 17 18 19 20

20             **

19             ***

18           ***

17        ***

16        ***

15  ***

14  ***

13 * * *

Figure 5.28 Combinations of bolt 3
and bolt 4 torques with |x - y|  1
282 Chapter 5 Probability: The Mathematics of Randomness

Example 17                      Finally, P[X = 17], the probability that the measured bolt 3 torque is 17 ft lb,
 (continued )              is obtained by adding down the x = 17 column in Table 5.10. That is,

                                             P[X = 17] = f (17, 17) + f (17, 18) + f (17, 19)
                                                           =1+1+2
                                                               34 34 34
                                                           =4
                                                               34

     Finding marginal           In bivariate problems like the present one, one can add down columns in a two-
probability functions      way table giving f (x, y) to get values for the probability function of X , fX (x). And
                           one can add across rows in the same table to get values for the probability function
      using a bivariate    of Y , fY (y). One can then write these sums in the margins of the two-way table.
      joint probability    So it should not be surprising that probability distributions for individual random
                           variables obtained from their joint distribution are called marginal distributions.
                 function  A formal statement of this terminology in the case of two discrete variables is
                           next.

Definition 20              The individual probability functions for discrete random variables X and
                           Y with joint probability function f (x, y) are called marginal probability
                           functions. They are obtained by summing f (x, y) values over all possible
                           values of the other variable. In symbols, the marginal probability function for
                           X is

                                                             fX(x) = f (x, y)

                                                                                              y

                           and the marginal probability function for Y is

                           fY (y) = f (x, y)

                                            x

Example 17                 Table 5.11 is a copy of Table 5.10, augmented by the addition of marginal
 (continued )              probabilities for X and Y . Separating off the margins from the two-way table
                           produces tables of marginal probabilities in the familiar format of Section 5.1.
                           For example, the marginal probability function of Y is given separately in Table
                           5.12.
                                                    5.4 Joint Distributions and Independence 283

Table 5.11
Joint and Marginal Probabilities for X and Y

  y       x 11 12 13 14 15 16 17 18 19 20 fY (y)

 20                                                       2/34 2/34 1/34 5/34
 19
 18                                                 2/34                  2/34
 17
 16                  1/34 1/34                      1/34 1/34 1/34        5/34
 15
 14                                           2/34 1/34 1/34 2/34         6/34
 13
                     1/34 2/34 2/34                                 2/34  7/34
f X (x )
          1/34 1/34                           3/34                        5/34

                                              1/34        2/34            3/34

                                              1/34                        1/34

          1/34 1/34 1/34 2/34 9/34 3/34 4/34 7/34 5/34 1/34

                                                    Table 5.12
                                                    Marginal
                                                    Probability
                                                    Function for Y

                                                      y fY (y)

                                                     13 1/34
                                                     14 3/34
                                                     15 5/34
                                                     16 7/34
                                                     17 6/34
                                                     18 5/34
                                                     19 2/34
                                                     20 5/34

                      Getting marginal probability functions from joint probability functions raises
                 the natural question whether the process can be reversed. That is, if fX (x) and fY (y)
                 are known, is there then exactly one choice for f (x, y)? The answer to this question
                 is "No." Figure 5.29 shows two quite different bivariate joint distributions that
                 nonetheless possess the same marginal distributions. The marked difference between
                 the distributions in Figure 5.29 has to do with the joint, rather than individual,
                 behavior of X and Y .

          5.4.2  Conditional Distributions and Independence
                 for Discrete Random Variables

                 When working with several random variables, it is often useful to think about what
                 is expected of one of the variables, given the values assumed by all others. For
284 Chapter 5 Probability: The Mathematics of Randomness

                    Distribution 1                             Distribution 2
               yx 1 2 3                                   yx 1 2 3

                 3 .4 0 0 .4                                3 .16 .16 .08 .4
                 2 0 .4 0 .4                                2 .16 .16 .08 .4
                 1 0 0 .2 .2                                1 .08 .08 .04 .2

                      .4 .4 .2                                   .4 .4 .2

               Figure 5.29 Two different joint distributions with the same
               marginal distributions

               example, in the bolt (X ) torque situation, a technician who has just loosened bolt
               3 and measured the torque as 15 ft lb ought to have expectations for bolt 4 torque
               (Y ) somewhat different from those described by the marginal distribution in Table
               5.12. After all, returning to the data in Table 3.4 that led to Table 5.10, the relative
               frequency distribution of bolt 4 torques for those components with bolt 3 torque
               of 15 ft lb is as in Table 5.13. Somehow, knowing that X = 15 ought to make a
               probability distribution for Y like the relative frequency distribution in Table 5.13
               more relevant than the marginal distribution given in Table 5.12.

               Table 5.13
               Relative Frequency Distribution for Bolt 4
               Torques When Bolt 3 Torque Is 15 ft lb

               y, Torque (ft lb) Relative Frequency

               13                                         1/9

               14                                         1/9

               15                                         3/9

               16                                         2/9

               17                                         2/9

                    The theory of probability makes allowance for this notion of "distribution of
               one variable knowing the values of others" through the concept of conditional
               distributions. The two-variable version of this is defined next.

Definition 21  For discrete random variables X and Y with joint probability function f (x, y),
               the conditional probability function of X given Y = y is the function of x

               f X|Y (x | y) =                             f (x, y)

                                                             f (x, y)

                                                          x
                                5.4 Joint Distributions and Independence 285

                              The conditional probability function of Y given X = x is the function
                         of y

                                fY|X(y | x) =                     f (x, y)

                                                                    f (x, y)

                                                                 y

                                Comparing Definitions 20 and 21

      The conditional                             f (x, y)                    (5.36)
probability function            f X|Y (x | y) =                               (5.37)

   for X given Y = y                               fY (y)
                                                  f (x, y)
                           and  fY|X(y | x) =
                                                   f X (x )
      The conditional
probability function

   for Y given X = x

Finding conditional           And formulas (5.36) and (5.37) are perfectly sensible. Equation (5.36) says
  distributions from
  a joint probability    that starting from f (x, y) given in a two-way table and looking only at the row
               function  specified by Y = y, the appropriate (conditional) distribution for X is given by
                         the probabilities in that row (the f (x, y) values) divided by their sum ( fY (y) =

                            x f (x, y)), so that they are renormalized to total to 1. Similarly, equation (5.37)
                         says that looking only at the column specified by X = x, the appropriate conditional
                         distribution for Y is given by the probabilities in that column divided by their sum.

Example 17               To illustrate the use of equations (5.36) and (5.37), consider several of the condi-
 (continued )            tional distributions associated with the joint distribution for the bolt 3 and bolt 4
                         torques, beginning with the conditional distribution for Y given that X = 15.

                              From equation (5.37),

                                                                              f (15, y)
                                                          fY |X (y | 15) =

                                                                               fX (15)

                         Referring to Table 5.11, the marginal probability associated with X = 15 is 349 .
                         So dividing values in the X = 15 column of that table by 349 , leads to the
                         conditional distribution for Y given in Table 5.14. Comparing this to Table 5.13,
                         indeed formula (5.37) produces a conditional distribution that agrees with
                         intuition.
286 Chapter 5 Probability: The Mathematics of Randomness

Example 17     Table 5.14
 (continued )  The Conditional Probability
               Function for Y Given X = 15

               y                                          fY |X (y | 15)

               13 1 ÷ 9 = 1
                      34                                  34 9

               14 1 ÷ 9 = 1
                      34                                  34 9

               15 3 ÷ 9 = 3
                      34                                  34 9

               16 2 ÷ 9 = 2
                      34                                  34 9

               17 2 ÷ 9 = 2
                      34                                  34 9

               Next consider fY |X (y | 18) specified by
                                                               f (18, y)

                                           fY |X (y | 18) =
                                                                fX (18)

               Consulting Table 5.11 again leads to the conditional distribution for Y given that

               X = 18, shown in Table 5.15. Tables 5.14 and 5.15 confirm that the conditional
               distributions of Y given X = 15 and given X = 18 are quite different. For exam-
               ple, knowing that X = 18 would on the whole make one expect Y to be larger
               than when X = 15.

                  Table 5.15
                  The Conditional
                  Probability Function for
                  Y Given X = 18

                  y fY |X (y | 18)

                  14                                      2/7

                  17                                      2/7

                  18                                      1/7

                  20                                      2/7

                    To make sure that the meaning of equation (5.36) is also clear, consider the
               conditional distribution of the bolt 3 torque (X ) given that the bolt 4 torque is 20
                                                  5.4 Joint Distributions and Independence 287

            (Y = 20). In this situation, equation (5.36) gives

                                                                 f (x, 20)
                                             fX|Y (x | 20) =

                                                                  fY (20)
            (Conditional probabilities for X are the values in the Y = 20 row of Table
            5.11 divided by the marginal Y = 20 value.) Thus, fX|Y (x | 20) is given in
            Table 5.16.

            Table 5.16
            The Conditional Probability
            Function for X Given Y = 20

            x      f X|Y (x | 20)

            18 2 ÷ 5 = 2
               34  34 5

            19 2 ÷ 5 = 2
               34  34 5

            20 1 ÷ 5 = 1
               34  34 5

                 The bolt torque example has the feature that the conditional distributions for Y
            given various possible values for X differ. Further, these are not generally the same
            as the marginal distribution for Y . X provides some information about Y , in that
            depending upon its value there are differing probability assessments for Y . Contrast
            this with the following example.

Example 18  Random Sampling Two Bolt 4 Torques
            Suppose that the 34 bolt 4 torques obtained by Brenny, Christensen, and Schneider
            and given in Table 3.4 are written on slips of paper and placed in a hat. Suppose
            further that the slips are mixed, one is selected, the corresponding torque is noted,
            and the slip is replaced. Then the slips are again mixed, another is selected, and
            the second torque is noted. Define the two random variables

                                   U = the value of the first torque selected

            and

                                 V = the value of the second torque selected
288 Chapter 5 Probability: The Mathematics of Randomness

Example 18     Intuition dictates that (in contrast to the situation of X and Y in Example 17) the
 (continued )  variables U and V don't furnish any information about each other. Regardless of
               what value U takes, the relative frequency distribution of bolt 4 torques in the hat
               is appropriate as the (conditional) probability distribution for V , and vice versa.
               That is, not only do U and V share the common marginal distribution given in
               Table 5.17 but it is also the case that for all u and v, both

                         fU|V (u | v) = fU (u)                              (5.38)

               and

                         fV |U (v | u) = fV (v)                             (5.39)

                    Equations (5.38) and (5.39) say that the marginal probabilities in Table 5.17
               also serve as conditional probabilities. They also specify how joint probabilities
               for U and V must be structured. That is, rewriting the left-hand side of equation
               (5.38) using expression (5.36),

                                 f (u, v)
                                          = fU (u)

                                 fV (v)

               That is,

                         f (u, v) = fU (u) fV (v)                           (5.40)

               (The same logic applied to equation (5.39) also leads to equation (5.40).) Ex-
               pression (5.40) says that joint probability values for U and V are obtained by
               multiplying corresponding marginal probabilities. Table 5.18 gives the joint prob-
               ability function for U and V .

                         Table 5.17
                         The Common Marginal
                         Probability Function for U
                         and V

                         u or v                           fU (u) or fV (v)

                           13                                  1/34
                           14                                  3/34
                           15                                  5/34
                           16                                  7/34
                           17                                  6/34
                           18                                  5/34
                           19                                  2/34
                           20                                  5/35
                                                      5.4 Joint Distributions and Independence 289

                     Table 5.18
                     Joint Probabilities for U and V

                       v     u 13          14         15     16     17     18     19     20     fV (v)
                      20                                                                        5/34
                      19               5    15         25     35     30     25     10     25    2/34
                      18            (34)2  (34)2      (34)2  (34)2  (34)2  (34)2  (34)2  (34)2  5/34
                      17                                                                        6/34
                      16               2     6         10     14     12     10      4     10    7/34
                      15            (34)2  (34)2      (34)2  (34)2  (34)2  (34)2  (34)2  (34)2  5/34
                      14                                                                        3/34
                      13               5    15         25     35     30     25     10     25    1/34
                                    (34)2  (34)2      (34)2  (34)2  (34)2  (34)2  (34)2  (34)2
                     fU (u)
                                       6    18         30     42     36     30     12     30
                                    (34)2  (34)2      (34)2  (34)2  (34)2  (34)2  (34)2  (34)2

                                       7    21         35     49     42     35     14     35
                                    (34)2  (34)2      (34)2  (34)2  (34)2  (34)2  (34)2  (34)2

                                       5    15         25     35     30     25     10     25
                                    (34)2  (34)2      (34)2  (34)2  (34)2  (34)2  (34)2  (34)2

                                       3     9         15     21     18     15      6     15
                                    (34)2  (34)2      (34)2  (34)2  (34)2  (34)2  (34)2  (34)2

                                       1     3          5      7      6      5      2      5
                                    (34)2  (34)2      (34)2  (34)2  (34)2  (34)2  (34)2  (34)2

                             1/34 3/34 5/34 7/34 6/34 5/34 2/34 5/34

                          Example 18 suggests that the intuitive notion that several random variables are
                     unrelated might be formalized in terms of all conditional distributions being equal to
                     their corresponding marginal distributions. Equivalently, it might be phrased in terms
                     of joint probabilities being the products of corresponding marginal probabilities. The
                     formal mathematical terminology is that of independence of the random variables.
                     The definition for the two-variable case is next.

Definition 22        Discrete random variables X and Y are called independent if their joint prob-
                     ability function f (x, y) is the product of their respective marginal probability
                     functions. That is, independence means that

                                           f (x, y) = fX (x) fY (y) for all x, y                (5.41)

                     If formula (5.41) does not hold, the variables X and Y are called dependent.

Independence of      (Formula (5.41) does imply that conditional distributions are all equal to their cor-
  observations in    responding marginals, so that the definition does fit its "unrelatedness" motivation.)

statistical studies       U and V in Example 18 are independent, whereas X and Y in Example 17
                     are dependent. Further, the two joint distributions depicted in Figure 5.29 give an
                     example of a highly dependent joint distribution (the first) and one of independence
                     (the second) that have the same marginals.

                          The notion of independence is a fundamental one. When it is sensible to model
                     random variables as independent, great mathematical simplicity results. Where
290 Chapter 5 Probability: The Mathematics of Randomness

               engineering data are being collected in an analytical context, and care is taken to
               make sure that all obvious physical causes of carryover effects that might influence
               successive observations are minimal, an assumption of independence between
               observations is often appropriate. And in enumerative contexts, relatively small
               (compared to the population size) simple random samples yield observations that
               can typically be considered as at least approximately independent.

Example 18     Again consider putting bolt torques on slips of paper in a hat. The method of torque
 (continued )  selection described earlier for producing U and V is not simple random sam-
               pling. Simple random sampling as defined in Section 2.2 is without-replacement
               sampling, not the with-replacement sampling method used to produce U and V .
               Indeed, if the first slip is not replaced before the second is selected, the proba-
               bilities in Table 5.18 are not appropriate for describing U and V . For example,
               if no replacement is done, since only one slip is labeled 13 ft lb, one clearly
               wants

                              f (13, 13) = P[U = 13 and V = 13] = 0

               not the value

                                                1
                              f (13, 13) = 2

                                             (34)

               indicated in Table 5.18. Put differently, if no replacement is done, one clearly
               wants to use

                                                     fV |U (13 | 13) = 0

               rather than the value

                                                                               1
                                             fV |U (13 | 13) = fV (13) =

                                                                              34

               which would be appropriate if sampling is done with replacement. Simple random
               sampling doesn't lead to exactly independent observations.

                    But suppose that instead of containing 34 slips labeled with torques, the
               hat contained 100 × 34 slips labeled with torques with relative frequencies as in
               Table 5.17. Then even if sampling is done without replacement, the probabilities
               developed earlier for U and V (and placed in Table 5.18) remain at least ap-
               proximately valid. For example, with 3,400 slips and using without-replacement
               sampling,

                                                    99
                              fV |U (13 | 13) =

                                                  3,399
                           5.4 Joint Distributions and Independence 291

                  is appropriate. Then, using the fact that

                                             f (u, v)
                           fV |U (v | u) =

                                              fU (u)

                  so that

                           f (u, v) = fV |U (v | u) fU (u)

                  without replacement, the assignment

                           f (13, 13) = 99 · 1
                                          3,399 34

                  is appropriate. But the point is that

                             99  1
                           3,399 34

                  and so

                                                      f (13, 13)  1 · 1
                                                                      34 34

                  For this hypothetical situation where the population size N = 3,400 is much
                  larger than the sample size n = 2, independence is a suitable approximate de-
                  scription of observations obtained using simple random sampling.

                       Where several variables are both independent and have the same marginal
                  distributions, some additional jargon is used.

Definition 23     If random variables X1, X2, . . . , Xn all have the same marginal distribution
                  and are independent, they are termed iid or independent and identically
                  distributed.

        When can  For example, the joint distribution of U and V given in Table 5.18 shows U and V
observations be   to be iid random variables.
modeled as iid?
                       The standard statistical examples of iid random variables are successive mea-
                  surements taken from a stable process and the results of random sampling with
                  replacement from a single population. The question of whether an iid model is
                  appropriate in a statistical application thus depends on whether or not the data-
                  generating mechanism being studied can be thought of as conceptually equivalent
                  to these.
292 Chapter 5 Probability: The Mathematics of Randomness

5.4.3                    Describing Jointly Continuous Random
                         Variables (Optional )

                         All that has been said about joint description of discrete random variables has its
                         analog for continuous variables. Conceptually and computationally, however, the
                         jointly continuous case is more challenging. Probability density functions replace
                         probability functions, and multivariate calculus substitutes for simple arithmetic.
                         So most readers will be best served in the following introduction to multivariate
                         continuous distributions by reading for the main ideas and not getting bogged down
                         in details.

                              The counterpart of a joint probability function, the device that is commonly
                         used to specify probabilities for several continuous random variables, is a joint
                         probability density. The two-variable version of this is defined next.

Definition 24            A joint probability density for continuous random variables X and Y is a
                         nonnegative function f (x, y) with

                         f (x, y) dx dy = 1

                         and such that for any region R, one is willing to assign

                         P[(X, Y )  R] =                     f (x, y) dx dy        (5.42)

                                                          R

                              Instead of summing values of a probability function to find probabilities for a
                         discrete distribution, equation (5.42) says (as in Section 5.2) to integrate a probability
                         density. The new complication here is that the integral is two-dimensional. But it
                         is still possible to draw on intuition developed in mechanics, remembering that
                         this is exactly the sort of thing that is done to specify mass distributions in several
                         dimensions. (Here, mass is probability, and the total mass is 1.)

            Example 19   Residence Hall Depot Counter Service Time
(Example 15 revisited )  and a Continuous Joint Distribution

                         Consider again the depot service time example. As Section 5.3 showed, the
                         students' data suggest an exponential model with  = 16.5 for the random
                         variable

                                        S = the excess (over a 7.5 sec threshold) time required
                                              to complete the next sale
                          5.4 Joint Distributions and Independence 293

Imagine that the true value of S will be measured with a (very imprecise) analog
stopwatch, producing the random variable

                          R = the measured excess service time

Consider the function of two variables

                 1 e-s/16.5  1     e-(r -s)2/2(.25)  for s > 0
              16.5        2 (.25)                    otherwise     (5.43)
f (s, r ) = 
              0

as a potential joint probability density for S and R. Figure 5.30 provides a
representation of f (s, r ), sketched as a surface in three-dimensional space.

     As defined in equation (5.43), f (s, r ) is nonnegative, and its integral (the
volume underneath the surface sketched in Figure 5.30 over the region in the
(s, r )-plane where s is positive) is

  f (s, r ) ds dr = 1                      e-(s/16.5)-((r-s)2/2(.25)) dr ds
                          0 - 16.5 2(.25)

                     = 1 e-s/16.5        1           e-(r-s)2/2(.25) dr ds
                        0 16.5          - 2(.25)

                     = 1 e-s/16.5 ds
                        0 16.5

                    =1

(The integral in braces is 1 because it is the integral of a normal density with

                 f(s, r)

   r                                    0 10 20 30 s
10

        0 -10

              Figure 5.30 A joint probability density for S and R
294 Chapter 5 Probability: The Mathematics of Randomness

Example 19     mean s and standard deviation .5.) Thus, equation (5.43) specifies a mathemati-
 (continued )  cally legitimate joint probability density.

                    To illustrate the use of a joint probability density in finding probabilities, first
               consider evaluating P[R > S]. Figure 5.31 shows the region in the (s, r )-plane
               where f (s, r ) > 0 and r > s. It is over this region that one must integrate in
               order to evaluate P[R > S]. Then,

               P[R > S] =         f (s, r ) ds dr

                               R

                               

               =                       f (s, r ) dr ds

                               0s

                = 1 e-s/16.5                                  1     e-(r-s)2/2(.25) dr ds
                   0 16.5                                 s 2(.25)

               = e  1 -s/16.5 1 ds
                               0 16.5                     2

               =1
                  2

               (once again using the fact that the integral in braces is a normal (mean s and
               standard deviation .5) probability).

                    As a second example, consider the problem of evaluating P[S > 20]. Figure
               5.32 shows the region over which f (s, r ) must be integrated in order to evaluate
               P[S > 20]. Then,

               P[S > 20] =          f (s, r ) ds dr
                            =
                                  R

                                

                                         f (s, r ) dr ds

                               20 -

                = 1 e-s/16.5                                  1     e-(r-s)2/2(.25)dr ds
                   20 16.5                                - 2(.25)

                = 1 e-s/16.5 ds
                   20 16.5

               = e-20/16.5

                .30
                                      5.4 Joint Distributions and Independence 295

                                                                                r

                  r
                                                                            20

                                                                                   Region

               3                                                                   where
                                                            10 s > 20

               2                                                                                        s

                                                            0                      10 20

               1     Region where r > s and                 -10

                     f (s, r) > 0

                                                            -20

                  1  2             3                  s

               Figure 5.31 Region where f (s, r) > 0        Figure 5.32 Region where f (s, r) > 0
               and r > s                                    and s > 20

                    The last part of the example essentially illustrates the fact that for X and Y with
               joint density f (x, y),

                                                            x

                     F(x) = P[X  x] =                                           f (t, y) dy dt

                                                         - -

               This is a statement giving the cumulative probability function for X . Differentiation
               with respect to x shows that a marginal probability density for X is obtained from
               the joint density by integrating out y. Putting this in the form of a definition gives
               the following.

Definition 25  The individual probability densities for continuous random variables X and
               Y with joint probability density f (x, y) are called marginal probability
               densities. They are obtained by integrating f (x, y) over all possible values of
               the other variable. In symbols, the marginal probability density function for
               X is

                                                         

                                   fX(x) =                  f (x, y) dy                         (5.44)

                                                         -

               and the marginal probability density function for Y is

                                                         

                                   fY (y) =                 f (x, y) dx                         (5.45)

                                                         -
296 Chapter 5 Probability: The Mathematics of Randomness

                    Compare Definitions 20 and 25 (page 282). The same kind of thing is done
               for jointly continuous variables to find marginal distributions as for jointly discrete
               variables, except that integration is substituted for summation.

Example 19     Starting with the joint density specified by equation (5.43), it is possible to arrive
 (continued )  at reasonably explicit expressions for the marginal densities for S and R. First
               considering the density of S, Definition 25 declares that for s > 0,

               fS(s) =  1 e-s/16.5  1                                   e-(r-s)2/2(.25) dr
                          - 16.5                          2 (.25)

               = 1 e-s/16.5
                  16.5

               Further, since f (s, r ) is 0 for negative s, if s < 0,

                                                   

                                 fS(s) = 0 dr = 0

                                                 -

               That is, the form of f (s, r ) was chosen so that (as suggested by Example 15)
               S has an exponential distribution with mean  = 16.5.

                    The determination of fR(r ) is conceptually no different than the determi-
               nation of fS(s), but the details are more complicated. Some work (involving
               completion of a square in the argument of the exponential function and recogni-
               tion of an integral as a normal probability) will show the determined reader that
               for any r ,

               f R(r ) =   1                              e-(s/16.5)-((r-s)2/2(.25)) d s
                          0 16.5 2(.25)

               = 1 1-                                     1 - 2r  exp 1 - r                  (5.46)
                  16.5                                    33             2,178 16.5

               where, as usual, is the standard normal cumulative probability function. A
               graph of fR(r ) is given in Figure 5.33.

                          fR(r)

               -5 0              5 10 15 20 25 30                                         r

               Figure 5.33 Marginal probability density for R
                                  5.4 Joint Distributions and Independence 297

                                  The marginal density for R derived from equation (5.43) does not belong to
                             any standard family of distributions. Indeed, there is generally no guarantee that the
                             process of finding marginal densities from a joint density will produce expressions
                             for the densities even as explicit as that in display (5.46).

5.4.4                        Conditional Distributions and Independence
                             for Continuous Random Variables (Optional )

                             In order to motivate the definition for conditional distributions derived from a joint
                             probability density, consider again Definition 21 (page 284). For jointly discrete
                             variables X and Y , the conditional distribution for X given Y = y is specified by
                             holding y fixed and treating f (x, y) as a probability function for X after appropri-
                             ately renormalizing it--i.e., seeing that its values total to 1. The analogous operation
                             for two jointly continuous variables is described next.

Definition 26                For continuous random variables X and Y with joint probability density
                             f (x, y), the conditional probability density function of X given Y = y,
                             is the function of x

                                  f X|Y (x | y) =                    f (x, y)

                                                                  

                                                                      f (x, y) dx

                                                                 -

                             The conditional probability density function of Y given X = x is the function
                             of y

                                  fY|X(y | x) =                      f (x, y)

                                                                  

                                                                      f (x, y) dy

                                                                 -

                                  Definitions 25 and 26 lead to

Conditional probability                             f (x, y)                       (5.47)
              density for X       f X|Y (x | y) =                                  (5.48)
                given Y = y
                                                     fY (y)

                             and

Conditional probability                             f (x, y)
              density for Y       fY|X(y | x) =
                given X = x
                                                     f X (x )
298 Chapter 5 Probability: The Mathematics of Randomness
                                                               f(x, y)

                y
                                                                                            x

                Figure 5.34 A Joint probability density f (x, y) and the
                shape of a conditional density for X given a value of Y

Geometry of     Expressions (5.47) and (5.48) are formally identical to the expressions (5.36) and
 conditional
     densities  (5.37) relevant for discrete variables. The geometry indicated by equation (5.47) is
                that the shape of fX|Y (x | y) as a function of x is determined by cutting the f (x, y)
                surface in a graph like that in Figure 5.34 with the Y = y-plane. In Figure 5.34,
                the divisor in equation (5.47) is the area of the shaded figure above the (x, y)-plane
                below the f (x, y) surface on the Y = y plane. That division serves to produce a
                function of x that will integrate to 1. (Of course, there is a corresponding geometric
                story told for the conditional distribution of Y given X = x in expression (5.48)).

Example 19      In the service time example, it is fairly easy to recognize the conditional distribu-
 (continued )   tion of R given S = s as having a familiar form. For s > 0, applying expression
                (5.48),

                f R|S(r | s) =  f (s, r )                               = f (s, r ) ÷   1 e-s/16.5
                                                                                       16.5
                                f S (s )

                which, using equation (5.43), gives

                                     1                                           e-(r -s)2/2(.25)   (5.49)
                f R|S(r | s) = 
                                                                        2 (.25)

                That is, given that S = s, the conditional distribution of R is normal with mean
                s and standard deviation .5.

                     This realization is consistent with the bell-shaped cross sections of f (s, r )
                shown in Figure 5.30. The form of fR|S(r | s) given in equation (5.49) says that
                the measured excess service time is the true excess service time plus a normally
                distributed measurement error that has mean 0 and standard deviation .5.
                                                                   5.4 Joint Distributions and Independence 299

                              It is evident from expression (5.49) (or from the way the positions of the bell-
                         shaped contours on Figure 5.30 vary with s) that the variables S and R ought to be
                         called dependent. After all, knowing that S = s gives the value of R except for a
                         normal error of measurement with mean 0 and standard deviation .5. On the other
                         hand, had it been the case that all conditional distributions of R given S = s were
                         the same (and equal to the marginal distribution of R), S and R should be called
                         independent. The notion of unchanging conditional distributions, all equal to their
                         corresponding marginal, is equivalently and more conveniently expressed in terms
                         of the joint probability density factoring into a product of marginals. The formal
                         version of this for two variables is next.

Definition 27            Continuous random variables X and Y are called independent if their joint
                         probability density function f (x, y) is the product of their respective marginal
                         probability densities. That is, independence means that

                         f (x, y) = fX (x) fY (y) for all x, y  (5.50)

                         If expression (5.50) does not hold, the variables X and Y are called dependent.

                         Expression (5.50) is formally identical to expression (5.41), which appeared in Def-
                         inition 22 for discrete variables. The type of factorization given in these expressions
                         provides great mathematical convenience.

                              It remains in this section to remark that the concept of iid random variables
                         introduced in Definition 23 is as relevant to continuous cases as it is to discrete
                         ones. In statistical contexts, it can be appropriate where analytical problems are free
                         of carryover effects and in enumerative problems where (relatively) small simple
                         random samples are being described.

            Example 20   Residence Hall Depot Counter Service Times and iid Variables
(Example 15 revisited )  Returning once more to the service time example of Jenkins, Milbrath, and Worth,
                         consider the next two excess service times encountered,

                           S1 = the first/next excess (over a threshold of 7.5 sec) time required
                                   to complete a postage stamp sale at the residence hall service counter

                           S2 = the second excess service time

                         To the extent that the service process is physically stable (i.e., excess service times
                         can be thought of in terms of sampling with replacement from a single population),
                         an iid model seems appropriate for S1 and S2. Treating excess service times as
300 Chapter 5 Probability: The Mathematics of Randomness

Example 20     marginally exponential with mean  = 16.5 thus leads to the joint density for S1
 (continued )  and S2:

                                                                1 e-(s1+s2)/16.5  if s1 > 0 and s2 > 0
                                                            (16.5)2               otherwise
                f (s1, s2) =                                0

Section 4 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Explain in qualitative terms what it means for two               What is the mean of this conditional distribu-
   random variables X and Y to be independent. What                 tion?
   advantage is there when X and Y can be described
   as independent?                                          3. A laboratory receives four specimens having iden-
                                                               tical appearances. However, it is possible that (a
2. Quality audit records are kept on numbers of major          single unknown) one of the specimens is contam-
   and minor failures of circuit packs during burn-in          inated with a toxic material. The lab must test the
   of large electronic switching devices. They indicate        specimens to find the toxic specimen (if in fact one
   that for a device of this type, the random variables        is contaminated). The testing plan first put forth
                                                               by the laboratory staff is to test the specimens one
              X = the number of major failures                 at a time, stopping when (and if) a contaminated
                                                               specimen is found.
   and                                                             Define two random variables

              Y = the number of minor failures                     X = the number of contaminated specimens

   can be described at least approximately by the ac-          and
   companying joint distribution.
                                                                   Y = the number of specimens tested

yx 0 1 2                                                    Let p = P[X = 0] and therefore P[X = 1] =
                                                            1 - p.
0  .15 .05 .01                                              (a) Give the conditional distributions of Y given

1  .10 .08 .01                                                   X = 0 and X = 1 for the staff's initial test-
                                                                 ing plan. Then use them to determine the joint
2  .10 .14 .02                                                   probability function of X and Y . (Your joint
                                                                 distribution will involve p, and you may sim-
3  .10 .08 .03                                                   ply fill out tables like the accompanying ones.)

4  .05 .05 .03

(a) Find the marginal probability functions for             y fY |X (y | 0)       y fY |X (y | 1)
     both X and Y -- fX (x) and fY (y).
                                                            1                     1
(b) Are X and Y independent? Explain.                       2                     2
(c) Find the mean and variance of X--EX and                 3                     3
                                                            4                     3
     Var X.
(d) Find the mean and variance of Y --EY and

     Var Y .
(e) Find the conditional probability function for Y ,

     given that X = 0--i.e., that there are no major
     circuit pack failures. (That is, find fY |X (y | 0).)
                                                            5.4 Joint Distributions and Independence 301

                                        f (x, y)            (b) Evaluate P[Y - X < 0], the probability of an
                                                                 interference in assembly.
                           y x01
                                                            5. Suppose that a pair of random variables have the
                           1                                   joint probability density
                           2
                           3                                                        if 0  x  1
                           4                                             4x(1 - y)  and 0  y  1
                                                            f (x, y) = 
   (b) Based on your work in (a), find the marginal                                 otherwise
        distribution of Y . What is EY , the mean num-                      0
        ber of specimens tested using the staff's origi-
        nal plan?                                               (a) Find the marginal probability densities for X
                                                                    and Y . What is the mean of X ?
    (c) A second testing method devised by the staff
        involves testing composite samples of material         (b) Are X and Y independent? Explain.
        taken from possibly more than one of the origi-         (c) Evaluate P[X + 2Y  1] .
        nal specimens. By initially testing a composite        (d) Find the conditional probability density for X
        of all four specimens, then (if the first test re-
        veals the presence of toxic material) following             given that Y = .5. (Find fX|Y (x | .5).) What is
        up with a composite of two, and then an ap-                 the mean of this (conditional) distribution?
        propriate single specimen, it is possible to do
        the lab's job in one test if X = 0, and in three    6. An engineering system consists of two subsystems
        tests if X = 1. Suppose that because testing is        operating independently of each other. Let
        expensive, it is desirable to hold the number
        of tests to a minimum. For what values of p         X = the time till failure of the first subsystem
        is this second method preferable to the first?
        (Hint: What is EY for this second method?)          and

4. A machine element is made up of a rod and a              Y = the time till failure of the second subsystem
   ring bearing. The rod must fit through the bearing.
   Model                                                    Suppose that X and Y are independent exponen-
                                                            tial random variables each with mean  = 1 (in
                 X = the diameter of the rod                appropriate time units).
                                                            (a) Write out the joint probability density for X
   and
                                                                 and Y . Be sure to state carefully where the
        Y = the inside diameter of the ring bearing              density is positive and where it is 0.
                                                            Suppose first that the system is a series system (i.e.,
   as independent random variables, X uniform on            one that fails when either of the subsystems fail).
   (1.97, 2.02) and Y uniform on (2.00, 2.06).              (b) The probability that the system is still func-
   ( fX (x) = 1/.05 for 1.97 < x < 2.02, while                   tioning at time t > 0 is then
    fX (x) = 0 otherwise. Similarly, fY (y) = 1/.06 for
   2.00 < y < 2.06, while fY (y) = 0 otherwise.)                 P[X > t and Y > t]
   With this model, do the following:
    (a) Write out the joint probability density for X           Find this probability using your answer to (a).
                                                                (What region in the (x, y)-plane corresponds
        and Y . (It will be positive only when 1.97 <           to the possibility that the system is still func-
         x < 2.02 and 2.00 < y < 2.06.)                         tioning at time t?)
                                                            (c) If one then defines the random variable

                                                                 T = the time until the system fails
302 Chapter 5 Probability: The Mathematics of Randomness

     the cumulative probability function for T is       (d) The probability that the system has failed by
                                                             time t is
             F(t) = 1 - P[X > t and Y > t]
                                                                            P[X  t and Y  t]
     so that your answer to (b) can be used to find
     the distribution for T . Use your answer to (b)         Find this probability using your answer to
     and some differentiation to find the probability        part (a).
     density for T . What kind of distribution does     (e) Now, as before, let T be the time until the
     T have? What is its mean?                               system fails. Use your answer to (d) and some
Suppose now that the system is a parallel system             differentiation to find the probability density
(i.e., one that fails only when both subsystems fail).       for T . Then calculate the mean of T .

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

5.5 Functions of Several Random Variables

                    The last section introduced the mathematics used to simultaneously model several
                    random variables. An important engineering use of that material is in the analysis
                    of system outputs that are functions of random inputs.

                         This section studies how the variation seen in an output random variable depends
                    upon that of the variables used to produce it. It begins with a few comments on what
                    is possible using exact methods of mathematical analysis. Then the simple and
                    general tool of simulation is introduced. Next, formulas for means and variances
                    of linear combinations of random variables and the related propagation of error
                    formulas are presented. Last is the pervasive central limit effect, which often causes
                    variables to have approximately normal distributions.

5.5.1       The Distribution of a Function of Random Variables

            The problem considered in this section is this. Given a joint distribution for the
            random variables X, Y, . . . , Z and a function g(x, y, . . . , z), the object is to predict
            the behavior of the random variable

                                                        U = g(X, Y, . . . , Z)  (5.51)

            In some special simple cases, it is possible to figure out exactly what distribution U
            inherits from X, Y, . . . , Z .

Example 21  The Distribution of the Clearance Between Two Mating Parts
            with Randomly Determined Dimensions

            Suppose that a steel plate with nominal thickness .15 in. is to rest in a groove
            of nominal width .155 in., machined on the surface of a steel block. A lot
            of plates has been made and thicknesses measured, producing the relative fre-
      5.5 Functions of Several Random Variables 303

Table 5.19
Relative Frequency Distribution of Plate
Thicknesses

Plate Thickness (in.) Relative Frequency

.148      .4

.149      .3

.150      .3

Table 5.20
Relative Frequency Distribution of Slot
Widths

Slot Width (in.) Relative Frequency

.153  .2

.154  .2

.155  .4

.156  .2

quency distribution in Table 5.19; a relative frequency distribution for the slot
widths measured on a lot of machined blocks is given in Table 5.20.

     If a plate is randomly selected and a block is separately randomly selected,
a natural joint distribution for the random variables

                                  X = the plate thickness
                                  Y = the slot width

is one of independence, where the marginal distribution of X is given in Table
5.19 and the marginal distribution of Y is given in Table 5.20. That is, Table 5.21
gives a plausible joint probability function for X and Y .

     A variable derived from X and Y that is of substantial potential interest is
the clearance involved in the plate/block assembly,

                                         U =Y-X

Notice that taking the extremes represented in Tables 5.19 and 5.20, U is guaran-
teed to be at least .153 - .150 = .003 in. but no more than .156 - .148 = .008 in.
In fact, much more than this can be said. Looking at Table 5.21, one can see that
the diagonals of entries (lower left to upper right) all correspond to the same value
of Y - X. Adding probabilities on those diagonals produces the distribution of
U given in Table 5.22.
304 Chapter 5 Probability: The Mathematics of Randomness

Example 21            Table 5.21
 (continued )         Marginal and Joint Probabilities for X and Y

                      y x .148 .149 .150 fY (y)

                      .156    .08 .06 .06                     .2

                      .155    .16 .12 .12                     .4

                      .154    .08 .06 .06                     .2

                      .153    .08 .06 .06                     .2

                      fX (x)  .4 .3 .3

                            Table 5.22
                            The Probability Function for the
                            Clearance U = Y - X

                               u f (u)

                             .003 .06
                             .004 .12 = .06 + .06
                             .005 .26 = .08 + .06 + .12
                             .006 .26 = .08 + .12 + .06
                             .007 .22 = .16 + .06
                             .008 .08

                           Example 21 involves a very simple discrete joint distribution and a very simple
                      function g--namely, g(x, y) = y - x. In general, exact complete solution of the
                      problem of finding the distribution of U = g(X, Y, . . . , Z ) is not practically possi-
                      ble. Happily, for many engineering applications of probability, approximate and/or
                      partial solutions suffice to answer the questions of practical interest. The balance
                      of this section studies methods of producing these approximate and/or partial de-
                      scriptions of the distribution of U , beginning with a brief look at simulation-based
                      methods.

         5.5.2        Simulations to Approximate the Distribution
                      of U = g(X, Y, . . . , Z )
Simulation for
  independent         Many computer programs and packages can be used to produce pseudorandom
     X, Y, . . . , Z  values, intended to behave as if they were realizations of independent random
                      variables following user-chosen marginal distributions. If the model for X, Y, . . . , Z
                      is one of independence, it is then a simple matter to generate a simulated value for
                      each of X, Y, . . . , Z and plug those into g to produce a simulated value for U .
                      If this process is repeated a number of times, a relative frequency distribution for
                      these simulated values of U is developed. One might reasonably use this relative
                      frequency distribution to approximate an exact distribution for U .
                   5.5 Functions of Several Random Variables 305

Example 22  Uncertainty in the Calculated Efficiency of an Air Solar Collector

            The article "Thermal Performance Representation and Testing of Air Solar Col-
            lectors" by Bernier and Plett (Journal of Solar Energy Engineering, May 1988)
            considers the testing of air solar collectors. Its analysis of thermal performance
            based on enthalpy balance leads to the conclusion that under inward leakage
            conditions, the thermal efficiency of a collector can be expressed as

            Efficiency = MoC(To - Ti) + (Mo - Mi)C(Ti - Ta)
                                                  GA

                =  C   MoTo - MiTi - (Mo - Mi)Ta                 (5.52)

                   GA

            where

                    C = air specific heat (J/kgC)
                    G = global irradiance incident on the plane of the collector (W/m2)
                    A = collector gross area (m2)

                   Mi = inlet mass flowrate (kg/s)
                  Mo = outlet mass flowrate (kg/s)
                   Ta = ambient temperature (C)
                    Ti = collector inlet temperature (C)
                   To = collector outlet temperature (C)

            The authors further give some uncertainty values associated with each of the terms
            appearing on the right side of equation (5.52) for an example set of measured
            values of the variables. These are given in Table 5.23.

            Table 5.23
            Reported Uncertainties in the Measured Inputs
            to Collector Efficiency

            Variable Measured Value       Uncertainty

            C      1003.8            1.004 (i.e., ± .1%)

            G      1121.4            33.6 (i.e., ± 3%)

            A          1.58          .005

            Mi         .0234 .00035 (i.e., ± 1.5%)

            Mo         .0247 .00037 (i.e., ± 1.5%)

            Ta     -13.22            .5

            Ti     -6.08             .5

            To        24.72          .5*

            *This value is not given explicitly in the article.
306 Chapter 5 Probability: The Mathematics of Randomness

Example 22          Plugging the measured values from Table 5.23 into formula (5.52) produces
 (continued )  a measured efficiency of about .44. But how good is the .44 value? That is, how
               do the uncertainties associated with the measured values affect the reliability of
      WWW      the .44 figure? Should you think of the calculated solar collector efficiency as .44
               plus or minus .001, or plus or minus .1, or what?

                    One way of approaching this is to ask the related question, "What would
               be the standard deviation of Efficiency if all of C through To were independent
               random variables with means approximately equal to the measured values and
               standard deviations related to the uncertainties as, say, half of the uncertainty
               values?" (This "two sigma" interpretation of uncertainty appears to be at least
               close to the intention in the original article.)

                    Printout 1 is from a MINITAB session in which 100 normally distributed
               realizations of variables C through To were generated (using means equal to
               measured values and standard deviations equal to half of the corresponding
               uncertainties) and the resulting efficiencies calculated. (The routine under the
               "Calc/Random Data/Normal" menu was used to generate the realizations of
               C through To. The "Calc/Calculator" menu was used to combine these val-
               ues according to equation (5.52). Then routines under the "Stat/Basic Statis-
               tics/Describe" and "Graph/Character Graphs/Stem-and-Leaf" menus were used
               to produce the summaries of the simulated efficiencies.) The simulation produced
               a roughly bell-shaped distribution of calculated efficiencies, possessing a mean
               value of approximately .437 and standard deviation of about .009. Evidently,
               if one continues with the understanding that uncertainty means something like
               "2 standard deviations," an uncertainty of about .02 is appropriate for the nominal
               efficiency figure of .44.

               Printout 1 Simulation of Solar Collector Efficiency

               Descriptive Statistics

               Variable           N      Mean              Median     TrMean       StDev  SE Mean
               Efficien        100   0.43729              0.43773    0.43730    0.00949   0.00095

               Variable  Minimum     Maximum                     Q1         Q3
               Efficien  0.41546     0.46088              0.43050    0.44426

               Character Stem-and-Leaf Display

               Stem-and-leaf of Efficien N = 100
               Leaf Unit = 0.0010

                  5  41 58899
                10   42 22334
                24   42 66666777788999
                39   43 001112233333444
               (21)  43 555556666777889999999
                40   44 00000011122333444
                                                    5.5 Functions of Several Random Variables 307

                    23 44 555556667788889
                     8 45 023344
                     2 45 7
                     1 46 0

Practical cautions       The beauty of Example 22 is the ease with which a simulation can be employed
                    to approximate the distribution of U . But the method is so powerful and easy to use
                    that some cautions need to be given about the application of this whole topic before
                    going any further.

                         Be careful not to expect more than is sensible from a derived probability distri-
                    bution ("exact" or approximate) for

                    U = g(X, Y, . . . , Z)

                    The output distribution can be no more realistic than are the assumptions used
                    to produce it (i.e., the form of the joint distribution and the form of the function
                    g(x, y, . . . , z)). It is all too common for people to apply the methods of this section
                    using a g representing some approximate physical law and U some measurable
                    physical quantity, only to be surprised that the variation in U observed in the real
                    world is substantially larger than that predicted by methods of this section. The fault
                    lies not with the methods, but with the naivete of the user. Approximate physical
                    laws are just that, often involving so-called constants that aren't constant, using
                    functional forms that are too simple, and ignoring the influence of variables that
                    aren't obvious or easily measured. Further, although independence of X, Y, . . . , Z
                    is a very convenient mathematical property, its use is not always justified. When
                    it is inappropriately used as a model assumption, it can produce an inappropriate
                    distribution for U . For these reasons, think of the methods of this section as useful
                    but likely to provide only a best-case picture of the variation you should expect
                    to see.

5.5.3               Means and Variances for Linear Combinations
                    of Random Variables

                    For engineering purposes, it often suffices to know the mean and variance for U
                    given in formula (5.51) (as opposed to knowing the whole distribution of U ). When
                    this is the case and g is linear, there are explicit formulas for these.

Proposition 1       If X, Y, . . . , Z are n independent random variables and a0, a1, a2, . . . , an are
                    n + 1 constants, then the random variable U = a0 + a1 X + a2Y + · · · + an Z
                    has mean

                    EU = a0 + a1EX + a2 EY + · · · + anEZ  (5.53)
308 Chapter 5 Probability: The Mathematics of Randomness                                                   (5.54)
                                           and variance
                                                        Var U = a12 Var X + a22 Var Y + · · · + an2 Var Z

               Formula (5.53) actually holds regardless of whether or not the variables X, Y, . . . , Z
               are independent, and although formula (5.54) does depend upon independence, there
               is a generalization of it that can be used even if the variables are dependent. However,
               the form of Proposition 1 given here is adequate for present purposes.

                    One type of application in which Proposition 1 is immediately useful is that of
               geometrical tolerancing problems, where it is applied with a0 = 0 and the other ai 's
               equal to plus and minus 1's.

Example 21     Consider again the situation of the clearance involved in placing a steel plate
 (continued )  in a machined slot on a steel block. With X , Y , and U being (respectively) the
               plate thickness, slot width, and clearance, means and variances for these variables
               can be calculated from Tables 5.19, 5.20, and 5.22, respectively. The reader is
               encouraged to verify that

                                      EX = .1489 and Var X = 6.9 × 10-7
                                     EY = .1546 and Var Y = 1.04 × 10-6

               Now, since

                           U = Y - X = (-1)X + 1Y

               Proposition 1 can be applied to conclude that

                          EU = -1EX + 1EY = -.1489 + .1546 = .0057 in.
                        Var U = (-1)26.9 × 10-7 + (1)21.04 × 10-6 = 1.73 × 10-6

               so that

                           
                             Var U = .0013 in.

               It is worth the effort to verify that the mean and standard deviation of the clearance
               produced using Proposition 1 agree with those obtained using the distribution of
               U given in Table 5.22 and the formulas for the mean and variance given in Section
               5.1. The advantage of using Proposition 1 is that if all that is needed are EU
               and Var U , there is no need to go through the intermediate step of deriving the
                                                             5.5 Functions of Several Random Variables 309

                         distribution of U . The calculations via Proposition 1 use only characteristics of
                         the marginal distributions.

                              Another particularly important use of Proposition 1 concerns n iid random vari-
                         ables where each ai is n1 . That is, in cases where random variables X1, X2, . . . , Xn
                         are conceptually equivalent to random selections (with replacement) from a single

                         numerical population, Proposition 1 tells how the mean and variance of the random

                         variable

                                       1     1     1
                                 X = X1 + X2 + · · · + Xn
                                       n     n     n

                         are related to the population parameters µ and  2. For independent variables
                         X1, X2, . . . , Xn with common mean µ and variance  2, Proposition 1 shows that

  The mean of an              1     1           1                          1
  average of n iid            EX = EX1 + EX2 + · · · + E Xn = n µ = µ                   (5.55)
random variables              n     n           n                          n

                         and  Var X = n1 2 Var X1 + n1 2 Var X2 + · · · +  n1 2 Var Xn
                              = n 1 22 = 2
The variance of an                                                                      (5.56)
   average of n iid
                                 n        n
 random variables

                         Since  2/n is decreasing in n, equations (5.55) and (5.56) give the reassuring picture
                         of X having a probability distribution centered at the population mean µ, with spread
                         that decreases as the sample size increases.

            Example 23   The Expected Value and Standard Deviation
(Example 15 revisited )  for a Sample Mean Service Time

                         To illustrate the application of formulas (5.55) and (5.56), consider again the
                         stamp sale service time example. Suppose that the exponential model with  =
                         16.5 that was derived in Example 15 for excess service times continues to be
                         appropriate and that several more postage stamp sales are observed and excess
                         service times noted. With

                                       Si = the excess (over a 7.5 sec threshold) time required
                                              to complete the ith additional stamp sale
310 Chapter 5 Probability: The Mathematics of Randomness

Example 23     consider what means and standard deviations are associated with the probability
 (continued )
               distributions of the sample average, S, of first the next 4 and then the next 100

               excess service times.

                    S1, S2, . . . , S100 are, to the extent that the service process is physically stable,
               reasonably modeled as independent, identically distributed, exponential random
               variables with mean  = 16.5. The exponential distribution with mean  = 16.5
               has variance equal to 2 = (16.5)2. So, using formulas (5.55) and (5.56), for the
               first 4 additional service times,

                  E S =  = 16.5 sec

                  Var S =                                    2 = 8.25 sec
                                                             4

               Then, for the first 100 additional service times,

                  E S =  = 16.5 sec

                  Var S =                                 2 = 1.65 sec
                                                          100

               Notice that going from a sample size of 4 to a sample size of 100 decreases the
               standard deviation of S by a factor of 5 (= 4100 ).

                    Relationships (5.55) and (5.56), which perfectly describe the random behavior
               of X under random sampling with replacement, are also approximate descriptions of
               the behavior of X under simple random sampling in enumerative contexts. (Recall
               Example 18 and the discussion about the approximate independence of observations
               resulting from simple random sampling of large populations.)

5.5.4 The Propagation of Error Formulas

               Proposition 1 gives exact values for the mean and variance of U = g(X, Y, . . . , Z )
               only when g is linear. It doesn't seem to say anything about situations involving
               nonlinear functions like the one specified by the right-hand side of expression (5.52)
               in the solar collector example. But it is often possible to obtain useful approximations
               to the mean and variance of U by applying Proposition 1 to a first-order multivariate
               Taylor expansion of a not-too-nonlinear g. That is, if g is reasonably well-behaved,
               then for x, y, . . . , z (respectively) close to EX, EY, . . . , EZ,

                                                          g                g  
                                                                              
               g(x, y, . . . , z)  g(EX, EY, . . . , EZ) + · (x - EX) + · (y - EY )
                                                          x                y
               + · · · + g cdot (z - EZ)                                             (5.57)

               z
                                                                       5.5 Functions of Several Random Variables 311

                               where the partial derivatives are evaluated at (x, y, . . . , z) = (EX, EY, . . . , EZ).
                               Now the right side of approximation (5.57) is linear in x, y, . . . , z. Thus, if the vari-
                               ances of X, Y, . . . , Z are small enough so that with high probability, X, Y, . . . , Z are
                               such that approximation (5.57) is effective, one might think of plugging X, Y, . . . , Z
                               into expression (5.57) and applying Proposition 1, thus winding up with approxi-
                               mations for the mean and variance of U = g(X, Y, . . . , Z ).

             Proposition 2     If X, Y, . . . , Z are independent random variables and g is well-behaved, for
(The Propagation of Error      small enough variances Var X, Var Y, . . . , Var Z , the random variable U =
                               g(X, Y, . . . , Z ) has approximate mean
                   Formulas )

                                  EU  g(EX, EY, . . . , EZ)                      (5.58)

                               and approximate variance

                               Var U  g 2 Var X + g 2 Var Y + · · · + g 2 Var Z  (5.59)
                               x                         y   z

                               Formulas (5.58) and (5.59) are often called the propagation of error or transmis-
                               sion of variance formulas. They describe how variability or error is propagated or
                               transmitted through an exact mathematical function.

                                    Comparison of Propositions 1 and 2 shows that when g is exactly linear, ex-
                               pressions (5.58) and (5.59) reduce to expressions (5.53) and (5.54), respectively.
                               (a1 through an are the partial derivatives of g in the case where g(x, y, . . . , z) =
                               a0 + a1x + a2 y + · · · + an z.) Proposition 2 is purposely vague about when the
                               approximations (5.58) and (5.59) will be adequate for engineering purposes. Mathe-
                               matically inclined readers will not have much trouble constructing examples where
                               the approximations are quite poor. But often in engineering applications, expres-
                               sions (5.58) and (5.59) are at least of the right order of magnitude and certainly
                               better than not having any usable approximations.

Example 24                     A Simple Electrical Circuit and the Propagation of Error
                               Figure 5.35 is a schematic of an assembly of three resistors. If R1, R2, and R3 are
                               the respective resistances of the three resistors making up the assembly, standard
                               theory says that

                                                              R = the assembly resistance
312 Chapter 5 Probability: The Mathematics of Randomness

Example 24     is related to R1, R2, and R3 by
 (continued )

                                    R = R1 + R2 R3                               (5.60)
                                                 R2 + R3

                    A large lot of resistors is manufactured and has a mean resistance of 100
               with a standard deviation of resistance of 2 . If three resistors are taken at
               random from this lot and assembled as in Figure 5.35, consider what formulas
               (5.58) and (5.59) suggest for an approximate mean and an approximate standard
               deviation for the resulting assembly resistance.

                    The g involved here is g(r1, r2, r3) = r1 + r2r3 , so
                                                                        r2 + r3

                             g = 1
                             r1

                             g = 2 (r2 + r3)r3 - r2r3 =            r32
                                                (r2 + r3)
                                                                           2
                             r2                                    (r2 + r3)

                             g = 2 (r2 + r3)r2 - r2r3 =            r22

                                                                           2
                             r3                 (r2 + r3)          (r2 + r3)

               Also, R1, R2, and R3 are approximately independent with means 100 and stan-
               dard deviations 2. Then formulas (5.58) and (5.59) suggest that the probability
               distribution inherited by R has mean

                             E R  g(100,100,100) = 100 + (100)(100) = 150
                                                                      100 + 100

               and variance

               Var R  (1)2(2)2 +      (100)2              2            (100)2    2
                                  (100 + 100)2                     (100 + 100)2
                                                           (2)2 +                 (2)2 = 4.5

               so that the standard deviation inherited by R is

                                                          
                                  Var R  4.5 = 2.12

               As something of a check on how good the 150 and 2.12 values are, 1,000
               sets of normally distributed R1, R2, and R3 values with the specified population
               mean and standard deviation were simulated and resulting values of R calculated
               via formula (5.60). These simulated assembly resistances had R = 149.80 and
               a sample standard deviation of 2.14 . A histogram of these values is given in
               Figure 5.36.
                              5.5 Functions of Several Random Variables 313
                                             Resistor 2

                             Resistor 1
                                             Resistor 3

                       Figure 5.35 Schematic of a simple
                       assembly of three resistors

           200

Frequency  100

           0    145  150                   155

                     Simulated value of R

           Figure 5.36 Histogram of 1,000 simulated values of R

     Example 24 is one to which the cautions following Example 22 (page 307)
apply. Suppose you were to actually take a large batch of resistors possessing a
mean resistance of 100 and a standard deviation of resistances of 2 , make
up a number of assemblies of the type represented in Figure 5.35, and measure
the assembly resistances. The standard deviation figures in Example 24 will likely
underpredict the variation observed in the assembly resistances.

     The propagation of error and simulation methods may do a good job of approx-
imating the (exact) theoretical mean and standard deviation of assembly resistances.
But the extent to which the probability model used for assembly resistances can
be expected to represent the physical situation is another matter. Equation (5.60) is
highly useful, but of necessity only an approximate description of real assemblies.
For example, it ignores small but real temperature, inductance, and other second-
order physical effects on measured resistance. In addition, although the probability
model allows for variation in the resistances of individual components, it does not
account for instrument variation or such vagaries of real-world assembly as the
quality of contacts achieved when several parts are connected.

     In Example 24, the simulation and propagation of error methods produce com-
parable results. Since the simulation method is so easy to use, why bother to do
the calculus and arithmetic necessary to use the propagation of error formulas? One
important answer to this question concerns intuition that formula (5.59) provides.
314 Chapter 5 Probability: The Mathematics of Randomness
                                                                                       u = g(x)

                                   Potential output distributions                     Relatively               Relatively
                                      for U = g(X)                                    large g                  small g

                                                                                             x                        x
                                                                                                                                 x

                                                             Potential input distributions for X
                                                                                        g

                                   Figure 5.37 Illustration of the Effect of on Var U
                                                                                        x

   The effects of   Consider first the effect that g's partial derivatives have on Var U . Formula (5.59)
       the partial
                    implies  that  depending                       on  the  size  of  g  ,  the  variance  of  X  is  either  inflated  or  deflated
derivatives of g                                                                      x
          on Var U  before becoming an ingredient of Var U . And even though formula (5.59) may not

Partitioning the    be an exact expression, it provides correct intuition. If a given change in x produces
   variance of U
                    a big change in g(x, y, . . . , z), the impact Var X has on Var U will be greater than

                    if the change in x produces a small change in g(x, y, . . . , z). Figure 5.37 is a rough

                    illustration of this point. In the case that U = g(X ), two different approximately

                    normal distributions for X with different means but a common variance produce

                    radically different spreads in the distribution of U , due to differing rates of change

                    of g (different derivatives).

                    Then, consider the possibility of partitioning the variance of U into interpretable

                    pieces. Formula (5.59) suggests thinking of (for example)

                                                                                      g 2
                                                                                             Var X

                                                                                      x

                    as the contribution the variation in X makes to the variation inherent in U . Com-
                    parison of such individual contributions makes it possible to analyze how various
                    potential reductions in input variation can be expected to affect the output variability
                    in U .

Example 22          Return to the solar collector example. For means of C through To taken to be
 (continued )       the measured values in Table 5.23 (page 305), and standard deviations of C
                    through To equal to half of the uncertainties listed in the same table, formula
                                    5.5 Functions of Several Random Variables 315

(5.59) might well be applied to the calculated efficiency given in formula (5.52).
The squared partial derivatives of Efficiency with respect to each of the inputs,
times the variances of those inputs, are as given in Table 5.24. Thus, the ap-
proximate standard deviation for the efficiency variable provided by formula
(5.59) is

                                     8.28 × 10-5  .009

which agrees quite well with the value obtained earlier via simulation.
     What's given in Table 5.24 that doesn't come out of a simulation is some

understanding of the biggest contributors to the uncertainty. The largest contri-
bution listed in Table 5.24 corresponds to variable G, followed in order by those
corresponding to variables Mo, To, and Ti. At least for the values of the means
used in this example, it is the uncertainties in those variables that principally
produce the uncertainty in Efficiency. Knowing this gives direction to efforts to
improve measurement methods. Subject to considerations of feasibility and cost,
measurement of the variable G deserves first attention, followed by measurement
of the variables Mo, To, and Ti.

     Notice, however, that reduction of the uncertainty in G alone to essentially 0
would still leave a total in Table 5.24 of about 4.01×10-5 and thus an approximate
standard deviation for Efficiency of about 4.01 × 10-5  .006. Calculations of
this kind emphasize the need for reductions in the uncertainties of Mo, To, and
Ti as well, if dramatic (order of magnitude) improvements in overall uncertainty
are to be realized.

Table 5.24
Contributions to the Output Variation in
Collector Efficiency

Variable Contributions to Var Efficiency

 C     4.73 × 10-8
 G     4.27 × 10-5
  A    4.76 × 10-7
 Mi    5.01 × 10-7
 Mo    1.58 × 10-5
 Ta    3.39 × 10-8
 Ti    1.10 × 10-5
 To    1.22 × 10-5

Total  8.28 × 10-5
316 Chapter 5 Probability: The Mathematics of Randomness

5.5.5                   The Central Limit Effect

                        One of the most frequently used statistics in engineering applications is the sample
                        mean. Formulas (5.55) and (5.56) relate the mean and variance of the probability
                        distribution of the sample mean to those of a single observation when an iid model
                        is appropriate. One of the most useful facts of applied probability is that if the
                        sample size is reasonably large, it is also possible to approximate the shape of the
                        probability distribution of X , independent of the shape of the underlying distribution
                        of individual observations. That is, there is the following fact:

    Proposition 3       If X1, X2, . . . , Xn are iid random variables (with mean µ and variance  2),
(The Central Limit      then for large n, the variable X is approximately normally distributed. (That is,

          Theorem )     approximate probabilities for X can be calculated using the normal distribution
                        with mean µ and variance  2/n.)

                        A proof of Proposition 3 is outside the purposes of this text. But intuition about the
                        effect is fairly easy to develop through an example.

          Example 25    The Central Limit Effect and the Sample Mean of Tool Serial Numbers
(Example 2 revisited )
                        Consider again the example from Section 5.1 involving the last digit of essentially
                        randomly selected serial numbers of pneumatic tools. Suppose now that

                        W1 = the last digit of the serial number observed next Monday at 9 A.M.
                        W2 = the last digit of the serial number observed the following Monday at 9 A.M.

                        A plausible model for the pair of random variables W1, W2 is that they are
                        independent, each with the marginal probability function

                                     .1                   if w = 0, 1, 2, . . . , 9  (5.61)
                        f (w) =                           otherwise

                                     0

                        that is pictured in Figure 5.38.

                             Using such a model, it is a straightforward exercise (along the lines of
                        Example 21, page 303) to reason that W = 12 (W1 + W2) has the probability
                        function given in Table 5.25 and pictured in Figure 5.39.
            5.5 Functions of Several Random Variables 317

    f (w)   EW = 4.5
.1          Var W = 8.25

0123456789 w

Figure 5.38 Probability histogram for W

      f(w)                n = 2
.10
.05                       EW = 4.5
                          Var W = 2 8.25 = 4.125

           0123456789 w

Figure 5.39 Probability histogram for W based on
n=2

Table 5.25                                f (w¯ ) w¯  f (w¯ )
The Probability Function for W for n = 2
                                          .07 8.0     .03
  w¯ f (w¯ ) w¯ f (w¯ ) w¯ f (w¯ ) w¯     .06 8.5     .02
                                          .05 9.0     .01
 0.0 .01 2.0 .05 4.0 .09 6.0              .04
 0.5 .02 2.5 .06 4.5 .10 6.5
 1.0 .03 3.0 .07 5.0 .09 7.0
 1.5 .04 3.5 .08 5.5 .08 7.5

     Comparing Figures 5.38 and 5.39, it is clear that even for a completely
flat/uniform underlying distribution of W and the small sample size of n = 2,

the probability distribution of W looks far more bell-shaped than the underlying

distribution. It is clear why this is so. As you move away from the mean or central

value of W , there are relatively fewer and fewer combinations of w1 and w2 that
can produce a given value of w¯ . For example, to observe W = 0, you must have
W1 = 0 and W2 = 0--that is, you must observe not one but two extreme values.
On the other hand, there are ten different combinations of w1 and w2 that lead to
W = 4.5.

     It is possible to use the same kind of logic leading to Table 5.25 to produce

exact probability distributions for W based on larger sample sizes n. But such
318 Chapter 5 Probability: The Mathematics of Randomness

Example 25            work is tedious, and for the purpose of indicating roughly how the central limit
 (continued )
                      effect takes over as n gets larger, it is sufficient to approximate the distribution

                      of W via simulation for a larger sample size. To this end, 1,000 sets of values for

                      iid variables W1, W2, . . . , W8 (with marginal distribution (5.61)) were simulated
                      and each set averaged to produce 1,000 simulated values of W based on n = 8.
                      Figure 5.40 is a histogram of these 1,000 values. Notice the bell-shaped character
                      of the plot. (The simulated mean of W was 4.508  4.5 = E W = E W , while the
                      variance of W was 1.025  1.013 = Var W = 8.25/8, in close agreement with
                      formulas (5.55) and (5.56).)

                                 200

                      Frequency  100

                                     0
                                           012 3 4567 8 9
                                                       Mean of n = 8 W 's

                                 Figure 5.40 Histogram of 1,000 simulated
                                 values of W based on n = 8

Sample size and            What constitutes "large n" in Proposition 3 isn't obvious. The truth of the
the central limit     matter is that what sample size is required before X can be treated as essentially
                      normal depends on the shape of the underlying distribution of a single observation.
              effect  Underlying distributions with decidedly nonnormal shapes require somewhat bigger
                      values of n. But for most engineering purposes, n  25 or so is adequate to make X
                      essentially normal for the majority of data-generating mechanisms met in practice.
                      (The exceptions are those subject to the occasional production of wildly outlying
                      values.) Indeed, as Example 25 suggests, in many cases X is essentially normal for
                      sample sizes much smaller than 25.

                           The practical usefulness of Proposition 3 is that in many circumstances, only a
                      normal table is needed to evaluate probabilities for sample averages.

Example 23            Return one more time to the stamp sale time requirements problem and consider
 (continued )         observing and averaging the next n = 100 excess service times, to produce

                          S = the sample mean time (over a 7.5 sec threshold) required to
                               complete the next 100 stamp sales
5.5 Functions of Several Random Variables 319

And consider approximating P[S > 17].
     As discussed before, an iid model with marginal exponential  = 16.5 dis-

tribution is plausible for the individual excess service times, S. Then

                                    E S =  = 16.5 sec

and

Var S =                      2 = 1.65 sec
                             100

are appropriate for S, via formulas (5.55) and (5.56). Further, in view of the
fact that n = 100 is large, the normal probability table may be used to find
approximate probabilities for S. Figure 5.41 shows an approximate distribution

for S and the area corresponding to P[S > 17].

The approximate probability
distribution of S is normal
with mean 16.5 and standard
deviation 1.65

                             Approximate P[ S > 17]

                                            16 17
                  Figure 5.41 Approximate probability distribution for S
                  and P [S > 17]

     As always, one must convert to z-values before consulting the standard
normal table. In this case, the mean and standard deviation to be used are (re-
spectively) 16.5 sec and 1.65 sec. That is, a z-value is calculated as

                                   z = 17 - 16.5 = .30
                                            1.65

so

                   P[S > 17]  P[Z > .30] = 1 - (.30) = .38
320 Chapter 5 Probability: The Mathematics of Randomness
                                       The z-value calculated in the example is an application of the general form

z-value for a            z = x¯ - EX =  x¯ - µ      (5.62)
sample mean                      Var X 
                                                 n

                         appropriate when using the central limit theorem to find approximate probabilities
                         for a sample mean. Formula (5.62) is relevant because by Proposition 3, X is
                         approximately normal for large n and formulas (5.55) and (5.56) give its mean and
                         standard deviation.

                              The final example in this section illustrates how the central limit theorem and
                         some idea of a process or population standard deviation can help guide the choice
                         of sample size in statistical applications.

            Example 26   Sampling a Jar-Filling Process
(Example 10 revisited )  The process of filling food containers, discussed by J. Fisher in his 1983 "Quality
                         Progress" article, appears (from a histogram in the paper) to have an inherent
                         standard deviation of measured fill weights on the order of 1.6 g. Suppose that
                         in order to calibrate a fill-level adjustment knob on such a process, you set the
                         knob and fill a run of n jars. Their sample mean net contents will then serve as
                         an indication of the process mean fill level corresponding to that knob setting.
                         Suppose further that you would like to choose a sample size, n, large enough
                         that a priori there is an 80% chance the sample mean is within .3 g of the actual
                         process mean.

                              If the filling process can be thought of as physically stable, it makes sense
                         to model the n observed net weights as iid random variables with (unknown)
                         marginal mean µ and standard deviation  = 1.6 g. For large n,

                                              V = the observed sample average net weight

                         canbe thought of as approximately normal with mean µ and standard deviation
                         / n = 1.6/ n (by Proposition 3 and formulas (5.55) and (5.56)).

                              Now the requirement that V be within .3 g of µ can be written as

                                                            µ - .3 < V < µ + .3

                         so the problem at hand is to choose n such that

                                                      P[µ - .3 < V < µ + .3] = .80
                                                                       5.5 Functions of Several Random Variables 321

                                   Figure 5.42 pictures the situation. The .90 quantile of the standard normal distri-
                                   bution is roughly 1.28--that is, P[-1.28 < Z < 1.28] = .8. So evidently Figure
                                   5.42 indicates that µ + .3 should have z-value 1.28. That is, you want

                                                                      1.28 = (µ + .3) - µ
                                                                                      1.6
                                                                                      
                                                                                        n

                                   or
                                                                                       1.6

                                                                           .3 = 1.28 
                                                                                         n

                                   So, solving for n, a sample size of n  47 would be required to provide the kind
                                   of precision of measurement desired.

                                            P[µ - .3 < V < µ + .3] = .8  The approximate probability
                                            is desired                   distribution of V is normal
                                                                         with mean µ and standard
                                                                         deviation  = 1.6

                                                                                           nn

                                                      µ - .3  µ          µ + .3

                                                Figure 5.42 Approximate probability distribution for V

Section 5 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1.  A  type  of  nominal  3  inch  plywood  is  made  of         Find the mean and standard deviation of total thick-
                          4                                      ness associated with the combination of these indi-
    five layers. These layers can be thought of as hav-          vidual values.

    ing thicknesses roughly describable as independent        2. The coefficient of linear expansion of brass is to be
                                                                 obtained as a laboratory exercise. For a brass bar
    random variables with means and standard devia-              that is L1 meters long at T1C and L2 meters long
                                                                 at T2C, this coefficient is
    tions as follows:

    Layer Mean (in.) Standard Deviation (in.)

       1         .094              .001                                           = L2 - L1
                                                                                        L1(T2 - T1)
       2         .156              .002
                                                              Suppose that the equipment to be used in the lab-
       3         .234              .002                       oratory is thought to have a standard deviation for
                                                              repeated length measurements of about .00005 m
       4         .172              .002

       5         .094              .001
322 Chapter 5 Probability: The Mathematics of Randomness

   and a standard deviation for repeated temperature           (e) Redo parts (a) through (d) using a sample size
   measurements of about .1C.                                      of 100 instead of 25.
    (a) If using T1  50C and T2  100C, L1 
                                                           4. Passing a large production run of piston rings
        1.00000 m and L2  1.00095 m are obtained,             through a grinding operation produces edge widths
        and it is desired to attach an approximate stan-      possessing a standard deviation of .0004 in. A sim-
        dard deviation to the derived value of , find         ple random sample of rings is to be taken and their
        such an approximate standard deviation two            edge widths measured, with the intention of using
        different ways. First, use simulation as was           X as an estimate of the population mean thickness
        done in Printout 1. Then use the propagation          µ. Approximate the probabilities that X is within
        of error formula. How well do your two values         .0001 in. of µ for samples of size n = 25, 100, and
        agree?                                                400.
   (b) In this particular lab exercise, the precision of
        which measurements (the lengths or the tem-        5. A pendulum swinging through small angles ap-
        peratures) is the primary limiting factor in the      proximates simple harmonic motion. The period of
        precision of the derived coefficient of linear        the pendulum,  , is (approximately) given by
        expansion? Explain.
    (c) Within limits, the larger T2 - T1, the better the                              = 2 L
        value for . What (in qualitative terms) is the                                            g
        physical origin of those limits?
                                                              where L is the length of the pendulum and g is
3. Consider again the random number generator dis-            the acceleration due to gravity. This fact can be
   cussed in Exercise 1 of Section 5.2. Suppose that          used to derive an experimental value for g. Suppose
   it is used to generate 25 random numbers and that          that the length L of about 5 ft can be measured
   these may reasonably be thought of as indepen-             with a standard deviation of about .25 in. (about
   dent random variables with common individual               .0208 foot), and the period  of about 2.48 sec
   (marginal) distribution as given in Exercise 1 of          can be measured with standard deviation of about
   Section 5.2. Let X be the sample mean of these 25          .1 sec. What is a reasonable standard deviation to
   values.                                                    attach to a value of g derived using this equipment?
    (a) What are the mean and standard deviation of           Is the precision of the length measurement or the
        the random variable X?                                precision of the period measurement the principal
   (b) What is the approximate probability distribu-          limitation on the precision of the derived g?
        tion of X?
    (c) Approximate the probability that X exceeds .5.
   (d) Approximate the probability that X takes a
        value within .02 of its mean.

Chapter 5 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Suppose 90% of all students taking a beginning              (c) less than four fail on their first submissions
   programming course fail to get their first program         Continuing to use this binomial model,
   to run on first submission. Use a binomial distri-         (d) what is the mean number who will fail?
   bution and assign probabilities to the possibilities        (e) what are the variance and standard deviation of
   that among a group of six such students,
    (a) all fail on their first submissions                        the number who will fail?
   (b) at least four fail on their first submissions
                                                           2. Suppose that for single launches of a space shuttle,
                                                              there is a constant probability of O-ring failure (say,
                                                                                   Chapter 5 Exercises 323

.15). Consider ten future launches, and let X be the     6. Suppose that X is a normal random variable with

number of those involving an O-ring failure. Use         mean µ = 10.2 and standard deviation  = .7.

an appropriate probability model and evaluate all        Evaluate the following probabilities involving X :

of the following:                                        (a) P[X  10.1]                    (b) P[X > 10.5]

(a) P[X = 2]           (b) P[X  1]                       (c) P[9.0 < X < 10.3] (d) P[|X - 10.2|  .25]

(c) EX                 (d) Var X                         (e) P[|X - 10.2| > 1.51]

(e) the standard deviation of X                          Find numbers # such that the following statements

3. An injection molding process for making auto          about X are true:

bumpers leaves an average of 1.3 visual defects          (f) P[|X - 10.2| < #] = .80

per bumper prior to painting. Let Y and Z be the         (g) P[X < #] = .80

numbers of visual defects on (respectively) the next     (h) P[|X - 10.2| > #] = .04

two bumpers produced. Use an appropriate proba-          7. In a grinding operation, there is an upper speci-
                                                            fication of 3.150 in. on a dimension of a certain
bility distribution and evaluate the following:             part after grinding. Suppose that the standard de-
                                                            viation of this normally distributed dimension for
(a) P[Y = 2]           (b) P[Y  1]                          parts of this type ground to any particular mean
                                                            dimension µ is  = .002 in. Suppose further that
                                                            you desire to have no more than 3% of the parts
(c) Var Y                                                   fail to meet specifications. What is the maximum
                                                            (minimum machining cost) µ that can be used if
(d) P[Y + Z  2] (Hint: What is a sensible                   this 3% requirement is to be met?

distribution for Y + Z , the number of blem-

ishes on two bumpers?)

4. Suppose that the random number generator sup-         8. A 10 ft cable is made of 50 strands. Suppose that,
   plied in a pocket calculator actually generates val-     individually, 10 ft strands have breaking strengths
   ues in such a way that if X is the next value gener-     with mean 45 lb and standard deviation 4 lb. Sup-
   ated, X can be adequately described using a prob-        pose further that the breaking strength of a cable
   ability density of the form                              is roughly the sum of the strengths of the strands
                                                            that make it up.
f (x) =  k((x - .5)2 + 1)        for 0 < x < 1               (a) Find a plausible mean and standard deviation
         0                       otherwise                       for the breaking strengths of such 10 ft cables.
                                                            (b) Evaluate the probability that a 10 ft cable
(a) Evaluate k and sketch a graph of f (x) .                     of this type will support a load of 2230 lb.
(b) Evaluate P[X  .5], P[X > .5], P[.75 >                        (Hint: If X is the mean breaking strength of
                                                                 the strands, (Strengths)  2230 is the same
     X  .5], and P[|X - .5|  .2].                                as X  ( 50 2230 ). Now use the central limit the-
(c) Compute EX and Var X.                                        orem.)
(d) Compute and graph F(x), the cumulative prob-

     ability function for X. Read from your graph
     the .8 quantile of the distribution of X .

5. Suppose that Z is a standard normal random vari-      9. The electrical resistivity, , of a piece of wire is a

able. Evaluate the following probabilities involv-       property of the material involved and the temper-

ing Z :                                                  ature at which it is measured. At a given tempera-

(a) P[Z  1.13]         (b) P[Z > -.54]                   ture, if a cylindrical piece of wire of length L and

(c) P[-1.02 < Z < .06] (d) P[|Z |  .25]                  cross-sectional area A has resistance R, the ma-

(e) P[|Z | > 1.51]     (f) P[-3.0 < Z < 3.0]             terial's resistivity is calculated using the formula

Find numbers # such that the following statements          =  RA  .  Thus,  if  a  wire's  cross  s  ection  is  assumed
                                                               L
about Z are true:

(g) P[|Z | < #] = .80 (h) P[Z < #] = .80

(i) P[|Z | > #] = .04
324 Chapter 5 Probability: The Mathematics of Randomness

to be circular with diameter D, the resistivity at a            means µR = 10 and standard deviations R =
given temperature is                                            .1 and that V has mean µV = 9 volt and V =
                                                                .2 volt.
                            = R D2                              (a) Find an approximate mean and standard de-
                                    4L
                                                                     viation for I , treating R1, R2, and V as inde-
     In a lab exercise to determine the resistivity of               pendent random variables.
     copper at 20C, students measure lengths, diam-             (b) Based on your work in (a), would you say
     eters, and resistances of wire nominally 1.0 m in               that the variation in voltage or the combined
     length (L), 2.0 × 10-3 m in diameter (D), and                   variations in R1 and R2 are the biggest con-
     of resistance (R) .54 × 10-2 . Suppose that it                  tributors to variation in current? Explain.
     is sensible to describe the measurement preci-
     sions in this laboratory with the standard devi-      12. Students in a materials lab are required to ex-
     ations L  10-3 m, D  10-4 m, and R                         perimentally determine the heat conductivity of
     5 × 10-4 .                                                 aluminum.
     (a) Find an approximate standard deviation that            (a) If student-derived values are normally distrib-
                                                                     uted about a mean of .5 (cal/(cm)(sec)(C))
          might be used to describe the expected pre-                with standard deviation of .03, evaluate the
          cision for an experimentally derived value                 probability that an individual student will ob-
          of .                                                       tain a conductivity from .48 to .52.
     (b) Imprecision in which of the measurements is            (b) If student values have the mean and standard
          likely to be the largest contributor to impre-             deviation given in (a), evaluate the probabil-
          cision in measured resistivity? Explain.                   ity that a class of 25 students will produce a
     (c) You should expect that the value derived in (a)             sample mean conductivity from .48 to .52.
          underpredict the kind of variation that would         (c) If student values have the mean and standard
          be observed in such laboratory exercises over              deviation given in (a), evaluate the probabil-
          a period of years. Explain why this is so.                 ity that at least 2 of the next 5 values pro-
                                                                     duced by students will be in the range from .48
10. Suppose that the thickness of sheets of a certain                to .52.
     weight of book paper have mean .1 mm and a
     standard deviation of .003 mm. A particular text-     13. Suppose that 10 ft lengths of a certain type of cable
     book will be printed on 370 sheets of this paper.          have breaking strengths with mean µ = 450 lb
     Find sensible values for the mean and standard             and standard deviation  = 50 lb.
     deviation of the thicknesses of copies of this text        (a) If five of these cables are used to support
     (excluding, of course, the book's cover).                       a single load L, suppose that the cables are
                                                                     loaded in such a way that support fails if any
11. Pairs of resistors are to be connected in paral-                 one of the cables has strength below 5L . With
     lel and a difference in electrical potential applied            L = 2,000 lb, assess the probability that the
     across the resistor assembly. Ohm's law predicts                support fails, if individual cable strength is
     that in such a situation, the current flowing in the            normally distributed. Do this in two steps.
     circuit will be                                                 First find the probability that a particular in-
                                                                     dividual cable fails, then use that to evaluate
I =V  1+1                                                            the desired probability.
      R1 R2                                                     (b) Approximate the probability that the sample
                                                                     mean strength of 100 of these cables is below
where R1 and R2 are the two resistances and V                        457 lb.
the potential applied. Suppose that R1 and R2 have
                                                                            Chapter 5 Exercises 325

14. Find EX and Var X for a continuous distribution         17. The viscosity of a liquid may be measured by
     with probability density                                    placing it in a cylindrical container and determin-
                                                                 ing the force needed to turn a cylindrical rotor (of
                if 0 < x < 1                                     nearly the same diameter as the container) at a
          .3    if 1 < x < 2                                     given velocity in the liquid. The relationship be-
f (x) =  .7     otherwise                                        tween the viscosity , force F, area A of the side
                                                                 of the rotor in contact with the liquid, the size L
             0                                                   of the gap between the rotor and the inside of the
                                                                 container, and the velocity v at which the rotor
15. Suppose that it is adequate to describe the 14-              surface moves is
     day compressive strengths of test specimens of a
     certain concrete mixture as normally distributed                                      = FL
     with mean µ = 2,930 psi and standard deviation                                             vA
      = 20 psi.
     (a) Assess the probability that the next specimen           Suppose that students are to determine an experi-
          of this type tested for compressive strength           mental viscosity for SAE no. 10 oil as a laboratory
          will have strength above 2,945 psi.                    exercise and that appropriate means and standard
     (b) Use your answer to part (a) and assess the              deviations for the measured variables F, L, v, and
          probability that in the next four specimens            A in this laboratory are as follows:
          tested, at least one has compressive strength
          above 2,945 psi.                                  µF = 151 N           F = .05 N
     (c) Assess the probability that the next 25 speci-     µA = 1257 cm2        A = .2 cm2
          mens tested have a sample mean compressive        µL = .5 cm           L = .05 cm
          strength within 5 psi of µ = 2,930 psi.           µv = 30 cm/sec       v = 1 cm/sec
     (d) Suppose that although the particular concrete
          formula under consideration in this problem            (a) Use the propagation of error formulas and
          is relatively strong, it is difficult to pour in            find an approximate standard deviation that
          large quantities without serious air pockets                might serve as a measure of precision for an
          developing (which can have important impli-                 experimentally derived value of  from this
          cations for structural integrity). In fact, sup-            laboratory.
          pose that using standard methods of pouring,
          serious air pockets form at an average rate of         (b) Explain why, if experimental values of  ob-
          1 per 50 cubic yards of poured concrete. Use                tained for SAE no. 10 oil in similar laboratory
          an appropriate probability distribution and as-             exercises conducted over a number of years at
          sess the probability that two or more serious               a number of different universities were com-
          air pockets will appear in a 150 cubic yard                 pared, the approximate standard deviation de-
          pour to be made tomorrow.                                   rived in (a) would be likely to understate the
                                                                      variability actually observed in those values.
16. For X with a continuous distribution specified by
     the probability density                                18. The heat conductivity, , of a cylindrical bar of
                                                                 diameter D and length L, connected between two
f (x) =  .5x for 0 < x < 2                                       constant temperature devices of temperatures T1
         0 otherwise                                             and T2 (respectively), that conducts Q calories in
                                                                 t seconds is

find P[X < 1.0] and find the mean, EX.                      =               4QL

                                                               (T1 - T2)t D      2
326 Chapter 5 Probability: The Mathematics of Randomness

In a materials laboratory exercise to determine           21. Students are going to measure Young's Modulus
 for brass, the following means and standard                   for copper by measuring the elongation of a piece
deviations for the variables D, L, T1, T2, Q, and t            of copper wire under a tensile force. For a cylin-
are appropriate, as are the partial derivatives of             drical wire of diameter D subjected to a tensile
with respect to the various variables (evaluated at            force F, if the initial length (length before apply-
the means of the variables):                                   ing the force) is L0 and final length is L1, Young's
                                                               Modulus for the material in question is

  µ         D       L         T1                                           Y = 2 4FL0
                                                                                  D (L1 - L0)
partial  1.6 cm  100 cm    100C
         .1 cm    .1 cm      1C                           The test and measuring equipment used in a par-
         -.249    .199                                    ticular lab are characterized by the standard devi-
                          -.00199                         ations

  µ        T2       Q         t                                F  10 lb D  .001 in.
                                                                     L0 = L1 = .01 in.
partial   0C     240 cal  600 sec
          1C      10 cal   1 sec
         .00199  .000825
                          .000332

     (The units of the partial derivatives are the units  and in the setup employed, F  300 lb, D 
     of (cal/(cm)(sec)(C)) divided by the units of the
     variable in question.)                               .050 in., L0  10.00 in., and L1  10.10 in.
     (a) Find an approximate standard deviation to as-    (a) Treating the measured force, diameter, and

          sociate with an experimentally derived value    lengths as independent variables with the pre-
          of .
     (b) Which of the variables appears to be the         ceding means and standard deviations, find an
          biggest contributor to variation in experimen-
          tally determined values of ? Explain your       approximate standard deviation to attach to
          choice.
                                                          an experimentally derived value of Y . (Partial
19. Suppose that 15% of all daily oxygen purities
     delivered by an air-products supplier are below      derivatives of Y at the nominal values of F, D,
     99.5% purity and that it is plausible to think of
     daily purities as independent random variables.      L0,  and    L1  are  approximately  Y   5.09 ×
     Evaluate the probability that in the next five-day                                       F
     workweek, 1 or less delivered purities will fall     104, Y  D  -6.11 × 108, Y  L  1.54 × 108,
     below 99.5%.                                                                         0
                                                               Y       -1.53 × 108
20. Suppose that the raw daily oxygen purities de-        and  L                    in    the  appropriate
     livered by an air-products supplier have a stan-
     dard deviation   .1 (percent), and it is plausi-     units.)  1
     ble to think of daily purities as independent ran-
     dom variables. Approximate the probability that      (b) Uncertainty in which of the variables is the
     the sample mean X of n = 25 delivered purities
     falls within .03 (percent) of the raw daily purity   biggest contributor to uncertainty in Y ?
     mean, µ.
                                                          (c) Notice that the equation for Y says that for

                                                          a particular material (and thus supposedly

                                                          constant Y ), circular wires of constant initial

                                                          lengths L0, but of different diameters and sub-
                                                          jected to different tensile forces, will undergo

                                                          elongations L = L1 - L0 of approximately

                                                                                       F
                                                                               L 2

                                                                                       D

                                                          for  a constant depending on the material and
                                                          the initial length. Suppose that you decide to
                                                                                 Chapter 5 Exercises 327

          measure L for a factorial arrangement of           of the simpler of these approximations are dis-
          levels of F and D. Does the equation predict
          that F and D will or will not have important       cussed in the articles "A Simpler Approximation
          interactions? Explain.
                                                             for Areas Under the Standard Normal Curve,"
22. Exercise 6 of Chapter 3 concerns the lifetimes (in
     numbers of 24 mm deep holes drilled in 1045 steel       by A. Shah (The American Statistician, 1985),
     before failure) of 12 D952-II (8 mm) drills.
     (a) Make a normal plot of the data given in Ex-         "Pocket-Calculator Approximation for Areas un-
          ercise 6 of Chapter 3. In what specific way
          does the shape of the data distribution appear     der the Standard Normal Curve," by R. Norton
          to depart from a Gaussian shape?
     (b) The 12 lifetimes have mean y¯ = 117.75 and          (The American Statistician, 1989), and "Approx-
          standard deviation s  51.1. Simply using
          these in place of µ and  for the underly-          imations for Hand Calculators Using Small In-
          ing drill life distribution, use the normal table
          to find an approximate fraction of drill lives     teger Coefficients," by S. Derenzo (Mathematics
          below 40 holes.
     (c) Based on your answer to (a), if your answer to      of Computation, 1977). For z > 0, consider the
          (b) is seriously different from the real fraction
          of drill lives below 40, is it most likely high    approximations offered in these articles:
          or low? Explain.
                                                                                                     0  z  2.2
23. Metal fatigue causes cracks to appear on the skin                           z(4.4 - z)           2.2 < z < 2.6
     of older aircraft. Assume that it is reasonable to                         .5 +                 2.6  z
     model the number of cracks appearing on a 1 m2
     surface of planes of a certain model and vintage                                        10
     as Poisson with mean  = .03.                            (z)  gS(z) =  .99
     (a) If 1 m2 is inspected, assess the probability that
          at least one crack is present on that surface.                       
     (b) If 10 m2 are inspected, assess the probability                           1.00
          that at least one crack (total) is present.
     (c) If ten areas, each of size 1 m2, are inspected,                         1   z2 + 1.2z.8
          assess the probability that exactly one of these   (z)  gN(z) = 1 - exp -
          has cracks.                                                            2                2

24. If a dimension on a mechanical part is normally          (z)  gD(z)          - (83z + 351)z + 562
     distributed, how small must the standard devi-                 = 1 - 1 exp         703/z + 165
     ation be if 95% of such parts are to be within                          2
     specifications of 2 cm ± .002 cm when the mean
     dimension is ideal (µ = 2 cm)?                               Evaluate gS(z), gN(z), and gD(z) for z = .5, 1.0,
                                                                  1.5, 2.0, and 2.5. How do these values compare to
25. The fact that the "exact" calculation of normal               the corresponding entries in Table B.3?
     probabilities requires either numerical integration
     or the use of tables (ultimately generated using        26. Exercise 25 concerned approximations for nor-
     numerical integration) has inspired many peo-                mal probabilities. People have also invested a fair
     ple to develop approximations to the standard                amount of effort in finding useful formulas ap-
     normal cumulative distribution function. Several             proximating standard normal quantiles. One such
                                                                  approximation was given in formula (3.3). A more
                                                                  complicated one, again taken from the article by
                                                                  S. Derenzo mentioned in Exercise 25, is as fol-
                                                                  lows. For p > .50, let y = - ln (2(1 - p)) and

                                                             Qz(p)     ((4y + 100)y + 205) y2
                                                                     ((2y + 56)y + 192) y + 131

                                                             For p < .50, let y = - ln (2 p) and

                                                             Qz( p)  - ((4y + 100)y + 205) y2
                                                                             ((2y + 56)y + 192) y + 131
328 Chapter 5 Probability: The Mathematics of Randomness

Use these formulas to approximate Qz( p) for                      the other two possibilities as a description of
p = .01, .05, .1, .3, .7, .9, .95, and .99. How do the            the flexural strength?
values you obtain compare with the correspond-               (d) Eye-fit lines to your plots from part (c). Use
ing entries in Table 3.10 and the results of using                them to help you determine appropriate means
formula (3.3)?                                                    and standard deviations for normal distribu-
                                                                  tions used to describe flexural strength and
27. The article "Statistical Strength Evaluation of               the logarithm of flexural strength. Compare
                                                                  the .01, .10, .20, and .50 quantiles of the fit-
Hot-pressed Si3N4" by R. Govila (Ceramic Bul-                     ted normal and lognormal distributions for
letin, 1983) contains summary statistics from an                  strength to the quantiles you computed in
                                                                  part (b).
extensive study of the flexural strengths of two
                                                        28. The article "Using Statistical Thinking to Solve
high-strength  hot-pressed  silicon  nitrides  in  1         Maintenance Problems" by Brick, Michael, and
                                                   4         Morganstein (Quality Progress, 1989) contains
point, 4 point bending. The values below are frac-           the following data on lifetimes of sinker rollers.
                                                             Given are the numbers of 8-hour shifts that 17
ture strengths of 30 specimens of one of the ma-             sinker rollers (at the bottom of a galvanizing pot
terials tested at 20C. (The units are MPa, and               and used to direct steel sheet through a coating
                                                             operation) lasted before failing and requiring re-
the data were read from a graph in the paper                 placement.

and may therefore individually differ by perhaps                       10, 12, 15, 17, 18, 18, 20, 20,
                                                                       21, 21, 23, 25, 27, 29, 29, 30, 35
as much as 10 MPa from the actual measured
                                                             (a) The authors of the article considered a Weibull
values.)                                                          distribution to be a likely model for the life-
                                                                  times of such rollers. Make a zero-threshold
514, 533, 543, 547, 584, 619, 653, 684,                           Weibull probability plot for use in assessing
689, 695, 700, 705, 709, 729, 729, 753,                           the reasonableness of such a description of
763, 800, 805, 805, 814, 819, 819, 839,                           roller life.
839, 849, 879, 900, 919, 979
                                                             (b) Eye-fit a line to your plot in (a) and use it to
(a) The materials researcher who collected the                    estimate parameters for a Weibull distribution
     original data believed the Weibull distribution              for describing roller life.
     to be an adequate model for flexural strength
     of this material. Make a Weibull probability            (c) Use your estimated parameters from (a) and
     plot using the method of display (5.35) of                   the form of the Weibull cumulative distribu-
     Section 5.3 and investigate this possibility.                tion function given in Section 5.2 to estimate
     Does a Weibull model fit these data?                         the .10 quantile of the roller life distribution.

(b) Eye-fit a line through your plot from part (a).     29. The article "Elementary Probability Plotting for
     Use it to help you determine an appropriate             Statistical Data Analysis" by J. King (Quality
     shape parameter, , and an appropriate scale             Progress, 1988) contains 24 measurements of de-
     parameter, , for a Weibull distribution used            viations from nominal of a distance between two
     to describe flexural strength of this material
     at 20C. For a Weibull distribution with your
     fitted values of  and , what is the median
     strength? What is a strength exceeded by 80%
     of such Si3N4 specimens? By 90% of such
     specimens? By 99% of such specimens?

(c) Make normal plots of the raw data and of the
     logarithms of the raw data. Comparing the
     three probability plots made in this exercise, is
     there strong reason to prefer a Weibull model,
     a normal model, or a lognormal model over
                                                             Chapter 5 Exercises 329

     holes drilled in a steel plate. These are reproduced    (a) Find the probabilities that a given diameter
     here. The units are mm.                                      falls into each of the three zones.

         -2, -2, 7, -10, 4, -3, 0, 8, -5, 5, -6, 0,          (b) Suppose that a technician simply begins mea-
         2, -2, 1, 3, 3, -4, -6, -13, -7, -2, 2, 2                suring diameters on consecutive parts and
                                                                  continues until a Red Zone measurement is
     (a) Make a dot diagram for these data and com-               found. Assess the probability that more than
          pute x¯ and s.                                          ten parts must be measured. Also, give the
                                                                  expected number of measurements that must
     (b) Make a normal plot for these data. Eye-fit a             be made.
          line on the plot and use it to find graphical
          estimates of a process mean and standard de-       The engineer decides to use the Green/Yellow/Red
          viation for this deviation from nominal. Com-      gauging system in the following way. Every hour,
          pare these graphical estimates with the values     parts coming off the lathe will be checked. First,
          you calculated in (a).                             a single part will be measured. If it is in the Green
                                                             Zone, no further action is needed that hour. If the
     (c) Engineering specifications on this deviation        initial part is in the Red Zone, the lathe will be
          from nominal were ±10 mm. Suppose that x¯          stopped and a supervisor alerted. If the first part
          and s from (a) are adequate approximations         is in the Yellow Zone, a second part is measured.
          of the process mean and standard deviation         If this second measurement is in the Green Zone,
          for this variable. Use the normal distribution     no further action is required, but if it is in the
          with those parameters and compute a frac-          Yellow or the Red Zone, the lathe is stopped and
          tion of deviations that fall outside specifica-    a supervisor alerted. It is possible to argue that
          tions. Does it appear from this exercise that      under this scheme (continuing to suppose that
          the drilling operation is capable (i.e., precise)  measurements are independent normal variables
          enough to produce essentially all measured         with mean 1.181 in. and standard deviation .002
          deviations in specifications, at least if prop-    in.), the probability that the lathe is stopped in any
          erly aimed? Explain.                               given hour is .1865.
                                                             (c) Use the preceding fact and evaluate the prob-
30. An engineer is responsible for setting up a mon-
     itoring system for a critical diameter on a turned           ability that the lathe is stopped exactly twice
     metal part produced in his plant. Engineering                in 8 consecutive hours. Also, what is the
     specifications for the diameter are 1.180 in. ±              expected number of times the lathe will be
     .004 in. For ease of communication, the engineer             stopped in 8 time periods?
     sets up the following nomenclature for measured
     diameters on these parts:                               31. A random variable X has a cumulative distribution
                                                                  function
     Green Zone Diameters: 1.178 in.  Diameter
                                     1.182 in.                                for x  0
                                                                        0     for 0 < x  /2
     Red Zone Diameters: Diameter  1.176 in. or              F(x) =  sin(x)   for /2 < x
                                  Diameter  1.184 in.
                                                                           1
     Yellow Zone Diameters: any other Diameter
                                                                  (a) Find P[X  .32].
     Suppose that in fact the diameters of parts com-             (b) Give the probability density for X , f (x).
     ing off the lathe in question can be thought of as           (c) Evaluate EX and Var X .
     independent normal random variables with mean
     µ = 1.181 in. and standard deviation  = .002 in.        32. Return to the situation of Exercise 2 of Section
                                                                  5.4.
330 Chapter 5 Probability: The Mathematics of Randomness

        Suppose that demerits are assigned to devices          Y = the number of flaws identified by the inspector
     of the type considered there according to the for-
     mula D = 5X + Y .                                         (a) What is a sensible conditional distribution for
     (a) Find the mean value of D, ED. (Use your an-                Y given that X = 5? Given that X = 5, find
                                                                    the (conditional) probability that Y = 3.
          swers to (c) and (d) Exercise 2 of Section 5.4
          and formula (5.53) of Section 5.5. Formula           In general, a sensible conditional probability func-
          (5.53) holds whether or not X and Y are in-          tion for Y given X = x is the binomial probability
          dependent.)                                          function with number of trials x and success prob-
     (b) Find the probability a device of this type
          scores 7 or less demerits. That is, find             ability .8. That is, one could use
          P[D  7].
     (c) On average, how many of these devices will                                  .8 y .2x - y  for y = 0,
          have to be inspected in order to find one that                         x                 1, 2, . . . , x
          scores 7 or less demerits? (Use your answer          fY |X (y | x) = y 
          to (b).)                                                                                 otherwise
                                                                                  0
33. Consider jointly continuous random variables X
     and Y with density                                        Now suppose that X is modeled as Poisson with
                                                               mean  = 3--i.e.,

              x+y  for 0 < x < 1 and 0 < y < 1                            -3 x       for x = 0, 1, 2, 3, . . .
f (x, y) =         otherwise                                             e 3         otherwise
                                                               fX(x) =  x!
              0
                                                                            0

     (a) Find the probability that the product of X and             Multiplication of the two formulas gives a joint
          Y is at least 41 .                                        probability function for X and Y .
                                                                    (b) Find the (marginal) probability that Y = 0.
     (b) Find the marginal probability density for X .
          (Notice that Y 's is similar.) Use this to find the            (Note that this is obtained by summing
          expected value and standard deviation of X.                     f (x, 0) over all possible values of x.)
                                                                    (c) Find fY (y) in general. What (marginal) dis-
     (c) Are X and Y independent? Explain.                               tribution does Y have?
     (d) Compute the mean of X + Y . Why can't for-
                                                               36. Suppose that cans to be filled with a liquid are cir-
          mula (5.54) of Section 5.5 be used to find the            cular cylinders. The radii of these cans have mean
          variance of X + Y ?                                       µr = 1.00 in. and standard deviation r = .02 in.
                                                                    The volumes of liquid dispensed into these cans
34. Return to the situation of Exercise 4 of Section                have mean µv = 15.10 in.3 and standard devia-
     5.4.                                                           tion v = .05 in.3.
     (a) Find EX, Var X, EY , and Var Y using the                   (a) If the volumes dispensed into the cans are ap-
          marginal densities for X and Y .                               proximately normally distributed, about what
     (b) Use your answer to (a) and Proposition 1 to                     fraction will exceed 15.07 in.3?
          find the mean and variance of Y - X.                      (b) Approximate the probability that the total vol-
                                                                         ume dispensed into the next 100 cans exceeds
35. Visual inspection of integrated circuit chips, even                  1510.5 in.3 (if the total exceeds 1510.5, X ex-
     under high magnification, is often less than per-                   ceeds 15.105).
     fect. Suppose that an inspector has an 80% chance              (c) Approximate the mean µh and standard de-
     of detecting any given flaw. We will suppose that                   viation h of the heights of the liquid in the
     the inspector never "cries wolf"--that is, sees a
     flaw where none exists. Then consider the random
     variables

     X = the true number of flaws on a chip
                                                                              Chapter 5 Exercises 331

          filled cans. (Recall that the volume of a cir-              operating characteristic for  = .25, .5, and
          cular cylinder is v = r 2h, where h is the                  1.0.
          height of the cylinder.)                               (b) Suppose that instead of the rule in (a), this
     (d) Does the variation in bottle radius or the vari-             rule is followed: "Accept the lot if on 2 stan-
          ation in volume of liquid dispensed into the                dard size inspection units, 2 or fewer total
          bottles have the biggest impact on the varia-               nonconformities are seen." Make a plot of the
          tion in liquid height? Explain.                             operating characteristic curve for this second
                                                                      plan and compare it with the plot from part
37. Suppose that a pair of random variables have the                  (a). (Note that here, for X = the total number
     joint probability density                                        of nonconformities seen, X has a Poisson dis-
                                                                      tribution with mean 2 and OC() = P[X 
f (x, y) =  exp(x - y)  if 0  x  1 and x  y                           2].) List values of the operating characteristic
            0           otherwise                                     for  = .25, .5, and 1.0.

     (a) Evaluate P[Y  1.5].                                39. A discrete random variable X can be described
     (b) Find the marginal probability densities for X           using the following probability function:

          and Y .                                           x  1 2 3 45
     (c) Are X and Y independent? Explain.
     (d) Find the conditional probability density for       f (x) .61 .24 .10 .04 .01

          Y given X = .25, fY |X (y | .25). Given that      (a) Make a probability histogram for X. Also
          X = .25, what is the mean of Y ? (Hint: Use            plot F(x), the cumulative probability func-
           fY |X (y | .25).)                                     tion for X .

38. (Defects per Unit Acceptance Sampling) Sup-             (b) Find the mean and standard deviation for the
     pose that in the inspection of an incoming prod-            random variable X .
     uct, nonconformities on an inspection unit are
     counted. If too many are seen, the incoming lot        (c) Evaluate P[X  3] and then find P[X < 3].
     is rejected and returned to the manufacturer. (For
     concreteness, you might think of blemishes on          40. A classical data set of Rutherford and Geiger (re-
     rolled paper or wire, where an inspection unit con-
     sists of a certain length of material from the roll.)  ferred to in Example 6) suggests that for a partic-
     Suppose further that the number of nonconformi-
     ties on a piece of product of any particular size      ular experimental setup involving a small bar of
     can be modeled as Poisson with an appropriate
     mean.                                                  polonium, the number of collisions of  particles
     (a) Suppose that this rule is followed: "Accept
          the lot if on a standard size inspection unit,    with a small screen placed near the bar during
          1 or fewer nonconformities are seen." The
          operating characteristic curve of this accep-     an 8-minute period can be modeled as a Poisson
          tance sampling plan is a plot of the proba-
          bility that the lot is accepted as a function of  variable with mean  = 3.87. Consider an exper-
           = the mean defects per inspection unit. (For
          X = the number of nonconformities seen, X         imental setup of this type, and let X and Y be (re-
          has Poisson distribution with mean  and
          OC() = P[X  1].) Make a plot of the op-           spectively) the numbers of collisions in the next
          erating characteristic curve. List values of the
                                                            two 8-minute periods. Evaluate the following:
                                                            (a) P[X  2]       (b) Var X

                                                            (c) P[X + Y = 6]  (d) P[X + Y  3]

                                                            (Hint for parts (c) and (d): What is a sensible

                                                            probability distribution for X + Y , the number of

                                                            collisions in a 16-minute period?)
332 Chapter 5 Probability: The Mathematics of Randomness

41. Suppose that X is a continuous random variable               (a) If lengths are normally distributed about a
     with probability density of the form                             mean µ (which can be changed by altering
                                                                      the setup of a jig) and specifications on this
f (x) =  k x2(1 - x)  for 0 < x < 1                                   length are 33.69 in. ± .01 in., what appears
         0            otherwise                                       to be the best possible fraction of the lengths
                                                                      in specifications? What does µ need to be in
     (a) Evaluate k and sketch a graph of f (x).                      order to achieve this fraction?
     (b) Evaluate P[X  .25], P[X  .75], P[.25 <
                                                                 (b) Suppose now that in an effort to determine
          X  .75], and P[|X - .5| > .1].                              the mean length produced using the current
     (c) Compute EX and Var X.                                        setup of the jig, a sample of rods is to be taken
     (d) Compute and graph F(x), the cumulative                       and their lengths measured, with the intention
                                                                      of using the value of X as an estimate of µ.
          distribution function for X. Read from your                 Approximate the probabilities that X is within
          graph the .6 quantile of the distribution of X .            .0005 in. of µ for samples of size n = 25, 100,
                                                                      and 400. Do your calculations for this part of
42. Suppose that engineering specifications on the                    the question depend for their validity on the
     shelf depth of a certain slug to be turned on a                  length distribution being normal? Explain.
     CNC lathe are from .0275 in. to .0278 in. and that
     values of this dimension produced on the lathe         45. Suppose that the measurement of the diameters of
     can be described using a normal distribution with           #10 machine screws produced on a particular ma-
     mean µ and standard deviation  .                            chine yields values that are normally distributed
     (a) If µ = .0276 and  = .0001, about what frac-             with mean µ and standard deviation  = .03 mm.
          tion of shelf depths are in specifications?            (a) If µ = 4.68 mm, about what fraction of all
     (b) What machine precision (as measured by  )                    measured diameters will fall in the range from
          would be required in order to produce about                 4.65 mm to 4.70 mm?
          98% of shelf depths within engineering spec-           (b) Use your value from (a) and an appropri-
          ifications (assuming that µ is at the midpoint              ate discrete probability distribution to evalu-
          of the specifications)?                                     ate the probability (assuming µ = 4.68) that
                                                                      among the next five measurements made, ex-
43. The resistance of an assembly of several resistors                actly four will fall in the range from 4.65 mm
     connected in series is the sum of the resistances of             to 4.70 mm.
     the individual resistors. Suppose that a large lot of       (c) Use your value from (a) and an appropriate
     nominal 10 resistors has mean resistance µ =                     discrete probability distribution to evaluate
     9.91 and standard deviation of resistances  =                    the probability (assuming that µ = 4.68) that
     .08 . Suppose that 30 resistors are randomly                     if one begins sampling and measuring these
     selected from this lot and connected in series.                  screws, the first diameter in the range from
     (a) Find a plausible mean and variance for the                   4.65 mm to 4.70 mm will be found on the
          resistance of the assembly.                                 second, third, or fourth screw measured.
     (b) Evaluate the probability that the resistance            (d) Now suppose that µ is unknown but is to be
          of the assembly exceeds 298.2 . (Hint: If                   estimated by X obtained from measuring a
          X is the mean resistance of the 30 resistors                sample of n = 25 screws. Evaluate the prob-
          involved, the resistance of the assembly ex-                ability that the sample mean, X , takes a value
          ceeding 298.2 is the same as X exceeding                    within .01 mm of the long-run (population)
          9.94 . Now apply the central limit theorem.)                mean µ.

44. At a small metal fabrication company, steel rods
     of a particular type cut to length have lengths with
     standard deviation .005 in.
     (e) What sample size, n, would be required in                                   Chapter 5 Exercises 333
          order to a priori be 90% sure that X from n
          measurements will fall within .005 mm of µ?     (a) If X1, X2, X3, and X4 are the actual widths of
                                                               four of the product units and Y is the actual
46. The random variable X = the number of hours                inside length of a box into which they are to
     till failure of a disk drive is described using an        be packed, then the "head space" in the box is
     exponential distribution with mean 15,000 hours.         U = Y - (X1 + X2 + X3 + X4). What are a
     (a) Evaluate the probability that a given drive           sensible mean and standard deviation for U ?
          lasts at least 20,000 hours.
     (b) A new computer network has ten of these          (b) If X1, X2, X3, X4, and Y are normally dis-
          drives installed on computers in the network.        tributed and independent, it turns out that U
          Use your answer to (a) and an assumption of          is also normal. Suppose this is the case. About
          independence of the ten drive lifetimes and          what fraction of the time should the company
          evaluate the probability that at least nine of       expect to experience difficulty packing a box?
          these drives are failure-free through 20,000         (What is the probability that the head space
          hours.                                               as calculated in (a) is negative?)

47. Miles, Baumhover, and Miller worked with a            (c) If it is your job to recommend a new mean
     company on a packaging problem. Cardboard                 inside length of the boxes and the company
     boxes, nominally 9.5 in. in length were supposed          wishes to have packing problems in only .5%
     to hold four units of product stacked side by side.       of the attempts to load four units of product
     They did some measuring and found that in fact            into a box, what is the minimum mean inside
     the individual product units had widths with mean         length you would recommend? (Assume that
     approximately 2.577 in. and standard deviation            standard deviations will remain unchanged.)
     approximately .061 in. Further, the boxes had (in-
     side) lengths with mean approximately 9.566 in.
     and standard deviation approximately .053 in.
  6q q q q q q q q q q q q q q q q q q q q q q q q q q

               Introduction to
               Formal Statistical
               Inference

               Formal statistical inference uses probability theory to quantify the reliability of

                                  data-based conclusions. This chapter introduces the logic involved in several general
                                  types of formal statistical inference. Then the most common specific methods for
                                  one- and two-sample statistical studies are discussed.

                                       The chapter begins with an introduction to confidence interval estimation, using
                                  the important case of large-sample inference for a mean. Then the topic of signif-
                                  icance testing is considered, again using the case of large-sample inference for a
                                  mean. With the general notions in hand, successive sections treat the standard one-
                                  and two-sample confidence interval and significance-testing methods for means,
                                  then variances, and then proportions. Finally, the important topics of tolerance and
                                  prediction intervals are introduced.

                                                qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

         6.1 Large-Sample Confidence Intervals for a Mean

                                  Many important engineering applications of statistics fit the following standard
                                  mold. Values for parameters of a data-generating process are unknown. Based on
                                  data, the object is

                                       1. identify an interval of values likely to contain an unknown parameter (or a
                                           function of one or more parameters) and

                                       2. quantify "how likely" the interval is to cover the correct value.

                                       For example, a piece of equipment that dispenses baby food into jars might
                                  produce an unknown mean fill level, µ. Determining a data-based interval likely to

334
                                            6.1 Large-Sample Confidence Intervals for a Mean 335

              contain µ and an evaluation of the reliability of the interval might be important. Or
              a machine that puts threads on U-bolts might have an inherent variation in thread
              lengths, describable in terms of a standard deviation,  . The point of data collection
              might then be to produce an interval of likely values for  , together with a statement
              of how reliable the interval is. Or two different methods of running a pelletizing
              machine might have different unknown propensities to produce defective pellets,
              (say, p1 and p2). A data-based interval for p1 - p2, together with an associated
              statement of reliability, might be needed.

                   The type of formal statistical inference designed to deal with such problems is
              called confidence interval estimation.

Definition 1  A confidence interval for a parameter (or function of one or more parameters)
              is a data-based interval of numbers thought likely to contain the parameter (or
              function of one or more parameters) possessing a stated probability-based
              confidence or reliability.

                   This section discusses how basic probability facts lead to simple large-sample
              formulas for confidence intervals for a mean, µ. The unusual case where the standard
              deviation  is known is treated first. Then parallel reasoning produces a formula for
              the much more common situation where  is not known. The section closes with
              discussions of three practical issues in the application of confidence intervals.

6.1.1         A Large-n Confidence Interval for µ Involving 

              The final example in Section 5.5 involved a physically stable filling process known
              to have a net weight standard deviation of  = 1.6 g. Since, for large n, the sample
              mean of iid random variables is approximately normal, Example 26 of Chapter 5
              argued that for n = 47 and

              x¯ = the sample mean net fill weight of 47 jars filled by the process (g)

 Notational   there is an approximately 80% chance that x¯ is within .3 gram of µ. This fact is
conventions   pictured again in Figure 6.1.

                   We need to interrupt for a moment to discuss notation. In Chapter 5, capital
              letters were carefully used as symbols for random variables and corresponding
              lowercase letters for their possible or observed values. But here a lowercase symbol,
              x¯ , has been used for the sample mean random variable. This is fairly standard
              statistical usage, and it is in keeping with the kind of convention used in Chapters 3
              and 4. We are thus going to now abandon strict adherence to the capitalization
              convention introduced in Chapter 5. Random variables will often be symbolized
              using lowercase letters and the same symbols used for their observed values. The
              Chapter 5 capitalization convention is especially helpful in learning the basics of
              probability. But once those basics are mastered, it is common to abuse notation and
336 Chapter 6 Introduction to Formal Statistical Inference
                                                        P[µ - .3 < x < µ + .3]  .8

                                                                                    For n = 47, the approximate

                                                                                    distribution of x has standard
                                                                                    deviation 1.6  .23 g

                                                                                                      47

           µ - .3  µ                                                                µ + .3

           Figure 6.1 Approximate probability distribution for x¯ based on
           n = 47

           to determine from context whether a random variable or its observed value is being
           discussed.

                The most common way of thinking about a graphic like Figure 6.1 is to think
           of the possibility that

                   µ - .3 < x¯ < µ + .3                                                                             (6.1)

           in terms of whether or not x¯ falls in an interval of length 2(.3) = .6 centered at µ.
           But the equivalent is to consider whether or not an interval of length .6 centered at
           x¯ falls on top of µ. Algebraically, inequality (6.1) is equivalent to

                   x¯ - .3 < µ < x¯ + .3                                                                            (6.2)

           which shifts attention to this second way of thinking. The fact that expression (6.2)
           has about an 80% chance of holding true anytime a sample of 47 fill weights is taken
           suggests that the random interval

                   (x¯ - .3, x¯ + .3)                                                                               (6.3)

           might be used as a confidence interval for µ, with 80% associated reliability or
           confidence.

Example 1  A Confidence Interval for a Process Mean Fill Weight
           Suppose a sample of n = 47 jars produces x¯ = 138.2 g. Then expression (6.3)
           suggests that the interval with endpoints

                                                  138.2 g ± .3 g

           (i.e., the interval from 137.9 g to 138.5 g) be used as an 80% confidence interval
           for the process mean fill weight.
                           6.1 Large-Sample Confidence Intervals for a Mean 337

                                It is not hard to generalize the logic that led to expression (6.3). Anytime an iid
                           model is appropriate for the elements of a large sample, the central limit theorem
                           implies that the sample mean x¯ is approximately normal with mean µ and standard
                           deviation / n. Then, if for p > .5, z is the p quantile of the standard normal
                           distribution, the probability that

                                                     
                           µ - z  < x¯ < µ + z                                                        (6.4)
                             n                         n

                           is approximately 1 - 2(1 - p). But inequality (6.4) can be rewritten as

                                                     
                           x¯ - z  < µ < x¯ + z                                                       (6.5)
                           n                         n

                           and thought of as the eventuality that the random interval with endpoints

          Large-sample          
known  confidence            x¯ ± z                                                                   (6.6)
                                    n
             limits for µ

                           brackets the unknown µ. So an interval with endpoints (6.6) is an approximate
                           confidence interval for µ (with confidence level 1 - 2(1 - p)).

                                In an application, z in equation (6.6) is chosen so that the standard normal
                           probability between -z and z corresponds to a desired confidence level. Table
                           3.10 (of standard normal quantiles) on page 89 or Table B.3 (of standard normal
                           cumulative probabilities) can be used to verify the appropriateness of the entries in
                           Table 6.1. (This table gives values of z for use in expression (6.6) for some common
                           confidence levels.)

                           Table 6.1
                           z's for Use in Two-sided
                           Large-n Intervals for µ

                             Desired
                           Confidence z

                           80%        1.28

                           90%        1.645

                           95%        1.96

                           98%        2.33

                           99%        2.58
338 Chapter 6 Introduction to Formal Statistical Inference

Example 2  Confidence Interval for the Mean Deviation
           from Nominal in a Grinding Operation

           Dib, Smith, and Thompson studied a grinding process used in the rebuilding
           of automobile engines. The natural short-term variability associated with the
           diameters of rod journals on engine crankshafts ground using the process was
           on the order of  = .7 × 10-4 in. Suppose that the rod journal grinding process
           can be thought of as physically stable over runs of, say, 50 journals or less. Then
           if 32 consecutive rod journal diameters have mean deviation from nominal of
           x¯ = -.16 × 10-4 in., it is possible to apply expression (6.6) to make a confidence
           interval for the current process mean deviation from nominal. Consider a 95%
           confidence level. Consulting Table 6.1 (or otherwise, realizing that 1.96 is the
           p =.975 quantile of the standard normal distribution), z = 1.96 is called for in
           formula (6.6) (since .95 = 1 - 2(1 - .975)). Thus, a 95% confidence interval for
           the current process mean deviation from nominal journal diameter has endpoints

                                                            -4  .7 × 10-4
                               -.16 × 10 ± (1.96) 
                                                                32

           that is, endpoints

                               -.40 × 10-4 in. and .08 × 10-4 in.

                An interval like this one could be of engineering importance in determining
           the advisability of making an adjustment to the process aim. The interval includes
           both positive and negative values. So although x¯ < 0, the information in hand
           doesn't provide enough precision to tell with any certainty in which direction the
           grinding process should be adjusted. This, coupled with the fact that potential
           machine adjustments are probably much coarser than the best-guess misadjust-
           ment of x¯ = -.16 × 10-4 in., speaks strongly against making a change in the
           process aim based on the current data.

6.1.2      A Generally Applicable Large-n Confidence Interval for µ

           Although expression (6.6) provides a mathematically correct confidence interval, the
           appearance of  in the formula severely limits its practical usefulness. It is unusual to
           have to estimate a mean µ when the corresponding  is known (and can therefore be
           plugged into a formula). These situations occur primarily in manufacturing situations
           like those of Examples 1 and 2. Considerable past experience can sometimes give
           a sensible value for  , while physical process drifts over time can put the current
           value of µ in question.

                Happily, modification of the line of reasoning that led to expression (6.6) pro-
           duces a confidence interval formula for µ that depends only on the characteristics of
                                         6.1 Large-Sample Confidence Intervals for a Mean 339

                       a sample. The argument leading to formula (6.6) depends on the factthat for large
                       n, x¯ is approximately normal with mean µ and standard deviation / n--i.e., that

                                                                  Z = x¯ - µ (6.7)
                                                                           
                                                                          
                                                                            n

                       is approximately standard normal. The appearance of  in expression (6.7) is what
                       leads to its appearance in the confidence interval formula (6.6). But a slight gener-
                       alization of the central limit theorem guarantees that for large n,

                                                                  Z = x¯ - µ (6.8)
                                                                           s
                                                                          
                                                                            n

                       is also approximately standard normal. And the variable (6.8) doesn't involve  .
                            Beginning with the fact that (when an iid model for observations is appropriate

                       and n is large) the variable (6.8) is approximately standard normal, the reasoning is
                       much as before. For a positive z,

                                                              -z < x¯ - µ < z
                                                                         s
                                                                        
                                                                          n

                       is equivalent to

                                                       s            s
                                         µ - z  < x¯ < µ + z 
                                                          n            n

                       which in turn is equivalent to

                                                       s            s
                                         x¯ - z  < µ < x¯ + z 
                                                       n            n

                                                                                                         
                       Thus, the interval with random center x¯ and random length 2zs/ n--i.e., with
                       random endpoints

     Large-sample                                            s
confidence limits                                         x¯ ± z          (6.9)
                                                                 n
                for µ

                       can be used as an approximate confidence interval for µ. For a desired confidence,
                       z should be chosen such that the standard normal probability between -z and z
                       corresponds to that confidence level.
340 Chapter 6 Introduction to Formal Statistical Inference

Example 3  Breakaway Torques and Hard Disk Failures

           F. Willett, in the article "The Case of the Derailed Disk Drives" (Mechanical
           Engineering, 1988), discusses a study done to isolate the cause of "blink code
           A failure" in a model of Winchester hard disk drive. Included in that article are
           the data given in Figure 6.2. These are breakaway torques (units are inch ounces)
           required to loosen the drive's interrupter flag on the stepper motor shaft for 26
           disk drives returned to the manufacturer for blink code A failure. For these data,
           x¯ = 11.5 in. oz and s = 5.1 in. oz.

                0023
                078899
                1000112223
                155667779
                20
                2

           Figure 6.2 Torques required to
           loosen 26 interrupter flags

                If the disk drives that produced the data in Figure 6.2 are thought of as
           representing the population of drives subject to blink code A failure, it seems
           reasonable to use an iid model and formula (6.9) to estimate the population mean
           breakaway torque. Choosing to make a 90% confidence interval for µ, z = 1.645
           is indicated in Table 6.1. And using formula (6.9), endpoints

                                                                 5.1
                                                11.5 ± 1.645 

                                                                   26

           (i.e., endpoints 9.9 in. oz and 13.1 in. oz) are indicated.
                The interval shows that the mean breakaway torque for drives with blink

           code A failure was substantially below the factory's 33.5 in. oz target value.
           Recognizing this turned out to be key in finding and eliminating a design flaw in
           the drives.

6.1.3      Some Additional Comments Concerning
           Confidence Intervals

           Formulas (6.6) and (6.9) have been used to make confidence statements of the type
           "µ is between a and b." But often a statement like "µ is at least c" or "µ is no more
           than d" would be of more practical value. For example, an automotive engineer
           might wish to state, "The mean NO emission for this engine is at most 5 ppm."
           Or a civil engineer might want to make a statement like "the mean compressive
                   6.1 Large-Sample Confidence Intervals for a Mean 341

   Making          strength for specimens of this type of concrete is at least 4188 psi." That is, practical
one-sided          engineering problems are sometimes best addressed using one-sided confidence
                   intervals.
 intervals
                        There is no real problem in coming up with formulas for one-sided confidence
                   intervals. If you have a workable two-sided formula, all that must be done is to

                        1. replace the lower limit with - or the upper limit with + and

                        2. adjust the stated confidence level appropriately upward (this usually means
                            dividing the "unconfidence level" by 2).

                   This prescription works not only with formulas (6.6) and (6.9) but also with the rest
                   of the two-sided confidence intervals introduced in this chapter.

Example 3          For the mean breakaway torque for defective disk drives, consider making a one-
(continued )       sided 90% confidence interval for µ of the form (-, #), for # an appropriate
                   number. Put slightly differently, consider finding a 90% upper confidence bound
                   for µ, (say, #).

                        Beginning with a two-sided 80% confidence interval for µ, the lower limit can
                   be replaced with - and a one-sided 90% confidence interval determined. That
                   is, using formula (6.9), a 90% upper confidence bound for the mean breakaway
                   torque is

                   s     5.1
                   x¯ + 1.28  = 11.5 + 1.28  = 12.8 in. oz
                   n     26

                   Equivalently, a 90% one-sided confidence interval for µ is (-, 12.8).
                        The 12.8 in. oz figure here is less than (and closer to the sample mean than)

                   the 13.1 in. oz upper limit from the 90% two-sided interval found earlier. In the
                   one-sided case, - is declared as a lower limit so there is no risk of producing
                   an interval containing only numbers larger than the unknown µ. Thus an upper
                   limit smaller than that for a corresponding two-sided interval can be used.

   Interpreting a       A second issue in the application of confidence intervals is a correct understand-
confidence level   ing of the technical meaning of the term confidence. Unfortunately, there are many
                   possible misunderstandings. So it is important to carefully lay out what confidence
                   does and doesn't mean.

                        Prior to selecting a sample and plugging into a formula like (6.6) or (6.9), the
                   meaning of a confidence level is obvious. Choosing a (two-sided) 90% confidence
                   level and thus z = 1.645 for use in formula (6.9), before the fact of sample selection
                   and calculation, "there is about a 90% chance of winding up with an interval that
                   brackets µ." In symbols, this might be expressed as

                      s       s
                   P x¯ - 1.645  < µ < x¯ + 1.645   .90
                      n       n
342 Chapter 6 Introduction to Formal Statistical Inference

                                  But how to think about a confidence level after sample selection? This is an entirely
                                  different matter. Once numbers have been plugged into a formula like (6.6) or (6.9),
                                  the die has already been cast, and the numerical interval is either right or wrong.
                                  The practical difficulty is that while which is the case can't be determined, it no
                                  longer makes logical sense to attach a probability to the correctness of the interval.
                                  For example, it would make no sense to look again at the two-sided interval found
                                  in Example 3 and try to say something like "there is a 90% probability that µ
                                  is between 9.9 in. oz and 13.1 in. oz." µ is not a random variable. It is a fixed
                                  (although unknown) quantity that either is or is not between 9.9 and 13.1. There is
                                  no probability left in the situation to be discussed.

                                       So what does it mean that (9.9, 13.1) is a 90% confidence interval for µ? Like
                                  it or not, the phrase "90% confidence" refers more to the method used to obtain
                                  the interval (9.9, 13.1) than to the interval itself. In coming up with the interval,
                                  methodology has been used that would produce numerical intervals bracketing µ in
                                  about 90% of repeated applications. But the effectiveness of the particular interval
                                  in this application is unknown, and it is not quantifiable in terms of a probability. A
                                  person who (in the course of a lifetime) makes many 90% confidence intervals can
                                  expect to have a "lifetime success rate" of about 90%. But the effectiveness of any
                                  particular application will typically be unknown.

                                       A short statement summarizing this discussion as "the authorized interpretation
                                  of confidence" will be useful.

         Definition 2  To say that a numerical interval (a, b) is (for example) a 90% confidence
 (Interpretation of a  interval for a parameter is to say that in obtaining it, one has applied methods
Confidence Interval )  of data collection and calculation that would produce intervals bracketing the
                       parameter in about 90% of repeated applications. Whether or not the particular
                       interval (a, b) brackets the parameter is unknown and not describable in terms
                       of a probability.

     Sample sizes           The reader may feel that the statement in Definition 2 is a rather weak meaning
for estimating µ       for the reliability figure associated with a confidence interval. Nevertheless, the
                       statement in Definition 2 is the correct interpretation and is all that can be rationally
                       expected. And despite the fact that the correct interpretation may initially seem
                       somewhat unappealing, confidence interval methods have proved themselves to be
                       of great practical use.

                            As a final consideration in this introduction to confidence intervals, note that
                       formulas like (6.6) and (6.9) can give some crude quantitative answers to the ques-
                       tion, "How big must n be?" Using formula (6.9), for example, if you have in mind
                       (1) a desired confidence level, (2) a worst-case expectation for the sample standard
                       deviation, and (3) a desired precision of estimation for µ, it is a simple matter to
                       solve for a corresponding sample size. That is, suppose that the desired confidence
                       level dictates the use of the value z in formula (6.9), s is some likely worst-case
              6.1 Large-Sample Confidence Intervals for a Mean 343

              value for the sample standard deviation, and you want to have confidence limits (or
              a limit) of the form x¯ ± . Setting

                                                                   s
                                                             = z

                                                                    n

              and solving for n produces the requirement

                                                         n = zs 2

Example 3     Suppose that in the disk drive problem, engineers plan to follow up the analysis
(continued )  of the data in Figure 6.2 with the testing of a number of new drives. This will
              be done after subjecting them to accelerated (high) temperature conditions, in an
              effort to understand the mechanism behind the creation of low breakaway torques.
              Further suppose that the mean breakaway torque for temperature-stressed drives
              is to be estimated with a two-sided 95% confidence interval and that the torque
              variability expected in the new temperature-stressed drives is no worse than the
              s = 5.1 in. oz figure obtained from the returned drives. A ±1 in. oz precision of
              estimation is desired. Then using the plus-or-minus part of formula (6.9) and
              remembering Table 6.1, the requirement is

                                                                  5.1
                                                      1 = 1.96 

                                                                    n

              which, when solved for n, gives

                                             n = (1.96)(5.1) 2  100
                                                           1

                   A study involving in the neighborhood of n = 100 temperature-stressed
              new disk drives is indicated. If this figure is impractical, the calculations at
              least indicate that dropping below this sample size will (unless the variability
              associated with the stressed new drives is less than that of the returned drives)
              force a reduction in either the confidence or the precision associated with the
              final interval.

                   For two reasons, the kind of calculations in the previous example give somewhat
              less than an ironclad answer to the question of sample size. The first is that they
              are only as good as the prediction of the sample standard deviation, s. If s is
              underpredicted, an n that is not really large enough will result. (By the same token,
              if one is excessively conservative and overpredicts s, an unnecessarily large sample
              size will result.) The second issue is that expression (6.9) remains a large-sample
              formula. If calculations like the preceding ones produce n smaller than, say, 25 or 30,
              the value should be increased enough to guarantee that formula (6.9) can be applied.
344 Chapter 6 Introduction to Formal Statistical Inference

Section 1 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Interpret the statement, "The interval from 6.3 to      their data are recorded here. Given in the small
   7.9 is a 95% confidence interval for the mean µ."       frequency table are the measurements obtained on
                                                           50 screws by one of the students using the digital
2. In Chapter Exercise 2 of Chapter 3, there is a          calipers.
   data set consisting of the aluminum contents of
   26 bihourly samples of recycled PET plastic from         Diameter (mm)  Frequency
   a recycling facility. Those 26 measurements have
    y¯ = 142.7 ppm and s  98.2 ppm. Use these facts               4.52           1
   to respond to the following. (Assume that n = 26               4.66           4
   is large enough to permit the use of large-sample              4.67           7
   formulas in this case.)                                        4.68           7
    (a) Make a 90% two-sided confidence interval for              4.69         14
        the mean aluminum content of such specimens               4.70           9
        over the 52-hour study period.                            4.71           4
   (b) Make a 95% two-sided confidence interval for               4.72           4
        the mean aluminum content of such specimens
        over the 52-hour study period. How does this       (a) Compute the sample mean and standard devi-
        compare to your answer to part (a)?                     ation for these data.
    (c) Make a 90% upper confidence bound for the
        mean aluminum content of such samples over         (b) Use your sample values from (a) and make
        the 52-hour study period. (Find # such that             a 98% two-sided confidence interval for the
        (-, #) is a 90% confidence interval.) How               mean diameter of such screws as measured by
        does this value compare to the upper endpoint           this student with these calipers.
        of your interval from part (a)?
   (d) Make a 95% upper confidence bound for the           (c) Repeat part (b) using 99% confidence. How
        mean aluminum content of such samples over              does this interval compare with the one from
        the 52-hour study period. How does this value           (b)?
        compare to your answer to part (c)?
    (e) Interpret your interval from (a) for someone       (d) Use your values from (a) and find a 98% lower
        with little statistical background. (Speak in the       confidence bound for the mean diameter. (Find
        context of the recycling study and use Defini-          a number # such that (#, ) is a 98% confi-
        tion 2 as your guide.)                                  dence interval.) How does this value compare
                                                                to the lower endpoint of your interval from (b)?
3. Return to the context of Exercise 2. Suppose that in
   order to monitor for possible process changes, fu-      (e) Repeat (d) using 99% confidence. How does
   ture samples of PET will be taken. If it is desirable        the value computed here compare to your an-
   to estimate the mean aluminum content with ±20               swer to (d)?
   ppm precision and 90% confidence, what future
   sample size do you recommend?                           (f) Interpret your interval from (b) for someone
                                                                with little statistical background. (Speak in the
4. DuToit, Hansen, and Osborne measured the diam-               context of the diameter measurement study and
   eters of some no. 10 machine screws with two dif-            use Definition 2 as your guide.)
   ferent calipers (digital and vernier scale). Part of
               6.2 Large-Sample Significance Tests for a Mean 345

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

6.2 Large-Sample Significance Tests for a Mean

The goal of    The last section illustrated how probability can enable confidence interval estimation.
significance   This section makes a parallel introduction of significance testing.

      testing       Significance testing amounts to using data to quantitatively assess the plausi-
               bility of a trial value of a parameter (or function of one or more parameters). This
               trial value typically embodies a status quo/"pre-data" view. For example, a process
               engineer might employ significance testing to assess the plausibility of an ideal
               value of 138 g as the current process mean fill level of baby food jars. Or two dif-
               ferent methods of running a pelletizing machine might have unknown propensities
               to produce defective pellets, (say, p1 and p2), and significance testing could be used
               to assess the plausibility of p1 - p2 = 0--i.e., that the two methods are equally
               effective.

                    This section describes how basic probability facts lead to simple large-sample
               significance tests for a mean, µ. It introduces significance testing terminology in
               the case where the standard deviation  is known. Next, a five-step format for
               summarizing significance testing is presented. Then the more common situation of
               significance testing for µ where  is not known is considered. The section closes
               with two discussions about practical issues in the application of significance-testing
               logic.

6.2.1          Large-n Significance Tests for µ Involving 

               Recall once more Example 26 in Chapter 5, where a physically stable filling process
               is known to have  = 1.6 g for net weight. Suppose further that with a declared
               (label) weight of 135 g, process engineers have set a target mean net fill weight
               at 135 + 3 = 139.8 g. Finally, suppose that in a routine check of filling-process
               performance, intended to detect any change of the process mean from its target
               value, a sample of n = 25 jars produces x¯ = 139.0 g. What does this value have to
               say about the plausibility of the current process mean actually being at the target of
               139.8 g?

                    The central limit theorem can be called on here. If indeed the current process
               mean is at 139.8 g, x¯ has an approximately normal distribution with mean 139.8 g
               and standard deviation / n = 1.6/ 25 = .32 g, as pictured in Figure 6.3 along
               with the observed value of x¯ = 139.0 g.

                    Figure 6.4 shows the standard normal picture that corresponds to Figure 6.3. It is
               based on the fact that if the current process mean is on target at 139.8 g, then the fact
               that x¯ is approximately normal with mean µ and standard deviation / n = .32 g
               implies that

               Z = x¯ - 139.8 = x¯ - 139.8                                      (6.10)
                  .32
               
               n
346 Chapter 6 Introduction to Formal Statistical Inference

Observed x                                                     If µ = 139.8, the approximate
                                                               distribution of x is normal with
   *                                                           mean 139.8 and standard
                                                               deviation .32

139.0         139.8

Figure 6.3 Approximate probability distribution for x¯ if
µ = 139.8, and the observed value of x¯ = 139.0

is approximately standard normal. The observed x¯ = 139.0 g in Figure 6.3 has
corresponding observed z = -2.5 in Figure 6.4.

     It is obvious from either Figure 6.3 or Figure 6.4 that if the process mean
is on target at 139.8 g (and thus the figures are correct), a fairly extreme/rare x¯ ,
or equivalently z, has been observed. Of course, extreme/rare things occasionally
happen. But the nature of the observed x¯ (or z) might instead be considered as
making the possibility that the process is on target implausible.

     The figures even suggest a way of quantifying their own implausibility--through
calculating a probability associated with values of x¯ (or Z ) at least as extreme as
the one actually observed. Now "at least as extreme" must be defined in relation
to the original purpose of data collection--to detect either a decrease of µ below
target or an increase above target. Not only are values x¯  139.0 g (z  -2.5) as
extreme as that observed but so also are values x¯  140.6 g (z  2.5). (The first
kind of x¯ suggests a decrease in µ, and the second suggests an increase.) That is,
the implausibility of being on target might be quantified by noting that if this were
so, only a fraction

                                 (-2.5) + 1 - (2.5) = .01

of all samples would produce a value of x¯ (or Z ) as extreme as the one actually
observed. Put in those terms, the data seem to speak rather convincingly against the
process being on target.

Observed z                                                     If µ = 139.8, the approximate
                                                               distribution of Z = x - 139.8
   *
                                                                                                 1.6
                                                                                                  25

                                                               is standard normal

       -2 -1  0                                             1  2

Figure 6.4 The standard normal picture corresponding to
Figure 6.3
                                                6.2 Large-Sample Significance Tests for a Mean 347

                   The argument that has just been made is an application of typical significance-
              testing logic. In order to make the pattern of thought obvious, it is useful to isolate
              some elements of it in definition form. This is done next, beginning with a formal
              restatement of the overall purpose.

Definition 3  Statistical significance testing is the use of data in the quantitative assessment
              of the plausibility of some trial value for a parameter (or function of one or
              more parameters).

                   Logically, significance testing begins with the specification of the trial or hy-
              pothesized value. Special jargon and notation exist for the statement of this value.

Definition 4  A null hypothesis is a statement of the form

                                                   Parameter = #

              or

                                           Function of parameters = #

              (for some number, #) that forms the basis of investigation in a significance
              test. A null hypothesis is usually formed to embody a status quo/"pre-data"
              view of the parameter (or function of the parameter(s)). It is typically denoted
              as H0.

                   The notion of a null hypothesis is so central to significance testing that it is
              common to use the term hypothesis testing in place of significance testing. The
              "null" part of the phrase "null hypothesis" refers to the fact that null hypotheses are
              statements of no difference, or equality. For example, in the context of the filling
              operation, standard usage would be to write

              H0: µ = 139.8  (6.11)

              meaning that there is no difference between µ and the target value of 139.8 g.
                   After formulating a null hypothesis, what kinds of departures from it are of

              interest must be specified.

Definition 5  An alternative hypothesis is a statement that stands in opposition to the null
              hypothesis. It specifies what forms of departure from the null hypothesis are
              of concern. An alternative hypothesis is typically denoted as Ha. It is of the
348 Chapter 6 Introduction to Formal Statistical Inference

                                      same form as the corresponding null hypothesis, except that the equality sign
                                      is replaced by =, >, or <.

              Often, the alternative hypothesis is based on an investigator's suspicions and/or
              hopes about the true state of affairs, amounting to a kind of research hypothesis
              that the investigator hopes to establish. For example, if an engineer tests what is
              intended to be a device for improving automotive gas mileage, a null hypothesis
              expressing "no mileage change" and an alternative hypothesis expressing "mileage
              improvement" would be appropriate.

                   Definitions 4 and 5 together imply that for the case of testing about a single
              mean, the three possible pairs of null and alternative hypotheses are

              H0: µ = #  H0: µ = #      H0: µ = #
              Ha: µ > #  Ha: µ < #      Ha: µ = #

              In the example of the filling operation, there is a need to detect both the possibility of
              consistently underfilled (µ < 139.8 g) and the possibility of consistently overfilled
              (µ > 139.8 g) jars. Thus, an appropriate alternative hypothesis is

                         Ha: µ = 139.8             (6.12)

                   Once null and alternative hypotheses have been established, it is necessary
              to lay out carefully how the data will be used to evaluate the plausibility of the
              null hypothesis. This involves specifying a statistic to be calculated, a probability
              distribution appropriate for it if the null hypothesis is true, and what kinds of observed
              values will make the null hypothesis seem implausible.

Definition 6  A test statistic is the particular form of numerical data summarization used
              in a significance test. The formula for the test statistic typically involves the
              number appearing in the null hypothesis.

Definition 7  A reference (or null) distribution for a test statistic is the probability dis-
              tribution describing the test statistic, provided the null hypothesis is in fact
              true.

                   The values of the test statistic considered to cast doubt on the validity of the
              null hypothesis are specified after looking at the form of the alternative hypothesis.
              Roughly speaking, values are identified that are more likely to occur if the alternative
              hypothesis is true than if the null hypothesis holds.
                 6.2 Large-Sample Significance Tests for a Mean 349

                      The discussion of the filling process scenario has vacillated between using x¯
                 and its standardized version Z given in equation (6.10) for a test statistic. Equation
                 (6.10) is a specialized form of the general (large-n, known  ) test statistic for µ,

Large-sample     Z = x¯ - #                                        (6.13)
known  test              
statistic for µ         
                           n

                 for the present scenario, where the hypothesized value of µ is 139.8, n = 25, and
                  = 1.6. It is most convenient to think of the test statistic for this kind of problem
                 in the standardized form shown in equation (6.13) rather than as x¯ itself. Using
                 form (6.13), the reference distribution will always be the same--namely, standard

                 normal.

                      Continuing with the filling example, note that if instead of the null hypothesis
                 (6.11), the alternative hypothesis (6.12) is operating, observed x¯ 's much larger or
                 much smaller than 139.8 will tend to result. Such x¯ 's will then, via equation (6.13),
                 translate respectively to large or small (that is, large negative numbers in this case)
                 observed values of Z --i.e., large values |z|. Such observed values render the null
                 hypothesis implausible.

                      Having specified how data will be used to judge the plausibility of the null

                 hypothesis, it remains to collect them, plug them into the formula for the test

                 statistic, and (using the calculated value and the reference distribution) arrive at a

                 quantitative assessment of the plausibility of H0. There is jargon for the form this
                 will take.

Definition 8     The observed level of significance or p-value in a significance test is the
                 probability that the reference distribution assigns to the set of possible values
                 of the test statistic that are at least as extreme as the one actually observed (in
                 terms of casting doubt on the null hypothesis).

Small p-values   The smaller the observed level of significance, the stronger the evidence against
  are evidence   the validity of the null hypothesis. In the context of the filling operation, with an
     against H0  observed value of the test statistic of

                                                             z = -2.5

                 the p-value or observed level of significance is

                 (-2.5) + 1 - (2.5) = .01

                 which gives fairly strong evidence against the possibility that the process mean is
                 on target.
350 Chapter 6 Introduction to Formal Statistical Inference

         6.2.2    A Five-Step Format for Summarizing Significance Tests

       Five-step  It is helpful to lay down a step-by-step format for organizing write-ups of significance
   significance   tests. The one that will be used in this text includes the following five steps:
testing format

                  Step 1  State the null hypothesis.
                  Step 2
                  Step 3  State the alternative hypothesis.

                  Step 4  State the test criteria. That is, give the formula for the test statistic
                  Step 5  (plugging in only a hypothesized value from the null hypothesis,
                          but not any sample information) and the reference distribution. Then
                          state in general terms what observed values of the test statistic will
                          constitute evidence against the null hypothesis.

                          Show the sample-based calculations.

                          Report an observed level of significance and (to the extent possible)
                          state its implications in the context of the real engineering problem.

Example 4         A Significance Test Regarding a Process Mean Fill Level

                  The five-step significance-testing format can be used to write up the preceding
                  discussion of the filling process.

                  1. H0: µ = 139.8.
                  2. Ha: µ = 139.8.
                  3. The test statistic is

                                                            Z = x¯ - 139.8
                                                                       
                                                                      
                                                                        n

                      The reference distribution is standard normal, and large observed values
                      |z| will constitute evidence against H0.

                  4. The sample gives

                                                 z = 139.0 - 139.8 = -2.5
                                                             1.6

                                                           
                                                              100

                  5. The observed level of significance is

                          P[a standard normal variable  -2.5]
                                              +P[a standard normal variable  2.5]

                               = P [|a standard normal variable|  2.5]
                               = .01
                                             6.2 Large-Sample Significance Tests for a Mean 351

                        This is reasonably strong evidence that the process mean fill level is not
                        on target.

6.2.3                   Generally Applicable Large-n Significance Tests for µ

                        The significance-testing method used to carry the discussion thus far is easy to
                        discuss and understand but of limited practical use. The problem with it is that
                        statistic (6.13) involves the parameter  . As remarked in Section 6.1, there are few
                        engineering contexts where one needs to make inferences regarding µ but knows
                        the corresponding  . Happily, because of the same probability fact that made it
                        possible to produce a large-sample confidence interval formula for µ free of  , it is
                        also possible to do large-n significance testing for µ without having to supply  .

                             For observations that are describable as essentially equivalent to random selec-
                        tions with replacement from a single population with mean µ and variance  2, if n
                        is large,

                                                       Z = x¯ - µ
                                                                s
                                                               
                                                                 n

                        is approximately standard normal. This means that for large n, to test

                                                                    H0: µ = #

                        a widely applicable method will simply be to use the logic already introduced but
                        with the statistic

Large-sample                                           Z = x¯ - #                               (6.14)
  test statistic                                                s
           for µ                                              
                                                                 n

                        in place of statistic (6.13).

            Example 5   Significance Testing and Hard Disk Failures
(Example 3 revisited )
                        Consider again the problem of disk drive blink code A failure. Breakaway torques
                        set at the factory on the interrupter flag connection to the stepper motor shaft
                        averaged 33.5 in. oz, and there was suspicion that blink code A failure was
                        associated with reduced breakaway torque. Recall that a sample of n = 26 failed
                        drives had breakaway torques (given in Figure 6.2) with x¯ = 11.5 in. oz and
                        s = 5.1 in. oz.

                             Consider the situation of an engineer wishing to judge the extent to which the
                        data in hand debunk the possibility that drives experiencing blink code A failure
352 Chapter 6 Introduction to Formal Statistical Inference

Example 5     have mean breakaway torque equal to the factory-set mean value of 33.5 in. oz.
(continued )  The five-step significance-testing format can be used.

              1. H0: µ = 33.5.
              2. Ha: µ < 33.5.

                  (Here the alternative hypothesis is directional, amounting to a research
                  hypothesis based on the engineer's suspicions about the relationship be-
                  tween drive failure and breakaway torque.)
              3. The test statistic is

                                                     Z = x¯ - 33.5
                                                                s
                                                              
                                                                 n

                  The reference distribution is standard normal, and small observed values
                  z will constitute evidence against the validity of H0. (Means less than 33.5
                  will tend to produce x¯ 's of the same nature and therefore small--i.e., large
                  negative--z's.)
              4. The sample gives

                                             z = 11.5 - 33.5 = -22.0
                                                        5.1
                                                       
                                                          26

              5. The observed level of significance is

              P[a standard normal variable < -22.0]  0

              The sample provides overwhelming evidence that failed drives have a
              mean breakaway torque below the factory-set level.

                   It is important not to make too much of a logical jump here to an incorrect
              conclusion that this work constitutes the complete solution to the real engineering
              problem. Drives returned for blink code A failure have substandard breakaway
              torques. But in the absence of evidence to the contrary, it is possible that they
              are no different in that respect from nonfailing drives currently in the field. And
              even if reduced breakaway torque is at fault, a real-world fix of the drive failure
              problem requires the identification and prevention of the physical mechanism
              producing it. This is not to say the significance test lacks importance, but rather
              to remind the reader that it is but one of many tools an engineer uses to do a job.
               6.2 Large-Sample Significance Tests for a Mean 353

6.2.4          Significance Testing and Formal Statistical
               Decision Making (Optional )

               The basic logic introduced in this section is sometimes applied in a decision-making
               context, where data are being counted on to provide guidance in choosing between
               two rival courses of action. In such cases, a decision-making framework is often
               built into the formal statistical analysis in an explicit way, and some additional
               terminology and patterns of thought are standard.

                    In some decision-making contexts, it is possible to conceive of two different
               possible decisions or courses of action as being related to a null and an alternative
               hypothesis. For example, in the filling-process scenario, H0: µ = 139.8 might cor-
               respond to the course of action "leave the process alone," and Ha: µ = 139.8 could
               correspond to the course of action "adjust the process." When such a correspondence
               holds, two different errors are possible in the decision-making process.

Definition 9   When significance testing is used in a decision-making context, deciding in
               favor of Ha when in fact H0 is true is called a type I error.

Definition 10  When significance testing is used in a decision-making context, deciding in
               favor of H0 when in fact Ha is true is called a type II error.

               The content of these two definitions is represented in the 2 × 2 table pictured in
               Figure 6.5. In the filling-process problem, a type I error would be adjusting an
               on-target process. A type II error would be failing to adjust an off-target process.

                    Significance testing is harnessed and used to come to a decision by choosing
               a critical value and, if the observed level of significance is smaller than the critical
               value (thus making the null hypothesis correspondingly implausible), deciding in
               favor of Ha. Otherwise, the course of action corresponding to H0 is followed. The
               critical value for the observed level of significance ends up being the a priori

               The ultimate decision is in favor of:

                                      H0  Ha

               The true state H0                  Type I
               of affairs is                       error
               described by:
                                      Type II
                                  Ha   error

               Figure 6.5 Four potential outcomes in a
               decision problem
354 Chapter 6 Introduction to Formal Statistical Inference
                                  probability the decision maker runs of deciding in favor of Ha, calculated supposing
                                  H0 to be true. There is special terminology for this concept.

Definition 11  When significance testing is used in a decision-making context, a critical
               value separating those large observed levels of significance for which H0 will
               be accepted from those small observed levels of significance for which H0
               will be rejected in favor of Ha is called the type I error probability or the
               significance level. The symbol  is usually used to stand for the type I error
               probability.

                    It is standard practice to use small numbers, like .1, .05, or even .01, for . This
               puts some inertia in favor of H0 into the decision-making process. (Such a practice
               guarantees that type I errors won't be made very often. But at the same time, it
               creates an asymmetry in the treatment of H0 and Ha that is not always justified.)

                    Definition 10 and Figure 6.5 make it clear that type I errors are not the only
               undesirable possibility. The possibility of type II errors must also be considered.

Definition 12  When significance testing is used in a decision-making context, the prob-
               ability--calculated supposing a particular parameter value described by Ha
               holds--that the observed level of significance is bigger than  (i.e., H0 is not
               rejected) is called a type II error probability. The symbol  is usually used
               to stand for a type II error probability.

                    For most of the testing methods studied in this book, calculation of 's is more
               than the limited introduction to probability given in Chapter 5 will support. But the
               job can be handled for the simple known- situation that was used to introduce the
               topic of significance testing. And making a few such calculations will provide some
               intuition consistent with what, qualitatively at least, holds in general.

Example 4      Again consider the filling process and testing H0: µ = 139.8 vs. Ha: µ = 139.8.
(continued )   This time suppose that significance testing based on n = 25 will be used tomorrow
               to decide whether or not to adjust the process. Type II error probabilities, calcu-
               lated supposing µ = 139.5 and µ = 139.2 for tests using  = .05 and  = .2,
               will be compared.

                    First consider  = .05. The decision will be made in favor of H0 if the p-
               value exceeds .05. That is, the decision will be in favor of the null hypothesis if
               the observed value of Z given in equation (6.10) (generalized in formula (6.13))
               is such that

                                                         |z| < 1.96
                 6.2 Large-Sample Significance Tests for a Mean 355

i.e., if

          139.8 - 1.96(.32) < x¯ < 139.8 + 1.96(.32)

i.e., if

                 139.2 < x¯ < 140.4                                        (6.15)

Now if µ described by Ha given in display (6.12) is the true process mean, x¯ is
not approximately normal with mean 139.8 and standard deviation .32, but rather
approximately normal with mean µ and standard deviation .32. So for such a µ,
expression (6.15) and Definition 12 show that the corresponding  will be the
probability the corresponding normal distribution assigns to the possibility that
139.2 < x¯ < 140.4. This is pictured in Figure 6.6 for the two means µ = 139.5
and µ = 139.2.

     It is an easy matter to calculate z-values corresponding to x¯ = 139.2 and
x¯ = 140.4 using means of 139.5 and 139.2 and a standard deviation of .32 and to
consult a standard normal table in order to verify the correctness of the two 's
marked in Figure 6.6.

     Parallel reasoning for the situation with  = .2 is as follows. The decision
will be in favor of H0 if the p-value exceeds .2. That is, the decision will be in
favor of H0 if |z| < 1.28--i.e., if

                                    139.4 < x¯ < 140.2

            .83                        The approximate distribution
                                       of x if µ = 139.5 has mean 139.5
                                       and standard deviation .32

                 139.2 139.5 139.8     140.4
                                  .50

                 The approximate distribution
                 of x if µ = 139.2 has mean 139.2
                 and standard deviation .32

                 139.2 139.5 139.8     140.4

          Figure 6.6 Approximate probability distributions for x¯ for two
          different values of µ described by Ha and the corresponding 's,
          when  = .05
356 Chapter 6 Introduction to Formal Statistical Inference

Example 4     If µ described by Ha is the true process mean, x¯ is approximately normal with
(continued )  mean µ and standard deviation .32. So the corresponding  will be the probability
              this normal distribution assigns to the possibility that 139.4 < x¯ < 140.2. This
              is pictured in Figure 6.7 for the two means µ = 139.5 and µ = 139.2, having
              corresponding type II error probabilities  = .61 and  = .27.

                   The calculations represented by the two figures are collected in Table 6.2.
              Notice two features of the table. First, the  values for  = .05 are larger than
              those for  = .2. If one wants to run only a 5% chance of (incorrectly) deciding

              to adjust an on-target process, the price to be paid is a larger probability of failure
              to recognize an off-target condition. Secondly, the  values for µ = 139.2 are
              smaller than the  values for µ = 139.5. The further the filling process is from

              being on target, the less likely it is that the off-target condition will fail to be

              detected.

                                                            The approximate distribution
                                                            of x if µ = 139.5 has mean 139.5
                                                            and standard deviation .32

                                                                            .61

              139.4 139.5 139.8 140.2

                                                              .27

                                                            The approximate distribution
                                                            of x if µ = 139.2 has mean 139.2
                                                            and standard deviation .32

              139.2 139.4 139.8                                    140.2

              Figure 6.7 Approximate probability distributions for x¯ for two
              different values of µ described by Ha and the corresponding 's,
              when  = .2

              Table 6.2
              n = 25 type II error
              probabilities ()

                                     µ
                             139.2 139.5

                     .05 .50 .83
               

                      .2 .27 .61
                       6.2 Large-Sample Significance Tests for a Mean 357

      The effect of         The story told by Table 6.2 applies in qualitative terms to all uses of significance
         sample size   testing in decision-making contexts. The further H0 is from being true, the smaller
                on 's  the corresponding . And small 's imply large 's and vice versa.

Analogy between             There is one other element of this general picture that plays an important role in
      testing and a    the determination of error probabilities. That is the matter of sample size. If a sample
       criminal trial  size can be increased, for a given , the corresponding 's can be reduced. Redo the
                       calculations of the previous example, this time supposing that n = 100 rather than
                       25. Table 6.3 shows the type II error probabilities that should result, and comparison
                       with Table 6.2 serves to indicate the sample-size effect in the filling-process example.

                            An analogy helpful in understanding the standard logic applied when signifi-
                       cance testing is employed in decision-making involves thinking of the process of
                       coming to a decision as a sort of legal proceeding, like a criminal trial. In a criminal
                       trial, there are two opposing hypotheses, namely

                                                      H0 : The defendant is innocent

                                                      Ha : The defendant is guilty

                       Evidence, playing a role similar to the data used in testing, is gathered and used to
                       decide between the two hypotheses. Two types of potential error exist in a criminal
                       trial: the possibility of convicting an innocent person (parallel to the type I error)
                       and the possibility of acquitting a guilty person (similar to the type II error). A
                       criminal trial is a situation where the two types of error are definitely thought of as
                       having differing consequences, and the two hypotheses are treated asymmetrically.
                       The a priori presumption in a criminal trial is in favor of H0, the defendant's
                       innocence. In order to keep the chance of a false conviction small (i.e., keep 
                       small), overwhelming evidence is required for conviction, in much the same way
                       that if small  is used in testing, extreme values of the test statistic are needed in
                       order to indicate rejection of H0. One consequence of this method of operation in
                       criminal trials is that there is a substantial chance that a guilty individual will be
                       acquitted, in the same way that small 's produce big 's in testing contexts.

                            This significance testing/criminal trial parallel is useful, but do not make more
                       of it than is justified. Not all significance-testing applications are properly thought
                       of in this light. And few engineering scenarios are simple enough to reduce to a
                       "decide between H0 and Ha" choice. Sensible applications of significance testing are

                       Table 6.3
                       n = 100 Type II Error
                       Probabilities ()

                                              µ
                                      139.2 139.5

                         .05 .04  .53

                         .2 .01   .28
358 Chapter 6 Introduction to Formal Statistical Inference

                 often only steps of "evidence evaluation" in a many-faceted, data-based detective
                 job necessary to solve an engineering problem. And even when a real problem can
                 be reduced to a simple "decide between H0 and Ha" framework, it need not be the
                 case that the "choose a small " logic is appropriate. In some engineering contexts,
                 the practical consequences of a type II error are such that rational decision-making
                 strikes a balance between the opposing goals of small  and small 's.

        6.2.5    Some Comments Concerning Significance
                 Testing and Estimation
   "Statistical
significance"    Confidence interval estimation and significance testing are the two most commonly
and practical    used forms of formal statistical inference. These having been introduced, it is ap-
                 propriate to offer some comparative comments about their practical usefulness and,
  importance     in the process, admit to an estimation orientation that will be reflected in much of
                 the rest of this book's treatment of formal inference.

                      More often than not, engineers need to know "What is the value of the pa-
                 rameter?" rather than "Is the parameter equal to some hypothesized value?" And
                 it is confidence interval estimation, not significance testing, that is designed to an-
                 swer the first question. A confidence interval for a mean breakaway torque of from
                 9.9 in. oz to 13.1 in. oz says what values of µ seem plausible. A tiny observed level
                 of significance in testing H0: µ = 33.5 says only that the data speak clearly against
                 the possibility that µ = 33.5, but it doesn't give any clue to the likely value of µ.

                      The fact that significance testing doesn't produce any useful indication of what
                 parameter values are plausible is sometimes obscured by careless interpretation of
                 semistandard jargon. For example, it is common in some fields to term p-values less
                 than .05 "statistically significant" and ones less than .01 "highly significant." The
                 danger in this kind of usage is that "significant" can be incorrectly heard to mean "of
                 great practical consequence" and the p-value incorrectly interpreted as a measure of
                 how much a parameter differs from a value stated in a null hypothesis. One reason
                 this interpretation doesn't follow is that the observed level of significance in a test
                 depends not only on how far H0 appears to be from being correct but on the sample
                 size as well. Given a large enough sample size, any departure from H0, whether of
                 practical importance or not, can be shown to be "highly significant."

Example 6        Statistical Significance and Practical Importance
                 in a Regulatory Agency Test

                 A good example of the previous points involves the newspaper article in Figure
                 6.8. Apparently the Pass Master manufacturer did enough physical mileage testing
                 (used a large enough n) to produce a p-value less than .05 for testing a null
                 hypothesis of no mileage improvement. That is, a "statistically significant" result
                 was obtained.

                      But the size of the actual mileage improvement reported is only "small
                 but real," amounting to about .8 mpg. Whether or not this improvement is of
                 practical importance is a matter largely separate from the significance-testing
                                               6.2 Large-Sample Significance Tests for a Mean 359

                    WASHINGTON (AP) --A gadget that cuts off a car's air conditioner when the
                    vehicle accelerates has become the first product aimed at cutting gasoline
                    consumption to win government endorsement.

                          The device, marketed under the name "Pass Master," can provide a
                    "small but real fuel economy benefit," the Environmental Protection Agency
                    said Wednesday.

                          Motorists could realize up to 4 percent fuel reduction while using their air
                    conditioners on cars equipped with the device, the agency said. That would
                    translate into .8-miles-per-gallon improvement for a car that normally gets 20
                    miles to the gallon with the air conditioner on.

                          The agency cautioned that the 4 percent figure was a maximum amount
                    and could be less depending on a motorist's driving habits, the type of car and
                    the type of air conditioner.

                          But still the Pass Master, which sells for less than $15, is the first of 40
                    products to pass the EPA's tests as making any "statistically significant"
                    improvement in a car's mileage.

              Figure 6.8 Article from The Lafayette Journal and Courier, Page D-3, August 28, 1980.
              Reprinted by permission of the Associated Press. c 1980 the Associated Press.

                result. And an engineer equipped with a confidence interval for the mean mileage
                improvement is in a better position to judge this than is one who knows only that
                the p-value was less than .05.

Example 5     To illustrate the effect that sample size has on observed level of significance,
(continued )  return to the breakaway torque problem and consider two hypothetical samples,
              one based on n = 25 and the other on n = 100 but both giving x¯ = 32.5 in. oz
              and s = 5.1 in. oz.

                   For testing H0: µ = 33.5 with Ha: µ < 33.5, the first hypothetical sample
              gives

                                               z = 32.5 - 33.5 = -.98
                                                          5.1
                                                         
                                                           25

              with associated observed level of significance

                                                       (-.98) = .16

              The second hypothetical sample gives

                                              z = 32.5 - 33.5 = -1.96
                                                         5.1

                                                       
                                                          100
360 Chapter 6 Introduction to Formal Statistical Inference

Example 5     with corresponding p-value
(continued )
                                                      (-1.96) = .02

              Because the second sample size is larger, the second sample gives stronger
              evidence that the mean breakaway torque is below 33.5 in. oz. But the best data-
              based guess at the difference between µ and 33.5 is x¯ - 33.5 = -1.0 in. oz in
              both cases. And it is the size of the difference between µ and 33.5 that is of
              primary engineering importance.

                   It is further useful to realize that in addition to doing its primary job of providing
              an interval of plausible values for a parameter, a confidence interval itself also pro-
              vides some significance-testing information. For example, a 95% confidence interval
              for a parameter contains all those values of the parameter for which significance
              tests using the data in hand would produce p-values bigger than 5%. (Those values
              not covered by the interval would have associated p-values smaller than 5%.)

Example 5     Recall from Section 6.1 that a 90% one-sided confidence interval for the mean
(continued )  breakaway torque for failed drives is (-, 12.8). This means that for any value,
              #, larger than 12.8 in. oz, a significance test of H0: µ = # with Ha: µ < # would
              produce a p-value less than .1. So clearly, the observed level of significance
              corresponding to the null hypothesis H0: µ = 33.5 is less than .1 . (In fact, as
              was seen earlier in this section, the p-value is 0 to two decimal places.) Put more
              loosely, the interval (-, 12.8) is a long way from containing 33.5 in. oz and

              therefore makes such a value of µ quite implausible.

                   The discussion here could well raise the question "What practical role remains
              for significance testing?" Some legitimate answers to this question are

                   1. In an almost negative way, p-values can help an engineer gauge the extent to
                       which data in hand are inconclusive. When observed levels of significance
                       are large, more information is needed in order to arrive at any definitive
                       judgment.

                   2. Sometimes legal requirements force the use of significance testing in a
                       compliance or effectiveness demonstration. (This was the case in Figure 6.8,
                       where before the Pass Master could be marketed, some mileage improvement
                       had to be legally demonstrated.)

                   3. There are cases where the use of significance testing in a decision-making
                       framework is necessary and appropriate. (An example is acceptance sam-
                       pling: Based on information from a sample of items from a large lot, one
                       must determine whether or not to receive shipment of the lot.)
                                  6.3 One- and Two-Sample Inference for Means 361

So, properly understood and handled, significance testing does have its place in
engineering practice. Thus, although the rest of this book features estimation over
significance testing, methods of significance testing will not be completely ignored.

Section 2 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. In the aluminum contamination study discussed in           (c) In the students' application, the mean height of
   Exercise 2 of Section 6.1 and in Chapter Exer-                 the punches did not tell the whole story about
   cise 2 of Chapter 3, it was desirable to have mean             how they worked in the stamping machine.
   aluminum content for samples of recycled plas-                 Several of these punches had to be placed side
   tic below 200 ppm. Use the five-step significance-             by side and used to stamp the same piece of
   testing format and determine the strength of the               material. In this context, what other feature of
   evidence in the data that in fact this contamination           the height distribution is almost certainly of
   goal has been violated. (You will want to begin with           practical importance?
   H0: µ = 200 ppm and use Ha: µ > 200 ppm.)
                                                          3. Discuss, in the context of Exercise 2, part (a), the
2. Heyde, Kuebrick, and Swanson measured the                 potential difference between statistical significance
   heights of 405 steel punches of a particular type.        and practical importance.
   These were all from a single manufacturer and were
   supposed to have heights of .500 in. (The stamping     4. In the context of the machine screw diameter study
   machine in which these are used is designed to use        of Exercise 4 of Section 6.1, suppose that the nom-
   .500 in. punches.) The students' measurements had         inal diameter of such screws is 4.70 mm. Use
   x¯ = .5002 in. and s = .0026 in. (The raw data are        the five-step significance-testing format and as-
   given in Chapter Exercise 9 of Chapter 3.)                sess the strength of the evidence provided by the
    (a) Use the five-step format and test the hypothesis     data that the long-run mean measured diameter dif-
        that the mean height of such punches is "on          fers from nominal. (You will want to begin with
        spec" (i.e., is .500 in.).                           H0: µ = 4.70 mm and use Ha: µ = 4.70 mm.)
   (b) Make a 98% two-sided confidence interval for
        the mean height of such punches produced by       5. Discuss, in the context of Exercise 4, the poten-
        this manufacturer under conditions similar to        tial difference between statistical significance and
        those existing when the students' punches were       practical importance.
        manufactured. Is your interval consistent with
        the outcome of the test in part (a)? Explain.

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

6.3 One- and Two-Sample Inference for Means

                    Sections 6.1 and 6.2 introduced the basic concepts of confidence interval estimation
                    and significance testing. There are thousands of specific methods of these two types.
                    This book can only discuss a small fraction that are particularly well known and
                    useful to engineers. The next three sections consider the most elementary of these--
                    some of those that are applicable to one- and two-sample studies--beginning in this
                    section with methods of formal inference for means.

                         Inferences for a single mean, based not on the large samples of Sections 6.1 and
                    6.2 but instead on small samples, are considered first. In the process, it is necessary
362 Chapter 6 Introduction to Formal Statistical Inference

                                  to introduce the so-called (Student) t probability distributions. Presented next are
                                  methods of formal inference for paired data. The section concludes with discussions
                                  of both large- and small-n methods for data-based comparison of two means based
                                  on independent samples.

6.3.1          Small-Sample Inference for a Single Mean

               The most important practical limitation on the use of the methods of the previous
               two sections is the requirement that n must be large. That restriction comes from
               the fact that without it, there is no way to conclude that

                                x¯ - µ                          (6.16)
                                  s
                                 
                                    n

               is approximately standard normal. So if, for example, one mechanically uses the
               large-n confidence interval formula

                                        s                       (6.17)
                                x¯ ± z 

                                         n

               with a small sample, there is no way of assessing what actual level of confidence
               should be declared. That is, for small n, using z = 1.96 in formula (6.17) generally
               doesn't produce 95% confidence intervals. And without a further condition, there is
               neither any way to tell what confidence might be associated with z = 1.96 nor any
               way to tell how to choose z in order to produce a 95% confidence level.

                    There is one important special circumstance in which it is possible to reason in
               a way parallel to the work in Sections 6.1 and 6.2 and arrive at inference methods
               for means based on small sample sizes. That is the situation where it is sensible to
               model the observations as iid normal random variables. The normal observations
               case is convenient because although the variable (6.16) is not standard normal, it
               does have a recognized, tabled distribution. This is the Student t distribution.

Definition 13  The (Student) t distribution with degrees of freedom parameter  is a
               continuous probability distribution with probability density

                        +1      1 + t2      -(+1)/2
                           2         
               f (t) =                               for all t  (6.18)
                        
                              

                        2

               If a random variable has the probability density given by formula (6.18), it is
               said to have a t distribution.
                               6.3 One- and Two-Sample Inference for Means 363

                        f (t)       Standard normal

                                        = 11
                                          =5
                                          =2
                                            =1

                        -3 -2 -1 0  1  2        3t

                        Figure 6.9 t Probability densities for  = 1, 2, 5, and
                        11 and the standard normal density

       t distributions  The word Student in Definition 13 was the pen name of the statistician who first
   and the standard     came upon formula (6.18). Expression (6.18) is rather formidable looking. No direct
normal distribution     computations with it will actually be required in this book. But, it is useful to have
                        expression (6.18) available in order to sketch several t probability densities, to get a
                        feel for their shape. Figure 6.9 pictures the t densities for degrees of freedom  = 1,
                        2, 5, and 11, along with the standard normal density.

                             The message carried by Figure 6.9 is that the t probability densities are bell
                        shaped and symmetric about 0. They are flatter than the standard normal density but
                        are increasingly like it as  gets larger. In fact, for most practical purposes, for 
                        larger than about 30, the t distribution with  degrees of freedom and the standard
                        normal distribution are indistinguishable.

                             Probabilities for the t distributions are not typically found using the density in
                        expression (6.18), as no simple antiderivative for f (t) exists. Instead, it is common
                        to use tables (or statistical software) to evaluate common t distribution quantiles
                        and to get at least crude bounds on the types of probabilities needed in significance
                        testing. Table B.4 is a typical table of t quantiles. Across the top of the table
                        are several cumulative probabilities. Down the left side are values of the degrees
                        of freedom parameter, . In the body of the table are corresponding quantiles.
                        Notice also that the last line of the table is a " = " (i.e., standard normal)
                        line.

Example 7               Use of a Table of t Distribution Quantiles

                        Suppose that T is a random variable having a t distribution with  = 5 degrees
                        of freedom. Consider first finding the .95 quantile of T 's distribution, then seeing
                        what Table B.4 reveals about P[T < -1.9] and then about P[|T | > 2.3].
364 Chapter 6 Introduction to Formal Statistical Inference

Example 7          First, looking at the  = 5 row of Table B.4 under the cumulative proba-
(continued )  bility .95, 2.015 is found in the body of the table. That is, Q(.95) = 2.015 or
              (equivalently) P[T  2.015] = .95.

                   Then note that by symmetry,

                          P[T < -1.9] = P[T > 1.9] = 1 - P[T  1.9]

              Looking at the  = 5 row of Table B.4, 1.9 is between the .90 and .95 quantiles
              of the t5 distribution. That is,

                                               .90 < P[T  1.9]  .95

              so finally

                          .05 < P[T < -1.9] < .10

                  Lastly, again by symmetry,

                  P[|T | > 2.3] = P[T < -2.3] + P[T > 2.3] = 2P[T > 2.3]
                                  = 2(1 - P[T  2.3])

              Then, from the  = 5 row of Table B.4, 2.3 is seen to be between the .95 and
              .975 quantiles of the t5 distribution. That is,

                                              .95 < P[T  2.3] < .975

              so

                          .05 < P[|T | > 2.3] < .10

                  The three calculations of this example are pictured in Figure 6.10.
6.3 One- and Two-Sample Inference for Means 365

                                       t5 Distribution

P[T  2.015] = .95

                        -2 -1 0        1
.05 < P[T < -1.9] < .10                                2.015 = Q(.95)

                                       t5 Distribution

-2 -1 0                                1

-1.9 1.476 = Q(.9) 1.9                        2.015 = Q(.95)

.05 < P[|T| > 2.3] < .10                  t5 Distribution

-2 -1                     0            1

                          2.015 = Q(.95) 2.3            2.571 = Q(.975)

Figure 6.10 Three t5 probability calculations for Example 7

     The connection between expressions (6.18) and (6.16) that allows the develop-
ment of small-n inference methods for normal observations is that if an iid normal
model is appropriate,

                          T = x¯ - µ                                     (6.19)
                                   s
                                  
                                    n

has the t distribution with  = n - 1 degrees of freedom. (This is consistent with
the basic fact used in the previous two sections. That is, for large n,  is large, so the
t distribution is approximately standard normal; and for large n, the variable (6.19)
has already been treated as approximately standard normal.)

     Since the variable (6.19) can under appropriate circumstances be treated as a
tn-1 random variable, we are in a position to work in exact analogy to what was
done in Sections 6.1 and 6.2 to find methods for confidence interval estimation and
significance testing. That is, if a data-generating mechanism can be thought of as
366 Chapter 6 Introduction to Formal Statistical Inference

                           essentially equivalent to drawing independent observations from a single normal
                           distribution, a two-sided confidence interval for µ has endpoints

Normal distribution                                                   s    (6.20)
   confidence limits                                          x¯ ± t 
                    for µ
                                                                       n

                           where t is chosen such that the tn-1 distribution assigns probability corresponding
                           to the desired confidence level to the interval between -t and t. Further, the null
                           hypothesis

                                                              H0: µ = #

                           can be tested using the statistic

Normal distribution                                           T = x¯ - #   (6.21)
   test statistic for µ                                                s
                                                                     
                                                                        n

                           and a tn-1 reference distribution.
                                Operationally, the only difference between the inference methods indicated

                           here and the large-sample methods of the previous two sections is the exchange of
                           standard normal quantiles and probabilities for ones corresponding to the tn-1 distri-
                           bution. Conceptually, however, the nominal confidence and significance properties
                           here are practically relevant only under the extra condition of a reasonably normal
                           underlying distribution. Before applying either expression (6.20) or (6.21) in prac-
                           tice, it is advisable to investigate the appropriateness of a normal model assumption.

Example 8                  Small-Sample Confidence Limits for a Mean Spring Lifetime

                           Part of a data set of W. Armstrong (appearing in Analysis of Survival Data by
                           Cox and Oakes) gives numbers of cycles to failure of ten springs of a particular
                           type under a stress of 950 N/mm2. These spring-life observations are given in
                           Table 6.4, in units of 1,000 cycles.

                                                                Table 6.4
                                                                Cycles to Failure of Ten
                                                                Springs under 950 N/mm2
                                                                Stress (103 cycles)

                                                            Spring Lifetimes

                                                            225, 171, 198, 189, 189
                                                            135, 162, 135, 117, 162
Standard normal quantile       6.3 One- and Two-Sample Inference for Means 367

                        2.4

                        1.2

                        0.0

                       -1.2

                               120 140 160 180 200 220
                                           Life quantile (10 3 cycles)

                         Figure 6.11 Normal plot of spring lifetimes

     An important question here might be "What is the average spring lifetime
under conditions of 950 N/mm2 stress?" Since only n = 10 observations are
available, the large-sample method of Section 6.1 is not applicable. Instead,
only the method indicated by expression (6.20) is a possible option. For it to be
appropriate, lifetimes must be normally distributed.

     Without a relevant base of experience in materials, it is difficult to speculate
a priori about the appropriateness of a normal lifetime model in this context. But
at least it is possible to examine the data in Table 6.4 themselves for evidence
of strong departure from normality. Figure 6.11 is a normal plot for the data. It
shows that in fact no such evidence exists.

     For the ten lifetimes, x¯ = 168.3 (× 103 cycles) and s = 33.1 (×103 cycles).
So to estimate the mean spring lifetime, these values may be used in expression
(6.20), along with an appropriately chosen value of t. Using, for example, a 90%
confidence level and a two-sided interval, t should be chosen as the .95 quantile
of the t distribution with  = n - 1 = 9 degrees of freedom. That is, one uses
the t9 distribution and chooses t > 0 such that

                       P[-t < a t9 random variable < t] = .90

Consulting Table B.4, the choice t = 1.833 is in order. So a two-sided 90%
confidence interval for µ has endpoints

                                                      33.1
                                    168.3 ± 1.833 

                                                         10
i.e.,

                                        168.3 ± 19.2
i.e.,

                    149.1 × 103 cycles and 187.5 × 103 cycles
368 Chapter 6 Introduction to Formal Statistical Inference

     What is a       As illustrated in Example 8, normal-plotting the data as a rough check on the
 "nonlinear"    plausibility of an underlying normal distribution is a sound practice, and one that
normal plot?    is used repeatedly in this text. However, it is important not to expect more than
                is justified from the method. It is certainly preferable to use it rather than making
Small sample    an unexamined leap to a possibly inappropriate normal assumption. But it is also
   tests for µ  true that when used with small samples, the method doesn't often provide definitive
                indications as to whether a normal model can be used. Small samples from normal
                distributions will often have only marginally linear-looking normal plots. At the
                same time, small samples from even quite nonnormal distributions can often have
                reasonably linear normal plots. In short, because of sampling variability, small
                samples don't carry much information about underlying distributional shape. About
                all that can be counted on from a small-sample preliminary normal plot, like that in
                Example 8, is a warning in case of gross departure from normality associated with
                an underlying distributional shape that is much heavier in the tails than a normal
                distribution (i.e., producing more extreme values than a normal shape would).

                     It is a good idea to make the effort to (so to speak) calibrate normal-plot
                perceptions if they are going to be used as a tool for checking a model. One way to
                do this is to use simulation and generate a number of samples of the size in question
                from a standard normal distribution and normal-plot these. Then the shape of the
                normal plot of the data in hand can be compared to the simulations to get some
                feeling as to whether any nonlinearity it exhibits is really unusual. To illustrate,
                Figure 6.12 shows normal plots for several simulated samples of size n = 10 from
                the standard normal distribution. Comparing Figures 6.11 and 6.12, it is clear that
                indeed the spring-life data carry no strong indication of nonnormality.

                     Example 8 shows the use of the confidence interval formula (6.20) but not
                the significance testing method (6.21). Since the small-sample method is exactly
                analogous to the large-sample method of Section 6.2 (except for the substitution of
                the t distribution for the standard normal distribution), and the source from which
                the data were taken doesn't indicate any particular value of µ belonging naturally
                in a null hypothesis, the use of the method indicated in expression (6.21) by itself
                will not be illustrated at this point. (There is, however, an application of the testing
                method to paired differences in Example 9.)

6.3.2           Inference for the Mean of Paired Differences

                An important type of application of the foregoing methods of confidence interval
                estimation and significance testing is to paired data. In many engineering problems,
                it is natural to make two measurements of essentially the same kind, but differing
                in timing or physical location, on a single sample of physical objects. The goal
                in such situations is often to investigate the possibility of consistent differences
                between the two measurements. (Review the discussion of paired data terminology
                in Section 1.2.)
      6.3 One- and Two-Sample Inference for Means 369

 1.2                             1.2
 0.0                             0.0
-1.2                            -1.2

         -2.4 -1.2 0.0 1.2 2.4           -2.4 -1.2 0.0 1.2 2.4

 1.2                             1.2
 0.0                             0.0
-1.2                            -1.2

         -2.4 -1.2 0.0 1.2 2.4           -2.4 -1.2 0.0 1.2 2.4

 1.2                             1.2
 0.0                             0.0
-1.2                            -1.2

         -2.4 -1.2 0.0 1.2 2.4           -2.4 -1.2 0.0 1.2 2.4

 1.2                             1.2
 0.0                             0.0
-1.2                            -1.2

         -2.4 -1.2 0.0 1.2 2.4           -2.4 -1.2 0.0 1.2 2.4

1.2                             1.2

0.0                             0.0

-1.2                            -1.2

      -2.4 -1.2 0.0 1.2 2.4           -2.4 -1.2 0.0 1.2 2.4

Figure 6.12 Normal plots of samples of size n = 10 from a standard normal
distribution (data quantiles on the horizontal axes)
370 Chapter 6 Introduction to Formal Statistical Inference

Example 9       Comparing Leading-Edge and Trailing-Edge Measurements
                on a Shaped Wood Product

                Drake, Hones, and Mulholland worked with a company on the monitoring of
                the operation of an end-cut router in the manufacture of a wood product. They
                measured a critical dimension of a number of pieces of a particular type as they
                came off the router. Both a leading-edge and a trailing-edge measurement were
                made on each piece. The design for the piece in question specified that both
                leading-edge and trailing-edge values were to have a target value of .172 in.
                Table 6.5 gives leading- and trailing-edge measurements taken by the students
                on five consecutive pieces.

                Table 6.5
                Leading-Edge and Trailing-Edge Dimensions for Five
                Workpieces

                Piece    Leading-Edge                                        Trailing-Edge
                       Measurement (in.)                                   Measurement (in.)

                1      .168                                                .169

                2      .170                                                .168

                3      .165                                                .168

                4      .165                                                .168

                5      .170                                                .169

                     In this situation, the correspondence between leading- and trailing-edge di-
                mensions was at least as critical to proper fit in a later assembly operation as was
                the conformance of the individual dimensions to the nominal value of .172 in.
                This was thus a paired-data situation, where one issue of concern was the pos-
                sibility of a consistent difference between leading- and trailing-edge dimensions
                that might be traced to a machine misadjustment or unwise method of router
                operation.

                     In situations like Example 9, one simple method of investigating the possibil-
                ity of a consistent difference between paired data is to first reduce the two mea-
                surements on each physical object to a single difference between them. Then the
                methods of confidence interval estimation and significance testing studied thus far
                may be applied to the differences. That is, after reducing paired data to differences
                d1, d2, . . . , dn, if n (the number of data pairs) is large, endpoints of a confidence
                interval for the underlying mean difference, µd , are

Large-sample                                                d¯  ±  z  sdn                     (6.22)
   confidence                                                         

 limits for µd
                           6.3 One- and Two-Sample Inference for Means 371

                           where sd is the sample standard deviation of d1, d2, . . . , dn. Similarly, the null
                           hypothesis

                                                                   H0: µd = #                     (6.23)

                           can be tested using the test statistic

Large-sample                                                       Z = d¯ - #                     (6.24)
  test statistic                                                           sd
          for µd                                                          
                                                                             n

                           and a standard normal reference distribution.
                                If n is small, in order to come up with methods of formal inference, an underlying

                           normal distribution of differences must be plausible. If that is the case, a confidence
                           interval for µd has endpoints

Normal distribution                                                d¯  ±  t  sdn                  (6.25)
   confidence limits                                                         
                   for µd

                           and the null hypothesis (6.23) can be tested using the test statistic

Normal distribution                                                T = d¯ - #                     (6.26)
 test statistic for µd                                                     sd
                                                                          
                                                                             n

                           and a tn-1 reference distribution.

Example 9                  To illustrate this method of paired differences, consider testing the null hypothesis
(continued )               H0: µd = 0 and making a 95% confidence interval for any consistent difference
                           between leading- and trailing-edge dimensions, µd , based on the data in Table
                           6.5.

                                Begin by reducing the n = 5 paired observations in Table 6.5 to differences

                           d = leading-edge dimension - trailing-edge dimension

                           appearing in Table 6.6. Figure 6.13 is a normal plot of the n = 5 differences
                           in Table 6.6. A little experimenting with normal plots of simulated samples of
                           size n = 5 from a normal distribution will convince you that the lack of linear-
                           ity in Figure 6.13 would in no way be atypical of normal data. This, together
                           with the fact that normal distributions are very often appropriate for describ-
372 Chapter 6 Introduction to Formal Statistical Inference

Example 9     Table 6.6
(continued )  Five Differences in Leading- and Trailing-Edge
              Measurements

              Piece d = Difference in Dimensions (in.)

              1 -.001 (= .168 - .169)

              2                               .002 (= .170 - .168)

              3 -.003 (= .165 - .168)

              4 -.003 (= .165 - .168)

              5                               .001 (= .170 - .169)

                                        1.0

              Standard normal quantile  0

                                        -1.0

                                              -.003         0.000   .003

                                              Difference quantile (in.)

                                        Figure 6.13 Normal plot of n = 5
                                        differences

              ing machined dimensions of mass-produced parts, suggests the conclusion that
              the methods represented by expressions (6.25) and (6.26) are in order in this
              example.

                   The differences in Table 6.6 have d¯ = -.0008 in. and sd = .0023 in. So,
              first investigating the plausibility of a "no consistent difference" hypothesis in a
              five-step significance testing format, gives the following:

                   1. H0: µd = 0.
                   2. Ha: µd = 0.

                       (There is a priori no reason to adopt a one-sided alternative hypothesis.)
       6.3 One- and Two-Sample Inference for Means 373

       3. The test statistic will be

                                      T = d¯ - 0
                                              sd
                                             
                                                n

           The reference distribution will be the t distribution with  = n - 1 = 4
           degrees of freedom. Large observed |t| will count as evidence against H0
           and in favor of Ha.

       4. The sample gives

                                      t = -.0008 = -.78
                                            .0023
                                              
                                                5

       5. The observed level of significance is P[|a t4 random variable|  .78],
           which can be seen from Table B.4 to be larger than 2(.10) = .2. The data
           in hand are not convincing in favor of a systematic difference between
           leading- and trailing-edge measurements.

     Consulting Table B.4 for the .975 quantile of the t4 distribution, t = 2.776
is the appropriate multiplier for use in expression (6.25) for 95% confidence.
That is, a two-sided 95% confidence interval for the mean difference between the
leading- and trailing-edge dimensions has endpoints

                                                      .0023
                                  -.0008 ± 2.776 

                                                          5
i.e.,

       -.0008 in. ± .0029 in.                                (6.27)

i.e.,

       -.0037 in. and .0021 in.

     This confidence interval for µd implicitly says (since 0 is in the calculated
interval) that the observed level of significance for testing H0: µd = 0 is more
than .05 (= 1 - .95). Put slightly differently, it is clear from display (6.27) that
the imprecision represented by the plus-or-minus part of the expression is large
enough to make it believable that the perceived difference, d¯ = -.0008, is just a
result of sampling variability.
374 Chapter 6 Introduction to Formal Statistical Inference

   Large-sample        Example 9 treats a small-sample problem. No example for large n is included
inference for µd  here, because after the taking of differences just illustrated, such an example would
                  reduce to a rehash of things in Sections 6.1 and 6.2. In fact, since for large n
                  the t distribution with  = n - 1 degrees of freedom becomes essentially standard
                  normal, one could even imitate Example 9 for large n and get into no logical
                  problems. So at this point, it makes sense to move on from consideration of the
                  paired-difference method.

6.3.3             Large-Sample Comparisons of Two Means
                  (Based on Independent Samples)

                  One of the principles of effective engineering data collection discussed in Section 2.3
                  was comparative study. The idea of paired differences provides inference methods
                  of a very special kind for comparison, where one sample of items in some sense
                  provides its own basis for comparison. Methods that can be used to compare two
                  means where two different "unrelated" samples form the basis of inference are
                  studied next, beginning with large-sample methods.

Example 10        Comparing the Packing Properties of Molded
                  and Crushed Pieces of a Solid

                  A company research effort involved finding a workable geometry for molded
                  pieces of a solid. One comparison made was between the weight of molded
                  pieces of a particular geometry, that could be poured into a standard con-
                  tainer, and the weight of irregularly shaped pieces (obtained through crush-
                  ing), that could be poured into the same container. A series of 24 attempts
                  to pack both molded and crushed pieces of the solid produced the data (in
                  grams) that are given in Figure 6.14 in the form of back-to-back stem-and-leaf
                  diagrams.

                       Notice that although the same number of molded and crushed weights are
                  represented in the figure, there are two distinctly different samples represented.
                  This is in no way comparable to the paired-difference situation treated in Exam-
                  ple 9, and a different method of statistical inference is appropriate.

                       In situations like Example 10, it is useful to adopt subscript notation for both the

                  parameters and the statistics--for example, letting µ1 and µ2 stand for underlying
                  distributional means corresponding to the first and second conditions and x¯ 1 and x¯ 2
                  stand for corresponding sample means. Now if the two data-generating mechanisms

                  are conceptually essentially equivalent to sampling with replacement from two
                  distributions, Section 5.5 says that x¯ 1 has mean µ1 and variance 12/n1, and x¯ 2 has
                  mean µ2 and variance 22/n2.

                       The difference in sample means x¯ 1 - x¯ 2 is a natural statistic to use in comparing
                  µ1 and µ2. Proposition 1 in Chapter 5 (see page 307) implies that if it is reasonable
     6.3 One- and Two-Sample Inference for Means 375

     Molded  Crushed

                                    7.9 11
                          4.5, 3.6, 1.2 12
     9.8, 8.9, 7.9, 7.1, 6.1, 5.7, 5.1 12
                          2.3, 1.3, 0.0 13
               8.0, 7.0, 6.5, 6.3, 6.2 13

                               2.2, 0.1 14
                                           14

                          2.1, 1.2, 0.2 15
                                           15
                                           16 1.8
                                           16 5.8, 9.6
                                           17 1.3, 2.0, 2.4, 3.3, 3.4, 3.7
                                           17 6.6, 9.8
                                           18 0.2, 0.9, 3.3, 3.8, 4.9
                                           18 5.5, 6.5, 7.1, 7.3, 9.1, 9.8
                                           19 0.0, 1.0
                                           19

     Figure 6.14 Back-to-back stem-and-leaf plots of
     packing weights for molded and crushed pieces

to think of the two samples as separately chosen/independent, the random variable
has

     E(x¯ 1 - x¯ 2) = µ1 - µ2

and

                         2 2
     Var(x¯ 1 - x¯ 2) = 1 + 2

                         n1 n2

If, in addition, n1 and n2 are large (so that x¯ 1 and x¯ 2 are each approximately normal),
x¯ 1 - x¯ 2 is approximately normal--i.e.,

     Z = x¯ 1 - x¯ 2 - (µ1 - µ2)                                            (6.28)
                    12 22
                        +
                    n1 n2

has an approximately standard normal probability distribution.
376 Chapter 6 Introduction to Formal Statistical Inference

                          It is possible to begin with the fact that the variable (6.28) is approximately
                     standard normal and end up with confidence interval and significance-testing meth-
                     ods for µ1 - µ2 by using logic exactly parallel to that in the "known- " parts of
                     Sections 6.1 and 6.2. But practically, it is far more useful to begin instead with an
                     expression that is free of the parameters 1 and 2. Happily, for large n1 and n2, not
                     only is the variable (6.28) approximately standard normal but so is

                     Z = x¯ 1 - x¯ 2 - (µ1 - µ2)                     (6.29)
                                    s12 s22
                                        +
                                    n1 n2

                     Then the standard logic of Section 6.1 shows that a two-sided large-sample confi-
                     dence interval for the difference µ1 - µ2 based on two independent samples has
                     endpoints

     Large-sample    x¯ 1 - x¯ 2 ± z                        s12 s22  (6.30)
confidence limits                                              +

        for µ1 - µ2                                         n1 n2

                     where z is chosen such that the probability that the standard normal distribution
                     assigns to the interval between -z and z corresponds to the desired confidence. And
                     the logic of Section 6.2 shows that under the same conditions,

                                                            H0: µ1 - µ2 = #

                     can be tested using the statistic

Large-sample         Z = x¯ 1 - x¯ 2 - #                             (6.31)
  test statistic               s12 s22
   for µ1 - µ2                    +
                              n1 n2

                     and a standard normal reference distribution.

Example 10           In the molding problem, the crushed pieces were a priori expected to pack better
 (continued )        than the molded pieces (that for other purposes are more convenient). Consider
                     testing the statistical significance of the difference in mean weights and also
                     making a 95% one-sided confidence interval for the difference (declaring that the
                     crushed mean weight minus the molded mean weight is at least some number).

                          The sample sizes here (n1 = n2 = 24) are borderline for being called large.
                     It would be preferable to have a few more observations of each type. Lacking
                     them, we will go ahead and use the methods of expressions (6.30) and (6.31) but
               6.3 One- and Two-Sample Inference for Means 377

remain properly cautious of the results should they in any way produce a "close
call" in engineering or business terms.

     Arbitrarily labeling "crushed" condition 1 and "molded" condition 2 and
calculating from the data in Figure 6.14 that x¯ 1 = 179.55 g, s1 = 8.34 g, x¯ 2 =
132.97 g, and s2 = 9.31 g, the five-step testing format produces the following
summary:

1. H0: µ1 - µ2 = 0.
2. Ha: µ1 - µ2 > 0.

    (The research hypothesis here is that the crushed mean exceeds the molded
    mean so that the difference, taken in this order, is positive.)

3. The test statistic is

                                     Z = x¯ 1 - x¯ 2 - 0
                                               s12 s22
                                                   +
                                               n1 n2

    The reference distribution is standard normal, and large observed values
    z will constitute evidence against H0 and in favor of Ha.

4. The samples give

               z = 179.55 - 132.97 - 0 = 18.3

               (8.34)2 + (9.31)2
               24  24

5. The observed level of significance is P[a standard normal variable 
    18.3]  0. The data present overwhelming evidence that µ1 - µ2 > 0--
    i.e., that the mean packed weight of crushed pieces exceeds that of the

    molded pieces.

     Then turning to a one-sided confidence interval for µ1 - µ2, note that only
the lower endpoint given in display (6.30) will be used. So z = 1.645 will be
appropriate. That is, with 95% confidence, we conclude that the difference in
means (crushed minus molded) exceeds

               (179.55 - 132.97) - 1.645 (8.34)2 + (9.31)2
                   24                  24

i.e., exceeds

               46.58 - 4.20 = 42.38 g
378 Chapter 6 Introduction to Formal Statistical Inference

Example 10           Or differently put, a 95% one-sided confidence interval for µ1 - µ2 is
 (continued )                                                 (42.38, )

                          Students are sometimes uneasy about the arbitrary choice involved in labeling
                     the two conditions in a two-sample study. The fact is that either one can be used. As
                     long as a given choice is followed through consistently, the real-world conclusions
                     reached will be completely unaffected by the choice. In Example 10, if the molded
                     condition is labeled number 1 and the crushed condition number 2, an appropriate
                     one-sided confidence for the molded mean minus the crushed mean is

                                                             (-, -42.38)

                     This has the same meaning in practical terms as the interval in the example.
                          The present methods apply where single measurements are made on each ele-

                     ment of two different samples. This stands in contrast to problems of paired data
                     (where there are bivariate observations on a single sample). In the woodworking
                     case of Example 9, the data were paired because both leading-edge and trailing-edge
                     measurements were made on each piece. If leading-edge measurements were taken
                     from one group of items and trailing-edge measurements from another, a two-sample
                     (not a paired difference) analysis would be in order.

              6.3.4  Small-Sample Comparisons of Two Means (Based on
                     Independent Samples from Normal Distributions)
   Graphical check
on the plausibility  The last inference methods presented in this section are those for the difference in
                     two means in cases where at least one of n1 and n2 is small. All of the discussion
       of the model  for this problem is limited to cases where observations are normal. And in fact, the
                     most straightforward methods are for cases where, in addition, the two underlying
                     standard deviations are comparable. The discussion begins with these.

                          A way of making at least a rough check on the plausibility of "normal distribu-
                     tions with a common variance" model assumptions in an application is to normal-plot
                     two samples on the same set of axes, checking not only for approximate linearity
                     but also for approximate equality of slope.

Example 8            The data of W. Armstrong on spring lifetimes (appearing in the book by Cox
(continued )         and Oakes) not only concern spring longevity at a 950 N/mm2 stress level but
                     also longevity at a 900 N/mm2 stress level. Table 6.7 repeats the 950 N/mm2 data
                     from before and gives the lifetimes of ten springs at the 900 N/mm2 stress level

                     as well.
                                     6.3 One- and Two-Sample Inference for Means 379

Table 6.7

Spring Lifetimes under Two Different Levels of Stress
(103 cycles)

950 N/mm2 Stress                          900 N/mm2 Stress

225, 171, 198, 189, 189                   216, 162, 153, 216, 225
135, 162, 135, 117, 162                   216, 306, 225, 243, 189

Standard normal quantile  1.0
                                                                       950 N/mm2 data
                                                                       900 N/mm2 data

                            0

                          -1.0

                                100  200  300

                                     Life-length quantile (10 3 cycles)

Figure 6.15 Normal plots of spring lifetimes under
two different levels of stress

     Figure 6.15 consists of normal plots for the two samples made on a single
set of axes. In light of the kind of variation in linearity and slope exhibited in
Figure 6.12 by the normal plots for samples of this size (n = 10) from a single
normal distribution, there is certainly no strong evidence in Figure 6.15 against
the appropriateness of an "equal variances, normal distributions" model for spring
lifetimes.

     If the assumption that 1 = 2 is used, then the common value is called  , and
it makes sense that both s1 and s2 will approximate  . That suggests that they should
somehow be combined into a single estimate of the basic, baseline variation. As it
turns out, mathematical convenience dictates a particular method of combining or
pooling the individual s's to arrive at a single estimate of  .
380 Chapter 6 Introduction to Formal Statistical Inference

Definition 14  If two numerical samples of respective sizes n1 and n2 produce respective
               sample variances s12 and s22, the pooled sample variance, sP2, is the weighted
               average of s12 and s22 where the weights are the sample sizes minus 1. That is,

               2 (n1 - 1)s12 + (n2 - 1)s22 (n1 - 1)s12 + (n2 - 1)s22
               sP =                                         =                        (6.32)
                     (n1 - 1) + (n2 - 1)                       n1 + n2 - 2

               The pooled sample standard deviation, sP, is the square root of sP2.

               sP is a kind of average of s1 and s2 that is guaranteed to fall between the two
               values s1 and s2. Its exact form is dictated more by considerations of mathematical
               convenience than by obvious intuition.

Example 8      In the spring-life case, making the arbitrary choice to call the 900 N/mm2 stress
(continued )   level condition 1 and the 950 N/mm2 stress level condition 2, s1 = 42.9 (103
               cycles) and s2 = 33.1 (103 cycles). So pooling the two sample variances via
               formula (6.32) produces

               2 (10 - 1)(42.9)2 + (10 - 1)(33.1)2                          3        2
               sP =                                            = 1,468(10 cycles)
                     (10 - 1) + (10 - 1)

               Then, taking the square root,

                     sP = 1,468 = 38.3(103 cycles)

                    In the argument leading to large-sample inference methods for µ1 - µ2, the
               quantity given in expression (6.28),

                     Z = x¯ 1 - x¯ 2 - (µ1 - µ2)
                                    12 22
                                        +
                                    n1 n2

               was briefly considered. In the 1 = 2 =  context, this can be rewritten as

                     Z = x¯ 1 - x¯ 2 - (µ1 - µ2)                                          (6.33)
                                 1+1
                                     n1 n2
                        6.3 One- and Two-Sample Inference for Means 381

                        One could use the fact that expression (6.33) is standard normal to produce methods
                        for confidence interval estimation and significance testing. But for use, these would
                        require the input of the parameter  . So instead of beginning with expression (6.28)
                        or (6.33), it is standard to replace  in expression (6.33) with sP and begin with the
                        quantity

                        T = (x¯ 1 - x¯ 2) - (µ1 - µ2)                      (6.34)

                                                                    11
                                                                sP  +
                                                                    n1 n2

                             Expression (6.34) is crafted exactly so that under the present model assumptions,

                        the variable (6.34) has a well-known, tabled probability distribution: the t distribu-
                        tion with  = (n1 - 1) + (n2 - 1) = n1 + n2 - 2 degrees of freedom. (Notice that
                        the n1 - 1 degrees of freedom associated with the first sample add together with
                        the n2 - 1 degrees of freedom associated with the second to produce n1 + n2 - 2
                        overall.) This probability fact, again via the kind of reasoning developed in Sec-
                        tions 6.1 and 6.2, produces inference methods for µ1 - µ2. That is, a two-sided
                        confidence interval for the difference µ1 - µ2, based on independent samples from
                        normal distributions with a common variance, has endpoints

Normal distributions    x¯ 1 - x¯ 2 ± tsP                           1+1    (6.35)
                                                                    n1 n2
(1 = 2) confidence
    limits for µ1 - µ2

                        where t is chosen such that the probability that the tn1+n2-2 distribution assigns to
                        the interval between -t and t corresponds to the desired confidence. And under the
                        same conditions,

                                                               H0: µ1 - µ2 = #

                        can be tested using the statistic

Normal distributions    T = x¯ 1 - x¯ 2 - #                                (6.36)

          (1 = 2) test                                              11
 statistic for µ1 - µ2                                          sP  +
                                                                    n1 n2

                        and a tn1+n2-2 reference distribution.

Example 8               We return to the spring-life case to illustrate small-sample inference for two
(continued )            means. First consider testing the hypothesis of equal mean lifetimes with an
                        alternative of increased lifetime accompanying a reduction in stress level. Then
382 Chapter 6 Introduction to Formal Statistical Inference

Example 8     consider making a two-sided 95% confidence interval for the difference in mean
(continued )
              lifetimes.
                   Continuing to call the 900 N/mm2 stress level condition 1 and the 950 N/mm2

              stress level condition 2, from Table 6.7 x¯ 1 = 215.1 and x¯ 2 = 168.3, while (from
              before) sP = 38.3. The five-step significance-testing format then gives the fol-
              lowing:

              1. H0: µ1 - µ2 = 0.

              2. Ha: µ1 - µ2 > 0.
                  (The engineering expectation is that condition 1 produces the larger life-
                  times.)

              3. The test statistic is T = x¯ 1 - x¯ 2 - 0

                                                                11
                                                            sP  +
                                                                n1 n2

              The reference distribution is t with 10 + 10 - 2 = 18 degrees of freedom,
              and large observed t will count as evidence against H0.

              4. The samples give

                                   t = 215.1 - 168.3 - 0 = 2.7
                                          38.3 1 + 1
                                                  10 10

              5. The observed level of significance is P[a t18 random variable  2.7],
                  which (according to Table B.4) is between .01 and .005. This is strong
                  evidence that the lower stress level is associated with larger mean spring
                  lifetimes.

                   Then, if the expression (6.35) is used to produce a two-sided 95% confidence
              interval, the choice of t as the .975 quantile of the t18 distribution is in order.
              Endpoints of the confidence interval for µ1 - µ2 are

                                    (215.1 - 168.3) ± 2.101(38.3) 1 + 1
                                                                             10 10

              i.e.,

                                                            46.8 ± 36.0

              i.e.,
                                   10.8 × 103 cycles and 82.8 × 103 cycles
                                                       6.3 One- and Two-Sample Inference for Means 383

                        The data in Table 6.7 provide enough information to establish convincingly that
                        increased stress is associated with reduced mean spring life. But although the
                        apparent size of that reduction when moving from the 900 N/mm2 level (condition
                        1) to the 950 N/mm2 level (condition 2) is 46.8 × 103 cycles, the variability
                        present in the data is large enough (and the sample sizes small enough) that only
                        a precision of ±36.0 × 103 cycles can be attached to the figure 46.8 × 103 cycles.

    Small-sample             There is no completely satisfactory answer to the question of how to do inference
                        for µ1 - µ2 when it is not sensible to assume that 1 = 2. The most widely accepted
    inference for       (but approximate) method for the problem is one due to Satterthwaite that is related to
                        the large-sample formula (6.30). That is, while endpoints (6.30) are not appropriate
µ1 - µ2 without         when n1 or n2 is small (they don't produce actual confidence levels near the nominal
       the 1 = 2        one), a modification of them is appropriate. Let
      assumption

      Satterthwaite's           s12 + s22 2
"estimated degrees              n1 n2
                        ^ =                             (6.37)
         of freedom"         4s1 + 2 s2        4

                             (n1 - 1)n21 (n2 - 1)n2

                        and for a desired confidence level, suppose that t^ is such that the t distribution with
                        ^ degrees of freedom assigns that probability to the interval between -t^ and t^. Then

                        the two endpoints

        Satterthwaite        x¯ 1 - x¯ 2 ± t^  s12 s22  (6.38)
       (approximate)                              +
normal distribution
   confidence limits                           n1 n2

           for µ1 - µ2

                        can serve as confidence limits for µ1 - µ2 with a confidence level approximating
                        the desired one. (One of the two limits (6.38) may be used as a single confidence
                        bound with the two-sided unconfidence level halved.)

Example 8               Armstrong collected spring lifetime data at stress levels besides the 900 and 950
(continued )            N/mm2 levels used thus far in this example. Ten springs tested at 850 N/mm2
                        had lifetimes with x¯ = 348.1 and s = 57.9 (both in 103 cycles) and a reasonably
                        linear normal plot. But taking the 850, 900, and 950 N/mm2 data together, there

                        is a clear trend to smaller and more consistent lifetimes as stress is increased. In
                        light of this fact, should mean lifetimes at the 850 and 950 N/mm2 stress levels

                        be compared, use of a constant variance assumption seems questionable.
384 Chapter 6 Introduction to Formal Statistical Inference

Example 8          Consider then what the Satterthwaite method (6.38) gives for two-sided
(continued )  approximate 95% confidence limits for the difference in 850 and 950 N/mm2

              mean lifetimes. Equation (6.37) gives

                     (57.9)2 (33.1)2 2
                                                            +
                     10                                        10
                     ^ = (57.9)4 (33.1)4 = 14.3
                                                            +
                     9(100) 9(100)

              and (rounding "degrees of freedom" down) the .975 quantile of the t14 distribution
              is 2.145. So the 95% limits (6.38) for the (850 N/mm2 minus 950 N/mm2)
              difference in mean lifetimes (µ850 - µ950) are

                     348.1 - 168.3 ± 2.145 (57.9)2 + (33.1)2
                                                                   10     10

              i.e.,

                                                            179.8 ± 45.2

              i.e.,
                                  134.6 × 103 cycles and 225.0 × 103 cycles

                   The inference methods represented by displays (6.35), (6.36), and (6.38) are
              the last of the standard one- and two-sample methods for means. In the next two
              sections, parallel methods for variances and proportions are considered. But before
              leaving this section to consider those methods, a final comment is appropriate about
              the small-sample methods.

                   This discussion has emphasized that, strictly speaking, the nominal properties
              (in terms of coverage probabilities for confidence intervals and relevant p-value
              declarations for significance tests) of the small-sample methods depend on the
              appropriateness of exactly normal underlying distributions and (in the cases of the
              methods (6.35) and (6.36)) exactly equal variances. On the other hand, when actually
              applying the methods, rather crude probability-plotting checks have been used for
              verifying (only) that the models are roughly plausible. According to conventional
              statistical wisdom, the small-sample methods presented here are remarkably robust
              to all but gross departures from the model assumptions. That is, as long as the model
              assumptions are at least roughly a description of reality, the nominal confidence
              levels and p-values will not be ridiculously incorrect. (For example, a nominally
              90% confidence interval method might in reality be only an 80% method, but it will
              not be only a 20% confidence interval method.) So the kind of plotting that has been
              illustrated here is often taken as adequate precaution against unjustified application
              of the small-sample inference methods for means.
                                                          6.3 One- and Two-Sample Inference for Means 385

Section 3 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. What is the practical consequence of using a "nor-         (a) Make a 90% two-sided confidence interval for
   mal distribution" confidence interval formula when             the mean difference in digital and vernier read-
   in fact the underlying data-generating mechanism               ings for this student.
   cannot be adequately described using a normal dis-
   tribution? Say something more specific/informa-           (b) Assess the strength of the evidence provided
   tive than "an error might be made," or "the interval           by these differences to the effect that there is a
   might not be valid." (What, for example, can be said           systematic difference in the readings produced
   about the real confidence level that ought to be as-           by the two calipers (at least when employed by
   sociated with a nominally 90% confidence interval              this student).
   in such a situation?)
                                                              (c) Briefly discuss why your answers to parts (a)
2. Consider again the situation of Exercise 3 of Sec-             and (b) of this exercise are compatible. (Dis-
   tion 3.1. (It concerns the torques required to loosen          cuss how the outcome of part (b) could easily
   two particular bolts holding an assembly on a piece            have been anticipated from the outcome of part
   of machinery.)                                                 (a).)
    (a) What model assumptions are needed in order
        to do inference for the mean top-bolt torque      4. B. Choi tested the stopping properties of various
        here? Make a plot to investigate the necessary       bike tires on various surfaces. For one thing, he
        distributional assumption.                           tested both treaded and smooth tires on dry con-
   (b) Assess the strength of the evidence in the data       crete. The lengths of skid marks produced in his
        that the mean top-bolt torque differs from a         study under these two conditions were as follows
        target value of 100 ft lb.                           (in cm).
    (c) Make a two-sided 98% confidence interval for
        the mean top-bolt torque.                         Treaded        Smooth
   (d) What model assumptions are needed in order
        to compare top-bolt and bottom-bolt torques       365, 374, 376  341, 348, 349
        here? Make a plot for investigating the neces-    391, 401, 402  355, 375, 391
        sary distributional assumption.
    (e) Assess the strength of the evidence that there    (a) In order to make formal inferences about
        is a mean increase in required torque as one           µTreaded - µSmooth based on these data, what
        moves from the top to the bottom bolts.                must you be willing to use for model assump-
    (f) Give a 98% two-sided confidence interval for           tions? Make a plot to investigate the reason-
        the mean difference in torques between the top         ableness of those assumptions.
        and bottom bolts.
                                                          (b) Proceed under the necessary model assump-
3. The machine screw measurement study of DuToit,              tions to assess the strength of Choi's evidence
   Hansen, and Osborne referred to in Exercise 4 of            of a difference in mean skid lengths.
   Section 6.1 involved measurement of diameters of
   each of 50 screws with both digital and vernier-       (c) Make a 95% two-sided confidence interval for
   scale calipers. For the student referred to in that         µTreaded - µSmooth assuming that treaded and
   exercise, the differences in measured diameters             smooth skid marks have the same variability.
   (digital minus vernier, with units of mm) had the
   following frequency distribution:                      (d) Use the Satterthwaite method and make an ap-
                                                               proximate 95% two-sided confidence interval
                                                               for µTreaded - µSmooth assuming only that skid
                                                               mark lengths for both types of tires are nor-
                                                               mally distributed.

Difference -.03 -.02 -.01 .00 .01 .02

Frequency 1  3  11 19 10 6
386 Chapter 6 Introduction to Formal Statistical Inference

                                                qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

         6.4 One- and Two-Sample Inference for Variances

                                  This text has repeatedly indicated that engineers must often pay close attention to
                                  the measurement, the prediction, and sometimes the physical reduction of variability
                                  associated with a system response. Accordingly, it makes sense to consider inference
                                  for a single variance and inference for comparing two variances. In doing so, two
                                  more standard families of probability distributions--the  2 distributions and the F
                                  distributions--will be introduced.

6.4.1          Inference for the Variance of a Normal Distribution

               The key step in developing most of the formal inference methods discussed in this
               chapter has been to find a random quantity involving both the parameter (or func-
               tion of parameters) of interest and sample-based quantities that under appropriate
               assumptions can be shown to have some well-known distribution. Inference methods
               for a single variance rely on a type of continuous probability distribution that has
               not yet been discussed in this book: the  2 distributions.

Definition 15  The 2 (Chi-squared) distribution with degrees of freedom parameter, ,
               is a continuous probability distribution with probability density

                    1
               
               f (x) = 2/2   x  (/2)-1 -x/2e for x > 0
                                             otherwise
                       2                                (6.39)

                 0

               If a random variable has the probability density given by formula (6.39), it is
               said to have the 2 distribution.

                    Form (6.39) is not terribly inviting, but neither is it unmanageable. For instance,

               it is easy enough to use it to make the kind of plots in Figure 6.16 for comparing the
               shapes of the 2 distributions for various choices of .

                    The 2 distribution has mean  and variance 2. For  = 2, it is exactly the
               exponential distribution with mean 2. For large , the 2 distributions look increas-
               ingly bell-shaped (and can in fact be approximated by normal distributions with
               matching means and variances). Rather than using form (6.39) to find  2 probabil-
               ities, it is more common to use tables of  2 quantiles. Table B.5 is one such table.

               Across the top of the table are several cumulative probabilities. Down the left side

               of the table are values of the degrees of freedom parameter, . In the body of the

               table are corresponding quantiles.
                             6.4 One- and Two-Sample Inference for Variances 387
                        f (x)

                              =1

                         =2

                              =3
                                      =5

                                             =8

                        5                   10   15 x

                        Figure 6.16  2 probability densities for  = 1, 2,
                        3, 5, and 8

       Example 11       Use of a Table of 2 Distribution Quantiles

Using the  2 table,     Suppose that V is a random variable with a 32 distribution. Consider first finding
             Table B.5  the .95 quantile of V 's distribution and then seeing what Table B.5 says about
                        P[V < .4] and P[V > 10.0].

                             First, looking at the  = 3 row of Table B.5 under the cumulative probability
                        .95, one finds 7.815 in the body of the table. That is, Q(.95) = 7.815, or (equiv-
                        alently) P[V  7.815] = .95. Then note that again using the  = 3 line of Table
                        B.5, .4 lies between the .05 and .10 quantiles of the 32 distribution. Thus,

                                                         .05 < P[V < .4] < .10

                        Finally, since 10.0 lies between the ( = 3 line) entries of the table corresponding
                        to cumulative probabilities .975 and .99 (i.e., the .975 and .99 quantiles of the 32
                        distribution), one may reason that

                                                       .01 < P[V > 10.0] < .025

                             The  2 distributions are of interest here because of a probability fact concerning
                        the behavior of the random variable s2 if the observations from which it is calculated

                        are iid normal random variables. Under such assumptions,

                           2 (n - 1)s2
                           X=                                              (6.40)
                                            2
388 Chapter 6 Introduction to Formal Statistical Inference

                           has  a    2      distribution.  This    fact  is  what   is  needed    to  identify  inference  methods

                                   n-1

                           for  .

                                That is, given a desired confidence level concerning  , one can choose  2

                           quantiles  (say,  L  and    U)    such  that  the  probability  that     a    2  random         variable  will

                                                                                                       n-1

                           take a value between L and U corresponds to that confidence level. (Typically, L

                           and  U     are  chosen  to    "split  the  `unconfidence'     between       the  upper    and   lower     2

                                                                                      2                                            n-1

                           tails"--for     example,    using  the  .05   and  .95   n-1  quantiles     for  L   and  U  ,  respectively,

                           if  90%    confidence     is  of  interest.)  Then,      because    the    variable  (6.40)     has  a    2

                                                                                                                                   n-1

                           distribution, the probability that

                                                                            (n - 1)s2                                           (6.41)
                                                                      L < 2 < U

                           corresponds to the desired confidence level. But expression (6.41) is algebraically
                           equivalent to the eventuality that

                                                              (n - 1)s2 <  2 < (n - 1)s2
                                                                   U                           L

                           This then means that when an engineering data-generating mechanism can be

                           thought of as essentially equivalent to random sampling from a normal distribu-
                           tion, a two-sided confidence interval for  2 has endpoints

Normal distribution                                              (n - 1)s2    and       (n - 1)s2                               (6.42)

   confidence limits                                               U                       L
                   for  2

                           where   L  and   U   are  such    that  the     2  probability      assigned     to  the  interval   (L , U )

                                                                         n-1

                           corresponds to the desired confidence.

                                Further, there is an obvious significance-testing method for  2. That is, subject

                           to the same modeling limitations needed to support the confidence interval method,

                                                                         H0:  2 = #

                           can be tested using the statistic

Normal distribution                                                     X 2 = (n - 1)s2                                         (6.43)
 test statistic for  2                                                              #

                           and  a    2     reference   distribution.

                                   n-1

                                One feature of the testing methodology that needs comment concerns the com-

p-values for               puting of p-values in the case that the alternative hypothesis is of the form Ha:
                            2 = #. ( p-values for the one-sided alternative hypotheses Ha:  2 < # and Ha:
      testing              2                                                          2
  H0:  2 = #                    >  #  are,  respectively,    the  left  and  right       tail  areas   beyond   the  observed      value
                                                                                    n-1
                                                     6.4 One- and Two-Sample Inference for Variances 389

                    of X 2.) The fact that the  2 distributions have no point of symmetry leaves some
                    doubt for two-sided significance testing as to how an observed value of X 2 should

                    be translated into a (two-sided) p-value. The convention that will be used here is

                    as  follows.  If  the    observed  value  is    larger   than     the    2    median,  the  (two-sided)

                                                        2                                  n-1

                    p-value    will  be  twice   the  n-1  probability       to  the  right  of  the  observed  value.  If  the

                    observed   value     of  X2  is  smaller  than  the    2      median,    the  (two-sided)   p-value  will

                                       2                                 n-1

                    be  twice  the   n-1     probability  to  the  left  of  the  observed     value.

     Confidence         Knowing that display (6.42) gives endpoints for a confidence interval for  2

        limits for  also leads to confidence intervals for functions of  2. The square roots of the values
functions of  2
                    in display (6.42) give endpoints for a confidence interval for the standard deviation,

                     . And six times the square roots of the values in display (6.42) could be used as

                    endpoints of a confidence interval for the "6 " capability of a process.

Example 12              Inference for the Capability of a CNC Lathe

                        Cowan, Renk, Vander Leest, and Yakes worked with a manufacturer of high-
                        precision metal parts on a project involving a computer numerically controlled
                        (CNC) lathe. A critical dimension of one particular part produced on the lathe
                        had engineering specifications of the form

                                                     Nominal dimension ± .0020 in.

                        An important practical issue in such situations is whether or not the machine is
                        capable of meeting specifications of this type. One way of addressing this is to
                        collect data and do inference for the intrinsic machine short-term variability, rep-
                        resented as a standard deviation. Table 6.8 gives values of the critical dimension
                        measured on 20 parts machined on the lathe in question over a three-hour period.
                        The units are .0001 in. over nominal.

                                                 Table 6.8
                                                 Measurements of a Dimension on 20 Parts
                                                 Machined on a CNC Lathe

                                                  Measured Dimension                  Frequency
                                                 (.0001 in. over nominal)

                                                              8                            1

                                                              9                            1

                                                              10                           10

                                                              11                           4

                                                              12                           3

                                                              13                           1
390 Chapter 6 Introduction to Formal Statistical Inference

Example 12

(continued )             Standard normal quantile  3.0

                                                   1.5

                                                    0.0                                    2
                                                   -1.5
                                                                           2
                                                                           2
                                                                           2
                                                                           2

                                     8.0 9.0 10.0 11.0 12.0 13.0
                                    Measurement quantile (.0001 in. over nominal)

                         Figure 6.17 Normal plot of measurements on 20 parts
                         machined on a CNC lathe

                    Suppose one takes the ±.0020 in. engineering specifications as a statement

              of worst acceptable "±3 " machine capability, accordingly uses the data in Table

              6.8,  and  (since                    .0020    .0007)  tests  H0:   =  .0007.    The  relevance  of  the  methods
                                                     3
              represented by displays (6.42) and (6.43) depends on the appropriateness of a

              normal distribution as a description of the critical dimension (as machined in the

              three-hour period in question). In this regard, note that (after allowing for the

              fact of the obvious discreteness of measurement introduced by gauging read to

              .0001 in.) the normal plot of the data from Table 6.8 shown in Figure 6.17 is

              not distressing in its departure from linearity. Further, at least over periods where

              manufacturing processes like the one in question are physically stable, normal

              distributions often prove to be quite adequate models for measured dimensions

              of mass-produced parts. Other evidence available on the machining process in-

              dicated that for practical purposes, the machining process was stable over the

              three-hour period in question. So one may proceed to use the normal-based

              methods, with no strong reason to doubt their relevance.
                   Direct calculation with the data of Table 6.8 shows that s = 1.1 × 10-4 in.

              So, using the five-step significance-testing format produces the following:

                    1. H0:  = .0007.

                    2. Ha:  > .0007.
                        (The most practical concern is the possibility that the machine is not
                        capable of holding to the stated tolerances, and this is described in terms
                        of  larger than standard.)

                    3. The test statistic is

                                                                           2 (n - 1)s2
                                                                           X=
                                                                                 (.0007)      2
                                  6.4 One- and Two-Sample Inference for Variances 391

                   The reference distribution is  2 with  = (20 - 1) = 19 degrees of free-
                   dom, and large observed values x2 (resulting from large values of s2) will
                   constitute evidence against H0.

               4. The sample gives

                                         2 (20 - 1)(.00011)2
                                         x=                   = .5
                                                  (.0007)  2

               5. The observed level of significance is P[a 192 random variable  .5]. Now
                   .5 is smaller than the .005 quantile of the 192 distribution, so the p-value
                   exceeds .995. There is nothing in the data in hand to indicate that the

                   machine is incapable of holding to the given tolerances.

                    Consider, too, making a one-sided 99% confidence interval of the form
               (0, #) for 3 . According to Table B.5, the .01 quantile of the 192 distribution is
               L = 7.633. So using display (6.42), a 99% upper confidence bound for 3 is

                                  3 (20 - 1)(1.1 × 10-4 in.)2 = 5.0 × 10-4 in.
                                                   7.633

               When this is compared to the ±20 × 10-4 in. engineering requirement, it shows
               that the lathe in question is clearly capable of producing the kind of precision
               specified for the given dimension.

6.4.2          Inference for the Ratio of Two Variances (Based on
               Independent Samples from Normal Distributions)

               To move from inference for a single variance to inference for comparing two vari-
               ances requires the introduction of yet another new family of probability distributions:
               (Snedecor's) F distributions.

Definition 16  The (Snedecor) F distribution with numerator and denominator degrees
               of freedom parameters 1 and 2 is a continuous probability distribution
               with probability density

                                  1 + 2  1 x (1/2)-1 1/2      for x > 0
                                     2   2                    otherwise
                        
               f (x) =         1  2      1 + 1x   (1 +2 )/2              (6.44)
                                               2
                               2  2

                            0
392 Chapter 6 Introduction to Formal Statistical Inference

                                      If a random variable has the probability density given by formula (6.44), it is
                                      said to have the F1,2 distribution.

          Using the F        As Figure 6.18 reveals, the F distributions are strongly right-skewed distribu-
distribution tables,    tions, whose densities achieve their maximum values at arguments somewhat less
                        than 1. Roughly speaking, the smaller the values 1 and 2, the more asymmetric
            Tables B.6  and spread out is the corresponding F distribution.

                             Direct use of formula (6.44) to find probabilities for the F distributions requires
                        numerical integration methods. For purposes of applying the F distributions in
                        statistical inference, the typical path is to instead make use of either statistical
                        software or some fairly abbreviated tables of F distribution quantiles. Tables B.6
                        are tables of F quantiles. The body of a particular one of these tables, for a single p,
                        gives the F distribution p quantiles for various combinations of 1 (the numerator
                        degrees of freedom) and 2 (the denominator degrees of freedom). The values of
                        1 are given across the top margin of the table and the values of 2 down the left
                        margin.

                             Tables B.6 give only p quantiles for p larger than .5. Often F distribution
                        quantiles for p smaller than .5 are needed as well. Rather than making up tables of
                        such values, it is standard practice to instead make use of a computational trick. By
                        using a relationship between F1,2 and F2,1 quantiles, quantiles for small p can
                        be determined. If one lets Q1,2 stand for the F1,2 quantile function and Q2,1
                        stand for the quantile function for the F2,1 distribution,

Relationship between                                1                          (6.45)
                             Q , ( p) =
F , and F ,                  12  Q , (1 - p)
12  21
    quantiles                         21

                        f (x) 1 = 10 2 = 100
                                               1 = 10 2 = 10
                                                     1 = 10 2 = 4
                                                          1 = 4 2 = 4

                        1.0      2.0                                   3.0  x

                        Figure 6.18 Four different F probability densities
                                           6.4 One- and Two-Sample Inference for Variances 393

            Fact (6.45) means that a small lower percentage point of an F distribution may be
            obtained by taking the reciprocal of a corresponding small upper percentage point
            of the F distribution with degrees of freedom reversed.

Example 13  Use of Tables of F Distribution Quantiles

            Suppose that V is an F3,5 random variable. Consider finding the .95 and .01
            quantiles of V 's distribution and then seeing what Tables B.6 reveal about P[V >
            4.0] and P[V < .3].

                 First, a direct look-up in the p = .95 table of quantiles, in the 1 = 3 column
            and 2 = 5 row, produces the number 5.41. That is, Q(.95) = 5.41, or (equiva-
            lently) P[V < 5.41] = .95.

                 To find the p = .01 quantile of the F3,5 distribution, expression (6.45) must
            be used. That is,

                                         1
                     Q3,5(.01) =

                                     Q5,3(.99)

            so that using the 1 = 5 column and 2 = 3 row of the table of F .99 quantiles,
            one has

                     Q3,5(.01) =  1      = .04

                                  28.24

                 Next, considering P[V > 4.0], one finds (using the 1 = 3 columns and
            2 = 5 rows of Tables B.6) that 4.0 lies between the .90 and .95 quantiles of the
            F3,5 distribution. That is,

                     .90 < P[V  4.0] < .95

            so that

                     .05 < P[V > 4.0] < .10

                 Finally, considering P[V < .3], note that none of the entries in Tables B.6 is
            less than 1.00. So to place the value .3 in the F3,5 distribution, one must locate its
            reciprocal, 3.33(= 1/.3), in the F5,3 distribution and then make use of expression
            (6.45). Using the 1 = 5 columns and 2 = 3 rows of Tables B.6, one finds that
            3.33 is between the .75 and .90 quantiles of the F5,3 distribution. So by expression
            (6.45), .3 is between the .1 and .25 quantiles of the F3,5 distribution, and

                                             .10 < P[V < .3] < .25
394 Chapter 6 Introduction to Formal Statistical Inference

                                  The extra effort required to find small F distribution quantiles is an artifact
                             of standard table-making practice, rather than being any intrinsic extra difficulty
                             associated with the F distributions. One way to eliminate the difficulty entirely is
                             to use standard statistical software or a statistical calculator to find F quantiles.

                                  The F distributions are of use here because a probability fact ties the behavior of
                             ratios of independent sample variances based on samples from normal distributions
                             to the variances 12 and 22 of those underlying distributions. That is, when s12 and
                             s22 come from independent samples from normal distributions, the variable

                                                                   s12 22                             (6.46)
                                                             F= 2· 2

                                                                   1 s2

                             has an Fn -1,n -1 distribution. (s12 has n1 - 1 associated degrees of freedom and12

                             is in the numerator of this expression, while s22 has n2 - 1 associated degrees of
                             freedom and is in the denominator, providing motivation for the language introduced

                             in Definition 16.)

                                  This fact is exactly what is needed to produce formal inference methods for
                             the ratio 12/22. For example, it is possible to pick appropriate F quantiles L
                             and U such that the probability that the variable (6.46) falls between L and U

                             corresponds to a desired confidence level. (Typically, L and U are chosen to "split

                             the `unconfidence' " between the upper and lower Fn1-1,n2-1 tails.) But

                                                                   s12 22
                                                             L < 2 · 2 <U

                                                                   1 s2

                             is algebraically equivalent to

                                                                 1 s21  21 1 s21
                                                                    · 2< 2< · 2

                                                                 U s2 2 L s2

                             That is, when a data-generating mechanism can be thought of as essentially equiv-
                             alent to independent random sampling from two normal distributions, a two-sided
                             confidence interval for  21 / 22 has endpoints

Normal distributions                                           s12             s12                    (6.47)
                                                             U · s22  and
     confidence limits
               for  21 / 22                                                  L · s22

                             where L and U are (Fn1-1,n2-1 quantiles) such that the Fn1-1,n2-1 probability as-
                             signed to the interval (L , U ) corresponds to the desired confidence.
                 6.4 One- and Two-Sample Inference for Variances 395

                      In addition, there is an obvious significance-testing method for 12/22. That
                 is, subject to the same modeling limitations as needed to support the confidence

                 interval method,

                                                         2                                 (6.48)
                                                    H0: 21 = #

                                                         2

                 can be tested using the statistic

       Normal                                       F = s12/s22                            (6.49)
distributions                                                #
 test statistic

    for 12/22

                 and an Fn1-1,n2-1 reference distribution. (The choice of # = 1 in displays (6.48)
                 and (6.49), so that the null hypothesis is one of equality of variances, is the only

p-values for     one commonly used in practice.) p-values for the one-sided alternative hypotheses

testing          Ha: 12/22 < # and Ha: 12/22 > # are (respectively) the left and right Fn -1,n -1 tail

        21                                                                        1     2
                 areas beyond the observed values of the test statistic. For the two-sided alternative
H0: 2        =#
                 hypothesis Ha:  21 / 22 = #, the standard convention is to report twice the Fn -1,n -1
       
          2

                                                                                     1     2
                 probability to the right of the observed f if f > 1 and to report twice the Fn1-1,n2-1
                 probability to the left of the observed f if f < 1.

Example 14       Comparing Uniformity of Hardness Test Results for Two Types of Steel

                 Condon, Smith, and Woodford did some hardness testing on specimens of 4%
                 carbon steel. Part of their data are given in Table 6.9, where Rockwell hardness
                 measurements for ten specimens from a lot of heat-treated steel specimens and
                 five specimens from a lot of cold-rolled steel specimens are represented.

                      Consider comparing measured hardness uniformity for these two steel types
                 (rather than mean hardness, as might have been done in Section 6.3). Figure 6.19
                 shows side-by-side dot diagrams for the two samples and suggests that there
                 is a larger variability associated with the heat-treated specimens than with the
                 cold-rolled specimens. The two normal plots in Figure 6.20 indicate no obvious
                 problems with a model assumption of normal underlying distributions.

                 Table 6.9
                 Rockwell Hardness Measurements for Steel Specimens
                 of Two Types

                 Heat-Treated                       Cold-Rolled

                 32.8, 44.9, 34.4, 37.0, 23.6,      21.0, 24.5, 19.9, 14.8, 18.8
                 29.1, 39.5, 30.1, 29.2, 19.2
396 Chapter 6 Introduction to Formal Statistical Inference

Example 14                                 Heat-treated
 (continued )                                                                           Rockwell hardness

               10 15 20 25 30 35 40 45

                                            Cold-rolled

                                                                                        Rockwell hardness
               10 15 20 25 30 35 40 45
               Figure 6.19 Dot diagrams of hardness for heat-treated and cold-rolled
               steels

               Standard normal quantile 2.4
                                        1.2
                                        0.0Standard normal quantile
                                       -1.2

                                                    16.0 24.0 32.0 40.0 48.0
                                                        Heat-treated hardness quantile

                                        2.4
                                        1.2
                                        0.0
                                       -1.2

                                                    16.0 24.0 32.0 40.0 48.0
                                                        Cold-rolled hardness quantile

                                         Figure 6.20 Normal plots of hardness for
                                         heat-treated and cold-rolled steels

                    Then, arbitrarily choosing to call the heat-treated condition number 1 and
               the cold-rolled condition 2, s1 = 7.52 and s2 = 3.52, and a five-step significance
               test of equality of variances based on the variable (6.49) proceeds as follows:

                             2
                    1. H0: 12 = 1.

                             2
          6.4 One- and Two-Sample Inference for Variances 397

         2
2. Ha: 12 = 1.

         2
    (If there is any materials-related reason to pick a one-sided alternative
    hypothesis here, the authors don't know it.)

3. The test statistic is

                                                 s12
                                           F= 2

                                                 s2

    The reference distribution is the F9,4 distribution, and both large observed
    f and small observed f will constitute evidence against H0.
4. The samples give

          f=              (7.52)2
                                2 = 4.6
                          (3.52)

5. Since the observed f is larger than 1, for the two-sided alternative, the
    p-value is

                                2P[an F9,4 random variable  4.6]

          From Tables B.6, 4.6 is between the F9,4 distribution .9 and .95 quantiles,
          so the observed level of significance is between .1 and .2. This makes
          it moderately (but not completely) implausible that the heat-treated and
          cold-rolled variabilities are the same.

     In an effort to pin down the relative sizes of the heat-treated and cold-rolled
hardness variabilities, the square roots of the expressions in display (6.47) may be
used to give a 90% two-sided confidence interval for 1/2. Now the .95 quantile
of the F9,4 distribution is 6.0, while the .95 quantile of the F4,9 distribution is
3.63, implying that the .05 quantile of the F9,4 distribution is 3.63 1 . Thus, a 90%
confidence interval for the ratio of standard deviations 1/2 has endpoints

           (7.52)2             (7.52)2
          6.0(3.52)2 and  (1/3.63)(3.52)2

That is,

          .87 and 4.07
398 Chapter 6 Introduction to Formal Statistical Inference

Example 14        The fact that the interval (.87, 4.07) covers values both smaller and larger than 1
 (continued )     indicates that the data in hand do not provide definitive evidence even as to which
                  of the two variabilities in material hardness is larger.

                       One of the most important engineering applications of the inference methods
                  represented by displays (6.47) through (6.49) is in the comparison of inherent
                  precisions for different pieces of equipment and for different methods of operating
                  a single piece of equipment.

Example 15        Comparing Uniformities of Operation of Two Ream Cutters

                  Abassi, Afinson, Shezad, and Yeo worked with a company that cuts rolls of paper
                  into sheets. The uniformity of the sheet lengths is important, because the better
                  the uniformity, the closer the average sheet length can be set to the nominal value
                  without producing undersized sheets, thereby reducing the company's giveaway
                  costs. The students compared the uniformity of sheets cut on a ream cutter
                  having a manual brake to the uniformity of sheets cut on a ream cutter that had an
                  automatic brake. The basis of that comparison was estimated standard deviations
                  of sheet lengths cut by the two machines--just the kind of information used to
                  frame formal inferences in this section. The students estimated manual/automatic
                  to be on the order of 1.5 and predicted a period of two years or less for the
                  recovery of the capital improvement cost of equipping all the company's ream
                  cutters with automatic brakes.

Caveats about          The methods of this section are, strictly speaking, normal distribution methods.
inferences for    It is worthwhile to ask, "How essential is this normal distribution restriction to the
                  predictable behavior of these inference methods for one and two variances?" There
       variances  is a remark at the end of Section 6.3 to the effect that the methods presented there for
                  means are fairly robust to moderate violation of the section's model assumptions.
                  Unfortunately, such is not the case for the methods for variances presented here.

                       These are methods whose nominal confidence levels and p-values can be fairly
                  badly misleading unless the normal models are good ones. This makes the kind of
                  careful data scrutiny that has been implemented in the examples (in the form of
                  normal-plotting) essential to the responsible use of the methods of this section. And
                  it suggests that since normal-plotting itself isn't typically terribly revealing unless
                  the sample size involved is moderate to large, formal inferences for variances will
                  be most safely made on the basis of moderate to large normal-looking samples.

                       The importance of the "normal distribution(s)" restriction to the predictable
                  operation of the methods of this section is not the only reason to prefer large sample
                  sizes for inferences on variances. A little experience with the formulas in this section
                  will convince the reader that (even granting the appropriateness of normal models)
                  small samples often do not prove adequate to answer practical questions about
                  variances.  2 and F confidence intervals for variances and variance ratios based on
                            6.5 One- and Two-Sample Inference for Proportions 399

small samples can be so big as to be of little practical value, and the engineer will
typically be driven to large sample sizes in order to solve variance-related real-world
problems. This is not in any way a failing of the present methods. It is simply a
warning and quantification of the fact that learning about variances requires more
data than (for example) learning about means.

Section 4 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Return to data on Choi's bicycle stopping distance             treaded and smooth tires produce normally dis-
   given in Exercise 4 of Section 6.3.                            tributed stopping distances, give a 90% two-
    (a) Operating under the assumption that treaded               sided confidence interval for the ratio Treaded/
        tires produce normally distributed stopping               Smooth.
        distances, give a two-sided 95% confidence        2. Consider again the situation of Exercise 3 of Sec-
        interval for the standard deviation of treaded       tion 3.1 and Exercise 2 of Section 6.3. (It concerns
        tire stopping distances.                             the torques required to loosen two particular bolts
   (b) Operating under the assumption that smooth            holding an assembly on a piece of machinery.)
        tires produce normally distributed stopping           (a) Operating under the assumption that top-bolt
        distances, give a 99% upper confidence bound              torques are normally distributed, give a 95%
        for the standard deviation of smooth tire stop-           lower confidence bound for the standard devi-
        ping distances.                                           ation of the top-bolt torques.
    (c) Operating under the assumption that both             (b) Translate your answer to part (a) into a 95%
        treaded and smooth tires produce normally dis-            lower confidence bound on the "6 process
        tributed stopping distances, assess the strength          capability" of the top-bolt tightening process.
        of Choi's evidence that treaded and smooth            (c) It is not appropriate to use the methods (6.47)
        stopping distances differ in their variability.           through (6.49) and the data given in Exercise
        (Use H0: Treaded = Smooth and Ha: Treaded =               3 of Section 3.1 to compare the consistency of
        Smooth and show the whole five-step format.)              top-bolt and bottom-bolt torques. Why?
   (d) Operating under the assumption that both

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

6.5 One- and Two-Sample Inference
       for Proportions

                    The methods of formal statistical inference in the previous four sections are useful in
                    the analysis of quantitative data. Occasionally, however, engineering studies produce
                    only qualitative data, and one is faced with the problem of making properly hedged
                    inferences from such data. This section considers how the sample fraction p^ (defined
                    in Section 3.4) can be used as the basis for formal statistical inferences. It begins
                    with the use of p^ from a single sample to make formal inferences about a single
                    system or population. The section then treats the use of sample proportions from
                    two samples to make inferences comparing two systems or populations.
400 Chapter 6 Introduction to Formal Statistical Inference

6.5.1                Inference for a Single Proportion

                     Recall from display (3.6) (page 104) that the notation p^ is used for the fraction
                     of a sample that possesses a characteristic of engineering interest. A sample of
                     pellets produced by a pelletizing machine might prove individually conforming or
                     nonconforming, and p^ could be the sample fraction conforming. Or in another case,
                     a sample of turned steel shafts might individually prove acceptable, reworkable, or
                     scrap; p^ could be the sample fraction reworkable.

                          If formal statistical inferences are to be based on p^ , one must think of the
                     physical situation in such a way that p^ is related to some parameter characterizing it.
                     Accordingly, this section considers scenarios where p^ is derived from an independent
                     identical success/failure trials data-generating mechanism. (See again Section 5.1.4
                     to review this terminology.) Applications will include inferences about physically
                     stable processes, where p is a system's propensity to produce an item with the
                     characteristic of interest. And they will include inferences drawn about population
                     proportions p in enumerative contexts involving large populations. For example,
                     the methods of this section can be used both to make inferences about the routine
                     operation of a physically stable pelletizing machine and also to make inferences
                     about the fraction of nonconforming machine parts contained in a specific lot of
                     10,000 such parts.

                          Review of the material on independent success/failure trials (and particularly
                     the binomial distributions) in Section 5.1.4 should convince the reader that

                       X = n p^ = the number of items in the sample with the characteristic of interest

                     has the binomial (n, p) distribution. The sample fraction p^ is just a scale change
                     away from X = n p^ , so facts about the distribution of X have immediate counterparts
                     regarding the distribution of p^ . For example, Section 5.1.4 stated that the mean and
                     variance for the binomial (n, p) distribution are (respectively) np and np(1 - p).
                     This (together with Proposition 1 in Chapter 5) implies that p^ has

        Mean of the                     Ep^ = E X = 1 EX = 1 · np = p                    (6.50)
sample proportion                                nn                  n

                     and

    Variance of the                     X 12                         np(1 - p) p(1 - p)
sample proportion         Var p^ = Var     =                Var X =     2=               (6.51)
                                        n     n                      n      n

                     Equations (6.50) and (6.51) provide a reassuring picture of the behavior of the statis-
                     tic p^ . They show that the probability distribution of p^ is centered at the underlying
                     parameter p, with a variability that decreases as n increases.
                         6.5 One- and Two-Sample Inference for Proportions 401

             Example 16  Means and Standard Deviations of Sample Fractions
(Example 3, Chapter 5,   of Reworkable Shafts

  revisited--page 234)   Return again to the case of the performance of a process for turning steel shafts.
                         Assume for the time being that the process is physically stable and that the
                         likelihood that a given shaft is reworkable is p = .20. Consider p^ , the sample
                         fraction of reworkable shafts in samples of first n = 4 and then n = 100 shafts.

                              Expressions (6.50) and (6.51) show that for the n = 4 sample size,

                         Ep^ = p = .2

                         Var p^ =  p(1 - p) =                     (.2)(.8) = .2
                                       n                             4

                         Similarly, for the n = 100 sample size,

                                                              Ep^ = p = .2

                                                           Var p^ = (.2)(.8) = .04
                                                                           100

                         Comparing the two standard deviations, it is clear that the effect of a change
                         in sample size from n = 4 to n = 100 is to produce a factor of 5 (= 100/4)
                         decrease in the standard deviation of p^ , while the distribution of p^ is centered at
                         p for both sample sizes.

       Approximate            The basic new insight needed to provide large-sample inference methods based
   normality of the      on p^ is the fact that for large n, the binomial (n, p) distribution (and therefore also
sample proportion        the distribution of p^ ) is approximately normal. That is, for large n, approximate
                         probabilities for X = n p^ (or p^ ) can be found using the normal distribution with
                         mean µ = np (or µ = p) and variance  2 = np(1 - p) (or  2 = n p(1-p) ).

Example 16               In the shaft-turning example, consider the probability that for a sample of n = 100
 (continued )            shafts, p^  .25. Notice that p^  .25 is equivalent here to the eventuality that
                         n p^  25. So in theory the form of the binomial probability function given in
                         Definition 9 of Chapter 5 could be used and the desired probability could be
                         evaluated exactly as

                              P[ p^  .25] = P[X  25] = f (25) + f (26) + · · · + f (99) + f (100)

                         But instead of making such laborious calculations, it is common (and typically
                         adequate for practical purposes) to settle instead for a normal approximation to
                         probabilities such as this one.
402 Chapter 6 Introduction to Formal Statistical Inference

Example 16                                                  For n = 100, the approximate
 (continued )                                               distribution of p is normal with
                                                            mean .2 and standard deviation .04

                                                                      Approximate
                                                                      probability that
                                                                      p  .25

                              .15  .2                       .25

                              Figure 6.21 Approximate probability distribution
                              for p^

                               Figure 6.21 shows the normal distribution with mean µ = p = .2 and stan-
                          dard deviation  = p(1 - p)/n = .04 and the corresponding probability as-
                          signed to the interval [.25, ). Conversion of .25 to a z-value and then an

                          approximate probability proceeds as follows:

                              z = .25 - E p^ = .25 - .2 = 1.25
                                   Var p^                        .04

                          so

                              P[ p^  .25]  1 - (1.25) = .1056  .11

                          The exact value of P[ p^  .25] (calculated to four decimal places using the
                          binomial probability function) is .1314. (This can, for example, be obtained
                          using the MINITAB routine under the "Calc/Probability Distributions/Binomial"
                          menu.)

                               The statement that for large n, the random variable p^ is approximately normal
                          is actually a version of the central limit theorem. For a given n, the approximation
                          is best for moderate p (i.e., p near .5), and a common rule of thumb is to require
                          that both the expected number of successes and the expected number of failures
                          be at least 5 before making use of a normal approximation to the binomial (n, p)
                          distribution. This is a requirement that

                              np  5 and n(1 - p)  5

      Conditions for the  which amounts to a requirement that                                   (6.52)
normal approximation                                              5  np  n - 5

         to the binomial  (Notice that in Example 16, np = 100(.2) = 20 and 5  20  95.)
                           6.5 One- and Two-Sample Inference for Proportions 403

                                An alternative, and typically somewhat stricter rule of thumb (which comes
                           from a requirement that the mean of the binomial distribution be at least 3 standard
                           deviations from both 0 and n) is to require that

           Another set of                                     9  (n + 9) p  n       (6.53)
      conditions for the
normal approximation       before using the normal approximation. (Again in Example 16, (n + 9) p = (100 +
                           9)(.2) = 21.8 and 9 21.8  100.)
         to the binomial
                                The approximate normality of p^ for large n implies that for large n,

                                                              Z = p^ - p            (6.54)
                                                                       p(1 - p)
                                                                            n

                           is approximately standard normal. This and the reasoning of Section 6.2 then imply
                           that the null hypothesis

                                                              H0: p = #

                           can be tested using the statistic

Large-sample                                                  Z = p^ - #            (6.55)
  test statistic                                                       #(1 - #)
           for p                                                           n

                           and a standard normal reference distribution. Further, reasoning parallel to that
                           in Section 6.1 (beginning with the fact that the variable (6.54) is approximately
                           standard normal), leads to the conclusion that an interval with endpoints

                                                              p^ ± z p(1 - p)       (6.56)
                                                                            n

                           (where z is chosen such that the standard normal probability between -z and z
                           corresponds to a desired confidence) is a mathematically valid two-sided confidence

                           interval for p.

                                However, the endpoints indicated by expression (6.54) are of no practical use

                           as they stand, since they involve the unknown parameter p. There are two standard

                           ways of remedying this situation. One draws its motivation from the simple plot
                           of p(1 - p) shown in Figure 6.22. That is, from Figure 6.22 it is easy to see that
                           p(1 - p)  (.5)2 = .25, so the plus-or-minus part of formula (6.56) has (for z > 0)

                                                              p(1 - p)           1
                           z                                            z 
                                                              n          2n
404 Chapter 6 Introduction to Formal Statistical Inference
                                                                              p (1 - p)

                        .20
                        .10

                                                                                         .5          1.0 p

                        Figure 6.22 Plot of p(1 - p) versus p

                        Thus, modifying the endpoints in formula (6.56) by replacing the plus-or-minus part
                        with ±z/2 n produces an interval that is guaranteed to be as wide as necessary to
                        give the desired approximate confidence level. That is, the interval with endpoints

     Large-sample                                                                                 1         (6.57)
      conservative                                                                       p^ ± z 
confidence limits
                                                                                                2n
                 for p

                        where z is chosen such that the standard normal probability between -z and z
                        corresponds to a desired confidence, is a practically usable large-n, two-sided,
                        conservative confidence interval for p. (Appropriate use of only one of the endpoints
                        in display (6.57) gives a one-sided confidence interval.)

                             The other common method of dealing with the fact that the endpoints in formula
                        (6.56) are of no practical use is to begin the search for a formula from a point other
                        than the approximate standard normal distribution of the variable (6.54). For large
                        n, not only is the variable (6.54) approximately standard normal, but so is

                             Z = p^ - p                                                                     (6.58)
                                      p^ (1 - p^ )
                                           n

                        And the denominator of the quantity (6.58) (which amounts to an estimated standard
                        deviation for p^ ) is free of the parameter p. So when manipulations parallel to those
                        in Section 6.1 are applied to expression (6.58), the conclusion is that the interval
                        with endpoints

     Large-sample            p^ ± z p^ (1 - p^ )                                                            (6.59)
confidence limits                          n

                 for p

                        can be used as a two-sided, large-n confidence interval for p with confidence level
                        corresponding to the standard normal probability assigned to the interval between
                        -z and z. (One-sided confidence limits are obtained in the usual way, using only
                        one of the endpoints in display (6.59) and appropriately adjusting the confidence
                        level.)
                                    6.5 One- and Two-Sample Inference for Proportions 405

Example 17  Inference for the Fraction of Dry Cells with Internal Shorts

            The article "A Case Study of the Use of an Experimental Design in Preventing

            Shorts in Nickel-Cadmium Cells" by Ophir, El-Gad, and Snyder (Journal of

            Quality Technology, 1988) describes a series of experiments conducted to find

            how to reduce the proportion of cells scrapped by a battery plant because of

            internal shorts. At the beginning of the study, about 6% of the cells produced

            were being scrapped because of internal shorts.

                   Among a sample of 235 cells made under a particular trial set of plant

            operating conditions, 9 cells had shorts. Consider what formal inferences can be

            drawn  about  the  set  of  operating  conditions  based  on  such  data.  p^  =   9   =  .038,
                                                                                              235
            so two-sided 95% confidence limits for p, are by expression (6.59)

                                         .038 ± 1.96 (.038)(1 - .038)
                                                                  235

            i.e.,

                                                   .038 ± .025

            i.e.,

                                             .013 and .063                                         (6.60)

            Notice that according to display (6.60), although p^ = .038 < .06 (and thus indi-
            cates that the trial conditions were an improvement over the standard ones), the
            case for this is not airtight. The data in hand allow some possibility that p for the
            trial conditions even exceeds .06. And the ambiguity is further emphasized if the
            conservative formula (6.57) is used in place of expression (6.59). Instead of 95%
            confidence endpoints of .038 ± .025, formula (6.57) gives endpoints .038 ± .064.

                 To illustrate the significance-testing method represented by expression (6.55),
            consider testing with an alternative hypothesis that the trial plant conditions are
            an improvement over the standard ones. One then has the following summary:

                   1. H0: p = .06.
                   2. Ha: p < .06.
                   3. The test statistic is

                                                   Z = p^ - .06
                                                            (.06)(1 - .06)

                                                                    n

                   The reference distribution is standard normal, and small observed values
                   z will count as evidence against H0.
406 Chapter 6 Introduction to Formal Statistical Inference

Example 17         4. The sample gives
 (continued )
                                               z = .038 - .06 = -1.42
                                                        (.06)(1 - .06)
                                                              235

                   5. The observed level of significance is then

                                                            (-1.42) = .08

                   This is strong but not overwhelming evidence that the trial plant conditions
                   are an improvement on the standard ones.

                        It needs to be emphasized again that these inferences depend for their practi-
                   cal relevance on the appropriateness of the "stable process/independent, identical
                   trials" model for the battery-making process and extend only as far as that de-
                   scription continues to make sense. It is important that the experience reported in
                   the article was gained under (presumably physically stable) regular production,
                   so there is reason to hope that a single "independent, identical trials" model can
                   describe both experimental and future process behavior.

      Sample size       Section 6.1 illustrated the fact that the form of the large-n confidence interval
  determination    for a mean can be used to guide sample-size choices for estimating µ. The same is
for estimating p   true regarding the estimation of p. If one (1) has in mind a desired confidence level,
                   (2) plans to use expression (6.57) or has in mind a worst-case (largest) expectation
                   for p^ (1 - p^ ) in expression (6.59), and (3) has a desired precision of estimation of
                   p, it is a simple matter to solve for a corresponding sample size. That is, suppose
                   that the desired confidence level dictates the use of the value z in formula (6.57) and
                   one wants to have confidence limits (or a limit) of the form p^ ± . Setting

                                                                   1
                                                            =z 

                                                                2n

                   and solving for n produces the requirement

                                                            n= z 2
                                                                   2

Example 17         Return to the nicad battery case and suppose that for some reason a better fix on
 (continued )      the implications of the new operating conditions was desired. In fact, suppose
                   that p is to be estimated with a two-sided conservative 95% confidence interval,
                   and ±.01 (fraction defective) precision of estimation is desired. Then, using the
                                             6.5 One- and Two-Sample Inference for Proportions 407

                     plus-or-minus part of expression (6.57) (or equivalently, the plus-or-minus part
                     of expression (6.59) under the worst-case scenario that p^ = .5), one is led to set

                                                                           1
                                                           .01 = 1.96 

                                                                         2n

                     From this, a sample size of

                                                               n  9,604

                     is required.
                          In most engineering contexts this sample size is impractically large. Rethink-

                     ing the calculation by planning the use of expression (6.59) and adopting the point
                     of view that, say, 10% is a worst-case expectation for p^ (and thus .1(1 - .1) = .09
                     is a worst-case expectation for p^ (1 - p^ )), one might be led instead to set

                                                      .01 = 1.96 (.1)(1 - .1)
                                                                            n

                     However, solving for n, one has

                                                               n  3,458

                     which is still beyond what is typically practical.
                          The moral of these calculations is that something has to give. The kind of large

                     confidence and somewhat precise estimation requirements set at the beginning
                     here cannot typically be simultaneously satisfied using a realistic sample size.
                     One or the other of the requirements must be relaxed.

Cautions concerning       The sample-size conclusions just illustrated are typical, and they justify two
 inference based on  important points about the use of qualitative data. First, qualitative data carry less
 sample proportions  information than corresponding numbers of quantitative data (and therefore usually
                     require very large samples to produce definitive inferences). This makes measure-
                     ments generally preferable to qualitative observations in engineering applications.
                     Second, if inferences about p based on even large values of n are often disappoint-
                     ing in their precision or reliability, there is little practical motivation to consider
                     small-sample inference for p in a beginning text like this.

6.5.2                Inference for the Difference Between Two Proportions
                     (Based on Independent Samples)

                     Two separately derived sample proportions p^ 1 and p^ 2, representing different pro-
                     cesses or populations, can enable formal comparison of those processes or pop-
                     ulations. The logic behind those methods of inference concerns the difference
                     p^ 1 - p^ 2. If
408 Chapter 6 Introduction to Formal Statistical Inference

                             1. the "independent, identical success-failure trials" description applies sepa-
                                 rately to the mechanisms that generate two samples,

                             2. the two samples are reasonably described as independent, and

                             3. both n1 and n2 are large,

                        a very simple approximate description of the distribution of p^ 1 - p^ 2 results.
                             Assuming p^ 1 and p^ 2 are independent, Proposition 1 in Chapter 5 and the

                        discussion in this section concerning the mean and variance of a single sample
                        proportion imply that p^ 1 - p^ 2 has

            Mean of a            E( p^ 1 - p^ 2) = E p^ 1 + (-1)E p^ 2 = p1 - p2                   (6.61)
         difference in
sample proportions

                        and

        Variance of a   Var( p^ 1- p^ 2) = (1)2 Var p^ 1+(-1)2 Var p^ 2 = p1(1 - p1) + p2(1 - p2)  (6.62)
         difference in                                               n1                 n2
sample proportions

Approximate             Then the approximate normality of p^ 1 and p^ 2 for large sample sizes turns out to
 normality of           imply the approximate normality of the difference p^ 1 - p^ 2.

        p^ 1 - p^ 2

Example 16              Consider again the turning of steel shafts, and imagine that two different, physi-
 (continued )
                        cally stable lathes produce reworkable shafts at respective rates of 20 and 25%.
                        Then suppose that samples of (respectively) n1 = 50 and n2 = 50 shafts pro-
                        duced by the machines are taken, and the reworkable sample fractions p^ 1 and
                        p^ 2 are found. Consider approximating the probability that p^ 1  p^ 2 (i.e., that
                        p^ 1 - p^ 2  0).

                             Using expressions (6.61) and (6.62), the variable p^ 1 - p^ 2 has

                                                    E( p^ 1 - p^ 2) = .20 - .25 = -.05
                        and

                             Var( p^ 1 - p^ 2) =  (.20)(1 - .20) + (.25)(1 - .25) = .00695 = .083
                                                  50                 50

                        Figure 6.23 shows the approximately normal distribution of p^ 1 - p^ 2 and the area
                        corresponding to P[ p^ 1 - p^ 2  0]. The z-value corresponding to p^ 1 - p^ 2 = 0 is

                                 z = 0 - E( p^ 1 - p^ 2) = 0 - (-.05) = .60
                                                  Var( p^ 1 - p^ 2)  .083

                        so that

                                                  P[ p^ 1 - p^ 2  0] = 1 - (.60) = .27
                     6.5 One- and Two-Sample Inference for Proportions 409

                                          The approximate distribution

                                          of p1 - p2 is normal with
                                          mean -.05 and standard

                                          deviation .083

                                                 Approximate
                                                 probability that
                                                 p1  p2

                     -.20 -.10                0  .10

                     Figure 6.23 Approximate probability distribution for
                     p^ 1 - p^ 2

                          The large-sample approximate normality of p^ 1 - p^ 2 translates to the realization
                     that

                     Z = p^ 1 - p^ 2 - ( p1 - p2)                          (6.63)

                     p1(1 - p1) + p2(1 - p2)
                                      n1         n2

                     is approximately standard normal, and this observation forms the basis for inference
                     concerning p1 - p2. First consider confidence interval estimation for p1 - p2. The
                     familiar argument of Section 6.1 (beginning with the quantity (6.63)) shows

                     p^ 1 - p^ 2 ± z  p1(1 - p1) + p2(1 - p2)              (6.64)
                                          n1            n2

                     to be a mathematically correct but practically unusable formula for endpoints of a
                     confidence interval for p1 - p2. Conservative modification of expression (6.64), via
                     replacement of both p1(1 - p1) and p2(1 - p2) with .25, shows that the two-sided
                     interval with endpoints

     Large-sample                     1          1+1                       (6.65)
      conservative   p^ 1 - p^ 2 ± z ·           n1 n2
confidence limits
                                      2
        for p1 - p2

                     is a large-sample, two-sided, conservative confidence interval for p1 - p2 with
                     confidence at least that corresponding to the standard normal probability between
                     -z and z. (One-sided intervals are obtained from expression (6.65) in the usual
                     way.)
410 Chapter 6 Introduction to Formal Statistical Inference

                               In addition, in by now familiar fashion, beginning with the fact that for large
                          sample sizes, the modification of the variable (6.63),

                                 Z = p^ 1 - p^ 2 - ( p1 - p2)                                (6.66)

                                 p^ 1(1 - p^ 1) + p^ 2(1 - p^ 2)
                                                            n1          n2

                          is approximately standard normal leads to the conclusion that the interval with
                          endpoints

     Large-sample                p^ 1 - p^ 2 ± z            p^ 1(1 - p^ 1) + p^ 2(1 - p^ 2)  (6.67)
confidence limits                                               n1          n2

        for p1 - p2

                          is a large-sample, two-sided confidence interval for p1 - p2 with confidence cor-
                          responding to the standard normal probability assigned to the interval between -z
                          and z. (Again, use of only one of the endpoints in display (6.67) gives a one-sided
                          confidence interval.)

              Example 18  Comparing Fractions Conforming for Two Methods
(Example 14, Chapter 3,   of Operating a Pelletizing Process

    revisited--page 111)  Greiner, Grim, Larson, and Lukomski studied a number of different methods of
                          running a pelletizing process. Two of these involved a mix with 20% reground
                          powder with respectively small (condition 1) and large (condition 2) shot sizes.
                          Of n1 = n2 = 100 pellets produced under these two sets of conditions, sam-
                          ple fractions p^ 1 = .38 and p^ 2 = .29 of the pellets conformed to specifications.
                          Consider making a 90% confidence interval for comparing the two methods of
                          process operation (i.e., an interval for p1 - p2).

                               Use of expression (6.67) shows that the interval with endpoints

                                 .38 - .29 ± 1.645 (.38)(1 - .38) + (.29)(1 - .29)
                                                                100             100

                          i.e.,

                                                            .09 ± .109

                          i.e.,

                                                  -.019 and .199                             (6.68)
                    6.5 One- and Two-Sample Inference for Proportions 411

                    is a 90% confidence interval for p1 - p2, the difference in long-run fractions
                    of conforming pellets that would be produced under the two sets of conditions.
                    Notice that although appearances are that condition 1 has the higher associated
                    likelihood of producing a conforming pellet, the case for this made by the data in
                    hand is not airtight. The interval (6.68) allows some possibility that p1 - p2 <
                    0--i.e., that p2 actually exceeds p1. (The conservative interval indicated by
                    expression (6.65) has endpoints of the form .09 ± .116 and thus tells a similar
                    story.)

                    The usual significance-testing method for p1 - p2 concerns the null hypothesis

                    H0: p1 - p2 = 0         (6.69)

                    i.e., the hypothesis that the parameters p1 and p2 are equal. Notice that if p1 = p2
                    and the common value is denoted as p, expression (6.63) can be rewritten as

                    Z = p^ 1 - p^ 2         (6.70)

                    p(1 - p)      1+1
                                  n1 n2

                    The variable (6.70) cannot serve as a test statistic for the null hypothesis (6.69),
                    since it involves the unknown hypothesized common value of p1 and p2. What is
                    done to modify the variable (6.70) to arrive at a usable test statistic, is to replace p
                    with a sample-based estimate, obtained by pooling together the two samples. That
                    is, let

Pooled estimator    p^ = n1 p^ 1 + n2 p^ 2  (6.71)
  of a common p              n1 + n2

                    ( p^ is the total number of items in the two samples with the characteristic of interest
                    divided by the total number of items in the two samples). Then a significance test
                    of hypothesis (6.69) can be carried out using the test statistic

  Large-sample      Z = p^ 1 - p^ 2         (6.72)
test statistic for
 H0: p1 - p2 = 0    p^ (1 - p^ )  1+1
                                  n1 n2

                    If H0: p1 - p2 = 0 is true, Z in equation (6.72) is approximately standard normal,
                    so a standard normal reference distribution is in order.
412 Chapter 6 Introduction to Formal Statistical Inference

Example 18     As further confirmation of the fact that in the pelletizing problem sample fractions
 (continued )  of p^ 1 = .38 and p^ 2 = .29 based on samples of size n1 = n2 = 100 are not com-
               pletely convincing evidence of a real difference in process performance for small
               and large shot sizes, consider testing H0: p1 - p2 = 0 with Ha: p1 - p2 = 0. As
               a preliminary step, from expression (6.71),

               p^ = 100(.38) + 100(.29) = 67 = .335
                                         100 + 100                        200

               Then the five-step summary gives the following:

               1. H0: p1 - p2 = 0.
               2. Ha: p1 - p2 = 0.
               3. The test statistic is

                                         Z = p^ 1 - p^ 2

                                                            p^ (1 - p^ )  1+1
                                                                          n1 n2

                   The reference distribution is standard normal, and large observed values
                   |z| will constitute evidence against H0.
               4. The samples give

                                   z = .38 - .29 = 1.35
                                           (.335)(1 - .335) 1 + 1
                                                                   100 100

               5. The p-value is P[|a standard normal variable|  1.35]. That is, the p-
                   value is

                                         (-1.35) + 1 - (1.35) = .18

               The data furnish only fairly weak evidence of a real difference in long-run
               fractions of conforming pellets for the two shot sizes.

                    The kind of results seen in Example 18 may take some getting used to. Even
               with sample sizes as large as 100, sample fractions differing by nearly .1 are still
               not necessarily conclusive evidence of a difference in p1 and p2. But this is just
               another manifestation of the point that individual qualitative observations carry
               disappointingly little information.

                    A final reminder of the large-sample nature of the methods presented here is in
               order. The methods here all rely (for the agreement of nominal and actual confidence
                            6.5 One- and Two-Sample Inference for Proportions 413

levels or the validity of their p-values) on the adequacy of normal approximations
to binomial distributions. The approximations are workable provided expression
(6.52) or (6.53) holds. When testing H0: p = #, it is easy to plug both n and # into
expression (6.52) or (6.53) before putting great stock in normal-based p-values.
But when estimating p or p1 - p2 or testing H0: p1 - p2 = 0, no parallel check is
obvious. So it is not completely clear how to screen potential applications for ones
where the nominal confidence levels or p-values are possibly misleading. What is
often done is to plug both n and p^ (or both n1 and p^ 1 and n2 and p^ 2) into expression
(6.52) or (6.53) and verify that the inequalities hold before trusting nominal (normal-
based) confidence levels and p-values. Since these random quantities are only
approximations to the corresponding nonrandom quantities, one will occasionally
be misled regarding the appropriateness of the normal approximations by such
empirical checks. But they are better than automatic application, protected by no
check at all.

Section 5 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Consider the situation of Example 14 of Chapter 3,       2. In estimating a proportion p, a two-sided interval
   and in particular the results for the 50% reground           p^ ± is used. Suppose that 95% confidence and
   mixture.                                                         .01 are desired. About what sample size will
    (a) Make and interpret 95% one-sided and two-              be needed to guarantee this?
        sided confidence intervals for the fraction of
        conforming pellets that would be produced us-       3. Specifications on the punch heights referred to in
        ing the 50% mixture and the small shot size.           Chapter Exercise 9 of Chapter 3 were .500 in. to
        (For the one-sided interval, give a lower con-         .505 in. In the sample of 405 punches measured
        fidence bound.) Use both methods of dealing            by Hyde, Kuebrick, and Swanson, there were only
        with the fact that p^ is not known and compare         290 punches meeting these specifications. Suppose
        the resulting pairs of intervals.                      that the 405 punches can be thought of as a random
   (b) If records show that past pelletizing perfor-           sample of all such punches manufactured by the
        mance was such that 55% of the pellets pro-            supplier under standard manufacturing conditions.
        duced were conforming, does the value in Table         Give an approximate 99% two-sided confidence in-
        3.20 constitute strong evidence that the condi-        terval for the standard fraction of nonconforming
        tions of 50% reground mixture and small shot-          punches of this type produced by the punch sup-
        size provide an improvement in yield? Show             plier.
        the five-step format.
    (c) Compare the small and large shot-size condi-        4. Consider two hypothetical machines producing a
        tions using a 95% two-sided confidence inter-          particular widget. If samples of n1 = 25 and n2 =
        val for the difference in fractions conforming.        25 widgets produced by the respective machines
        Interpret the interval in the context of the ex-       have fractions nonconforming p^ 1 = .2 and p^ 2 =
        ample.                                                 .32, is this strong evidence of a difference in ma-
   (d) Assess the strength of the evidence given in            chine nonconforming rates? What does this suggest
        Table 3.20 that the shot size affects the fraction     about the kind of sample sizes typically needed in
        of pellets conforming (when the 50% reground           order to reach definitive conclusions based on at-
        mixture is used).                                      tributes or qualitative data?
414 Chapter 6 Introduction to Formal Statistical Inference

                                                qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

         6.6 Prediction and Tolerance Intervals

                                  Methods of confidence interval estimation and significance testing concern the
                                  problem of reasoning from sample information to statements about underlying pa-
                                  rameters of the data generation, such as µ,  , and p. These are extremely important
                                  engineering tools, but they often fail to directly address the question of real interest.
                                  Sometimes what is really needed as the ultimate product of a statistical analysis is
                                  not a statement about a parameter but rather an indication of reasonable bounds on
                                  other individual values generated by the process under study. For example, suppose
                                  you are about to purchase a new car. For some purposes, knowing that "the mean
                                  EPA mileage for this model is likely in the range 25 mpg ± .5 mpg" is not nearly as
                                  useful as knowing that "the EPA mileage figure for the particular car you are order-
                                  ing is likely in the range 25 mpg ± 3 mpg." Both of these statements may be quite
                                  accurate, but they serve different purposes. The first statement is one about a mean
                                  mileage and the second is about an individual mileage. And it is only statements of
                                  the first type that have been directly treated thus far.

                                       This section indicates what is possible in the way of formal statistical in-
                                  ferences, not for parameters but rather for individual values generated by a stable
                                  data-generating mechanism. There are two types of formal inference methods aimed
                                  in this general direction--statistical prediction interval methods and statistical tol-
                                  erance interval methods--and both types will be discussed. The section begins with
                                  prediction intervals for a normal distribution. Then tolerance intervals for a normal
                                  distribution are considered. Finally, there is a discussion of how it is possible to use
                                  minimum and/or maximum values in a sample to create prediction and tolerance
                                  intervals for even nonnormal underlying distributions.

6.6.1  Prediction Intervals for a Normal Distribution

       One fruitful way to phrase the question of inference for additional individual values
       produced by a process is the following: How might data in hand, x1, x2, . . . , xn,
       be used to create a numerical interval likely to bracket one additional (as yet
       unobserved) value, xn+1, from the same data-generating mechanism? How, for
       example, might mileage tests on ten cars of a particular model be used to predict the
       results of the same test applied to an eleventh?

            If the underlying distribution is adequately described as normal with mean µ
       and variance  2, there is a simple line of reasoning based on the random variable

       x¯ - xn+1                                     (6.73)

       that leads to an answer to this question. That is, the random variable in expression
       (6.73) has, by the methods of Section 5.5 (Proposition 1 in particular),

       E(x¯ - xn+1) = E x¯ + (-1)E xn+1 = µ - µ = 0  (6.74)
                                6.6 Prediction and Tolerance Intervals 415

                        and

                                                 2 2  2 2 1 2                                   (6.75)
                        Var(x¯ - xn+1) = (1) Var x¯ + (-1) Var xn+1 = +  = 1 + 
                                               n                                             n

                        Further, it turns out that the difference (6.73) is normally distributed, so the variable

                             Z = (x¯ - xn+1) - 0                                                (6.76)
                                       1+ 1
                                                n

                        is standard normal. And taking one more step, if s2 is the usual sample variance of
                        x1, x2, . . . , xn, substituting s for  in expression (6.76) produces a variable

                             T = (x¯ - xn+1) - 0                                                (6.77)
                                       s 1+ 1
                                                n

                        which has a t distribution with  = n - 1 degrees of freedom. 
                             Now (upon identifying xn+1 with µ and 1 + (1/n) with 1/n), the variable

                        (6.77) is formally similar to the t-distributed variable used to derive a small-sample
                        confidence interval for µ. In fact, algebraic steps parallel to those used in the first
                        part of Section 6.3 show that if t > 0 is such that the tn-1 distribution assigns, say,
                        .95 probability to the interval between -t and t, there is then .95 probability that

                             1                                 1
                             x¯ - ts 1 + < xn+1 < x¯ + ts 1 +
                             n                                 n

                        This reasoning suggests in general that the interval with endpoints

Normal distribution          x¯ ± ts 1 + 1                                                      (6.78)
prediction limits for                       n

 a single additional
           observation

                        can be used as a two-sided interval to predict xn+1 and that the probability-based
                        reliability figure attached to the interval should be the tn-1 probability assigned to
                        the interval from -t to t. The interval (6.78) is a called a prediction interval with
                        associated confidence the tn-1 probability assigned to the interval from -t to t. In
                        general, the language indicated in Definition 17 will be used.
416 Chapter 6 Introduction to Formal Statistical Inference

Definition 17           A prediction interval for a single additional observation is a data-based
                        interval of numbers thought likely to contain the observation, possessing a
                        stated probability-based confidence or reliability.

                             It is the fact that a finite sample gives only a somewhat clouded picture of a
                        distribution that prevents the making of a normal distribution prediction interval
                        from being a trivial matter of probability calculations like those in Section 5.2. That
                        is, suppose there were enough data to "know" the mean, µ, and variance,  2, of
                        a normal distribution. Then, since 1.96 is the .975 standard normal quantile, the
                        interval with endpoints

                               µ - 1.96 and µ + 1.96                      (6.79)

                        has a 95% chance of bracketing the next value generated by the distribution. The fact
                        that (when based only on small samples), the knowledge of µ and  is noisy forces
                        expression (6.79) to be abandoned for an interval like (6.78). It is thus comforting that
                        for large n and 95% confidence, formula (6.78) produces an interval with endpoints
                        approximating those in display (6.79). That is, for large n and 95% confidence,
                        t  1.96, 1 + (1/n)  1, and one expects that typically x¯  µ and s   , so that
                        expressions (6.78) and (6.79) will essentially agree. The beauty of expression (6.78)
                        is that it allows in a rational fashion for the uncertainties involved in the µ  x¯ and
                          s approximations.

          Example 19    Predicting a Spring Lifetime
(Example 8 revisited )
                        Recall from Section 6.3 that n = 10 spring lifetimes under 950 N/mm2 stress
                        conditions given in Table 6.4 (page 366) produced a fairly linear normal plot,
                        x¯ = 168.3 (×103 cycles) and s = 33.1 (×103 cycles). Consider now predicting
                        the lifetime of an additional spring of this type (under the same test conditions)
                        with 90% confidence.

                             Using  = 10 - 1 = 9 degrees of freedom, the .95 quantile of the t distri-
                        bution is (from Table B.4) 1.833. So, employing expression (6.78), there are
                        two-sided 90% prediction limits for an additional spring lifetime

                               168.3 ± 1.833(33.1) 1 + 1
                                                                10

                        i.e.,

                               104.7 × 103 cycles and 231.9 × 103 cycles  (6.80)
                                                       6.6 Prediction and Tolerance Intervals 417

            The interval indicated by display (6.80) is not at all the same as the confidence
            interval for µ found in Example 8. The limits of

                                149.1 × 103 cycles and 187.5 × 103 cycles

            found on page 367 apply to the mean spring lifetime, µ, not to an additional
            observation x11 as the ones in display (6.80) do.

Example 20  Predicting the Weight of a Newly Minted Penny

            The delightful book Experimentation and Measurement by W. J. Youden (pub-
            lished as NBS Special Publication 672 by the U.S. Department of Commerce)
            contains a data set giving the weights of n = 100 newly minted U.S. pennies
            measured to 10-4 g but reported only to the nearest .02 g. These data are repro-
            duced in Table 6.10. Figure 6.24 is a normal plot of these data and shows that a
            normal distribution is a plausible model for weights of newly minted pennies.

                 Further, calculation with the values in Table 6.10 shows that for the penny
            weights, x¯ = 3.108 g and s = .043 g. Then interpolation in Table B.4 shows
            the .9 quantile of the t99 distribution to be about 1.290, so that using only the
            "plus" part of expression (6.78), a one-sided 90% prediction interval of the form
            (-, #) for the weight of a single additional penny has upper endpoint

                                         3.108 + 1.290(.043) 1 + 1
                                                                          100

            i.e.,

                      3.164 g                                                             (6.81)

            Table 6.10
            Weights of 100 Newly Minted U.S. Pennies

            Penny Weight (g) Frequency Penny Weight (g)                        Frequency

            2.99  1                                   3.11                         24
                                                                                   17
            3.01  4                                   3.13                         13

            3.03  4                                   3.15                           6
                                                                                     2
            3.05  4                                   3.17                           1

            3.07  7                                   3.19

            3.09  17                                  3.21
418 Chapter 6 Introduction to Formal Statistical Inference

Example 20       Standard normal quantile  3
 (continued )

                                           2

                                           1

                                           0

                                           -1

                                           -2

                                           -3  3.0          3.1              3.2

                                                            Weight quantile

                 Figure 6.24 Normal plot of the penny weights

                      This example illustrates at least two important points. First, the two-sided
                 prediction limits in display (6.78) can be modified to get a one-sided limit exactly

                 as two-sided confidence limits can be modified to get a one-sided limit. Second,
                 the calculation represented by the result (6.81) is, because n = 100 is a fairly
                 large sample size, only marginally different from what one would get assuming
                 µ = 3.108 g exactly and  = .043 g exactly. That is, since the .9 normal quantile
                 is 1.282, "knowing" µ and  leads to an upper prediction limit of

                 µ + 1.282 = 3.108 + (1.282)(.043) = 3.163 g                      (6.82)

                 The fact that the result (6.81) is slightly larger than the final result in display
                 (6.82) reflects the small uncertainty involved in the use of x¯ in place of µ and s
                 in place of  .

Cautions about        The name "prediction interval" probably has some suggested meanings that
   "prediction"  should be dismissed before going any further. Prediction suggests the future and
                 thus potentially different conditions. But no such meaning should be associated
                 with statistical prediction intervals. The assumption behind formula (6.78) is that
                 x1, x2, . . . , xn and xn+1 are all generated according to the same underlying distribu-
                 tion. If (for example, because of potential physical changes in a system during a time
                 lapse between the generation of x1, x2, . . . , xn and the generation of xn+1) no single
                 stable process model for the generation of all n + 1 observations is appropriate, then
                 neither is formula (6.78). Statistical inference is not a crystal ball for foretelling an
                 erratic and patternless future. It is rather a methodology for quantifying the extent
                 of knowledge about a pattern of variation existing in a consistent present. It has
                 implications in other times and at other places only if that same pattern of variation
                 can be expected to repeat itself in those conditions.
                                                                      6.6 Prediction and Tolerance Intervals 419

                            It is also appropriate to comment on the meaning of the confidence or reliability
                       figure attached to a prediction interval. Since a prediction interval is doing a different
                       job than the confidence intervals of previous sections, the meaning of confidence
                       given in Definition 2 doesn't quite apply here.

                            Prior to the generation of any of x1, x2, . . . , xn, xn+1, planned use of expression
                       (6.78) gives a guaranteed probability of success in bracketing xn+1. And after all of
                       x1, x2, . . . , xn, xn+1 have been generated, one has either been completely successful
                       or completely unsuccessful in bracketing xn+1. But it is not altogether obvious how
                       to think about "confidence" of prediction when x1, x2, . . . , xn are in hand, but prior
                       to the generation of xn+1. For example, in the context of Example 19, having used
                       sample data to arrive at the prediction limits in display (6.80)--i.e.,

                                             104.7 × 103 cycles to 231.9 × 103 cycles

                       since x11 is a random variable, it would make sense to contemplate

                                                  P[104.7 × 103  x11  231.9 × 103]

                       However, there is no guarantee on this probability nor any way to determine it. In
                       particular, it is not necessarily .9 (the confidence level associated with the prediction
                       interval). That is, there is no practical way to employ probability to describe the
                       likely effectiveness of a numerical prediction interval. One is thus left with the
                       interpretation of confidence of prediction given in Definition 18.

     Definition 18     To say that a numerical interval (a, b) is (for example) a 90% prediction interval
(Interpretation of a   for an additional observation xn+1 is to say that in obtaining it, methods of
Prediction Interval )  data collection and calculation have been applied that would produce intervals
                       bracketing an (n + 1)th observation in about 90% of repeated applications of
                       the entire process of (1) selecting the sample x1, . . . , xn, (2) calculating an
                       interval, and (3) generating a single additional observation xn+1. Whether or
                       not xn+1 will fall into the numerical interval (a, b) is not known, and although
                       there is some probability associated with that eventuality, it is not possible to
                       evaluate it. And in particular, it need not be 90%.

                            When using a 90% prediction interval method, although some samples x1, . . . , xn
                       produce numerical intervals with probability less than .9 of bracketing xn+1 and oth-
                       ers produce numerical intervals with probability more than .9, the average for all
                       samples x1, . . . , xn does turn out to be .9. The practical problem is simply that with
                       data x1, . . . , xn in hand, you don't know whether you are above, below, or at the .9
                       figure.
420 Chapter 6 Introduction to Formal Statistical Inference

6.6.2                          Tolerance Intervals for a Normal Distribution

                               The emphasis, when making a prediction interval of the type just discussed, is on a
                               single additional observation beyond those n already in hand. But in some practical
                               engineering problems, many additional items are of interest. In such cases, one may
                               wish to declare a data-based interval likely to encompass most measurements from
                               the rest of these items.

                                    Prediction intervals are not designed for the purpose of encompassing most of
                               the measurements from the additional items of interest. The paragraph following
                               Definition 18 argues that only on average is the fraction of a normal distribution
                               bracketed by a 90% prediction interval equal to 90%. So a crude analysis (identifying
                               the mean fraction bracketed with the median fraction bracketed) then suggests that
                               the probability that the actual fraction bracketed is at least 90% is only about .5.
                               That is, a 90% prediction interval is not constructed to be big enough for the present
                               purpose. What is needed instead is a statistical tolerance interval.

Definition 19                  A statistical tolerance interval for a fraction p of an underlying distribu-
                               tion is a data-based interval thought likely to contain at least a fraction p and
                               possessing a stated (usually large) probability-based confidence or reliability.

                                    The derivation of normal distribution tolerance interval formulas requires prob-
                               ability background well beyond what has been developed in this text. But results of
                               that work look about as would be expected. It is possible, for a desired confidence
                               level and fraction p of an underlying normal distribution, to find a corresponding
                               constant 2 such that the two-sided interval with endpoints

     Two-sided normal                                       x¯ ± 2s                            (6.83)
distribution tolerance
                               is a tolerance interval for a fraction p of the underlying distribution. The 2 appear-
                       limits  ing in expression (6.83) is, for common (large) confidence levels, larger than the
                               multiplier t 1 + (1/n) appearing in expression (6.78) for two-sided confidence of
             A one-sided
      normal tolerance         prediction p. On the other hand, as n gets large, both 2 from expression (6.83) and
                                                                              1+ p
                   interval    t 1 + (1/n) from expression (6.78) tend to the ( 2 ) standard normal quantile.
                               Table B.7A gives some values of 2 for 95% and 99% confidence and p = .9, .95,
                               and .99. (The use of this table will be demonstrated shortly.)

                                    The factors 2 are not used to make one-sided tolerance intervals. Instead,
                               another set of constants that will here be called 1 values have been developed.
                               They are such that for a given confidence and fraction p of an underlying normal

                               distribution, both of the one-sided intervals

                                 (-, x¯ + 1s)                                                  (6.84)
                               6.6 Prediction and Tolerance Intervals 421

                          and  (x¯ - 1s, )                               (6.85)

Another one-sided
  normal tolerance
               interval

               are tolerance intervals for a fraction p of the distribution. 1 appearing in inter-
               vals (6.84) and (6.85) is, for common confidence levels, larger than the multiplier
               t 1 + (1/n) appearing in expression (6.78) for one-sided confidence of prediction
               p. And as n gets large, both 1 from expression (6.84) or (6.85) and t 1 + (1/n)
               from expression (6.78) tend to the standard normal p quantile. Table B.7B gives
               some values of 1.

Example 19     Consider making a two-sided 95% tolerance interval for 90% of additional spring
 (continued )
               lifetimes based on the data of Table 6.4. As earlier, for these data, x¯ = 168.3
               (×103 cycles) and s = 33.1 (×103 cycles). Then consulting Table B.7A, since

               n = 10, 2 = 2.856 is appropriate for use in expression (6.83). That is, two-sided
               95% tolerance limits for 90% of additional spring lifetimes are

                               168.3 ± 2.856 (33.1)

               i.e.,

                               73.8 × 103 cycles and 262.8 × 103 cycles  (6.86)

               It is obvious from comparing displays (6.80) and (6.86) that the effect of moving
               from the prediction of a single additional spring lifetime to attempting to bracket
               most of a large number of additional lifetimes is to increase the size of the
               declared interval.

Example 20     Consider again the new penny weights given in Table 6.10 and now the problem of
 (continued )  making a one-sided 95% tolerance interval of the form (-, #) for the weights of
               90% of additional pennies. Remembering that for the penny weights, x¯ = 3.108 g
               and s = .043 g, and using Table B.7B for n = 100, the desired upper tolerance
               bound for 90% of the penny weights is

                                            3.108 + 1.527(.043) = 3.174 g

               As expected, this is larger (more conservative) than the value of 3.164 g given in
               display (6.81) as a one-sided 90% prediction limit for a single additional penny
               weight.
422 Chapter 6 Introduction to Formal Statistical Inference

                                       The correct interpretation of the confidence level for a tolerance interval should
                                  be fairly easy to grasp. Prior to the generation of x1, x2, . . . , xn, planned use of
                                  expression (6.83), (6.84), or (6.85) gives a guaranteed probability of success in
                                  bracketing a fraction of at least p of the underlying distribution. But after observing
                                  x1, . . . , xn and making a numerical interval, it is impossible to know whether the
                                  attempt has or has not been successful. Thus the following interpretation:

     Definition 20        To say that a numerical interval (a, b) is (for example) a 90% tolerance in-
(Interpretation of a      terval for a fraction p of an underlying distribution is to say that in obtaining
Tolerance Interval )      it, methods of data collection and calculation have been applied that would
                          produce intervals bracketing a fraction of at least p of the underlying distri-
                          bution in about 90% of repeated applications (of generation of x1, . . . , xn and
                          subsequent calculation). Whether or not the numerical interval (a, b) actually
                          contains at least a fraction p is unknown and not describable in terms of a
                          probability.

6.6.3                     Prediction and Tolerance Intervals Based on Minimum
                          and/or Maximum Values in a Sample

                          Formulas (6.78), (6.83), (6.84), and (6.85) for prediction and tolerance limits are
                          definitely normal distribution formulas. So what if an engineering data-generation
                          process is stable but does not produce normally distributed observations? How,
                          if at all, can prediction or tolerance limits be made? Two kinds of answers to
                          these questions will be illustrated in this text. The first employs the transformation
                          idea presented in Section 4.4, and the second involves the use of minimum and/or
                          maximum sample values to establish prediction and/or tolerance bounds.

                               First (as observed in Section 4.4) if a response variable y fails to be normally
                          distributed, it may still be possible to find some transformation g (essentially speci-
                          fying a revised scale of measurement) such that g(y) is normal. Then normal-based
                          methods might be applied to g(y) and answers of interest translated back into
                          statements about y.

              Example 21  Prediction and Tolerance Intervals for Discovery Times
(Example 11, Chapter 4,   Obtained Using a Transformation

    revisited--page 192)  Section 5.3 argued that the auto service discovery time data of Elliot, Kibby, and
                          Meyer given in Figure 4.31 (see page 192) are not themselves normal-looking,
                          but that their natural logarithms are. This, together with the facts that the n = 30
                          natural logarithms have x¯ = 2.46 and s = .68, can be used to make prediction or
                          tolerance intervals for log discovery times.
       6.6 Prediction and Tolerance Intervals 423

     For example, using expression (6.78) to make a two-sided 99% prediction
interval for an additional log discovery time produces endpoints

                               2.46 ± 2.756(.68) 1 + 1
                                                             30

i.e.,

       .55 ln min and 4.37 ln min                                (6.87)

And using expression (6.83) to make, for example, a 95% tolerance interval for
99% of additional log discovery times produces endpoints

                                     2.46 ± 3.355(.68)

i.e.,

       .18 ln min and 4.74 ln min                                (6.88)

Then the intervals specified in displays (6.87) and (6.88) for log discovery times
have, via exponentiation, their counterparts for raw discovery times. That is,
exponentiation of the values in display (6.87) gives a 99% prediction interval for
another discovery time of from

       1.7 min to 79.0 min

And exponentiation of the values in display (6.88) gives a 95% tolerance interval
for 99% of additional discovery times of from

       1.2 min to 114.4 min

     When it is not possible to find a transformation that will allow normal-based
methods to be used, prediction and tolerance interval formulas derived for other
standard families of distributions (e.g., the Weibull family) can sometimes be ap-
propriate. (The book Statistical Intervals: A Guide for Practitioners, by Hahn and
Meeker, is a good place to look for these methods.) What can be done here is to
point out that intervals from the smallest observation and/or to the largest value in
a sample can be used as prediction and/or tolerance intervals for any underlying
continuous distribution.
424 Chapter 6 Introduction to Formal Statistical Inference

                                  That is, if x1, x2, . . . , xn are values in a sample and min(x1, . . . , xn) and
                             max(x1, . . . , xn) are (respectively) the smallest and largest values among x1,
                             x2, . . . , xn, consider the use of the intervals

     Interval based on            (-,max(x1, . . . , xn))                                (6.89)
the sample maximum

                             and

     Interval based on            (min(x1, . . . , xn), )                                (6.90)
the sample minimum

                             and

     Interval based on            (min(x1, . . . , xn), max(x1, . . . , xn))             (6.91)
the sample minimum

         and maximum

                             as prediction or tolerance intervals. Independent of exactly what underlying contin-
                             uous distribution is operating, if the generation of x1, x2, . . . , xn (and if relevant,
                             xn+1) can be described as a stable process, it is possible to evaluate the confidence
                             levels associated with intervals (6.89), (6.90), and (6.91).

                                  Consider first intervals (6.89) or (6.90) used as one-sided prediction intervals
                             for a single additional observation xn+1. The associated confidence level is

  Prediction confidence           One-sided prediction confidence level = n              (6.92)
for a one-sided interval                                                            n+1

                             Then, considering interval (6.91) as a two-sided prediction interval for a single
                             additional observation xn+1, the associated confidence level is

  Prediction confidence           Two-sided prediction confidence level = n - 1          (6.93)
for a two-sided interval                                                            n+1

                                  The confidence levels for intervals (6.89), (6.90), and (6.91) as tolerance in-
                             tervals must of necessity involve p, the fraction of the underlying distribution one
                             hopes to bracket. The fact is that using interval (6.89) or (6.90) as a one-sided toler-
                             ance interval for a fraction p of an underlying distribution, the associated confidence
                             level is

  Confidence level for            One-sided confidence level = 1 - pn                    (6.94)
a one-sided tolerance

                   interval
                             6.6 Prediction and Tolerance Intervals 425

                             And when interval (6.91) is used as a tolerance interval for a fraction p of an
                             underlying distribution, the appropriate associated confidence is

  Confidence level for       Two-sided confidence level = 1 - pn - n(1 - p) pn-1  (6.95)
a two-sided tolerance

                   interval

Example 19                   Return one more time to the spring-life scenario, and consider the use of interval
 (continued )                (6.91) as first a prediction interval and then a tolerance interval for 90% of
                             additional spring lifetimes. Notice in Table 6.4 (page 366) that the smallest and
                             largest of the observed spring lifetimes are, respectively,

                                                      min(x1, . . . , x10) = 117 × 103 cycles

                             and

                                                      max(x1, . . . , x10) = 225 × 103 cycles

                             so the numerical interval under consideration is the one with endpoints 117
                             (×103 cycles) and 225 (×103 cycles).

                                  Then expression (6.93) means that this interval can be used as a prediction
                             interval with

                                                 Prediction confidence = 10 - 1 = 9 = 82%
                                                                               10 + 1 11

                             And expression (6.95) says that as a tolerance interval for a fraction p = .9
                             of many additional spring lifetimes, the interval can be used with associated
                             confidence

                                              Confidence = 1 - (.9)10 - 10(1 - .9)(.9)9 = 26%

Example 20                   Looking for a final time at the penny weight data in Table 6.10, consider the use
 (continued )                of interval (6.89) as first a prediction interval and then a tolerance interval for
                             99% of additional penny weights. Notice that in Table 6.10, the largest of the
                             n = 100 weights is 3.21 g, so

                                                            max(x1, . . . , x100) = 3.21 g
426 Chapter 6 Introduction to Formal Statistical Inference

Example 20     Then expression (6.92) says that when used as an upper prediction limit for a
 (continued )  single additional penny weight, the prediction confidence associated with 3.21 g is

                                      Prediction confidence = 100 = 99%
                                                                    100 + 1

               And expression (6.94) shows that as a tolerance interval for 99% of many addi-
               tional penny weights, the interval (-, 3.21) has associated confidence

                                          Confidence = 1 - (.99)100 = 63%

                    A little experience with formulas (6.92), (6.93), (6.94), and (6.95) will convince
               the reader that the intervals (6.89), (6.90), and (6.91) often carry disappointingly
               small confidence coefficients. Usually (but not always), you can do better in terms
               of high confidence and short intervals if (possibly after transformation) the normal
               distribution methods discussed earlier can be applied. But the beauty of intervals
               (6.89), (6.90), and (6.91) is that they are both widely applicable (in even nonnormal
               contexts) and extremely simple.

                    Prediction and tolerance interval methods are very useful engineering tools.
               Historically, they probably haven't been used as much as they should be for lack of
               accessible textbook material on the methods. We hope the reader is now aware of the
               existence of the methods as the appropriate form of formal inference when the focus
               is on individual values generated by a process rather than on process parameters.
               When the few particular methods discussed here don't prove adequate for practical
               purposes, the reader should look into the topic further, beginning with the book by
               Hahn and Meeker mentioned earlier.

Section 6 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Confidence, prediction, and tolerance intervals are     (a) Make a two-sided 90% prediction interval for
   all intended to do different jobs. What are these            an additional spring lifetime under this stress.
   jobs? Consider the differing situations of an official
   of the EPA, a consumer about to purchase a single       (b) Make a two-sided 95% tolerance interval for
   car, and a design engineer trying to equip a certain         90% of all spring lifetimes under this stress.
   model with a gas tank large enough that most cars
   produced will have highway cruising ranges of at        (c) How do the intervals from (a) and (b) compare?
   least 350 miles. Argue that depending on the point           (Consider both size and interpretation.)
   of view adopted, a lower confidence bound for a
   mean mileage, a lower prediction bound for an in-       (d) There is a two-sided 90% confidence interval
   dividual mileage, or a lower tolerance bound for             for the mean spring lifetime under this stress
   most mileages would be of interest.                          given in Example 8. How do your intervals
                                                                from (a) and (b) compare to the interval in
2. The 900 N/mm2 stress spring lifetime data in Table           Example 8? (Consider both size and interpre-
   6.7 used in Example 8 have a fairly linear normal            tation.)
   plot.
                                                           (e) Make a 90% lower prediction bound for an
                                                                additional spring lifetime under this stress.
                                                          Chapter 6 Exercises 427

    (f) Make a 95% lower tolerance bound for 90% of               into a prediction interval for a single additional
        all spring lifetimes under this stress.                   aluminum content.
                                                              (c) How do the intervals from (a) and (b) compare?
3. The natural logarithms of the aluminum contents
   discussed in Exercise 2 of Chapter 3 have a rea-       4. Again in the context of Chapter Exercise 2 of Chap-
   sonably bell-shaped relative frequency distribution.      ter 3, if the interval from 30 ppm to 511 ppm
   Further, these 26 log aluminum contents have sam-         is used as a prediction interval for a single addi-
   ple mean 4.9 and sample standard deviation .59.           tional aluminum content measurement from the
   Use this information to respond to the following:         study period, what associated prediction confi-
    (a) Give a two-sided 99% tolerance interval for          dence level can be stated? What confidence can
        90% of additional log aluminum contents at           be associated with this interval as a tolerance in-
        the Rutgers recycling facility. Then translate       terval for 90% of all such aluminum content mea-
        this interval into a 99% tolerance interval for      surements?
        90% of additional raw aluminum contents.
   (b) Make a 90% prediction interval for one ad-
        ditional log aluminum content and translate it

Chapter 6 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Consider the breaking strength data of Table 3.6.              strength of the evidence in the data that the
   Notice that the normal plot of these data given as             mean generic towel strength is in fact below
   Figure 3.18 is reasonably linear. It may thus be sen-          the 9,500 g target. (Show the whole five-step
   sible to suppose that breaking strengths for generic           significance-testing format.)
   towel of this type (as measured by the students) are       (f) Now put yourself in the place of a quality
   adequately modeled as normal. Under this assump-               control inspector concerned that the breaking
   tion,                                                          strength be reasonably consistent--i.e., that 
    (a) Make and interpret 95% two-sided and one-                 be small. Suppose in fact it is desirable that 
        sided confidence intervals for the mean break-            be no more than 400 g. Use the significance-
        ing strength of generic towels (make a one-               testing format and assess the strength of the
        sided interval of the form (#,)).                         evidence given in the data that in fact  ex-
   (b) Make and interpret 95% two-sided and one-                  ceeds the target standard deviation.
        sided prediction intervals for a single addi-
        tional generic towel breaking strength (for the   2. Consider the situation of Example 1 in Chapter 1.
        one-sided interval, give the lower prediction         (a) Use the five-step significance-testing format to
        bound).                                                   assess the strength of the evidence collected in
    (c) Make and interpret 95% two-sided and one-                 this study to the effect that the laying method
        sided tolerance intervals for 99% of generic              is superior to the hanging method in terms of
        towel breaking strengths (for the one-sided in-           mean runouts produced.
        terval, give the lower tolerance bound).             (b) Make and interpret 90% two-sided and one-
   (d) Make and interpret 95% two-sided and one-                  sided confidence intervals for the improvement
        sided confidence intervals for  , the standard            in mean runout produced by the laying method
        deviation of generic towel breaking strengths.            over the hanging method (for the one-sided
    (e) Put yourself in the position of a quality con-            interval, give a lower bound for µhung - µlaid).
        trol inspector, concerned that the mean break-        (c) Make and interpret a 90% two-sided confi-
        ing strength not fall under 9,500 g. Assess the           dence interval for the mean runout for laid
                                                                  gears.
428 Chapter 6 Introduction to Formal Statistical Inference

   (d) What is it about Figure 1.1 that makes it ques-       lengths obtained by one of the students were as
        tionable whether "normal distribution" predic-       follows (the units are inches):
        tion and tolerance interval formulas ought to be
        used to describe runouts for laid gears? Sup-          1.1375, 1.1390, 1.1420, 1.1430, 1.1410, 1.1360,
        pose instead that you used the methods of Sec-         1.1395, 1.1380, 1.1350, 1.1370, 1.1345, 1.1340,
        tion 6.6.3 to make prediction and tolerance in-        1.1405, 1.1340, 1.1380, 1.1355
        tervals for laid gear runouts. What confidence
        could be associated with the largest observed         (a) If you were to, for example, make a confi-
        laid runout as an upper prediction bound for a            dence interval for the population mean mea-
        single additional laid runout? What confidence            sured length of these bushings via the formu-
        could be associated with the largest observed             las in Section 6.3, what model assumption must
        laid runout as an upper tolerance bound for               you employ? Make a probability plot to assess
        95% of additional laid gear runouts?                      the reasonableness of the assumption.

3. Consider the situation of Example 1 in Chapter 4.         (b) Make a 90% two-sided confidence interval for
   In particular, limit attention to those densities ob-          the mean measured length for bushings of this
   tained under the 2,000 and 4,000 psi pressures.                type measured by this student.
   (One can view the six corresponding densities as
   two samples of size n1 = n2 = 3.)                          (c) Give an upper bound for the mean length with
    (a) Assess the strength of the evidence that in-              90% associated confidence.
        creasing pressure increases the mean density
        of the resulting cylinders. Use the five-step        (d) Make a 90% two-sided prediction interval for
        significance-testing format.                              a single additional measured bushing length.
   (b) Give a 99% lower confidence bound for the
        increase in mean density associated with the          (e) Make a 95% two-sided tolerance interval for
        change from 2,000 to 4,000 psi conditions.                99% of additional measured bushing lengths.
    (c) Assess the strength of the evidence (in the six
        density values) that the variability in density       (f) Consider the statistical interval derived from
        differs for the 2,000 and 4,000 psi conditions            the minimum and maximum sample values--
        (i.e., that 2,000 = 4,000).                               namely, (1.1340, 1.1430). What confidence
   (d) Give a 90% two-sided confidence interval for               level should be associated with this interval
        the ratio of density standard deviations for the          as a prediction interval for a single additional
        two pressures.                                            bushing length? What confidence level should
    (e) What model assumptions stand behind the for-              be associated with this interval as a tolerance
        mal inferences you made in parts (a) through              interval for 99% of additional bushing lengths?
        (d) above?
                                                          6. The study mentioned in Exercise 5 also included
4. Simple counting with the data of Chapter Exercise 2       measurement of the outside diameters of the 16
   in Chapter 3 shows that 18 out of the 26 PET sam-         bushings. Two of the students measured each of
   ples had aluminum contents above 100 ppm. Give            the bushings, with the results given here.
   a two-sided approximate 95% confidence interval
   for the fraction of all such samples with aluminum       Bushing      1      2      3      4
   contents above 100 ppm.                                  Student A  .3690  .3690  .3690  .3700
                                                            Student B  .3690  .3695  .3695  .3695
5. Losen, Cahoy, and Lewis measured the lengths of
   some spanner bushings of a particular type pur-          Bushing      5       6     7      8
   chased from a local machine supply shop. The             Student A  .3695  .3700  .3695  .3690
                                                            Student B  .3695  .3700  .3700  .3690
                                                                        Chapter 6 Exercises 429

Bushing       9    10     11     12                           obtained had mean x¯ = .0287 in. and standard de-
Student A  .3690  .3695  .3690  .3690                         viation s = .0119 in.
Student B  .3700  .3690  .3695  .3695                         (a) Give a 95% two-sided confidence interval for

Bushing     13     14     15     16                                the mean wobble of all such disks.
Student A  .3695  .3700  .3690  .3690                         (b) Give a lower bound for the mean wobble pos-
Student B  .3690  .3695  .3690  .3690
                                                                   sessing a 95% confidence level.
    (a) If you want to compare the two students' aver-        (c) Suppose that these disks are ordered with the
        age measurements, the methods of formulas
        (6.35), (6.36), and (6.38) are not appropriate.            requirement that the mean wobble not exceed
        Why?                                                       .025 in. Assess the strength of the evidence
                                                                   in the students' data to the effect that the re-
   (b) Make a 95% two-sided confidence interval                    quirement is being violated. Show the whole
        for the mean difference in outside diameter                five-step format.
        measurements for the two students.                    (d) Is the requirement of part (c) the same as an
                                                                   upper specification of .025 in. on individual
7. Find the following quantiles using the tables of                wobbles? Explain. Is it possible for a lot with
   Appendix B:                                                     many individual wobbles exceeding .025 in.
    (a) the .90 quantile of the t5 distribution                    to meet the requirement of part (c)?
   (b) the .10 quantile of the t5 distribution                (e) Of the measured wobbles, 19 were .030 in.
    (c) the .95 quantile of the 72 distribution                    or more. Use this fact and make an approx-
   (d) the .05 quantile of the 72 distribution                     imate 90% two-sided confidence interval for
    (e) the .95 quantile of the F distribution with                the fraction of all such disks with wobbles of
        numerator degrees of freedom 8 and denom-                  at least .030 in.
        inator degrees of freedom 4
    (f) the .05 quantile of the F distribution with      10. T. Johnson tested properties of several brands of
        numerator degrees of freedom 8 and denom-             10 lb test monofilament fishing line. Part of his
        inator degrees of freedom 4                           study involved measuring the stretch of a fixed
                                                              length of line under a 3.5 kg load. Test results for
8. Find the following quantiles using the tables of           three pieces of two of the brands follow. The units
   Appendix B:                                                are cm.
    (a) the .99 quantile of the t13 distribution
   (b) the .01 quantile of the t13 distribution          Brand B        Brand D
    (c) the .975 quantile of the 32 distribution         .86, .88, .88  1.06, 1.02, 1.04
   (d) the .025 quantile of the 32 distribution
    (e) the .75 quantile of the F distribution with      (a) Considering first only Brand B, use "normal
        numerator degrees of freedom 6 and denom-             distribution" model assumptions and give a
        inator degrees of freedom 12                          90% upper prediction bound for the stretch
    (f) the .25 quantile of the F distribution with           of an additional piece of Brand B line.
        numerator degrees of freedom 6 and denom-
        inator degrees of freedom 12                     (b) Again considering only Brand B, use "normal
                                                              distribution" model assumptions and give a
9. Ho, Lewer, Peterson, and Riegel worked with the            95% upper tolerance bound for stretch mea-
   lack of flatness in a particular kind of manufac-          surements of 90% of such pieces of Brand B
   tured steel disk. Fifty different parts of this type       line.
   were measured for what the students called "wob-
   ble," with the results that the 50 (positive) values  (c) Again considering only Brand B, use "nor-
                                                              mal distribution" model assumptions and give
                                                              90% two-sided confidence intervals for the
430 Chapter 6 Introduction to Formal Statistical Inference

          mean and for the standard deviation of the            (c) Make a normal plot of the transformed values
          Brand B stretch distribution.                              and verify that it is very linear.
     (d) Compare the Brand B and Brand D stan-
          dard deviations of stretch using an appropriate       (d) Make a 95% two-sided prediction interval for
          90% two-sided confidence interval.                         the next transformed purity delivered by this
     (e) Compare the Brand B and Brand D mean                        supplier. What does this "untransform" to in
          stretch values using an appropriate 90% two-               terms of raw purity?
          sided confidence interval. Does this interval
          give clear indication of a difference in mean         (e) Make a 99% two-sided tolerance interval for
          stretch values for the two brands?                         95% of additional transformed purities from
      (f) Carry out a formal significance test of the hy-            this supplier. What does this "untransform"
          pothesis that the two brands have the same                 to in terms of raw purity?
          mean stretch values (use a two-sided alter-
          native hypothesis). Does the conclusion you            (f) Suppose that the air products supplier ad-
          reach here agree with your answer to part (e)?             vertises a median purity of at least 99.5%.
                                                                     This corresponds to a median (and therefore
11. The accompanying data are n = 10 daily mea-                      mean) transformed value of at least -1.61.
     surements of the purity (in percent) of oxygen be-              Test the supplier's claim (H0: µy = -1.61)
     ing delivered by a certain industrial air products              against the possibility that the purity is sub-
     supplier. (These data are similar to some given in              standard. Show and carefully label all five
     a November 1990 article in Chemical Engineer-                   steps.
     ing Progress and used in Chapter Exercise 10 of
     Chapter 3.)                                           12. Chapter Exercise 6 of Chapter 3 contains a data
                                                                set on the lifetimes (in numbers of 24 mm deep
99.77 99.66 99.61 99.59 99.55                                   holes drilled in 1045 steel before tool failure) of 12
99.64 99.53 99.68 99.49 99.58                                   D952-II (8 mm) drills. The data there have mean
                                                                y¯ = 117.75 and s = 51.1 holes drilled. Suppose
(a) Make a normal plot of these data. What does                 that a normal distribution can be used to roughly
     the normal plot reveal about the shape of the              describe drill lifetimes.
     purity distribution? ("It is not bell-shaped" is           (a) Give a 90% lower confidence bound for the
     not an adequate answer. Say how its shape                       mean lifetime of drills of this type in this kind
     departs from the normal shape.)                                 of industrial application.
                                                                (b) Based on your answer to (a), do you think a
(b) What statistical "problems" are caused by                        hypothesis test of H0: µ = 100 versus Ha: µ >
     lack of a normal distribution shape for data                    100 would have a large p-value or a small p-
     such as these?                                                  value? Explain.
                                                                (c) Give a 90% lower prediction bound for the
As a way to deal with problems like those from                       next life length of a drill of this type in this
part (b), you might try transforming the original                    kind of industrial application.
data. Next are values of y = ln(y - 99.3) corre-                (d) Give two-sided tolerance limits with 95%
sponding to each of the original data values y,                      confidence for 90% of all life lengths for
and some summary statistics for the transformed                      drills of this type in this kind of industrial
values.                                                              application.
                                                                (e) Give two-sided 90% confidence limits for the
- .76 -1.02  -1.17  -1.24 -1.39                                      standard deviation of life lengths for drills of
-1.08 -1.47  - .97  -1.66 -1.27                                      this type in this kind of industrial application.

y¯ = -1.203 and     sy = .263                              13. M. Murphy recorded the mileages he obtained
                                                                while commuting to school in his nine-year-old
                                                                economy car. He kept track of the mileage for ten
                                                                  Chapter 6 Exercises 431

different tankfuls of fuel, involving gasoline of                   a roll of plastic, with a constant probability ( pH or
two different octanes. His data follow.                             pL) of any particular bag produced being faulty.
                                                                    (a) Give a 95% upper confidence bound for pH.
87 Octane                        90 Octane                          (b) Give a 95% upper confidence bound for pL.
                                                                    (c) Compare pH and pL using an appropriate two-
26.43, 27.61, 28.71,             30.57, 30.91, 31.21,
28.94, 29.30                     31.77, 32.86                            sided 95% confidence interval. Does this in-
                                                                         terval provide a clear indication of a differ-
(a) Make normal plots for these two samples of                           ence in the effectiveness of the machine at
     size 5 on the same set of axes. Does the "equal                     start-up when run at the two speeds? What
     variances, normal distributions" model ap-                          kind of a p-value (big or small) would you
     pear reasonable for describing this situation?                      expect to find in a test of H0: pH = pL versus
                                                                         Ha: pH = pL?
(b) Find sP for these data. What is this quantity                   (d) Use the five-step format and test H0: pH = pL
     measuring in the present context?                                   versus Ha: pH = pL.
                                                               15. Hamilton, Seavey, and Stucker measured resis-
(c) Give a 95% two-sided confidence interval for                    tances, diameters, and lengths for seven copper
     the difference in mean mileages obtainable                     wires at two different temperatures and used these
     under these circumstances using the fuels of                   to compute experimental resistivities for copper
     the two different octanes. From the nature of                  at these two temperatures. Their data follow. The
     this confidence interval, would you expect to                  units are 10-8 m.
     find a large p-value or a small p-value when
     testing H0: µ87 = µ90 versus Ha: µ87 = µ90?               Wire 0.0C 21.8C

(d) Conduct a significance test of H0: µ87 = µ90               1  1.52  1.72
     against the alternative that the higher-octane
     gasoline provides a higher mean mileage.                  2  1.44  1.56

(e) Give 95% lower prediction bounds for the                   3  1.52  1.68
     next mileages experienced, using first 87 oc-
     tane fuel and then 90 octane fuel.                        4  1.52  1.64

(f) Give 95% lower tolerance bounds for 95% of                 5  1.56  1.69
     additional mileages experienced, using first
     87 octane fuel and then 90 octane fuel.                   6  1.49  1.71

14. Eastman, Frye, and Schnepf worked with a com-              7  1.56  1.72

pany that mass-produces plastic bags. They fo-                 (a) Suppose that primary interest here centers on
                                                                    the difference between resistivities at the two
cused on start-up problems of a particular machine                  different temperatures. Make a normal plot of
                                                                    the seven observed differences. Does it appear
that could be operated at either a high speed or a                  that a normal distribution description of the
                                                                    observed difference in resistivities at these
low speed. One part of the data they collected con-                 two temperatures is plausible?

sisted of counts of faulty bags produced in the first          (b) Give a 90% two-sided confidence interval for
                                                                    the mean difference in resistivity measure-
250 manufactured after changing a roll of plastic                   ments for copper wire of this type at 21.8C
                                                                    and 0.0C.
feedstock. The counts they obtained for both low-

and high-speed operation of the machine were 147

faulty ( p^ H  =  147  )  under  high-speed  operation  a  nd
                  250
12 faulty under low-speed operation ( p^ L = 250 12 ).
Suppose that it is sensible to think of the machine

as operating in a physically stable fashion during

the production of the first 250 bags after changing
432 Chapter 6 Introduction to Formal Statistical Inference

     (c) Give a 90% two-sided prediction interval for               Suppose that one wishes to use an interval of
          an additional difference in resistivity mea-           the form x¯ ± with a particular confidence co-
          surements for copper wire of this type at              efficient to estimate the mean µ of a normal dis-
          21.8C and 0.0C.                                        tribution. If it is desirable to have  # for some
                                                                 number # and one can collect data in two stages,
16. The students referred to in Exercise 15 also mea-            it is possible to choose an overall sample size to
     sured the resistivities for seven aluminum wires at         satisfy these criteria as follows. After taking a
     the same temperatures. The 21.8C measurements               small or moderate initial sample of size n1 (n1
     that they obtained follow:                                  must be at least 2 and is typically at least 4 or
                                                                 5), one computes the sample standard deviation
           2.65, 2.83, 2.69, 2.73, 2.53, 2.65, 2.69              of the initial data--say, s1. Then if t is the ap-
                                                                 propriate tn1-1 distribution quantile for producing
     (a) Give a 99% two-sided confidence interval for            the desired (one- or two-sided) confidence, it is
          the mean resistivity value derived from such           necessary to find the smallest integer n such that
          experimental determinations.
                                                                                       n  ts1 2
     (b) Give a 95% two-sided prediction interval for                                           #
          the next resistivity value that would be derived
          from such an experimental determination.               If this integer is larger than n1, then n2 = n -
                                                                 n1 additional observations are taken. (Otherwise,
     (c) Give a 95% two-sided tolerance interval for             n2 = 0.) Finally, with x¯ the sample mean of all the
          99% of resistivity values derived from such            observations (from both the initial and any sub-
          experimental determinations.                           sequent sample), the formula x¯ ± ts1/ n1 + n2
                                                                 (with t still based on n1 - 1 degrees of freedom)
     (d) Give a 95% two-sided confidence interval for            is used to estimate µ.
          the standard deviation of resistivity values de-
          rived from such experimental determinations.              Suppose that in estimating the mean resistance
                                                                 of a production run of resistors, it is desirable to
     (e) How strong is the evidence that there is a real         have the two-sided confidence level be 95% and
          difference in the precisions with which the            the "± part" of the interval no longer than .5 .
          aluminum resistivities and the copper resistiv-        (a) If an initial sample of n1 = 5 resistors pro-
          ities can be measured at 21.8C? (Carry out
          a significance test of H0: copper = aluminum                duces a sample standard deviation of 1.27 ,
          versus Ha: copper = aluminum using the data                 how many (if any) additional resistors should
          of this problem and the 21.8C data of Exer-                 be sampled in order to meet the stated goals?
          cise 15.)                                              (b) If all of the n1 + n2 resistors taken together
                                                                      produce the sample mean x¯ = 102.8 , what
      (f) Again using the data of this exercise and Ex-               confidence interval for µ should be declared?
          ercise 15, give a 90% two-sided confidence
          interval for the ratio copper/aluminum.           18. Example 15 of Chapter 5 concerns some data on
                                                                 service times at a residence hall depot counter.
17. (The Stein Two-Stage Estimation Procedure)                   The data portrayed in Figure 5.21 are decidedly
     One of the most common of all questions faced               nonnormal-looking, so prediction and tolerance
     by engineers planning a data-based study is how             interval formulas based on normal distributions
     much data to collect. The last part of Example 3            are not appropriate for use with these data. How-
     illustrates a rather crude method of producing an           ever, the largest of the n = 65 observed service
     answer to the sample-size question when estima-             times in that figure is 87 sec.
     tion of a single mean is involved. In fact, in such
     circumstances, a more careful two-stage proce-
     dure due to Charles Stein can sometimes be used
     to find appropriate sample sizes.
                                                                      Chapter 6 Exercises 433

     (a) What prediction confidence level can be as-              Improved settings
          sociated with 87 sec as an upper prediction      Gear y1 (mm) y2 (mm)
          bound for a single additional service time?
                                                           1A   .036  .050
     (b) What confidence level can be associated with
          87 sec as an upper tolerance bound for 95%       2A   .040  .054
          of service times?
                                                           3A   .026  .043
19. Caliste, Duffie, and Rodriguez studied the pro-
     cess of keymaking using a manual machine at a         4A   .051  .071
     local lumber yard. The records of two different
     employees who made keys during the study pe-          5A   .034  .043
     riod were as follows. Employee 1 made a total of
     54 different keys, 5 of which were returned as not    6A   .050  .058
     fitting their locks. Employee 2 made a total of 73
     different keys, 22 of which were returned as not      7A   .059  .061
     fitting their locks.
     (a) Give approximate 95% two-sided confidence         8A   .055  .048
          intervals for the long-run fractions of faulty
          keys produced by these two different employ-     9A   .051  .060
          ees.
     (b) Give an approximate 95% two-sided confi-          10A  .050  .033
          dence interval for the difference in long-run
          fractions of faulty keys produced by these two           Original settings
          different employees.                             Gear y1 (mm) y2 (mm)
     (c) Assess the strength of the evidence provided
          in these two samples of a real difference in     1B   .056  .070
          the keymaking proficiencies of these two em-
          ployees. (Test H0: p1 = p2 using a two-sided     2B   .064  .062
          alternative hypothesis.)
                                                           3B   .070  .075
20. The article "Optimizing Heat Treatment with Fac-
     torial Design" by T. Lim (JOM, 1989) discusses        4B   .037  .060
     the improvement of a heat-treating process for
     gears through the use of factorial experimenta-       5B   .054  .071
     tion. To compare the performance of the heat-
     treating process under the original settings of pro-  6B   .060  .070
     cess variables to that using the "improved" set-
     tings (identified through factorial experimenta-      7B   .065  .060
     tion), n1 = n2 = 10 gears were treated under both
     sets of conditions. Then measures of flatness, y1     8B   .060  .060
     (in mm of distortion), and concentricity, y2 (again
     in mm of distortion), were made on each of the        9B   .051  .070
     gears. The data shown were read from graphs in
     the article (and may in some cases differ by per-     10B  .062  .070
     haps ±.002 mm from the original measurements).
                                                           (a) What assumptions are necessary in order to
                                                                make inferences regarding the parameters of
                                                                the y1 (or y2) distribution for the improved
                                                                settings of the process variables?

                                                           (b) Make a normal plot for the improved settings'
                                                                y1 values. Does it appear that it is reasonable
                                                                to treat the improved settings' flatness distri-
                                                                bution as normal? Explain.

                                                           (c) Suppose that the improved settings' flatness
                                                                distribution is normal, and do the following:
                                                                (i) Give a 90% two-sided confidence interval
                                                                for the mean flatness distortion value for gears
                                                                of this type.
                                                                (ii) Give a 90% two-sided prediction interval
                                                                for an additional flatness distortion value.
434 Chapter 6 Introduction to Formal Statistical Inference

          (iii) Give a 95% two-sided tolerance inter-       counts in the accompanying table are the num-
          val for 90% of additional flatness distortion     bers of cars (out of 25 checked) falling into the
          values.                                           four possible categories.
          (iv) Give a 90% two-sided confidence inter-
          val for the standard deviation of flatness dis-                       Underinflated
          tortion values for gears of this type.                                     tires
     (d) Repeat parts (b) and (c) using the improved
          settings' concentricity values, y2, instead of                        None  At Least
          flatness.                                                                   One Tire
     (e) Explain why it is not possible to base formal
          inferences (tests and confidence intervals), for  Overinflated  None  6     5
          comparing the standard deviations of the y1
          and y2 distributions for the improved process     tires At Least One Tire 10 4
          settings, on the sample standard deviations of
          the y1 and y2 measurements from gears 1A               (a) Behne's sample was in all likelihood a con-
          through 10A.                                                venience sample (as opposed to a genuinely
      (f) What assumptions are necessary in order to                  simple random sample) of the cars in the large
          make comparisons between parameters of the                  lot. Does it make sense to argue in this case
          y1 (or y2) distributions for the original and               that the data can be treated as if the sample
          improved settings of the process variables?                 were a simple random sample? On what ba-
     (g) Make normal plots of the y1 data for the                     sis? Explain.
          original settings and for the improved set-
          tings on the same set of axes. Does an "equal          (b) Give a two-sided 90% confidence interval for
          variances, normal distributions" model ap-                  the fraction of all cars in the lot with at least
          pear tenable here? Explain.                                 one underinflated tire.
     (h) Supposing that the flatness distortion distri-
          butions for the original and improved process          (c) Give a two-sided 90% confidence interval for
          settings are adequately described as normal                 the fraction of the cars in the lot with at least
          with a common standard deviation, do the                    one overinflated tire.
          following.
           (i) Use an appropriate significance test to as-       (d) Give a 90% lower confidence bound on the
          sess the strength of the evidence in the data to            fraction of cars in the lot with at least one
          the effect that the improved settings produce               misinflated tire.
          a reduction in mean flatness distortion.
          (ii) Give a 90% lower confidence bound on              (e) Why can't the data here be used with formula
          the reduction in mean flatness distortion pro-              (6.67) of Section 6.5 to make a confidence
          vided by the improved process settings.                     interval for the difference in the fraction of
      (i) Repeat parts (g) and (h) using the y2 values                cars with at least one underinflated tire and
          and concentricity instead of flatness.                      the fraction with at least one overinflated tire?

21. R. Behne measured air pressure in car tires in a        22. The article "A Recursive Partitioning Method for
     student parking lot. Shown here is one summary of           the Selection of Quality Assurance Tests" by Raz
     the data he reported. Any tire with pressure read-          and Bousum (Quality Engineering, 1990) con-
     ing more than 3 psi below its recommended value             tains some data on the fractions of torque convert-
     was considered underinflated, while any tire with           ers manufactured in a particular facility failing a
     pressure reading more than 3 psi above its recom-           final inspection (and thus requiring some rework).
     mended value was considered overinflated. The               For a particular family of four-element convert-
                                                                 ers, about 39% of 442 converters tested were out
                                                                 of specifications on a high-speed operation inlet
                                                                 flow test.
                                                                                      Chapter 6 Exercises 435

     (a) If plant conditions tomorrow are like those               (a) Compare the variabilities of the gripping pres-
          under which the 442 converters were man-                      sures delivered to the two different objects
          ufactured, give a two-sided 98% confidence                    using an appropriate 98% two-sided confi-
          interval for the probability that a given con-                dence interval. Does there appear to be much
          verter manufactured will fail the high-speed                  evidence in the data of a difference between
          inlet flow test.                                              these? Explain.

     (b) Suppose that a process change is instituted in            (b) Supposing that the variabilities of gripping
          an effort to reduce the fraction of converters                pressure delivered by the gripper to the two
          failing the high-speed inlet flow test. If only               different objects are comparable, give a 95%
          32 out of the first 100 converters manufac-                   two-sided confidence interval for the differ-
          tured fail the high-speed inlet flow test, is this            ence in mean gripping pressures delivered.
          convincing evidence that a real process im-
          provement has been accomplished? (Give and               (c) The data here came from the operation of a
          interpret a 90% two-sided confidence interval                 single prototype gripper. Why would you ex-
          for the change in test failure probability.)                  pect to see more variation in measured grip-
                                                                        ping pressures than that represented here if
23. Return to the situation of Chapter Exercise 1 in                    each measurement in a sample were made on
     Chapter 3 and the measured gains of 120 ampli-                     a different gripper? Strictly speaking, to what
     fiers. The nominal/design value of the gain was                    do the inferences in (a) and (b) apply? To the
     10.0 dB; 16 of the 120 amplifiers measured had                     single prototype gripper or to all grippers of
     gains above nominal. Give a 95% two-sided con-                     this design? Discuss this issue.
     fidence interval for the fraction of all such ampli-
     fiers with above-nominal gains.                          25. A sample of 95 U-bolts produced by a small com-
                                                                   pany has thread lengths with a mean of x¯ = 10.1
24. The article "Multi-functional Pneumatic Gripper                (.001 in. above nominal) and s = 3.2 (.001 in.).
     Operating Under Constant Input Actuation Air                  (a) Give a 95% two-sided confidence interval for
     Pressure" by J. Przybyl (Journal of Engineering                    the mean thread length (measured in .001 in.
     Technology, 1988) discusses the performance of a                   above nominal). Judging from this interval,
     6-digit pneumatic robotic gripper. One part of the                 would you expect a small or a large p-value
     article concerns the gripping pressure (measured                   when testing H0: µ = 0 versus Ha: µ = 0?
     by manometers) delivered to objects of different                   Explain.
     shapes for fixed input air pressures. The data given          (b) Use the five-step format of Section 6.2 and
     here are the measurements (in psi) reported for                    assess the strength of the evidence provided
     an actuation pressure of 40 psi for (respectively)                 by the data to the effect that the population
     a 1.7 in. × 1.5 in. × 3.5 in. rectangular bar and a                mean thread length exceeds nominal.
     circular bar of radius 1.0 in. and length 3.5 in.
                                                              26. D. Kim did some crude tensile strength testing on
Rectangular Bar  Circular Bar                                      pieces of some nominally .012 in. diameter wire
                                                                   of various lengths. Below are Kim's measured
        76             84                                          strengths (kg) for pieces of wire of lengths 25 cm
        82             87                                          and 30 cm.
        85             94
        88             80                                     25 cm Lengths           30 cm Lengths
        82             92
                                                              4.00, 4.65, 4.70, 4.50  4.10, 4.50, 3.80, 4.60
                                                              4.40, 4.50, 4.50, 4.20  4.20, 4.60, 4.60, 3.90
436 Chapter 6 Introduction to Formal Statistical Inference

     (a) If one is to make a confidence interval for the          the diameters of 821 particles observed in a bright
          mean measured strength of 25 cm pieces of               field TEM micrograph of a Zircaloy-4 specimen.
          this wire using the methods of Section 6.3,             The sample mean diameter was x¯ = .055 µm, and
          what model assumption must be employed?                 the sample standard deviation of the diameters
          Make a probability plot useful in assessing             was s = .028 µm.
          the reasonableness of the assumption.                   (a) The engineering researchers wished to es-

     (b) Make a 95% two-sided confidence interval for                  tablish from their observation of this single
          the mean measured strength of 25 cm pieces                   specimen the impact of a certain combination
          of this wire.                                                of specimen lot and heat-treating regimen on
                                                                       particle size. Briefly discuss why data such as
     (c) Give a 95% lower confidence bound for the                     the ones summarized have serious limitations
          mean measured strength of 25 cm pieces.                      for this purpose. (Hints: The apparent "sam-
                                                                       ple size" here is huge. But of what is there a
     (d) Make a 95% two-sided prediction interval for                  sample? How widely do the researchers want
          a single additional measured strength for a                  their results to apply? Given this desire, is the
          25 cm piece of wire.                                         "real" sample size really so large?)
                                                                  (b) Use the sample information and give a 98%
     (e) Make a 99% two-sided tolerance interval for                   two-sided confidence interval for the mean di-
          95% of additional measured strengths of                      ameter of particles in this particular Zircaloy-
          25 cm pieces of this wire.                                   4 specimen.
                                                                  (c) Suppose that a standard method of heat treat-
      (f) Consider the statistical interval derived from               ing for such specimens is believed to produce
          the minimum and maximum sample values                        a mean particle diameter of .057 µm. Assess
          for the 25 cm lengths--namely, (4.00, 4.70).                 the strength of the evidence contained in the
          What confidence should be associated with                    sample of diameter measurements to the ef-
          this interval as a prediction interval for a sin-            fect that the specimen's mean particle diam-
          gle additional measured strength? What con-                  eter is different from the standard. Show the
          fidence should be associated with this interval              whole five-step format.
          as a tolerance interval for 95% of additional           (d) Discuss, in the context of part (c), the po-
          measured strengths for 25 cm pieces of this                  tential difference between the mean diameter
          wire?                                                        being statistically different from .057 µm and
                                                                       there being a difference between µ and .057
     (g) In order to make formal inferences about                      that is of practical importance.
          µ25 - µ30 based on these data, what must
          you be willing to use for model assumptions?       28. Return to Kim's tensile strength data given in Ex-
          Make a plot useful for investigating the rea-           ercise 26.
          sonableness of those assumptions.                       (a) Operating under the assumption that mea-
                                                                       sured tensile strengths of 25 cm lengths of
     (h) Proceed under the assumptions discussed in                    the wire studied are normally distributed, give
          part (g) and assess the strength of the evi-                 a two-sided 98% confidence interval for the
          dence provided by Kim's data to the effect                   standard deviation of measured strengths.
          that an increase in specimen length produces            (b) Operating under the assumption that mea-
          a decrease in measured strength.                             sured tensile strengths of 30 cm lengths of the
                                                                       wire studied are normally distributed, give a
      (i) Proceed under the necessary model assump-                    95% upper confidence bound for the standard
          tions to give a 98% two-sided confidence in-                 deviation of measured strengths.
          terval for µ25 - µ30.

27. The article "Influence of Final Recrystallization
     Heat Treatment on Zircaloy-4 Strip Corrosion"
     by Foster, Dougherty, Burke, Bates, and Worces-
     ter (Journal of Nuclear Materials, 1990) reported
     some summary statistics from the measurement of
                                                            Chapter 6 Exercises 437

     (c) Operating under the assumption that both 25                  is a 95% confidence interval.) How does this
          and 30 cm lengths of the wire have normally                 number compare to the lower end point of
          distributed measured tensile strengths, assess              your interval from (a)?
          the strength of Kim's evidence that 25 and             (c) Repeat (a) using 90% confidence. How does
          30 cm lengths differ in variability of their                this interval compare with the one from (a)?
          measured tensile strengths. (Use H0: 25 =              (d) Repeat (b) using 90% confidence. How does
          30 and Ha: 25 = 30 and show the whole                       this bound compare to the one found in (b)?
          five-step format.)                                     (e) Interpret your interval from (a) for someone
                                                                      with little statistical background. (Speak in
     (d) Operating under the assumption that both 25                  the context of the drilling study and use the
          and 30 cm lengths produce normally dis-                     "authorized interpretation" of confidence as
          tributed tensile strengths, give a 98% two-                 your guide.)
          sided confidence interval for the ratio 25/30.          (f) Based on your confidence intervals, would
                                                                      you expect the p-value in a test of H0: µ =
29. Find the following quantiles:                                     .0210 versus Ha: µ = .0210 to be small? Ex-
     (a) the .99 quantile of the 42 distribution                      plain.
     (b) the .025 quantile of the 42 distribution                (g) Based on your confidence intervals, would
     (c) the .99 quantile of the F distribution with                  you expect the p-value in a test of H0: µ =
          numerator degrees of freedom 3 and denom-                   .0210 versus Ha: µ > .0210 to be small? Ex-
          inator degrees of freedom 15                                plain.
     (d) the .25 quantile of the F distribution with             (h) Consider again your answer to part (a). A col-
          numerator degrees of freedom 3 and denom-                   league sees your calculations and says, "Oh,
          inator degrees of freedom 15                                so 95% of the measured diameters would be
                                                                      in that range?" What do you say to this per-
30. The digital and vernier caliper measurements of                   son?
     no. 10 machine screw diameters summarized in                 (i) Use the five step significance-testing format
     Exercise 3 of Section 6.3 are such that for 19 out               of Section 6.2 and assess the strength of the
     of 50 of the screws, there was no difference in                  evidence provided by the data to the effect
     the measurements. Based on these results, give a                 that the process mean diameter differs from
     95% confidence interval for the long-run fraction                the mid-specification of .0210. (Begin with
     of such measurements by the student technician                   H0: µ = .0210 and use Ha: µ = .0210.
     that would produce agreement between the digital             (j) Thus far in this exercise, inference for the
     and vernier caliper measurements.                                mean hole diameter has been of interest. Ex-
                                                                      plain why in practice the variability of di-
31. Duren, Leng, and Patterson studied the drilling of                ameters is also important. The methods of
     holes in a miniature metal part using electrical dis-            Sections 6.1 are not designed for analyzing
     charge machining. Blueprint specifications on a                  distributional spread. Where in Chapter 6 can
     certain hole called for diameters of .0210 ± .0003               you find inference methods for this feature?
     in. The diameters of this hole were measured on 50
     parts with plug gauges and produced x¯ = .02046        32. Return to Babcock's fatigue life testing data in
     and s = .00178. Assume that the holes the stu-              Chapter Exercise 18 of Chapter 3 and for now
     dents measured were representative of the output            focus on the fatigue life data for heat 1.
     of a physically stable drilling process.                    (a) In order to do inference based on this small
     (a) Give a 95% two-sided confidence interval for                 sample, what model assumptions must you
          the mean diameter of holes drilled by this                  employ? What does a normal plot say about
          process.                                                    the appropriateness of these assumptions?
     (b) Give a 95% lower confidence bound for the
          mean diameter of the holes drilled by this
          process. (Find a number, #, so that (#, )
438 Chapter 6 Introduction to Formal Statistical Inference

     (b) Give a 90% two-sided confidence interval for            (b) What assumption must you make in order to
          the mean fatigue life of such specimens from                do formal inference on the mean difference
          this heat.                                                  in dial bore and air spindler gauge measure-
                                                                      ments? Make a plot useful for assessing the
     (c) Give a 90% lower confidence bound for the                    reasonableness of this assumption. Comment
          mean fatigue life of such specimens from this               on what it indicates in this problem.
          heat.
                                                                 (c) Make the necessary assumptions about the
     (d) If you are interested in quantifying the vari-               dial bore and air spindler measurements and
          ability in fatigue lives produced by this heat              assess the strength of the evidence in the data
          of steel, inference for  becomes relevant.                  of a systematic difference between the two
          Give a 95% two-sided confidence interval for                gauges.
           based on display (6.42) of the text.
                                                                 (d) Make a 95% two-sided confidence interval
     (e) Make a 90% two-sided prediction interval for                 for the mean difference in dial bore and air
          a single additional fatigue life for a specimen             spindler measurements.
          from this heat.
                                                                 (e) Briefly discuss how your answers for parts (c)
      (f) Make a 95% two-sided tolerance interval for                 and (d) of this problem are consistent.
          90% of additional fatigue lives for specimens
          from this heat. How does this interval com-       34. Chapter Exercise 20 in Chapter 3 concerned the
          pare to your interval from (e)?                        drilling of holes in miniature metal parts using
                                                                 laser drilling and electrical discharge machining.
     (g) Now consider the statistical interval derived           Return to that problem and consider first only the
          from the minimum and maximum sample val-               EDM values.
          ues from heat 1, namely (11, 548). What con-           (a) In order to use the methods of inference of
          fidence should be associated with this interval             Section 6.3 with these data, what model as-
          as a prediction interval for a single additional            sumptions must be made? Make a plot useful
          fatigue life from this heat? What confidence                for investigating the appropriateness of those
          should be associated with the interval as a tol-            assumptions. Comment on the shape of that
          erance interval for 90% of additional fatigue               plot and what it says about the appropriate-
          lives?                                                      ness of the model assumptions.
                                                                 (b) Give a 99% two-sided confidence interval for
     Now consider both the data for heat 1 and the data               the mean angle produced by the EDM drilling
     for heat 3.                                                      of this hole.
     (h) In order to make formal inferences about                (c) Give a 99% upper confidence bound for the
                                                                      mean angle produced by the EDM drilling of
          µ1 - µ3 based on these data, what must be                   this hole.
          assumed about fatigue lives for specimens              (d) Give a 95% two-sided confidence interval for
          from these two heats? Make a plot useful for                the standard deviation of angles produced by
          investigating the reasonableness of these as-               the EDM drilling of this hole.
          sumptions.                                             (e) Make a 99% two-sided prediction interval
      (i) Under the appropriate assumptions (state                    for the next measured angle produced by the
          them), give a 95% two-sided confidence in-                  EDM drilling of this hole.
          terval for µ1 - µ3.                                     (f) Make a 95% two-sided tolerance interval for
33. Consider the Notch/Dial Bore and Notch/Air                        99% of angles produced by the EDM drilling
     Spindler measurements on ten servo sleeves re-                   of this hole.
     corded in Chapter Exercise 19 in Chapter 3.                 (g) Consider the statistical interval derived from
     (a) If one wishes to compare the dial bore gauge                 the minimum and maximum sample EDM
          and the air spindler gauge measurements, the
          methods of formulas (6.35), (6.36), and (6.38)
          are not appropriate. Why?
                                                                           Chapter 6 Exercises 439

          values, namely (43.2, 46.1). What confidence      roll over on their sides. "Tilttable ratios" (which
          should be associated with this interval as        are the tangents of the angles at which lift-off
          a prediction interval for a single additional     occurred) were measured for two minivans of dif-
          measured angle? What confidence should be         ferent makes four times each with the following
          associated with this interval as a tolerance in-  results.
          terval for 99% of additional measured angles?
     Now consider both the EDM and initial set of           Van 1          Van 2
     Laser values in Chapter Exercise 20 of Chapter 3
     (two sets of 13 parts).                                1.096, 1.093,  .962, .970,
     (h) In order to make formal inferences about           1.090, 1.093   .967, .966
          µLaser - µEDM based on these data, what must
          you be willing to use for model assumptions?      (a) If you were to make a confidence interval
          Make a plot useful for investigating the rea-          for the long-run mean measured tilttable ratio
          sonableness of those assumptions.                      for Van 1 (under conditions like those expe-
      (i) Proceed under appropriate assumptions to as-           rienced during the testing) using the methods
          sess the strength of the evidence provided by          of Section 6.3, what model assumption must
          the data that there is a difference in the mean        be made?
          angles produced by the two drilling methods.
      (j) Give a 95% two-sided confidence interval for      (b) Make a 95% two-sided confidence interval for
          µLaser - µEDM.                                         the mean measured tilttable ratio for Van 1 un-
     (k) Give a 90% two-sided confidence interval for            der conditions like those experienced during
          comparing the standard deviations of angles            the testing.
          produced by Laser and EDM drilling of this
          hole.                                             (c) Give a 95% lower confidence bound for the
     Now consider both sets of Laser measurements                mean measured tilttable ratio for Van 1.
     given in Chapter Exercise 20 of Chapter 3. (Holes
     A and B are on the same 13 parts.)                     (d) Give a 95% lower confidence bound for the
      (l) If you wished to compare the mean angle                standard deviation of tilttable ratios for Van 1.
          measurements for the two holes, the formulas
          used in (i) and (j) are not appropriate. Why?     (e) Make a 95% two-sided prediction interval for
     (m) Make a 90% two-sided confidence interval                a single additional measured tilttable ratio for
          for the mean difference in angles for the two          Van 1 under conditions such as those experi-
          holes made with the laser equipment.                   enced during testing.
     (n) Assess the strength of the evidence provided
          by these data that there is a systematic differ-  (f) Make a 99% two-sided tolerance interval for
          ence in the angles of the holes made with the          95% of additional measured tilttable ratios for
          laser equipment.                                       Van 1.
     (o) Briefly discuss why your answers to parts (m)
          and (n) of this exercise are compatible. (Dis-    (g) Consider the statistical interval derived from
          cuss how the outcome of part (n) could have            the minimum and maximum sample values
          been anticipated from the outcome of part              for Van 1, namely (1.090, 1.096). What con-
          (m).)                                                  fidence should be associated with this inter-
                                                                 val as a prediction interval for a single ad-
35. A so-called "tilttable" test was run in order to             ditional measured tilttable ratio? What confi-
     determine the angles at which certain vehicles ex-          dence should be associated with this interval
     perience lift-off of one set of wheels and begin to         as a tolerance interval for 95% of additional
                                                                 tilttable test results for Van 1?

                                                            Now consider the data for both vans.
                                                            (h) In order to make formal inferences about

                                                                 µ1 - µ2 based on these data, what must you
                                                                 be willing to use for model assumptions?
440 Chapter 6 Introduction to Formal Statistical Inference

(i) Proceed under the necessary assumptions to                (k) Proceed under the necessary model assump-
    assess the strength of the evidence provided                   tions to give a 90% two-sided confidence in-
    by the data that there is a difference in mean                 terval for 1/2.
    measured tilttable ratios for the two vans.

(j) Proceed under the necessary model assump-
    tions to give a 90% two-sided confidence in-
    terval for µ1 - µ2.

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

Chapter 6 Summary Tables

                    The methods presented in Chapter 6 can seem overwhelming in their variety. It is
                    sometimes helpful to have a summary of them. The tables here give such a summary
                    and can be used to help you locate methods appropriate in a particular problem or
                    application.

Table 1
Inference Methods for Individual Values

Inference For                            Assumptions          Interval                                    Section
                                                                                                          6.6
xn+1 (a single additional value)                              (min(x1, . . . , xn), max(x1, . . . , xn))  6.6
                                                              or (min(x1, . . . , xn), )                  6.6
most of the distribution                 observations normal  or (-, max(x1, . . . , xn))                 6.6
                                         observations normal
                                                              x¯ ± ts 1 + 1
                                                                             n

                                                              (min(x1, . . . , xn), max(x1, . . . , xn))
                                                              or (min(x1, . . . , xn), )
                                                              or (-, max(x1, . . . , xn))

                                                              x¯ ± 2s
                                                              or (x¯ - 1s, )
                                                              or (-, x¯ + 1s)
                                                                       Chapter 6 Summary Tables 441

Table 2                                      H0, Test Stat, Reference            Interval  Section
Inference Methods for One and Two Means                                        s           6.1, 6.2
Inference For Sample Size Assumptions        H0 : µ = #                x¯ ± z 
                                                   x¯ - #                       n          6.3
µ (one mean) large n
                                             Z=                                s
             small n         observations          s/ n                x¯ ± t 
                             normal
                                             standard normal                    n

                                             H0 : µ = #
                                                   x¯ - #

                                             T= 
                                                   s/ n

                                             t with  = n - 1

µ1 - µ2      large n1, n2    independent     H0 : µ1 - µ2 = #                           s2 s2
(difference                  samples                                   x¯ 1 - x¯ 2 ± z 1 + 2 6.3
                                             Z = x¯ 1 - x¯ 2 - #
in means)                                                                               n1 n2
                                                            s2 s2

                                                       1 n1 + 2 n2
                                             standard normal

             small n1 or n2  independent     H0 : µ1 - µ2 = #                              11
                             normal samples                            x¯ 1 - x¯ 2 ± tsP + 6.3

                             1 = 2                                                        n1 n2

                                             T = x¯ 1 - x¯ 2 - #
                                             sP n1 + n1
                                             1         2

                                             t with  = n1 + n2 - 2

                             possibly 1 = 2                                             s2 s2
                                                                       x¯ 1 - x¯ 2 ± t^ 1 + 2 6.3
µd           large n         (paired data)   H0 : µd = #
(mean                                                                                   n1 n2
                             (paired data)          d¯ - #
difference)                  normal          Z=                          use random ^ given in (6.37)
                             differences
                                                   sd / n              d¯ ± z sdn 6.3
                                             standard normal
             small n                                                   d¯  ±  t  sdn       6.3
                                             H0 : µd = #                         

                                                    d¯ - #
                                             T= 

                                                   sd / n
                                             t with  = n - 1
442 Chapter 6 Introduction to Formal Statistical Inference

Table 3
Inference Methods for Variances

Inference For            Assumptions               H0, Test Stat, Reference                        Interval              Section
                                                                                                                           6.4
 2 (one variance)        observations normal       H0 :  2 = #
                                                   X 2 = (n - 1)s2                                                          6.4
                                                                             (n - 1)s2             and/or    (n - 1)s2
                                                                #
                                                    2 with  = n - 1                        U                 L

12/22 (variance ratio) observations normal              12
                              independent samples  H0 : 2 = #

                                                        2                                  s12 s12
                                                                                                 2 and/or 2
                                                   F = s12/s22                             U · s2            L · s2
                                                            #

                                                   F with 1 = n1 - 1
                                                   and 2 = n2 - 1

Table 4
Inference Methods for Proportions

Inference       Sample   Assumptions       H0, Test Stat,                                     Interval                         Section
For             Size                        Reference
                                            H0 : p = #
p (one          large n
proportion)                             Z = p^ - #
                                                 #(1 - #)
                                                      n                      p^ ± z p^ (1 - p^ ) 6.5
                                                                                           n
                                        standard normal                                    1

                                        H0 : p1 - p2 = 0                       or p^ ± z 
                                                                                         2n
                                      Z =  p^ 1 - p^ 2
p1 - p2         large
                                                   p^ (1- p^ ) n11 + n12
difference n1, n2                          use p^ given in (6.71)

in proportions           independent    standard normal                   p^ 1 - p^ 2 ± z     p^ 1(1 - p^ 1) + p^ 2(1 - p^ 2)
                         samples
                                                                                                   n1                n2

                                                                                                       11 1
                                                                          or p^ 1 - p^ 2 ± z ·               +                 6.5
                                                                                                       2 n1 n2
7q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

         Inference for
         Unstructured
         Multisample Studies

         Chapter 6 introduced the basics of formal statistical inference in one- and two-

                      sample studies. This chapter begins to consider formal inference for multisample
                      studies, with a look at methods that make no explicit use of structure relating the
                      samples (beyond time order of data collection). That is, the study of inference
                      methods specifically crafted for use in factorial and fractional factorial studies and
                      in curve- and surface-fitting analyses will be delayed until subsequent chapters.

                           The chapter opens with a discussion of the standard one-way model typically
                      used in the analysis of measurement data from multisample studies and of the role
                      of residuals in judging its appropriateness. The making of confidence intervals in
                      multisample contexts is then considered, including both individual and simultane-
                      ous confidence interval methods. The one-way analysis of variance (ANOVA) test
                      for the hypothesis of equality of several means and a related method of estimating
                      variance components are introduced next. The chapter then covers the basics of
                      Shewhart control (or process monitoring) charts. The x¯ , R, and s control charts for
                      measurement data are studied. The chapter then closes with a section on p charts
                      and u charts for attributes data.

     qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

 7.1 The One-Way Normal Model

                      Statistical engineering studies often produce samples taken under not one or two,
                      but rather many different sets of conditions. So although the inference methods
                      of Chapter 6 are a start, they are not a complete statistical toolkit for engineering
                      problem solving. Methods of formal inference appropriate to multisample studies
                      are also needed.

                                                                                                                           443
444 Chapter 7 Inference for Unstructured Multisample Studies

                                       This section begins to provide such methods. First the reader is reminded of the
                                  usefulness of some of the simple graphical tools of Chapter 3 for making informal
                                  comparisons in multisample studies. Next the "equal variances, normal distribu-
                                  tions" model is introduced. The role of residuals in evaluating the reasonableness
                                  of that model in an application is explained and emphasized. The section then pro-
                                  ceeds to introduce the notion of combining several sample variances to produce a
                                  single pooled estimate of baseline variation. Finally, there is a discussion of how
                                  standardized residuals can be helpful when sample sizes vary considerably.

7.1.1      Graphical Comparison of Several Samples
           of Measurement Data

           Any thoughtful analysis of several samples of engineering measurement data should
           begin with the making of graphical representations of those data. Where samples
           are small, side-by-side dot diagrams are the most natural graphical tool. Where
           sample sizes are moderate to large (say, at least six or so data points per sample),
           side-by-side boxplots are effective.

Example 1  Comparing Compressive Strengths for Eight Different Concrete Formulas

           Armstrong, Babb, and Campen did compressive strength testing on 16 different
           concrete formulas. Part of their data are given in Table 7.1, where eight different

                                       6000

           Compressive strength (psi)  5000

                                       4000

                                       3000

                                       2000

                                       1000

                                             1  2  3  4  5           6  7                            8

                                                   Concrete formula

                                             Figure 7.1 Side-by-side dot diagrams for eight samples
                                             of compressive strengths
       7.1 The One-Way Normal Model 445

Table 7.1
Compressive Strengths for 24 Concrete Specimens

Specimen Concrete Formula 28-Day Compressive Strength (psi)

1   1                                            5,800
                                                 4,598
2   1                                            6,508
                                                 5,659
3   1                                            6,225
                                                 5,376
4   2                                            5,093
                                                 4,386
5   2                                            4,103
                                                 3,395
6   2                                            3,820
                                                 3,112
7   3                                            3,820
                                                 2,829
8   3                                            2,122
                                                 2,971
9   3                                            3,678
                                                 3,325
10  4                                            2,122
                                                 1,372
11  4                                            1,160
                                                 2,051
12  4                                            2,631
                                                 2,490
13  5

14  5

15  5

16  6

17  6

18  6

19  7

20  7

21  7

22  8

23  8

24  8

formulas are represented. (The only differences between formulas 1 through 8
are their water/cement ratios. Formula 1 had the lowest water/cement ratio, and
the ratio increased with formula number in the progression .40, .44, .49, .53,
.58, .62, .66, .71. Of course, knowing these water/cement ratios suggests that a
curve-fitting analysis might be useful with these data, but for the time being this
possibility will be ignored.)

     Making side-by-side dot diagrams for these eight samples of sizes n1 = n2 =
n3 = n4 = n5 = n6 = n7 = n8 = 3 amounts to making a scatterplot of compres-
sive strength versus formula number. Such a plot is shown in Figure 7.1. The
general message conveyed by Figure 7.1 is that there are clear differences in
mean compressive strengths between the formulas but that the variabilities in
compressive strengths are roughly comparable for the eight different formulas.
446 Chapter 7 Inference for Unstructured Multisample Studies

Example 2  Comparing Empirical Spring Constants
           for Three Different Types of Springs

           Hunwardsen, Springer, and Wattonville did some testing of three different types
           of steel springs. They made experimental determinations of spring constants for
           n1 = 7 springs of type 1 (a 4 in. design with a theoretical spring constant of
           1.86), n2 = 6 springs of type 2 (a 6 in. design with a theoretical spring constant
           of 2.63), and n3 = 6 springs of type 3 (a 4 in. design with a theoretical spring
           constant of 2.12), using an 8.8 lb load. The students' experimental values are
           given in Table 7.2.

                These samples are just barely large enough to produce meaningful boxplots.
           Figure 7.2 gives a side-by-side boxplot representation of these data. The primary
           qualitative message carried by Figure 7.2 is that there is a substantial difference in
           empirical spring constants between the 6 in. spring type and the two 4 in. spring
           types but that no such difference between the two 4 in. spring types is obvious.
           Of course, the information in Table 7.2 could also be presented in side-by-side
           dot diagram form, as in Figure 7.3.

           Table 7.2
           Empirical Spring Constants

           Type 1 Springs              Type 2 Springs                                           Type 3 Springs

           1.99, 2.06, 1.99            2.85, 2.74, 2.74                                         2.10, 2.01, 1.93
           1.94, 2.05, 1.88            2.63, 2.74, 2.80                                         2.02, 2.10, 2.05
           2.30

           2.5Experimental spring constantType 2                                                2.5                Type 2
                                                                  Experimental spring constant
                              springs                                                                              springs

           2.0                         Type 3                                                   2.0                         Type 3
                                       springs                                                                              springs
                     Type 1                                                                               Type 1
                     springs                                                                              springs

           Figure 7.2 Side-by-side boxplots of                                                  Figure 7.3 Side-by-side dot
           empirical spring constants for springs                                               diagrams for three samples of
           of three types                                                                       empirical spring constants
                                                                           7.1 The One-Way Normal Model 447

                           Methods of formal statistical inference are meant to sharpen and quantify the
                      impressions that one gets when making a descriptive analysis of data. But an intel-
                      ligent graphical look at data and a correct application of formal inference methods
                      rarely tell completely different stories. Indeed, the methods of formal inference of-
                      fered here for simple, unstructured multisample studies are confirmatory--in cases
                      like Examples 1 and 2, they should confirm what is clear from a descriptive or
                      exploratory look at the data.

               7.1.2  The One-Way (Normal) Multisample Model,
                      Fitted Values, and Residuals
   One-way normal
model assumptions     Chapter 6 emphasized repeatedly that to make one- and two-sample inferences,
                      one must adopt a model for data generation that is both manageable and plausible.
                      The present situation is no different, and standard inference methods for unstruc-
                      tured multisample studies are based on a natural extension of the model used in
                      Section 6.3 to support small-sample comparison of two means. The present dis-
                      cussion will be carried out under the assumption that r samples of respective sizes
                      n1, n2, . . . , nr are independent samples from normal underlying distributions with a
                      common variance--say,  2. Just as in Section 6.3 the r = 2 version of this one-way
                      (as opposed, for example, to several-way factorial) model led to useful inference
                      methods for µ1 - µ2, this general version will support a variety of useful infer-
                      ence methods for r -sample studies. Figure 7.4 shows a number of different normal
                      distributions with a common standard deviation. It represents essentially what must
                      be generating measured responses if the methods of this chapter are to be applied.

                           In addition to a description of the one-way model in words and the pictorial
                      representation given in Figure 7.4, it is helpful to have a description of the model in
                      symbols. This and the next three sections will employ the notation

                                      yi j = the j th observation in sample i

One-way model         The model equation used to specify the one-way model is then                  (7.1)
    statement in                                               yi j = µi + i j
           symbols

                                      Distribution 1                                Distribution 2
                                                   Distribution r
                      Distribution 3

                                      µ3 µ1 µr                     µ2

                      Figure 7.4 r normal distributions with a common standard deviation
448 Chapter 7 Inference for Unstructured Multisample Studies

                                  where µi is the i th underlying mean and the quantities 11, 12, . . . , 1n1 , 21, 22, . . . ,
                                   2n2 , . . . , r1, r2, . . . , rnr are independent normal random variables with mean 0
                                  and variance  2. (In this statement, the means µ1, µ2, . . . , µr and the variance  2
                                  are typically unknown parameters.)

                                       Equation (7.1) says exactly what is conveyed by Figure 7.4 and the statement
                                  of the one-way assumptions in words. But it says it in a way that is suggestive of
                                  another useful pattern of thinking, reminiscent of the "residual" notion that was
                                  used extensively in Sections 4.1, 4.2, and 4.3. That is, equation (7.1) says that an
                                  observation in sample i is made up of the corresponding underlying mean plus some
                                  random noise, namely

                                                                             i j = yi j - µi

                                  This is a theoretical counterpart of an empirical notion met in Chapter 4. There, it
                                  was useful to decompose data into fitted values and the corresponding residuals.

                                       In the present situation, since any structure relating the r different samples is
                                  specifically being ignored, it may not be obvious how to apply the notions of fitted
                                  values and residuals. But a plausible meaning for

                                                           y^ i j = the fitted value corresponding to yi j

                                  in the present context is the ith sample mean

ith sample mean                   1 ni
                           y¯ i = ni j=1 yi j

                 That is,

Fitted values              y^ i j = y¯ i                              (7.2)
for the one-
  way model

                 (This is not only intuitively plausible but also consistent with what was done in
                 Sections 4.1 and 4.2. If one fits the approximate relationship yi j  µi to the data via
                 least squares--i.e., by minimizing i j (yi j - µi )2 over choices of µ1, µ2, . . . , µr --
                 each minimizing value of µi is y¯ i .)

                      Taking equation (7.2) to specify fitted values for an r -sample study, the pattern

                 established in Chapter 4 (specifically, Definition 4, page 132) then says that residuals

                 are differences between observed values and sample means. That is, with

                           ei j = the residual corresponding to yi j
                         7.1 The One-Way Normal Model 449

                one has

Residuals for            ei j = yi j - y^ i j = yi j - y¯ i       (7.3)
the one-way

         model

                Rearranging display (7.3) gives the relationship

                         yi j = y^ i j + ei j = y¯ i + ei j       (7.4)

                which is an empirical counterpart of the theoretical statement (7.1). In fact, combin-
                ing equations (7.1) and (7.4) into a single statement gives

                         yi j = µi + i j = y¯ i + ei j            (7.5)

                This is a specific instance of a pattern of thinking that runs through all of the common
                normal-distribution-based methods of analysis for multisample studies. In words,
                equation (7.5) says

                Observation = deterministic response + noise = fitted value + residual (7.6)

                and display (7.6) is a paradigm that provides a unified way of approaching the
                majority of the analysis methods presented in the rest of this book.

                     The decompositions (7.5) and (7.6) suggest that

                1. the fitted values (y^ i j = y¯ i ) are meant to approximate the deterministic part
                    of a system response (µi ), and

                2. the residuals (ei j ) are therefore meant to approximate the corresponding
                    noise in the response ( i j ).

                The fact that the i j in equation (7.1) are assumed to be iid normal (0,  2) random
                variables then suggests that the ei j ought to look at least approximately like a random
                sample from a normal distribution.

                     So the normal-plotting of an entire set of residuals (as in Chapter 4) is a way
                of checking on the reasonableness of the one-way model. The plotting of residuals
                against (1) fitted values, (2) time order of observation, or (3) any other potentially
                relevant variable--hoping (as in Chapter 4) to see only random scatter--are other
                ways of investigating the appropriateness of the model assumptions.

                     These kinds of plotting, which combine residuals from all r samples, are often
                especially useful in practice. When r is large at all, budget constraints on total data
                collection costs often force the individual sample sizes n1, n2, . . . , nr to be fairly
                small. This makes it fruitless to investigate "single variance, normal distributions"
                model assumptions using (for example) sample-by-sample normal plots. (Of course,
                where all of n1, n2, . . . , nr are of a decent size, a sample-by-sample approach can
                be effective.)
450 Chapter 7 Inference for Unstructured Multisample Studies

Example 1     Returning again to the concrete strength study, consider investigating the reason-
(continued )  ableness of model (7.1) for this case. Figure 7.1 is a first step in this investigation.
              As remarked earlier, it conveys the visual impression that at least the "equal
              variances" part of the one-way model assumptions is plausible. Next, it makes
              sense to compute some summary statistics and examine them, particularly the
              sample standard deviations. Table 7.3 gives sample sizes, sample means, and
              sample standard deviations for the data in Table 7.1.

                   At first glance, it might seem worrisome that in this table s1 is more than three
              times the size of s8. But the sample sizes here are so small that a largest ratio of

              Table 7.3
              Summary Statistics for the Concrete Strength Study

                  i,      ni ,         y¯ i ,                              si ,
              Concrete  Sample      Sample                         Sample Standard
              Formula
                         Size     Mean (psi)                        Deviation (psi)

                   1    n1 = 3 y¯ 1 = 5,635.3                       s1 = 965.6

                   2    n2 = 3 y¯ 2 = 5,753.3                       s2 = 432.3

                   3    n3 = 3 y¯ 3 = 4,527.3                       s3 = 509.9

                   4    n4 = 3 y¯ 4 = 3,442.3                       s4 = 356.4

                   5    n5 = 3 y¯ 5 = 2,923.7                       s5 = 852.9

                   6    n6 = 3 y¯ 6 = 3,324.7                       s6 = 353.5

                   7    n7 = 3 y¯ 7 = 1,551.3                       s7 = 505.5

                   8    n8 = 3 y¯ 8 = 2,390.7                       s8 = 302.5

              Table 7.4
              Example Computations of Residuals for the Concrete Strength Study

              Specimen      i,          yi j ,                     y^ i j = y¯ i ,     ei j ,
                        Concrete  Compressive                       Fitted          Residual
                        Formula
                                  Strength (psi)                    Value

                1       1         5,800                            5,635.3          164.7

                2       1         4,598                            5,635.3 -1,037.3

                3       1         6,508                            5,635.3          872.7

                4       2         5,659                            5,753.3          -94.3

                5       2         6,225                            5,753.3          471.7

              ...       ...                                   ...  ...               ...

              22        8         2,051                            2,390.7          -339.7

              23        8         2,631                            2,390.7          240.3

              24        8         2,490                            2,390.7          99.3
                                                 7.1 The One-Way Normal Model 451

sample standard deviations on the order of 3.2 is hardly unusual (for r = 8 sam-
ples of size 3 from a normal distribution). Note from the F tables (Tables B.6)
that for samples of size 3, even if only 2 (rather than 8) sample standard de-
viations were involved, a ratio of sample variances of (965.6/302.5)2  10.2
would yield a p-value between .10 and .20 for testing the null hypothesis
of equal variances with a two-sided alternative. The sample standard devia-
tions in Table 7.3 really carry no strong indication that the one-way model
is inappropriate.

     Since the individual sample sizes are so small, trying to see anything useful
in eight separate normal plots of the samples is hopeless. But some insight can
be gained by calculating and plotting all 8 × 3 = 24 residuals. Some of the
calculations necessary to compute residuals for the data in Table 7.1 (using the
fitted values appearing as sample means in Table 7.3) are shown in Table 7.4.
Figures 7.5 and 7.6 are, respectively, a plot of residuals versus fitted y (ei j versus
y¯ i j ) and a normal plot of all 24 residuals.

                          Residual, eij   700
                                             0

                                         -700

                                                1600  2400 3200 4000 4800               5600
                                                       Fitted strength, yij = yi (psi)

                               Figure 7.5 Plot of residuals versus fitted responses for
                               the compressive strengths

Standard normal quantile  3.0

                          1.5                                                  2

                           0.0

                                                                  2

                          -1.5

                          -1050 -700                  -350           0  350             700 1050

                                                      Residual quantile (psi)

                          Figure 7.6 Normal plot of the compressive strength residuals
452 Chapter 7 Inference for Unstructured Multisample Studies

Example 1          Figure 7.5 gives no indication of any kind of strong dependence of  on
(continued )  µ (which would violate the "constant variance" restriction). And the plot in
              Figure 7.6 is reasonably linear, thus identifying no obvious difficulty with the
              assumption of normal distributions. In all, it seems from examination of both the
              raw data and the residuals that analysis of the data in Table 7.1 on the basis of
              model (7.1) is perfectly sensible.

Example 2     The spring testing data can also be examined with the potential use of the one-way
(continued )  normal model (7.1) in mind. Figures 7.2 and 7.3 indicate reasonably comparable
              variabilities of experimental spring constants for the r = 3 different spring types.
              The single very large value (for spring type 1) causes some doubt both in terms of
              this judgment and also (by virtue of its position on its boxplot as an outlying value)
              regarding a "normal distribution" description of type 1 experimental constants.
              Summary statistics for these samples are given in Table 7.5.

              Table 7.5
              Summary Statistics for the Empirical
              Spring Constants

              i, Spring Type ni y¯ i                          si

              1                                               7 2.030 .134

              2                                               6 2.750 .074

              3                                               6 2.035 .064

                   Without the single extreme value of 2.30, the first sample standard deviation

              would be .068, completely in line with those of the second and third samples.

              But even the observed ratio of largest to smallest sample variance (namely
              (.134/.064)2 = 4.38) is not a compelling reason to abandon a one-way model
              description of the spring constants. (A look at the F tables with 1 = 6 and 2 = 5
              shows that 4.38 is between the F6,5 distribution .9 and .95 quantiles. So even if
              there were only two rather than three samples involved, a variance ratio of 4.38

              would yield a p-value between .1 and .2 for (two-sided) testing of equality of

              variances.) Before letting the single type 1 empirical spring constant of 2.30 force

              abandonment of the highly tractable model (7.1) some additional investigation

              is warranted.
                   Sample sizes n1 = 7 and n2 = n3 = 6 are large enough that it makes sense

              to look at sample-by-sample normal plots of the spring constant data. Such plots,

              drawn on the same set of axes, are shown in Figure 7.7. Further, use of the fitted
              values (y¯ i ) listed in Table 7.5 with the original data given in Table 7.2 produces
     Standard normal quantile                                    7.1 The One-Way Normal Model 453

                                                                                              Type 1 springs
                                                                                              Type 2 springs
                                                                                              Type 3 springs

                               1.0

                               0

                               -1.0

                                          2.0           2.5                            3.0

                                          Experimental spring constant quantile

     Figure 7.7 Normal plots of empirical spring constants for springs
     of three types

Table 7.6
Example Computations of Residuals for the Spring Constant Study

      i,                                   j,            yi j ,       y^ i j = y¯ i ,     ei j ,
Spring Type                          Observation  Spring Constant  Sample Mean         Residual

                                       Number                                           -.040
                                                                                           ..
1                                    1            1.99             2.030                   .
                                                                                          .270
...                                  ...           ...             ...                    .100
                                                                                           ..
1                                    7            2.30             2.030                   .
                                                                                          .050
2                                    1            2.85             2.750                  .065
                                                                                           ..
...                                  ...          ...              ...                     .
                                                                                          .015
2                                    6            2.80             2.750

3                                    1            2.10             2.035

...                                  ...          ...              . ..

3                                    6            2.05             2.035

19 residuals, as partially illustrated in Table 7.6. Then Figures 7.8 and 7.9, re-
spectively, show a plot of residuals versus fitted responses and a normal plot of
all 19 residuals.
454 Chapter 7 Inference for Unstructured Multisample Studies

Example 2                                              0.30
(continued )

                                        Residual, eij  0.15

                                                                           2

                                                       0.00 2 3

                                                                          22

                                                       -0.15        2.10 2.25 2.40 2.55 2.70
                                                              1.95    Fitted response, yij = yi (psi)

                                        Figure 7.8 Plot of residuals versus fitted responses for
                                        the empirical spring constants

              Standard normal quantile  3.0

                                        1.5

                                        0.0                            3  2

                                                                    2

                                        -1.5

                    -0.160 -0.080 0.000 0.080 0.160 0.240 0.320
                                           Residual quantile

              Figure 7.9 Normal plot of the spring constant residuals

                   But Figures 7.8 and 7.9 again draw attention to the largest type 1 empirical
              spring constant. Compared to the other measured values, 2.30 is simply too large
              (and thus produces a residual that is too large compared to all the rest) to permit
              serious use of model (7.1) with the spring constant data. Barring the possibility
              that checking of original data sheets would show the 2.30 value to be an arithmetic
              blunder or gross error of measurement (which could be corrected or legitimately
              force elimination of the 2.30 value from consideration), it appears that the use of
              model (7.1) with the r = 3 spring types could produce inferences with true (and
              unknown) properties quite different from their nominal properties.

                   One might, of course, limit attention to spring types 2 and 3. There is nothing
              in the second or third samples to render the "equal variances, normal distributions"
              model untenable for those two spring types. But the pattern of variation for
              springs of type 1 appears to be detectably different from that for springs of types
              2 and 3, and the one-way model is not appropriate when all three types are
              considered.
                                                                                    7.1 The One-Way Normal Model 455

7.1.3               A Pooled Estimate of Variance for Multisample Studies

                    The "equal variances, normal distributions" model (7.1) has as a fundamental pa-
                    rameter,  , the standard deviation associated with responses from any of conditions
                    1, 2, 3, . . . , r . Similar to what was done in the r = 2 situation of Section 6.3, it is
                    typical in multisample studies to pool the r sample variances to arrive at a single
                    estimate of  derived from all r samples.

Definition 1        If r numerical samples of respective sizes n1, n2, . . . , nr produce sample

                    variances  s  2  ,  s  2  ,  .  .  .  ,  sr2  ,  the  pooled    sample  variance,   sP2,     is  the  weighted
                                  1        2

                    average of the sample variances, where the weights are the sample sizes

                    minus 1. That is,

                                         2 (n1 - 1)s12 + (n2 - 1)s22 + · · · + (nr - 1)sr2                                    (7.7)
                                        sP =

                                                  (n1 - 1) + (n2 - 1) + · · · + (nr - 1)

                    The pooled sample standard deviation, sP, is the square root of sP2.

                         Definition 1 is just Definition 14 in Chapter 6 restated for the case of more than
                    two samples. As was the case for sP based on two samples, sP is guaranteed to lie
                    between the largest and smallest of the si and is a mathematically convenient form
                    of compromise value.

                         Equation (7.7) can be rewritten in a number of equivalent forms. For one thing,
                    letting

The total number    n=                  r        ni    =     the     total  number  of  observations    in  all  r   samples
of observations in                      i =1
an r-sample study

                    it is common to rewrite the denominator on the right of equation (7.7) as

                                                          r                   r         r

                                                             (ni - 1) = ni - 1 = n - r

                                                       i =1                   i =1      i =1

                    And noting that the ith sample variance is

                                                                     si2 = 1        ni

                                                                                       (yi j - y¯ i )2
                                                                            ni - 1 j=1
456 Chapter 7 Inference for Unstructured Multisample Studies

                   the numerator on the right of equation (7.7) is

                                                ni                       r ni

                   r     (ni - 1)  1
                                      (ni - 1)
                                                      (yi j - y¯ i )2 =           (yi j - y¯ i )2  (7.8)
                                                                                                   (7.9)
                   i =1                         j =1                     i=1 j=1

                                                                            r ni

                                                                      = ei j2

                                                                           i=1 j=1

     Alternative   So one can define sP2 in terms of the right-hand side of equation (7.8) or (7.9) divided
formulas for sP2   by n - r .

Example 1          For the compressive strength data, each of n1, n2, . . . , n8 are 3, and s1 through s8
(continued )       are given in Table 7.3. So using equation (7.7),

                            2 (3 - 1)(965.6)2 + (3 - 1)(432.3)2 + · · · + (3 - 1)(302.5)2
                           sP =

                                                 (3 - 1) + (3 - 1) + · · · + (3 - 1)
                              = 2[(965.6)2 + (432.3)2 + · · · + (302.5)2]

                                                         16
                              = 2,705,705

                                       8
                              = 338, 213 (psi)2

                   and thus

                                                  sP = 338,213 = 581.6 psi

                   One estimates that if a large number of specimens of any one of formulas 1
                   through 8 were tested, a standard deviation of compressive strengths on the order
                   of 582 psi would be obtained.

The meaning             sP is an estimate of the intrinsic or baseline variation present in a response
            of sP  variable at a fixed set of conditions, calculated supposing that the baseline variation
                   is constant across the conditions under which the samples were collected. When
                   that supposition is reasonable, the pooling idea allows a number of individually
                   unreliable small-sample estimates to be combined into a single, relatively more
                   reliable combined estimate. It is a fundamental measure that figures prominently in
                   a variety of useful methods of formal inference.
                                                                     7.1 The One-Way Normal Model 457

                        On occasion, it is helpful to have not only a single number as a data-based best
                   guess at  2 but a confidence interval as well. Under model restrictions (7.1), the

                   variable

                                                              (n - r )sP2
                                                                  2

                   has  a    2  distribution.  Thus,  in  a  manner  exactly  parallel  to  the  derivation  in  Section

                           n-r
                   6.4, a two-sided confidence interval for  2 has endpoints

Confidence limits                                (n - r )sP2 and (n - r )sP2                                     (7.10)
 for the one-way                                       U                      L
  model variance

                   where   L  and  U  are  such  that  the     2     probability  assigned  to   the  interval   (L , U )

                                                             n-r

                   is the desired confidence level. And, of course, a one-sided interval is available by

                   using   only  one  of  the  endpoints     (7.10)  and  choosing  U   or  L    such  that  the    2

                   probability assigned to the interval (0, U ) or (L , ) is the desired confidence.              n-r

Example 1          In the concrete compressive strength case, consider the use of display (7.10) in
(continued )
                   making a two-sided 90% confidence interval for  . Since n - r = 16 degrees
                   of freedom are associated with sP2, one consults Table B.5 for the .05 and .95
                   quantiles of the 162 distribution. These are 7.962 and 26.296, respectively. Thus,
                   from display (7.10), a confidence interval for  2 has endpoints

                                                 16(581.6)2          and   16(581.6)2

                                                      26.296                  7.962

                   So a two-sided 90% confidence interval for  has endpoints

                                                 16(581.6)2                   16(581.6)2
                                                                 and             7.962

                                                   26.296

                   that is,

                                                       453.7psi and 824.5psi

7.1.4              Standardized Residuals

                   In discussing the use of residuals, the reasoning has been that the ei j are meant to
                   approximate the corresponding random errors i j . Since the model assumptions are
458 Chapter 7 Inference for Unstructured Multisample Studies

              that the i j are iid normal variables, the ei j ought to look approximately like iid
              normal variables. This is sensible rough-and-ready reasoning, adequate for many
              circumstances. But strictly speaking, the ei j are neither independent nor identically
              distributed, and it can be important to recognize this.

                   As an extreme example of the dependence of the residuals for a given sample i,
              consider a case where ni = 2. Since

                                                       ei j = yi j - y¯ i

              one immediately knows that ei1 = -ei2. So ei1 and ei2 are clearly dependent.
                   One can further apply Proposition 1 of Chapter 5 to show that if the sample

              sizes ni are varied, the residuals don't have the same variance (and therefore can't
              be identically distributed). That is, since

                                   ei j = yi j - y¯ i =    ni - 1  yi j -  1        yi j
                                                             ni
                                                                           ni j =j

              it is the case that

                                   ni - 1 2 2                 12           2 ni - 1 2
              Var ei j =                + - (ni - 1) =                                      (7.11)
                                   ni                      ni                       ni

              So, for example, residuals from a sample of size ni = 2 have variance  2/2, while
              those from a sample of size ni = 100 have variance 99 2/100, and one ought to
              expect residuals from larger samples to be somewhat bigger in magnitude than those
              from small samples.

                   A way of addressing at least the issue that residuals need not have a common
              variance is through the use of standardized residuals.

Definition 2  If a residual e has variance a ·  2 for some positive constant a, and s is some
              estimate of  , the standardized residual corresponding to e is

                                                               e
                                                         e=                                 (7.12)
                                                               sa

                                    
              The division by s a in equation (7.12) is a division by an estimated standard
              deviation of e. It serves, so to speak, to put all of the residuals on the same scale.
                                                         7.1 The One-Way Normal Model 459

                   Plotting with standardized residuals

    Standardized                  ei j =                 ei j                            (7.13)
residuals for the                        sP
 one-way model                                           ni - 1
                                                           ni

                   is a somewhat more refined way of judging the adequacy of the one-way model
                   than the plotting of raw residuals ei j illustrated in Examples 1 and 2. When all ni
                   are the same, as in Example 1, the plotting of the standardized residuals in equation
                   (7.13) is completely equivalent to plotting with the raw residuals. And as a practical
                   matter, unless some ni are very small and others are very large, the standardization
                   used in equation (7.13) typically doesn't have much effect on the appearance of
                   residual plots.

Example 2          In the spring constant study, allowing for the fact that sample 1 is larger than the
(continued )       other two (and thus according to the model (7.1) should produce larger residuals)
                   doesn't materially change the outcome of the residual analysis. To see this, note
                   that using the summary statistics in Table 7.5,

                            2 (7 - 1)(.134)2 + (6 - 1)(.074)2 + (6 - 1)(.064)2
                            sP =                                                = .0097
                                  (7 - 1) + (6 - 1) + (6 - 1)

                   so that

                                        
                                  sP = .0097 = .099

                   Then using equation (7.13), each residual from sample 1 should be divided by

                                                      .099 7 - 1 = .0913
                                                                 7

                   to get standardized residuals, while each residual from the second and third
                   samples should be divided by

                                                      .099 6 - 1 = .0900
                                                                 6

                   Clearly, .0913 and .0900 are not much different, and the division before plotting
                   has little effect on the appearance of residual plots. By way of example, a normal
                   plot of all 19 standardized residuals is given in Figure 7.10. Verify its similarity
                   to the normal plot of all 19 raw residuals given in Figure 7.9 on page 454.
460 Chapter 7 Inference for Unstructured Multisample Studies

Example 2     Standard normal quantile  3.0
(continued )

                                        1.5

                                        0.0                   3         2

                                                          2

                                        -1.5

                                                 -2.0 -1.0 0.0 1.0 2.0 3.0
                                                              Standardized residual quantile

                                        Figure 7.10 Normal plot of the spring constant
                                        standardized residuals

                   The notion of standardized residuals is often introduced only in the context
              of curve- and surface-fitting analyses, where the variances of residuals e = (y - y^ )
              depend not only on the sizes of the samples involved but also on the associated values
              of the independent or predictor variables (x1, x2, . . . , etc.). The concept has been
              introduced here, not only because it can be of importance in the present situation if
              the sample sizes vary widely but also because it is particularly easy to motivate the
              idea in the present context.

Section 1 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Return again to the data of Example 1 in Chapter       Van #1           Van #2      Van #3        Van #4
   4. These may be viewed as simply r = 5 samples
   of m = 3 densities. (For the time being, ignore the    1.096, 1.093     .962, .970  1.010, 1.024  1.002, 1.001
   fact that the pressure variable is quantitative and    1.090, 1.093     .967, .966  1.021, 1.020  1.002, 1.004
   that curve fitting seems a most natural method of                                   1.022
   analysis to apply to these data.)
    (a) Compute and make a normal plot of the residu-     (Notice that Van #3 was tested five times while the
        als for the one-way model. What does the plot     others were tested four times each.) Vans #1 and #2
        indicate about the appropriateness of the one-    were minivans, and Vans #3 and #4 were full-size
        way model assumptions here?                       vans.
   (b) Using the five samples, find sP, the pooled es-    (a) Compute and normal-plot residuals as a crude
        timate of  . What does this value measure in
        this context? Give a two-sided 90% confidence          means of investigating the appropriateness of
        interval for  based on sP.                             the one-way model assumptions for tilttable ra-
                                                               tios. Comment on the appearance of your plot.
2. In an ISU engineering research project, so called      (b) Redo part (a) using standardized residuals.
   "tilttable tests" were done in order to determine the  (c) Compute a pooled estimate of the standard de-
   angles at which vehicles experience lift-off of the         viation based on these four samples. What is
   "high-side" wheels and begin to roll over. So called        sP supposed to be measuring in this example?
   "tilttable ratios" (which are the tangents of angles        Give a two-sided 95% confidence interval for
   at which lift-off occurs) were measured for four             based on sP.
   different vans with the following results:
                                              7.2 Simple Confidence Intervals in Multisample Studies 461

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

7.2 Simple Confidence Intervals
       in Multisample Studies

                    Section 6.3 illustrates how useful confidence intervals for means and differences in
                    means can be in one- and two-sample studies. Estimating an individual mean and
                    comparing a pair of means are every bit as important when there are r samples as
                    they are when there are only one or two. The methods of Chapter 6 can be applied
                    in r -sample studies by simply limiting attention to one or two of the samples at
                    a time. But since individual sample sizes in multisample studies are often small,
                    such a strategy of inference often turns out to be relatively uninformative. Under the
                    one-way model assumptions discussed in the previous section, it is possible to base
                    inference methods on the pooled standard deviation, sP. Those tend to be relatively
                    more informative than the direct application of the formulas from Section 6.3 in the
                    present context.

                         This section first considers the confidence interval estimation of a single mean
                    and of the difference between two means under the "equal variances, normal dis-
                    tributions" model. There follows a discussion of confidence intervals for any linear
                    combination of underlying means. Finally, the section closes with some comments
                    concerning the notions of individual and simultaneous confidence levels.

7.2.1                 Intervals for Means and for Comparing Means

                      The primary drawback to applying the formulas from Section 6.3 in a multisample
                      context is that typical small sample sizes lead to small degrees of freedom, large t
                      multipliers in the plus-or-minus parts of the interval formulas, and thus long intervals.
                      But based on the one-way model assumptions, confidence interval formulas can be
                      developed that tend to produce shorter intervals.

                           That is, in a development parallel to that in Section 6.3, under the one-way
                      normal model,

                      T = y¯ i - µi
                               sP
                                 ni

                      has a tn-r distribution. Hence, a two-sided confidence interval for the ith mean, µi ,
                      has endpoints

   Confidence limits  y¯ i ± t sP    (7.14)
                                ni
     for µi based on
the one-way model

                      where the associated confidence is the probability assigned to the interval from -t
                      to t by the tn-r distribution. This is exactly formula (6.20) from Section 6.3, except
                      that sP has replaced si and the degrees of freedom have been adjusted from ni - 1
                      to n - r .
462 Chapter 7 Inference for Unstructured Multisample Studies

                             In the same way, for conditions i and i , the variable

                                                         T = y¯ i - y¯ i - µi - µi
                                                                           11

                                                                     sP +
                                                                          ni ni

                        has a tn-r distribution. Hence, a two-sided confidence interval for µi - µi has
                        endpoints

Confidence limits       y¯ i - y¯ i ± tsP                     1+ 1   (7.15)
for µi - µi based                                             ni ni

  on the one-way

               model

                        where the associated confidence is the probability assigned to the interval from -t to
                        t by the tn-r distribution. Display (7.15) is essentially formula (6.35) of Section 6.3,
                        except that sP is calculated based on r samples instead of two and the degrees of
                        freedom are n - r instead of ni + ni - 2.

                             Of course, use of only one endpoint from formula (7.14) or (7.15) produces a
                        one-sided confidence interval with associated confidence corresponding to the tn-r
                        probability assigned to the interval (-, t) (for t > 0). The virtues of formulas
                        (7.14) and (7.15) (in comparison to the corresponding formulas from Section 6.3)
                        are that (when appropriate) for a given confidence, they will tend to produce shorter
                        intervals than their Chapter 6 counterparts.

            Example 3   Confidence Intervals for Individual, and Differences of,
(Example 1 revisited )  Mean Concrete Compressive Strengths

                        Return to the concrete strength study of Armstrong, Babb, and Campen. Con-
                        sider making first a 90% two-sided confidence interval for the mean compressive
                        strength of an individual concrete formula and then a 90% two-sided confidence
                        interval for the difference in mean compressive strengths for two different formu-
                        las. Since n = 24 and r = 8, there are n - r = 16 degrees of freedom associated
                        with sP = 581.6. So the .95 quantile of the t16 distribution, namely 1.746, is
                        appropriate for use in both formulas (7.14) and (7.15).

                             Turning first to the estimation of a single mean compressive strength, since
                        each ni is 3, the plus-or-minus part of formula (7.14) gives

                        sP                                    581.6
                        t = 1.746  = 586.3 psi
                        ni                                    3

                        So ±586.3 psi precision could be attached to any one of the sample means
                        in Table 7.7 as an estimate of the corresponding formula's mean strength. For
          7.2 Simple Confidence Intervals in Multisample Studies 463

example, since y¯ 3 = 4,527.3 psi, a 90% two-sided confidence interval for µ3 has
endpoints

                                      4,527.3 ± 586.3
that is,

                             3, 941.0 psi and 5,113.6 psi
     In parallel fashion, consider estimation of the difference in two mean com-
pressive strengths with 90% confidence. Again, since each ni is 3, the plus-or-
minus part of formula (7.15) gives

          11     11
          tsP + = 1.746(581.6) + = 829.1 psi
          ni ni  33

Thus, ±829.1 psi precision could be attached to any difference between sample
means in Table 7.7 as an estimate of the corresponding difference in formula
mean strengths. For instance, since y¯ 3 = 4,527.3 psi and y¯ 7 = 1,551.3 psi, a
90% two-sided confidence interval for µ3 - µ7 has endpoints

             (4,527.3 - 1,551.3) ± 829.1

That is,

             2,146.9 psi and 3,805.1 psi

          Table 7.7
          Concrete Formula Sample Mean Strengths

          Concrete Formula Sample Mean Strength (psi)

          1      5,635.3

          2      5,753.3

          3      4,527.3

          4      3,442.3

          5      2,923.7

          6      3,324.7

          7      1,551.3

          8      2,390.7

     The use of n - r = 16 degrees of freedom in Example 3 instead of ni - 1 = 2
and ni + ni - 2 = 4 reflects the reduction in uncertainty associated with sP as an
464 Chapter 7 Inference for Unstructured Multisample Studies

                                  estimate of  as compared to that of si and of sP based on only two samples. That
                                  reduction is, of course, bought at the price of restriction to problems where the
                                  "equal variances" model is tenable.

                 7.2.2  Intervals for General Linear Combinations of Means

A linear combination    There is an important and simple generalization of the formulas (7.14) and (7.15)
of population means     that is easy to state and motivate at this point. Its most common applications are
                        in the context of factorial studies. But it is pedagogically most sound to introduce
                        the method in the unstructured r -sample context, so that the logic behind it is clear
                        and is seen not to be limited to factorial analyses. The basic notion is that µi and
                        µi - µi are particular linear combinations of the r means µ1, µ2, . . . , µr , and the
                        same logic that produces confidence intervals for µi and µi - µi will produce a
                        confidence interval for any linear combination of the r means.

                             That is, suppose that for constants c1, c2, . . . , cr , the quantity

                             L = c1µ1 + c2µ2 + · · · + cr µr                              (7.16)

                        is of engineering interest. (Note that, for example, if all ci 's except c3 are 0 and c3 =
                        1, L = µ3, the mean response from condition 3. Similarly, if c3 = 1, c5 = -1, and
                        all other ci 's are 0, L = µ3 - µ5, the difference in mean responses from conditions
                        3 and 5.) A natural data-based way to approximate L is to replace the theoretical
                        or underlying means, µi , with empirical or sample means, y¯ i . That is, define an
                        estimator of L by

A linear combination         L^ = c1 y¯ 1 + c2 y¯ 2 + · · · + cr y¯ r                     (7.17)
      of sample means

                        (Clearly, if L = µ3, then L^ = y¯ 3, while if L = µ3 - µ5, then L^ = y¯ 3 - y¯ 5.)
                             The one-way model assumptions make it very easy to describe the distribution

                        of L^ given in equation (7.17). Since E y¯ i = µi and Var y¯ i =  2/ni , one can appeal
                        again to Proposition 1 of Chapter 5 (page 307) and conclude that

                                                   EL^ = c1 E y¯ 1 + c2 E y¯ 2 + · · · + cr E y¯ r
                                                        = c1µ1 + c2µ2 + · · · + cr µr
                                                        =L

                        and

                             Var L^ = c12 Var y¯ 1 + c22 Var y¯ 2 + · · · + cr2 Var y¯ r

                             2   2                      2
                             = c12 + c22 + · · · + cr2
                             n1  n2                     nr
                      7.2 Simple Confidence Intervals in Multisample Studies 465

                      = 2  c21 + c22 + · · · + c2r
                           n1 n2                    nr

                      The one-way model restrictions imply that the y¯ i are independent and normal and,
                      in turn, that L^ is normal. So the standardized version of L^ ,

                      Z = L^ - EL^ =        L^ - L                   (7.18)
                               Var L^
                                            c21 + c22 + · · · + c2r
                                            n1 n2           nr

                      is standard normal. The usual manipulations beginning with this fact would produce
                      an unusable confidence interval for L involving the unknown parameter  . A way to
                      reason to something of practical importance is to begin not with the variable (7.18),
                      but with

                      T=                    L^ - L                   (7.19)
                            sP
                                c21 + c22 + · · · + c2r
                                n1 n2                   nr

                      instead. The fact is that under the current assumptions, the variable (7.19) has a tn-r
                      distribution. And this leads in the standard way to the fact that the interval with
                      endpoints

Confidence limits     L^ ± tsP  c21 + c22 + · · · + c2r              (7.20)
        for a linear            n1 n2                   nr

  combination of
               means

                      can be used as a two-sided confidence interval for L with associated confidence
                      the tn-r probability assigned to the interval between -t and t. Further, a one-sided
                      confidence interval for L can be obtained by using only one of the endpoints in
                      display (7.20) and appropriately adjusting the confidence level upward by reducing
                      the unconfidence in half.

                           It is worthwhile to verify that the general formula (7.20) reduces to the formula
                      (7.14) if a single ci is 1 and all others are 0. And if one ci is 1, one other is -1, and
                      all others are 0, the general formula (7.20) reduces to formula (7.15).

Example 4             Comparing Absorbency Properties for Three Brands of Paper Towels

                      D. Speltz did some absorbency testing for several brands of paper towels. His
                      study included (among others) a generic brand and two national brands. n1 =
                      n2 = n3 = 5 tests were made on towels of each of these r = 3 brands, and the
                      numbers of milliliters of water (out of a possible 100) not absorbed out of a
466 Chapter 7 Inference for Unstructured Multisample Studies

Example 4     graduated cylinder were recorded. Some summary statistics for the tests on these
(continued )  brands are given in Table 7.8. Plots (not shown here) of the raw absorbency
              values and residuals indicate no problems with the use of the one-way model in
              the analysis of the absorbency data.

                   One question of practical interest is "On average, do the national brands
              absorb more than the generic brand?" A way of quantifying this is to ask for a
              two-sided 95% confidence interval for

                                                    1                                (7.21)
                                       L = µ1 - (µ2 + µ3)

                                                    2

              the difference between the average liquid left by the generic brand and the
              arithmetic mean of the national brand averages.

                   With L as in equation (7.21), formula (7.17) shows that

                              ^        1                      1
                              L = 93.2 - (81.0) - (83.8) = 10.8 ml
                                       2                      2

              is an estimate of the increased absorbency offered by the national brands. Using
              the standard deviations given in Table 7.8,

                        2 (5 - 1)(.8)2 + (5 - 1)(.7)2 + (5 - 1)(.8)2
                        sP =                                                  = .59
                                       (5 - 1) + (5 - 1) + (5 - 1)

              and thus

                                             
                                       sP = .59 = .77 ml

              Now n - r = 15 - 3 = 12 degrees of freedom are associated with sP, and the
              .975 quantile of the t12 distribution for use in (7.20) is 2.179. In addition, since
              c1 = 1, c2 = - 12 , and c3 = - 21 and all three sample sizes are 5,

                        c12 c22 c32                           -1 2 -1 2
                           ++=            1 + 2 + 2 = .55
                                          5                      5         5
                        n1 n2 n3

                              Table 7.8
                              Summary Statistics for Absorbencies of Three
                              Brands of Paper Towels

                              Brand    i ni                      y¯ i  si

                              Generic  1 5 93.2 ml .8 ml

                              National B 2 5 81.0 ml .7 ml

                              National V 3 5 83.8 ml .8 ml
                        7.2 Simple Confidence Intervals in Multisample Studies 467

           So finally, endpoints for a two-sided 95% confidence interval for L given in
           equation (7.21) are

                                   10.8 ± 2.179(.77)(.55)

           that is,

                                        10.8 ± .9

           i.e.,

                                   9.9 ml and 11.7 ml                    (7.22)

           The interval indicated in display (7.22) shows definitively the substantial advan-
           tage in absorbency held by the national brands over the generic, particularly in
           view of the fact that the amount actually absorbed by the generic brand appears
           to average only about 6.8 ml (= 100 ml - 93.2 ml).

Example 5  A Confidence Interval for a Main Effect in a 22 Factorial
           Brick Fracture Strength Study

           Graves, Lundeen, and Micheli studied the fracture strength properties of brick
           bars. They included several experimental variables in their study, including both
           bar composition and heat-treating regimen. Part of their data are given in Table 7.9.
           Modulus of rupture values under a bending load are given in psi for n1 = n2 =
           n3 = n4 = 3 bars of r = 4 types.

                     Table 7.9

                     Modulus of Rupture Measurements for Brick Bars
                     in a 22 Factorial Study

                         i,    % Water  Heat-Treating      MOR (psi)
                     Bar Type   in Mix    Regimen

                     1         17       slow cool 4911, 5998, 5676

                     2         19       slow cool 4387, 5388, 5007

                     3         17       fast cool      3824, 3140, 3502

                     4         19       fast cool      4768, 3672, 3242

                Notice that the data represented in Table 7.9 have a 2 × 2 complete factorial
           structure. Indeed, returning to Section 4.3 (in particular, to Definition 5, page 166),
468 Chapter 7 Inference for Unstructured Multisample Studies

Example 5     it becomes clear that the fitted main effect of the factor Heat-Treating Regimen
(continued )  at its slow cool level is

                   1               1
                      (y¯ 1 + y¯ 2) - (y¯ 1 + y¯ 2 + y¯ 3 + y¯ 4)                (7.23)
                   2               4

              But the variable (7.23) is the L^ for the linear combination of mean strengths µ1,
              µ2, µ3, and µ4 given by

                         1                                    1        1      1
                      L = µ1 + µ2 - µ3 - µ4                                      (7.24)
                         4                                    4        4      4

              So subject to the relevance of the "equal variances, normal distributions" de-
              scription of modulus of rupture for fired brick clay bodies of these four types,
              formula (7.20) can be applied to develop a precision figure to attach to the fitted
              effect (7.23).

                   Table 7.10 gives summary statistics for the data of Table 7.9. Using the values
              in Table 7.10 leads to

                   L^ = 1 (y¯ 1 + y¯ 2) - 1 (y¯ 1 + y¯ 2 + y¯ 3 + y¯ 4)
                   2               4

                      1111
                   = y¯ 1 + y¯ 2 - y¯ 3 - y¯ 4

                      4444

                   = 1 (5,528.3 + 4,927.3 - 3,488.7 - 3,894.0)
                      4

                   = 768.2 psi

              and

                      (3 - 1)(558.3)2 + (3 - 1)(505.2)2 + (3 - 1)(342.2)2 + (3 - 1)(786.8)2
              sP =

                                           (3 - 1) + (3 - 1) + (3 - 1) + (3 - 1)
                 = 570.8 psi

                      Table 7.10
                      Summary Statistics for the
                      Modulus of Rupture Measurements

                      i, Bar Type                                y¯ i     si

                      1                                       5,528.3 558.3

                      2                                       4,927.3 505.2

                      3                                       3,488.7 342.2

                      4                                       3,894.0 786.8
                 7.2 Simple Confidence Intervals in Multisample Studies 469

       Further, n - r = 12 - 4 = 8 degrees of freedom are associated with sP. There-
       fore, if one wanted (for example) a two-sided 98% confidence interval for L
       given in equation (7.24), the necessary .99 quantile of the t8 distribution is 2.896.
       Then, since

                 12  12  -1 2  -1 2
                 4+  4+    4+    4 = .2887
                 3   3     3     3

       a two-sided 98% confidence interval for L has endpoints

                 768.2 ± 2.896(570.8)(.2887)

       that is,

                 291.1 psi and 1,245.4 psi                      (7.25)

            Display (7.25) establishes convincingly the effectiveness of a slow cool
       regimen in increasing MOR. It says that the differences in sample mean MOR
       values for slow- and fast-cooled bricks are not simply reflecting background
       variation. In fact, multiplying the endpoints in display (7.25) each by 2 in order
       to get a confidence interval for

                     1   1
                 2L = (µ1 + µ2) - (µ3 + µ4)
                     2   2

       shows that (when averaged over 17% and 19% water mixtures) the slow, cool
       regimen seems to offer an increase in MOR in the range from

                     582.2 psi to 2,490.8 psi

7.2.3  Individual and Simultaneous Confidence Levels

       This section has introduced a variety of confidence intervals for multisample studies.
       In a particular application, several of these might be used, perhaps several times each.
       For example, even in the relatively simple context of Example 4 (the paper towel
       absorbency study), it would be reasonable to desire confidence intervals for each of

                                                                                 1
                  µ1, µ2, µ3, µ1 - µ2, µ1 - µ3, µ2 - µ3, and µ1 - (µ2 + µ3)

                                                                                 2

       Since many confidence statements are often made in multisample studies, it is
       important to reflect on the meaning of a confidence level and realize that it is
       attached to one interval at a time. If many 90% confidence intervals are made,
470 Chapter 7 Inference for Unstructured Multisample Studies

                   the 90% figure applies to the intervals individually. One is "90% sure" of the
                   first interval, separately "90% sure" of the second, separately "90% sure" of the
                   third, and so on. It is not at all clear how to arrive at a reliability figure for the
                   intervals jointly or simultaneously (i.e., an a priori probability that all the intervals
                   are effective). But it is fairly obvious that it must be less than 90%. That is, the
                   simultaneous or joint confidence (the overall reliability figure) to be associated
                   with a group of intervals is generally not easy to determine, but it is typically less
                   (and sometimes much less) than the individual confidence level(s) associated with
                   the intervals one at a time.

                        There are at least three different approaches to be taken once the difference
                   between simultaneous and individual confidence levels is recognized. The most
                   obvious option is to make individual confidence intervals and be careful to interpret
                   them as such (being careful to recognize that as the number of intervals one makes
                   increases, so does the likelihood that among them are one or more intervals that fail
                   to cover the quantities they are meant to locate).

                        A second way of handling the issue of simultaneous versus individual confidence
                   is to use very large individual confidence levels for the separate intervals and then
                   employ a somewhat crude inequality to find at least a minimum value for the
                   simultaneous confidence associated with an entire group of intervals. That is, if
                   k confidence intervals have associated confidences 1, 2, . . . , k, the Bonferroni
                   inequality says that the simultaneous or joint confidence that all k intervals are
                   effective (say,  ) satisfies

The Bonferroni       1 - (1 - 1) + (1 - 2) + · · · + (1 - k)  (7.26)
       inequality

                   (Basically, this statement says that the joint "unconfidence" associated with k inter-
                   vals (1 -  ) is no larger than the sum of the k individual unconfidences. For example,
                   five intervals with individual 99% confidence levels have a joint or simultaneous
                   confidence level of at least 95%.)

                        The third way of approaching the issue of simultaneous confidence is to develop
                   and employ methods that for some specific, useful set of unknown quantities provide
                   intervals with a known level of simultaneous confidence. There are whole books
                   full of such simultaneous inference methods. In the next section, two of the better
                   known and simplest of these are discussed.

Section 2 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Return to the situation of Exercise 1 of Section         confidence is desired, what is ? If all five of
   7.1 (and the pressure/density data of Example 1 in       these intervals are made, what does the Bonfer-
   Chapter 4).                                              roni inequality guarantee for a minimum joint
    (a) Individual two-sided confidence intervals for       or simultaneous confidence?
        the five different means here would be of the  (b) Individual two-sided confidence intervals for
        form y¯ i ± for a number . If 95% individual        the differences in the five different means
       7.3 Two Simultaneous Confidence Interval Methods 471

        would be of the form y¯ i - y¯ i ± for a num-        (b) Individual confidence intervals for the differ-
        ber . If 95% individual confidence is desired,
        what is ?                                            ences between particular pairs of mean tilttable
    (c) Note that if mean density is a linear func-
        tion of pressure over the range of pressures         ratios are of the form y¯ i - y¯ i ± for appro-
        from 2,000 to 6,000 psi, then µ4000 - µ2000 =        priate values of . Find values of if individ-
        µ6000 - µ4000, that is L = µ6000 - 2µ4000 +
        µ2000 has the value 0. Give 95% two-sided            ual 99% two-sided intervals are desired, first
        confidence limits for this L. What does your
        interval indicate about the linearity of the pres-   for pairs of means with samples of size 4 and
        sure/density relationship?
                                                             then for pairs of means where one sample size
2. Return to the tilttable testing problem of Exercise
   2 of Section 7.1.                                         is 4 and the other is 5.
    (a) Make (individual) 99% two-sided confidence
        intervals for the four different mean tilttable ra-  (c) It might be of interest to compare the average
        tios for the four vans, µ1, µ2, µ3 and µ4. What
        does the Bonferroni inequality guarantee for a       of the tilttable ratios for the minivans to that of
        minimum joint or simultaneous confidence for
        these four intervals?                                the full-size vans. Give a 99% two-sided con-

                                                             fidence interval for the quantity 12 (µ1 + µ2) -
                                                             1  (µ3+
                                                             2        µ4)  .

                                                             3. Explain the difference between several intervals

                                                             having associated 95% individual confidences and

                                                             having associated 95% simultaneous confidence.

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

7.3 Two Simultaneous
       Confidence Interval Methods

                    As Section 7.2 illustrated, there are several kinds of confidence intervals for means
                    and linear combinations of means that could be made in a multisample study. The
                    issue of individual versus simultaneous confidence was also raised, but only the use
                    of the Bonferroni inequality was given as a means of controlling a simultaneous
                    confidence level.

                         This section presents two methods for making a number of confidence intervals
                    and in the process maintaining a desired simultaneous confidence. The first of these is
                    due to Pillai and Ramachandran; it provides a guaranteed simultaneous confidence in
                    the estimation of all r individual underlying means. The second is Tukey's method
                    for the simultaneous confidence interval estimation of all differences in pairs of
                    underlying means.

7.3.1  The Pillai-Ramachandran Method

       One of the things typically of interest in an r -sample statistical engineering study is
       the estimation of all r individual mean responses µ1, µ2, . . . , µr . If the individual
       confidence interval formula of Section 7.2,

                                                             y¯ i ± t sP               (7.27)
                                                                       ni
472 Chapter 7 Inference for Unstructured Multisample Studies

                        is applied r times to estimate these means, the only handle one has on the corre-

                        sponding simultaneous confidence is given by the Bonferroni inequality (7.26). This

                        fairly crude tool says that if r = 8 and one wants 95% simultaneous confidence, in-

                        dividual  "unconfidences"  of  .05  =  .00625  (i.e.,  individual  confidences  of  99.375%)
                                                        8
                        for the eight different intervals will suffice to produce the desired simultaneous

                        confidence.

                        Another approach to the setting of simultaneous confidence limits on all of

                        µ1, µ2, . . . , µr is to replace t in formula (7.27) with a multiplier derived specif-
                        ically for the purpose of providing an exact, stated, simultaneous confidence in

                        the estimation of all the means. Such multipliers were derived by Pillai and Ra-

                        machandran, where either all of the intervals for the r means are two-sided or all are
                        one-sided. That is, Table B.8A gives values of constants k2 such that the r two-sided
                        intervals with respective endpoints

      P-R two-sided                                            y¯ i ± k2 sP                                 (7.28)
simultaneous 95%                                                          ni

  confidence limits
         for r means

                        have simultaneous 95% confidence as intervals for the means µ1, µ2, . . . , µr .
                        (These values k2 are in fact .95 quantiles of the Studentized maximum modulus
                        distributions.)

                             Table B.8B gives values of some other constants k1 such that if for each i from
                        1 to r , an interval of the form

         P-R one-sided                                      -, y¯ i + k1 sP                                 (7.29)
  simultaneous 95%                                                            ni
confidence intervals

           for r means

                           or of the form

         P-R one-sided                                      y¯ i - k1 sP ,                                  (7.30)
  simultaneous 95%                                                     ni
confidence intervals

           for r means

                        is made as a confidence interval for µi , the simultaneous confidence associated
                        with the collection of r one-sided intervals is 95%. (These k1 values are in fact .95
                        quantiles of the Studentized extreme deviate distributions.)

                             In this book, the use of r intervals of one of the forms (7.28) through (7.30) will

                        be called the P-R method of simultaneous confidence intervals. In order to apply the

                        P-R method, one must find (using interpolation as needed) the entry in Tables B.8 in

                        the column corresponding to the number of samples, r , and the row corresponding
                        to the degrees of freedom associated with sP, namely  = n - r .
                                  7.3 Two Simultaneous Confidence Interval Methods 473

            Example 6   Simultaneous Confidence Intervals
(Example 3 revisited )  for Eight Mean Concrete Compressive Strengths

                        Consider again the concrete strength study of Armstrong, Babb, and Campen.
                        Recall that tests on ni = 3 specimens of each of r = 8 different concrete formulas
                        gave sP = 581.6 psi. Using formula (7.27) and remembering that there are n -
                        r = 16 degrees of freedom associated with sP, one has endpoints for 95% two-
                        sided intervals for the formula mean compressive strengths

                                               581.6
                                  y¯ i ± 2.120 

                                                   3

                        that is,

                                  y¯ i ± 711.9 psi    (7.31)

                             In contrast to intervals (7.31), consider the use of formula (7.28) to produce

                        r = 8 two-sided intervals for the formula mean strengths with simultaneous 95%
                        confidence. Table B.8A shows that k2 = 3.099 is appropriate in this application.
                        So each concrete formula mean compressive strength, µi , should be estimated
                        using

                                               581.6
                                  y¯ i ± 3.099 

                                                   3

                        that is,

                                  y¯ i ± 1,040.6 psi  (7.32)

                             Expressions (7.31) and (7.32) provide two-sided intervals for the eight mean
                        compressive strengths. If one-sided intervals of the form (#, ) were desired
                        instead, consulting the t table for the .95 quantile of the t16 distribution and use
                        of formula (7.27) shows that the values

                                               581.6
                                  y¯ i - 1.746 

                                                   3

                        that is,

                                  y¯ i - 586.3 psi    (7.33)

                        are individual 95% lower confidence bounds for the formula mean compres-
                        sive strengths, µi . At the same time, consulting Table B.8B shows that for
474 Chapter 7 Inference for Unstructured Multisample Studies

Example 6     simultaneous 95% confidence, use of k1 = 2.779 in formula (7.30) is appro-
(continued )  priate, and the values

                                                                      581.6
                                                         y¯ i - 2.779 

                                                                          3

              that is,

                                                         y¯ i - 933.2 psi                                  (7.34)

              are simultaneous 95% lower confidence bounds for the formula mean compressive
              strengths, µi .

                   Comparing intervals (7.31) with intervals (7.32) and bounds (7.33) with bounds
              (7.34) shows clearly the impact of requiring simultaneous rather than individual
              confidence. For a given nominal confidence level, the simultaneous intervals must
              be bigger (more conservative) than the corresponding individual intervals.

                   It is common practice to summarize the information about mean responses
              gained in a multisample study in a plot of sample means versus sample numbers,
              enhanced with "error bars" around the sample means to indicate the uncertainty
              associated with locating the means. There are various conventions for the making
              of these bars. When looking at such a plot, one typically forms an overall visual
              impression. Therefore, it is our opinion that error bars derived from the P-R simul-
              taneous confidence limits of display (7.28) are the most sensible representation of
              what is known about a group of r means. For example, Figure 7.11 is a graphical
              representation of the eight formula sample mean strengths given in Table 7.7 with
              ±1,040.6 psi error bars, as indicated by expression (7.32).

                   When looking at a display like Figure 7.11, it is important to remember that
              what is represented is the precision of knowledge about the mean strengths, rather
              than any kind of predictions for individual compressive strengths. In this regard,
              the similarity of the spread of the samples on the side-by-side dot diagram given
              as Figure 7.1 and the size of the error bars here is coincidental. As sample sizes
              increase, spreads on displays of individual measurements like Figure 7.1 will tend to
              stabilize (representing the spreads of the underlying distributions), while the lengths
              of error bars associated with means will shrink to 0 as increased information gives
              sharper and sharper evidence about the underlying means.

                   In any case, Figure 7.11 shows clearly that the information in the data is quite
              adequate to establish the existence of differences in formula mean compressive
              strengths.

7.3.2         Tukey's Method

              A second set of quantities often of interest in an r -sample study consists of the
                                    r (r -1)
              differences  in  all     2      pairs  of  mean  responses     µi  and  µi  .  Section  7.2  argued
                                       7.3 Two Simultaneous Confidence Interval Methods 475

Mean compressive strength (psi)  7000
                                 6000
                                 5000
                                 4000
                                 3000
                                 2000
                                 1000

                                       1  2  3  4            5      6  7  8

                                             Concrete formula

Figure 7.11 Plot of eight sample mean compressive strengths, enhanced
with error bars derived from P-R simultaneous confidence limits

that a single difference in mean responses, µi - µi , can be estimated using an
interval with endpoints

                                          y¯ i - y¯ i ± tsP  1+ 1            (7.35)
                                                             ni ni

where the associated confidence level is an individual one. But if, for example,
r = 8, there are 28 different two-at-a-time comparisons of underlying means to be
considered (µ1 versus µ2, µ1 versus µ3, . . . , µ1 versus µ8, µ2 versus µ3, . . . , and
µ7 versus µ8). If one wishes to guarantee a reasonable simultaneous confidence
level for all these comparisons via the crude Bonferroni idea, a huge individual
confidence level is required for the intervals (7.35). For example, the Bonferroni in-
equality requires 99.82% individual confidence for 28 intervals in order to guarantee
simultaneous 95% confidence.

     A better approach to the setting of simultaneous confidence limits on all of
the differences µi - µi is to replace t in formula (7.35) with a multiplier derived
specifically for the purpose of providing exact, stated, simultaneous confidence in
the estimation of all such differences. J. Tukey first pointed out that it is possible
to provide such multipliers using quantiles of the Studentized range distributions.
476 Chapter 7 Inference for Unstructured Multisample Studies

                        Tables B.9A and B.9B give values of constants q such that the set of two-sided
                        intervals with endpoints

         Tukey's two-                          q                 1+ 1   (7.36)
sided simultaneous                y¯ i - y¯ i ±  sP              ni ni

   confidence limits                              2
  for all differences

            in r means

                        has simultaneous confidence at least 95% or 99% (depending on whether Q(.95)
                        is read from Table B.9A or Q(.99) is read from Table B.9B) in the estimation of
                        all differences µi - µi . If all the sample sizes n1, n2, . . . , nr are equal, the 95% or
                        99% nominal simultaneous confidence figure is exact, while if the sample sizes are
                        not all equal, the true value is at least as big as the nominal value.

                             In order to apply Tukey's method, one must find (using interpolation as needed)
                        the column in Tables B.9 corresponding to the number of samples/means to be
                        compared and the row corresponding to the degrees of freedom associated with sP,
                        (namely,  = n - r ).

Example 6               Consider the making of confidence intervals for differences in formula mean
(continued )
                        compressive strengths. If a 95% two-sided individual confidence interval is de-
                        sired for a specific difference µi - µi , formula (7.35) shows that appropriate
                        endpoints are

                                                                  11
                                  y¯ i - y¯ i ± 2.120(581.6) +

                                                                  33

                        that is,

                                  y¯ i - y¯ i ± 1,006.7 psi             (7.37)

                             On the other hand, if one plans to estimate all differences in mean com-
                        pressive strengths with simultaneous 95% confidence, by formula (7.36) Tukey
                        two-sided intervals with endpoints

                                  4.90                           11
                                  y¯ i - y¯ i ±  (581.6) +
                                                              2  33

                        that is,

                                  y¯ i - y¯ i ± 1,645.4 psi             (7.38)

                        are in order (4.90 is the value in the r = 8 column and  = 16 row of Table B.9A.)
                            7.3 Two Simultaneous Confidence Interval Methods 477

   In keeping with the fact that the confidence level associated with the intervals
   (7.38) is a simultaneous one, the Tukey intervals are wider than those indicated
   in formula (7.37).

        The plus-or-minus part of display (7.38) is not as big as twice the plus-or-
   minus part of expression (7.32). Thus, when looking at Figure 7.11, it is not
   necessary that the error bars around two means fail to overlap before it is safe to
   judge the corresponding underlying means to be detectably different. Rather, it
   is only necessary that the two sample means differ by the plus-or-minus part of
   formula (7.36)--1,645.4 psi in the present situation.

     This section has mentioned only two of many existing methods of simultane-
ous confidence interval estimation for multisample studies. These should serve to
indicate the general character of such methods and illustrate the implications of a
simultaneous (as opposed to individual) confidence guarantee.

     One final word of caution has to do with the theoretical justification of all of
the methods found in this section. It is the "equal variances, normal distributions"
model that supports these engineering tools. If any real faith is to be put in the
nominal confidence levels attached to the P-R and Tukey methods presented here,
that faith should be based on evidence (typically gathered, at least to some extent,
as illustrated in Section 7.1) that the standard one-way normal model is a sensible
description of a physical situation.

Section 3 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Return to the situation of Exercises 1 of Sections    (a) Use the P-R method of simultaneous confi-
   7.1 and 7.2 (and the pressure/density data of Ex-          dence intervals and make simultaneous 95%
   ample 1 in Chapter 4).                                     two-sided confidence intervals for the four
    (a) Using the P-R method, what can be em-                 mean tilttable ratios.
        ployed to make two-sided intervals of the form
         y¯ i ± for all five mean densities, possessing  (b) Simultaneous confidence intervals for the dif-
        simultaneous 95% confidence? How does this            ferences in all pairs of mean tilttable ratios
            compare to the one computed in part (a) of        are of the form y¯ i - y¯ i ± . Find appropriate
        Exercise 1 of Section 7.2?                            values of if simultaneous 99% two-sided in-
   (b) Using the Tukey method, what can be em-                tervals are desired, first for pairs of means with
        ployed to make two-sided intervals of the form        samples of size 4 and then for pairs of means
         y¯ i - y¯ i ± for all differences in the five        where one sample size is 4 and the other is
        mean densities, possessing simultaneous 95%           5. How do these compare to the intervals you
        confidence? How does this compare to the              found in part (b) of Exercise 2 of Section 7.2?
        one computed in part (b) of Exercise 1 of Sec-        Why is it reasonable that the 's should be
        tion 7.2?                                             related in this way?

2. Return to the tilttable study of Exercises 2 of Sec-
   tions 7.1 and 7.2.
478 Chapter 7 Inference for Unstructured Multisample Studies

                                                qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

         7.4 One-Way Analysis of Variance (ANOVA)

                                  This book's approach to inference in multisample studies has to this point been
                                  completely "interval-oriented." But there are also significance-testing methods that
                                  are appropriate to the multiple-sample context. This section considers some of these
                                  and the issues raised by their introduction. It begins with some general comments
                                  regarding significance testing in r -sample studies. Then the one-way analysis of
                                  variance (ANOVA) test for the equality of r means is discussed. Next, the one-
                                  way ANOVA table and the organization and intuition that it provides are presented.
                                  Finally, there is a brief look at the one-way random effects model and ANOVA-based
                                  inference for its parameters.

7.4.1  Significance Testing and Multisample Studies

       Just as there are many quantities one might want to estimate in a multisample study,
       there are potentially many issues of statistical significance to be judged. For instance,
       one might desire p-values for hypotheses like

                       H0: µ3 = 7                                       (7.39)
                       H0: µ3 - µ7 = 0                                  (7.40)

                                   1                                    (7.41)
                       H0: µ1 - (µ2 + µ3) = 0

                                   2

       The confidence interval methods discussed in Section 7.2 have their significance-
       testing analogs for treating hypotheses that, like all three of these, involve linear
       combinations of the means µ1, µ2, . . . , µr .

            In general (under the standard one-way model), if

                       L = c1µ1 + c2µ2 + · · · + cr µr

       the hypothesis

                                               H0: L = #                (7.42)

       can be tested using the test statistic

                       T=                      L^ - #                   (7.43)
                             sP
                                               c21 + c22 + · · · + c2r
                                               n1 n2      nr

       and a tn-r reference distribution. This fact specializes to cover hypotheses of types
       (7.39) to (7.41) by appropriate choice of the ci and #.
                                       7.4 One-Way Analysis of Variance (ANOVA) 479

     But the significance-testing method most often associated with the one-way
normal model is not for hypotheses of the type (7.42). Instead, the most common
method concerns the hypothesis that all r underlying means have the same value. In
symbols, this is

                                  H0: µ1 = µ2 = · · · = µr                       (7.44)

Given that one is working under the assumptions of the one-way model to begin

with, hypothesis (7.44) amounts to a statement that all r underlying distributions are

essentially the same--or "There are no differences between treatments."

Hypothesis (7.44) can be thought of in terms of the simultaneous equality of
r (r -1)
   2      pairs  of  means--that  is,  as  equivalent  to  the  statement  that  simultaneously

                     µ1 - µ2 = 0, µ1 - µ3 = 0, . . . , µ1 - µr = 0,
                          µ2 - µ3 = 0, . . . , and µr-1 - µr = 0

And this fact should remind the reader of the ideas about simultaneous confidence
intervals from the previous section (specifically, Tukey's method). In fact, one way of
judging the statistical significance of an r -sample data set in reference to hypothesis
(7.44) is to apply Tukey's method of simultaneous interval estimation and note
whether or not all the intervals for differences in means include 0. If they all do,
the associated p-value is larger than 1 minus the simultaneous confidence level. If
not all of the intervals include 0, the associated p-value is smaller than 1 minus
the simultaneous confidence level. (If simultaneous 95% intervals all include 0,
no differences between means are definitively established, and the corresponding
p-value exceeds .05.)

     We admit a bias toward estimation over testing per se. A consequence of this
bias is a fondness for deriving a rough idea of a p-value for hypothesis (7.44) as a
byproduct of Tukey's method. But a most famous significance-testing method for
hypothesis (7.44) also deserves discussion: the one-way analysis of variance test.
(At this point it may seem strange that a test about means has a name apparently
emphasizing variance. The motivation for this jargon is that the test is associated
with a very helpful way of thinking about partitioning the overall variability that is
encountered in a response variable.)

7.4.2 The One-Way ANOVA F Test                                                   (7.45)
          The standard method of testing the hypothesis (7.44)
                                             H0: µ1 = µ2 = · · · = µr
          of no differences among r means against
                                                      Ha: not H0
480 Chapter 7 Inference for Unstructured Multisample Studies

                                  is based essentially on a comparison of a measure of variability among the sample
                                  means to the pooled sample variance, sP2. In order to fully describe this method some
                                  additional notational conventions are needed.

                                       Repeatedly in the balance of this book, it will be convenient to have symbols for
                                  the summary measures of Section 3.3 (sample means and variances) applied to the
                                  data from multisample studies, ignoring the fact that there are r different samples
                                  involved. Already the unsubscripted letter n has been used to stand for n1 + n2 +
                                  · · · + nr , the number of observations in hand ignoring the fact that r samples are
                                  involved. This kind of convention will now be formally extended to include statistics
                                  calculated from the n responses. For emphasis, this will be stated in definition form.

               Definition 3  In multisample studies, symbols for sample sizes and sample statistics appear-
(A Notational Convention     ing without subscript indices or dots will be understood to be calculated from
 for Multisample Studies)    all responses in hand, obtained by combining all samples.

                             So n will stand for the total number of data points (even in an r -sample study),
                             y¯ for the grand sample average of response y, and s2 for a grand sample variance

                             calculated completely ignoring sample boundaries.

                                  For present purposes (of writing down a test statistic for testing hypothesis
                             (7.44)), one needs to make use of y¯ , the grand sample average. It is important to
                             recognize that y¯ and

  The (unweighted)                                 1r                                      (7.46)
average of r sample                          y¯ . = r y¯ i

                  means                                   i =1

                             are not necessarily the same unless all sample sizes are equal. That is, when sample
                             sizes vary, y¯ is the (unweighted) arithmetic average of the raw data values yi j but is a
                             weighted average of the sample means y¯ i . On the other hand, y¯ . is the (unweighted)
                             arithmetic mean of the sample means y¯ i but is a weighted average of the raw data
                             values yi j . For example, in the simple case that r = 2, n1 = 2, and n2 = 3,

                                       1                                            23
                                    y¯ = (y11 + y12 + y21 + y22 + y23) = y¯ 1 + y¯ 2
                                       5                                            55

                             while

                                    1     1  1                                   1  1   1
                                    y¯ . = (y¯ 1 + y¯ 2) = y11 + y12 + y21 + y22 + y23
                                    2     4  4                                   6  6   6

                             and, in general, y¯ and y¯ . will not be the same.
                         7.4 One-Way Analysis of Variance (ANOVA) 481

                              Now, under the hypothesis (7.44), that µ1 = µ2 = · · · = µr , y¯ is a natural
                         estimate of the common mean. (All underlying distributions are the same, so the

                         data in hand are reasonably thought of not as r different samples, but rather as
                         a single sample of size n.) Then the differences y¯ i - y¯ are indicators of possible
                         differences among the µi . It is convenient to summarize the size of these differences
                         y¯ i - y¯ in terms of a kind of total of their squares--namely,

                              r                       (7.47)

                                 ni (y¯ i - y¯ )2

                             i =1

                         One can think of statistic (7.47) either as a weighted sum of the quantities (y¯ i - y¯ )2
                         or as an unweighted sum, where there is a term in the sum for each raw data point
                         and therefore ni of the type (y¯ i - y¯ )2. The quantity (7.47) is a measure of the
                         between-sample variation in the data. For a given set of sample sizes, the larger it

                         is, the more variation there is between the sample means y¯ i .
                              In order to produce a test statistic for hypothesis (7.44), one simply divides the

                         measure (7.47) by (r - 1)sP2, giving

                             1  r

   One-way ANOVA             r - 1 ni (y¯ i - y¯ ) 2
     test statistic for
                         F=     i =1                  (7.48)
equality of r means
                                   s2
                                     P

                         The fact is that if H0: µ1 = µ2 = · · · = µr is true, the one-way model assumptions
                         imply that this statistic has an Fr-1, n-r distribution. So the hypothesis of equality of
                         r means can be tested using the statistic in equation (7.48) with an Fr-1, n-r reference
                         distribution, where large observed values of F are taken as evidence against H0 in
                         favor of Ha: not H0.

            Example 7    Returning again to the concrete compressive strength study of Armstrong, Babb,
(Example 1 revisited )   and Campen, y¯ = 3,693.6 and the 8 sample means y¯ i have differences from this
                         value given in Table 7.11.

                              Then since each ni = 3, in this situation,

                                                   r

                                              ni (y¯ i - y¯ )2 = 3(1,941.7)2 + 3(2,059.7)2 + · · ·

                                                 i =1

                                                                + 3(-2,142.3)2 + 3(-1,302.9)2

                                                             = 47,360,780 (psi)2
482 Chapter 7 Inference for Unstructured Multisample Studies

Example 7     Table 7.11
(continued )  Sample Means and Their
              Deviations from y¯ in the Concrete
              Strength Study

                  i,      y¯ i                                  y¯ i - y¯
              Formula
                       5,635.3                                  1,941.7
                  1    5,753.3                                  2,059.7
                  2    4,527.3
                  3    3,442.3                                    833.7
                  4    2,923.7                                  -251.3
                  5    3,324.7                                  -769.9
                  6    1,551.3                                  -368.9
                  7    2,390.7                                -2,142.3
                  8                                           -1,302.9

                 In order to use this figure to judge statistical significance, one standardizes via
                 equation (7.48) to arrive at the observed value of the test statistic

                                                       1
                                                    8 - 1 (47,360,780)
                                              f = (581.6)2 = 20.0

                 It is easy to verify from Tables B.6 that 20.0 is larger than the .999 quantile of
                 the F7,16 distribution. So

                                p-value = P[an F7,16 random variable  20.0] < .001

                 That is, the data provide overwhelming evidence that µ1, µ2, . . . , µ8 are not all
                 equal.

                   For pedagogical reasons, the one-way ANOVA test has been presented after
              discussing interval-oriented methods of inference for r -sample studies. But if it is to
              be used in applications, the testing method typically belongs chronologically before
              estimation. That is, the ANOVA test can serve as a screening device to determine
              whether the data in hand are adequate to differentiate conclusively between the
              means, or whether more data are needed.

7.4.3         The One-Way ANOVA Identity and Table

              Associated with the ANOVA test statistic is some strong intuition related to the
              partitioning of observed variability. This is related to an algebraic identity that is
              stated here in the form of a proposition.
                                                   7.4 One-Way Analysis of Variance (ANOVA) 483

      Proposition 1        For any n numbers yi j

               One-way                                   r                                                   (7.49)
                 ANOVA                                                                                       (7.50)
                 identity        (n - 1)s2 = ni (y¯ i - y¯ )2 + (n - r )sP2

A second statement                                      i =1
      of the one-way
     ANOVA identity        or in other symbols,

                                                   r                        r ni

                                 (yi j - y¯ )2 = ni (y¯ i - y¯ )2 +                         (yi j - y¯ i )2

                           i, j                    i =1                     i=1 j=1

                                Proposition 1 should begin to shed some light on the phrase "analysis of vari-
                           ance." It says that an overall measure of variability in the response y, namely,

                                                   (n - 1)s2 = (yi j - y¯ )2

                                                                          i, j

                           can be partitioned or decomposed algebraically into two parts. One,

                                                       r

                                                          ni (y¯ i - y¯ )2

                                                      i =1

                           can be thought of as measuring variation between the samples or "treatments," and
                           the other,

                                                                  r ni

                                                   (n - r )sP2 =           (yi j - y¯ i )2

                                                                  i=1 j=1

                           measures variation within the samples (and in fact consists of the sum of the squared
                           residuals). The F statistic (7.48), developed for testing H0: µ1 = µ2 = · · · = µr , has
                           a numerator related to the first of these and a denominator related to the second. So
                           using the ANOVA F statistic amounts to a kind of analyzing of the raw variability
                           in y.

                                In recognition of their prominence in the calculation of the one-way ANOVA
                           F statistic and their usefulness as descriptive statistics in their own right, the three
                           sums (of squares) appearing in formulas (7.49) and (7.50) are usually given special
                           names and shorthand. These are stated here in definition form.
484 Chapter 7 Inference for Unstructured Multisample Studies

Definition 4       In a multisample study, (n - 1)s2, the sum of squared differences between the
                   raw data values and the grand sample mean, will be called the total sum of
                   squares and denoted as SSTot.

Definition 5       In an unstructured multisample study, ni (y¯ i - y¯ )2 will be called the treat-
                   ment sum of squares and denoted as SSTr.

Definition 6       In a multisample study, the sum of squared residuals, (y - y^ )2 (which is
                   (n - r )sP2 in the unstructured situation) will be called the error sum of squares
                   and denoted as SSE.

                        In the new notation introduced in these definitions, Proposition 1 states that in
                   an unstructured multisample context,

A third statement                   SSTot = SSTr + SSE                                (7.51)
   of the one-way
  ANOVA identity

                        Partially as a means of organizing calculation of the F statistic given in formula
                   (7.48) and partially because it reinforces and extends the variance partitioning
                   insight provided by formulas (7.49), (7.50), and (7.51), it is useful to make an
                   ANOVA table. There are many forms of ANOVA tables corresponding to various
                   multisample analyses. The form most relevant to the present situation is given in
                   symbolic form as Table 7.12.

                        The column headings in Table 7.12 are Source (of variation), Sum of Squares
                   (corresponding to the source), degrees of freedom (corresponding to the source),
                   Mean Square (corresponding to the source), and F (for testing the significance of
                   the source in contributing to the overall observed variability). The entries in the
                   Source column of the table are shown here as being Treatments, Error, and Total.
                   But the name Treatments is sometimes replaced by Between (Samples), and the

                   Table 7.12
                   General Form of the One-Way ANOVA Table

                   ANOVA Table (for testing H0: µ1 = µ2 = · · · = µr )

                   Source  SS       df                        MS            F

                   Treatments SSTr  r -1                      SSTr/(r - 1)  MSTr/MSE
                                    n-r                       SSE/(n - r )
                   Error   SSE

                   Total   SSTot n - 1
                      7.4 One-Way Analysis of Variance (ANOVA) 485

              name Error is sometimes replaced by Within (Samples) or Residual. The first two

              entries in the SS column must sum to the third, as indicated in equation (7.51).

              Similarly, the Treatments and Error degrees of freedom add to the Total degrees of
              freedom, (n - 1). Notice that the entries in the d f column are those attached to the
              numerator and denominator, respectively, of the test statistic in equation (7.48). The

              ratios of sums of squares to degrees of freedom are called mean squares, here the

              mean square for treatments (MSTr) and the mean square for error (MSE). Verify that
              in the present context, MSE = sP2 and MSTr is the numerator of the F statistic given
              in equation (7.48). So the single ratio appearing in the F column is the observed
              value of F for testing H0: µ1 = µ2 = · · · = µr .

Example 7     Consider once more the concrete strength study. It is possible to return to the raw
(continued )  data given in Table 7.1 and find that y¯ = 3,693.6, so

              SSTot = (n - 1)s2
                     = (5,800 - 3,693.6)2 + (4,598 - 3,693.6)2 + (6,508 - 3,693.6)2
                         + · · · + (2,631 - 3,693.6)2 + (2,490 - 3,693.6)2
                     = 52,772,190 (psi)2

              Further, as in Section 7.1, sP2 = 338,213.1 (psi)2 and n - r = 16, so

                      SSE = (n - r )sP2 = 5,411,410 (psi)2

              And from earlier in this section,

                                      r

                      SSTr = ni (y¯ i - y¯ )2 = 47,360,780

                                    i =1

              Then, plugging these and appropriate degrees of freedom values into the general
              form of the one-way ANOVA table produces the table for the concrete compres-
              sive strength study, presented here as Table 7.13.

              Table 7.13
              One-Way ANOVA Table for the Concrete Strength Study

              ANOVA Table (for testing H0: µ1 = µ2 = · · · = µ8)

              Source  SS                         df  MS     F

              Treatments 47,360,780 7 6,765,826 20.0

              Error   5,411,410 16 338,213

              Total   52,772,190 23
486 Chapter 7 Inference for Unstructured Multisample Studies

Example 7                Notice that, as promised by the one-way ANOVA identity, the sum of the
(continued )
                    treatment and error sums of squares is the total sum of squares. Also, Table

                    7.13 serves as a helpful summary of the testing process, showing at a glance the
                    observed value of F, the appropriate degrees of freedom, and sP2 = M S E.

                         The computations here are by no means impossible to do "by hand." But the

                    most sensible way to handle them is to employ a statistical package. Printout 1

                    shows the results of using MINTAB to create an ANOVA table. (The routine

                    under MINITAB's "Stat/ANOVA/One-way" menu was used.)

WWW                 Printout 1 ANOVA Table for a One-Way Analysis
                                     of the Concrete Strength Data

                    One-way Analysis of Variance

                    Analysis of Variance for strength

                    Source   DF     SS            MS                F        P
                                                              20.00    0.000
                    formula  7 47360781 6765826

                    Error    16 5411409 338213

                    Total    23 52772190

                                                       Individual 95% CIs For Mean

                                                       Based on Pooled StDev

                    Level    N         Mean  StDev     -----+---------+---------+---------+-
                                    5635.3   965.6
                    1        3      5753.3   432.3                                 (---*----)
                                    4527.3   509.9
                    2        3      3442.3   356.4                                       (---*---)
                                    2923.7   852.9
                    3        3      3324.7   353.5                              (---*----)
                                    1551.3   505.5
                    4        3      2390.7   302.5                     (----*---)

                    5        3       581.6                             (---*----)

                    6        3                                         (----*---)

                    7        3                         (----*---)

                    8        3                                (----*---)

                                                       -----+---------+---------+---------+-

                    Pooled StDev =                            1600     3200        4800     6400

                         You may recall having used a breakdown of a "raw variation in the data" earlier
                    in this text (namely, in Chapter 4). In fact, there is a direct connection between the
                    present discussion and the discussion and use of R2 in Sections 4.1, 4.2, and 4.3.
                    (See Definition 3 in Chapter 4 and its use throughout those three sections.) In the
                    present notation, the coefficient of determination defined as a descriptive measure
                    in Section 4.1 is

The coefficient of                                R2 = SSTot - SSE                                  (7.52)
determination in                                             SSTot

  general sums of
 squares notation

                    (Fitted values for the present situation are the sample means and SSE is the sum

                    of squared residuals here, just as it was earlier.) Expression (7.52) is a perfectly
                    general recasting of the definition of R2 into "SS" notation. In the present one-way

                    context, the one-way identity (7.51) makes it possible to rewrite the numerator of
                       7.4 One-Way Analysis of Variance (ANOVA) 487

                       the right-hand side of formula (7.52) as SSTr. So in an unstructured r -sample study
                       (where the fitted values are the sample means)

   The coefficient     R2 = SSTr                      (7.53)
of determination              SSTot

      in a one-way
             analysis

                       That is, the first entry in the SS column of the ANOVA table divided by the total entry
                       of that column can be taken as "the fraction of the raw variability in y accounted for
                       in the process of fitting the equation yi j  µi to the data."

Example 7              In the concrete compressive strength study, a look at Table 7.13 and equation
(continued )           (7.53) shows that

                       R2 = SSTr = 47,360,780 = .897
                              SSTot 52,772,190

                       That is, another way to describe these data is to say that differences between
                       concrete formulas account for nearly 90% of the raw variability observed in
                       compressive strength.

                       So the ANOVA breakdown of variability not only facilitates the testing of H0: µ1 =
                       µ2 = · · · = µr but it also makes direct connection with the earlier descriptive anal-
                       yses of what part of the raw variability is accounted for in fitting a model equation.

7.4.4                  Random Effects Models and Analyses (Optional )

                       On occasion, the r particular conditions leading to the r samples in a multisample
                       study are not so much of interest in and of themselves, as they are of interest as
                       representing a wider set of conditions. For example, in the nondestructive testing of
                       critical metal parts, if ni = 3 mechanical wave travel-time measurements are made
                       on each of r = 6 parts selected from a large lot of such parts, the six particular parts
                       are of interest primarily as they provide information on the whole lot.

                            In such situations, rather than focusing formal inference on the particular r
                       means actually represented in the data (i.e., µ1, µ2, . . . , µr ), it is more natural
                       to make inferences about the mechanism that generates the means µi . And it is
                       possible, under appropriate model assumptions, to use the ANOVA ideas introduced
                       in this section in this way. The balance of this section is concerned with how this
                       is done.

                            The most commonly used probability model for the analysis of r -sample data,
                       where the r conditions actually studied represent a much wider set of conditions
488 Chapter 7 Inference for Unstructured Multisample Studies

                     of interest, is a variation on the one-way model of this chapter called the one-way
                     random effects model. It is built on the usual one-way assumptions that

                                               yi j = µi + i j                               (7.54)

     Random effects  where the i j are iid normal (0,  2) random variables. But it doesn't treat the means
model assumptions    µi as parameters/unknown constants. Instead, the means µ1, µ2, . . . , µr are treated
                     as (unobservable) random variables independent of the i j 's and themselves iid
                     according to some normal distribution with an unknown mean µ and unknown
                     variance 2. The random variables µi are now called random (treatment) effects,
                     and the variances  2 and 2 are called variance components. The objects of
                     formal inference become µ (the mean of the random effects) and the two variance
                     components  2 and 2.

Example 8            Magnesium Contents at Different Locations on an Alloy Rod
                     and the Random Effects Model

                     Youden's Experimentation and Measurement contains an interesting data set
                     concerned with the magnesium contents of different parts of a long rod of mag-
                     nesium alloy. A single ingot had been drawn into a rod of about 100 m in length,
                     with a square cross section about 4.5 cm on a side. r = 5 flat test pieces 1.2 cm
                     thick were cut from the rod (after it had been cut into 100 bars and 5 of these
                     randomly selected to represent the rod), and multiple magnesium determinations
                     were made on the 5 specimens. ni = 10 of the resulting measurements for each
                     specimen are given in Table 7.14. (There were actually other observations made
                     not listed in Table 7.14. And some additional structure in Youden's original data

                     Table 7.14
                     Measured Magnesium Contents for Five Alloy Specimens

                     Specimen 1 Specimen 2 Specimen 3 Specimen 4               Specimen 5

                     76           69                          73  73                70
                                                                                    66
                     71           71                          69  75                68
                                                                                    68
                     70           68                          68  69                64
                                                                                    70
                     67           71                          69  72                69
                                                                                    67
                     71           66                          70  69                69
                                                                                    67
                     65           68                          70  69
                                                                                y¯ 5 = 67.8
                     67           71                          65  72            s5 = 1.9

                     71           69                          67  63

                     66           70                          67  69

                     68           68                          64  69

                     y¯ 1 = 69.2  y¯ 2 = 69.1  y¯ 3 = 68.2        y¯ 4 = 70.0
                     s1 = 3.3     s2 = 1.7     s3 = 2.6           s4 = 3.3
              7.4 One-Way Analysis of Variance (ANOVA) 489

              will also be ignored for present purposes.) The units of measurement in Table

              7.14 are .001% magnesium.

                   In this example, on the order of 8,300 test specimens could be cut from the

              100 m rod. The purpose of creating the rod was to provide secondary standards for

              field calibration of chemical analysis instruments. That is, laboratories purchasing

              pieces of this rod could use them as being of "known" magnesium content to

              calibrate their instruments. As such, the practical issues at stake here are not
              primarily how the r = 5 particular test specimens analyzed compare. Rather, the
              issues are what the overall magnesium content is and whether or not the rod is

              consistent enough in content along its length to be of any use as a calibration tool.

              A random effects model and inference for the mean effect µ and the variance
              components are quite natural in this situation. Here, 2 represents the variation
              in magnesium content among the potentially 8,300 different test specimens, and
               2 represents measurement error plus variation in magnesium content within the

              1.2 cm thick specimens, test location to test location.

                   When all of the r sample sizes ni are the same (say, equal to m), it turns out to
              be quite easy to do some diagnostic checking of the aptness of the normal random
              effects model (7.54) and make subsequent inferences about µ,  2, and 2. So this
              discussion will be limited to cases of equal sample sizes.

                   As far as investigation of the reasonableness of the model restrictions on the

              distribution of the µi and inference for µ are concerned, a key observation is that

                     1m
              y¯ i = m (µi + i j ) = µi + ¯i

                            j =1

              (where, of course, ¯i is the sample mean of i1, . . . , im). Under the random effects
              model (7.54), these y¯ i = µi + ¯i are iid normal variables with mean µ and variance
              2 +  2/m. So normal-plotting the y¯ i is a sensible method of at least indirectly
              investigating the appropriateness of the normal distribution assumption for the µi .
              In addition, the fact that the model says the y¯ i are independent normal variables with
              mean µ and a common variance suggests that the small-sample inference methods
              from Section 6.3 should simply be applied to the sample means y¯ i in order to do infer-
              ence for µ. In doing so, the "sample size" involved is the number of y¯ i 's--namely, r .

Example 8     For the magnesium alloy rod, the r = 5 sample means are in Table 7.14. Figure
(continued )  7.12 gives a normal plot of those five values, showing no obvious problems with
              a normal random effects model for specimen magnesium contents.

                   To find a 95% two-sided confidence interval for µ, we calculate as follows
              (treating the five values y¯ i as "observations"). The sample mean (of y¯ i 's) is

                    15
              y¯ . = 5 y¯ i = 68.86

                           i =1
490 Chapter 7 Inference for Unstructured Multisample Studies

                  Example 8
                 (continued )

                                                                    1.0

          Standard normal quantile  0

                                    -1.0

                                          68                             69        70

                                                 Sample mean quantile

                                    Figure 7.12 Normal plot of five specimen mean
                                    magnesium contents

and the sample variance (of y¯ i 's) is

                                              1  5

                                          5 - 1 (y¯ i - y¯ .) = .76          2

                                                 i =1

so that the sample standard deviation (of y¯ i 's) is

                                              1     5

                                          5 - 1 (y¯ i - y¯ .) = .87             2

                                                 i =1

Applying the small-sample confidence interval formula for a single mean from
Section 6.3 (since r - 1 = 4 degrees of freedom are appropriate), a two-sided
95% confidence for µ has endpoints

                                                                  .87
                                                 68.86 ± 2.776 

                                                                     5

that is,

                                          67.78 × 10-3% and 69.94 × 10-3%

These limits provide a notion of precision appropriate for the number 68.86 ×
10-3% as an estimate of the rod's mean magnesium content.
                                        7.4 One-Way Analysis of Variance (ANOVA) 491

                                 It is useful to write out in symbols what was just done to get a confidence
                            interval for µ. That is, a sample variance of y¯ i 's was used. This is

                            1  r        1  r                 1  1

                            r - 1 (y¯ i - y¯ .) = m(r - 1) m(y¯ i - y¯ ) = m(r - 1) SSTr = m MSTr22

                               i =1        i =1

                            because all ni are m and y¯ . = y¯ in this case. But this means that under the assumptions
                            of the one-way normal random effects model, a two-sided confidence interval for µ
                            has endpoints

    Balanced data                                   MSTr           (7.55)
confidence limits                          y¯ . ± t

   for the overall                                    mr
       mean in the
                            where t is such that the probability the tr-1 distribution assigns to the interval
one-way random              between -t and t is the desired confidence. One-sided intervals are obtained in the
    effects model           usual way, by employing only one of the endpoints in display (7.55).

7.4.5                       ANOVA-Based Inference

                            for Variance Components (Optional )

                            Turning attention to the variance components in the random effects model (7.54),

                            first note that as far as diagnostic checking of the assumption that the i j are iid
                            normal variables and inference for  2 = Var i j are concerned, all of the methods of
                            Section 7.1 remain in force. If one thinks of holding the µi fixed in formula (7.54),
                            it is clear that (conditional on the µi ) the random effects model treats the r samples
                            as random samples from normal distributions with a common variance. So before
                            doing inference for  2 (or 2 for that matter) via usual normal theory formulas,
                            it is advisable to do the kind of sample-by-sample normal-plotting and plotting of

                            residuals illustrated in Section 7.1. And if it is then plausible that the i j are iid
                            normal (0,  2) variables, formula (7.10) of Section 7.1 can be used to produce a
                            confidence interval for  2, and significance testing for  2 can be done based on the
                            fact that r (m - 1)sP2/ 2 has a r(m 2 -1) distribution.

                                 Inference for 2 borrows from things already discussed but also provides a new
                            wrinkle or two of its own. First, significance testing for

                                           H0: 2 = 0               (7.56)

                            is made possible by the observation that if H0 is true, then (just as when H0: µ1 =
                            µ2 = · · · = µr in the case where the µi are not random effects but fixed parameters)
                            the n = mr observations are all coming from a single normal distribution. So

  ANOVA test statistic for                 F = MSTr                (7.57)
H0: 2 = 0 in the one-way                         MSE

    random effects model
492 Chapter 7 Inference for Unstructured Multisample Studies

                      has an Fr-1, n-r distribution under the assumptions of the random effects model
                      (7.54) when the null hypothesis (7.56) holds. Thus, the same one-way ANOVA F
                      test used to test H0: µ1 = µ2 = · · · = µr when the means µi are considered fixed
                      parameters can also be used to test H0: 2 = 0 under the assumptions of the random
                      effects model.

                           As far as estimation goes, it doesn't turn out to be possible to give a simple
                      confidence interval formula for 2 directly. But what can be done in a straightforward
                      fashion is to give both a natural ANOVA-based single-number estimate of 2 and
                      a confidence interval for the ratio 2/ 2. To accomplish the first of these, consider
                      the mean values of random variables MSTr and MSE (= sP2) under the assumptions
                      of the random effects model. Not too surprisingly,

                      E(MSE) = EsP2 =  2                                         (7.58)

                      (After all, sP2 has been used to approximate  2. That the "center" of the probability
                      distribution of sP2 is  2 should therefore seem only reassuring.) And further,

                      E(MSTr) =  2 + m2                                          (7.59)

                      Then, from equations (7.58) and (7.59),

                                                     1 E(MSTr) - E(MSE) = 2      (7.60)
                                                     m
                      or

                                                        E 1 (MSTr - MSE) = 2
                                                           m

                           So equation (7.60) suggests that the random variable

                      1 (MSTr - MSE)                                             (7.61)
                      m

                      is one whose distribution is centered about the variance component 2 and thus is a
                      natural ANOVA-based estimator of 2. The variable in display (7.61) is potentially
                      negative. When that occurs, common practice is to estimate 2 by 0. So the variable
                      actually used to estimate 2 is

 An ANOVA-based       ^ 2 = max 0, 1 (MSTr - MSE)                                (7.62)
    estimator of the                    m

treatment variance

                      Facts (7.58) and (7.60), which motivate this method of estimating 2, are important
                      enough that they are often included as entries in an Expected Mean Square column
                      added to the one-way ANOVA table when testing H0: 2 = 0.
                         7.4 One-Way Analysis of Variance (ANOVA) 493

                          Although no elementary confidence interval for 2 is known, it is possible to
                     give one for the ratio 2/ 2. A basic probability fact is that under the assumptions
                     of the random effects model (7.54),

                                  MSTr
                         F =  2 + m2

                                   MSE
                                    2

                     has an Fr-1, n-r distribution. Some algebraic manipulations beginning from this fact
                     show that the interval with endpoints

Confidence limits    1 MSTr - 1  and 1 MSTr - 1                    (7.63)
  for  2 / 2 in the  m U · MSE          m L · MSE

one-way random

     effects model

                     can be used as a two-sided confidence interval for 2/ 2, where the associated
                     confidence is the probability the Fr-1, n-r distribution assigns to the interval (L , U ).
                     One-sided intervals for  2 / 2 can be had by using only one of the endpoints and
                     choosing L or U such that the probability assigned by the Fr-1, n-r distribution to
                     (L , ) or (0, U ) is the desired confidence.

Example 8            Consider again the measured magnesium contents for specimens cut from the
(continued )
                     100 m alloy rod. Some normal plotting shows the "single variance normal i j " part
                     of the model assumptions (7.54) to be at least not obviously flawed. Sample-by-

                     sample normal plots show fair linearity (at least after allowing for the discreteness

                     introduced in the data by the measurement scale used), except perhaps for sample

                     4, with its five identical values. The five sample standard deviations are roughly

                     of the same order of magnitude, and the normal plot of residuals in Figure 7.13
                     is pleasantly linear. So it is sensible to consider formal inference for  2 and 2
                     based on the normal theory model.

                          Table 7.15 is an ANOVA table for the data of Table 7.14. From Table 7.15,
                     the p-value for testing H0: 2 = 0 is the F4,45 probability to the right of 1.10.
                     According to Tables B.6, this is larger than .25, giving very weak evidence of

                     detectable variation between specimen mean magnesium contents.

                          The EMS column in Table 7.15 is based on relationships (7.58) and (7.59)
                     and is a reminder first that MSE = sP2 = 6.88 serves as an estimate of  2. So
                     multiple magnesium determinations on a give to have a standard deviation on the order of n specimen would be estimated 6.88 = 2.6 × 10-3%. Then the
                     expected mean squares further suggest that 2 be estimated by

                     ^ 2 = 1 (MSTr - MSE) = 1 (7.58 - 6.88) = .07
                     10          10
494 Chapter 7 Inference for Unstructured Multisample Studies

Example 8                 Standard normal quantile  3.0
(continued )

                                                     1.5                                               2
                                                     0.0                                             4
                                                    -1.5                                            5
                                                                                              32
                                                                                          22
                                                                                    5
                                                                                  23
                                                                            2
                                                                       2

                                          -5.0 -2.5 0.0 2.5 5.0 7.5
                                                          Residual quantile

                          Figure 7.13 Normal plot of residuals for the magnesium
                          content study

                          Table 7.15
                          ANOVA Table for the Magnesium Content Study

                                                          ANOVA Table (for testing H0: 2 = 0)

                          Source                               SS d f MS                                  EMS  F

                          Treatments 30.32 4 7.58  2 + 102 1.10
                                                                                                            2
                          Error                                309.70 45 6.88

                          Total                                340.02 49

              as in equation (7.62). So an estimate of  is
                                                 .07 = .26 × 10-3%

              That is, the standard deviation of specimen mean magnesium contents is estimated

              to  be  on  the                       order  of  1   of  the  standard  deviation                associated  with  multiple
                                                               10
              measurements on a single specimen.

                  A confidence interval for  2 could be made using formula (7.10) of Section

              7.1. That will not be done here, but formula (7.63) will be used to make a one-

              sided 90% confidence interval of the form (0, #) for  / . The .90 quantile of
              the F45,4 distribution is about 3.80, so the .10 quantile of the F4,45 distribution is
              about 3.80 1 . Then taking the root of the second endpoint given in display (7.63), a
              90% upper confidence bound for  / is

                                                                                                          

                                                               1         7.58 - 1 = .56
                                                                        1

                                                               10              6.88
                                                                       3.80
                                   7.4 One-Way Analysis of Variance (ANOVA) 495

The bottom line here is that  is small compared to  and is not even clearly
other than 0. Most of the variation in the data of Table 7.14 is associated with the
making of multiple measurements on a single specimen. Of course, this is good
news if the rod is to be cut up and distributed as pieces having known magnesium
contents and thus useful for measurement instrument calibration.

Section 4 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Return to the situation in Exercises 1 of Sections      Waves in Railroad Rail" by Bray and Leon-
   7.1 through 7.3 (and the pressure/density data of       Salamanca (Materials Evaluation, 1985). Given
   Example 1 in Chapter 4).                                are measurements in nanoseconds of the travel time
    (a) In part (b) of Exercise 1 of Section 7.3, you      (in excess of 36.1 µs) of a certain type of mechan-
        were asked to make simultaneous confidence         ical wave induced by mechanical stress in railroad
        intervals for all differences in the r = 5 mean    rails. Three measurements were made on each of
        densities. From your intervals, what kind of       six different rails.
        a p-value (small or large) do you expect to
        find when testing the equality of these means?                         Travel Time
        Explain.                                           Rail (nanoseconds above 36.1 µs)
   (b) Make an ANOVA table (in the form of Table
        7.12) for the data of Example 1 in Chapter 4.      1  55, 53, 54
        You should do the calculations by hand first and
        then check your arithmetic using a statistical     2  26, 37, 32
        computer package. Then use the calculations
        to find both R2 for the one-way model and also     3  78, 91, 85
        the observed level of significance for an F test
        of the null hypothesis that all five pressures     4  92, 100, 96
        produce the same mean density.
                                                           5  49, 51, 50
2. Return to the tilttable study of Exercises 2 of Sec-
   tions 7.1 through 7.3.                                  6  80, 85, 83
    (a) In part (b) of Exercise 2 of Section 7.3, you
        were asked to make simultaneous confidence             (a) Make plots to check the appropriateness of a
        intervals for all differences in the r = 4 mean            one-way random effects analysis of these data.
        tilttable ratios. From your intervals, what kind           What do these suggest?
        of a p-value (small or large) do you expect to
        find when testing the equality of these means?        (b) Ignoring any possible problems with the stan-
        Explain.                                                   dard assumptions of the random effects model
   (b) Make an ANOVA table (in the form of Table                   revealed in (a), make an ANOVA table for these
        7.12) for the data of Exercise 2 of Section 7.1.           data (like Table 7.15) and find estimates of 
        Then find both R2 for the one-way model and                and  . What, in the context of this problem,
        also the observed level of significance for an             do these two estimates measure?
         F test of the null hypothesis that all four vans
        have the same mean tilttable ratio.                    (c) Find and interpret a two-sided 90% confidence
                                                                   interval for the ratio  / .
3. The following data are taken from the paper "Zero-
   Force Travel-Time Parameters for Ultrasonic Head-       4. The following are some general questions about the
                                                              random effects analyses:
                                                               (a) Explain in general terms when a random effects
                                                                   analysis is appropriate for use with multisam-
                                                                   ple data.
                                                              (b) Consider a scenario where r = 5 different tech-
                                                                   nicians employed by a company each make
496 Chapter 7 Inference for Unstructured Multisample Studies

    m = 2 measurements of the diameter of a par-      run mean measurements for various techni-
    ticular widget using a particular gauge in a      cians ( ). The sums of squares are in units
    study of how technician differences show up       of square inches.
    in diameter data the company collects. Under
    what circumstances would a random effects                 ANOVA Table
    analysis of the resulting data be appropriate?
(c) Suppose that the following ANOVA table was        Source  SS  df       MS      F
    made in a random effects analysis of data like
    those described in part (b). Give estimates of    Technician .0000136 4 .0000034 1.42
    the standard deviation associated with repeat
    diameter measurements for a given technician      Error   .0000120 5 .0000024
    ( ) and then for the standard deviation of long-
                                                      Total   .0000256 9

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

7.5 Shewhart Control Charts
       for Measurement Data

                    This text has repeatedly made use of the phrase "stable process" and emphasized
                    that unless data generation has associated with it a single, repeatable pattern of
                    variation, there is no way to move from data in hand to predictions and inferences.
                    The notion that "baseline" or "inherent" variation evident in the output of a process
                    is a principal limitation on system performance has also been stressed. But no tools
                    have yet been presented that are specifically crafted for evaluating the extent to
                    which a data-generation mechanism can be thought of as stable, or for determining
                    the size of the baseline variation of a process.

                         W. Shewhart, working in the late 1920s and early 1930s at Bell Laboratories,
                    developed an extremely simple yet effective device for doing these jobs. This tool
                    has become known as the Shewhart control chart. (Actually, the nonstandard name
                    Shewhart monitoring chart is far more descriptive. It also avoids the connotations of
                    automatic/feedback process adjustment that the word control may carry for readers
                    familiar with the field of engineering control.)

                         This section and the next introduce the topic of Shewhart control charts, be-
                    ginning here with charts for measurement data. This section begins with some
                    generalities, discussing Shewhart's conceptualization of process variability. Then
                    the specific instances of Shewhart control charts for means, ranges, and standard
                    deviations are considered in turn. Finally, the section closes with comments about
                    the place of control charts in the improvement of modern industrial processes.

7.5.1  Generalities about Shewhart Control Charts

       Stability of an engineering data-generating process refers to a consistency or re-
       peatability over time. When one thinks of empirically assessing the stability of a
       process, it is therefore clear that samples of data taken from it at different points in
       time will be needed.
                             7.5 Shewhart Control Charts for Measurement Data 497

Example 9  Monitoring the Lengths of Sheets Cut on a Ream Cutter

           Shervheim and Snider worked with a company on the cutting of a rolled material

           into sheets using a ream cutter. Every two minutes they sampled five consecutive

           sheets and measured their lengths. Part of the students' length data are given in

           Table  7.16,  in  units  of  1   inch  over  a  certain  reference  length.
                                        64
           One of the goals of the study was to investigate the stability of the cutting

           process over time. The kind of multisample data the students collected, where

           the samples were separated and ordered in time, are ideal for that purpose.

                                    Table 7.16
                                    Lengths of 22 Samples of Five Sheets
                                    Cut on a Ream Cutter

                                    Sample Time Excess Length

                                        1 12:40 9, 10, 7, 8, 10

                                        2 12:42 6, 10, 8, 8, 10

                                        3 12:44 11, 10, 9, 5, 11

                                        4 12:46 10, 9, 9, 8, 7

                                        5 12:48 7, 5, 11, 9, 5

                                        6 12:50 9, 9, 10, 7, 9

                                        7 12:52 10, 8, 6, 11, 8

                                        8 12:54 7, 10, 8, 8, 9

                                        9 12:56 10, 9, 9, 5, 12

                                        10 12:58 8, 10, 6, 8, 10

                                        11        1:00 8, 10, 4, 7, 8

                                        12        1:02 8, 10, 10, 6, 9

                                        13        1:04 10, 8, 6, 7, 10

                                        14        1:06 8, 6, 10, 8, 8

                                        15        1:08 13, 5, 8, 8, 13

                                        16        1:10 10, 4, 9, 10, 8

                                        17        1:12 7, 7, 9, 7, 8

                                        18        1:14 9, 7, 7, 9, 6

                                        19        1:16 5, 10, 5, 8, 10

                                        20        1:18 9, 6, 8, 9, 11

                                        21        1:20 6, 10, 11, 5, 6

                                        22        1:22 15, 3, 7, 9, 11

                Data (like those in Table 7.16) collected for purposes of assessing process
           stability will often be r samples of some fixed sample size m, lacking any structure
           except for the fact that they were taken in a particular time order. So Shewhart control
498 Chapter 7 Inference for Unstructured Multisample Studies

                       charting is at home in this chapter that treats inference methods for unstructured
                       multisample studies.

                            Shewhart's fundamental qualitative insight regarding variation seen in process
                       data over time is that

Shewhart's partition   Overall process = baseline variation + variation that     (7.64)
 of process variation  variation                              can be eliminated

Shewhart control       Shewhart conceived of baseline variation as that which will remain even under the
               charts  most careful process monitoring and appropriate physical interventions--an inherent
                       property of a particular system configuration, which cannot be reduced without basic
                       changes in the physical process or how it is run. This is variation due to common
                       (universal) causes or system causes. Other terms used for it are random variation
                       and short-term variation. In the context of the cutting operation of Example 9, this
                       kind of variation might be seen in consecutive sheet lengths cut on a single ream
                       cutter, from a single roll of material, without any intervening operator adjustments,
                       following a particular plant standard method of machine operation, etc. It is variation
                       that comes from hundreds of small unnameable, unidentifiable physical causes.
                       When only this kind of variation is acting, it is reasonable to call a process "stable."

                            The second component of overall process variation is variation that can poten-
                       tially be eliminated by appropriate physical intervention. This kind of variation has
                       been called variation due to special or assignable causes, nonrandom variation,
                       and long-term variation. In the sheet-cutting example, this might be variation in
                       sheet length brought about by undesirable changes in tension on the material being
                       cut, roller slippage on the cutter, unwarranted operator adjustments to the machine,
                       eccentricities associated with how a particular incoming roll of material was wound,
                       etc. Shewhart reasoned that being able to separate the two kinds of variation is a
                       prerequisite to ensuring good process performance. It provides a basis for knowing
                       when to intervene and find and eliminate the cause of any assignable variation,
                       thereby producing process stability.

                            Shewhart's method for separating the two components of overall variation in
                       equation (7.64) is graphical and based on the following logic. First, periodically
                       taken samples are reduced to appropriate summary statistics, and the summary
                       statistics are plotted against time order of observation. To this simple time-plotting
                       of summary statistics, Shewhart added the notion that lines be drawn on the chart to
                       separate values that are consistent with a "baseline variation only" view of process
                       performance from those that are not. Shewhart called these lines of demarcation
                       control limits. When all plotted points fall within the control limits, the process is
                       judged to be stable, subject only to chance causes. But when a point falls outside
                       the limits, physical investigation and intervention is called for, to eliminate any
                       assignable cause of variation. Figure 7.14 is a plot of a generic control chart for a
                       summary statistic, w. It shows upper and lower control limits (UCL and LCL), some
                       plotted values, and one "out of control" point.

                            There are any number of charts that fit the general pattern of Figure 7.14.
                       For example, common possibilities relevant in the sheet-cutting case of Example 9
                       include control charts for the sample mean, sample range, and sample standard
                                 7.5 Shewhart Control Charts for Measurement Data 499
                          w

                                                                         Upper Control Limit

                          Center line

                                                                       Lower Control Limit

                                                                      "Out of Control"
                                                                      point

                          1 2 3 4 5 6 7 8 9 10 11
                                                    Time

                          Figure 7.14 Generic Shewhart control chart for a
                          statistic w

    Setting control       deviation of sheet lengths. These will presently be discussed in detail. But first,
                  limits  some additional generalities still need to be considered.

"Standards given"              For one thing, there remains the matter of how to set the position of the control
              contexts    limits. Shewhart argued that probability theory can be applied and appropriate stable-
                          process/iid-observations distributions developed for the plotted statistics. Then small
                          upper and lower percentage points for these can be used to establish control limits.
                          As an example, the central limit material in Section 5.5 should have conditioned
                          the reader to think of sample means as approximately normal with mean µ and
                          standard deviation / m, where µ and  describe individual observations and m is
                          the sample size. So for plotting sample means, the upper and lower control limits
                          might be set at small upper and lower percentage points of the normal distribution
                          with mean µ and standard deviation / m, where µ and  are a process mean and
                          short-term standard deviation, respectively.

                               Two different circumstances are possible regarding the origin of values for
                          process parameters used to produce control limits. In some applications, values of
                          process parameters (and therefore, parameters for the "stable process" distribution
                          of the plotted statistic) and thus control limits are provided from outside the data
                          producing the charted values. Such circumstances will be called "standards given"
                          situations. For emphasis, the meaning of this term is stated here in definition form.

Definition 7              When control limits are derived from data, requirements, or knowledge of the
                          behavior of a process that are outside the information contained in the samples
                          whose summary statistics are to be plotted, the charting is said to be done with
                          standards given.
500 Chapter 7 Inference for Unstructured Multisample Studies

"Standards given"         For example, suppose that in the sheet-cutting context of Example 9, past
       charting and
                     experience with the ream cutter indicates that a process short-term standard deviation
hypothesis testing   of  = 1.9 ( 641 in.) is appropriate when the cutter is operating as it should. Further,
                     suppose that legal and other considerations have led to the establishment of a target
                     process mean of µ = 10.0 ( 641 in. above the reference length). Then control limits
                     based on these values and applied to data collected tomorrow would be "standards

                     given" control limits.

                          One way to think about a "standards given" control chart is as a graphical means

                     of repeatedly testing the hypothesis

                     H0: Process parameters are at their standard values               (7.65)

Retrospective        When a plotted point lies inside control limits, one is directed to a decision in favor
       contexts      of hypothesis (7.65) for the time period in question. A point plotting outside limits
                     makes hypothesis (7.65) untenable at the time represented by the sample.

                          In contrast to "standards given" applications, there are situations in which no
                     external values for process parameters are used. Instead, a single set of samples
                     taken from the process is used to both develop a plausible set of parameters for the
                     process and also to judge the stability of the process over the period represented by
                     the data. The terms retrospective or "as past data" will be used in this text for such
                     control charting applications.

Definition 8         When control limits are derived from the same samples whose summary
                     statistics are plotted, the charting is said to be done retrospectively or "as
                     past data."

      Retrospective       In the context of Example 9, control limits derived from the data in Table 7.16
       charting and  and applied to summary statistics for those same data would be "as past data" control
hypothesis testing   limits for assessing the cutting process stability over the period from 12:40 through
                     1:22 on the day the data were taken.

                          A way of thinking about a retrospective control chart is a graphical means of
                     testing the hypothesis

                     H0: A single set of process parameters was acting throughout the  (7.66)
                         time period studied

                     When a point or points plot outside of control limits derived from the whole data
                     set, the hypothesis (7.66) of process stability over the period represented by the data
                     becomes untenable.

7.5.2                "Standards Given" x¯ Control Charts

                     The single most famous and frequently used Shewhart control chart is the one
                     where sample mean measurements are plotted. Control charts are typically named
                                   7.5 Shewhart Control Charts for Measurement Data 501

Notational conventions        by the symbols used for the plotted statistics. So the following discussion con-
             for x¯ charting  cerns Shewhart x¯ charts. In using this terminology (and other notation from
                              the statistical quality control field), this text must choose a path through nota-
                              tional conflicts that exist between the most common usages in control charting and
                              those for other multisample analyses. The options that will be exercised here must
                              be explained.

                                   In the first place, to this point in Chapter 7 (also in Chapter 4, for that matter) the
                              symbol y has been used for the basic response variable in a multisample statistical
                              engineering study, y¯ i for a sample mean, and y¯ . and y¯ for unweighted and weighted
                              averages of the y¯ i , respectively . In contrast, in Chapters 3 and 6, where the discussion
                              centered primarily on one- and two-sample studies, x was used as the basic response
                              variable and x¯ (or x¯ i in the case of two-sample studies) to stand for a sample
                              mean. Standard usage in Shewhart control charting is to use the x and x¯ (x¯ i )
                              convention, and the precedent is so strong that this section will adopt it as well.
                              In addition, historical momentum in control charting dictates that rather than using
                              x¯ . notation,

 Average sample                    x¯¯ =  1r     x¯ i  (7.67)
    mean (quality
                                          r i=1
control notation)

                              is used for the average of sample means. But this "bar bar" or "double bar" notation
                              is used in this book only in this section.

                                   Something must also be said about notation for sample sizes. It is universal
                              to use the notation ni for an individual sample size. But there is some conflict
                              when all sample sizes ni have a common value. The convention in this chapter has
                              been to use m for such a common value and n for ni . Standard quality control
                              notation is to instead use n for a common sample size. In this matter, we will
                              continue to use the conventions established thus far in Chapter 7, believing that to
                              do otherwise invites too much confusion. But the reader is hereby alerted to the
                              fact that the m used here is usually going to appear as n in other treatments of
                              control charting.

                                   Having dealt with the notational problems, we turn to the making of a "standards
                              given" Shewhart x¯ chart based on samples of size m. An iid model for observations
                              from a process with mean µ and standard deviation  produces

                                     E x¯ = µ          (7.68)

                              and

                                                 
                                     Var x¯ =          (7.69)
                                                 m

                              and often an approximately normal distribution for x¯ . The fact that essentially all of
                              the probability of a normal distribution is within 3 standard deviations of its mean
502 Chapter 7 Inference for Unstructured Multisample Studies

                                  led Shewhart to suggest that given process standards µ and  , x¯ chart control limits
                                  could be set at

"Standards given"                                                                                        m                                   m                     (7.70)
control limits for x¯                                                            LCLx¯ = µ - 3              and UCLx¯ = µ + 3 

                       Additionally, he suggested drawing a center line on an x¯ chart at the standard

                       mean µ.

                       Limits (7.70) have proved themselves of great utility even in cases where m is

                       fairly small and there is no reason to expect a normal distribution for observations

                       in a sampling period. Formulas (7.68) and (7.69) hold regardless of whether a

                       process distribution is normal, and the 3-sigma (of the plotted statistic x¯ ) control

                       limits in display (7.70) tend to bracket most of the distribution of x¯ under nearly

                       any circumstances. (Indeed, a crude but universal analysis, based on a probability

                       version of the Chebyschev theorem stated in Section 3.3 for relative frequency

                       distributions,  guarantees                                           that   limits  (7.70)  will  bracket  at  least  8  of  the  distribution
                                                                                                                                             9
                       of x¯ in any stable process context.)

Example 9              Consider the use of process standards µ = 10 and  = 1.9 in x¯ charting based
(continued )
                       on  the  data                                             given  in  Table  7.16  (recall  the  values  there  are  in   units  of      1   in.  over
     WWW                                                                                                                                                       64
                       a reference length). With these standard values for µ and  , since the r = 22

                                Sample mean length, x (641 in. above reference)                                                                           UCL
                                                                                 12

                                                                                 11
                                                                                                                                                  Center line

                                                                                 10

                                                                                 9

                                                                                 8

                                                                                 7 LCL

                                                                                                   5              10                            20

                                                                                                            Sample number, i

                                Figure 7.15 "Standards given" Shewhart x¯ control chart for cut
                                sheet lengths
    7.5 Shewhart Control Charts for Measurement Data 503

samples are all of size m = 5, formulas (7.70) indicate control limits

           1.9                             1.9
UCLx¯ = 10 + 3  = 12.55 and LCLx¯ = 10 - 3  = 7.45
           5                               5

along with a center line drawn at µ = 10. Table 7.17 gives some sample-by-
sample summary statistics for the data of Table 7.16, including the sample means
x¯ i . Figure 7.15 is a "standards given" Shewhart x¯ chart for the same data.

     Figure 7.15 shows two points plotting below the lower control limit: the

means for samples 5 and 11. But it is perfectly obvious from the plot what was

going on in the data of Table 7.16 to produce the "out of control" points and
corresponding debunking of hypothesis (7.65). Not one of the r = 22 plotted

Table 7.17
Sample-by-Sample Summary Statistics
for 22 Samples of Sheet Lengths

i, Sample       x¯ i                 si          Ri

1               8.8                  1.30          3
                                                   4
2               8.4                  1.67          6
                                                   3
3               9.2                  2.49          6
                                                   3
4               8.6                  1.14          5
                                                   3
5               7.4                  2.61          7
                                                   4
6               8.8                  1.10          6
                                                   4
7               8.6                  1.95          4
                                                   4
8               8.4                  1.14          8
                                                   6
9               9.0                  2.55          2
                                                   3
10              8.4                  1.67          5
                                                   5
11              7.4                  2.19          6
                                                 12
12              8.6                  1.67
                                           R = 109
13              8.2                  1.79

14              8.0                  1.41

15              9.4                  3.51

16              8.2                  2.49

17              7.6                  .89

18              7.6                  1.34

19              7.6                  2.51

20              8.6                  1.82

21              7.6                  2.70

22              9.0                  4.47

           x¯ = 183.4  s = 44.41
504 Chapter 7 Inference for Unstructured Multisample Studies

Example 9     sample means lies at or above 10. If an average sheet length of µ = 10 was truly
(continued )  desired, a simple adjustment was needed, to increase sheet lengths roughly

                                10 - x¯¯ = 10 - 8.3 = 1.7 164 in.

              The true process mean operating to produce the data was clearly below the
              standard mean.

7.5.3         Retrospective x¯ Control Charts

              Retrospective (or "as past data") control limits for x¯ come about by replacing µ and
               in formulas (7.70) with estimates made from data in hand, under the provisional
              assumption that the process was stable over the period represented by the data.
              That is, in calculating such estimates, a single set of parameters is presumed to be
              adequate to describe process behavior during the study period. Notice that supposing
              process stability the present situation is exactly the one met in the ANOVA material
              of Section 7.4 under the hypothesis of equality of r means. So one way to think about
              a retrospective x¯ chart is as a graphical test of the constancy of the process mean
              over time. Further, the analogy with the material of Section 7.4 suggests natural
              estimates of µ and  for use in formulas (7.70).

                   In Section 7.4, y¯ was used to approximate a hypothesized common value of
              µ1, µ2, . . . , µr . In the present notation, this suggests replacing µ in formulas (7.70)
              with x¯¯ . Regarding an estimate of  for use in formulas (7.70), analogy with all that
              has gone before in this chapter suggests sP. And indeed, sP is a perfectly rational
              choice. But it is not one that is commonly used. Historical precedent/accident in the
              quality control field has made other estimates much more widely used. These must
              therefore be discussed, not so much because they are better than sP, but because they
              represent standard practice.

                   The most common way of approximating a supposedly constant  in control
              charting contexts is based on probability facts about the range, R, of a sample of
              m observations from a normal distribution. It is possible to derive the probability
              density for R defined in Definition 8 in Chapter 3 (see page 95), supposing m iid
              normal variables with mean µ and standard deviation  are involved. That density
              will not be given in this book. But it is useful to know that the mean of that distribution
              is (for a given sample size m) proportional to  . The constant of proportionality is
              typically called d2, and in symbols,

                                ER = d2                            (7.71)

              or equivalently,

                                 = ER                              (7.72)
                                      d2
                                                7.5 Shewhart Control Charts for Measurement Data 505

                     Values of d2 for various m are given in Table B.2. (Return to the comments preceding
                     Proposition 1 in Section 3.3 and recognize that what was cryptic there should now
                     make sense.)

                          Statements (7.71) and (7.72) are theoretical. The way they find practical rel-
                     evance is to think that under the hypothesis that the process standard deviation is
                     constant, the sample mean of sample ranges

Average sample                                     1r                                   (7.73)
              range                          R = r Ri

                                                          i =1

                     can be expected to approximate the theoretical mean range, ER. That is, from
                     statement (7.72), it seems that

A range-based                                ^ = R                                      (7.74)
estimator of                                       d2

                     is a plausible way to estimate  . On theoretical grounds, R/d2 is inferior to sP, but
                     it has the weight of historical precedent behind it, and it is simple to calculate (an

                     important virtue before the advent of widespread computing power).

                          A second estimator of  with quality control origins comes about by making the

                     same kind of argument that led to statistic (7.74), beginning not with R but instead
                     with s. That is, the fact that it is possible to derive a m2-1 probability density for
                     (m - 1)s2/ 2 if s2 is based on m iid normal (µ,  2) random variables has been used
                     extensively (beginning in Section 6.4) in this text. That density can in turn be used
                     to find a theoretical mean for s. As it turns out, although Es2 =  2, the theoretical
                     mean of s is not quite  , but rather a multiple of  (for a given sample size m). The

                     constant of proportionality is typically called c4, and in symbols,

                                             Es = c4                                    (7.75)

                     or equivalently,

                                              = Es                                      (7.76)
                                                   c4

                     It is possible to write out an explicit expression for c4, namely

                                                                m  

                                             2                  2  
                                       c4 =                        
                                             m-1       m-1 

                                                                2
506 Chapter 7 Inference for Unstructured Multisample Studies

                       Values of c4 for various m are given in Table B.2. From that table, it is easy to see
                       that as a function of m, c4 increases from about .8 when m = 2 to essentially 1 for
                       large m.

                            The practical use made of the theoretical statements (7.75) and (7.76) is to think
                       that the sample average of the sample standard deviations

    Average sample          1r                                           (7.77)
standard deviation     s¯ = r si

                                   i =1

                       can be expected to approximate the theoretical mean (sample) standard deviation
                       Es, so that (from statement (7.76)) a plausible estimator of  becomes

A standard deviation-        ^ = s¯                                      (7.78)
 based estimator of                c4

                       (It is worth remarking that s¯ is not the same as sP, even when all sample sizes are
                       the same. sP is derived by averaging sample variances and then taking a square root.
                       s¯ comes from taking the square roots of the sample variances and then averaging.
                       In general, these two orders of operation do not produce the same results.)

                            In any case, commonly used retrospective control limits for x¯ are obtained by
                       substituting x¯¯ given in formula (7.67) for µ and either of the estimates of  given
                       in displays (7.74) or (7.78) for  in the formulas (7.70). Further, an "as past data"
                       center line for an x¯ chart is typically set at x¯¯ .

Example 9              Consider retrospective x¯ control charting for the ream cutter data. Using the
(continued )           column totals given in Table 7.17, one finds from formulas (7.67), (7.73), and
                       (7.77) that

                       ¯ 183.4
                       x¯ =                                   22  = 8.3

                       ¯ 109
                       R = = 4.95

                              22

                       s¯ = 44.41 = 2.019
                              22

                       Then, consulting Table B.2 with a sample size of m = 5, d2 = 2.326, so an
                       estimate of  based on R is (from expression (7.74))

                       R = 4.95 = 2.13
                       d2 2.326
                                                     7.5 Shewhart Control Charts for Measurement Data 507

Also, Table B.2 shows that for a sample size of m = 5, c4 = .9400, so an estimate
of  based on s¯ is (from expression (7.78))

                                                              s¯ = 2.019 = 2.15
                                                              c4 .94

(Note that beginning from the standard deviations in Table 7.17, sP = 2.19, and
clearly sP = s¯.)

     Using (for example) statistic (7.74), one is thus led to substitute 8.3 for µ
and 2.13 for  in "standards given" formulas (7.70) to obtain the retrospective
limits

                                                     2.13                            2.13
LCLx¯ = 8.3 - 3  = 5.44 and UCLx¯ = 8.3 + 3  = 11.16
                                                     5                                                5

Figure 7.16 shows an "as past data" Shewhart x¯ control chart for the ream cutter
data, using limits based on R.

     Notice the contrast between the pictures of the ream cutter performance given
in Figures 7.15 and 7.16. Figure 7.15 shows clearly that process parameters are
not at their standard values, but Figure 7.16 shows that it is perhaps plausible
to think of the data in Table 7.16 as coming from some stable data-generating
mechanism. The observed x¯ 's hover nicely (indeed--as will be argued at the end
of the next section--perhaps too nicely) about a central value, showing no "out of
control" points or obvious trends. That hypothesis (7.66) is at least approximately
true is believable on the basis of Figure 7.16.

Sample mean length, x (641 in. above reference)                                                                           UCL
                                                 11

                                                 10

                                                 9

                                                 8                                   Center line

                                                 7

                                                 6
                                                                                                                         LCL

                                                           5  10                 15  20

                                                              Sample number, i

                                                 Figure 7.16 Retrospective Shewhart x¯ control chart
                                                 for cut sheet lengths
508 Chapter 7 Inference for Unstructured Multisample Studies

       Control limits         Several comments should be made before turning to a discussion of other Shew-
          for x¯ versus  hart control charts for measurements. First, note that what is represented on an x¯
                         chart is behavior (both expected and observed) of sample means, not individual
specifications for x     measurements. It is unfortunately all too common to see engineering specifications
                         (which refer to individual measurements) marked on x¯ control charts either in place
                         of, or in addition to, proper control limits. But how sample means compare to spec-
                         ifications for individual measurements tells nothing about either the stability of the
                         process as represented in the means or the acceptability of individual measurements
                         according to the stated engineering requirements. It is simply bad practice to mix
                         (or mix up) control limits and specifications.

                              A second comment has to do with the fairly arbitrary choice of 3-sigma control
                         limits in formulas (7.70). A legitimate question is, "Why not 2-sigma or 2.5-sigma
                         or 3.09-sigma limits?" There is no completely convincing theoretical answer to this
                         question. Indeed, arguments in favor of other multiples than 3 for use in formulas
                         (7.70) are heard from time to time. But the forces of historical precedent and many
                         years of successful application combine to make the use of 3-sigma limits nearly
                         universal.

                              As a final point regarding x¯ charts, the basic "standards given" formulas for
                         control limits (7.70) are sometimes combined with formula (7.74) or (7.78) for
                         estimating  , and x¯¯ is put in place of µ to obtain formulas for retrospective control
                         limits for x¯ . For example, using the estimate of  in display (7.74), one obtains the
                         formulas

                                             R                                            R    (7.79)
                         LCLx¯ = x¯¯ - 3                      and UCLx¯ = x¯¯ + 3 
                         d2 m                                                      d2 m

                         In fact, it is standard practice to use the abbreviation

                                                            3
                                                  A2 = 

                                                         d2 m

                         and rewrite the limits in formulas (7.79) as

Range-based              LCLx¯ = x¯¯ - A2 R and UCLx¯ = x¯¯ + A2 R                             (7.80)
retrospective
control limits

           for x¯

                         Values of A2 are given along with the other control chart constants in Table B.2. It
                         is worthwhile to verify that the use of formulas (7.80) in the context of Example 9
                         produces exactly the retrospective control limits for x¯ found earlier.

                              The version of retrospective x¯ chart limits related to the estimate of  in display

                         (7.78) is

                                              s¯                                           s¯  (7.81)
                         LCLx¯ = x¯¯ - 3                      and UCLx¯ = x¯¯ + 3 
                         c4 m                                                      c4 m
                        7.5 Shewhart Control Charts for Measurement Data 509

                        It is also standard practice to use the abbreviation

                                                                             3
                                                                  A3 = 

                                                                          c4 m
                        and rewrite the limits in display (7.81) as

Standard deviation-     LCLx¯ = x¯¯ - A3s¯ and UCLx¯ = x¯¯ + A3s¯               (7.82)
based retrospective
 control limits for x¯

                        Values of A3 are given in Table B.2.

7.5.4                   Control Charts for Ranges

                        The x¯ control chart is aimed primarily at monitoring the constancy of the average
                        process response, µ, over time. It deals only indirectly with the process short-
                        term variation  . (If  increases beyond a standard value, it will produce x¯ i more
                        variable than expected and eventually trigger an "out of control" point. But such a

                        possible change in  is detected most effectively by directly monitoring the spread
                        of samples.) Thus, in applications, x¯ charts are almost always accompanied by
                        companion charts intended to monitor  .

                             The conceptually simplest and most common Shewhart control charts for mon-

                        itoring the process standard deviation are the R charts, the charts for sample ranges.

                        In their "standards given" version, they are based again on the fact that it is possible
                        to find a probability density for R based on m iid normal (µ,  2) random variables.
                        Using this density, not only is it possible to show that ER = d2 but the standard
                        deviation of the probability distribution can be found as well. It turns out (for a given

                        m) to be proportional to  . The constant of proportionality is called d3 and is tabled
                        for various m in Table B.2. That is, for R based on m iid normal observations,

                                                                                (7.83)
                                                     Var R = d3

                             Although the information about the theoretical distribution of R provided by
                        formulas (7.71) and (7.83) is somewhat sketchy, it is enough to suggest possible
                        "standards given" 3-sigma (of R) control limits for R. A plausible center line for a
                        "standards given" R chart is at ER = d2 , and (using formula (7.83)) control limits
                        are

                                                                                (7.84)
                        LCLR = ER - 3 Var R = d2 - 3d3 = (d2 - 3d3)             (7.85)

                                            
                        UCLR = ER + 3 Var R = (d2 + 3d3)

                        The limit indicated in formula (7.84) turns out to be negative for m  6. For those
                        sample sizes, since ranges are nonnegative, no lower control limit is used. Formulas
510 Chapter 7 Inference for Unstructured Multisample Studies

                      (7.84) and (7.85) are typically simplified by the introduction of yet more notation.
                      That is, standard quality control usage is to let

                                              D1 = (d2 - 3d3) and D2 = (d2 + 3d3)
                      and rewrite formulas (7.84) and (7.85) as

"Standards given"                                       LCLR = D1 and UCLR = D2                                                      (7.86)
control limits for R

                      Like the other control chart constants, D1 and D2 appear in Table B.2. Note that for
                      m  6, there is no tabled value for D1, as no lower limit is in order.

Example 9             Consider a "standards given" control chart analysis for the sheet length ranges
(continued )          given in Table 7.17, using a standard  = 1.9 ( 164 in.). Since samples of size m = 5
                      are involved, Table B.2 shows that d2 = 2.326 and D2 = 4.918 are appropriate
     WWW              for establishing a "standards given" control chart for R. The center line should

                      be drawn at

                                                       d2 = 2.326(1.9) = 4.4
                      and the upper control limit should be set at

                                                       D2 = 4.918(1.9) = 9.3

                      (Since m  6, no lower control limit will be used.) Figure 7.17 shows a "standards
                      given" control chart for ranges of the sheet lengths. It is clear from the figure that

                      in.)                                        "Out of Control" point

                      (1

                         64

                      Sheet length sample range, R  10                                                  UCL

                                                    5
                                                                                                                        Center line

                                                        5     10             15                         20

                                                           Sample number, i

                                                        Figure 7.17 "Standards given" Shewhart R chart
                                                        for cut sheet lengths
                                               7.5 Shewhart Control Charts for Measurement Data 511

                            for the most part, a constant process standard deviation of  = 1.9 is plausible,
                            except for the clear indication to the contrary at sample 22. The 22nd observed
                            range, R = 12, is simply larger than expected based on a sample of size m = 5
                            from a normal distribution with  = 1.9. In practice, it would be appropriate
                            to undertake a physical search for the cause of the apparent increase in process

                            variability associated with the last sample taken.

                                 As was the case for x¯ charts, combination of formulas for the estimation of
                            (supposedly constant) process parameters with the "standards given" limits (7.86)
                            produces retrospective control limits for R charts. For example, basing an estimate
                            of  on R as in display (7.74), leads (not too surprisingly) to a retrospective center
                            line for R at d2(R/d2) = R and retrospective control limits

                                               LCLR = D1 R and UCLR = D2 R               (7.87)
                                               d2                                    d2

                            The abbreviations

                                               D3 = D1 and D4 = D2
                                               d2                                d2

                            are commonly used, and limits (7.87) are written as

Retrospective control                          LCLR = D3 R and UCLR = D4 R               (7.88)
              limits for R

                            Values of the constants D3 and D4 are found in Table B.2.

Example 9                   For the ream cutter data, R = 22 109 , so retrospective control limits for ranges of
(continued )                the type (7.88) put a center line at

                                                              R = 4.95

                            and since for m = 5, D4 = 2.114,

                                               UCLR = 2.114   109       = 10.5

                                                              22

                                 Look again at Figure 7.17 and note that the use of these retrospective limits
                            (instead of the  = 1.9 "standards given" limits of Figure 7.17) does not materi-
                            ally alter the appearance of the plot. The range for sample 22 still plots above the
                            upper control limit. It is not plausible that a single  stands behind all of the 22
512 Chapter 7 Inference for Unstructured Multisample Studies

Example 9             plotted ranges (not even   R/d2 = 2.13). It is pretty clear that a different phys-
(continued )          ical mechanism must have been acting at sample 22 than was operative earlier.

                           For pedagogical reasons, x¯ charts were considered first before turning to charts
                      aimed at monitoring  . In terms of order of attention in an application, however, R
                      (or s) charts are traditionally (and correctly) given first priority. They deal directly
                      with the baseline component of process variation. Thus (so conventional wisdom
                      goes), if they show lack of stability, there is little reason to go on to considering
                      the behavior of means (which deals primarily with the long-term component of
                      process variation) until appropriate physical changes bring the ranges (or standard
                      deviations) to the place of repeatability.

7.5.5                 Control Charts for Standard Deviations

                      Less common but nevertheless important alternatives to range charts are control

                      charts for standard deviations, s. In their "standards given" version, s charts are

                      based on the fact that it is possible to find both a mean and variance for s calculated
                      from m iid normal (µ,  2) random variables. We have already used the fact that
                      Es = c4 . And it turns out that

                      Var s = 1 - c42                                                  (7.89)

                           Then formulas (7.75) and (7.89) taken together yield "standards given" 3-sigma
                      control limits for s. That is, with a center line at c4 , one employs the limits

                      LCLs = c4 - 3 1 - c42  = c4 - 3 1 - c42 
                      UCLs = c4 + 3 1 - c42  = c4 + 3 1 - c42 

                      Standard notation is to let             and B6 = c4 + 3 1 - c42
                                     B5 = c4 - 3 1 - c42

                      so, ultimately, "standards given" control limits for s become

"Standards given"     LCLs = B5 and UCLs = B6                                          (7.90)
control limits for s

                      As expected, the constants B5 and B6 are tabled in Table B.2. For m  5, c4 -
                      3 1 - c42 turns out to be negative, so no value is shown in Table B.2 for B5, and no
                      lower control limit for s is typically used for such sample sizes.
Example 9                             7.5 Shewhart Control Charts for Measurement Data 513
(continued )
              Returning once more to the ream cutter example of Shervheim and Snider, con-
     WWW      sider the monitoring of  through the use of sample standard deviations rather
              than ranges, based on a standard of  = 1.9 ( 641 in.). Table B.2 with sample size
              m = 5 once again gives c4 = .9400 and also shows that B6 = 1.964. So an s
              chart for the data of Table 7.16 has a center line at

                                               c4 = (.94)(1.9) = 1.79

              and an upper control limit at

                                         UCLs = B6 = 1.964(1.9) = 3.73

              and, since the sample size is only 5, no lower control limit.
                   Figure 7.18 is a "standards given" Shewhart s chart for the s values given in

              Table 7.17. The story told by Figure 7.18 is essentially identical to that conveyed
              by the range chart in Figure 7.17. Only at sample 22 does the hypothesis that  =
              1.9 become untenable, and the need for physical intervention is indicated there.

              Sheet length sample standard deviation, s (641 in.)  5.0

                                                                   4.0

                                                                   3.0

                                                                   2.0
                                                                                                                                         Center line

                                                                   1.0

                                                                   5      10            15      20

                                                                      Sample number, i

                                                                   Figure 7.18 "Standards given" s chart for cut sheet lengths

                   As was the case for x¯ and R charts, retrospective control limits for s can
              be had by replacing the parameter  in the "standards given" limits (7.90) with

              any appropriate estimate. The most common way of proceeding is to employ the
              estimator s¯/c4 and thus end up with a retrospective center line for an s chart at
              c4(s¯/c4) = s¯ and retrospective control limits

                                                                   LCLs = B5s¯ and UCLs = B6s¯                                                        (7.91)
                                                                      c4                    c4
514 Chapter 7 Inference for Unstructured Multisample Studies

                   And using the abbreviations

                                           B3 = B5 and B4 = B6
                                                         c4                   c4

Retrospective      the retrospective limits (7.91) are written as                                     (7.92)
control limits                                  LCLs = B3s¯ and UCLs = B4s¯

            for s

                   Values of B3 and B4 are given in Table B.2.

Example 9          For  the  ream  cutter  data,  s¯  =  44.41  =  2.02,  so  retrospective  control  limits  for
(continued )                                              22
                   standard deviations of the type (7.92) put a center line at

                                                             s¯ = 2.02

                   and, since B4 = 2.089 for m = 5,

                                           UCLs = 2.089            44.41      = 4.22

                                                                   22

                   Look again at Figure 7.18 and verify that the use of these retrospective limits (in-
                   stead of the  = 1.9 "standards given" limits) wouldn't much change the appear-
                   ance of the plot. As was the case for the retrospective R chart analysis, these retro-
                   spective s chart limits still put sample 22 in a class by itself, suggesting that a dif-
                   ferent physical mechanism produced it than that which led to the other 21 samples.

                        Ranges are easier to calculate "by hand" than standard deviations and are
                   easier to explain as well. As a result, R charts are more popular than s charts. In
                   fact, R charts are so common that the phrase "x¯ and R charts" is often spoken in
                   quality control circles in such a way that the x¯ /R pair is almost implied to be a
                   single inseparable entity. However, when computational problems and conceptual
                   understanding are not issues, s charts are preferable to R charts because of their
                   superior sensitivity to changes in  .

                        A useful final observation about the s chart idea is that for r -sample statistical en-
                   gineering studies where all sample sizes are the same, the "as past data" control limits
                   in display (7.92) can provide some rough help in the model-checking activities of
                   Section 7.1 (in reference to the "single variance" assumption of the one-way model).
                   B3s¯ and B4s¯ can be treated as rough limits on the variation in sample standard devia-
                   tions deemed to be consistent with the one-way model's single variance assumption.
                        7.5 Shewhart Control Charts for Measurement Data 515

          Example 10    s Chart Control Limits and the "Equal Variances" Assumption
(Example 1 revisited )  in the Concrete Strength Study

                        In the concrete compressive strength study of Armstrong, Babb, and Campen,
                        the r = 8 sample standard deviations based on samples of size m = 3 given in
                        Table 7.3 (page 450) have s¯ = 534.8 psi. Then for m = 3, B4 = 2.568, and so

                                                    B4s¯ = 2.568(534.8) = 1,373 psi

                        The largest of the eight values si in Table 7.3 is 965.6, and there are thus no "out
                        of control" standard deviations. So as in Section 7.1, no strong evidence against
                        the relevance of the "single variance" model assumption is discovered here.

             7.5.6      Control Charts for Measurements
                        and Industrial Process Improvement
    Out-of-control
       signals must     The x¯ and R (or x¯ and s) control chart combination is an important engineering
                        tool for the improvement of manufacturing processes. U.S. companies have trained
   produce action       literally hundreds of thousands of workers in the making of Shewhart x¯ and R charts
                        over the past few years, hoping for help in meeting the challenge of international
     Control charts     competition. The record of success produced by this training effort is mixed. It
        can prevent     is thus worth pausing briefly to reflect on what aid the tools of this section can
                        and cannot rationally be expected to provide in the effort to improve industrial
 over-adjustment        processes.

     Control charts          In the first place, warnings of assignable variation provided by Shewhart control
     help maintain      charts are helpful in reducing the variation of an industrial process only to the extent
   current process      that they are acted on in a timely and competent fashion. If "out of control" signals
best performance        don't lead to appropriate physical investigation and action to eliminate assignable
                        causes, they contribute nothing toward improved process behavior. If workers collect
                        data to be archived away on x¯ and R chart forms and do not have the authority, skills,
                        or motivation to intervene intelligently when excess process variation is indicated,
                        they are engaged in a futile activity.

                             Control charts can signal the need for process intervention. But perhaps nearly
                        as important is the fact that they also tell a user when not to be alarmed at observed
                        variation and give in to the temptation to adjust a stable process. This is the other side
                        of the intervention coin. Inadvisably adjusting an industrial process that is subject
                        only to common or random causes degrades its behavior rather than improves it.
                        Rational use of Shewhart control charts can help prevent this possibility.

                             It is also important to say that even when properly made and acted on, Shewhart
                        control charts can do only so much towards the improvement of industrial processes.
                        They can be a tool for helping to reduce variation to the minimum possible for a
                        given system configuration (in terms of equipment, methods of operation, etc.). But
                        once that minimum has been reached, all that Shewhart charting does is to help
516 Chapter 7 Inference for Unstructured Multisample Studies

Control charts are   maintain that configuration's best performance--to maintain the "baseline variation
 not directly tools  only" situation corresponding to the status quo way of doing things.

    for innovation        In a modern world economy, however, companies cannot hope to be leaders
                     in their industries by being content simply to maintain stable, status quo methods
                     of operation. Instead, ways must be found for improving beyond today's methods
                     for tomorrow. This requires thought and, often, engineering experimentation. The
                     philosophies and methods of experimental design and engineering data collection
                     and analysis discussed in this book have an important role in that search for improve-
                     ment beyond today's best industrial methodology. But the particular role of control
                     charting in such efforts is only indirect. By using control charts and bringing a current
                     process to stability, a basis or foundation for improvement through experimentation
                     and reconfiguration is provided. Indeed, it can be argued fairly convincingly that
                     unless an existing process is repeatable, there is no sensible way of evaluating the
                     impact of experimental changes made to it, trying to find tomorrow's improved
                     version of the process. It is important to realize, however, that the Shewhart control
                     charts provide only the foundation rather than the necessary subject matter expertise
                     or statistical tools needed to guide the experimental search for improved ways of
                     doing things.

Section 5 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. The following are some data taken from a larger set  (a) Suppose that standard values for the process
   in Statistical Quality Control by Grant and Leav-         mean and standard deviation of drained
   enworth, giving the drained weights (in ounces)           weights (µ and  ) in this canning plant are
   of contents of size No. 2 21 cans of standard grade       21.0 oz and 1.0 oz, respectively. Make and in-
   tomatoes in puree. Twenty samples of three cans           terpret "standards given" x¯ and R charts based
   taken from a canning process at regular intervals         on these samples. What do these charts indi-
   are represented.                                          cate about the behavior of the filling process
                                                             over the time period represented by these data?
Sample x1    x2 x3 Sample x1 x2 x3
                                                        (b) As an alternative to the "standards given" range
    1 22.0  22.5 22.5 11 20.0 19.5 21.0                      chart made in part (a), make a "standards given"
    2 20.5  22.5 22.5 12 19.0 21.0 21.0                      s chart based on the 20 samples. How does its
    3 20.0  20.5 23.0 13 19.5 20.5 21.0                      appearance compare to that of the R chart?
    4 21.0  22.0 22.0 14 20.0 21.5 24.0
    5 22.5  19.5 22.5 15 22.5 19.5 21.0                 Now suppose that no standard values for µ and 
    6 23.0  23.5 21.0 16 21.5 20.5 22.0                 have been provided.
    7 19.0  20.0 22.0 17 19.0 21.5 23.0                 (c) Find one estimate of  for the filling process
    8 21.5  20.5 19.0 18 21.0 20.5 19.5
    9 21.0  22.5 20.0 19 20.0 23.5 24.0                      based on the average of the 20 sample ranges,
   10 21.5  23.0 22.0 20 22.0 20.5 21.0                      R, and another based on the average of 20 sam-
                                                             ple standard deviations, s¯. How do these com-
                                                             pare to the pooled sample standard deviation
                                                             (of Section 7.1), sP, here?
                                                        (d) Use x¯¯ and your estimate of  based on R and
                                                             make retrospective control charts for x¯ and R.
                                   7.5 Shewhart Control Charts for Measurement Data 517

        What do these indicate about the stability of the     (b) Use your estimate from (a) based on sample
        filling process over the time period represented           standard deviations and compute control lim-
        by these data?                                             its for the sample ranges R, and then compute
    (e) Use x¯¯ and your estimate of  based on s¯ and              control limits for the sample standard devia-
        make retrospective control charts for x¯ and s.            tions s. Applying these to the R and s values,
        How do these compare in appearance to the                  what is suggested about the threading process?
        retrospective charts for process mean and vari-
        ability made in part (d)?                              (c) Using a center line at x¯¯ , and your estimate
                                                                   of  based on the sample standard deviations,
2. A manufacturer of U-bolts collects data on the                  compute control limits for the sample means
   thread lengths of the bolts that it produces. Nine-              x¯ . Applying these to the x¯ values here, what is
   teen samples of five consecutive bolts gave the                 suggested about the threading process?
   thread lengths indicated the accompanying table (in
   .001 in. above nominal).                                   (d) A check of the control chart form from which
                                                                   these data were taken shows that the coil of the
Sample Thread Lengths x¯ R s                                       heavy wire from which these bolts are made
                                                                   was changed just before samples 1, 9, and 16
 1 11, 14, 14, 10, 8 11.4 6 2.61                                   were taken. What insight, if any, does this in-
 2 14, 10, 11, 10, 11 11.2 4 1.64                                  formation provide into the possible origins of
 3 8, 13, 14, 13, 10 11.6 6 2.51                                   any patterns you see in the data?
 4 11, 8, 13, 11, 13 11.2 5 2.05
 5 13, 10, 11, 11, 11 11.2 3 1.10                              (e) Suppose that a customer will purchase bolts
 6 11, 10, 10, 11, 13 11.0 3 1.22                                  of the type represented in the data only if es-
 7 8, 6, 11, 11, 11 9.4 5 2.30                                     sentially all bolts received can be guaranteed
 8 10, 11, 10, 14, 10 11.0 4 1.73                                  to have thread lengths within .01 in. of nom-
 9 11, 8, 11, 8, 10 9.6 3 1.52                                     inal. Does it appear that with proper process
10 6, 6, 11, 13, 11 9.4 7 3.21                                     monitoring and adjustment, the equipment and
11 11, 14, 13, 8, 11 11.4 6 2.30                                   manufacturing practices in use at this company
12 8, 11, 10, 11, 14 10.8 6 2.17                                   will be able to produce only bolts meeting these
13 11, 11, 13, 8, 13 11.2 5 2.05                                   standards? Explain in quantitative terms. If the
14 11, 8, 11, 11, 11 10.4 3 1.34                                   equipment was not adequate to meet such re-
15 11, 11, 13, 11, 11 11.4 2 .89                                   quirements, name two options that might be
16 14, 13, 13, 13, 14 13.4 1 .55                                   taken and their practical pros and cons.
17 14, 13, 14, 13, 11 13.0 3 1.22
18 13, 11, 11, 11, 13 11.8 2 1.10                          3. State briefly the practical goals of control charting
19 14, 11, 11, 11, 13 12.0 3 1.41                             and action on "out of control" signals produced by
                                                              the charts.
x¯ = 212.4  R = 77  s = 32.92
                                                           4. Why might it well be argued that the name control
(a) Compute two different estimates of the process            chart invites confusion?
    short-term standard deviation of thread length,
    one based on the sample ranges and one based           5. What must an engineering application of control
    on the sample standard deviations.                        charting involve beyond the simple naming of
                                                              points plotting out of control if it is to be prac-
                                                              tically effective?

                                                           6. Explain briefly how a Shewhart x¯ chart can help
                                                              reduce variation in, say, a widget diameter, first
                                                              by signaling the need for process intervention/
                                                              adjustment and then also by preventing adjustments
                                                              when no "out of control" signal is given.
518 Chapter 7 Inference for Unstructured Multisample Studies

                                                qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

         7.6 Shewhart Control Charts
                for Qualitative and Count Data

                                  The previous section discussed Shewhart x¯ , R, and s control charts, treating them
                                  as tools for studying the stability of a system over time. This section focuses on how
                                  the Shewhart control charting idea can be applied to attributes data (i.e., counts).

                                       The discussion begins with p charts. Next u charts and their specialization to
                                  the case of a constant-size inspection unit, the c charts, are introduced. Finally,
                                  consideration is given to a number of common nonrandom patterns that can appear
                                  on both variables control charts and attributes control charts. Possible physical
                                  causes for them and some formal rules that are often recommended for automating
                                  their recognition are discussed.

7.6.1  p Charts

       This text has consistently indicated that measurements are generally preferable to
       attributes data. But in some situations, the only available information on the stability
       of a process takes the form of qualitative or count data. Consideration of the topic
       of control charting in such situations will begin here with p charts for cases where
       what is available for plotting are sample fractions, p^ i . The most common use of this
       is where p^ i is the fraction of a sample of ni items that is nonconforming according
       to some engineering standard or specification. So this section will use the "fraction
       nonconforming" language, in spite of the fact that p^ i can be the sample fraction
       having any attribute of interest (desirable, undesirable, or indifferent).

            The probability facts supporting control charting for the fraction nonconform-
       ing are exactly those used in Section 6.5 to develop inference methods based on p^ .
       That is, if a process is stable over time, each ni p^ i is usefully modeled as binomial
       (ni , p), where p is a constant likelihood that any sampled item is nonconform-
       ing. (This section will explicitly allow for sample sizes ni varying in time. Charts
       for measurements are almost always based on fairly small but constant sample
       sizes. But charts for attributes data typically involve larger sample sizes that some-
       times vary.)

            As in Section 6.5, a binomial model for ni p^ i leads immediately to

            E p^ i = p            (7.93)

       and

            Var p^ i =  p(1 - p)  (7.94)
                            ni

       But then formulas (7.93) and (7.94) suggest obvious "standards given" 3-sigma
       control limits for the sample "fraction nonconforming" p^ i . That is, if p is a standard
                                         7.6 Shewhart Control Charts for Qualitative and Count Data 519

                        likelihood that any single item is nonconforming, then a "standards given" p chart
                        has a center line at p and control limits

                        LCLp^ i = p - 3  p(1 - p)             (7.95)
                                             ni               (7.96)
"Standards given" p
  chart control limits

                        UCLp^ i = p + 3  p(1 - p)
                                             ni

                        In the event that formula (7.95) produces a negative value, no lower control limit
                        is used.

Example 11              p Chart Monitoring of a Pelletizing Process
      WWW
                        Kaminski, Rasavahn, Smith, and Weitekamper worked on the same pelletizing
                        process already used as an example several times in this book. (See Examples 2
                        (Chapter 1), 14 (Chapter 3), 4 (Chapter 5), and 18 (Chapter 6).) Extensive data
                        collection on two different days led the students to establish p = .61 as a standard
                        rate of nonconforming tablets produced by the process, when run under a shop
                        standard operating regimen. On a third day, the students took r = 25 samples
                        of n1 = n2 = · · · = n25 = m = 30 consecutive pellets at intervals as they came
                        off the machine and plotted sample fractions nonconforming p^ i , on a "standards
                        given" p chart made with p = .61. Their data are given in Table 7.18.

                             For samples of size ni = m = 30, 3-sigma "standards given" p chart control
                        limits are, from formulas (7.95) and (7.96),

                        LCLp^ = .61 - 3 (.61)(1 - .61) = .34
                        i                30

                        UCLp^ = .61 + 3 (.61)(1 - .61) = .88
                        i                30

                        and a center line at .61 is appropriate. Figure 7.19 is a "standards given" p chart
                        for the data of Table 7.18.

                             Four p^ i values plot below the lower control limit in Figure 7.19, and the p^ i
                        values run consistently below the chart's center line. These facts make untenable
                        the hypothesis that the pelletizing process was stable at the standard value of
                        61% nonconforming on the day these data were gathered. In this example, points
                        plotting "out of control" on the low side are an indication of process improve-
                        ment. They nevertheless represent a circumstance warranting physical attention
                        to determine the physical cause for the reduced fraction defective and possibly
                        to learn how to make the improvement permanent.
520 Chapter 7 Inference for Unstructured Multisample Studies

Example 11        Table 7.18
 (continued )     Numbers and Fractions of Nonconforming
                  Pellets in 25 Samples of Size 30

                  i,                                          ni p^ i ,

                  Sample Number Nonconforming p^ i

                  1                                           13         .43

                  2                                           12         .40

                  3                                           9          .30

                  4                                           15         .50

                  5                                           17         .57

                  6                                           13         .43

                  7                                           20         .67

                  8                                           18         .60

                  9                                           18         .60

                  10                                          16         .53

                  11                                          15         .50

                  12                                          17         .57

                  13                                          15         .50

                  14                                          20         .67

                  15                                          10         .33

                  16                                          12         .40

                  17                                          17         .57

                  18                                          14         .47

                  19                                          16         .53

                  20                                          10         .33

                  21                                          14         .47

                  22                                          13         .43

                  23                                          17         .57

                  24                                          10         .33

                  25                                          12         .40

                      ni p^ i = 363

                       To make retrospective limits for a p chart, one must settle on a method of
                  estimating the (supposedly constant) process parameter p. Here the pooling idea
                  introduced in the two-sample context of Section 6.5 can be used. That is, as a direct
                  extension of formula (6.71) of Section 6.5, let

Pooled estimator      p^ = n1 p^ 1 + n2 p^ 2 + · · · + nr p^ r                (7.97)
  of a common p                 n1 + n2 + · · · + nr
                                                       7.6 Shewhart Control Charts for Qualitative and Count Data 521

                                                                                                  UCL

                     Sample fraction nonconforming, p  .8

                                                       .7

                                                                                                                             Center line
                                                       .6

                                                       .5

                                                       .4
                                                                                                                                     LCL

                                                       .3

                                                           5  10                15            20  25

                                                              Sample number, i

                     Figure 7.19 "Standards given" p chart for nonconforming pellets

                     ( p^ is the total number nonconforming divided by the total number inspected. When
                     sample sizes vary, it is a weighted average of the p^ i .)

                          With p^ as in formula (7.97), an "as past data" Shewhart p chart has a center
                     line at p^ and

 Retrospective                                                LCLp^ i = p^ - 3  p^ (1 - p^ )                                              (7.98)
p chart control                                                                     ni                                                    (7.99)

             limits

                                                              UCLp^ i = p^ + 3  p^ (1 - p^ )
                                                                                    ni

                     As in the "standards given" context, when formula (7.98) produces a negative value,
                     no lower control limit is used for p^ i .

Example 11           In the pelletizing case, the total number nonconforming in the samples was
 (continued )           ni p^ i = 363. Then, since mr = 30(25) = 750 pellets were actually inspected
522 Chapter 7 Inference for Unstructured Multisample Studies

Example 11     on the day in question,
 (continued )

                                                     p^ = 363 = .484
                                                           750

               So a retrospective 3-sigma p chart for the data of Table 7.18 has a center line at
               p^ = .484 and, from formulas (7.98) and (7.99),

                                                     LCLp^ = .484 - 3 (.484)(1 - .484) = .21
                                                        i          30

                                                     UCLp^ = .484 + 3 (.484)(1 - .484) = .76
                                                        i          30

                    Figure 7.20 is a retrospective p chart for the situation of Kaminski et al. All

               points plot within control limits on Figure 7.20. So although it is not tenable
               that the pelletizing process was stable at p = .61 over the study period, it is
               completely plausible that it was stable at some value of p (and p^ = .484 is a
               sensible guess for that value).

               Sample fraction nonconforming, p                                                                                 UCL
                                                 .7

                                                 .6

                                                 .5                              Center line

                                                 .4

                                                 .3
                                                                                                                                 LCL

                                                 .2

                                                     5     10  15            20               25

                                                           Sample number, i

                                                 Figure 7.20 Retrospective p chart for nonconforming pellets

                    Because of the inherent limitations of categorical data in engineering contexts,
               little more will be said in this book about formal inference based on sample fractions
                                     7.6 Shewhart Control Charts for Qualitative and Count Data 523

                    beyond what is in Section 6.5. For example, formal significance tests of equality of r
                    proportions, parallel to the tests of equality of r means presented in Section 7.4, won't
                    be discussed. However, the retrospective p chart can be interpreted as a rough graph-
                    ical tool for judging how sensible the hypothesis H0: p1 = p2 = · · · = pr appears.

7.6.2               u Charts

                    Section 3.4 introduced the notation u^ for the ratio of the number of occurrences of
                    a phenomenon of interest to the total number of inspection units or items sampled
                    in contexts where there may be multiple occurrences on a given item or inspection
                    unit. The most common application of u charts based on such ratios is that of
                    nonconformance to some engineering standard or specification. This section will
                    use the terminology of "nonconformances per unit" in spite of the fact that u^ can be
                    the sample occurrence rate for any type of phenomenon (desirable, undesirable, or
                    indifferent).

                         The theoretical basis for control charting based on nonconformances per unit
                    is found in the Poisson distributions of Section 5.1. That is, suppose that for some
                    specified inspection unit or unit of process output of a given size, a physically stable
                    process has an associated mean nonconformances per unit of  and

                    Xi = the number of nonconformances observed on ki units inspected at time i

                    Then a reasonable model for Xi is often the Poisson distribution with mean ki . The
                    material in Section 5.1 then says that both E Xi = ki  and Var Xi = ki .

                         But notice that if u^ i is the sample nonconformances per unit observed at period i,

Rate plotted on         u^ i = Xi
         a u chart             ki

                    so Proposition 1 in Chapter 5 (page 307) can be applied to produce a mean and
                    standard deviation for u^ i . That is,

                        Eu^ i = E Xi = 1 EXi = 1 (ki ) = 
                        ki ki      ki
                                                                     (7.100)

                        Var u^ i = Var Xi = 21 Var Xi = 21 (ki ) = 
                        ki ki             ki               ki

                    so

                                                                     (7.101)
                        Var u^ i =

                                      ki
524 Chapter 7 Inference for Unstructured Multisample Studies

                                  The relationships (7.100) and (7.101) then motivate "standards given" 3-sigma
                                  control limits for u^ i . That is, if  is a standard mean nonconformances per unit, then
                                  a "standards given" u chart has a center line at  and

                                                               (7.102)
                          LCLu^ =  - 3                         (7.103)
                          i  ki

"Standards given"                               
     u chart control      UCLu^ =  + 3
                  limits

                          i  ki

                          The difference in formula (7.102) can turn out negative. When it does, no lower
                          control limit is used.

                               Another matter of notation must be discussed at this point.  is the symbol
                          commonly used (as in Section 5.1) for a Poisson mean, and this fact is the basis for
                          the usage here. However, it is more common in statistical quality control circles to
                          use c or even c for a standard mean nonconformances per unit. In fact, the case of
                          the u chart where all ki are 1 is usually referred to as a c chart. The  notation used
                          here represents the path of least confusion through this notational conflict and thus
                          c or c will not be used in this text. However, be aware that at least in the quality
                          control world, there is a more popular alternative to the present  convention.

                               When the limits (7.102) and (7.103) are used with nonconformances per unit
                          data, one is essentially checking whether the prespecified  is a plausible description
                          of a physical process at each time period covered by the data. Often, however, there
                          is no obvious standard occurrence rate , and u charting is to be done retrospectively.
                          The question is then whether or not it is plausible that some (single)  describes
                          the process over all time periods covered by the data. What is needed in order to
                          produce retrospective control limits for such cases is a way to use the u^ i to make a
                          single estimate of a supposedly constant . This text's approach to this problem is to
                          make an estimate exactly analogous to the pooled estimate of p in formula (7.97).
                          That is, let

Pooled estimator          ^ k1u^ 1 + k2u^ 2 + · · · + kr u^ r  (7.104)
  of a common             =

                                    k1 + k2 + · · · + kr

                          ^ is the total number of nonconformances observed divided by the total number of

                          units inspected. Then combining formula (7.104) with limits (7.102) and (7.103), a
                          retrospective 3-sigma u chart has a center line at ^ and

    Retrospective u       LCLu^ = ^ - 3 ^                      (7.105)
chart control limits      i  ki
                            7.6 Shewhart Control Charts for Qualitative and Count Data 525

                                  UCLu^ = ^ + 3 ^                           (7.106)
                                  i                   ki

                            As the reader might by now expect, when formula (7.105) gives a negative value,
                            no lower control limit is employed.

               Example 12   u Chart Monitoring of the Defects per Truck Found at Final Assembly
(Example 13, Chapter 3,
revisited--see page 110)    In his book Statistical Quality Control Methods, I. W. Burr discusses the use of u
                            charts to monitor the performance of an assembly process at a station in a truck
                       WWW  assembly plant. Part of Burr's data were given earlier in Table 3.19. Table 7.19
                            gives a (partially overlapping) r = 30 production days' worth of Burr's data. (The
                            values were extrapolated from Burr's figures and the fact that truck production
                            through sample 13 was 95 trucks/day and was 130 trucks/day thereafter. Burr
                            gives only u^ i values, production rates, and the fact that all trucks produced
                            were inspected.)

                                 Consider the problem of control charting for these data. Since Burr gave no
                            figure  for the plant's standard errors per truck, this problem will be approached
                            as one of making a retrospective u chart. Using formula (7.104), and the column
                            totals from Table 7.19,

                               ^= Xi 6,078
                                               =      = 1.764
                                  ki 3,445

                            So an "as past data" u chart will have a center line at 1.764 errors/truck. From
                            formulas (7.105) and (7.106), for the first 13 days (where each ki was 95),

                            LCLu^ = 1.764 - 3  1.764  = 1.355 errors/truck

                            i                     95

                            UCLu^ = 1.764 + 3  1.764  = 2.173 errors/truck

                            i                     95

                            On the other hand, for the last 17 days (during which 130 trucks were produced
                            each day),

                            LCLu^ = 1.764 - 3  1.764  = 1.415 errors/truck

                            i                  130

                            UCLu^ = 1.764 + 3  1.764  = 2.113 errors/truck

                            i                  130
526 Chapter 7 Inference for Unstructured Multisample Studies

Example 12     Table 7.19
 (continued )  Numbers and Rates of Nonconformances for a Truck Assembly Process

               i,        ki ,                                 Xi = ki u^ i ,  u^ i ,

               Sample Date Trucks Produced Errors Found Errors/Truck

               1   11/4  95                                   114             1.20

               2   11/5  95                                   142             1.50

               3   11/6  95                                   146             1.54

               4   11/7  95                                   257             2.70

               5   11/8  95                                   185             1.95

               6 11/11   95                                   228             2.40

               7 11/12   95                                   327             3.44

               8 11/13   95                                   269             2.83

               9 11/14   95                                   167             1.76

               10 11/15  95                                   190             2.00

               11 11/18  95                                   199             2.09

               12 11/19  95                                   180             1.89

               13 11/20  95                                   171             1.80

               14 11/21  130                                  163             1.25

               15 11/22  130                                  205             1.58

               16 11/25  130                                  292             2.25

               17 11/26  130                                  325             2.50

               18 11/27  130                                  267             2.05

               19 11/29  130                                  190             1.46

               20  12/2  130                                  200             1.54

               21  12/3  130                                  185             1.42

               22  12/4  130                                  204             1.57

               23  12/5  130                                  182             1.40

               24  12/6  130                                  196             1.51

               25  12/9  130                                  140             1.08

               26 12/10  130                                  165             1.27

               27 12/11  130                                  153             1.18

               28 12/12  130                                  181             1.39

               29 12/13  130                                  185             1.42

               30 12/16  130                                  270             2.08

                         ki = 3,445                           Xi = 6,078

               Notice that since ki appears in the denominator of the plus-or-minus part of control
               limit formulas (7.102), (7.103), (7.105), and (7.106), the larger the inspection
               effort at a given time period, the tighter the corresponding control limits. This
               is perfectly logical. A bigger "sample size" at a given period ought to make the
                    7.6 Shewhart Control Charts for Qualitative and Count Data 527

       corresponding u^ i a more reliable indicator of , so less variation of u^ i 's about a
       standard or estimated common value is tolerated.

            Figure 7.21 is a retrospective u chart for the data of Table 7.19. The figure
       shows that the data-generating process can in no way be thought of as stable
       or subject to only random causes. There is too much variation in the u^ i to be
       explainable as due only to small unidentifiable causes. Some of the variation
       can probably be thought of in terms of a general downward trend, perhaps
       associated with workers gaining job skills. But even accounting for that, there
       is substantial erratic fluctuation of the u^ i --which couldn't fit between control
       limits no matter where they might be centered. These data simply represent a
       real engineering process that, according to accepted standards, is not repeatable
       enough to allow (without appropriate sleuthing and elimination of large causes
       of variation) anything but "one day at a time" inferences about its behavior.

                            3.0

       Errors per truck, u                                                                                       UCL
                            2.0

                                                        Center line

                                                                                                                 LCL
                            1.0

                                 5  10  15      20  25  30

                                        Day, i

                                 Figure 7.21 Retrospective u chart for truck assembly errors

            This book has had little to say about formal inference from data with an under-

       lying Poisson distribution. But retrospective u charts like the one in Example 12 can
       be thought of as rough graphical tests of the hypothesis H0: 1 = 2 = · · · = r for
       Poisson-distributed Xi = ki u^ i .

7.6.3  Common Control Chart Patterns and Special Checks

       Shewhart control charts (both those for measurements and those for attributes data)
       are useful for reasons beyond the fact that they supply semiformal information of a
       hypothesis-testing type. Much important qualitative information is also carried by
       patterns that can sometimes be seen in the charts' simple plots. Section 3.3 included
       some comments about engineering information carried in plots of summary statistics
       against time. Shewhart charts are such plots augmented with control limits. It is thus
528 Chapter 7 Inference for Unstructured Multisample Studies

     What is expected    appropriate to amplify and extend those comments somewhat, in light of the extra
if a process is stable?  element provided by the control limits.

                              Before discussing interesting possible departures from the norm, it should prob-
                         ably be explicitly stated how a 3-sigma control chart is expected to look if a process
                         is physically stable. One expects (tacitly assuming the distribution of the plotted
                         statistic to be mound-shaped) that

                         1.  most plotted points will lie in the middle, (say, the middle   2  )  of  the  region
                                                                                            3
                             delineated by the control limits around the center line,

                         2. a few (say, on the order of 1 in 20) points will lie outside this region but
                             inside the control limits,

                         3. essentially no points will lie outside the control limits, and

                         4. there will be no obvious trends in time for any sizable part of the chart.

    Cyclical patterns    That is, one expects to see a random-scatter/white-noise plot that fills, but essentially
 on a control chart      remains within, the region bounded by the control limits. When something else is
                         seen, even if no points plot outside the control limits, there is reason to consider
Too much variation       the possibility that something in addition to chance causes is active in the data-
 on a control chart      generating mechanism.

 Too little variation         Cyclical (repeated "up, then back down again") patterns sometimes show up on
 on a control chart      Shewhart control charts. Such behavior is not characteristic of plots resulting from
                         a stable-process data-generating mechanism. When it occurs, the alert engineer will
                         look for identifiable physical causes of variation whose effects would come and go on
                         about the same schedule as the ups and downs seen on the chart. Sometimes cyclical
                         patterns are associated with daily or seasonal variables like ambient temperature
                         effects, which may be largely beyond a user's control. But at other times, they have
                         to do with things like different (rotating) operators' slightly different methods of
                         machine operation, which can be mostly eliminated via standardization, training,
                         and awareness.

                              Again, the expectation is that points plotted on a Shewhart control chart should
                         (over time) pretty much fill up but rarely plot outside the region delineated by
                         control limits. This can be violated in two different ways, both of which suggest the
                         need for engineering attention. In the first place, more variation than expected (like
                         that evident on Figure 7.21), which produces multiple points outside the control
                         limits, is often termed instability. And (after eliminating the possibility of a blunder
                         in calculations) it is nearly airtight evidence of one or more unregulated process
                         variables having effects so large that they must be regulated. Such erratic behavior
                         can sometimes be traced to material or components from several different suppliers
                         having somewhat different physical properties and entering a production line in a
                         mixed or haphazard order. Also, ill-advised operators may overadjust equipment
                         (without any basis in control charting). This can take a fairly stable process and
                         make it unstable.

                              Less variation than expected on a Shewhart chart presents an interesting puzzle.
                         Look again at Figure 7.16 on page 507 and reflect on the fact that the plotted x¯ 's
                             7.6 Shewhart Control Charts for Qualitative and Count Data 529

                             on that chart hug the center line. They don't come close to filling up the region
                             between the control limits. The reader's first reaction to this might well be, "So
                             what? Isn't small variation good?" Small variation is indeed a virtue, but when
                             points on a control chart hug the center line, what one has is unbelievably small
                             variation, which may conceal a blunder in calculation or (almost paradoxically)
                             unnecessarily large but nonrandom variation.

                                  In the first place, the simplest possible explanation of a plot like Figure 7.16
                             is that the process short-term variation,  , has been overestimated--either because
                             a standard  is not applicable or because of some blunder in calculation or logic.
                             Notice that using a value for  that is bigger than what is really called for when
                             making the limits

                                                m                             m
                             LCLx¯ = µ - 3         and UCLx¯ = µ + 3 

Systematic differences       will spread the control limits too wide and produce an x¯ chart that is insensitive to
and too little variation     changes in µ. So this possibility should not be taken lightly.

     on a control chart/          A more subtle possible source of unbelievably small variation on a Shewhart
             stratification  chart has to do with the (usually unwitting) mixing of several consistently different
                             streams of observations in the calculation of a single statistic that is naively thought
                             to be representing only one stream of observations. This can happen when data are
                             being taken from a production stream where multiple heads or cavities on a machine
                             (or various channels of another type of multiple-channel process) are represented in
                             a regular order in the stream. For example, items machined on heads 1, 2, and 3 of
                             a machine might show up downstream in a production process in the order 1, 2, 3,
                             1, 2, 3, 1, 2, 3, etc. Then, if there is more difference between the different types of
                             observations than there is within a given type, values of a single statistic calculated
                             using observations of several types can be remarkably (excessively) consistent.

                                  Consider, for example, the possibility that a five-head machine has heads that
                             are detectably/consistently different. Suppose four of the five are perfectly adjusted
                             and always produce conforming items and the fifth is severely misadjusted and
                             always produces nonconforming items. Although 20% of the items produced are
                             nonconforming, a binomial distribution model with p = .2 will typically overpredict
                             the variation that will be seen in ni p^ i for samples of items from this process. Indeed,
                             samples of size m = 5 of consecutive items coming off this machine will have
                             p^ i = .2, always. Clearly, no p^ i 's would approach p chart control limits.

                                  Or in a measurement data context, with the same hypothetical five-head ma-
                             chine, consider the possibility that four of the five heads always produce a part
                             dimension at the target of 8 in. (plus or minus, say, .01 in.), whereas the fifth head is
                             grossly misadjusted, always producing the dimension at 9 in. (plus or minus .01 in.).
                             Then, in this exaggerated example, naive mixing together of the output of all five
                             heads will produce ranges unbelievably stable at about 1 in. and sample means (of
                             five consecutive pieces) unbelievably stable at about 8.2 in. But the super-stability
                             is not a cause for rejoicing. Rather it is a cause for thought and investigation that
                             could well lead to the physical elimination of the differences between the various
                             mechanisms producing the data--in this case, the fixing of the faulty head.
530 Chapter 7 Inference for Unstructured Multisample Studies

Changes in          The possibility of unnatural consistency on a Shewhart chart, brought on by
        level  more or less systematic sampling of detectably different data streams, is often
               called stratification in quality control circles. Although there is presently no way
  Bunching     of verifying this suspicion, some form of stratification may have been at work in the
               production of the ream cutter data of Shervheim and Snider and the x¯ chart in Figure
        Runs   7.16. For example, multiple blades set at not quite equal angles on a roller that cuts
               sheets (as sketched in Figure 7.22) could produce consistently different consecutive
               sheet lengths and unbelievably stable x¯ 's. Or even with only a single blade on the
               cutter roller, regular patterns in material tension, brought on by slight eccentricities
               of feeder rollers, could also produce consistent patterns in consecutive sheet lengths
               and thus too much stability on the x¯ chart.

                    Other nonrandom patterns sometimes appearing on control charts include both
               gradual and more sudden changes in level and unabated trends up or down. Gradual
               changes in level can sometimes be traced to machine warm-up phenomena, slow
               changeovers in a raw material source, or introduction of operator training. And
               phenomena like tool wear and machine degradation over time will typically produce
               patterns of plotted points moving in a single direction until there is some sort of
               human intervention.

                    The terms grouping and bunching are used to describe irregular patterns on
               control charts where plotted points tend to come in sets of similar values but where
               the pattern is neither regular/repeatable enough to be termed cyclical nor consistent
               enough in one direction to merit the use of the term trend. Such grouping can be
               brought about (for example) by calibration changes in a measuring instrument and,
               in machining processes, by fixture changes.

                    Finally, runs of many consecutive points on one side of a center line are
               sometimes seen on control charts. Figure 7.15, the "standards given" x¯ chart for the
               sheet-length data on page 502, is an extreme example of a chart exhibiting a run.
               On "standards given" charts, runs (even when not accompanied by points plotting
               outside control limits) tend to discredit the chart's center line value as a plausible
               median for the distribution of the plotted statistic. On x¯ charts, that translates to a
               discrediting of the target process mean as the value of the true process mean, thus
               indicating that the process is misaimed. (In the sheet-length situation of Figure 7.15,
               average sheet length is clearly below the target length.) And on a p or u chart, it

                                                              Cutter blades

               Material                                       Cut sheet

                         Feeder rollers
                         Figure 7.22 Schematic of a roller cutter
                 7.6 Shewhart Control Charts for Qualitative and Count Data 531

indicates the inappropriateness of the supposedly standard rate of nonconforming
items or nonconformances. On retrospective control charts, runs on one side of the
center line are usually matched by runs on the other side, and one of the earlier terms
(cycles, trends, or grouping) can typically be applied in addition to the term runs.

     In recognition of the fact that the elementary "wait for a point to plot outside
of control limits" mode of using control charts is blind to the various interpretable
patterns discussed here, a variety of special checks have been developed. To give the
reader the flavor of these checks for unnatural patterns, two of the most famous sets
are shown in Tables 7.20 and 7.21. Besides many other different sets appearing in
quality control books, companies making serious use of control charts often develop
their own collections of such rules. The two sets given here are included more to
show what is possible than to advocate them in particular. The real bottom line of this
discussion is simply that when used judiciously (overinterpretation of control chart
patterns is a real temptation that also must be avoided), the qualitative information
carried by patterns on Shewhart control charts can be an important engineering tool.

      Table 7.20
      Western Electric Alarm Rules (from the AT&T Quality Control Handbook)

          s A single point outside 3-sigma limits
          s 2 out of any 3 successive points outside 2-sigma limits on one side of

              the center line
          s 4 out of any 5 successive points outside 1-sigma limits on one side of

              the center line
          s 8 consecutive points on one side of the center line

      Table 7.21
      Alarm Rules of L. S. Nelson (from the Journal of Quality Technology)

          s a single point outside 3-sigma limits
          s 9 points in a row on one side of the center line
          s 6 points in a row increasing or decreasing
          s 14 points in a row alternating up and down
          s 2 out of any 3 successive points outside 2-sigma limits on one side of

              the center line
          s 4 out of any 5 successive points outside 1-sigma limits on one side of

              the center line
          s 15 points in a row inside 1-sigma limits
          s 8 points in a row with none inside 1-sigma limits
532 Chapter 7 Inference for Unstructured Multisample Studies

Section 6 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. The accompanying data are some taken from Statis-          Date Number Tested Leaks
   tical Quality Control Methods by I. W. Burr, giving
   the numbers of beverage cans found to be defective         6/11  50  4
   in periodic samples of 312 cans at a bottling facility.
                                                              6/12  50  11

                                                              6/13  50  8

Sample Defectives      Sample Defectives                      6/14  50  10

1         6            11      7                              6/17  32  3

2         7            12      7                              6/18  50  11

3         5            13      6                              6/19  33  1

4         7            14      6                              6/20  50  3

5         5            15      6                              6/24  50  6

6         5            16      6                              6/25  50  8

7         4            17      23                             6/26  50  5

8         5            18      10                             6/27  50  2

9         12           19      8                               (There were 841 radiators tested and a total of 116
                                                               leaks detected.) Make a retrospective u chart based
10        6            20      5                               on these data. What does it indicate about the sta-
                                                               bility of the assembly process?
    (a) Suppose that the company standard for the           3. In a particular defects/unit context, the number of
        fraction of cans defective is that p = .02 of the      standard size units inspected at a given opportunity
        cans be defective on average. Use this value           varies. With
        and make a "standards given" p chart based on
        these data. Does it appear that the process frac-         Xi = the number of defects found on sample i
        tion defective was stable at the p = .02 value             ki = the number of units inspected at time i
        over the period represented by these data?                u^ i = Xi /ki

   (b) Make a retrospective p chart for these data.            the following were obtained at eight consecutive
        What does this chart indicate about the stability      periods:
        of the canning process?
                                                            i 1 2 3 4 567 8
2. The accompanying table lists some data on out-
   let leaks found in the first assembling of two ra-       ki 1 2 1 3 2 1 1 3
   diator parts, again taken from Burr's Statistical        u^ i 0 1.5 0 .67 2 0 0 .33
   Quality Control Methods. Each radiator may have
   several leaks.

    Date Number Tested Leaks

    6/3            39      14                               (a) What do these values suggest about the stability
                                                                 of the process?
    6/4            45      4
                                                            (b) Suppose that from now on, ki is going to be
    6/5            46      5                                     held constant and that standard quality will
                                                                 be defined as a mean of 1.2 defects per unit.
    6/6            48      13                                    Compare 3-sigma Shewhart c charts based on

    6/7            40      6

    6/10           58      2
        ki = 1 and on ki = 2 in terms of the probabil-                                   Chapter 7 Exercises 533
        ities that a given sample produces an "out of
        control" signal if                                 What do these values indicate about the stability of
         (i) the actual defect rate is standard.           the bolt cutting process?
        (ii) the actual defect rate is twice standard.  5. Why is it essential to have an operational definition
                                                           of a nonconformance to make effective practical
4. Successive samples of carriage bolts are checked        use of a Shewhart c chart?
   for length using "a go-no go" gauge. The results     6. Explain why too little variation appearing on a
   from ten successive samples are as follows:             Shewhart control chart need not be a good sign.

Sample            1 2 3 4 5 6 7 8 9 10

Sample Size 30 20 40 30 20 20 30 20 20 20

Nonconforming 2 1 5 1 2 1 3 0 1 2

Chapter 7 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Hoffman, Jabaay, and Leuer did a study of pen-           differences in mean lead strengths, µ4H -
   cil lead strength. They loaded pieces of lead of         µH, µ4H - µB, and µH - µB.
   the same diameter (supported on two ends) in         (e) Suppose that for some reason it is desirable
   their centers and recorded the forces at which they
   failed. Part of their data are given here (in grams  to compare the mean strength of B lead to
   of load applied at failure).
                                                        the average of the mean strengths of 4H and

                                                        H leads. Give a 95% two-sided confidence

                                                        interval  for  the  quantity  1  µ4H + µH  - µB.
                                                                                      2
                                                        (f) Use the P-R method of simultaneous confi-
4H lead           H lead            B lead
                                                        dence intervals and make simultaneous 95%
56.7, 63.8, 56.7  99.2, 99.2, 92.1  56.7, 63.8, 70.9
63.8, 49.6        106.0, 99.2       63.8, 70.9          two-sided confidence intervals for the three

                                                        mean strengths, µ4H, µH, and µB. How do
                                                        the lengths of these intervals compare to the

(a) In applying the methods of this chapter in the      lengths of the intervals you found in part (c)?
     analysis of these data, what model assump-
     tions must be made? Make three normal plots        Why is it sensible that the lengths should be
     of these samples on the same set of axes and
     also make a normal plot of residuals for the       related in this way?
     one-way model as means of investigating the
     reasonableness of these assumptions. Com-          (g) Use the Tukey method of simultaneous confi-
     ment on the plots.
                                                        dence intervals and make simultaneous 95%
(b) Compute a pooled estimate of variance based
     on these three samples. What is the corre-         two-sided confidence intervals for the three
     sponding value of sP?
                                                        differences in mean lead strengths, µ4H -
(c) Use the value of sP that you calculated in (b)      µH, µ4H - µB, and µH - µB. How do the
     and make (individual) 95% two-sided con-           lengths of these intervals compare to the
     fidence intervals for each of the three mean
     lead strengths, µ4H, µH, and µB.                   lengths of the intervals you found in part (d)?

(d) Use sP and make (individual) 95% two-sided          (h) Use the one-way ANOVA test statistic and
     confidence intervals for each of the three
                                                        assess the strength of the evidence against

                                                        H0: µ4H = µH = µB in favor of Ha: not H0.
                                                        Show the whole five-step format.

                                                        (i) Make the ANOVA table corresponding to the

                                                        significance test you carried out in part (h).
534 Chapter 7 Inference for Unstructured Multisample Studies

    (j) As a means of checking your work for parts               on what the charts indicate about the turning
        (h) and (i) of this problem, use a statistical           process.
        package to produce the required ANOVA ta-            (c) If you were to judge the sample ranges to
        ble, F statistic, and p-value.                           be stable, it would then make sense to use R¯
                                                                 to develop an estimate of the turning process
2. Allan, Robbins, and Wyckoff worked with a ma-                 short-term standard deviation  . Find such an
   chine shop that employs a CNC (computer nu-                   estimate.
   merically controlled) lathe in the manufacture of        (d) The engineering specifications for the turned
   a part for a heavy equipment maker. Some sum-                 diameter are (still in .0001 in. above 1.1800
   mary statistics for measurements of a particular              in.) from 4 to 14. Supposing that the average
   diameter on the part for 20 hourly samples of                 diameter could be kept on target (at the mid-
   m = 4 parts turned on the lathe are given here.               specification), does your estimate of  from
   (The means are in 10-4 in. above 1.1800 in. and               part (c) suggest that the turning process would
   the ranges are in 10-4 in.)                                   then be capable of producing most diameters
                                                                 in these specifications? Explain.
Sample 1  2   3   4   5
                                                         3. Becker, Francis, and Nazarudin conducted a study
x¯  9.25 8.50 9.50 6.25 5.25                                of the effectiveness of commercial clothes dryers
                                                            in removing water from different types of fabric.
R   1     2   2   8   7                                     The following are some summary statistics from
                                                            a part of their study, where a garment made of one
Sample 6  7   8   9   10                                    of r = 3 different blends was wetted and dried for
                                                            10 minutes in a particular dryer and the (water)
x¯  5.25 5.75 19.50 10.0 9.50                               weight loss (in grams) measured. Each of the three
                                                            different garments was tested three times.
R   5     5   1   3   1

Sample 11 12  13  14  15                                 100% Cotton     Cotton/Polyester    Cotton/Acrylic

x¯  9.50 9.75 12.25 12.75 14.50                           n1 = 3             n2 = 3            n3 = 3
                                                          y¯ 1 = 85.0 g      y¯ 2 = 348.3 g    y¯ 3 = 258.3 g
R   6     1   9   2   7                                    s1 = 25.0 g       s2 = 88.1 g       s3 = 63.3 g

Sample 16 17  18  19  20

x¯  8.00 10.0 10.25 8.75 10.0                            (a) What restrictions/model assumptions are re-
                                                              quired in order to do formal inference based
R   3     0   1   3   0                                       on the data summarized here (if information
                                                              on the baseline variability involved is pooled
    (a) The midspecification for the diameter in ques-        and the formulas of this chapter are used)?
                                                              Assume that those model assumptions are a
         tion was 1.1809 in. Suppose that a standard          sensible description of this situation.

          for diameters turned on this machine is        (b) Find sP and the associated degrees of free-
         2.5 × 10-4 in. Use these two values and find         dom.
         "standards given" control limits for x¯ and R.
         Make both x¯ and R charts using these and       (c) What does sP measure?
         comment on what the charts indicate about       (d) Give a 90% lower confidence bound for the

         the turning process.                                 mean amount of water that can be removed
                                                              from the cotton garment by this dryer in a
    (b) In contrast to part (a) where standards were          10-minute period.

         furnished, compute retrospective or "as past
         data" control limits for both x¯ and R. Make
         both x¯ and R charts using these and comment
                                                                                  Chapter 7 Exercises 535

      (e) Give a 90% two-sided confidence interval for      (d) What model assumptions stand behind the
           comparing the means for the two blended gar-
           ments.                                           formulas you used in parts (a) and (b)? In

      (f) Suppose that all pairs of fabric means are        part (c)?
           to be compared using intervals of the form
           y¯ i - y¯ i ± and that simultaneous 95% con-     For the following questions, consider test results
           fidence is desired. Find .
                                                            from all eight glues when making your analyses.
      (g) A partially completed ANOVA table for test-
           ing H0: µ1 = µ2 = µ3 follows. Finish filling     (e) Find a pooled sample standard deviation and
           in the table then find a p-value for a signifi-
           cance test of this hypothesis.                   give its degrees of freedom.

                                                            (f) Repeat parts (a) and (b) using the pooled stan-

                                                            dard deviation instead of only s1. What extra
                                                            model assumption is required to do this (be-

                                                            yond what was used in parts (a) and (b))?

                                                            (g) Find the value of an F statistic for testing

              ANOVA Table                                   H0: µ1 = µ2 = · · · = µ8 and give its degrees
                                                            of freedom. (Hint: These data are balanced.
      Source  SS       d f MS F
                                                            You ought to be able to use the y¯ 's and the

              24,787                                        sample variance routine on your calculator to

              132,247                                       help get the numerator for this statistic.)

                                                            (h) Simultaneous 95% two-sided confidence lim-

4. The article "Behavior of Rubber-Based Elasto-            its for the mean strengths for the eight glues
   meric Construction Adhesive in Wood Joints" by
   P. Pellicane (Journal of Testing and Evaluation,         are of the form y¯ i ±            for an appropriate
   1990) compared the performance of r = 8 dif-             number . Find .
   ferent commercially available construction adhe-
   sives. m = 8 joints glued with each glue were            (i) Simultaneous 95% two-sided confidence lim-
   tested for strength, giving results summarized as
   follows (the units are kN):                              its for all differences in mean strengths for the

                                                            eight glues are of the form y¯ i - y¯ i ± for a
                                                            number . Find .

Glue (i) 1 2 3 4 5 6 7 8                                    5. Example 7 in Chapter 4 treats some data collected
                                                               by Kotlers, MacFarland, and Tomlinson while
y¯ i  1821 1968 1439 616 1354 1424 1694 1669                   studying strength properties of wood joints. Part
                                                               of those data (stress at failure values in units of psi
si    214 435 243 205 135 191 225 551                          for four out of the original nine wood/joint type
                                                               combinations) are reproduced here, along with y¯
                                                               and s for each of the four samples represented:

      (a) Temporarily considering only the test results                           Wood Type
           for glue 1, give a 95% lower tolerance bound
           for the strengths of 99% of joints made with                     Butt  Pine        Oak
           glue 1.                                                                            1169
                                                            Joint Type            829
      (b) Still considering only the test results for glue                  Lap   596         y¯ = 1169
           1, give a 95% lower confidence bound for the                           y¯ = 712.5
           mean strength of joints made with glue 1.                              s = 164.8   1295
                                                                                              1561
      (c) Now considering only the test results for                               1000        y¯ = 1428.0
           glues 1 and 2, assess the strength of the evi-                         859         s = 188.1
           dence against the possibility that glues 1 and                         y¯ = 929.5
           2 produce joints with the same mean strength.                          s = 99.7
           Show the whole five-step significance-testing
           format.
536 Chapter 7 Inference for Unstructured Multisample Studies

(a) Treating pine/butt joints alone, give a 95%                sometimes have difficulty distinguishing in
     two-sided confidence interval for mean                    their thinking and speech between specifica-
     strength for such joints. (Here, base your in-            tions and control limits. Briefly (but carefully)
     terval on only the pine/butt data.)                       discuss the difference in meaning between the
                                                               control limits for x¯ found in part (a) and these
(b) Treating only lap joints, how strong is the                engineering specifications. (To what quanti-
     evidence shown here of a difference in mean               ties do the two apply? What are the different
     joint strength between pine and oak woods?                purposes for the two? Where do the two come
     (Here use only the pine/lap and oak/lap data.)            from? And so on.)
     Use the five-step format.
                                                       7. Here are some summary statistics produced by
(c) Give a 90% two-sided confidence interval for          Davies and Sehili for ten samples of m = 4 pin
     comparing the strength standard deviations           head diameters formed on a type of electrical com-
     for pine/lap and oak/lap joints.                     ponent. The sampled components were groups
                                                          of consecutive items taken from the output of a
Consider all four samples in the following ques-          machine approximately once every ten minutes.
tions.                                                    The units are .001 in.
(d) Assuming that all four wood type/joint type
                                                       Sample x¯  Rs   Sample x¯              Rs
     conditions are thought to have approximately
     the same associated variability in joint          1 31.50 3 1.29   6 33.00 3 1.41
     strength, give an estimate of this supposedly     2 30.75 2 .96    7 33.00 2 .82
     common standard deviation.                        3 29.75 3 1.26   8 33.00 4 1.63
(e) It is possible to compute simultaneous 95%         4 30.50 3 1.29   9 34.00 2 .82
     lower (one-sided) confidence limits for mean      5 32.00 0 0     10 26.00 0 0
     joint strengths for all four wood type/joint
     type combinations. Give these (based on the       Some summaries for the statistics are
     P-R method).
(f) Suppose that you want to compare butt joint
     strength to lap joint strength and in fact want
     a 95% two-sided confidence interval for

1  1                                                   x¯ = 313.5     R = 22 and              s = 9.48
   (µpine/butt + µoak/butt) - (µpine/lap + µoak/lap)
2  2

        Give such a confidence interval, again making  (a) Assuming that the basic short-term variabil-
        use of your answer to (d).                          ity of the mechanism producing pin head di-
                                                            ameters is constant, it makes sense to try to
6. In an industrial application of Shewhart x¯ and R        quantify it in terms of a standard deviation  .
   control charts, 20 successive hourly samples of          Various estimates of that  are possible. Give
   m = 2 high-precision metal parts were taken, and         three such possible estimates based on R, s¯,
   a particular diameter on the parts was measured.         and sP.
   x¯ and R values were calculated for each of the 20
   samples, and these had                              (b) Using each of your estimates from (a), give
                                                            retrospective control limits for both x¯ and R.
   x¯¯ = .35080 in. and R = .00019 in.
                                                       (c) Compare the x¯ 's and R's given above to your
(a) Give retrospective control limits that you              control limits from (b) based on R. Are there
     would use in an analysis of the x¯ and R values.       any points that would plot outside control
                                                            limits on a Shewhart x¯ chart? On a Shewhart
(b) The engineering specifications for the diame-           R chart?
     ter being measured were .3500 in. ± .0020 in.
     Unfortunately, even practicing engineers          (d) For the company manufacturing these parts,
                                                            what are the practical implications of your
                                                            analysis in parts (b) and (c)?
                                                                                 Chapter 7 Exercises 537

8. Dunnwald, Post, and Kilcoin studied the viscosi-               number of visual imperfections on a square foot
   ties of various weights of various brands of motor             of plastic sheet is  = .04.
   oil. Some summary statistics for part of their data            (a) Give upper control limits for the number of
   are given here. Summarized are m = 10 measure-
   ments of the viscosities of each of r = 4 different                 imperfections found on pieces of material
   weights of Brand M motor oil at room tempera-                       .5 ft × .5 ft and then 5 ft × 5 ft.
   ture. Units are seconds required for a ball to drop            (b) What would you tell a worker who, instead
   a particular distance through the oil.                              of inspecting a 10 ft × 10 ft specimen of the
                                                                       plastic (counting total imperfections on the
 10W30       SAE 30         10W40         20W50                        whole), wants to inspect only a 1 ft × 1 ft
                                                                       specimen and multiply the observed count of
y¯ = 1.385  y¯ 2 = 2.066  y¯ 3 = 1.414  y¯ 4 = 4.498                   imperfections by 100?
s1 = .091   s2 = .097     s3 = .150     s4 = .204
                                                             11. Bailey, Goodman, and Scott worked on a process
(a) Find the pooled sample standard deviation                     for attaching metal connectors to the ends of hy-
                                                                  draulic hoses. One part of that process involved
here. What are the associated degrees of free-                    grinding rubber off the ends of the hoses. The
                                                                  amount of rubber removed is termed the skive
dom?                                                              length. The values in the accompanying table are
                                                                  skive length means and standard deviations for
(b) If the P-R method is used to find simultane-                  20 samples of five consecutive hoses ground on
                                                                  one grinder. Skive length is expressed in .001 in.
ous 95% two-sided confidence intervals for                        above the target length.

all four mean viscosities, the intervals pro-

duced are of the form y¯ i ± , for      an ap-
propriate number. Find .

(c) If the Tukey method is used to find simulta-             Sample x¯  s        Sample     x¯      s

neous 95% two-sided confidence intervals for                                        11    -2.2     5.50
                                                                                    12    -5.2     2.86
all differences in mean viscosities, the inter-              1   -.4 5.27           13      -.8    1.30
                                                                                    14             2.68
vals produced are of the form y¯ i - y¯ i ± ,                2   0.0 4.47           15        .8   2.92
for an appropriate number. Find .                                                  16     -2.0     1.30
                                                             3   -1.4 3.29         17       -.2    2.30
                                                                                   18     -6.6     4.21
(d) Carry out an ANOVA test of the hypothesis                4   1.8 2.28          19     -1.0     5.76
                                                                                   20     -3.2     4.28
that the four oil weights have the same mean                 5   1.4 1.14                 -2.4
                                                                                                  69.07
viscosity.                                                   6   0.0 4.24                -23.4

9. Because of modern business pressures, it is not           7   -.4 4.39
   uncommon for standards for fractions noncon-
   forming to be in the range of 10-4 to 10-6.               8   1.4 4.51
    (a) What are "standards given" 3-sigma control
        limits for a p chart with standard fraction          9          .2 4.32
        nonconforming 10-4 and sample size 100?
   (b) If p becomes twice the standard value (of             10  -3.2 2.05
        10-4), what is the probability that the scheme
        from (a) detects this state of affairs at the first  (a) What do these values indicate about the stabil-
        subsequent sample? (Use your answer to (a)                ity of the skiving process? Show appropriate
        and the binomial distribution for n = 100 and             work and explain fully.
         p = 2 × 10-4.)
    (c) What does (b) suggest about the feasibility          (b) Give an estimate of the process short-term
        of doing process monitoring for very small                standard deviation based on the given values.
        fractions defective based on attributes data?
                                                             (c) If specifications on the skive length are ±.006
10. Suppose that a company standard for the mean                  in. and, over short periods, skive length can
                                                                  be thought of as normally distributed, what
                                                                  does your answer to (b) indicate about the
538 Chapter 7 Inference for Unstructured Multisample Studies

          best possible fraction (for perfectly adjusted             might then consider charting
          grinders) of skives in specifications? Give a
          number.                                                             X = demerits
     (d) Based on your answer to (b), give control
          limits for future control of skive length means                        = 2(number of A defects)
          and ranges for samples of size m = 3.
     (e) Suppose that hoses from all grinders used dur-                             + (number of B defects)
          ing a given shift are all dumped into a com-
          mon bin. If upon sampling, say, 20 hoses from              If one can model (number of A defects) and
          this bin at the end of a shift, the 20 measured            (number of B defects) as independent Pois-
          skive lengths have a standard deviation twice              son random variables, it is relatively easy to
          the size of your answer to (b), what possible              come up with sensible control limits. (Re-
          explanations come to mind for this?                        member that the variance of a sum of inde-
      (f) Suppose current policy is to sample five con-              pendent random variables is the sum of the
          secutive hoses once an hour for each grinder.              variances.)
          An alternative possibility is to sample one                 (i) If the mean number of A defects per wid-
          hose every 12 minutes for each grinder.                    get is 1 and the mean number of B defects per
           (i) Briefly discuss practical trade-offs that             widget is 2, what are the mean and variance
          you see between the two possible sampling                  for X ? Use your answers to give "standards
          methods.                                                   given" control limits for X .
          (ii) If in fact the new sampling scheme were               (ii) In light of your answer to (i), what nu-
          adopted, would you recommend treating the                  merical limits for X would you use to analyze
          five hoses from each hour as a sample of size              these values "as past data"?
          5 and doing x¯ and R charting with m = 5?
          Explain.                                         13. (Variables Versus Attributes Control Chart-
                                                                ing) Suppose that a dimension of parts pro-
12. Two different types of nonconformance can ap-               duced on a certain machine over a short period can
     pear on widgets manufactured by Company V.                 be thought of as normally distributed with some
     Counts of these on ten widgets produced one per            mean µ and standard deviation  = .005 in. Sup-
     hour are given here.                                       pose further that values of this dimension more
                                                                than .0098 in. from the 1.000 in. nominal value are
Widget  1 2 3 4 5 6 7 8 9 10                                    considered nonconforming. Finally, suppose that
                                                                hourly samples of ten of these parts are to be taken.
Type A Defects 4 2 1 2 2 2 0 2 1 0                              (a) If µ is exactly on target (i.e., µ = 1.000 in.),
                                                                     about what fraction of parts will be noncon-
Type B Defects 0 2 2 4 2 4 3 3 7 2                                   forming? Is it possible for the fraction non-
                                                                     conforming ever to be any less than this fig-
Total Defects 4 4 3 6 4 6 3 5 8 2                                    ure?
                                                                (b) One could use a p chart based on m = 10 to
(a) Considering first total nonconformances, is                      monitor process performance in this situation.
     there evidence here of process instability?                     What would be "standards given" 3-sigma
     Show appropriate work.                                          control limits for the p chart, using your an-
                                                                     swer from part (a) as the standard value of p?
(b) What statistical indicators might you expect                (c) What is the probability that a particular sam-
     to observe in data like these if in fact type A                 ple of m = 10 parts will produce an "out of
     and B defects have a common cause mecha-                        control" signal on the chart from (b) if µ re-
     nism?                                                           mains at its standard value of µ = 1.000 in.?
                                                                     How does this compare to the same probability
(c) (Charts for Demerits) For the sake of ex-
     ample, suppose that type A defects are judged
     twice as important as type B defects. One
     for a 3-sigma x¯ chart for m = 10 set up                                               Chapter 7 Exercises 539
     with a center line at 1.000? (For the p chart,
                                                                (d) Find and interpret a two-sided 90% confi-
     use a binomial probability calculation. For                     dence interval for  and then the ratio  / .
     the x¯ chart, use the facts that µx¯ = µ and
     x¯ = / m.)                                                 (e) If there is variability in skew, customers must
(d) Compare the probability that a particular sam-                   continually adjust automatic folding and pack-
     ple of m = 10 parts will produce an "out of                     aging equipment in order to prevent machine
     control" signal on the p chart from (b) to the                  jam-ups. Such variability is therefore highly
                                                                     undesirable for the box manufacturer, who
     probability that the sample will produce an                     wishes to please customers. What does your
     "out of control" signal on the (m = 10) 3-                      analysis from (c) and (d) indicate about how
     sigma x¯ chart first mentioned in (c), suppos-                  the manufacturer should proceed in any at-
     ing that in fact µ = 1.005 in. What moral is                    tempts to reduce variability in skew? (What
     told by your calculations here and in part (c)?                 is the big component of variance, and what
                                                                     kind of actions might be taken to reduce it?
14. The article "How to Use Statistics Effectively in                For example, is there a need for the immediate
                                                                     purchase of new high-precision manufactur-
a Pseudo-Job Shop" by G. Fellers (Quality En-                        ing equipment?)

gineering, 1990) discusses some applications of            15. The article "High Tech, High Touch" by J. Ryan
                                                                (Quality Progress, 1987) discusses the quality en-
statistical methods in the manufacture of corru-                hancement processes used by Martin Marietta in
                                                                the production of the space shuttle external (liq-
gated cardboard boxes. One part of the article                  uid oxygen) fuel tanks. It includes a graph giving
                                                                counts of major hardware nonconformances for
concerns the analysis of a variable called box                  each of 41 tanks produced. The accompanying
                                                                data (see next page) are approximate counts read
"skew," which quantifies how far from being per-                from that graph for the last 35 tanks. (The first 6
                                                                tanks were of a different design than the others
fectly square boxes are. This response variable,                and are therefore not included here.)
                                                                (a) Make a retrospective c chart for these data.
which will here be called y, is measured in units                    Is there evidence of real quality improvement
                                                                     in this series of counts of nonconformances?
of  1      in.  r  = 24  customer  orders  (each  requir-            Explain.
    32                                                          (b) Consider only the last 17 tanks. Does it ap-
ing a different machine setup) were studied, and                     pear that quality was stable over the produc-
                                                                     tion period represented by these tanks? (Make
from each, the skews, y, of five randomly se-                        another retrospective c chart.)
                                                                (c) It is possible that some of the figures read
lected boxes were measured. A partial ANOVA                          from the graph in the original article may dif-
                                                                     fer from the real figures by as much as, say, 15
table made in summary of the data follows.                           nonconformances. Would this measurement
                                                                     error account for the apparent lack of stabil-
                   ANOVA Table                                       ity you found in (a) or (b) above? Explain.

    Source               SS        df  MS F

    Order (setup)  1052.39
    Error

    Total          1405.59 119

(a) Complete the ANOVA table.
(b) In a given day, hundreds of different orders

     are run in this plant. This situation is one
     in which a random effects analysis is most
     natural. Explain why.
(c) Find estimates of  and  . What, in the con-
     text of this situation, do these two estimates
     measure?
540 Chapter 7 Inference for Unstructured Multisample Studies

Tank Nonconformances  Tank Nonconformances                        Day 1                  Day 2

1   537               19  157                             Sample  Nonconforming  Sample  Nonconforming

2   463               20  120                                  1          16          1          14
                                                               2          18          2          20
3   417               21  148                                  3          17          3          17
                                                               4          18          4          13
4   370               22  65                                   5          22          5          12
                                                               6          14          6          12
5   333               23  130                                  7          16          7          14
                                                               8          18          8          15
6   241               24  111                                  9          18          9          19
                                                             10           19        10           21
7   194               25  65                                 11           20        11           18
                                                             12           25        12           14
8   185               26  74                                 13           14        13           13
                                                             14           13        14            9
9   204               27  65                                 15          23         15           16
                                                             16          13         16           16
10  185               28  148                                17          23         17           15
                                                             18          15         18           11
11  167               29  74                                 19          14         19           17
                                                             20          23         20            8
12  157               30  65                                 21          17         21           16
                                                             22          20         22           13
13  139               31  139                                23          16         23           16
                                                             24          19         24           15
14  130               32  213                                25          22         25           13

15  130               33  222

16  267               34  93

17  102               35  194

18  130

16. Kaminski, Rasavahn, Smith, and Weitekamper            (d) Repeat parts (a) and (b) for the day 2 data.
     worked with the same pelletizing machine re-         (e) Try making a single retrospective control
     ferred to in Examples 2 (Chapter 1), 14 (Chap-
     ter 3), and 18 (Chapter 6). They collected process        chart for the two days taken together. Do
     monitoring data on several different days of op-          points plot out of control on this single chart?
     eration. The accompanying table shows counts of           Explain why this does or does not contradict
     nonconforming pellets in periodic samples of size         the results of parts (a), (b), and (d).
     m = 30 from two different days. (The pelletizing     (f) Treating the data from days 1 and 2 as two
     on day 1 was done with 100% fresh material, and           samples of size 750 from the respective days'
     on the second day, a mixture of fresh and reground        production of pellets, give a two-sided 98%
     materials was used.)                                      confidence interval for the difference in
     (a) Make a retrospective p chart for the day 1            fractions of nonconforming pellets produced
          data. Is there evidence of process instability       on the two days.
          in the day 1 data? Explain.
     (b) Treating the day 1 data as a single sample of
          size 750 from the day's production of pellets,
          give a 90% two-sided confidence interval for
          the fraction nonconforming produced on the
          day in question.
     (c) In light of your answers to parts (a) and (b),
          explain why a process being in control or sta-
          ble does not necessarily mean that it is pro-
          ducing a satisfactory fraction of conforming
          product.
17. Eastman, Frye, and Schnepf counted defective                                           Chapter 7 Exercises 541
     plastic bags in 15 consecutive groups of 250 com-
     ing off a converting machine immediately after a               form y¯ i ± , for an appropriate number .
     changeover to a new roll of plastic. Their counts              If 90% individual confidence is desired, what
     are as follows:                                                value of should be used?
                                                               (d) Individual two-sided confidence intervals for
Sample Nonconforming Sample Nonconforming                           the differences in the five different means
                                                                    would be of the form y¯ i - y¯ i ± , for a num-
1        147               9        0                               ber . If 90% individual confidence is de-
                                                                    sired, what value of should be used here?
2        93                10       0                          (e) Using the P-R method, what would be used
                                                                    to make two-sided intervals of the form y¯ i ±
3        41                11       0
                                                                       for all five mean boiling points, possessing
4        0                 12       0                               simultaneous 95% confidence?
                                                                (f) Using the Tukey method, what would be
5        18                13       0                               used to make two-sided intervals of the form
                                                                    y¯ i - y¯ i ± for all differences in the five
6        0                 14       0                               mean boiling points, possessing simultaneous
                                                                    99% confidence?
7        31                15       0                          (g) Make an ANOVA table for these data. Then
                                                                    use the calculations to find both R2 for the
8        22                                                         one-way model and also the observed level
                                                                    of significance for an F test of the null hy-
     Is it plausible that these data came from a phys-              pothesis that all five oils have the same mean
     ically stable process, or is it clear that there is            boiling point.
     some kind of start-up phenomenon involved here?           (h) It is likely that the measurements represented
     Make and interpret an appropriate control chart to             here were all made on a single can of each
     support your answer.                                           brand of oil. (The students' report was not
                                                                    explicit about this point.) If so, the formal in-
18. Sinnott, Thomas, and White compared several                     ferences made here are really most honestly
     properties of five different brands of 10W30 mo-               thought of as applying to the five particu-
     tor oil. In one part of their study, they measured             lar cans used in the study. Discuss why the
     the boiling points of the oils. m = 3 measure-                 inferences would not necessarily extend to
     ments for each of the r = 5 oils follow. (Units are            all cans of the brands included in the study
     degrees F.)                                                    and describe the conditions under which you
                                                                    might be willing to make such an extension.
Brand C  Brand H  Brand W  Brand Q  Brand P                         Is the situation different if, for example, each
                                                                    of the measurements comes from a different
  378      357       321     353      390                           can of oil, taken from different shipping lots?
  386      365       303     349      378                           Explain.
  388      361       306     353      381
                                                          19. Baik, Johnson, and Umthun worked with a small
(a) Compute and make a normal plot for the                     metal fabrication company on monitoring the per-
     residuals for the one-way model. What does                formance of a process for cutting metal rods.
     the plot indicate about the appropriateness of            Specifications for the lengths of these rods were
     the one-way model assumptions?                            33.69 in. ± .03 in. Measured lengths of rods in 15
                                                               samples of m = 4 rods, made over a period of two
(b) Using the five samples, find sP, the pooled
     estimate of  . What does this value measure?
     Give a two-sided 90% confidence interval for
      based on sP.

(c) Individual two-sided confidence intervals for
     the five different means here would be of the
542 Chapter 7 Inference for Unstructured Multisample Studies

    days, are shown in the accompanying table. (The          (b) Repeat part (a) for the sample standard devi-
    data are recorded in inches above the target value            ations rather than ranges.
    of 33.69, and the first five samples were made on
    day 1, while the remainder were made on day 2.)          The initial five samples were taken while the op-
                                                             erators were first learning to cut these particu-
Sample Rod Lengths  x¯  R               s                    lar rods. Suppose that it therefore makes sense
                                                             to look separately at the last ten samples. These
1   .0075, .0100                                             samples have x¯¯ = -.00159, R = .00435, and s¯ =
                                                             .001964.
    .0135, .0135    .01113 .0060 .00293                      (c) Both the ranges and standard deviations of

2   -.0085, .0035                                                 the last ten samples look reasonably stable.
                                                                  What about the last ten x¯ 's? (Compute control
    -.0180, .0010 -.00550 .0215 .00981                            limits for the last ten x¯ 's, based on either R
                                                                  or s¯, and say what is indicated about the rod
3   .0085, .0000                                                  cutting process.)
                                                             As a matter of fact, the cutting process worked
    .0100, .0020    .00513 .0100 .00487                      as follows. Rods were welded together at one
                                                             end in bundles of 80, and the whole bundle cut
4   .0005, -.0005                                            at once. The four measurements in each sample
                                                             came from a single bundle. (There are 15 bundles
    .0145, .0170    .00788 .0175 .00916                      represented.)
                                                             (d) How does this explanation help you under-
5   .0130, .0035                                                  stand the origin of patterns discovered in the
                                                                  data in parts (a) through (c)?
    .0120, .0070    .00888 .0095 .00444                      (e) Find an estimate of the "process short-term
                                                                   " for the last ten samples. What is it really
6   -.0115, -.0110                                                measuring in the present context?
                                                              (f) Use your estimate from (e) and, assuming
    -.0085, -.0105 -.01038 .0030 .00131                           that lengths of rods from a single bundle are
                                                                  approximately normally distributed, compute
7   -.0080, -.0070                                                an estimate of the fraction of lengths in a
                                                                  bundle that are in specifications, if in fact µ =
    -.0060, -.0045 -.00638 .0035 .00149                           33.69 in.
                                                             (g) Simply pooling together the last ten samples
8   -.0095, -.0100                                                (making a single sample of size 40) and com-
                                                                  puting the sample standard deviation gives the
    -.0130, -.0165 -.01225 .0070 .00323                           value s = .00898. This is much larger than
                                                                  any s recorded for one of the samples and
9   .0090, .0125                                                  should be much larger than your value from
                                                                  (e). What is the origin of this difference in
    .0125, .0080    .01050 .0045 .00235                           magnitude?

10  -.0105, -.0100                                      20. Consider the last ten samples from Exercise 19.
                                                             Upon considering the physical circumstances that
    -.0150, -.0075 -.01075 .0075 .00312                      produced the data, it becomes sensible to replace
                                                             the control chart analysis done there with a ran-
11  .0115, .0150                                             dom effects analysis simply meant to quantify

    .0175, .0180    .01550 .0065 .00297

12  .0020, .0005

    .0010, .0010    .00113 .0015 .00063

13  -.0010, -.0025

    -.0020, -.0030 -.00213 .0020 .00085

14  -.0020, .0015

    .0025, .0025    .00113 .0045 .00214

15  -.0010, -.0015

    -.0020, -.0045 -.00225 .0035 .00155

    x¯¯ = .00078 R = .0072 s¯ = .00339

    (a) Find a retrospective center line and control
        limits for all 15 sample ranges. Apply them
        to the ranges and say what is indicated about
        the rod cutting process.
                                                            Chapter 7 Exercises 543

     the within- and between-bundle variance compo-              paper). The data that follow (the units are ounces)
     nents.                                                      are for m = 3 trials with each of the four paper
     (a) Make an ANOVA table for these ten samples               types and also for a "baseline" condition where
                                                                 no paper was loaded into the trimmer.
          of size 4. Based on the mean squares, find es-
          timates of  , the standard deviation of lengths   No Paper Newsprint Construction Computer Magazine
          for a given bundle, and  , the standard devi-
          ation of bundle mean lengths.                     24, 25, 31 61, 51, 52 72, 70, 77 59, 59, 70 54, 59, 61
     (b) Find and interpret a two-sided 90% confi-
          dence interval for the ratio  / .                      (a) If the methods of this chapter are applied in
     (c) What is the principal origin of variability in               the analysis of these data, what model as-
          the lengths of rods produced by this cutting                sumptions must be made? With small sample
          method? (Is it variability of lengths within                sizes such as those here, only fairly crude
          bundles or differences between bundles?)                    checks on the appropriateness of the assump-
                                                                      tions are possible. One possibility is to com-
21. The following data appear in the text Quality Con-                pute residuals and normal-plot them. Do this
     trol and Industrial Statistics by A. J. Duncan.                  and comment on the appearance of the plot.
     They represent the numbers of disabling injuries
     suffered and millions of man-hours worked at a              (b) Compute a pooled estimate of the standard
     large corporation in 12 consecutive months.                      deviation based on these five samples. What
                                                                      is sP supposed to be measuring in the present
Month     1  2  3  4  5  6                                            situation?

Injuries  11 4  5  8  4  4                                       (c) Use the value of sP and make (individual)
                                                                      95% two-sided confidence intervals for each
106 man-hr .175 .178 .175 .180 .183 .198                              of the five mean force requirements µNo paper,
                                                                      µNewsprint, µConstruction, µComputer, and µMagazine.
Month     7  8  9 10 11 12
                                                                 (d) Individual confidence intervals for the differ-
Injuries  9 12 2   6  6  7                                            ences between particular pairs of mean force
                                                                      requirements are of the form y¯ i - y¯ i ± ,
106 man-hr .210 .212 .210 .211 .195 .200                              for an appropriate value of . Use sP and find
                                                                         if individual 95% two-sided intervals are
     (a) Temporarily assuming the injury rate per man-                desired.
          hour to be stable over the period studied, find
          a sensible estimate of the mean injuries per           (e) Suppose that it is desirable to compare the
          106 man-hours.                                              "no paper" force requirement to the average
                                                                      of the force requirements for the various pa-
     (b) Based on your figure from (a), find "control                 pers. Give a 95% two-sided confidence inter-
          limits" for the observed rates in each of the 12            val for the quantity µNo paper - 41 (µNewsprint +
          months. Do these data appear to be consistent               µConstruction + µComputer + µMagazine).
          with a "stable system" view of the corpora-
          tion's injury production mechanisms? Or are            (f) Use the P-R method of simultaneous confi-
          there months that are clearly distinguishable               dence intervals and make simultaneous 95%
          from the others in terms of accident rates?                 two-sided confidence intervals for the five
                                                                      mean force requirements. How do the lengths
22. Eder, Williams, and Bruster studied the force (ap-                of these intervals compare to the lengths of
     plied to the cutting arm handle) required to cut                 the intervals you found in part (c)? Why is it
     various types of paper in a standard paper trim-                 sensible that the lengths should be related in
     mer. The students used stacks of five sheets of four             this way?
     different types of paper and recorded the forces
     needed to move the cutter arm (and thus cut the
544 Chapter 7 Inference for Unstructured Multisample Studies

     (g) Simultaneous confidence intervals for the dif-       of this hypothesis.
          ferences between all pairs of mean force re-
          quirements are of the form y¯ i - y¯ i ± , for                          ANOVA Table
          an appropriate value of . Use sP and find
          if Tukey simultaneous 95% two-sided inter-          Source              SS  d f MS F
          vals are desired. How does this value compare
          to the value you found in part (d)?                 Total 14.1608

     (h) Use the one-way ANOVA test statistic and             24. Davis, Martin, and Poppinga used a ytterbium
          assess the strength of the students' evidence            argon gas laser to make some cuts in stainless
          against H0: µNo paper = µNewsprint = µConstruction       steel-316. Using 95 mJ/pulse and 20 Hz settings
           = µComputer = µMagazine in favor of Ha: not             on the laser and a 15.5 mm distance to the steel
          H0. Show the whole five-step format.                     specimens (set at a 45angle to the laser beam),
                                                                   the students made cuts in specimens using 100,
      (i) Make the ANOVA table corresponding to the                500, and 1,000 pulses. (Although this is not ab-
          significance test you carried out in part (h).           solutely clear from the students' report, it seems
                                                                   that four specimens were cut using each number
23. Duffy, Marks, and O'Keefe did some testing of                  of pulses.) The depths of cut the students mea-
     the 28-day compressive strengths of 3 in. × 6 in.             sured were then as follows:
     concrete cylinders. In part of their study, concrete
     specimens made with a .50 water/cement ratio and         100 Pulses              500 Pulses
     different percentages of entrained air were cured        7.4, 8.6, 5.6, 8.0      24.2, 29.5, 26.5, 23.8
     in a moisture room and subsequently strength
     tested. m = 4 specimens of each type produced
     the measured strengths (in 103 psi) summarized
     as follows:

   3% Air         6% Air        10% Air                                        1000 Pulses

y¯ 1 = 5.3675  y¯ 2 = 4.9900  y¯ 3 = 2.9250                                    33.4, 37.5, 35.9, 34.8
s1 = .1638     s2 = .1203     s3 = .2626
                                                              (a) If the methods of this chapter are applied
(a) Find the pooled sample standard deviation                      in the analysis of these three samples, what
     and its associated degrees of freedom.                        model assumptions must be made? Compute
                                                                   residuals and normal plot them as something
Use your answer to part (a) throughout the rest of                 of a check on the reasonableness of these as-
this problem.                                                      sumptions. Comment on the appearance of
(b) Give a 99% lower confidence bound for the                      the plot.

     mean strength of 3% air specimens.                       (b) Compute a pooled estimate of the standard
(c) Give a 99% two-sided confidence interval for                   deviation based on these three samples. What
                                                                   is sP supposed to be measuring in the present
     comparing the mean strengths of 3% air and                    situation?
     10% air specimens.
(d) Suppose that mean strengths of specimens for              (c) Make (individual) 95% two-sided confidence
     all pairs of levels of entrained air are to be                intervals for each of the three mean depths of
     compared using intervals of the form y¯ i -                   cut, µ100, µ500, and µ1000.
     y¯ i ± . Find for Tukey simultaneous 99%
     two-sided confidence limits.                             (d) Confidence intervals for the differences be-
(e) A partially completed ANOVA table for test-                    tween particular pairs of mean depths of cut
     ing H0: µ1 = µ2 = µ3 follows. Finish filling                  are of the form y¯ i - y¯ i ± , for a number .
     in the table, then find a p-value for an F test
                                                                                        Chapter 7 Exercises 545

    Find if individual 95% two-sided intervals              tics for the tests on four particular designs are
    are desired.                                            given next. (The units are seconds.)
(e) Suppose that it is desirable to compare the per
    pulse change in average depth of cut between            Design #1     Design #2     Design #3     Design #4
    100 pulses and 500 pulses to the per pulse
    change in average depth of cut between 500              n1 = 4        n2 = 4        n3 = 4        n4 = 4
    pulses and 1,000 pulses. Give a 90% two-                y¯ 1 = 1.640  y¯ 2 = 2.545  y¯ 3 = 1.510  y¯ 4 = 2.600
    sided confidence interval for the quantity              s1 = .096     s2 = .426     s3 = .174     s4 = .168

1    µ500 - µ100 -  1    µ1000 - µ500                       (a) Find a pooled estimate of  in the one-way
                                                                 model. What does this quantity measure in
400                 500                                          the present context?

          (You will need to write this out as a linear      (b) Give 95% two-sided confidence limits for the
          combination of the three means before apply-           mean flight time of helicopters of Design #1.
          ing any formulas from Section 7.2.) Based on
          this interval, does it appear plausible that the  (c) P-R simultaneous two-sided 95% confidence
          depth of cut changes linearly in the number            limits for all mean flight times of the designs
          of pulses over the range from 100 to 1,000             are of the form y¯ i ± . Find .
          pulses? Explain.
      (f) Use the P-R method of simultaneous confi-         (d) Give 95% two-sided confidence limits for the
          dence intervals and make simultaneous 95%              difference in mean flight times of helicopters
          two-sided confidence intervals for the three           of Designs #1 and #2.
          mean depths of cut. How do the lengths of
          these intervals compare to the lengths of the     (e) Tukey simultaneous two-sided 95% confi-
          intervals you found in part (c)? Why is it sen-        dence limits for all differences in mean flight
          sible that the lengths should be related in this       times of the designs are of the form y¯ i - y¯ i ±
          way?                                                     , for a number . Find .
     (g) Simultaneous confidence intervals for the dif-
          ferences between all pairs of mean depths of      (f) Based on your answer to part (e), do you
          cut are of the form y¯ i - y¯ i ± , for a num-         believe that there are "statistically signifi-
          ber . Find if Tukey simultaneous 95%                   cant"/"statistically detectable" differences
          two-sided intervals are desired. How does this         among these four designs in terms of mean
          value compare to the one you found in part             flight times? Explain.
          (d)?
     (h) Use the one-way ANOVA test statistic and           (g) Do a formal significance test of H0: µ1 =
          assess the strength of the evidence against            µ2 = µ3 = µ4. Show the whole five-step for-
          H0: µ1 = µ2 = µ3. Show the whole five-step             mat.
          format.
      (i) Make the ANOVA table corresponding to the         (h) As a matter of fact, the four designs consid-
          significance test you carried out in part (h).         ered here were Design #1, 2 in. wings and
                                                                 1 in. body; Design #2, 4 in. wings and 1 in.
25. Anderson, Panchula, and Patrick tested several de-           body; Design #3, 2 in. wings and 3 in. body;
     signs of "paper helicopters" for flight times when          Design #4, 4 in. wings and 3 in. body. So the
     dropped from a point approximately 8 feet above             quantity
     the ground. Four different helicopters were made
     and tested for each design. Some summary statis-                     1             1
                                                                             µ1 + µ3 - µ2 + µ4
                                                                          2             2

                                                            is a measure of the effect of changing from 2
                                                            in. wings to 4 in. wings. Give 95% two-sided
                                                            confidence limits for this quantity.
  8q q q q q q q q q q q q q q q q q q q q q q q q q

               Inference for Full

               and Fractional

               Factorial Studies

               Chapter 7 began this book's exposition of inference methods for multisample

                                  studies. The methods there neither require nor make use of any special structure
                                  relating the samples. They are both widely applicable and practically informative
                                  tools. But Chapter 4 illustrated on an informal or descriptive level the engineering
                                  importance of discovering, interpreting, and ultimately exploiting structure relating
                                  a response to one or more other variables. This chapter begins to provide inference
                                  methods to support these activities.

                                       This chapter builds on the descriptive statistics material of Section 4.3 and
                                  the tools of Chapter 7 to provide methods for full and fractional factorial studies. It
                                  begins with a discussion of some inference methods for complete two-way factorials.
                                  Then complete p-way factorial inference is considered with special attention to the
                                  2p case. Then two successive sections describe what is possible in the way of
                                  factorial inference from well-chosen fractions of a 2p factorial. First, half fractions
                                  are considered, and then 1/2q fractions for q > 1.

                                                qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

         8.1 Basic Inference in Two-Way Factorials
                with Some Replication

                                  This section considers inference from complete two-way factorial data in cases
                                  where there is some replication--i.e., at least one of the sample sizes is larger than
                                  1. It begins by pointing out that the material in Sections 7.1 through 7.4 can often
                                  be useful in sharpening the preliminary graphical analyses suggested in Section 4.3.
                                  Then there is a discussion of inference based on the fitted two-way factorial effects
                                  defined in Chapter 4. These are used to develop both individual and simultaneous
                                  confidence interval methods.

546
                       8.1 Basic Inference in Two-Way Factorials with Some Replication 547

8.1.1 One-Way Methods in Two-Way Factorials
          Example 1 revives a case used extensively in Section 4.3.

              Example 1  Joint Strengths for Three Different Joint Types in Three Different Woods
(Example 7, Chapter 4,
                         Consider again the wood joint strength study of Kotlers, MacFarland, and Tom-
  revisited--page 163)   linson. Table 8.1 reorganizes the data given earlier in Table 4.11 into a 3 × 3 table
                         showing the nine different samples of one or two joint strengths for all combina-
                         tions of three woods and three joint types. The data in Table 8.1 have complete
                         two-way factorial structure, and seven of the nine combinations represented in
                         the table provide some replication.

                         Table 8.1
                         Joint Strengths for 32 Combinations of Joint Type and Wood

                                             Wood

                                   1 (Pine)  2 (Oak)  3 (Walnut)

                         1 (Butt)  829, 596 1169      1263, 1029

                         Joint 2 (Beveled) 1348, 1207 1518, 1927 2571, 2443

                         3 (Lap)   1000, 859 1295, 1561 1489

      Error bars              The data in Table 8.1 constitute r = 9 samples of sizes 1 or 2. Provided the
on interaction           graphical and numerical checks of Section 7.1 reveal no obvious problems with the
                         one-way model for joint strengths, all of the methods of Sections 7.2 through 7.4
             plots       can be brought to bear.

                              One way in which this is particularly helpful is in indicating the precision of
                         estimated means on interaction plots. Section 4.3 discussed how near-parallelism
                         on such plots leads to simple interpretations of two-way factorials. By marking
                         either individual or simultaneous confidence limits as error bars around the sample
                         means on an interaction plot, it is possible to get a rough idea of the detectability or
                         statistical significance of any apparent lack of parallelism.

Example 1                The place to begin a formal analysis of the wood joint strength data is with
(continued )             consideration of the appropriateness of the one-way (normal distributions with
                         a common variance) model for joint strength. Table 8.2 gives some summary
                         statistics for the data of Table 8.1.

                              Residuals for the joint strength data are obtained by subtracting the sample
                         means in Table 8.2 from the corresponding observations in Table 8.1. In this
                         data set, the sample sizes are so small that the residuals will obviously be highly
                         dependent. Those from samples of size 2 will be plus-and-minus a single number
548 Chapter 8 Inference for Full and Fractional Factorial Studies

Example 1     Table 8.2
(continued )  Sample Means and Standard Deviations for Nine Joint/Wood Combinations

                                                                         Wood

              Joint  1 (Butt)                        1 (Pine)         2 (Oak)          3 (Walnut)
                     2 (Beveled)
                     3 (Lap)                         y¯ 11 = 712.5    y¯ 12 = 1,169    y¯ 13 = 1,146
                                                     s11 = 164.8                       s13 = 165.5
                                                                      y¯ 22 = 1,722.5
                                                     y¯ 21 = 1,277.5  s22 = 289.2      y¯ 23 = 2,507
                                                     s21 = 99.7       y¯ 32 = 1,428    s23 = 90.5
                                                                      s32 = 188.1
                                                     y¯ 31 = 929.5                     y¯ 33 = 1489
                                                     s31 = 99.7

              corresponding to that sample. Those from samples of size 1 will be zero. So there
              is reason to expect residual plots to show some effects of this dependence. Figure
              8.1 is a normal plot of the 16 residuals, and its complete symmetry (with respect
              to the positive and negative residuals) is caused by this dependence.

                   Of course, the sample standard deviations in Table 8.2 vary somewhat, but
              the ratio between the largest and smallest (a factor of about 3) is in no way
              surprising based on these sample sizes of 2. (Even if only 2 rather than 7 sample
              variances were involved, since 9(= 32) is between the .75 and .9 quantiles of the
              F1,1 distribution, the observed level of significance for testing the equality of the
              two underlying variances would exceed .2 = 2(1 - .9).) And Figure 8.2, which
              is a plot of residuals versus sample means, suggests no trend in  as a function
              of mean response, µ.

                   In sum, the very small sample sizes represented in Table 8.1 make definitive
              investigation of the appropriateness of the one-way normal model assumptions

                     Standard normal quantile  2.4

                                               1.2

                                                                      2

                                               0.0                 2

                                                     2

                                               -1.2

                                     -160 -80 0 80 160 240
                                                   Residual quantile (psi)

                     Figure 8.1 Normal plot of 16 residuals for the wood joint
                     strength study
8.1 Basic Inference in Two-Way Factorials with Some Replication 549

Residual (psi)    300

                  150

                     0

                 -150

                          700 1050 1400 1750 2100 2450
                                         Sample mean (psi)

              Figure 8.2 Plot of residuals versus sample means for
              the joint strength study

impossible. But the limited checks that are possible provide no indication of
serious problems with operating under those restrictions.

     Notice that for these data,

            2 (2 - 1)s121 + (2 - 1)s123 + (2 - 1)s221 + · · · + (2 - 1)s322
          sP =

                        (2 - 1) + (2 - 1) + (2 - 1) + · · · + (2 - 1)
              = 1 (164.8)2 + (165.5)2 + · · · + (188.1)2

                 7
              = 28,805 (psi)2

So

                                sP = 28,805 = 169.7 psi

where sP has 7 associated degrees of freedom.
     Then, for example, from formula (7.14) of Section 7.2, individual two-

sided 99% confidence intervals for the combination mean strengths would have
endpoints

                                                           1
                                 y¯ i j ± 3.499(169.7)

                                                           ni j

For the samples of size 1, this is

                                    y¯ i j ± 593.9                  (8.1)

while for the samples of size 2, appropriate endpoints are

                                    y¯ i j ± 419.9                  (8.2)
550 Chapter 8 Inference for Full and Fractional Factorial Studies

Example 1     Figure 8.3 is an interaction plot (like Figure 4.22) enhanced with error bars made
(continued )  using limits (8.1) and (8.2). Notice, by the way, that the Bonferroni inequality puts
              the simultaneous confidence associated with all nine of the indicated intervals at
              a minimum of 91% (.91 = 1 - 9(1 - .99)).

                   The important message carried by Figure 8.3, not already present in Figure
              4.22, is the relatively large imprecision associated with the sample means as esti-
              mates of long-run mean strengths. And that imprecision has implications regard-
              ing the statistical detectability of factorial effects. For example, by moving near
              the extremes on some error bars in Figure 8.3, one might find nine means within
              the indicated intervals such that their connecting line segments would exhibit par-
              allelism. That is, the plot already suggests that the empirical interactions between
              Wood Type and Joint Type seen in these data may not be large enough to distin-
              guish from background noise. Or if they are detectable, they may be only barely so.

                   The issues of whether the empirical differences between woods and between
              joint types are distinguishable from experimental variation are perhaps somewhat
              easier to call. There is consistency in the patterns "Walnut is stronger than oak is
              stronger than pine" and "Beveled is stronger than lap is stronger than butt." This,
              combined with differences at least approaching the size of indicated imprecisions,

              Mean stress at failure  2500                              Beveled
                                      2000
                                      1500                                 Lap
                                      1000                                 Butt

                                      500

                                            Pine                   Oak           Walnut

                                                                   Wood

              Figure 8.3 Interaction plot of mean joint strength with
              error bars based on individual 99% confidence intervals
                                8.1 Basic Inference in Two-Way Factorials with Some Replication 551

                       suggests that firm statements about the main effects of Wood Type and Joint Type
                       are likely possible.

                         The kind of analysis made thus far on the joint strength data is extremely impor-
                    tant and illuminating. Our discussion will proceed to more complicated statistical
                    methods for such problems. But these often amount primarily to a further refinement
                    and quantification of the two-way factorial story already told graphically by a plot
                    like Figure 8.3.

           8.1.2    Two-Way Factorial Notation and Definitions of Effects
                    In order to discuss inference in two-way factorial studies, it is useful to modify the
          Two-way   generic multisample notation used in Chapter 7. Consider combinations of factor A
(triple subscript)  having I levels and factor B having J levels and use the triple subscript notation:

          notation             yi jk = the kth observation in the sample from the ith level of A
                                       and jth plevel of B

                    Then for I · J different samples corresponding to the possible combinations of a
                    level of A with a level of B, let

                    ni j = the number of observations in the sample from the ith level of A
                           and jth level of B

                    Use the notations y¯ i j , y¯ i., and y¯ . j introduced in Section 4.3, and in the obvious way
                    (actually already used in Example 1), let

                    si j = the sample standard deviation of the ni j observations in the sample
                           from the ith level of A and the jth level of B

                    This amounts to adding another subscript to the notation introduced in Chapter 7
                    in order to acknowledge the two-way structure. In Chapter 7, it was most natural
                    to think of r samples as numbered i = 1 to r and laid out in a single row. Here it
                    is appropriate to think of r = I · J samples laid out in the cells of a two-way table
                    like Table 8.1 and named by their row number i and column number j.

                         In addition to using this notation for empirical quantities, it is also useful to

                    modify the notation used in Chapter 7 for model parameters. That is, let

                    µi j = the underlying mean response corresponding to the ith level of A
                           and jth level of B

                    The model assumptions that the I · J samples are roughly describable as independent
                    samples from normal distributions with a common variance  2 can be written as

Two-way model       yi jk = µi j + i jk                                                          (8.3)
        statement
552 Chapter 8 Inference for Full and Fractional Factorial Studies

              where the quantities 111, . . . , 11n , 121, . . . , 12n , . . . , I J1, . . . , I Jn are in-

                                                      2 11 12 I J

              dependent normal (0,  ) random variables. Equation (8.3) is sometimes called the

              two-way (normal) model equation. It is nothing but a rewrite of the basic one-way

              model equation of Chapter 7 in a notation that recognizes the special organization
              of r = I · J samples into rows and columns, as in Table 8.1.

                   The descriptive analysis of two-way factorials in Section 4.3 relied on computing
              row averages y¯ i. and column averages y¯ . j from the sample means y¯ i j . These were
              then used to define fitted factorial effects. Analogous operations performed on the

              underlying or theoretical means µi j lead to appropriate definitions for theoretical
              factorial effects. That is, let

                      1J
              µi. = J µi j

                             j =1

              = the average underlying mean when factor A is at level i

                      1I
              µ. j = I µi j

                             i =1

                  = the average underlying mean when factor B is at level j

              µ.. =  1         µi j

                     IJ  i, j

              = the grand average underlying mean

              Figure 8.4 shows these as row, column, and grand averages of the µi j . (This is the
              theoretical counterpart of Figure 4.21.)

                   Then, following the pattern established in Definitions 5 and 6 in Chapter 4 for

              sample quantities, there are the following two definitions for theoretical quantities.

Definition 1  In a two-way complete factorial study with factors A and B, the main effect
              of factor A at its ith level is

                                                    i = µi. - µ..
              Similarly, the main effect of factor B at its jth level is

                                                   j = µ. j - µ..

                   These main effects are measures of how (theoretical) mean responses change
              from row to row or from column to column in Figure 8.4. The fitted main effects
              of Section 4.3 can be thought of as empirical approximations to them. It is a
              8.1 Basic Inference in Two-Way Factorials with Some Replication 553

                                                         Factor B  Level J
                                   Level 1 Level 2                   µ1J µ1.

                        Level 1 µ11 µ11

                        Level 2 µ 21 µ 22                          µ 2J µ2.

              Factor A

                        Level I µ I1 µ I2                          µIJ µI.

                        µ .1 µ .2                                  µ .J µ..

              Figure 8.4 Underlying cell mean responses and their row,
              column, and grand averages

              consequence of the form of Definition 1 that (like their empirical counterparts) main
              effects of a given factor sum to 0 over levels of that factor. That is, simple algebra
              shows that

                         I                  J

                            i = 0 and          j = 0

                        i =1               j =1

              Next is a definition of theoretical interactions.

Definition 2  In a two-way complete factorial study with factors A and B, the interaction
              of factor A at its ith level and factor B at its jth level is

                                           i j = µi j - (µ.. + i + j )

                   The interactions in a two-way set of underlying means µi j measure lack of
              parallelism on an interaction plot of the parameters µi j . They measure how much
              pattern there is in the theoretical means µi j that is not explainable in terms of
              the factors A and B acting individually. The fitted interactions of Section 4.3 are

              empirical approximations of these theoretical quantities. Small fitted interactions

              abi j indicate small underlying interactions i j and thus make it justifiable to think
              of the two factors A and B as operating separately on the response variable.
554 Chapter 8 Inference for Full and Fractional Factorial Studies

                            Definition 2 has several simple algebraic consequences that are occasionally

                       useful to know. One is that (like fitted interactions) interactions i j sum to 0 over
                       levels of either factor. That is, as defined,

                                      I                            J

                                            i j = i j = 0

                                      i =1                         j =1

                       Another simple consequence is that upon adding (µ.. + i + j ) to both sides of the
                       equation defining i j , one obtains a decomposition of each µi j into a grand mean
                       plus an A main effect plus a B main effect plus an AB interaction:

                                      µi j = µ.. + i + j + i j           (8.4)

                       The identity (8.4) is sometimes combined with the two-way model equation (8.3)
                       to obtain the equivalent model equation

   A second statement                 yi jk = µ.. + i + j + i j + i jk   (8.5)
of the two-way model

                       Here the factorial effects appear explicitly as going into the makeup of the observa-
                       tions. Although there are circumstances where representation (8.5) is essential, in
                       most cases it is best to think of the two-way model assumptions in form (8.3) and
                       just remember that the i , j , and i j are simple functions of the I · J means µi j .

8.1.3 Individual Confidence Intervals for Factorial Effects
          The primary new wrinkles in two-way factorial inference are

                       1. the drawing of inferences concerning the interactions and main effects, with

                       2. the possibility of finding A, B, or A and B "main effects only" models
                           adequate to describe responses, and subsequently using such simplified de-
                           scriptions in making predictions about system behavior.

  Factorial effects    The basis of inference for the i , j , and i j is that they are linear combinations
                       of the means µi j . (That is, for properly chosen "c's," the factorial effects are "L's"
      are L's, fitted  from Section 7.2.) And the fitted effects defined in Chapter 4's Definitions 5 and

         effects are   6 are the corresponding linear combinations of the sample means y¯ i j . (That is, the
corresponding L^'s     fitted factorial effects are the corresponding "L^ 's.")

Example 1              To illustrate that the effects defined in Definitions 1 and 2 are linear combinations
(continued )
                       of the underlying means µi j , consider 1 and 23 in the wood joint strength
                       study. First,

                       1 = µ1. - µ..
8.1 Basic Inference in Two-Way Factorials with Some Replication 555

1                 1
= (µ11 + µ12 + µ13) - (µ11 + µ12 + · · · + µ32 + µ33)
3                 9

2  2     2           1       1        1     1                     1        1
= µ11 + µ12 + µ13 - µ21 - µ22 - µ23 - µ31 - µ32 - µ33
9  9     9           9       9        9     9                     9        9

and a1 is the corresponding linear combination of the y¯ i j . Similarly,

   23 = µ23 - (µ.. + 2 + 3)

   = µ23 - µ.. + (µ2. - µ..) + (µ.3 - µ..)

   = µ23 - µ2. - µ.3 + µ..

         1                         1
   = µ23 - (µ21 + µ22 + µ23) - (µ13 + µ23 + µ33)
         3                         3

         1
      + (µ11 + µ12 + · · · + µ33)

         9

   4        2        2       2        2     1                     1
   = µ23 - µ21 - µ22 - µ13 - µ33 + µ11 + µ12
   9        9        9       9        9     9                     9

      1        1
      + µ31 + µ32
      9        9

and ab23 is the corresponding linear combination of the y¯ i j .

     Once one realizes that the factorial effects are simple linear combinations of

the µi j , it is a small step to recognize that formula (7.20) of Section 7.2 can be
applied to make confidence intervals for them. For example, the question of whether

the lack of parallelism evident in Figure 8.3 is large enough to be statistically

detectable can be approached by looking at confidence intervals for the i j . And
quantitative comparisons between joint types can be based on confidence intervals
for differences between the A main effects, i - i = µi. - µi .. And quantitative
comparisons between woods can be based on differences between the B main effects,
j - j = µ. j - µ. j .

     The only obstacle to applying formula (7.20) of Section 7.2 to do inference for
factorial effects is determining how the " ci2/ni " term appearing in the formula
should look for quantities of interest. In the preceding example, a number of rather

odd-looking coefficients ci j appeared when writing out expressions for 1 and 23
in terms of the basic means µi j . However, it is possible to discover and write
down general formulas for the sum ci j2 /ni j for some important functions of the
factorial effects. Table 8.3 gives the relatively simple formulas for the balanced data
case where all ni j = m. The less pleasant general versions of the formulas are given
in Table 8.4.
556 Chapter 8 Inference for Full and Fractional Factorial Studies

                       Table 8.3
                       Balanced Data Formulas to Use
                       with Limits (8.6)

                       L      L^                                                 ci j2
                                                                              i, j ni j

                         i j    abi j                              (I - 1)(J - 1)
                          i      ai                                      mIJ
                       i - i  ai - ai
                          j      bj                                      I -1
                       j - j  bj - bj                                    mIJ

                                                                           2
                                                                          mJ

                                                                        J -1
                                                                         mIJ

                                                                           2
                                                                          mI

                            Armed with Tables 8.3 and 8.4, the form of individual confidence intervals for

                       any of the quantities L = i j , i , j , i - i , or j - j is obvious. In the formula
                       for confidence interval endpoints

Confidence limits             L^ ± tsP                                 ci j2             (8.6)
         for a linear                                              i, j ni j

   combination of
two-way factorial

               means

                       1. sP is computed by pooling the I · J sample variances in the usual way
                           (arriving at an estimate with n - r = n - IJ associated degrees of freedom),

                       2. the fitted effects from Section 4.3 are used to find L^ ,

                       3. an appropriate formula from Table 8.3 or 8.4 is chosen to give the quantity
                           under the radical, and

                       4. t from Table B.4 is chosen according to a desired confidence and degrees of
                           freedom  = n - IJ.
                      8.1 Basic Inference in Two-Way Factorials with Some Replication 557

Table 8.4
General Formulas to use with Limits (8.6)

L    L^                                                              ci j2
                                                                  i, j ni j

                                                                                                                   

i j  abi j      1 2  (I - 1)2(J - 1)2 + (I - 1)2                  1 + (J - 1)2            1+  1
                IJ                         ni j         j =j ni j                      i =i ni j i =i, j = j ni j

                                                                                       

i    ai                                          1 2  (I - 1)2    1          +         1

                                                 IJ j ni j i =i, j ni j

                                                                                     

i - i ai - ai                                    1 1 + 1
                                                 J 2 j nij                   j ni j

                                                                                       

j    bj                                          1 2(J - 1)    2  1          +         1

                                                 IJ i ni j i, j = j ni j

j - j bj - bj                                    1 1+ 1
                                                 I 2 i nij                   i nij

     Example 2      A Synthetic 3 × 3 Balanced Data Example

                    To illustrate how easy it is to do inference for factorial effects when complete
                    two-way factorial data are balanced, consider a 3 × 3 factorial with m = 2 obser-
                    vations per cell. (This is the way that the wood joint strength study of Example
                    1 was planned. It was only circumstances beyond the control of the students
                    that conspired to produce the unbalanced data of Table 8.1 through the loss
                    of two specimens.) In this hypothetical situation, sP has degrees of freedom
                     = n - IJ = mIJ - IJ = 2 · 3 · 3 - 3 · 3 = 9. Definitions 5 and 6 in Chapter 4
                    show how to compute fitted main effects ai and bj and fitted interactions abi j .

                         To, for example, make a confidence interval for an interaction i j , consult
                    the first row of Table 8.3 and compute

                                           ci2j = (I - 1)(J - 1) = 2 · 2 = 2 and              ci j2
                      i, j ni j                  mIJ              2·3·3 9                          = .4714

                                                                                          i, j ni j
558 Chapter 8 Inference for Full and Fractional Factorial Studies

Example 2     Then choosing t (as a quantile of the t9 distribution) to produce the desired
(continued )  confidence level, equation (8.6) shows appropriate confidence limits to be

                                                   abi j ± tsP(.4714)

                   As a second example of this methodology, consider the estimation of the
              difference in two factor B main effects, L = j - j = µ. j - µ. j . Consulting
              the last row of Table 8.3,

                            ci2j = 2 = 2 = 1 and                       ci j2
                        i, j nij m I 2 · 3 3                                = .5774

                                                                   i, j ni j

              Then again choosing t to produce the desired confidence level, equation (8.6)
              shows appropriate confidence limits to be

                        bj - bj ± tsP(.5774)

              that is,

                        y¯ . j - y¯ . j ± tsP(.5774)

Example 1     Consider making formal inferences for the factorial effects in the (unbalanced)
(continued )  wood joint strength. Suppose that inferences are to be phrased in terms of two-
              sided 99% individual confidence intervals and begin by considering the interac-
              tions i j .

                   Despite the students' best efforts to the contrary, the sample sizes in Table
              8.1 are not all the same. So one is forced to use formulas in Table 8.4 instead of
              the simpler ones in Table 8.3. Table 8.5 collects the sums of reciprocal sample
              sizes appearing in the first row of Table 8.4 for each of the nine combinations of
              i = 1, 2, 3 and j = 1, 2, 3.

                   For example, for the combination i = 1 and j = 1,

                                 1 = 1 = .5
                                n11 2

                                 1 + 1 = 1 + 1 = 1.5
                                n12 n13 1 2

                                 1 + 1 = 1 + 1 = 1.0
                                n21 n31 2 2

                                 1 + 1 + 1 + 1 = 1 + 1 + 1 + 1 = 2.5
                                n22 n23 n32 n33 2 2 2 1
8.1 Basic Inference in Two-Way Factorials with Some Replication 559

Table 8.5
Sums of Reciprocal Sample Sizes Needed in Making
Confidence Intervals for Joint/Wood Interactions

            ij  1    1              1                1
                              i =i ni j  i =i, j = j ni j
                nij j =j nij

            1 1 .5   1.5      1.0          2.5

            1 2 1.0  1.0      1.0          2.5

            1 3 .5   1.5      1.5          2.0

            2 1 .5   1.0      1.0          3.0

            2 2 .5   1.0      1.5          2.5

            2 3 .5   1.0      1.5          2.5

            3 1 .5   1.5      1.0          2.5

            3 2 .5   1.5      1.5          2.0

            3 3 1.0  1.0      1.0          2.5

     The entries in Table 8.5 lead to values for ci j2 /ni j via the formula on the
first row of Table 8.4. Then, since (from before) sP = 169.7 psi with 7 associated
degrees of freedom, and since the .995 quantile of the t7 distribution is 3.499,
it is possible to calculate the plus-or-minus part of formula (8.6) in order to get

two-sided 99% confidence intervals for the i j . In addition, remember that all
nine fitted interactions were calculated in Section 4.3 and collected in Table 4.14
(page 170). Table 8.6 gives the ci j2 /ni j values, the fitted interactions abi j ,

and the plus-or-minus part of two-sided 99% individual confidence intervals for

the interactions i j .
     To illustrate the calculations summarized in the third column of Table 8.6,

consider the combination with i = 1 (butt joints) and j = 1 (pine wood). Since
I = 3 and J = 3, the first row of Table 8.4 shows that for L = 11

ci j2 =          12  22 · 22 + 22(1.5) + 22(1.0) + 2.5     = .2531
ni j            3·3     2

from which

                     ci2j = .2531 = .5031
                     ni j

     Consider the practical implications of the calculations summarized in Table

8.6. All but one of the intervals centered at an abi j with a half width given in
the last column of the table would cover 0. Only for i = 2 (beveled joints) and
j = 3 (walnut wood) is the magnitude of the fitted interaction big enough to put its
560 Chapter 8 Inference for Full and Fractional Factorial Studies

Example 1     Table 8.6
(continued )  99% Individual Two-Sided Confidence Intervals for Joint
              Type/Wood Type Interactions

              ij           ci j2                                   ci j2
                                   abi j (psi) tsP                     (psi)

                           ni j                                    ni j

              11  .5031      105.83                                298.7
              12  .5720       95.67                                339.6
              13  .5212    -201.5                                  309.5
              21  .4843    -155.67                                 287.6
              22  .5031    -177.33                                 298.7
              23  .5031      333.0                                 298.7
              31  .5031       49.83                                298.7
              32  .5212       81.67                                309.5
              33  .5720    -131.5                                  339.6

              associated confidence interval entirely to one side of 0. That is, most of the lack of
              parallelism seen in Figure 8.3 is potentially attributable to experimental variation.
              But that associated with beveled joints and walnut wood can be differentiated
              from background noise. This suggests that if mean joint strength differences on
              the order of 333 ± 299 psi are of engineering importance, it is not adequate to
              think of the factors Joint Type and Wood Type as operating separately on joint
              strength across all three levels of each factor. On the other hand, if attention was
              restricted to either butt and lap joints or to pine and oak woods, a "no detectable
              interactions" description of joint strength would perhaps be tenable.

                   To illustrate the use of formula (8.6) in making inferences about main effects
              on joint strength, consider comparing joint strengths for pine and oak woods.
              The rather extended analysis of interactions here and the character of Figure 8.3
              suggest that the strength profiles of pine and oak across the three joint types are
              comparable. If this is so, estimation of 1 - 2 = µ.1 - µ.2 amounts to more than
              the estimation of the difference in average (across joint types) mean strengths
              of pine and oak joints (pine minus oak). 1 - 2 is also the difference in mean
              strengths of pine and oak joints for any of the three joint types individually. It is
              thus a quantity of real interest.

                   Once again, since the data in Table 8.1 are not balanced, it is necessary to
              use the more complicated formula in Table 8.4 rather than the formula in Table
              8.3 in making a confidence interval for 1 - 2. For L = 1 - 2, the last row
              of Table 8.4 gives

                  ci j2 1  1+1+1 + 1+1+1                                  = .3889
                       =2  222                                     122

              i, j ni j 3
            8.1 Basic Inference in Two-Way Factorials with Some Replication 561

   So, since from the fitted effects in Section 4.3

                           b1 = -402.5 psi and b2 = 64.17 psi

   formula (8.6) shows that endpoints of a two-sided 99% confidence interval for
   L = 1 - 2 are

                                                                    
                          (-402.5 - 64.17) ± 3.499(169.7) .3889

   that is,

                                        -466.67 ± 370.29

   that is,

                               -836.96 psi and - 96.38 psi

   This analysis establishes that the oak joints are on average from 96 psi to 837 psi
   stronger than comparable pine joints. This may seem a rather weak conclusion,
   given the apparent strong increase in sample mean strengths as one moves from
   pine to oak in Figure 8.3. But it is as strong a statement as is justified in the light of
   the large confidence requirement (99%) and the substantial imprecision in the stu-
   dents' data (related to the small sample sizes and a large pooled standard deviation,
   sP = 169.7 psi). If ±370 psi precision for comparing pine and oak joint strength is
   not adequate for engineering purposes and large confidence is still desired, these
   calculations point to the need for more data in order to sharpen that comparison.

     The computational unpleasantness of the previous discussion results from the
fact that the data of Kotlers, MacFarland, and Tomlinson are unbalanced. Example 2
illustrated that with balanced data, "by hand" calculation is simple. Most statistical
packages have routines that will eliminate the need for a user to grind through the
most tedious of the computations just illustrated. Printout 1 is a MINITAB General
Linear Model output for the wood strength study (which is part of Printout 6 of
Chapter 4). The "Coef" values in that printout are (again) the fitted effects of
Definitions 5 and 6 in Chapter 4. The "StDev" values are the quantities

                                                     ci j2
                                          i, j sP ni j

from formula (8.6) needed to make confidence limits for main effects and inter-
actions. (The MINITAB printout lists this information for only (I - 1) factor A
main effects, (J - 1) factor B main effects, and (I - 1)(J - 1) A×B interactions.
Renaming levels of the factors to change their alphabetical order will produce
562 Chapter 8 Inference for Full and Fractional Factorial Studies

WWW                      a different printout giving this information for the remaining main effects and
                         interactions.)

                         Printout 1 Estimated Standard Deviations
                                          of Joint Strength Fitted Effects (Example 1 )

                         General Linear Model

                         Factor  Type Levels Values
                         joint
                         wood    fixed   3 beveled butt lap

                                 fixed   3         oak pine walnut

                         Analysis of Variance for strength, using Adjusted SS for Tests

                         Source  DF       Seq SS      Adj SS        Adj MS           F        P
                                         2153879     1881650        940825     32.67    0.000
                         joint   2       1641095     1481377        740689     25.72    0.001
                                                                    117102              0.052
                         wood    2        468408      468408                    4.07
                                          201614      201614         28802
                         joint*wood 4    4464996

                         Error   7

                         Total   15

                         Term                Coef  StDev            T       P
                         Constant        1375.67   44.22
                                                              31.11 0.000
                            joint         460.00   59.63
                         beveled         -366.50   63.95       7.71 0.000
                         butt                                 -5.73 0.001
                                            64.17  63.95
                            wood         -402.50   59.63       1.00 0.349
                         oak                                  -6.75 0.000
                         pine            -177.33   85.38
                                         -155.67   82.20      -2.08    0.076
                            joint* wood            97.07      -1.89    0.100
                         beveled oak        95.67  85.38               0.357
                         beveled pine     105.83               0.99    0.255
                         butt oak                              1.24
                         butt pine

                8.1.4    Tukey's Method for Comparing Main Effects (Optional )

 Tukey simultaneous      Formula (8.6) is meant to guarantee individual confidence levels for intervals made
confidence limits for    using it. When interactions in a two-way factorial study are negligible, questions of
                         practical engineering importance can usually be phrased in terms of comparing the
  all differences in A   various A or B main effects. It is then useful to have a method designed specifically
           main effects  to produce a simultaneous confidence level for the comparison of all pairs of A
                         or B main effects. Tukey's method (discussed in Section 7.3) can be modified to
                         produce simultaneous confidence intervals for all differences in i 's or in j 's. That
                         is, two-sided simultaneous confidence intervals for all possible differences in A
                         main effects i - i = µi. - µi . can be made using endpoints

                                                        q 1                 1+ 1                 (8.7)
                                         y¯ i. - y¯ i . ±  sP          j nij   j ni j

                                                          2J
                                   8.1 Basic Inference in Two-Way Factorials with Some Replication 563

                              where q is taken from Tables B.9 using  = n - IJ degrees of freedom, number
                              of means to be compared I , and the .95 or .99 quantile figure (depending whether
                              95% or 99% simultaneous confidence is desired). Expression (8.7) amounts to the
                              specialization of formula (8.6) to L = i - i with t replaced by q/ 2. When all
                              ni j = m, formula (8.7) simplifies to

     Balanced data Tukey                          qs                    (8.8)
simultaneous confidence            y¯ i. - y¯ i . ±  P

 limits for all differences                         Jm
           in A main effects

                                   Corresponding to formulas (8.7) and (8.8) are formulas for simultaneous two-
                              sided confidence limits for all possible differences in B main effects j - j =
                              µ. j - µ. j --namely,

 Tukey simultaneous                               q 1     1+ 1          (8.9)
confidence limits for              y¯ . j - y¯ . j ±  sP  i nij  i nij

  all differences in B                              2I
           main effects

                              and

     Balanced data Tukey                          qs                    (8.10)
simultaneous confidence            y¯ . j - y¯ . j ±  P

 limits for all differences                          Im
           in B main effects

                              where q is taken from Tables B.9 using  = n - IJ degrees of freedom and number
                              of means to be compared J .

Example 3                     A 3 × 2 Factorial Study of Ultimate Tensile
                              Strength for Drilled Aluminum Strips

                              Clubb and Goedken studied the effects on tensile strength of holes drilled in
                              6 in.-by-2 in. 2024-T3 aluminum strips .0525 in. thick. A hole of diameter
                              .149 in., .185 in., or .221 in. was centered either .5 in. or 1.0 in. from the edge
                              (and 3.0 in. from each end) of 18 strips. Ultimate axial stress was then measured
                              for each on an MTS machine. m = 3 tests were made for each of the 3 × 2
                              combinations of hole size and placement. Mean tensile strengths (in pounds)
                              obtained in the study are given in Table 8.7. Some plotting with the original data
                              (not given here) shows that (except for some hint that hole size 3 strengths were
                              less variable than the others) the one-way normal model assumptions provide a
                              plausible description of tensile strength. We will proceed to use the assumptions
                              (8.3) in what follows.
564 Chapter 8 Inference for Full and Fractional Factorial Studies

Example 3     Table 8.7
(continued )  Sample Means for 3 × 2 Size/Placement Combinations

                                                                   B Placement

              A Size  1 (.149 in.)  1 (.5 in. from Edge)           2 (1.0 in. from Edge)
                      2 (.185 in.)
                      3 (.221 in.)  y¯ 11 = 5635.3 lb              y¯ 12 = 5730.3 lb
                                    y¯ 21 = 5501.0 lb              y¯ 22 = 5638.0 lb
                                    y¯ 31 = 5456.3 lb              y¯ 32 = 5602.7 lb

              Pooling the 3 · 2 = 6 sample variances in the usual way produced

                                                      sP = 106.7 lb

              with  = mIJ- IJ = 3 · 3 · 2 - 3 · 2 = 12 associated degrees of freedom. Then
              consider summarizing the experimental results graphically. Notice that the P-R
              method for making simultaneous two-sided 95% confidence intervals for r = 6
              means based on  = 12 degrees of freedom is (from formula (7.28) of Section
              7.3) to use endpoints

                                                                  106.7
                                                   y¯ i j ± 3.095 

                                                                      3

              for estimating each µi j . (k2 = 3.095 was obtained from Table B.8A.) This is
              approximately

                                    y¯ i j ± 191

              Figure 8.5 is an interaction plot of the 3 × 2 = 6 sample mean tensile strengths
              enhanced with ±191 lb error bars.

                   The lack of parallelism in Figure 8.5 is fairly small, both compared to the
              absolute size of the strengths being measured and also relative to the kind of
              uncertainty about the individual mean strengths indicated by the error bars.
              Letting factor A be size and factor B be placement, it is straightforward to
              use the methods of Section 4.3 to calculate

                        a1 = 88.9                                    b1 = -63.1
                        a2 = -24.4                                   b2 = 63.1
                        a3 = -64.4
                      ab11 = 15.6                                  ab12 = -15.6
                      ab21 = -5.4                                  ab22 = 5.4
                      ab31 = -10.1                                 ab32 = 10.1
          8.1 Basic Inference in Two-Way Factorials with Some Replication 565

                                      5900

          Mean tensile strength (lb)  5800     9 in. Diameter
                                      5700  .14
                                      5600
                                      5500     5 in. Diameter
                                      5400  .18 iameter

                                                  in. D
                                            .221

                                      5300

                                                     1                    2
                                            (.5 in. from edge)  (1.0 in. from edge)

                                            Hole placement

          Figure 8.5 Interaction plot of aluminum strip
          sample means, enhanced with error bars based
          on 95% simultaneous confidence intervals

Then, since the data are balanced, one may use the formulas of Table 8.3 together

with formula (8.6). So individual confidence intervals for the interactions i j
are of the form

                                      abi j ± t (106.7) (3 - 1)(2 - 1) 3 · 3 · 2

that is,

                                            abi j ± t (35.6)

Clearly, for any sensible confidence level (producing t of at least 1), such intervals
all cover 0. This confirms the lack of statistical detectability of the interactions
already represented in Figure 8.5.

     It thus seems sensible to proceed to consideration of the main effects in this
tensile strength study. To illustrate the application of Tukey's method to factorial
main effects, consider first simultaneous 95% two-sided confidence intervals
for the three differences 1 - 2, 1 - 3, and 2 - 3. Applying formula (8.8)
566 Chapter 8 Inference for Full and Fractional Factorial Studies

Example 3     with  = 12 degrees of freedom and I = 3 means to be compared, Table B.9A
(continued )  indicates that intervals with endpoints

                                      (3.77)(106.7)
                        y¯ i. - y¯ i . ± 

                                             2·3

              that is,

                        y¯ i. - y¯ i . ± 164 lb

              are in order. No difference between the ai 's exceeds 164 lb. That is, if simultaneous
              95% confidence is desired in the comparison of the hole size main effects, one
              must judge the students' data to be interesting--perhaps even suggestive of
              a decrease in strength with increased diameter--but nevertheless statistically
              inconclusive. To really pin down the impact of hole size on tensile strength,
              larger samples are needed.

                   To see that the Clubb and Goedken data do tell at least some story in a
              reasonably conclusive manner, finally consider the use of the last row of Table
              8.3 with formula (8.6) to make a two-sided 95% confidence interval for 2 - 1,
              the difference in mean strengths for strips with centered holes as compared to
              ones with holes .5 in. from the strip edge. The desired interval has endpoints

                                            2
                        b2 - b1 ± tsP

                                           mI

              that is,

                                      63.1 - (-63.1) ± 2.179(106.7) 2
                                                                                3(3)

              that is,
                                                     126.2 ± 109.6

              that is,

                                                16.6 lb and 235.8 lb

              Thus, although the students' data don't provide much precision, they are adequate
              to establish clearly the existence of some decrease in tensile strength as a hole is
              moved from the center of the strip towards its edge.

                   Formulas (8.7) through (8.10) are, mathematically speaking, perfectly valid
              providing only that the basic "equal variances, underlying normal distributions"
              model is a reasonable description of an engineering application. (Under the basic
                      8.1 Basic Inference in Two-Way Factorials with Some Replication 567

What if interactions  model (8.3), formulas (8.7) and (8.9) provide an actual simultaneous confidence at
 are not negligible?  least as big as the nominal one, and when all ni j = m, formulas (8.8) and (8.10)
                      provide actual simultaneous confidence equal to the nominal one.) But in practical

                      terms, the inferences they provide (and indeed the ones provided by formula (8.6) for
                      individual differences in main effects) are not of much interest unless the interactions

                      i j have been judged to be negligible.
                           Nonnegligible interactions constitute a warning that the patterns of change in

                      mean response, as one moves between levels of one factor, (say, B) are different

                      for various levels of the other factor (say, A). That is, the pattern in the µi j is not
                      a simple one generally describable in terms of the two factors acting separately.
                      Rather than trying to understand the pattern in terms of main effects, something else

                      must be done.
                           As discussed in Section 4.4, sometimes a transformation can produce a response

                      variable describable in terms of main effects only. At other times, restriction of
                      attention to part of a factorial produces a study (of reduced scope) where it makes

                      sense to think in terms of main effects. (In Example 1, consideration of only butt
                      and lap joints gives an arena where "negligible interactions" may be a sensible

                      description of joint strength.) Or it may be most natural to mentally separate an
                      I × J factorial into I (J ) different J (I ) level studies on the effects of factor B(A) at
                      different levels of A(B). (The 3 × 3 wood joint strength study in Example 1 might
                      be thought of as three different studies, one for each joint type, of the effects of wood

                      type on strength.) Or if none of these approaches to analyzing two-way factorial data
                      with important interactions is attractive, it is always possible to ignore the two-way
                      structure completely and treat the I · J samples as arising from simply r = I · J
                      unstructured different conditions.

Section 1 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. The accompanying table shows part of the data of                using the P-R method of making 95% simul-
   Dimond and Dix, referred to in Examples 6 (Chap-                taneous two-sided confidence intervals. (Plot
   ter 1) and 9 (Chapter 3). The values are the shear              mean strength versus glue type.)
   strengths (in lb) for m = 3 tests on joints of various     (b) Compute the fitted main effects and interac-
   combinations of Wood Type and Glue Type.                        tions from the six combination sample means.
                                                                   Use these to make individual 95% confidence
Wood  Glue            Joint Shear Strengths                        intervals for all of the main effects and inter-
                                                                   actions in this 2 × 3 factorial study. What do
pine  white           130, 127, 138                                these indicate about the detectability of the var-
pine  carpenter's     195, 194, 189                                ious effects?
pine  cascamite       195, 202, 207                            (c) Use Tukey's method for simultaneous com-
fir   white           95, 119, 62                                  parison of main effects and give simultaneous
fir   carpenter's     137, 157, 145                                95% confidence intervals for all differences in
fir   cascamite       152, 163, 155                                Glue Type main effects.

(a) Make an interaction plot of the six combination        2. B. Choi conducted a replicated full factorial study
    means and enhance it with error bars derived              of the stopping properties of various types of bi-
                                                              cycle tires on various riding surfaces. Three dif-
                                                              ferent Types of Tires were used on the bike, and
568 Chapter 8 Inference for Full and Fractional Factorial Studies

three different Pavement Conditions were used. For    (d) Compute all of the fitted factorial effects for
each Tire Type/Pavement Condition combination,             Choi's data. (Find the ai 's, bj 's, and abi j 's de-
m = 6 skid mark lengths were measured. The ac-             fined in Section 4.3.)
companying table shows some summary statistics
for the study. (The units are cm.)                    (e) If one wishes to make individual 95% two-
                                                           sided confidence intervals for the interactions
Smooth       Dry            Wet            Dirt            i j , intervals of the form abi j ± are appro-
Tires     Concrete       Concrete      y¯ 13 = 393.0       priate. Find . Based on this value, are there
                                       s13 = 25.4          statistically detectable interactions here? How
Reverse  y¯ 11 = 359.8  y¯ 12 = 366.5  y¯ 23 = 375.7       does this conclusion compare with your more
Tread    s11 = 19.2     s12 = 26.4     s23 = 39.9          qualitative answer to part (c)?
         y¯ 21 = 343.0  y¯ 22 = 356.7  y¯ 33 = 402.5
Treaded  s21 = 15.5     s22 = 37.4     s33 = 32.8     (f) If one wishes to compare Tire Type main ef-
Tires    y¯ 31 = 384.8  y¯ 32 = 400.8                      fects, confidence intervals for the differences
         s31 = 15.4     s32 = 60.8                         i - i are in order. Find individual 95% two-
                                                           sided confidence intervals for 1 - 2, 1 -
(a) Compute sP for Choi's data set. What is this           3, and 2 - 3. Based on these, are there sta-
     supposed to be measuring?                             tistically detectable differences in Tire Type
                                                           main effects here? How does this conclusion
(b) Make an interaction plot of the sample means           compare with your answer to part (c)?
     similar to Figure 8.3. Use error bars for the
     means calculated from individual 95% two-        (g) Redo part (f), this time using (Tukey) simulta-
     sided confidence limits for the means. (Make          neous 95% two-sided confidence intervals.
     use of your value of sP from (a).)

(c) Based on your plot from (b), which factorial
     effects appear to be distinguishable from back-
     ground noise? (Tire Type main effects? Pave-
     ment Condition main effects? Tire × Pavement
     interactions?)

         qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

         8.2 p-Factor Studies with Two Levels
                for Each Factor

                             The previous section looked at inference for two-way factorial studies. This sec-
                             tion presents methods of inference for complete p-way factorials, paying primary
                             attention to those cases where each of p factors is represented at only two levels.

                                  The discussion begins by again pointing out the relevance of the one-way
                             methods of Chapter 7 to structured (in this case, p-way factorial) situations. Next,
                             the p-way factorial normal model, definitions of effects in that model, and basic
                             confidence interval methods for the effects are considered, paying particular attention
                             to the 2p case. Then attention is completely restricted to 2p studies, and a further
                             method for identifying detectable (2p factorial) effects is presented. For balanced 2p
                             studies, there follows a review of the fitting of reduced models via the reverse Yates
                             algorithm and the role of residuals in checking their efficacy. Finally, confidence
                             interval methods based on simplified models in balanced 2p studies are discussed.
                      8.2 p-Factor Studies with Two Levels for Each Factor 569

8.2.1      One-Way Methods in p-Way Factorials

           The place to begin the analysis of p-way factorial data is to recognize that funda-
           mentally one is just working with several samples. Subject to the relevance of the
           model assumptions of Chapter 7, the inference methods of that chapter are available
           for use in analyzing the data.

Example 4  A 23 Factorial Study of Power Requirements in Metal Cutting

           In Fundamental Concepts in the Design of Experiments, C. R. Hicks describes a
           study conducted by Purdue University engineering graduate student L. D. Miller
           on power requirements for cutting malleable iron using ceramic tooling. Miller
           studied the effects of the three factors

           Factor A   Tool Type         (type 1 or type 2)
           Factor B   Tool Bevel Angle  (15 or 30)
           Factor C   Type of Cut       (continuous or interrupted)

           on the power required to make a cut on a lathe at a particular depth of cut, feed
           rate, and spindle speed. The response variable was the vertical deflection (in
           mm) of the indicator needle on a dynamometer (a measurement proportional to
           the horsepower required to make the particular cut). Miller's data are given in
           Table 8.8.

                The most elementary view possible of the power requirement data in Table
           8.8 is as r = 8 samples of size m = 4. Simple summary statistics for these 23 = 8
           samples are given in Table 8.9.

                To the extent that the one-way normal model is an adequate description of
           this study, the methods of Chapter 7 are available for use in analyzing the data of
           Table 8.8. The reader is encouraged to verify that plotting of residuals (obtained
           by subtracting the y¯ values in Table 8.9 from the corresponding raw data values of

           Table 8.8
           Dynamometer Readings for 23 Treatment Combinations in a Metal Cutting Study

           Tool Type  Bevel Angle  Type of Cut  y, Dynamometer Reading (mm)

                1          15      continuous         29.0, 26.5, 30.5, 27.0
                2          15      continuous         28.0, 28.5, 28.0, 25.0
                1          30      continuous         28.5, 28.5, 30.0, 32.5
                2          30      continuous         29.5, 32.0, 29.0, 28.0
                1          15      interrupted        28.0, 25.0, 26.5, 26.5
                2          15      interrupted        24.5, 25.0, 28.0, 26.0
                1          30      interrupted        27.0, 29.0, 27.5, 27.5
                2          30      interrupted        27.5, 28.0, 27.0, 26.0
570 Chapter 8 Inference for Full and Fractional Factorial Studies

Example 4               Table 8.9
(continued )            Summary Statistics for 23 Samples of Dynamometer Readings in a

                        Metal Cutting Study

                        Tool Type  Bevel Angle  Type of Cut           y¯     s

                             1          15      continuous         28.250  1.848
                             2          15      continuous         27.375  1.601
                             1          30      continuous         29.875  1.887
                             2          30      continuous         29.625  1.702
                             1          15      interrupted        26.500  1.225
                             2          15      interrupted        25.875  1.548
                             1          30      interrupted        27.750  0.866
                             2          30      interrupted        27.125  0.854

              Table 8.8) reveals only one slightly unpleasant feature of the power requirement
              data relative to the potential use of standard methods of inference. When plotted
              against levels of the Type of Cut variable, the residuals for interrupted cuts are
              shown to be on the whole somewhat smaller than those for continuous cuts. (This
              phenomenon is also obvious in retrospect from the sample standard deviations in
              Table 8.9. These are smaller for the second four samples than for the first four.)
              But the disparity in the sizes of the residuals is not huge. So although there may
              be some basis for suspecting improvement in power requirement consistency for
              interrupted cuts as opposed to continuous ones, the tractability of the one-way
              model and the kind of robustness arguments put forth at the end of Section 6.3
              once again suggest that the standard model and methods be used. This is sensible,
              provided the resulting inferences are then treated as approximate and real-world
              "close calls" are not based on them.

                   The pooled sample variance here is

                  2 (4 - 1)(1.848)2 + (4 - 1)(1.601)2 + · · · + (4 - 1)(.854)2
                  sP =                                                                  = 2.226
                                   (4 - 1) + (4 - 1) + · · · + (4 - 1)

              so

                                                     sP = 1.492 mm

              with  = n - r = 32 - 8 = 24 associated degrees of freedom. Then, for exam-
              ple, the P-R method of simultaneous inference from Section 7.3 produces two-
              sided simultaneous 95% confidence intervals for mean dynamometer readings
              with endpoints

                                                  1.492
                                   y¯ i jk ± 2.969 

                                                      4
                                                   8.2 p-Factor Studies with Two Levels for Each Factor 571

                           that is,

                                                                  y¯ i jk ± 2.21 mm

                           (There is enough precision provided by the data to think of the sample means in
                           Table 8.9 as roughly "all good to within 2.21 mm.") And the other methods of
                           Sections 7.1 through 7.4 based on sP might be used as well.

                 8.2.2     p-Way Factorial Notation, Definitions of Effects,
                           and Related Confidence Interval Methods
            Three factor
(quadruple subscript)      Section 8.1 illustrated that standard notation in two-way factorials requires triple sub-
                           scripts for naming observations. In a general p-way factorial, "( p + 1)-subscript"
                 notation  notation is required. As p grows, such notation quickly gets out of hand. As in
                           Section 4.3 (on a descriptive level) the exposition here will explicitly develop only
                           the general factorial notation for p = 3, leaving the reader to infer by analogy how
                           things would have to go for p = 4, 5, etc. (When specializing to the 2p situation
                           later in this section, the special notation introduced in Section 4.3 makes it possible
                           to treat even large- p situations fairly explicitly.)

                                Then for p = 3 factors A, B, and C having (respectively) I , J , and K levels, let

                                     yi jkl = the lth observation in the sample from the ith level of A,
                                              jth level of B, and kth level of C

                           For the I · J · K different samples corresponding to the possible combinations of a
                           level of A with one of B and one of C, let

                           ni jk = the number of observations in the sample from the ith level of A,
                                    jth level of B, and kth level of C

                           y¯ i jk = the sample mean of the ni jk observations in the sample from the
                                   ith level of A, jth level of B, and kth level of C

                           si jk = the sample standard deviation of the ni jk observations in the sample
                                   from the ith level of A, jth level of B, and kth level of C

                           and further continue the dot notations used in Section 4.3 for unweighted averages
                           of the y¯ i jk. In comparison to the notation of Chapter 7, this amounts to adding two
                           subscripts in order to acknowledge the three-way structure in the samples.

                                The use of additional subscripts is helpful not only for naming empirical quan-

                           tities but also for naming theoretical quantities. That is, with

                           µi jk = the underlying mean response corresponding to the
                                    ith level of A, jth level of B, and kth level of C
572 Chapter 8 Inference for Full and Fractional Factorial Studies

Three-way model      the standard one-way normal model assumptions can be rewritten as  (8.11)
          statement                                        yi jkl = µi jk + i jkl

                     where the i jkl terms are iid normal random variables with mean 0 and variance  2.
                     Formula (8.11) could be called the three-way (normal) model equation because it
                     recognizes the special organization of the I · J · K samples according to combina-
                     tions of levels of the three factors. But beyond this, it says no more or less than the
                     one-way model equation from Section 7.1.

                          The initial objects of inference in three-way factorial analyses are linear com-
                     binations of theoretical means µi jk, analogous to the fitted effects of Section 4.3.
                     Thus, it is necessary to carefully define the theoretical or underlying main effects,
                     2-factor interactions, and 3-factor interactions for a three-way factorial study. In the
                     definitions that follow, a dot appearing as a subscript will (as usual) be understood to
                     indicate that an average has been taken over all levels of the factor corresponding to
                     the dotted subscript. Consider first main effects. Parallel to Definition 7 in Chapter 4
                     (page 182) for fitted main effects is a definition of theoretical main effects.

Definition 3         In a three-way complete factorial study with factors A, B, and C, the main
                     effect of factor A at its ith level is

                                                          i = µi.. - µ...
                     the main effect of factor B at its jth level is

                                                          j = µ. j. - µ...
                     and the main effect of factor C at its kth level is

                                                          k = µ..k - µ...

                          These main effects measure how (when averaged over all combinations of levels
                     of the other factors) underlying mean responses change from level to level of the
                     factor in question. Definition 3 has the algebraic consequences that

                     I             J                               K

                           i = 0,        j = 0, and                     k = 0

                     i =1          j =1                            k=1

                          The theoretical counterpart of Definition 8 in Chapter 4 is a definition of theo-
                     retical 2-factor interactions.
                         8.2 p-Factor Studies with Two Levels for Each Factor 573

Definition 4  In a three-way complete factorial study with factors A, B, and C, the 2-factor
              interaction of factor A at its ith level and factor B at its jth level is

                                          i j = µi j. - (µ... + i + j )
              the 2-factor interaction of A at its ith level and C at its kth level is

                                          ik = µi.k - (µ... + i + k )
              and the 2-factor interaction of B at its jth level and C at its kth level is

                                          jk = µ. jk - (µ... + j + k )

                   Like their empirical counterparts defined in Section 4.3, the 2-factor interactions
              in a three-way study are measures of lack of parallelism on two-way plots of means
              obtained by averaging out over levels of the "other" factor. And it is an algebraic
              consequence of the form of Definition 4 that

                   I     J                    I   K

                         i j = i j = 0,           ik = ik = 0

                   i =1  j =1               i =1  k=1

              and

                               J         K

                                  jk = jk = 0

                            j =1         k=1

                   Finally, there is the matter of three-way interactions in a three-way factorial
              study. Direct analogy with the meaning of fitted three-way interactions given as
              Definition 9 in Chapter 4 (page 183) gives the following:

Definition 5  In a three-way complete factorial study with factors A, B, and C, the 3-factor
              interaction of factor A at its ith level, factor B at its jth level, and factor C
              at its kth level is

                       i jk = µi jk - (µ... + i + j + k + i j + ik + jk )

              Like their fitted counterparts, the (theoretical) 3-factor interactions are measures of

              patterns in the µi jk not describable in terms of the factors acting separately or in pairs.
              Or differently put, they measure how much what one would call the AB interactions

              at a single level of C change from level to level of C. And, like the fitted 3-factor
574 Chapter 8 Inference for Full and Fractional Factorial Studies

                       interactions defined in Section 4.3, the theoretical 3-factor interactions defined here
                       sum to 0 over levels of any one of the factors. That is,

                                         I            J                    K

                                            i jk = i jk = i jk = 0

                                     i =1            j =1                 k=1

  Factorial effects         The fundamental fact that makes inference for the factorial effects defined in
                       Definitions 3, 4, and 5 possible is that they are particular linear combinations of the
      are L's, fitted  means µi jk (L's from Section 7.2). And the fitted effects from Section 4.3 are the
                       corresponding linear combinations of the sample means y¯ i jk (L^ 's from Section 7.2).
         effects are   So at least in theory, to make confidence intervals for the factorial effects, one needs
corresponding L^'s     only to figure out exactly what coefficients are applied to each of the means and use
                       formula (7.20) of Section 7.2.

Example 5              Finding Coefficients on Means for a Factorial
                       Effect in a Three-Way Factorial

                       Consider a hypothetical example in which A appears at I = 2 levels, B at J = 2
                       levels, and C at K = 3 levels. For the sake of illustration, consider how you
                       would make a confidence interval for 23. By Definitions 3 and 4,

                       23 = µ2.3 - (µ... + 2 + 3)

                       = µ2.3 - (µ2.. + µ..3 - µ...)

                       1                          1
                       = (µ213 + µ223) - (µ211 + µ221 + µ212 + µ222 + µ213 + µ223)
                       2                          6

                                  1                                           1
                       - (µ113 + µ213 + µ123 + µ223) + (µ111 + µ211 + · · · + µ223)
                                  4                                           12

                       1                    1     1                1              1       1
                       = µ213 + µ223 - µ211 - µ221 - µ212 - µ222
                       6                    6     12               12             12      12

                                  1            1      1                1              1       1
                       - µ113 - µ123 + µ111 + µ121 + µ112 + µ122
                                  6            6      12               12             12      12

                       so the " ci2/ni " applicable to estimating 23 via formula (7.20) of Section 7.2 is

                       ci jk 2 =  12 1 + 1 + 1 + 1
                       ni jk      6         n213 n223 n113 n123

                                  + 1 2 1+1+1+1+1+1+1+1
                                     12        n211 n221 n212 n222 n111 n121 n112 n122
                                                         8.2 p-Factor Studies with Two Levels for Each Factor 575
                                 and using this expression, endpoints for a confidence interval for 23 are

                                                                                         ci jk 2
                                                                    ac23 ± t sP

                                                                                         ni jk

                                      It is possible to work out (unpleasant) general formulas for the " ci2/ni " terms
                                 for factorial effects in arbitrary p-way factorials and implement them in computer

                                 software. It is not consistent with the purposes of this book to lay those out here.
                                 However, in the special case of 2p factorials, there is no difficulty in describing

                                 how to make confidence intervals for the effects or in carrying out a fairly complete
                                 analysis of all of these "by hand" for p as large as even 4 or 5. This is because the 2p

                                 case of the general p-way factorial structure allows three important simplifications.

Coefficients applied             First, for any factorial effect in a 2p factorial, the coefficients "ci " applied to the

to means to produce              means  to  produce  the  effect  are  all  ±  1   .  So  the  "  ci2/ni " term needed to make a
   2p factorial effects                                                        2p
                                 confidence interval for any effect in a 2p factorial is

are  all  ±  1
             2p
                                                             12
                                                          ±p           1 + 1 + 1 + 1 +···
                                                          2            n(1) na nb nab

                                 where the subscripts (1), a, b, ab, etc. refer to the combination-naming convention
                                 for 2p factorials introduced in Section 4.3.

                                      So let E stand for a generic effect in a 2p factorial (a particular kind of L from
                                 Section 7.2) and E^ be the corresponding fitted effect (the corresponding L^ from

                                 Section 7.2). Then endpoints of an individual two-sided confidence interval for E

                                 are

Individual confidence                                E^ ± tsP p1        1 + 1 + 1 + 1 +···        (8.12)
                                                              2        n(1) na nb nab
    limits for an effect
        in a 2p factorial

                                 where the associated confidence is the probability that the t distribution with
                                  = n - r = n - 2p degrees of freedom assigns to the interval between -t and
                                 t. The usual device of using only one endpoint from formula (8.12) and halving
                                 the unconfidence produces a one-sided confidence interval for the effect. And in

                                 balanced-data situations where all sample sizes are equal to m, formula (8.12) can
                                 be written even more simply as

Balanced data confidence                                                    E^ ± t sP             (8.13)
                                                                                     m2p
          limits for an effect
              in a 2p factorial

Estimating one 2p effect         There is a second simplification of the general p-way factorial situation afforded

of a given type is enough in the 2p case. Because of the way factorial effects sum to 0 over levels of any factor
576 Chapter 8 Inference for Full and Fractional Factorial Studies

                                  involved, estimating one effect of each type is sufficient to completely describe a 2p
                                  factorial. For example, since in a 2p factorial,

                                                                 11 = -21 = -12 = 22

                                  it is necessary to estimate only one AB interaction to have detailed what is known
                                  about 2-factor interactions of A and B. There is no need to labor in finding separate
                                  estimates of 11, 12, 21, and 22. Appropriate sign changes on an estimate of
                                  22 suffice to cover the matter.

                                       The third important fact making analysis of 2p factorial effects so tractable is
                                  the existence of the Yates algorithm. As demonstrated in Example 9 of Chapter 4, it
                                  is really quite simple to use the algorithm to mechanically generate one fitted effect
                                  of each type for a given 2p data set: those effects corresponding to the high levels
                                  of all factors.

Example 4     Consider again the metal working power requirement study. Agreeing to (arbi-
(continued )  trarily) name tool type 2, the 30 tool bevel angle, and the interrupted cut type as
              the "high" levels of (respectively) factors A, B, and C, the eight combinations of
              the three factors are listed in Table 8.9 in Yates standard order. Taking the sample
              means from that table in the order listed, the Yates algorithm can be applied to
              produce the fitted effects for the high levels of all factors, as in Table 8.10.

                   Recall that for the data of Table 8.8, m = 4 and sP = 1.492 mm with 24(=
              32 - 23) associated degrees of freedom. So one has (from formula (8.13)) that
              for (say) individual 90% confidence, the factorial effects in this example can be
              estimated with two-sided intervals having endpoints

                               ^         1.492
                               E ± 1.711 
                                           4 · 23

              Table 8.10
              The Yates Algorithm Applied to the Means in Table 8.9

              Combination  y¯  Cycle 1 Cycle 2 Cycle 3                        Cycle 3 ÷ 8

              (1)          28.250 55.625 115.125 222.375                   27.7969 = y¯ ...
                                                                           -.2969 = a2
              a            27.375 59.500 107.250 -2.375
                                                                              .7969 = b2
              b            29.875 52.375 -1.125 6.375                         .0781 = ab22
                                                                           -.9844 = c2
              ab           29.625 54.875 -1.250                      .625  -.0156 = ac22
                                                                           -.1719 = bc22
              c            26.500 -.875 3.875 -7.875                       -.0781 = abc222

              ac           25.875 -.250 2.500 -.125

              bc           27.750 -.625  .625 -1.375

              abc          27.125 -.625 0.000 -.625
                              8.2 p-Factor Studies with Two Levels for Each Factor 577

                    that is,

                              E^ ± .45

                    Then, comparing the fitted effects in the last column of Table 8.10 to the ±.45
                    value, note that only the main effects of Tool Bevel Angle (factor B) and Type
                    of Cut (factor C) are statistically detectable. And for example, it appears that
                    running the machining process at the high level of factor B (the 30 bevel angle)
                    produces a dynamometer reading that is on average between approximately

 The difference                    2(.80 - .45) = .7 mm and 2(.80 + .45) = 2.5 mm
 between main
 effects at high    higher than when the process is run at the low level of factor B (the 15 bevel
  and low levels    angle). (The difference between B main effects at the high and low levels of B is
                    2 - 1 = 2 - (-2) = 22, hence the multiplication by 2 of the endpoints of
    of a factor is  the confidence interval for 2.)
twice the effect

8.2.3               2p Studies Without Replication
                    and the Normal-Plotting of Fitted Effects

                    The use of formula (8.12) or (8.13) in judging the detectability of 2p factorial effects
                    is an extremely practical and effective method. But it depends for its applicability on
                    there being replication somewhere in the data set. One must have a pooled sample
                    standard deviation sP. Unfortunately, it is not uncommon that poorly informed
                    people do unreplicated 2p factorial studies. Although such studies should be avoided
                    whenever possible, various methods of analysis have been suggested for them. The
                    most popular one follows from a very clever line of reasoning due originally to
                    Cuthbert Daniel.

                         Daniel's idea was to invoke a principle of effect sparsity. He reasoned that in
                    many real engineering systems, the effects of only a relatively few factors are the
                    primary determiners of system mean response. Thus, in terms of the 2p factorial
                    effects used here, a relatively few of 2, 2, 22, 2, 22, . . . , etc., often dominate
                    the rest (are much larger in absolute value than the majority). In turn, this would im-
                    ply that often among the fitted effects a2, b2, ab22, c2, ac22, . . . , etc., there are a few
                    with sizable means, and the others have means that are (relatively speaking) near 0.
                    Daniel's idea for identifying those cases where a few effects dominate the rest was to
                    normal-plot the fitted effects for the "all high treatment" combination (obtained, for
                    example, by use of the Yates algorithm). When a few plot in positions much more ex-
                    treme than would be predicted from putting a line through the majority of the points,
                    they are identified as the likely principal determiners of system behavior. (Actually,
                    Daniel originally proposed making a half normal plot of the absolute values of
                    the fitted effects. This was to eliminate any visual effect of the somewhat arbitrary
                    naming of one level of each factor as the high level. For several reasons, among them
578 Chapter 8 Inference for Full and Fractional Factorial Studies

                                  simplicity, this presentation will use the full normal plot modification of Daniel's
                                  method. The idea of half normal plotting is considered further in Chapter Exercise 9.)

                Example 6  Identifying Detectable Effects in an Unreplicated 24 Factorial Drill
(Example 12, Chapter 4,    Advance Rate Study

    revisited--page 195 )  Section 4.4 discussed an example of an unreplicated 24 factorial experiment taken
                           from Daniel's Applications of Statistics to Industrial Experimentation. There the
                           effects of the four factors

                               Factor A Load

                               Factor B Flow Rate

                               Factor C Rotational Speed

                               Factor D Type of Mud

                           on the logarithm of an advance rate of a small stone drill were considered. (The
                           raw data are in Table 4.24.) The Yates algorithm applied to the 16 = 24 observed
                           log advance rates produced the following fitted effects:

                               y¯ ... = 1.5977                   b2 = .2900 c2 = .5772 d2 = .1633
                                a2 = .0650
                             ab22 = -.0172                            ac22 = .0052            ad22 = .0334
                             bc22 = -.0251
                           abc222 = .0052                             bd22 = -.0075 cd22 = .0491
                           bcd222 = -.0173
                                                                      abd222 = .0261 acd222 = .0266

                                                                 abcd2222 = .0193

                           Figure 8.6 is a normal plot of the 15 fitted effects a2 through abcd2222.

                           Standard normal quantile  2.0

                                                     1.0                                      c2

                                                     0.0              d2 b2

                                                     -1.0

                                                     -2.0

                                                           -.02  .13  .28           .43       .58

                                                                      Fitted effect quantile

                           Figure 8.6 Normal plot of the fitted effects for Daniel's drill
                           advance rate study
                                          8.2 p-Factor Studies with Two Levels for Each Factor 579

                       Applying Daniel's reasoning, it is obvious that the points corresponding to
                  the C, B, and D main effects plot off any sensible line established through the
                  bulk of the plotted points. So it becomes natural to think that these main effects
                  are detectably larger than the other effects, and therefore distinguishable from
                  experimental error even if the others are not. Thus, it seems that drill behavior is
                  potentially describable in terms of the (separate) action of the factors Rotational
                  Speed, Flow Rate, and Mud Type.

                       The plotted fitted effects concern the natural logarithm of advance rate. So
                  the fact that c2 = .5772 says that changing from the low level of rotational speed
                  to the high level produces roughly an increase of 2(.5772)  1.15 in the natural
                  log of the advance rate--i.e., increases the advance rate by a factor of e1.15  3.2.

Interpreting a         Example 6 is one in which the normal plotting clearly identifies a few effects
normal plot of    as larger than the others. However, a normal plot of fitted effects sometimes has
                  a fairly straight-line appearance. When this happens, the message is that the fitted
  fitted effects  effects are potentially explainable as resulting from background variation. And it is
                  risky to make real-world engineering decisions based on fitted effects that haven't
                  been definitively established as representing consistent system reactions to changes
                  in level of the corresponding factors. A linear normal plot of fitted effects from an
                  unreplicated 2p study says that more data are needed.

                       This normal-plotting device has been introduced primarily as a tool for analyzing
                  data lacking any replication. However, the method is useful even in cases where there
                  is some replication and sP can therefore be calculated and formula (8.12) or (8.13)
                  used to judge the detectability of the various factorial effects. Some practice making
                  and using such plots will show that the process often amounts to a helpful kind of
                  "data fondling." Many times, a bit of thought makes it possible to trace an unusual
                  pattern on such a plot back to a previously unnoticed peculiarity in the data.

                       As an example, consider what a normal plot of fitted effects would point out
                  about the following eight hypothetical sample means.

                  y¯ (1) = 95    y¯ c = 145
                   y¯ a = 101   y¯ ac = 103
                   y¯ b = 106  y¯ bc = 107
                  y¯ ab = 106  y¯ abc = 97

                  This is an exaggerated example of a phenomenon that sometimes occurs less bla-
                  tantly in practice. 2p - 1 of the sample means are more or less comparable, while
                  one of the means is clearly different. When this occurs (unless the unusual mean
                  corresponds to the "all high treatment" combination), a normal plot of fitted effects
                  roughly like the one in Figure 8.7 will follow. About half the fitted effects will be
                  large positively and the other half large negatively. (When the unusual mean is the
                  one corresponding to the "all high" combination, the fitted effects will all have the
                  same sign.)
580 Chapter 8 Inference for Full and Fractional Factorial Studies

              Standard normal quantile  2.0

                                        1.0

                                        0.0

                                        -1.0

                                              -5                   0         5

                                                  Fitted effect quantile

                                        Figure 8.7 Normal plot of fitted effects for eight
                                        hypothetical means

8.2.4         Fitting and Checking Simplified Models in Balanced 2p
              Factorial Studies and a Corresponding Variance Estimate
              (Optional )

              When beginning the analysis of a 2p factorial, one hopes that a simplified p-way
              model involving only a few main effects and/or low-order interactions will be ade-
              quate to describe it. Analyses based on formulas (8.12) or (8.13) or normal-plotting
              are ways of identifying such potential descriptions of special p-way structure. Once
              a potential simplification of the 2p analog of model (8.11) has been identified, it is
              often of interest to go beyond that identification to

              1. the fitting and checking (residual analysis) of the simplified model, and
                  even to

              2. the making of formal inferences under the restricted/simplified model as-
                  sumptions.

              When a 2p factorial data set is balanced, the model fitting, checking, and subsequent
              interval-oriented inference is straightforward.

                   With balanced 2p factorial data, producing least squares fitted values is no
              more difficult than adding together (with appropriate signs) desired fitted effects
              and the grand sample mean. Or equivalently and more efficiently, the reverse Yates
              algorithm can be used.

Example 4     In the power requirement study and the data of Table 8.8, only the B and C main
(continued )  effects seem detectably nonzero. So it is reasonable to think of the simplified
              version of model (8.11),

                                              yi jkl = µ... + j + k + i jkl                 (8.14)
                                      8.2 p-Factor Studies with Two Levels for Each Factor 581

              for possible use in describing dynamometer readings. From Table 8.10, the fitted
              version of µ... is y¯ ... = 27.7969, the fitted version of 2 is b2 = .7969, and the
              fitted version of 2 is c2 = -.9844. Then, simply adding together appropriate
              signed versions of the fitted effects, for the four possible combinations of j and
              k, produces the corresponding fitted responses in Table 8.11. So for example, as
              long as the 15 bevel angle (low level of B) and a continuous cut (low level of C)
              are being considered, a fitted dynamometer reading of about 27.98 is appropriate
              under the simplified model (8.14).

              Table 8.11
              Fitted Responses for a "B and C Main Effects Only"
              Description of Power Requirement

              jk  bj  ck y^ = y¯ ··· + bj + ck

              1 1 -.7969 .9844   27.9844
              2 1 .7969 .9844    29.5782
              1 2 -.7969 -.9844  26.0156
              2 2 .7969 -.9844   27.6094

Example 6     Having identified the C, B, and D main effects as detectably larger than the A
(continued )  main effect or any of the interactions in the drill advance rate study, it is natural
              to consider fitting the model

                  yi jkl = µ... + j + k + l + i jkl               (8.15)

              to the logarithms of the unreplicated 24 factorial data of Table 4.24. (Note that
              even though p = 4 factors are involved here, five subscripts are not required,
              since a subscript is not needed to differentiate between multiple members of the
              24 different samples in this unreplicated context. yi jkl is the single observation
              at the ith level of A, jth level of B, kth level of C, and lth level of D.) Since
              the drill advance rate data are balanced (all sample sizes are m = 1), the fitted
              effects given earlier (calculated without reference to the simplified model) serve

              as fitted effects under model (8.15). And fitted responses under model (8.15) are

              obtainable by simple addition and subtraction using those.

                   Since there are eight different combinations of j, k, and l, eight different
              linear combinations of y¯ ..., b2, c2, and d2 are required. While these could be
              treated one at time, it is more efficient to generate them all at once using the

              reverse Yates algorithm (from Section 4.3) as in Table 8.12. From Table 8.12 it

              is evident, for example, that the fitted mean responses for combinations bcd and
              abcd (y^ bcd and y^ abcd) are both 2.6282.
582 Chapter 8 Inference for Full and Fractional Factorial Studies

Example 6     Table 8.12
(continued )  The Reverse Yates Algorithm Used to Fit the "B, C, and D Main Effects"
              Model to Daniel's Data

              Fitted Effect Value Cycle 1 Cycle 2 Cycle 3 Cycle 4 (y^ )

              abcd2222     0       0                                  0     .1633  2.6282
              bcd222       0       0                                .1633  2.4649  2.6282
              acd222       0       0                                .5772          2.0482
              cd22         0     .1633                             1.8877   .1633  2.0482
              abd222       0       0                                       2.4649  1.4738
              bd22         0     .5772                                0     .1633  1.4738
              ad22         0     .2900                              .1633  1.8849
              d2         .1633  1.5977                              .5772           .8938
              abc222       0       0                               1.8877   .1633   .8938
              bc22         0       0                                       1.8849  2.3016
              ac22         0       0                                  0     .1633  2.3016
              c2         .5772   .1633                              .1633  1.3105  1.7216
              ab22         0       0                                .5772   .1633  1.7216
              b2         .2900   .5772                             1.3077  1.3105  1.1472
              a2           0     .2900                                      .1633  1.1472
              y¯ ...    1.5977  1.5977                                0     .7305   .5672
                                                                    .1633           .5672
                                                                    .5772   .1633
                                                                   1.3077   .7305

                   Fitted means derived as in these examples lead in the usual way to residuals,
              R2 values, and plots for checking on the reasonableness of simplified versions of
              the general 2p version of model (8.11). In addition, corresponding to simplified or
              reduced models like (8.14) or (8.15), there are what will here be called few-effects s2
              values. When m > 1, these can be compared to sP2 as another means of investigating
              the reasonableness of the corresponding models.

Definition 6  In a balanced complete 2p factorial study, if a reduced or simplified model

              involving u different effects (including the grand mean) has corresponding
              fitted values y^ and thus residuals y - y^ , the quantity

                        sFE 2 = m2p1- u (y - y^ )2                                         (8.16)

              will be called a few-effects sample variance. Associated with it are  =
              m2p - u degrees of freedom.
                        8.2 p-Factor Studies with Two Levels for Each Factor 583

                        The quantity (8.16) represents an estimator of the basic background variance

                   whenever the corresponding simplified/reduced/few-effects model is an adequate

                   description of the study. When it is not, sFE will tend to overestimate . So comparing
                   sFE to sP is a way of investigating the appropriateness of that description.

                        It is not obvious at this point, but there is a helpful alternative way to calculate
                   the value of sFE 2 given in formula (8.16). It turns out that

  An alternative        sFE 2 = p1      SSTot - m2p E^ 2                         (8.17)
   formula for a                m2 - u

      few effects
sample variance

                   where the sum is over the squares of the u - 1 fitted effects corresponding to those

                   main effects and interactions appearing in the reduced model equation, and (as
                   always) SSTot = (y - y¯ )2 = (n - 1)s2.

Example 4          Residuals for the power requirement data based on the full model (8.11) are
(continued )       obtained by subtracting sample means in Table 8.9 from observations in Table
                   8.8. Under the reduced model (8.14), however, the fitted values in Table 8.11 are
                   appropriate for producing residuals. The fitted means and residuals for a "B and
                   C main effects only" description of this 23 data set are given in Table 8.13. Figure
                   8.8 is a normal plot of these residuals, and Figure 8.9 is a plot of the residuals
                   against the fitted values.

                        If there is anything remarkable in these plots, it is that Figure 8.9 contains a
                   hint that smaller mean response has associated with it smaller response variability.
                   In fact, looking back at Table 8.13, it is easy to see that the two smallest fitted
                   means correspond to the high level of C (i.e., interrupted cuts). That is, the hint of
                   change in response variation shown in Figure 8.9 is the same phenomenon related

                   Table 8.13
                   Residuals for the "B and C Main Effects Only" Model of Power
                   Requirement

                   Combination  y^      Residuals (y - y^ )

                   (1)          27.9844 1.0156, -1.4844, 2.5156, -.9844

                   a            27.9844 .0156, .5156, .0156, -2.9844

                   b            29.5782 -1.0782, -1.0782, .4218, 2.9218

                   ab           29.5782 -.0782, 2.4218, -.5782, -1.5782

                   c            26.0156 1.9844, -1.0156, .4844, .4844

                   ac           26.0156 -1.5156, -1.0156, 1.9844, -.0156

                   bc           27.6094 -.6094, 1.3906, -.1094, -.1094

                   abc          27.6094 -.1094, .3906, -.6094, -1.6094
584 Chapter 8 Inference for Full and Fractional Factorial Studies

Example 4     Standard normal quantile  3.0
(continued )

                                         1.5                                                   2
                                         0.0                                 3
                                        -1.5                           2
                                                                     3
                                                               3
                                                           3
                                                          2

                             -2.4 -1.2 0.0 1.2 2.4
                                        Residual quantile

              Figure 8.8 Normal plot of residuals for the power
              requirement study (B and C main effects only)

                                        4.0

              Residual                  2.0 2

                                                       2           32

                                        0.0

                                              2                    2                              2

                                        -2.0

                                              26.0        27.0        28.0  29.0

                                                          Fitted response

                                        Figure 8.9 Plot of residuals versus fitted power
                                        requirements (B and C main effects only)

              to cut type that was discussed when these data were first introduced. It appears
              that power requirements for interrupted cuts may be slightly more consistent than
              for continuous cuts. But on the whole, there is little in the two figures to invalidate
              model (8.14) as at least a rough-and-ready description of the mechanism behind
              the data of Table 8.8.

                   For the power requirement data,

                                            SSTot = (n - 1)s2 = 108.93

              Then, since sP2 = 2.226, the one-way ANOVA identity (7.49, 7.50, or 7.51) of
              Section 7.4 says that

                              SSTr = SSTot - SSE = 108.93 - 24(2.226) = 55.51
8.2 p-Factor Studies with Two Levels for Each Factor 585

so R2 corresponding to the general or "full" model (8.11) is (as in equations
(7.52) or (7.53))

                              R2 = SStr = 55.51 = .51
                                      SStot 108.93

On the other hand, it is possible to verify that for the simplified model (8.14),
squaring and summing the residuals in Table 8.13 gives

                               SSE = (y - y^ )2 = 57.60

(Recall Definition 6 in Chapter 7 for SS E.) So for the "B and C main effects
only" description of dynamometer readings,

R2 = SStot - SSE = 108.93 - 57.60 = .47
             SStot  108.93

Thus, although at best only about 51% of the raw variation in dynamometer
readings will be accounted for, fitting the simple model (8.14) will account for
nearly all of that potentially assignable variation. So from this point of view as
well, model (8.14) seems attractive as a description of power requirement.

     Note that formulas (8.16) and (8.17) imply that for balanced 2p factorial
data, fitting reduced models gives

             (y - y^ )2 = SSTot - m2p E^ 2

So it is not surprising that using the b2 = .7969 and c2 = -.9844 figures from
before,

SSTot - m2p  E^ 2 = 108.93 - 4 · 23 · (.7969)2 + (-.9844)2
                 = 108.93 - 51.33
                 = 57.60

which is the value of (y - y^ )2 just used in finding R2 for the reduced model.
From formula (8.16) or (8.17), it is then clear that (corresponding to reduced
model (8.14))

                            sFE 2 = 4 · 231 - 3 (57.60) = 1.986

so
                                       

                               sFE = 1.986 = 1.409 mm
586 Chapter 8 Inference for Full and Fractional Factorial Studies

Example 4     which agrees closely with sP = 1.492. Once again on this account, description
(continued )  (8.14) seems quite workable.

Example 6     Table 8.14 contains the log advance rates, fitted values, and residuals for Daniel's
(continued )  unreplicated 24 example. (The raw data were given in Table 4.24, and it is the

              few-effects model (8.15) that is under consideration.)

                   The reader can verify by plotting that the residuals in Table 8.14 are not in

              any way remarkable. Further, it is possible to check that

                    SSTot = (y - y¯ )2 = 7.2774

              and
                                             SSE = (y - y^ )2 = .1736

              So (as indicated earlier in Example 12 in Chapter 4) for the use of model (8.15),

                   R2 = SStot - SSE = 7.2774 - .1736 = .976
                    SStot                                          7.2774

              Table 8.14
              Responses, Fitted Values, and Residuals for the "B, C, and D
              Main Effects" Model and Daniel's Drill Advance Rate Data

              Combination y, ln(advance rate)                      y^      e = y - y^

              (1)   .5188                                          .5672 -.0484

              a     .6831                                          .5672   .1159

              b     1.1878                                         1.1472  .0406

              ab    1.2355                                         1.1472  .0883

              c     1.6054                                         1.7216 -.1162

              ac    1.7405                                         1.7216  .0189

              bc    2.2996                                         2.3016 -.0020

              abc   2.2050                                         2.3016 -.0966

              d     .7275                                          .8938 -.1663

              ad    .8920                                          .8938 -.0018

              bd    1.4085                                         1.4738 -.0653

              abd   1.5107                                         1.4738  .0369

              cd    2.0503                                         2.0482  .0021

              acd   2.2439                                         2.0482  .1957

              bcd   2.4639                                         2.6282 -.1643

              abcd  2.7912                                         2.6282  .1630
                                                 8.2 p-Factor Studies with Two Levels for Each Factor 587

                         Since there is no replication in this data set, fitting the 4-factor version of the
                         general model (8.11) would give a perfect fit, R2 equal to 1.000, all residuals
                         equal to 0, and no value of sP2. Thus, there is really nothing to judge R2 = .976
                         against in relative terms. But even in absolute terms it appears that the "B, C, and
                         D main effects only" model for log advance rate fits the data well.

                              An estimate of the variability of log advance rates for a fixed combina-
                         tion of factor levels derived under the assumptions of model (8.15), is (from
                         formula (8.16))

                         sFE =       1 (.1736) = .120
                                1 · 24 - 4

                         As noted, there's no sP to compare this to, but it is at least consistent with the kind
                         of variation in y seen in Table 8.14 when responses are compared for pairs of
                         combinations that (like combinations b and ab) differ only in level of the factor A.

8.2.5                    Confidence Intervals for Balanced 2p Studies

                         under Few-Effects Models (Optional )

                         Since the basic p-way factorial model is just a rewritten version of the one-way
                         normal model from Chapter 7, the confidence interval methods of that chapter can
                         all see application in p-way factorial studies. But when a simplified/few-effects
                         model is appropriate, sharper real-world engineering conclusions can usually be
                         had by using methods based on the simplified model than by applying the general
                         methods of Chapter 7. And for balanced 2p studies, it is possible to write down
                         simple, explicit formulas for several useful forms of interval-oriented inference.

                              As a first example of what is possible under a few-effects model in a balanced 2p
                         factorial study, consider the estimation of a particular mean response. For balanced
                         data, the 2p fitted effects (including the grand mean) that come out of the Yates
                         algorithm are independent normal variables with means equal to the corresponding
                         underlying effects and variances  2/m2p . So, if a simplified version of model (8.11)
                         involving u effects (including the overall mean) is appropriate, a fitted response y^
                         has mean equal to the corresponding underlying mean, and

                                             2
                                Var y^ = u p

                                            m2

                         It should then be plausible that under a few-effects model in a balanced 2p factorial
                         study, a two-sided interval with endpoints

          Balanced data         y^ ± tsFE  u           (8.18)
individual confidence
                                           m2  p
      limits for a mean
        repsonse under

    a simplified model
588 Chapter 8 Inference for Full and Fractional Factorial Studies

                                  may be used as an individual confidence interval for the corresponding mean re-
                                  sponse. The associated confidence is the probability that the t distribution with
                                   = m2p - u degrees of freedom assigns to the interval between -t and t. And a
                                  one-sided confidence interval for the mean response can be obtained in the usual
                                  way, by employing only one of the endpoints indicated in formula (8.18) and appro-
                                  priately adjusting the confidence level.

Example 4     Consider estimating the mean dynamometer reading corresponding to a 15 bevel
(continued )  angle and interrupted cut using the "B and C main effects only" description of
              Miller's power requirement study. (These are the conditions that appear to produce
              the smallest mean power requirement.) Using (for example) 95% confidence, a
              fitted value of 26.02 from Table 8.11, and sFE = 1.409 mm possessing  =
              4 · 23 - 3 = 29 associated degrees of freedom in formula (8.18), leads to a two-
              sided interval with endpoints

                                  26.02 ± 2.045(1.409)  3

                                                        4·23

              that is, endpoints

                                  26.02 mm ± .88 mm           (8.19)

              that is,

                                  25.14 mm and 26.90 mm

                   In contrast to this interval, consider what the method of Section 7.2 provides
              for a 95% confidence interval for the mean reading for tool type 1, a 15 bevel
              angle, and interrupted cuts. Since sP = 1.492 with  = 24 associated degrees of
              freedom, and (from Table 8.9) y¯ c = 26.50, formula (7.14) of Section 7.2 produces
              a two-sided confidence interval for µc with endpoints

                                                                           1
                                              26.50 ± 2.064(1.492) 

                                                                            4

              that is,

                                  26.50 mm ± 1.54 mm          (8.20)

              A major practical difference between intervals (8.19) and (8.20) is the apparent
              increase in precision provided by interval (8.19), due in numerical terms primarily
              to the "extra" 3/8 factor present in the first plus-or-minus calculation but not
              in the second. However, it must be remembered that the extra precision is bought
              at the price of the use of model (8.14) and the consequent use of all observations
                                                    8.2 p-Factor Studies with Two Levels for Each Factor 589
                            in the generation of y^ c (rather than only the observations from the single sample
                            corresponding to combination c).

                                 A second balanced-data confidence interval method based on a few-effects
                            simplification of the general 2p model is that for estimating the effects included in the

                            model. It comes about by replacing sP in formula (8.13) with sFE and appropriately
                            adjusting the degrees of freedom associated with the t quantile. That is, under a few-
                            effects model in a 2p study with balanced data, a two-sided individual confidence

                            interval for an effect included in the model is

          Balanced data     E^  ±  t    sFE  (8.21)
individual confidence                 
                                      m2p
           limits for a 2p
          effect under a
      simplified model

                            where E^ is the corresponding fitted effect and the confidence associated with the
                            interval is the probability that the t distribution with  = m2p - u degrees of free-
                            dom assigns to the interval between -t and t. One-sided intervals are made from

                            formula (8.21) in the usual way.
                                 Unlike formula (8.13), formula (8.21) can be used in studies where m = 1. This

                            makes it possible to attach precision figures to estimated effects in unreplicated

                            factorial studies, provided one is willing to base them on a reduced or simplified

                            model.

Example 6                   Consider again Daniel's drill advance rate study and, for example, the effect of
(continued )                the high level of rotational speed on the natural logarithm of advance rate. Under
                            the "B, C, and D main effects only" description of log advance rate, sFE = .120
                            with  = 1 · 24 - 4 = 12 associated degrees of freedom. Also, c2 = .5772. Then
                            (for example) using a 95% confidence level, from formula (8.21), a two-sided
                            interval for 2 under the simplified model has endpoints

                                                                                  .120
                                                              .5772 ± 2.179 

                                                                                   1 · 24

                            that is,

                                                                   .5772 ± .0654

                            that is,

                                                                .5118 and .6426
590 Chapter 8 Inference for Full and Fractional Factorial Studies

Example 6          This in turn translates (via multiplication by 2, since 2 - 1 = 22) to an
(continued )  increase of between

                                                1.0236 and 1.2852

              in average log advance rate as one moves from the low level of rotational speed
              to the high level. And upon exponentiation, a multiplication of median advance
              rate by a factor between

                                                   2.78 and 3.62

              is indicated as one moves between levels of rotational speed. (A normal mean
              is also the distribution's median, and under a transformation the median of the
              transformed values is the transformation applied to the median. So the infer-
              ence about the mean logged rate can be translated to one about the median rate.
              However, since the mean of transformed values is not in general the transformed
              mean, the interval obtained by exponentiation unfortunately does not apply to
              the mean advance rate.)

                   There are other ways to use the reduced model ideas discussed here. For exam-
              ple, a simplified model for responses can be used to produce prediction and tolerance
              intervals for individuals. Section 8.3 of Vardeman's Statistics for Engineering Prob-
              lem Solving is one place to find an exposition of these additional methods.

Section 2 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Consider again the situation of Exercise 2 of Sec-          done so, use the Yates algorithm and compute fitted
   tion 4.3.                                                   24 factorial effects for the data set.
    (a) For the logged responses, make individual 95%           (a) Use normal-plotting to identify statistically de-
        confidence intervals for the effects correspond-
        ing to the high levels of all three factors. Which          tectable effects here.
        effects are statistically detectable?                  (b) Based on your analysis from (a), postulate a
   (b) Fit an appropriate few-effects model suggested
        by your work in (a) to these data. Compare the              possible few-effects model for this situation.
        corresponding value of sFE to the value of sP.              Use the reverse Yates algorithm to fit such a
    (c) Compare a two-sided individual 95% confi-                   model to these data. Use the fitted values to
        dence interval for the mean (logged) response               compute residuals. Normal-plot these and plot
        for combination (1) made using the fitted few-              them against levels of each of the four factors,
        effects model to one based on the methods of                looking for obvious problems with the model.
        Section 7.2.                                            (c) Based on your few-effects model, make a rec-
                                                                    ommendation for the future making of these
2. Chapter Exercise 9 in Chapter 4 concerns the mak-                devices. Give a 95% two-sided confidence in-
   ing of Dual In-line Packages and the number of                   terval (based on the few-effects model) for the
   pullouts produced on such devices under 24 dif-                  mean pullouts you expect to experience if your
   ferent combinations of manufacturing conditions.                 advice is followed.
   Return to that exercise, and if you have not already
                                                            3. A classic unreplicated 24 factorial study, used as an
                                                               example in Experimental Statistics (NBS Handbook
                              8.3  Standard Fractions of Two-Level Factorials, Part I:  1  Fractions  591
                                                                                        2

# 91) by M. G. Natrella, concerns flame tests of      (a) Use the (four-cycle) Yates algorithm and com-
fire-retardant treatments for cloth. The factors and       pute the fitted 24 factorial effects for the study.
levels used in the study were
                                                      (b) Make either a normal plot or a half normal
A Fabric Tested      sateen (-) vs. monk's cloth (+)       plot using the fitted effects from part (a). What
B Treatment          X (-) vs. Y (+)                       subject-matter interpretation of the data is sug-
C Laundering         before (-) vs. after (+)              gested by the plot? (See Chapter Exercise 9
                                                           regarding half normal-plotting.)
    Condition        warp (-) vs. fill (+)
D Direction of Test                                   (c) Natrella's original analysis of these data pro-
                                                           duced the conclusion that both the A main ef-
The response variable, y, is the inches burned on          fects and the AB two-factor interactions are
a standard-size sample in the flame test. The data         statistically detectable and of practical impor-
reported by Natrella follow:                               tance. We (based on a plot like the one asked for
                                                           in (b)) are inclined to doubt that the data are re-
Combination y        Combination y                         ally adequate to detect the AB interaction. But
                                                           for the sake of example, temporarily accept the
(1)  4.2                 d         4.0                     conclusion of Natrella's analysis. What does it
                                                           say in practical terms about the fire-retardant
a    3.1                 ad        3.0                     treating of cloth? (How would you explain the
                                                           results to a clothing manufacturer?)
b    4.5                 bd        5.0

ab   2.9                 abd       2.5

c    3.9                 cd        4.0

ac   2.8                 acd       2.5

bc   4.6                 bcd       5.0

abc  3.2                 abcd 2.3

   qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

   8.3 Standard Fractions of Two-Level

     Factorials,              Part  I:   1  Fractions
                                         2

                     The notion of a fractional factorial data structure was first introduced in Section

                     1.2. But as yet, this text has done little to indicate either how such a structure

                     might be chosen or how analysis of fractional factorial data might proceed. The

                     delay is a reflection of the subtle nature of these topics rather than any lack of

                     importance. Indeed, fractional factorial experimentation and analysis is one of the

                     most important tools in the modern engineer's kit. This is especially true where many

                     factors potentially affect a response and there is little a priori knowledge about the

                     relative impacts of these factors.
                          This section and the next treat the (standard) 2p-q fractional factorials--the

                     class of fractional factorials for which advantageous methods of data collection and

                     analysis can be presented most easily and completely. These structures, involving

                     1   of all possible combinations of levels of  p  two-level factors, are among the most
                     2q
                     useful fractional factorial designs for application in engineering experimentation.

                     In addition, they clearly illustrate the general issues that arise any time only a frac-

                     tion of a complete factorial set of factor-level combinations can be included in a

                     multifactor study.
592 Chapter 8 Inference for Full and Fractional Factorial Studies

           This section begins with some general qualitative remarks about fractional
                                                                                                     2 p-1
           factorial  experimentation.   The   standard  1         fractions  of  2p  studies  (the         fractional
                                                         2
           factorials) are then discussed in detail. The section covers in turn (1) the proper

           choice of such fractions, (2) the resultant aliasing or confounding patterns, and (3)

           corresponding methods of data analysis. The section closes with a few remarks
           about qualitative issues, addressed to the practical use of 2p-1 designs.

8.3.1      General Observations about Fractional Factorial Studies

           In many of the physical systems engineers work on, there are many factors potentially
           affecting a response y. In such cases, even when the number of levels considered
           for each factor is only two, there are a huge number of different combinations of
           levels of the factors to consider. For instance, if p = 10 factors are considered,
           even when limiting attention to only two levels of each factor, at least 210 = 1,024
           data points must be collected in order to complete a full factorial study. In most
           engineering contexts, restrictions on time and other resources would make a study
           of that size infeasible. One could try to guess which few factors are most important
           in determining the response and do a smaller complete factorial study on those
           factors (holding the levels of the remaining factors fixed). But there is obviously
           a risk of guessing wrong and therefore failing to discover the real pattern of how
           factors affect the response.

                A superior alternative is to conduct the investigation in at least two stages.
           A relatively small screening study (or several of them), intended to identify those
           factors most likely influencing the response, can be done first. This can be followed
           up with a more detailed study (or studies) in those variables. It is in the initial
           screening phase of such a program that fractions of 2p studies are most appropriate.
           Tools such as full factorials are appropriate for the later stage (or stages) of study.

                Once the reality of resource limitations leads to consideration of fractional
           factorial experimentation, several qualitative points become clear. For one, there is
           no way to learn as much from a fraction of a full factorial study as from the full
           factorial itself. (There is no Santa Claus who for the price of eight observations
           will give as much information as can be obtained from 16.) Fractional factorial
           experiments inevitably leave some ambiguity in the interpretation of their results.
           Through careful planning of exactly which fraction of a full factorial to use, the
           object is to hold the ambiguity to a minimum and to make sure it is of a type that is
           most tolerable. Not all fractions of a given size from a particular full factorial study
           have the same potential for producing useful information.

Example 7  Choosing Half of a 22 Factorial Study

           As a completely artificial but instructive example of the preceding points, sup-

           pose that two factors A and B each have two levels (low and high) and that

           instead    of  conducting  a  full  22  factorial       study,  data   at  only  1  of  the  four  possible
                                                                                            2
          8.3  Standard Fractions of Two-Level Factorials, Part I:             1    Fractions  593
                                                                               2

combinations will be collected

                                       (1), a, b, and ab

     If (1) is chosen as one of the two combinations to be studied, two of the
three possible choices of the other combination can easily be eliminated from
consideration. The possibility of studying the combinations

                                       (1) and a

is no good, since in both cases the factor B is held at its low level. Therefore, no
information at all would be obtained on B's impact on the response. Similarly,
the possibility of studying the combinations

                                       (1) and b

can be eliminated, since no information would be obtained on factor A's impact
on the response. So that leaves only the set of combinations

                                       (1) and ab

as  a  1  fraction  of  the  full  22  factorial  that  is  at  all  sensible  (if  combination  (1)
       2
is to be included). Similar reasoning eliminates all other pairs of combinations

from potential use except the pair

                                       a and b

       But now notice that any experiment that includes only combinations

                                       (1) and ab

or combinations

                                       a and b

must inevitably produce somewhat ambiguous results. Since one moves from
combination (1) to combination ab (or from a to b) by changing levels of both
factors, if a large difference in response is observed, it will not be clear whether
the difference is due to A or due to B.

     At least in qualitative terms, such is the nature of all fractional factorial stud-
ies. Although very poor choices of experimental combinations may be avoided,
some level of ambiguity must be accepted as the price for not conducting a full
factorial.
594 Chapter 8 Inference for Full and Fractional Factorial Studies

Example 8  Half of a Hypothetical 23 Factorial

           As a second hypothetical but instructive example of the issues that must be dealt
           with in fractional factorial experimentation, consider a system whose behavior is
           governed principally by the levels of three factors: A, B, and C. (For the sake of
           concreteness, suppose that A is a temperature, B is a pressure, and C is a catalyst
           type, and that the effects of these on the yield y of a chemical process are under
           consideration.) Suppose further that in a 23 study of this system, the factorial
           effects on an underlying mean response µ are given by

            µ... = 10,    2 = 3,                                     2 = 1,       2 = 2,
           22 = 2,      22 = 0,                                    22 = 0,   222 = 0

           Either through the use of the reverse Yates algorithm or otherwise, it is possible
           to verify that corresponding to these effects are then the eight combination means

           µ(1) = 6,    µa = 8,                                    µb = 4,   µab = 14,
            µc = 10,    µac = 12,                                  µbc = 8,  µabc = 18

                Now imagine that for some reason, only four of the eight combinations of
           levels of A, B, and C will be included in a study of this system, namely the
           combinations

                        a, b, c, and abc

           Suppose further that the background noise is negligible, so that observations
           for a given treatment combination are essentially equal to the corresponding
           underlying mean. Then one essentially knows the values of

                            µa = 8, µb = 4, µc = 10, µabc = 18

           Figure 8.10 shows the complete set of eight combination means laid out on a
           cube plot, with the four observed means circled.

                As a sidelight, note the admirable symmetry possessed by the four circled
           corners on Figure 8.10. Each face of the cube has two circled corners (both levels
           of all factors appear twice in the choice of treatment combinations). Each edge
           has one circled corner (each combination of all pairs of factors appears once).
           And collapsing the cube in any one of the three possible directions (left to right,
           top to bottom, or front to back) gives a full factorial set of four combinations.
           (Ignoring the level of any one of A, B, or C in the four combinations a, b, c, and
           abc gives a full factorial in the other two factors.)
8.3  Standard Fractions of Two-Level Factorials, Part I:     1       Fractions  595
                                                             2

                              µ bc = 8            µabc = 18

     (+) µb = 4                         µab = 14

     Factor B                 µc = 10             ( µac = 12 +)
                                                           Factor C
     (-)            µ(1) = 6            µa = 8
                                                      (-)

                    (-) Factor A (+)

           Figure 8.10 23 hypothetical means, with four
           known means circled

     Now consider what an engineer possessing only the values of µa, µb, µc,
and µabc might be led to conclude about the system. In particular, begin with the
matter of evaluating an A main effect. Definition 3 says that

2 = µ2.. - µ...                         

     the average of all four mean
=  responses where A is at its  -                 the grand average of all
                                                  eight mean responses
     second or high level

which can be thought of as the right-face average minus the grand average for the
cube in Figure 8.10. Armed only with the four means µa, µb, µc, and µabc (the
four circled corners on Figure 8.10), it is not possible to compute 2. But what
might be done is to make a calculation similar to the one that produces 2 using
only the available means. That is,

2 = a " 21 fraction A main effect"      

     the average of the available
=  two means where A is at its  -                 the grand average of the
                                                  available four means
     high level

1                   1
= (µa + µabc) - (µa + µb + µc + µabc)
2                   4

= 1 (8 + 18) - 1 (8 + 4 + 10 + 18)
2                4

= 13 - 10

=3
596 Chapter 8 Inference for Full and Fractional Factorial Studies

Example 8                 And, amazingly enough, 2 = 2 here.
(continued )                   It appears that using only four combinations, as much can be learned about

                          the A main effect as if all eight combination means were in hand! This is too good
                          to be true in general, as is illustrated by a parallel calculation for a C main effect.

                               2 = a " 21 fraction C main effect"                
                                          
                                                 the average of the two
                                    =  available means where  -                              the grand average of the
                                                                                             four available means
                                                 C is at its high level

                                          1                       1
                                    = (µc + µabc) - (µa + µb + µc + µabc)
                                          2                       4

                                    =4

                          while   this  hypothetical         example  began      with  2     =  2.  Here,    the  1  fraction  calcula-
                                                                                                                  2
                          tion gives something quite different from the full factorial calculation.

                               The key to understanding how one can apparently get something for nothing

                          in the case of the A main effects in this example, but cannot do so in the case of

                          the  C  main    effects,    is  to  know   that  (in  general)     for  this  1    fraction,
                                                                                                        2

                                                                      2 = 2 + 22

                          and

                                                                      2 = 2 + 22

                          Since this numerical example began with 22 = 0, one is "fortunate"--it turns
                          out numerically that 2 = 2. On the other hand, since 22 = 2 = 0, one is
                          "unfortunate"--it turns out numerically that 2 = 2 + 2 = 2.
                                                                          2         2                        1
                               Relationships          like   these   for     and       hold       for   all  2  fraction  versions  of

                          the full factorial effects. These relationships detail the nature of the ambiguity

                          inherent    in  the    use  of  the  1  fraction   of  the   full  23   factorial     set  of  combinations.
                                                               2
                          Essentially, based on data from four out of eight possible combinations, one will

                          be unable to distinguish between certain pairs of effects, such as the A main effect

                          and BC 2-factor interaction pair here.

               8.3.2      Choice  of  Standard            1  Fractions     of   2p  Studies
                                                          2
Three fundamental
    issues in the use     The use of standard 2p-q fractional factorial data structures depends on having
       of a fractional
               factorial  answers for the following three basic questions:

                          1.   How is        1   of 2p    possible combinations of factor levels to include in a study
                                             2q
                               rationally chosen?
                              8.3    Standard Fractions of Two-Level Factorials, Part I:                 1  Fractions       597
                                                                                                         2

                       2. How is the pattern of ambiguities implied by a given choice of 2p-q combi-
                           nations determined?

                       3. How is data analysis done for a particular choice of 2p-q combinations?

                       These  questions  will  be    answered   in  this     section   for  the   case  of  1  fractions   (2 p-1
                                                                                                            2
                       fractional factorials) and for general q in the next section.

     Prescription for  In     order  to  arrive  at  what   is  in  some     sense  a  best    possible     choice  of  1  of  2p
                                                                                                                        2
a best half fraction   combinations of levels of p factors, do the following. For the first p - 1 factors,
    of a 2p factorial  write out all 2p-1 possible combinations of these factors. By multiplying plus and

                       minus signs (thinking of multiplying plus and minus 1's) corresponding to levels

                       of the first factors, then arrive at a set of plus and minus signs that can be used to

                       prescribe how to choose levels for the last factor (to be used in combination with

                       the indicated levels of the first p - 1 factors).

Example 9              A 25-1 Chemical Process Experiment

                       In his article "Experimenting with a Large Number of Variables" (ASQC Techni-

                       cal Supplement Experiments in Industry, 1985), R. Snee discusses a successful
                       25-1 experiment on a chemical process, where the response of interest, y, was a

                       coded color index of the product. The factors studied and their levels are as in

                       Table 8.15.

                              The  standard    recommendation           for  choosing       a  1  fraction  was  followed      in
                                                                                               2
                       Snee's study. Table 8.16 shows an appropriate set of 16 lines of plus and minus

                       signs  for  generating    the  1  ·  32  =   16  combinations        included    in  Snee's  study.  The
                                                      2
                       first four columns of this table specify levels of factors A, B, C, and D for the

                       16 = 24 possible combinations of levels of these factors (written in Yates standard

                       order). (The first line, for example, indicates the low level of all of these first

                       four factors.) The last column of this table is obtained by multiplying the first

                       four plus or minus signs (plus or minus 1's) in a given row. It is this last column

                       that can be used to determine how to choose a level of factor E for use when the

                       factors A through D are at the levels indicated in the first four columns.

                                     Table 8.15
                                     Five Chemical Process Variables and Their Experimental Levels

                                     Factor Process Variable Factor Levels

                                         A Solvent/Reactant low (-) vs. high (+)

                                         B Catalyst/Reactant .025 (-) vs. .035 (+)

                                         C Temperature                       150C (-) vs. 160C (+)

                                         D Reactant Purity 92% (-) vs. 96% (+)

                                         E pH of Reactant 8.0 (-) vs. 8.7 (+)
598 Chapter 8 Inference for Full and Fractional Factorial Studies

Example 9                            Table 8.16
(continued )                         Signs for Specifying a Standard 25-1

                                     Fractional Factorial

                                     A B C D ABCD Product

                                     ----                              +

                                     +---                              -

                                     -+--                              -

                                     ++--                              +

                                     --+-                              -

                                     +-+-                              +

                                     -++-                              +

                                     +++-                              -

                                     ---+                              -

                                     +--+                              +

                                     -+-+                              +

                                     ++-+                              -

                                     --++                              +

                                     +-++                              -

                                     -+++                              -

                                     ++++                              +

                     In Snee's study, the signs in the ABCD Product column were used without

              modification to specify levels of E. The corresponding treatment combination

              names (written in the same order as in Table 8.16) and the data reported by Snee

              are given in Table 8.17. Notice that the 16 combinations listed in Table 8.17 are

              1  of  the  25  =  32  possible  combinations        of  levels  of  these  five  factors.  (They  are
              2
              those 16 that have an odd number of factors appearing at their high levels).

Example 10    A 25-1 Agricultural Engineering Study

              The article "An Application of Fractional Factorial Experimental Designs" by
              Mary Kilgo (Quality Engineering, 1988) provides an interesting complement to
              the previous example. In one part of an agricultural engineering study concerned
              with the use of carbon dioxide at very high pressures to extract oil from peanuts,
              the effects of five factors on a percent yield variable y were studied in a 25-1
              fractional factorial experiment. The five factors and their levels (as named in
              Kilgo's article) are given in Table 8.18.

                   Interestingly enough, rather than studying the 16 combinations obtainable
              using the final column of Table 8.16 directly, Kilgo switched all of the signs in
              the ABCD product column before assigning levels of E. This leads to the use of
              "the other" 16 out of 32 possible combinations (those having an even number of
8.3  Standard Fractions of Two-Level Factorials, Part I:  1     Fractions  599
                                                          2

     Table 8.17

     16 Combinations and Observed
     Color Indices in Snee's 25-1 Study

     (Example 9 )

     Combination Color Index, y

             e            -.63
             a             2.51
             b           -2.68
             abe         -1.66
             c             2.06
             ace           1.22
             bce         -2.09
             abc           1.93
             d             6.79
             ade           6.47
             bde           3.45
             abd           5.68
             cde           5.22
             acd           9.38
             bcd           4.30
             abcde         4.05

Table 8.18
Five Peanut Processing Variables and Their Experimental Levels

Factor Process Variable  Factor Levels

A Pressure               415 bars (-) vs. 550 bars (+)

B Temperature            25C (-) vs. 95C (+)

C Peanut Moisture        5% (-) vs. 15% (+)

D Flow Rate              40 1/min (-) vs. 60 1/min (+)

E Average Particle Size 1.28 mm (-) vs. 4.05 mm (+)

factors appearing at their high levels). The 16 combinations studied and corre-
sponding responses reported by Kilgo are given in Table 8.19 in the same order
for factors A through D as in Table 8.16.

     The difference between the combinations listed in Tables 8.17 and 8.19 deserves
some thought. As Kilgo named the factor levels, the two lists of combinations
are quite different. But verify that if she had made the slightly less natural but
nevertheless permissible choice to call the 4.05 mm level of factor E the low (-) level
600 Chapter 8 Inference for Full and Fractional Factorial Studies

                                                         Table 8.19

                                                         16 Combinations and Observed
                                                         Yields in Kilgo's 25-1 Study

                                                         (Example 10 )

                                                         Combination Yield, y (%)

                                                         (1)                      63

                                                         ae                       21

                                                         be                       36

                                                         ab                       99

                                                         ce                       24

                                                         ac                       66

                                                         bc                       71

                                                         abce                     54

                                                         de                       23

                                                         ad                       74

                                                         bd                       80

                                                         abde                     33

                                                         cd                       63

                                                         acde                     21

                                                         bcde                     44

                                                         abcd                     96

Fractional factorials   and the 1.28 mm level the high (+) level, the names of the physical combinations
 fully reveal system    actually studied would be exactly those in Table 8.17 rather than those in Table 8.19.
   structure only for
          simple cases       The point here is that due to the rather arbitrary nature of how one chooses to
                        name high and low levels of two factors, the names of different physical combinations
                        are themselves to some extent arbitrary. In choosing fractional factorials, one chooses
                        some particular naming convention and then has the freedom to choose levels of
                        the last factor (or factors for q > 1 cases) by either using the product column(s)
                        directly or after switching signs. The decision whether or not to switch signs does
                        affect exactly which physical combinations will be run and thus how the data should
                        be interpreted in the subject-matter context. But generally, the different possible
                        choices (to switch or not switch signs) are a priori equally attractive. For systems
                        that happen to have relatively simple structure, all possible results of these arbitrary
                        choices typically lead to similar engineering conclusions. When systems turn out to
                        have complicated structures, the whole notion of fractional factorial experimentation
                        loses its appeal. Different arbitrary choices lead to different perceptions of system
                        behavior, none of which (usually) correctly portrays the complicated real situation.

8.3.3                   Aliasing    in  the   Standard   1      Fractions
                                                         2

                        Once  a  1  fraction  of  a  2p  study  is  chosen,  the  next  issue  is  determining  the  nature
                        of the ambiguities that must arise from its use. For 2p-1 data structures of the2

                        type described here, one can begin with a kind of statement of how the fractional
                           8.3  Standard Fractions of Two-Level Factorials, Part I:                         1  Fractions  601
                                                                                                            2

                    factorial plan was derived and through a system of formal multiplication arrive at an
                    understanding of which (full) factorial effects cannot be separated on the basis of the
                    fractional factorial data. Some terminology is given next, in the form of a definition.

Definition 7        When it is only possible to estimate the sum (or difference) of two or more
                    (full) factorial effects on the basis of data from a fractional factorial, those
                    effects are said to be aliased or confounded and are sometimes called aliases.
                    In this text, the phrase alias structure of a fractional factorial plan will mean
                    a complete specification of all sets of aliased effects.

                    As an example of the use of this terminology, return to Example 8. There, it is
                    possible only to estimate 2 + 22, not either of 2 or 22 individually. So the A
                    main effect is confounded with (or aliased with) the BC 2-factor interaction.

                         The way the system of formal multiplication works for detailing the alias
                    structure of one of the recommended 2p-1 factorials is as follows. One begins
                    by writing

 Generator for                 the name of the           ± the product of names of the first p - 1 factors                (8.22)
a standard half                last factor
fraction of a 2p

         factorial

                    where the plus or minus sign is determined by whether the signs were left alone
                    or switched in the specification of levels of the last factor. The double arrow in
                    expression (8.22) will be read as "is aliased with." And since expression (8.22)
                    really says how the fractional factorial under consideration was chosen, expression
                    (8.22) will be called the plan's generator. The generator (8.22) for a 2p-1 plan says
                    that the (high level) main effect of the last factor will be aliased with plus or minus
                    the (all factors at their high levels) p - 1 factor interaction of the first p - 1 factors.

Example 9           In Snee's 25-1 study, the generator
(continued )

                                                             E  ABCD

                    was used. Therefore the (high level) E main effect is aliased with the (all high

                    levels) ABCD 4-factor interaction. That is, only 2 +  2222 can be estimated
                                    1
                    based  on  the  2  fraction  data,  not  either  of  its  summands  individually.

Example 10          In Kilgo's 25-1 study, the generator
 (continued )                                              E  -ABCD
602 Chapter 8 Inference for Full and Fractional Factorial Studies

Example 10                    was used. The (high level) E main effect is aliased with minus the (all high levels)
 (continued )
                              ABCD 4-factor interaction. That is, only 2 -  2222 can be estimated based
                                       1
                              on  the  2  fraction  data,  not  either  of  the  terms  individually.

          Conventions for     The  entire  alias    structure   for  a  1  fraction  follows  from  the  generator  (8.22)  by
              the system of                                             2
                              multiplying both sides of the expression by various factor names, using two special
   formal multiplication
                              conventions. These are that any letter multiplied by itself produces the symbol "I"
    Defining relation for
a standard half fraction      and that any letter multiplied by "I" is that letter again. Applying the first of these

           of a 2p factorial  conventions to expression (8.22), both sides of the expression may be multiplied by

                              the name of the last factor to produce the relation

                                                    I  ± the product of names of all p factors                      (8.23)

                              Expression (8.23) means that the grand mean is aliased with plus or minus the (all
                              factors at their high level) p-factor interaction. There is further special terminology
                              for an expression like that in display (8.23).

Definition 8                  The list of all aliases of the grand mean for a 2p-q fractional factorial is called
                              the defining relation for the design.

                                   By first translating a generator (or generators in the case of q > 1) into a defining
                              relation and then multiplying through the defining relation by a product of letters
                              corresponding to an effect of interest, one can identify all aliases of that effect.

Example 9                     In Snee's 25-1 experiment, the generator was
(continued )

                                                                           E  ABCD

                              When multiplied through by E, this gives the experiment's defining relation

                                                                        I  ABCDE                                    (8.24)

                              which indicates that the grand mean µ..... is aliased with the 5-factor interaction
                                22222. Then, for example, multiplying through defining relation (8.24) by
                              the product AC produces the relationship

                                                                           AC  BDE

                              Thus, the AC 2-factor interaction is aliased with the BDE 3-factor interaction.
                              In fact, the entire alias structure for the Snee study can be summarized in terms
                              of the aliasing of 16 different pairs of effects. These are indicated in Table 8.20,
               8.3  Standard Fractions of Two-Level Factorials, Part I:  1  Fractions  603
                                                                         2

               which was developed by using the defining relation (8.24) to find successively
               (in Yates order) the aliases of all effects involving only factors A, B, C, and D.
               Table 8.20 shows that main effects are confounded with 4-factor interactions and
               2-factor interactions with 3-factor interactions. This degree of ambiguity is as
               mild as is possible in a 25-1 study.

                    Table 8.20

                    The Complete Alias Structure for
                    Snee's 25-1 Study

                    I  ABCDE  D  ABCE
                    A  BCDE   AD  BCE
                    B  ACDE   BD  ACE
                    AB  CDE   ABD  CE
                    C  ABDE   CD  ABE
                    AC  BDE   ACD  BE
                    BC  ADE   BCD  AE
                    ABC  DE   ABCD  E

Example 10     In Kilgo's peanut oil extraction study, since the generator is E  -ABCD, the
 (continued )  defining relation is I  -ABCDE, and the alias structure is that given in Table

               8.20, except that a minus sign should be inserted on one side or the other of
               every row of the table. So, for example, 22 -   222 may be estimated based
               on Kilgo's data, but neither 22 nor   222 separately.

8.3.4          Data Analysis for 2p-1 Fractional Factorials

               Once the alias structure of a 2p-1 fractional factorial is understood, the question of
               how to analyze data from such a study has a simple answer.

               1. Temporarily ignore the last factor and compute the estimated or fitted "ef-
                   fects."

               2. Somehow judge the statistical significance and apparent real importance of
                   the "effects" computed for the complete factorial in p - 1 two-level factors.
                   (Where some replication is available, the judging of statistical significance
                   can be done through the use of confidence intervals. Where all 2p-1 samples
                   are of size 1, the device of normal-plotting fitted "effects" is standard.)

               3. Finally, seek a plausible simple interpretation of the important fitted "ef-
                   fects," recognizing that they are estimates not of the effects in the first p - 1
                   factors alone, but of those effects plus their aliases.
604 Chapter 8 Inference for Full and Fractional Factorial Studies

Example 9     Consider the analysis of Snee's data, listed in Table 8.17 in Yates standard order
(continued )  for factors A, B, C, and D (ignoring the existence of factor E). Then, according to
              the prescription for analysis just given, the first step is to use the Yates algorithm
              (for four factors) on the data. These calculations are summarized in Table 8.21.

                   Each entry in the final column of Table 8.21 gives the name of the effect
              that the corresponding numerical value in the "Cycle 4 ÷ 16" column would be
              estimating if factor E weren't present, plus the alias of that effect. The numbers
              in the next-to-last column must be interpreted in light of the fact that they are
              estimating sums of 25 factorial effects.

                   Since there is no replication indicated in Table 8.17, only normal-plotting
              fitted (sums of) effects is available to identify those that are distinguishable from
              noise. Figure 8.11 is a normal plot of the last 15 entries of the Cycle 4 ÷ 16
              column of Table 8.21. (Since in most contexts one is a priori willing to grant that
              the overall mean response is other than 0, the estimate of it plus its alias(es) is
              rarely included in such a plot.)

              Standard normal quantile  2.4                                                 D + ABCE

                                        1.2                                       A + BCDE

                                                                               2

                                        0.0 2

                                        -1.2      E + ABCD
                                              B + ACDE

                                              -0.80 0.00 0.80 1.60 2.40
                                                  Estimated sum of effects quantile

              Figure 8.11 Normal plot of estimated sums of effects
              in Snee's 25-1 study

                   Depending upon how the line is drawn through the small estimated (sums of)
              effects in Figure 8.11, the estimates corresponding to D + ABCE, and possibly
              B + ACDE, E + ABCD, and A + BCDE as well, are seen to be distinguishable
              in magnitude from the others. (The line in Figure 8.11 has been drawn in keeping
              with the view that there are four statistically detectable sums of effects, primarily
              because a half normal plot of the absolute values of the estimates--not included
              here--supports that view.) If one adopts the view that there are indeed four
              detectable (sums of) effects indicated by Figure 8.11, it is clear that the simplest
              possible interpretation of this outcome is that the four large estimates are each
              reflecting primarily the corresponding main effects (and not the aliased 4-factor
                       8.3  Standard Fractions of Two-Level Factorials, Part I:    1  Fractions  605
                                                                                   2

               Table 8.21
               The Yates Algorithm for a 24 Factorial Applied to Snee's 25-1 Data

               y Cycle 1 Cycle 2 Cycle 3 Cycle 4 Cycle 4 ÷ 16 Sum Estimated

                -.63     1.88  -2.46        .66    46.00     2.875                 µ..... +   22222
                 2.51  -4.34     3.12    45.34     13.16      .823                 2 +   2222
               -2.68           22.39             -20.04                            2 +   2222
               -1.66     3.28  22.95      7.34             -1.253                  22 +   222
                 2.06   -.16     4.16     5.82        .88     .055                 2 +  2222
                 1.22  13.26     3.18   -9.66       6.14      .384                 22 +  222
               -2.09     9.13    1.91  -10.38       1.02      .064                 22 +  222
                 1.93  14.60     3.91     2.74        .66     .041                 222 +  22
                 6.79    8.35           -1.86         .02     .001                 2 +  2222
                 6.47    3.14  -6.22      5.58     44.68                           22 +  222
                 3.45    1.02  -3.44              -1.52      2.793                 22 +  222
                 5.68   -.84   -4.13        .56     -.72    -.095                  222 +  22
                 5.22    4.02  -6.25      -.98    -4.60     -.045                   22 +  222
                 9.38   -.32   -2.12      2.00    -5.02     -.288                   222 +  22
                 4.30    2.23             2.78      2.98    -.314                   222 +  22
                 4.05    4.16    4.86   -2.12     -4.90                             2222 + 2
                        -.25     2.55     6.98   -13.94       .186
                               -4.41    -6.96               -.306
                                                            -.871

               interactions). That is, a tentative (because of the incomplete nature of fractional
               factorial information) description of the chemical process is that D (reactant
               purity), B (catalyst/reactant), A (solvent/reactant), and E (pH of reactant) main
               effects are (in that order) the principal determinants of product color. Depending
               on the engineering objectives for product color index y, this tentative description
               of the system could have several possible interpretations. If large y were desirable,
               the high levels of A and D and low levels of B and E appear most attractive. If
               small y were desirable, the situation would be reversed. But in fact, Snee's study
               was done not to figure out how to maximize or minimize y, but rather to determine
               how to reduce variation in y. The engineering implications of the "D, B, A, and
               E main effects only" system description are thus to focus attention on the need to
               control variation first in level of factor D (reactant purity), then in level of factor
               B (catalyst/reactant), then in level of factor A (solvent/reactant), and finally in
               level of factor E (pH of reactant).

Example 10     Verify that for Kilgo's data in Table 8.19, use of the (four-cycle) Yates algorithm
 (continued )  on the data as listed (in standard order for factors A, B, C, and D, ignoring factor
               E) produces the estimated (differences of) effects given in Table 8.22.
606 Chapter 8 Inference for Full and Fractional Factorial Studies

Example 10     Table 8.22
 (continued )  Estimated Differences of 25 Factorial Effects from Kilgo's 25-1 Study

               Value Difference Estimated Value Difference Estimated

               54.3                            µ..... -   22222    0.0                                     2 -  2222
                                                                                                           22 -  222
               3.8                             2 -   2222          -2.0                                    22 -  222
                                                                                                           222 -  22
               9.9                             2 -   2222          -.9                                      22 -  222
                                                                                                            222 -  22
               2.6                             22 -   222          -3.1                                     222 -  22
                                                                                                            2222 - 2
               .6                              2 -  2222           1.1

               .6                              22 -  222           .1

               1.5                             22 -  222           3.5

               1.8                             222 -  22           22.3

                    The last 15 of these estimated differences are normal-plotted in Figure 8.12.
               It is evident from the figure that the two estimated (differences of) effects corre-
               sponding to

                                        2 -   2222 and  2222 - 2

               are significantly larger than the other 13 estimates. The simplest possible interpre-
               tation of this outcome is that the two large estimates are each reflecting primarily
               the corresponding main effects (not the aliased 4-factor interactions). That is,
               a tentative description of the oil extraction process is that average particle size
               (factor E) and temperature (factor B), acting more or less separately, are the prin-
               ciple determinants of yield. This is an example where the ultimate engineering
               objective is to maximize response and the two large estimates are both positive.
               So, for best yield one would prefer the high level of B (95C temperature) and

                     Standard normal quantile  2.4                                                         ABCD - E

                                                                                                 B - ACDE

                                               1.2

                                                0.0 2
                                               -1.2

                            - 5.0 0.0 5.0 10.0 15.0 20.0
                                    Estimated difference of effects quantile

                     Figure 8.12 Normal plot of estimated differences of
                     effects in Kilgo's 25-1 study
                       8.3    Standard Fractions of Two-Level Factorials, Part I:                  1  Fractions   607
                                                                                                   2

                low level of E (1.28 mm particle size). (- 2 is apparently positive, and since
                 1 = - 2, the superiority of the low level of E is indicated.)

8.3.5       Some Additional Comments

            The next section treats general             1      fractions of 2p     factorials. But before closing
                                                        2q
            this discussion of the special case of q = 1, several issues deserve comment. The

            first concerns the range of statistical methods that will be provided here for use with

            fractional factorials. The data analysis methods presented in this section and the next

            are confined to those for the identification of potential "few effects" descriptions of

            a p-factor situation. (For example, we do not go on to issues of inference under such

            a reduced model.) This stance is consistent with the fact that fractional factorials

            are primarily screening devices, useful for gaining some idea about which of many

            factors might be important. They are typically not suited (at least without additional

            data collection) to serve as the basis for detailed modeling of a response. The insights

            they provide must be seen as tentative and as steps along a path of learning about

            what factors influence a response.

                 A  second    matter   regards     the  sense  in  which      the  1   fractions   recommended    here
                                                            1                      2
            are  the  best  ones   possible.       Other    2  fractions  could        be  developed  (essentially  by

            using a product column of signs derived from levels of fewer than all p - 1 of

            the first factors to assign levels of the last one). But the alias structures associated

            with those alternatives are less attractive than the ones encountered in this section.

            That is, here main effects have been aliased with p - 1 factor interactions, 2-

            factor  interactions   with    p   -   2-factor    interactions,     and   so  on.  Any   other  1  fractions
                                                                                                             2
            fundamentally different from the ones discussed here would have main effects

            aliased with interactions of p - 2 or less factors. They would thus be more likely to

            produce data incapable of separating important effects. The "l order effects aliased

            with p - l order effects" structure of this section is simply the best one can do with
            a 2p-1 fractional factorial.

                 The last matter for discussion concerns what directions a follow-up investigation
            might take in order to resolve ambiguities left after a 2p-1 study is completed.

            Sometimes several different simple descriptions of system structure remain equally

            plausible  after  analysis     of  an  initial  1  fraction   of  a  full  factorial  study.  One   approach
                                                            2
                                                                                                   1
            to  resolving   these  is  to  complete     the    factorial  and    "run  the  other  2  fraction."

Example 11      A 24-1 Fabric Tenacity Study Followed Up by a Second 24-1 Study

                Researchers Johnson, Clapp, and Baqai, in "Understanding the Effect of Con-
                founding in Design of Experiments: A Case Study in High Speed Weaving"
                (Quality Engineering, 1989), discuss a study done to evaluate the effects of four
                two-level factors on a measure of woven fabric tenacity. The factors that were
                studied are indicated in Table 8.23.
608 Chapter 8 Inference for Full and Fractional Factorial Studies

Example 11     Table 8.23
 (continued )  Four Weaving Process Variables and Their Experimental Levels

               Factor Weaving Process Variable Factor Levels

                    A Side of Cloth (l. to r.)                     nozzle side (-) vs. opposite side (+)
                    B Yarn Type                                    air spun (-) vs. ring spun (+)
                    C Pick Density                                 35 ppi (-) vs. 50 ppi (+)
                    D Air Pressure                                 30 psi (-) vs. 45 psi (+)

                    Factor A reflects the left-to-right location on the fabric width from which a

               tested sample is taken. Factor C reflects a count of yarns per inch inserted in the

               cloth, top to bottom, during weaving. Factor D reflects the air pressure used to

               propel the yarn across the fabric width during weaving.
                    Initially, a replicated 24-1 study was done using the generator D  ABC.

               m = 5 pieces of cloth were tested for each of the eight different factor-level

               combinations studied. The resulting mean fabric tenacities y¯ , expressed in terms

               of strength per unit linear density, are given in Table 8.24. Although it is not
               absolutely clear in the article, it also appears that pooling the eight s2 values from

               the  1  fraction  gave  sP    1.16.
                    2
                    Apply the (three-cycle) Yates algorithm to the means listed in Table 8.24 (in

               the order given) and verify that the estimated sums of effects corresponding to

               the means in Table 8.24 are those given in Table 8.25.

                    Temporarily ignoring the existence of factor D, confidence intervals based on

               these estimates can be made using the m = 5 and p = 3 version of formula (8.13)

               from Section 8.2. That is, using 95% two-sided individual confidence intervals,

               since  = 8(5 - 1) = 32 degrees of freedom are associated with sP, a precision
               of roughly

                                             (2.04)(1.16)
                                             ±                     = ±.375
                                                    5·8

               should be associated with each of the estimates in Table 8.25. By this standard, the
               estimates corresponding to the A + BCD, AB + CD, C + ABD, and BC + AD

                       Table 8.24
                       Eight Sample Means from a 24-1 Fabric Tenacity Experiment

                       Combination y¯ (g/den.) Combination y¯ (g/den.)

                                 (1)         24.50                 cd        25.68

                                 ad          22.05                 ac        24.51

                                 bd          24.52                 bc        24.68

                                 ab          25.00                 abcd      24.23
8.3  Standard Fractions of Two-Level Factorials, Part I:  1  Fractions  609
                                                          2

     Table 8.25
     Estimated Sums of 24 Effects in a 24-1

     Fabric Weaving Experiment

     Estimate Sum of Effects Estimated

        24.396          µ.... +  2222
        -.449           2 +  222
                        2 +  222
           .211         22 +  22
           .456         2 + 222
           .379         22 + 22
           .044         22 + 22
        -.531           222 + 2
        -.276

sums are statistically significant. Two reasonably plausible and equally simple
tentative interpretations of this outcome are

     1. There are detectable A and C main effects and detectable 2-factor inter-
         actions of A with B and D.

     2. There are detectable A and C main effects and detectable 2-factor inter-
         actions of C with B and D.

(For that matter, there are others that you may well find as plausible as these two.)
     In any case, the ambiguities left by the collection of the data summarized in

Table 8.24 were unacceptable. To remedy the situation, the authors subsequently
completed the 24 factorial study by collecting data from the other eight combina-
tions defined by the generator D  -ABC. The means they obtained are given
in Table 8.26.

     One should honestly consider (and hopefully eliminate) the possibility that
there is a systematic difference between the values in Table 8.24 and in Table
8.26 as a result of some unknown factor or factors that changed in the time lapse
between the collection of the first block of observations and the second block. If

     Table 8.26
     Eight More Sample Means from a Second 24-1

     Fabric Tenacity Study

     Combination y¯ Combination y¯

     d           23.73  c                    24.63

     a           23.55  acd                  25.78

     b           25.98  bcd                  24.10

     abd         23.64  abc                  23.93
610 Chapter 8 Inference for Full and Fractional Factorial Studies

Example 11     that possibility can be eliminated, it would make sense to put together the two data
 (continued )  sets, treat them as a single full 24 factorial data set, and employ the methods of

               Section 8.2 in their analysis. (Some repetition of a combination or combinations

               included in the first study phase--e.g., the center point of the design--would have

               been advisable to allow at least a cursory check on the possibility of a systematic

               block effect.)

                    Johnson, Clapp, and Baqai don't say explicitly what sample sizes were used
               to produce the y¯ 's in Table 8.26. (Presumably, m = 5 was used.) Nor do they
               give a value for sP based on all 24 samples, so it is not possible to give a complete
               analysis of the full factorial data a` la Section 8.2. But it is possible to note what
               results from the use of the Yates algorithm with the full factorial set of y¯ 's. This
               is summarized in Table 8.27.

               Table 8.27
               Fitted Effects from the Full 24 Factorial Fabric Tenacity Study

               Effect Estimate          Effect                     Estimate

               µ....  y¯ .... = 24.407  2                          d2 = -.191
               2      a2 = -.321        22                         ad22 = .029
               2      b2 = .103          22                        bd22 = -.197
               22     ab22 = .011        222                       abd222 = .093
               2      c2 = .286          22                        cd22 = .446
               22     ac22 = .241        222                       acd222 = .108
                22    bc22 = -.561       222                       bcd222 = -.128
                222   abc222 = -.086     2222                      abcd2222 = -.011

                    The statistical significance of the entries of Table 8.27 will not be judged here.
               But note that the picture of fabric tenacity given by the fitted effects in this table is
               somewhat more complicated than either of the tentative descriptions derived from
               the original 24-1 study. The fitted effects, listed in order of decreasing absolute
               value, are

                                         BC, CD, A, C, AC, BD, D, . . . , etc.

               Although tentative description (2) (page 609) accounts for the first four of these,
               the A and C main effects indicated in Table 8.27 are not really as large as
               one might have guessed looking only at Table 8.25. Further, the AC 2-factor
               interaction appears from Table 8.27 to be nearly as large as the C main effect.
               This is obscured in the original 24-1 fractional factorial because the AC 2-factor
               interaction is aliased with an apparently fairly large BD 2-factor interaction of
               opposite sign.
                                      8.3     Standard Fractions of Two-Level Factorials, Part I:            1  Fractions  611
                                                                                                             2

                                      Ultimately, this example is one of a fairly complicated system of effects. It
                                 admirably illustrates the difficulties and even errors of interpretation that can arise
                                 when only fractional factorial data are available for use in studying such systems.

                                 In conclusion, it should be said that when a 2p-1 fractional factorial seems to

                             leave only very mild ambiguities of interpretation, it can be possible to resolve those

                             with the use of only a few additional data points (rather than requiring the addition

                             of  the  entire  other  1  fraction  of  combinations).  But  this  is  a  more    advanced   topic
                                                     2
                             than is sensibly discussed here. The interested reader can refer to Chapter 14 of

                             Daniel's Applications of Statistics to Industrial Experimentation for an illuminating

                             discussion of this matter.

Section 3 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. In a 25-1 study with defining relation I  ABCDE,                   Combination y Combination y
   it is possible for both the A main effect and the
   BCDE 4-factor interaction to be of large magnitude                 e               8.5               d       5.8
   but for both of them to go undetected. How might
   this quite easily happen?                                          a               7.9               ade     8.0

2. The paper "How to Optimize and Control the Wire                    b               7.7               bde     7.8
   Binding Process: Part I" by Scheaffer and Levine
   (Solid State Technology, November 1990) contains                   abe             8.7               abd     8.7
   the results of a 25-1 fractional factorial experiment
   with additional repeated center point, run in an ef-               c               9.0               cde     6.9
   fort to determine how to improve the operation of a
   K&S Model 1484 XQ wire bonder. The generator                       ace             9.2               acd     8.5
   E  ABCD was used in setting up the 25-1 part
   of the experiment involving the factors and levels                 bce             8.6               bcd     8.6
   indicated in the accompanying table.
                                                                      abc             9.5               abcde 8.3

Factor A  Constant Velocity  .6 in./sec (-) vs. 1.2 in./sec (+)       In addition, three runs were made at a constant
Factor B  Temperature               150C (-) vs. 200C (+)             velocity of .9 in./sec, a temperature of 175C, a
Factor C  Bond Force                    80 g (-) vs. 120 g (+)        bond force of 100 g, a power of 160 mW, and a
Factor D  Ultrasonic Power                                            bond time of 15 ms. These produced y values of
Factor E  Bond Time            120 mW (-) vs. 200 mW (+)              8.1, 8.6, and 8.1.
                                     10 ms (-) vs. 20 ms (+)          (a) Place the 16 observations from the 25-1 part

The response variable, y, was a force (in grams) re-                       of the experiment in Yates standard order as
quired to pull wire bonds made on the machine un-                          regards levels of factors A through D. Use the
der a particular combination of levels of the factors.                     four-cycle Yates algorithm to compute fitted
(Each y was actually an average of the pull forces                         sums of 25 effects. Identify what sum of effects
required on a 30 lead test sample.) The responses                          each of these estimates. (For example, the first
from the 25-1 part of the study were as follows:                           estimates µ..... +   22222.)
                                                                      (b) The three center points can be thought of as
                                                                           providing a pooled sample variance here. You
                                                                           may verify that sP = .29. If one then wishes to
                                                                           make confidence intervals for the sums of ef-
                                                                           fects, it is possible to use the m = 1, p = 4, and
612 Chapter 8 Inference for Full and Fractional Factorial Studies

         = 2 version of formula (8.13) of Section 8.2.             fitted 24 factorial effects for the study. Normal-
        What is the plus-or-minus value that comes
        from this program, for individual 95% two-                 plot these. What subject-matter interpretation
        sided confidence intervals? Using this value,
        which of the fitted sums of effects would you              of the data is suggested by the normal plot?
        judge to be statistically detectable? Does this
        list suggest to you any particularly simple/           Now suppose that instead of a full factorial study,
        intuitive description of how bond strength de-
        pends on the levels of the five factors?               only  the    1  fraction  with   generator   D        ABC    had
    (c) Based on your analysis from (b), if you had                         2
        to guess what levels of the factors A, C, and          been conducted.
        D should be used for high bond strength, what
        would you recommend? If the CE + ABD fit-              (b) Which 8 of the 16 treatment combinations
        ted sum reflects primarily the CE 2-factor in-
        teraction, what level of E then seems best?                would have been run? List these combinations
        Which of the combinations actually observed
        had these levels of factors A, C, D, and E? How            in Yates standard order as regards factors A,
        does its response compare to the others?
                                                                   B, and C and use the (three-cycle) Yates al-
3. Return to the fire retardant flame test study of Ex-
   ercise 3 of Section 8.2. The original study, summa-             gorithm to compute the 8 estimated sums of
   rized in that exercise, was a full 24 factorial study.
    (a) If you have not done so previously, use the                effects that it is possible to derive from these
        (four-cycle) Yates algorithm and compute the
                                                                   8 treatment combinations. Verify that each of

                                                                   these 8 estimates is the sum of two of your

                                                                   fitted effects from part (a). (For example, you

                                                                   should find that the first estimated sum here is

                                                                    y¯ .... + abcd2222 from part (a).)
                                                               (c) Normal-plot the last 7 of the estimated sums

                                                                   from (b). Interpret this plot. If you had only the
                                                                   data from this 24-1 fractional factorial, would

                                                                   your subject-matter conclusions be the same as
                                                                   those reached in part (a), based on the full 24

                                                                   data set?

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

8.4 Standard Fractions of Two-Level Factorials
       Part II: General 2p-q Studies

       Section  8.3  began  the  study                         of  fractional  factorials    with   the  1  fractions   of  2p
                                                                                                         2
       factorials, considering in turn the issues of (1) choice, (2) determination of the

       corresponding alias structure, and (3) data analysis. The approaches used to treat
       2 p-1
              studies extend naturally to the smaller                          1   fractions of 2p  factorials for q    >   1.
                                                                               2q
            This section first shows how the ideas of Section 8.3 are generalized to cover
       the general 2p-q situation. Then it considers the notion of design resolution and
       its implications for comparing alternative possible 2p-q plans. Next an introduction
       is given to how the 2p-q ideas can be employed where a blocking variable (or

       variables) dictate the use of a number of blocks equal to a power of 2. The section
       concludes with some comments regarding wise use of this 2p-q material.

8.4.1  Using 2p-q Fractional Factorials

       The  recommended     method                         of  choosing  a  1  fraction  of  a  2p  factorial  uses  a  column
                                                                            2
       of signs developed as products of plus and minus signs for all of the first p - 1

       factors. The key to understanding how the ideas of the previous section generalize
                      8.4 Standard Fractions of Two-Level Factorials Part II: General 2p-q Studies 613

                      to 41 , 81 , 161 , etc. fractions of 2p studies is to realize that there are several possible
                      similar columns that could be developed using only some of the first p - 1 factors.

                      When moving from  1  fractions to  1   fractions of 2p  factorials, one makes use of
                                        2                2q
                      such columns in assigning levels of the last q factors and then develops and uses an

                      alias structure consistent with the choice of columns.

                           For example, first consider the situation for cases where p - q = 3--that is,
                      where 23 = 8 different combinations of levels of p two-level factors are going to be

                      included in a study. A table of signs specifying all eight possible combinations of

                      levels of the first three factors A, B, and C, with four additional columns made up

                      as the possible products of the first three columns, is given in Table 8.28.

                           The final column of Table 8.28 can be used to choose levels of factor D for a
                      best possible 24-1 fractional factorial study. But it is also true that two or more of

   Choosing a 2p-q    the product columns in Table 8.28 can be used to choose levels of several additional
fractional factorial
                      factors (beyond the first three). If this is done, one winds up with a fractional factorial
     with p - q = 3
                      that can be understood in the same ways it is possible to make sense of the standard
                      2p-1 data structures discussed in Section 8.3.

                      Table 8.28
                      Signs for Specifying all Eight Combinations of Three Two-Level Factors
                      and Four Sets of Products of Those Signs

                      A B C AB Product AC Product BC Product ABC Product

                      ---                  +                 +                +               -

                      +--                  -                 -                +               +

                      -+-                  -                 +                -               +

                      ++-                  +                 -                -               -

                      --+                  +                 -                -               +

                      +-+                  -                 +                -               -

                      -++                  -                 -                +               -

                      +++                  +                 +                +               +

Example 12            A 26-3 Propellant Slurry Study

                      The text Probability and Statistics for Engineers and Scientists, by Walpole and
                      Myers, contains an interesting 26-3 fractional factorial data set taken originally
                      from the Proceedings of the 10th Conference on the Design of Experiments
                      in Army Research, Development and Testing (ARO-D Report 65-3). The study
                      investigated the effects of six two-level factors on X-ray intensity ratios associated
                      with a particular component of propellant mixtures in X-ray fluorescent analyses
                      of propellant slurry. Factors A, B, C, and D represent the concentrations (at low
                      and high levels) of four propellant components. Factors E and F represent the
                      weights (also at low and high levels) of fine and coarse particles present.
614 Chapter 8 Inference for Full and Fractional Factorial Studies

Example 12                    Eight different combinations of levels of factors A, B, C, D, E, and F were
 (continued )
                          each tested twice for X-ray intensity ratio, y. The eight combinations actually

                          included in the study can be thought of as follows. Using the columns of Table

                          8.28, levels of factor D were chosen using the signs in the ABC product column

                          directly; levels of factor E were chosen by reversing the signs in the BC product

                          column; and levels of factor F were chosen by reversing the signs of the AC prod-

                          uct column. Verify that such a prescription implies that the eight combinations

                          included in the study (written down in Yates order for factors A, B, and C) were

                          as displayed in Table 8.29. The eight combinations indicated in Table 8.29 are,

                          of  course,  1  of  the  64  different  possible  combinations  of  levels  of  the  six  factors.
                                       8

                                          Table 8.29
                                          Combinations Included in the 26-3 Propellant Slurry Study

                                          A B C F E D Combination Name

                                          ------                            (1)

                                          +--+-+                            adf

                                          -+--++                            bde

                                          ++-++-                            abef

                                          --++++                            cdef

                                          +-+-+-                            ace

                                          -+++--                            bcf

                                          +++--+                            abcd

                          The development of 2p-q fractional factorials has been illustrated with eight-

                      combination (i.e., p - q = 3) plans. But it should be obvious that there are 16-row,

                      32-row, 64-row, . . . , etc. versions of Table 8.28. Using any of these, one can assign

                      levels for the last q factors according to signs in product columns and end up with a

                      1   fraction of a full 2p    factorial plan. When this is done, the 2p  factorial effects are
                      2q
                      aliased in 2p-q groups of 2q effects each. The determination of this alias structure

 Determining the      can be made by using q generators to develop a defining relation for the fractional
                      factorial. A general definition of the notion of generators for a 2p-q fractional
     alias structure
of a 2p-q factorial   factorial is next.

Definition 9              When a 2p-q fractional factorial comes about by assigning levels of each of

                          the "last" q factors based on a different column of products of signs for the
                          "first" p - q factors, the q different relationships

                                       the name of an     ±        a product of names of some
                                       additional factor           of the first p - q factors
               8.4 Standard Fractions of Two-Level Factorials Part II: General 2p-q Studies 615

                  corresponding to how the combinations are chosen are called generators of
                  the plan.

                    Each generator can be translated into a statement with I on the left side and
               then taken individually, multiplied in pairs, multiplied in triples, and so on until the
               whole defining relation is developed. (See again Definition 8, page 602, for the
               meaning of this term.) In doing so, use can be made of the convention that minus
               any letter times minus that letter is I.

Example 12     In the Army propellant example, the q = 3 generators that led to the combinations
 (continued )  in Table 8.29 were

                         D  ABC
                         E  -BC
                         F  -AC

               Multiplying through by the left sides of these, one obtains the three relationships

                         I  ABCD                                              (8.25)
                         I  -BCE                                              (8.26)
                         I  -ACF                                              (8.27)

                    But in light of the conventions of formal multiplication, if I  ABCD and
               I  -BCE, it should also be the case that

                         I  (ABCD) · (-BCE)

               that is,

                         I  -ADE

               Similarly, using relationships (8.25) and (8.27), one obtains

                         I  -BDF

               using relationships (8.26) and (8.27), one obtains

                         I  ABEF
616 Chapter 8 Inference for Full and Fractional Factorial Studies

Example 12          and finally, using all three relationships (8.25), (8.26), and (8.27), one has
 (continued )                                                I  CDEF

                    Combining all of this, the complete defining relation for this 26-3 study is

                                           I  ABCD  -BCE  -ACF                                                   (8.28)
                                                -ADE  -BDF  ABEF  CDEF

                    Defining relation (8.28) is rather formidable, but it tells the whole truth about

                    what  can  be  learned  based     on  the  1  of  64  possible   combinations       of  six  two-level
                                                               8
                    factors. Relation (8.28) specifies all effects that will be aliased with the grand

                    mean. Appropriately multiplying through expression (8.28) gives all aliases of

                    any effect of interest. For example, multiplying through relation (8.28) by A gives

                    A  BCD  -ABCE  -CF  -DE  -ABDF  BEF  ACDEF

                    and for example, the (high level) A main effect will be indistinguishable from
                    minus the (all high levels) CF 2-factor interaction.

Data analysis for   With a 2p-q fractional factorial's defining relation in hand, the analysis of data
       a 2p-qstudy
                    proceeds  exactly  as  indicated  earlier  for    1  fractions.  It  is  necessary  to
                                                                      2

                    1. compute estimates of (sums and differences of) effects ignoring the last q
                        factors,

                    2. judge their statistical detectability using confidence interval or normal plot-
                        ting methods, and then

                    3. seek a plausible tentative interpretation of the important estimates in light of
                        the alias structure.

Example 12          In the Army propellant study, m = 2 trials for each of the 26-3 combinations listed
 (continued )       in Table 8.29 gave sP2 = .02005 and the sample averages listed in Table 8.30.

                         Temporarily ignoring all but the ("first") three factors A, B, and C (since

                    the levels of D, E and F were derived or generated from the levels of A, B

                    and C), the (three-cycle) Yates algorithm can be used on the sample means, as

                    shown in Table 8.31. Remember that the estimates in the next-to-last column

                    of Table 8.31 must be interpreted in light of the alias structure for the original

                    experimental plan. So for example, since (both from the original generators and
8.4 Standard Fractions of Two-Level Factorials Part II: General 2p-q Studies 617

        Table 8.30
        Eight Sample Means from the 26-3 Propellant Slurry Study

        Combination y¯             Combination  y¯

        (1)      1.1214             cdef        .9285

        adf      1.0712             ace         1.1635

        bde      .9415              bcf         .9561

        abef     1.1240             abcd        .9039

from relation (8.28)) one knows that D  ABC, the -.0650 value on the last
line of Table 8.31 is estimating

                 222 + 2 ± (six other effects)

So if one were expecting a large main effect of factor D, one would expect it to
be evident in the -.0650 value.

     Since a value of sP is available here, there is no need to resort to normal-
plotting to judge the statistical detectability of the values coming out of the
Yates algorithm. Instead (still temporarily calculating as if only the first three
factors were present) one can make confidence intervals based on the estimates,
by employing the  = 8 = 16 - 8, m = 2, and p = 3 version of formula (8.13)
from Section 8.2. That is, using 95% two-sided individual confidence intervals,
a precision of

                          .02005

                 ±2.306     2 · 23  = ±.0817

should be attached to each of the estimates in Table 8.31. By this standard,
none of the estimates from the propellant study are clearly different from 0. For

Table 8.31
The Yates Algorithm for a 23 Factorial Applied to the 26-3 Propellant Data

y¯ Cycle 1 Cycle 2 Cycle 3 Cycle 3 ÷ 8 Sum Estimated

1.1214  2.1926   4.2581   8.2101    1.0263      µ.... + aliases
1.0712  2.0655   3.9520                         2 + aliases
        2.0920              .3151     .0394     2 + aliases
 .9415  1.8600     .1323  -.3591    -.0449      22 + aliases
1.1240  -.0502     .1828  -.0545    -.0068      2 + aliases
 .9285    .1825  -.1271   -.3061    -.0383      22 + aliases
1.1635    .2350  -.2320                         22 + aliases
 .9561  -.0522     .2327    .0505     .0063     222 + aliases
 .9039           -.2872   -.1049    -.0131
                          -.5199    -.0650
618 Chapter 8 Inference for Full and Fractional Factorial Studies

Example 12     engineering purposes, the bottom line is that more data are needed before even
 (continued )  the most tentative conclusions about system behavior should be made.

Example 13     A 25-2 Catalyst Development Experiment

               Hansen and Best, in their paper "How to Pick a Winner" (presented at the 1986
               annual meeting of the American Statistical Association), described several in-
               dustrial experiments conducted in a research program aimed at the development
               of an effective catalyst for producing ethyleneamines by the amination of mo-
               noethanolamine. One of these was a partially replicated 25-2 fractional factorial
               study in which the response variable, y, was percent water produced during the
               reaction period. The five two-level experimental factors were as in Table 8.32.
               (The T-372 support was an alpha-alumina support and the T-869 support was a
               silica alumina support.)

                    The fractional factorial described by Hansen and Best has (q = 2) generators
               D  ABC and E  BC. The resulting defining relation (involving 22 = 4 strings
               of letters) is then

                                             I  ABCD  BCE  ADE

               where the fact that the ADE 3-factor interaction is aliased with the grand mean can
               be seen by multiplying together ABCD and BCE, which (from the generators)
               themselves represent effects aliased with the grand mean. Here one sees that
               effects will be aliased together in eight groups of four.

                    The data reported by Hansen and Best, and some corresponding summary
               statistics, are given in Table 8.33. The pooled sample variance derived from the
               values in Table 8.33 is

                             sP2 = (3 - 1)(2.543) + (2 - 1)(2.163) + (2 - 1)(.238) (3 - 1) + (2 - 1) + (2 - 1)
                                 = 1.872

               Table 8.32
               Five Catalysis Variables and Their Experimental Levels

               Factor Process Variable                             Levels

               A Ni/Re Ratio                                       2/1 (-) vs. 20/1 (+)

               B Precipitant                                       (NH4)2CO3 (-) vs. none (+)

               C Calcining Temperature 300C (-) vs. 500C (+)

               D Reduction Temperature 300C (-) vs. 500C (+)

               E Support Used                                      T-372 (-) vs. T-869 (+)
8.4 Standard Fractions of Two-Level Factorials Part II: General 2p-q Studies 619

          Table 8.33
          Data from a 25-2 Catalyst Study and Corresponding Sample

          Means and Variances

          Combination % Water Produced, y  y¯       s2

          e      8.70, 11.60, 9.00          9.7670  2.543
          ade          26.80               26.800    --
          bd           24.88               24.880    --
          ab           33.15               33.150    --
          cd                               29.940
          ac      28.90, 30.980            30.200   2.163
          bce          30.20               8.345     --
          abcde                            29.300   .238
                     8.00, 8.69                      --
                       29.30

with  = (3 - 1) + (2 - 1) + (2 - 1) = 4 associated degrees of freedom. The
corresponding pooled sample standard deviation is

                                    sP2 = 1.872 = 1.368

     So temporarily ignoring the existence of factors D and E, it is possible to use
the p = 3 version of formula (8.12) to derive precisions to attach to the estimates
(of sums of 25 factorial effects) that result from the use of the Yates algorithm on
the sample means in Table 8.33. That is, for 95% two-sided individual confidence
intervals, precisions of

that is,                     11 1 1 1 1 1 1 1
          ±2.776(1.368) 3 + + + + + + +

                            231112121
                                  ±1.195% water

can be attached to the estimates.
     The reader can verify that the (three-cycle) Yates algorithm applied to the

means in Table 8.33 gives the estimates in Table 8.34. Identifying those estimates
in Table 8.34 whose magnitudes make them statistically detectable according to
a criterion of ±1.195, there are (in order of decreasing magnitude)

                2 +  222 +  2222 +  22 estimated as 5.815
                22 + 22 + 2 +   22222 estimated as -5.495
                222 + 2 +  22 +   2222 estimated as 3.682
                22 +  22 +  222 +  222 estimated as 1.492
620 Chapter 8 Inference for Full and Fractional Factorial Studies

Example 13     Table 8.34
 (continued )  Estimates of Sums of Effects
               for the Catalyst Study

               Sum of Effects Estimated Estimate

               grand mean + aliases                                24.048
               A + aliases                                           5.815
               B + aliases                                          -.129
               AB + aliases                                          1.492
               C + aliases                                            .399
               AC + aliases                                         -.511
               BC + aliases                                        -5.495
               ABC + aliases                                         3.682

               The simplest possible tentative interpretation of the first two of these results is that
               the A and E main effects are large enough to see above the background variation.
               What to make of the third, given the first two, is not so clear. The large 3.682
               estimate can equally simply be tentatively attributed to a D main effect or to an
               AE 2-factor interaction. (Interestingly, Hansen and Best reported that subsequent
               experimentation was done with the purpose of determining the importance of the
               D main effect, and indeed, the importance of this factor in determining y was
               established.)

                    Exactly what to make of the fourth statistically significant estimate is even
               less clear. It is therefore comforting that, although big enough to be detectable,
               it is less than half the size of the third largest estimate. In the particular real
               situation, the authors seem to have found an "A, E, and D main effects only"
               description of y useful in subsequent work with the chemical system.

                    The reader may have noticed that the possibilities discussed in the previous
               example do not even exhaust the plausible interpretations of the fact that three
               estimated sums of effects are especially large. For example, "large DE 2-factor
               interactions and large D and E main effects" is yet another alternative possibility.
               This ambiguity serves to again emphasize the tentative nature of conclusions that
               can be drawn on the basis of small fractions of full factorials. And it also underlines
               the absolute necessity of subject-matter expertise and follow-up study in sorting out
               the possibilities in a real problem. There is simply no synthetic way to tell which of
               various simple alternative explanations suggested by a fractional factorial analysis
               is the right one.

8.4.2          Design Resolution

               The results of five different real applications of 2p-q plans have been discussed in
               Examples 9, 10, 11, 12, and 13. From them, it should be clear how important it is to
                   8.4 Standard Fractions of Two-Level Factorials Part II: General 2p-q Studies 621

  Good choice      have the simplest alias structure possible when it comes time to interpret the results
of a fractional    of a fractional factorial study. The object is to have low-order effects (like main
                   effects and 2-factor interactions) aliased not with other low-order effects, but rather
        factorial  only with high-order effects (many-factor interactions). It is the defining relation
                   that governs how the 2p factorial effects are divided up into groups of aliases. If
                   there are only long products of factor names appearing in the defining relation,
                   low-order effects are aliased only with high-order effects. On the other hand, if there
                   are short products of factor names appearing, there will be low-order effects aliased
                   with other low-order effects. As a kind of measure of quality of a 2p-q plan, it is
                   thus common to adopt the following notion of design resolution.

Definition 10      The resolution of a 2p-q fractional factorial plan is the number of letters in
                   the shortest product appearing in its defining relation.

                        In general, when contemplating the use of a 2p-q design, one wants the largest
                   resolution possible for a given investment in 2p-q combinations. Not all choices of

                   generators  give  the  same  resolution.  In  Section  8.3,  the  prescription  given  for  the  1
                                                                                                                    2
                   fractions was intended to give 2p-1 fractional factorials of resolution p (the largest
                   resolution possible). For general 2p-q studies, one must be a bit careful in choosing

                   generators. What seems like the most obvious choice need not be the best in terms

                   of resolution.

Example 14         Resolution 4 in a 26-2 Study
                   Consider planning a 26-2 study--that is, a study including 16 out of 64 possible
                   combinations of levels of factors A, B, C, D, E, and F. A rather natural choice of
                   two generators for such a study is

                                                           E  ABCD
                                                            F  ABC

                   The corresponding defining relation is

                                               I  ABCDE  ABCF  DEF

                   The resulting design is of resolution 3, and there are some main effects aliased
                   with (only) 2-factor interactions.

                        On the other hand, the perhaps slightly less natural choice of generators

                                                             E  BCD
                                                             F  ABC
622 Chapter 8 Inference for Full and Fractional Factorial Studies

Example 14     has defining relation
 (continued )
                                           I  BCDE  ABCF  ADEF

               and is of resolution 4. No main effect is aliased with any interaction of order less
               than 3. This second choice is better than the first in terms of resolution.

                    Table 8.35 indicates what is possible in terms of resolution for various numbers
               of factors and combinations for a 2p-q fractional factorial. The table was derived
               from a more detailed one on page 410 of Statistics for Experimenters by Box, Hunter,
               and Hunter, which gives not only the best resolutions possible but also generators for
               designs achieving those resolutions. The more limited information in Table 8.35 is
               sufficient for most purposes. Once one is sure what is possible, it is usually relatively
               painless to do the trial-and-error work needed to produce a plan of highest possible
               resolution. And it is probably worth doing as an exercise, to help one consider the
               pros and cons of various choices of generators for a given set of real factors.

                    Table 8.35 has no entries in the "8 combinations" row for more than 7 factors.
               If the table were extended beyond 11 factors, there would be no entries in the "16
               samples" row beyond 15 factors, no entries in the "32 samples" row beyond 31
               factors, etc. The reason for this should be obvious. For 8 combinations, there are
               only 7 columns total to use in Table 8.28. Corresponding tables for 16 combinations
               would have only 15 columns total, for 32 combinations only 31 columns total, etc.

                    As they have been described here, 2p-q fractional factorials can be used to study
               at most 2t - 1 factors in 2t samples. The cases of 7 factors in 8 combinations, 15
               factors in 16 combinations, 31 factors in 32 combinations, etc. represent a kind of
               extreme situation where a maximum number of factors is studied (at the price of
               creating a worst possible alias structure) in a given number of combinations. For the
               case of p = 7 factors in 8 combinations, effects are aliased in 27-4 = 8 groups of
               24 = 16; for the case of p = 15 factors in 16 combinations, the effects are aliased in
               215-11 = 16 groups of 211 = 2,048; etc. These extreme cases of 2t - 1 factors in 2t
               combinations are sometimes called saturated fractional factorials. They have very
               complicated alias structures and can support only the most tentative of conclusions.

               Table 8.35
               Best Resolutions Possible for Various Numbers of Combinations in a 2p-q Study

                                                                   Number of Factors ( p)

                                          4 5 6 7 8 9 10 11

               8 4333--------

                            Number of 16  544 4 3 3 3
               Combinations (2p-q ) 32         64 4 4 4 4

               64                                                  75 4 4 4

               128                                                 8655
            8.4 Standard Fractions of Two-Level Factorials Part II: General 2p-q Studies 623

Example 15  A 16-Run 15-Factor Process Development Study

            The article "What Every Technologist Should Know About Experimental Design"
            by C. Hendrix (Chemtech, 1979) includes the results from an unreplicated 16-run
            (saturated) 15-factor experiment. The response, y, was a measure of cold crack
            resistance for an industrial product. Experimental factors and levels were as listed
            in Table 8.36.

            Table 8.36
            15 Process Variables and Their Experimental Levels

            Factor  Process Variable               Levels

              A     Coating Roll Temperature       115 (-) vs. 125 (+)
              B     Solvent                        Recycled (-) vs. Refined (+)
              C     Polymer X-12 Preheat           No (-) vs. Yes (+)
              D     Web Type                       LX-14 (-) vs. LB-17 (+)
              E     Coating Roll Tension           30 (-) vs. 40 (+)
               F    Number of Chill Rolls          1 (-) vs. 2 (+)
              G     Drying Roll Temperature        75 (-) vs. 80 (+)
              H     Humidity of Air Feed to Dryer  75% (-) vs. 90% (+)
               J    Feed Air to Dryer Preheat      No (-) vs. Yes (+)
              K     Dibutylfutile in Formula       12% (-) vs. 15% (+)
              L     Surfactant in Formula          .5% (-) vs. 1% (+)
              M     Dispersant in Formula          .1% (-) vs. 2% (+)
              N     Wetting Agent in Formula       1.5% (-) vs. 2.5% (+)
              O     Time Lapse Before Coating Web  10 min (-) vs. 30 min (+)
               P    Mixer Agitation Speed          100 RPM (-) vs. 250 RPM (+)

                 The experimental plan used was defined by the q = 11 generators

               E  ABCD, F  BCD, G  ACD, H  ABC, J  ABD, K  CD,
               L  BD, M  AD, N  BC, O  AC, and P  AB

            The combinations actually run and the cold crack resistances observed are given
            in Table 8.37.

                 Ignoring all factors but A, B, C, and D, the combinations listed in Table 8.37
            are in Yates standard order and are therefore ready for use in finding estimates
            of sums of effects. Table 8.38 shows the results of using the (four-cycle) Yates
            algorithm on the 16 observations listed in Table 8.37. A normal plot of the last
            15 of these estimates is shown in Figure 8.13. It is clear from the figure that the
            two corresponding to B + aliases and F + aliases are detectably larger than
            the rest.
624 Chapter 8 Inference for Full and Fractional Factorial Studies

Example 15     Table 8.37
 (continued )  16 Experimental Combinations and Measured
               Cold Crack Resistances

               Combination y                                       Combination  y

               eklmnop                         14.8 dfgjnop                     17.8
               aghjkln
               bfhjkmo                         16.3 adefhmn                     18.9
               abefgkp
               cfghlmp                         23.5 bdeghlo                     23.1
               acefjlo
               bcegjmn                         23.9 abdjlmp                     21.8
               abchnop
                                               19.6 cdehjkp                     16.6

                                               18.6 acdgkmo                     16.7

                                               22.3 bcdfkln                     23.5

                                               22.2 abcdefghjklmnop 24.9

               Standard normal quantile   2.4                                   B + Aliases
                                          1.2
                                          0.0                     F + Aliases
                                         -1.2
                                                2
                                               2

                                  0.0 0.6 1.2 1.8 2.4
                                    Estimated sum of effects quantile

               Figure 8.13 Normal plot of estimated sums of effects in
               the 215-11 process development study

                    It is not feasible to write out the whole defining relation for this 215-11
               study. Effects are aliased in 2p-q = 215-11 = 16 groups of 2q = 211 = 2,048. In
               particular (though it would certainly be convenient if the 2.87 estimate in Table
               8.38 could be thought of as essentially representing 2), 2 has 2,047 aliases,
               some of them as simple as 2-factor interactions. By the same token, it would
               certainly be convenient if the small estimates in Table 8.38 were indicating that
               all summands of the sums of effects they represent were small. But the possibility
               of cancellation in the summation must not be overlooked.

                    The point is that only the most tentative description of this system should
               be drawn from even this very simple "two large estimates" outcome. The data
               in Table 8.37 hint at the primary importance of factors B and F in determining
               cold crack resistance, but the case is hardly airtight. There is a suggestion of
               a direction for further experimentation and discussion with process experts but
               certainly no detailed map of the countryside where one is going.
       8.4 Standard Fractions of Two-Level Factorials Part II: General 2p-q Studies 625

       Table 8.38
       Estimates of Sums of Effects for the 215-11

       Process Development Study

       Sum of Effects Represented    Estimate

       grand mean + aliases          20.28
       A + aliases                      .13
       B + aliases                    2.87
       P + aliases (including AB)
       C + aliases                   -.08
       O + aliases (including AC)       .27
       N + aliases (including BC)
       H + aliases (including ABC)   -.08
       D + aliases                   -.19
       M + aliases (including AD)
       L + aliases (including BD)       .36
       J + aliases (including ABD)      .13
       K + aliases (including CD)       .03
       G + aliases (including ACD)      .04
       F + aliases (including BCD)   -.06
       E + aliases (including ABCD)  -.26
                                        .29
                                      1.06
                                        .11

            One thing that can be said fairly conclusively on the basis of this study is
       that the analysis points out what is in retrospect obvious in Table 8.37. Consistent
       with the "B + aliases and F + aliases sums are positive and large" story told
       in Figure 8.13, the largest four values of y listed in Table 8.37 correspond to
       combinations where both B and F are at their high levels.

8.4.3  Two-Level Factorials and Fractional
       Factorials in Blocks (Optional )

       A somewhat specialized but occasionally useful adaptation of the 2p-q material
       presented here has to do with the analysis of full or fractional two-level factorial
       studies run in complete or incomplete blocks. When the number of blocks under
       consideration is itself a power of 2, clever use of the methods developed in this
       chapter can guide the choice of which combinations to place in incomplete blocks,
       as well as the analysis of data from both incomplete and complete block studies.

            The basic idea used is to formally represent one 2t -level factor "Blocks" as t
       "extra" two-level factors. One lets combinations of levels of these extra factors define
       the blocks into which combinations of levels of the factors of interest are placed.
       In data analysis, effects involving only the extra factors as Block main effects and
       effects involving both the extra factors and the factors of interest are recognized
626 Chapter 8 Inference for Full and Fractional Factorial Studies

                                  as Block×Treatment interactions. In carrying out this program, it is fairly common
                                  (though not necessarily safe) to operate as if the Block×Treatment interactions
                                  were all negligible. How choice and analysis of blocked 2p-q studies proceed will
                                  be illustrated with a series of three examples that are variations on Example 11.

            Example 16   A 24 Fabric Tenacity Study Run in Two Blocks
(Example 11 revisited )
                         In the weaving study of Johnson, Clapp, and Baqai, four factors--A, B, C,

                         and D--were studied. The discussion in Section 8.3 described how the authors
                         originally ran a replicated 24-1 fractional factorial with defining relation I 
                         ABCD. This was followed up later with a second 24-1 fractional factorial having

                         defining relation I  -ABCD, thus completing the full 24 factorial. However,

                         since  the  study  of   the  two   1   fractions  was  separated       in  time,  it  is  sensible  to
                                                            2
                         think of the two parts of the study as different blocks--that is, to think of a fifth

                         two-level factor (say, E) representing the time of observation.

                            How then to use the formal multiplication idea to understand the alias struc-

                         ture? Notice that there are 16 different samples and five factors for consideration.

                         This suggests that somehow (at least in formal terms) this situation might be
                         thought of as a 25-1 data structure. Further, the two formal expressions

                                                                I  ABCD                                            (8.29)
                                                                I  -ABCD                                           (8.30)

                         define the two sets of 8 out of 16 ABCD combinations actually run. These result
                         from a formal expression like

                                                                I  ABCDE                                           (8.31)

                         where E can be thought of as contributing either the plus or the minus signs in

                         expressions (8.29) and (8.30). If one calls block 1 (the first set of 8 samples) the

                         high level of E, expression (8.31) leads to exactly the I  ABCD 12 -fraction of
                         24 combinations of A, B, C, and D for use as block 1. And the I  -ABCD

                         1  -fraction  for  use  as  block  2.  This  can  be  seen  in  Table  8.39.
                         2
                            With factor E designating block number, the two columns of Table 8.39

                         taken together designate the I  ABCDE 12 -fraction of 25 A, B, C, D, and E
                         combinations. And (ignoring the e) the first column of Table 8.39 designates
                         the I  ABCD 12 -fraction of 24 A, B, C, and D combinations, while the second
                         designates the I  -ABCD 12 -fraction of 24 A, B, C, and D combinations.

                              Once it is clear that the Johnson, Clapp, and Baqai study can be thought

                         of in terms of expression (8.31) with the two-level blocking factor E, it is also

                         clear how any block effects will show up during data analysis. One temporarily
                         ignores the blocks and uses the Yates algorithm to compute fitted 24 factorial

                         effects. It is then necessary to remember, for example, that the fitted ABCD 4-

                         factor interaction reflects not only  2222 but any block main effects as well.
            8.4 Standard Fractions of Two-Level Factorials Part II: General 2p-q Studies 627

            Table 8.39
            A 25-1 Fractional Factorial or

            a 24 Factorial in Two Blocks

            Block 1                           Block 2

            e                                   a
            abe                                 b
            ace                                 c
            bce                                 abc
            ade                                 d
            bde                                 abc
            cde                                 acd
            abcde                               bcd

            And for example, any 2-factor interaction of A and blocks will be reflected in

            the fitted BCD 3-factor interaction. Of course, if all interactions with blocks are

            negligible, all fitted effects except that for the ABCD 4-factor interaction would
            indeed represent the appropriate 24 factorial effects.

Example 17  A 24 Factorial Run in Four Blocks

            For the sake of illustration, suppose that Johnson, Clapp, and Baqai had a priori
            planned to conduct a full 24 factorial set of ABCD combinations in four incom-
            plete blocks (of four combinations each). Consider how those blocks might have
            been chosen and how subsequent data analysis would have proceeded.

                 The one four-level factor Blocks can here be thought of in terms of the
            combinations of two extra two-level factors, which can be designated as E and F.
            In order to accommodate the original four factors and these two additional ones
            in 16 ABCDEF combinations, one must choose a 26-2 design by specifying two
            generators. The choices

                                              E  BCD   (8.32)
                                              F  ABC   (8.33)

            leading to the defining relation

            I  BCDE  ABCF  ADEF                        (8.34)

            will be used here. Table 8.40 indicates the 16 combinations of levels of factors A
            through F prescribed by the generators (8.32) and (8.33).
628 Chapter 8 Inference for Full and Fractional Factorial Studies

Example 17          The four different combinations of levels of E and F ((1), e, f, and ef) can be
 (continued )
               thought as designating in which block a given ABCD combination should appear.
               So generators (8.32) and (8.33) prescribe the division of the full 24 factorial (in

               the factors A through D) into the blocks indicated in Table 8.40 and Table 8.41.

                    As always, the defining relation (given here in display (8.34)) describes how
               effects are aliased. Table 8.42 indicates the aliases of each of the 24 factorial

               effects, obtained by multiplying through relation (8.34) by the various combina-

               tions of the letters A, B, C, and D. Notice from Table 8.42 that the BCD and ABC

               3-factor interactions are aliased with block main effects. So is the AD 2-factor

               Table 8.40
               16 Combinations of Levels of A through F

               A B C D E F Block Prescribed by Levels of E and F

               ---- --                                                      1

               +--- -+                                                      3

               -+-- ++                                                      4

               ++-- +-                                                      2

               --+- ++                                                      4

               +-+- +-                                                      2

               -++- --                                                      1

               +++- -+                                                      3

               ---+ +-                                                      2

               +--+ ++                                                      4

               -+-+ -+                                                      3

               ++-+ --                                                      1

               --++ -+                                                      3

               +-++ --                                                      1

               -+++ +-                                                      2

               ++++ ++                                                      4

               Table 8.41

               A 24 Factorial in Four Blocks
               (from a 26-2 Fractional Factorial)

               Block 1  Block 2                                    Block 3  Block 4

               (1)      ab                                         a         b
                                                                             c
               bc       ac                                         abc       ad
                                                                             abcd
               abd      d                                          bd

               acd      bcd                                        cd
            8.4 Standard Fractions of Two-Level Factorials Part II: General 2p-q Studies 629

                                              Table 8.42
                                              Aliases of the 24 Factorial Effects
                                              When Run in Four Blocks Prescribed
                                              by Generators (8.32) and (8.33)

                                            I  BCDE  ABCF  ADEF
                                            A  ABCDE  BCF  DEF
                                            B  CDE  ACF  ABDEF
                                            AB  ACDE  CF  BDEF
                                            C  BDE  ABF  ACDEF
                                            AC  ABDE  BF  CDEF
                                            BC  DE  AF  ABCDEF
                                            ABC  ADE  F  BCDEF
                                            D  BCE  ABCDF  AEF
                                            AD  ABCE  BCDF  EF
                                            BD  CE  ACDF  ABEF
                                            ABD  ACE  CDF  BEF
                                            CD  BE  ABDF  ACEF
                                            ACD  ABE  BDF  CEF
                                            BCD  E  ADF  ABCEF
                                            ABCD  AE  DF  BCEF

               interaction, since one of its aliases is EF, which involves only the two-level extra
               factors E and F used to represent the four-level factor Blocks. On the other hand,
               if interactions with Blocks are negligible, it is only these three of the 24 factorial
               effects that are aliased with other possibly nonnegligible effects. (For any other
               of the 24 factorial effects, each alias involves letters both from the group A, B, C,
               and D and also from the group E and F--and is therefore some kind of Block ×
               Treatment interaction.)

                    Analysis of data from a plan like that in Table 8.41 would proceed as indicated
               repeatedly in this chapter. The Yates algorithm applied to sample means listed
               in Yates standard order for factors A, B, C, and D produces estimates that are
               interpreted in light of the alias structure laid out in Table 8.42.

Example 18  A 24-1 Fractional Factorial Run in Four Blocks

            As  a  final  variant  on  the  4-factor  weaving  example,  consider  how  the  original  1
                                                                                                       2
            fraction of the 24 factorial might itself have been run in four incomplete blocks of

            two combinations. (Imagine that for some reason, only two combinations could

            be prepared on any single day and that there was some fear of Day effects related

            to environmental changes, instrument drift, etc.)
630 Chapter 8 Inference for Full and Fractional Factorial Studies

Example 18          Only eight combinations are to be chosen. In doing so, one needs to account
 (continued )
               for the four experimental factors A, B, C, and D and two extras E and F, which

               can be used to represent the four-level factor Blocks. Starting with the first three
               experimental factors A, B, and C (three of them because 23 = 8), one needs to
               choose three generators. The original 24-1 study had generator

                                                 D  ABC

               so it is natural to begin there. For the sake of example, consider also the generators

                                                      E  BC
                                                      F  AC

               These give the defining relation

               I  ABCD  BCE  ACF  ADE  BDF  ABEF  CDEF (8.35)

               and the prescribed set of combinations listed in Table 8.43. (The four different

               combinations of levels of E and F ((1), e, f, and ef) designate in which block a

               given  ABCD  combination  from    the  1            fraction  should  appear.)
                                                      2

                      Table 8.43
                      A 26-3 Fractional Factorial or a 24-1 Fractional Factorial

                      in Four Blocks

                      Block 1   Block 2                            Block 3           Block 4

                            ab           ade                       bdf               ef

                            cd           bce                       acf               abcdef

                    Some experimenting with relation (8.35) will show that all 2-factor inter-
               actions of the four original experimental factors A, B, C, and D are aliased not
               only with other 2-factor interactions of experimental factors but also with Block
               main effects. Thus, any systematic block-to-block changes would further confuse
               one's perception of 2-factor interactions of the experimental factors. But at least
               the main effects of A, B, C, and D are not aliased with Block main effects.

                    Examples 16 through 18 all treat situations where blocks are incomplete--in

               the sense that they don't each contain every combination of the experimental factors
               studied. Complete block plans with 2t blocks can also be developed and analyzed
               through the use of t "extra" two-level factors to represent the single (2t -level) factor

               Blocks. The path to be followed is by now worn enough through use in this chapter

               that further examples will not be included. But the reader should have no trouble
                8.4 Standard Fractions of Two-Level Factorials Part II: General 2p-q Studies 631

                figuring out, for example, how to analyze a full 24 factorial that is run completely
                once in each of two blocks, or even how to analyze a standard 24-1 fractional
                factorial that is run completely once in each of four blocks.

      8.4.4     Some Additional Comments

  Choice of     This 2p-q fractional factorial material is fascinating, and extremely useful when
experiment      used with a proper understanding of both its power and its limitations. However, an
                engineer who tries to use it in a cookbook fashion will usually wind up frustrated
          size  and disillusioned. The implications of aliasing must be thoroughly understood for
                successful use of the material. And a clear understanding of these implications will
                work to keep the engineer from routinely trying to study many factors based on very
                small amounts of data in a one-shot experimentation mode.

                     Engineers newly introduced to fractional factorial experimentation sometimes
                try to routinely draw final engineering conclusions about multifactor systems based
                on as few as eight data points. The folly of such a method of operation should be
                apparent. Economy of experimental effort involves not just collecting a small amount
                of data on a multifactor system, but rather collecting the minimum amount sufficient
                for a practically useful and reliable understanding of system behavior. Just a few
                expensive engineering errors, traceable to naive and overzealous use of fractional
                factorial experimentation, will easily negate any supposed savings generated by
                overly frugal data collection.

                     Although several 8-combination plans have been used as examples in this sec-
                tion, such designs are often too small to provide much information on the behavior of
                real engineering systems. Typically, 2p-q studies with p - q  4 are recommended
                as far more likely to lead to a satisfactory understanding of system behavior.

                     It has been said several times that when intelligently used as factor-screening
                tools, 2p-q fractional factorial studies will usually be followed up with more com-
                plete experimentation, such as a larger fraction or a complete factorial (often in a
                reduced set of factors). It is also true that techniques exist for choosing a relatively
                small second fraction in such a way as to resolve certain particular types of am-
                biguities of interpretation that can remain after the analysis of an initial fractional
                factorial. The interested reader can refer to Section 12.5 of Statistics for Experi-
                menters by Box, Hunter, and Hunter for discussions of how to choose an additional
                fraction to "dealias" a particular main effect and all its associated interactions or to
                "dealias" all main effects.

Section 4 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. What are the advantages and disadvantages of frac-     3. What is the principle of "sparsity of effects" and
   tional factorial experimentation in comparison to         how can it be used in the analysis of unreplicated
   factorial experimentation?                                2p and 2p-q experiments?

2. Under what circumstances can one hope to be suc-       4. In a 7-factor study, only 32 different combinations
   cessful experimenting with (say) 12 factors in (say)      of levels of (two-level factors) A, B, C, D, E, F, and
   16 experimental runs (i.e., based on 16 data points)?
632 Chapter 8 Inference for Full and Fractional Factorial Studies

G will be included, at least initially. The genera-         5. In a 25-2 study, where four sample sizes are 1 and
tors F  ABCD and G  ABCE will be used to                       four sample sizes are 2, sP = 5. If 90% two-sided
choose the 32 combinations to include in the study.            confidence limits are going to be used to judge
(a) Write out the whole defining relation for the              the statistical detectability of sums of effects, what
                                                               plus-or-minus value will be used?
     experiment that is contemplated here.
(b) Based on your answer to part (a), what effects          6. Consider planning, executing, and analyzing the re-
                                                               sults of a 26-2 fractional factorial experiment based
     will be aliased with the C main effect in the             on the two generators E  ABC and F  BCD.
     experiment that is being planned?                          (a) Write out the defining relation (i.e., the whole
(c) When running the experiment, what levels of                     list of aliases of the grand mean) for such a
     factors F and G are used when all of A, B, C,                  plan.
     D, and E are at their low levels? What levels             (b) When running the experiment, what levels of
     of factors F and G are used when A, B, and C                   factors E and F are used when all of A, B, C,
     are at their high levels and D and E are at their              and D are at their low levels? When A is at
     low levels?                                                    its high level but B, C, and D are at their low
(d) Suppose that after listing the data (observed                   levels?
     y's) in Yates standard order as regards factors            (c) Suppose that m = 3 data points from each of
     A, B, C, D, and E, you use the Yates algo-                     the 16 combinations of levels of factors (spec-
     rithm to compute 32 fitted sums of effects.                    ified by the generators) give a value of sP 
     Suppose further that the fitted values appear-                 2.00. If individual 90% two-sided confidence
     ing on the A + aliases, ABCD + aliases, and                    intervals are to be made to judge the statistical
     BCD + aliases rows of the Yates computations                   significance of the estimated (sums of) effects,
     are the only ones judged to be of both sta-                    what is the value of the plus-or-minus part of
     tistical significance and practical importance.                each of those intervals?
     What is the simplest possible interpretation of
     this result?

Chapter 8 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Return to the situation of Chapter Exercise 4 of         (a) Compute sP for the logged data. Individual
   Chapter 4. That exercise concerns some unrepli-               confidence intervals for the theoretical 23 ef-
   cated 23 factorial data taken from a study of the             fects are of the form E^ ± . Find if 95%
   mechanical properties of a polymer. If you have               individual two-sided intervals are of interest.
   not already done so, use the Yates algorithm to
   compute fitted 23 factorial effects for the data         (b) Based on your value from part (a), which
   given in that exercise. Then make a normal plot               of the factorial effects are statistically de-
   of the seven fitted effects a2, b2, . . . , abc222 as a       tectable? Considering only those effects that
   means of judging the statistical detectability of             are both statistically detectable and large
   the various effects on impact strength. Interpret             enough to have a material impact on the
   this plot.                                                    breaking strength, interpret the results of the
                                                                 students' experiment. (For example, if the A
2. Chapter Exercise 5 in Chapter 4 concerns a 23                 main effect is judged to be both detectable
   study of mechanical pencil lead strength done by              and of practical importance, what does mov-
   Timp and M-Sidek. Return to that exercise, and if             ing from the .3 diameter to the .7 diameter do
   you have not already done so, use the Yates algo-             to the breaking strength? Remember to trans-
   rithm to compute fitted 23 effects for the logged             late back from the log scale when making
   data.                                                         these interpretations.)
                                                                   Chapter 8 Exercises 633

    (c) Use the reverse Yates algorithm to produce            (c) Compare the Student main effects using indi-
        fitted ln(y) values for a few-effects model               vidual 95% two-sided confidence intervals.
        corresponding to your answer to (b). Use the
        fitted values to compute residuals (still on the     (d) Compare the Student main effects using si-
        log scale). Normal-plot these and plot them               multaneous 95% two-sided confidence inter-
        against levels of each of the three factors and           vals.
        against the fitted values, looking for obvious
        problems with the few-effects model.              4. The oil viscosity study of Dunnwald, Post, and
                                                             Kilcoin (referred to in Chapter Exercise 8 of
   (d) Based on your few-effects model, give a 95%           Chapter 7) was actually a 3 × 4 full factorial
        two-sided confidence interval for the mean           study. Some summary statistics for the entire data
        ln(y) that would be produced by the abc treat-       set are recorded in the accompanying tables. Sum-
        ment combination. By exponentiating the              marized are m = 10 measurements of the viscosi-
        endpoints of this interval, give a 95% two-          ties of each of four different weights of three dif-
        sided confidence interval for the median num-        ferent brands of motor oil at room temperature.
        ber of clips required to break a piece of lead       Units are seconds required for a ball to drop a
        under this set of conditions.                        particular distance through the oil.

3. The following are the weights recorded by I = 3        Brand M    10W30          SAE 30
   different students when weighing the same nomi-        Brand C
   nally 5 g mass with J = 2 different scales m = 2       Brand H  y¯ 11 = 1.385  y¯ 12 = 2.066
   times apiece. (They are part of the much larger                 s11 = .091     s12 = .097
   data set given in Chapter Exercise 5 of Chapter 3.)
                                                                   y¯ 21 = 1.319  y¯ 22 = 2.002
Student 1  Scale 1     Scale 2                                     s21 = .088     s22 = .089
Student 2
Student 3  5.03, 5.02  5.07, 5.09                                  y¯ 31 = 1.344  y¯ 32 = 2.049
           5.03, 5.01  5.02, 5.07                                  s31 = .066     s32 = .089
           5.06, 5.00  5.10, 5.08

Corresponding fitted factorial effects are: a1 =          Brand M    10W40          20W50
.00417, a2 = -.01583, a3 = .01167, b1 =                   Brand C
-.02333, b2 = .02333, ab11 = -.00417, ab12 =              Brand H  y¯ 13 = 1.414  y¯ 14 = 4.498
.00417, ab21 = .01083, ab22 = -.01083, ab31 =                      s13 = .150     s14 = .204
-.00667, and ab32 = .00667. Further, a pooled
standard deviation is sP = .02483.                                 y¯ 23 = 1.415  y¯ 24 = 4.662
(a) To enhance an interaction plot of sample                       s23 = .115     s24 = .151

     means with error bars derived from 95% two-                   y¯ 33 = 1.544  y¯ 34 = 4.549
     sided individual confidence limits for the                    s33 = .068     s34 = .171
     mean weights, what plus-or-minus value
     would be used to make those error bars? Make         (a) Find the pooled sample standard deviation
     such a plot and discuss the likely statistical            here. What are the associated degrees of free-
     detectability of the interactions.                        dom?
(b) Individual 95% two-sided confidence limits
     for the interactions i j are of the form abi j ±     (b) Make an interaction plot of sample means.
                                                               Enhance this plot by adding error bars derived
       . Find here. Based on this, are the inter-              from 99% individual confidence intervals for
     actions statistically detectable?                         the cell means. Does it appear that there are
                                                               important and statistically detectable interac-
                                                               tions here?
634 Chapter 8 Inference for Full and Fractional Factorial Studies

    (c) If the Tukey method is used to find simulta-        .55 Water/Cement Ratio      0C             10C
        neous 95% two-sided confidence intervals for        .35 Water/Cement Ratio
        all differences in Brand main effects, the in-                              y¯ 11 = 28.99  y¯ 12 = 30.24
        tervals produced are of the form y¯ i. - y¯ i . ±                           s11 = .91      s12 = 1.26
           . Find .
                                                                                    y¯ 21 = 38.70  y¯ 22 = 36.16
   (d) If the Tukey method is used to find simulta-                                 s21 = .77      s22 = 1.92
        neous 95% two-sided confidence intervals for
        all differences in Weight main effects, the in-     .55 Water/Cement Ratio      20C            30C
        tervals produced are of the form y¯ . j - y¯ . j ±  .35 Water/Cement Ratio
           . Find .                                                                 y¯ 13 = 33.99  y¯ 14 = 36.02
                                                                                    s13 = 1.85     s14 = .93
    (e) Based on your answers to (c) and (d), would
        you say that there are statistically detectable                             y¯ 23 = 40.18  y¯ 24 = 42.36
        Brand and/or Weight main effects on viscos-                                 s23 = 2.86     s24 = 1.35
        ity?
                                                                (a) Find the pooled sample standard deviation
    (f) We strongly suspect that the "m = 10" vis-                  here. What are the associated degrees of free-
        cosity measurements made for each of the                    dom?
        12 Brand/Weight combinations were made on
        oil from a single quart of that type of oil. If        (b) Make an interaction plot of sample means.
        this is the case, sP, the baseline measure of               Enhance this plot by adding error bars derived
        variability, is measuring only the variability              from simultaneous 95% confidence intervals
        associated with experimental technique (not,                for the cell means. Does it appear that there
        for example, from quart to quart of a given                 are important and statistically detectable in-
        type of oil). One might thus argue that the                 teractions here? What practical implications
        real-world inferences to be made, properly                  would this have for a cold-climate civil engi-
        speaking, extend only to the particular quarts              neer?
        used in the study. Discuss how these inter-
        pretations (of sP and the extent of statistically       (c) Compute the fitted factorial effects from the
        based inferences) would be different if in fact             eight sample means.
        the students used different quarts of oil in
        producing the "m = 10" different viscosity             (d) If one wished to make individual 95% confi-
        measurements in each cell.                                  dence intervals for the Ratio × Temperature
                                                                    interactions i j , these would be of the form
5. The article "Effect of Temperature on the Early-                 abi j ± , for an appropriate value of . Find
   Age Properties of Type I, Type III and Type I/Fly                this . Based on this value, do you judge
   Ash Concretes" by N. Gardner (ACI Materials                      any of the interactions to be statistically de-
   Journal, 1990) contains summary statistics for a                 tectable?
   very large study of the properties of several con-
   cretes under a variety of curing conditions. The         6. The same article referred to in Exercise 5 reported
   accompanying tables present some of the statis-             summary statistics (similar to the ones for Type I
   tics from that paper. Given here are the sample             cement/fly ash concrete) for the 14-day compres-
   means and standard deviations of 14-day com-                sive strengths of Type III cement concrete. These
   pressive strengths for m = 5 specimens of Type              are shown in the accompanying tables.
   I cement/fly ash concrete for all possible combi-
   nations of I = 2 water-cement ratios and J = 4
   curing temperatures. The units are MPa.
                                                                             Chapter 8 Exercises 635

.55 Water/Cement Ratio      0C             10C              interactions, but that for the Type III cement con-
.35 Water/Cement Ratio                                      crete there are not important Ratio × Temperature
                        y¯ 11 = 47.82  y¯ 12 = 42.75        interactions. Taking the whole data set from both
                        s11 = 4.03     s12 = 2.96           exercises together (both concrete types), would
                                                            there be important (3-factor) Type × Ratio ×
                        y¯ 21 = 42.14  y¯ 22 = 36.72        Temperature interactions? Explain.
                        s21 = 2.64     s22 = 3.03
                                                         8. The ISU M.S. thesis, "An Accelerated Engine
.55 Water/Cement Ratio      20C            30C              Test for Crankshaft and Bearing Compatibility,"
.35 Water/Cement Ratio                                      by P. Honan, discusses an industrial experiment
                        y¯ 13 = 42.38  y¯ 14 = 43.45        run to investigate the effects of three factors on
                        s13 = 2.62     s14 = 1.80           the wear of engine bearings. The factors and lev-
                                                            els shown here were used in a 100-hour, 20-step
                        y¯ 23 = 36.72  y¯ 24 = 37.70        engine probe test.
                        s23 = 1.51     s24 = .89
                                                         A Crankshaft Material cast nodular iron (-)
                                                                                             vs. forged steel (+)

    (a) Find the pooled sample standard deviation        B Bearing Material  aluminum (-)
        here. What are the associated degrees of free-                       vs. copper/lead (+)
        dom? (m = 5, as in Exercise 5.)
                                                         C Debris Added to Oil none (-) vs. 5.5 g SAE
   (b) Make an interaction plot of sample means                                              fine dust every 25 hours (+)
        useful for investigating the size of Ratio ×
        Temperature interactions. Enhance this plot      Two response variables were measured:
        by adding error bars derived from simulta-
        neous 95% confidence intervals for the cell                 y1 = rod journal wear (µm)
        means. Does it appear that there are impor-                 y2 = main journal wear (µm)
        tant and statistically detectable interactions
        here? What practical implications would this     The values of y1 and y2 reported by Honan are as
        have for a cold-climate civil engineer?          follows.

    (c) Compute the fitted factorial effects from the    Combination y1 y2 Combination y1                          y2
        eight sample means.
                                                         (1)  2.7 5.6        c    3.1 3.2
   (d) If one wished to make individual 95% confi-
        dence intervals for the Ratio × Temperature      a    .9 1.4         ac   18.6 27.3
        interactions i j , these would be of the form
        abi j ± , for an appropriate value of . Find     b    3.0 7.1        bc   2.5 6.0
        this . Based on this value, do you judge
        any of the interactions to be statistically de-  ab   1.1 1.6        abc  60.3 99.7
        tectable?
                                                         (a) Use the Yates algorithm and compute the fit-
    (e) Give and interpret a 90% confidence interval          ted effects of the three experimental factors on
        for the difference in water/cement ratio main         both the rod and main bearing wear figures.
        effects, 2 - 1. How would this be of prac-
        tical use to a cold-climate civil engineer?      (b) Because there was no replication in this rela-
                                                              tively expensive industrial experiment, there
7. Suppose that in the context of Exercises 5 and             is no real option for judging the statistical
   6, you judge that for the Type I cement/fly ash            significance of the 23 factorial effects except
   concrete there are important Ratio × Temperature           the use of normal-plotting. Make normal plots
636 Chapter 8 Inference for Full and Fractional Factorial Studies

        of the seven fitted effects, a2, b2, . . . , abc222  establishing a pattern of random-looking variation
        for both response variables. Do these iden-
        tify one or two of the 23 factorial effects as       and identifies any of the larger values plotting off
        clearly larger than the others? How hopeful
        are you that there is a simple, intuitively ap-      a line on the plot established by the small values
        pealing few-effects description of the effects
        of factors A, B, and C on y1 and y2?                 as detectably larger than the others.
    (c) Your normal plots from (b) ought to each have
        an interesting gap in the middle of the plot.        (a) Redo part (a) of Exercise 2 of Section 8.2 us-
        Explain the origin of both that gap and the
        fact that all of your fitted effects should be          ing a half normal plot of the absolute values of
        positive, in terms of the relative magnitudes of
        the responses listed. (How, for example, does           the fitted effects. (Your ith plotted point will
        the response for combination abc enter into
        the calculation of the various fitted effects?)         have a horizontal coordinate equal to the ith
   (d) One simple way to describe the outcomes ob-
        tained in this study is as having one very big          smallest absolute fitted effect and a vertical
        response and one moderately big response.                                                   i -.5
        Is there much chance that this pattern in y1            coordinate   equal   to  the  p  =   15    half  normal
        and y2 is in fact due only to random variation
        (i.e., that none of the factors have any effect         quantile.) Are the conclusions about the sta-
        here)? Make a normal plot of the raw y1 val-
        ues and one for the raw y2 values to support            tistical detectability of effects here the same
        your answer.
                                                                as those you reached in Exercise 2 of Sec-
9. There is a certain degree of arbitrariness in the
   choice to use signs on the fitted effects corre-             tion 8.2?
   sponding to the "all high treatment" combination
   when normal-plotting fitted 2p factorial effects.         (b) Redo Exercise 1 here using a half normal plot
   This can be eliminated by probability plotting the
   absolute values of the fitted effects and using not          of the absolute values of the fitted effects.
   standard normal quantiles but rather quantiles for
   the distribution of the absolute value of a standard         (Your ith plotted point will have a horizontal
   normal random variable. This notion is called half
   normal-plotting the absolute fitted effects, since           coordinate equal to the ith smallest absolute
   the probability density of the absolute value of a
   standard normal variable looks like the right half           fitted effect and a vertical coordinate equal to
   of the standard normal density (multiplied by 2).                       i -.5
   The half normal quantiles are related to the stan-           the  p  =    7    half  normal    quantile.)     Are  the
   dard normal quantiles via
                                                                conclusions about the statistical detectability
                    Q(p) = Qz 1 + p
                                        2                       of effects here the same as those you reached

   and one interprets a half normal plot in essentially         in Exercise 1?
   the same way that a normal plot is interpreted.
   That is, one thinks of the smaller plotted values as      10. The text Engineering Statistics by Hogg and Led-
                                                                  olter contains an account (due originally to R.
                                                                  Snee) of a partially replicated 23 factorial indus-
                                                                  trial experiment. Under investigation were the ef-
                                                                  fects of the following factors and levels on the
                                                                  percentage impurity, y, in a chemical product:

                                                             A Polymer Type              standard (-) vs.
                                                                                         new (but expensive) (+)

                                                             B Polymer Concentration .01% (-) vs. .04% (+)

                                                             C Amount of an Additive 2 lb (-) vs. 12 lb (+)

                                                             The data that were obtained are as follows:

                                                             Combination y (%)           Combination y (%)

                                                             (1)             1.0              c            .9, .7

                                                             a             1.0, 1.2           ac           1.1

                                                             b               .2               bc           .2, .3

                                                             ab              .5               abc             .5
                                                                               Chapter 8 Exercises 637

     (a) Compute the fitted 23 factorial effects corre-   of factors A, B, C, and D gave the accompanying
          sponding to the "all high treatment" combi-     y¯ and s2 values.
          nation.
                                                          Combination y¯  s2 Combination  y¯    s2
     (b) Compute the pooled sample standard devia-
          tion, sP.                                       (1)  28.4 97.6       d          36.8 146.4

     (c) Use your value of sP from (b) and find the       a    21.9 15.1       ad         19.2 24.8
          plus-or-minus part of 90% individual two-
          sided confidence limits for the 23 factorial    b    20.2       5.1  bd         19.9  5.7
          effects.
                                                          ab   14.3 61.1       abd        22.5 22.5
     (d) Based on your calculation in (c), which of the
          effects do you judge to be detectable in this   c    30.4 43.5       cd         25.5 53.4
          23 study?
                                                          ac   25.1 96.2       acd        21.5 56.6
     (e) Write a paragraph or two for your engineer-
          ing manager, summarizing the results of this    bc   38.2 100.8      bcd        22.0 10.4
          experiment and making recommendations for
          the future running of this process. (Remem-     abc  12.8 23.6       abcd       22.5 123.8
          ber that you want low y and, all else being
          equal, low production cost.)                         (a) Compute the pooled sample standard devia-
                                                                    tion. What does it measure in the present con-
11. The article "Use of Factorial Designs in the De-                text? (Variability in hour-to-hour missed lead
     velopment of Lighting Products" by J. Scheesley                counts? Variability in shift-to-shift missed
     (Experiments in Industry: Design, Analysis and                 lead per hour figures?)
     Interpretation of Results, American Society for
     Quality Control, 1985) discusses a large indus-           (b) Use the Yates algorithm and compute the fit-
     trial experiment intended to compare the use of                ted 24 factorial effects.
     two different types of lead wire in the manufac-
     ture of incandescent light bulbs under a variety of       (c) Which of the effects are statistically detectable
     plant circumstances. The primary response vari-                here? (Use individual two-sided 98% confi-
     able in the study was                                          dence limits for the effects to make this de-
                                                                    termination.) Is there a simple interpretation
y = average number of leads missed per hour                         of this set of effects?
     (because of misfeeds into automatic
     assembly equipment)                                       (d) Would you be willing to say, on the basis of
                                                                    your analysis in (a) through (c), that the new
which was measured and recorded on the basis of                     lead type will provide an overall reduction in
                                                                    the number of missed leads? Explain.
eight-hour shifts. Consider here only part of the
original data, which may be thought of as having               (e) Would you be willing to say, on the basis of
replicated 24 factorial structure. That is, consider                your analysis in (a) through (c), that a switch
                                                                    to the new lead type will provide a reduction
the following factors and levels:                                   in missed leads for every set of plant circum-
                                                                    stances? Explain.
A Lead Type  standard (-) vs. new (+)
                                                          12. DeBlieck, Rohach, Topf, and Wilcox conducted
B Plant      1 (-) vs. 2 (+)                                   a replicated 3 × 3 factorial study of the uniaxial
                                                               force required to buckle household cans. A single
C Machine Type standard (-) vs. high speed (+)                 brand of cola cans, a single brand of beer cans, and
                                                               a single brand of soup cans were used in the study.
D Shift      1st (-) vs. 2nd (+)                               The cans were prepared by bringing them to 0C,
                                                               22C, or 200C before testing. The forces required
                                                               to buckle each of m = 3 cans for the nine different
                                                               Can Type/Temperature combinations follow.

m = 4 values of y (each requiring an eight-hour
shift to produce) for each combination of levels
638 Chapter 8 Inference for Full and Fractional Factorial Studies

Can Type  Temperature  Force Required, y (lb)               ure. Some summary statistics from the study are
                                                            presented in the accompanying table.
cola            0C     174, 306, 192
cola           22C     150, 188, 125                                                  1  Factor B Width
cola         200C      200, 198, 204                        Factor A Brand
beer                   234, 246, 300                                                         1 narrow
beer            0C     204, 339, 254                                                  2       (<2 mm)
beer           22C     414, 200, 286
soup         200C      570, 704, 632                                                  1   y¯ 11 = 2.811 kg
soup                   667, 593, 647                        Factor A Brand                s11 = .0453 kg
soup            0C     600, 620, 596
               22C                                                                    2   y¯ 21 = 2.459 kg
             200C                                                                         s21 = .4697 kg
                                                                                      1
     (a) Make an interaction plot of the nine combi-        Factor A Brand                  2 medium
          nation sample means. Enhance it with error                                          (3.5 mm)
          bars derived using 98% individual two-sided                                 2
          confidence intervals.                                                           y¯ 12 = 4.164 kg
                                                                                          s12 = .2490 kg
     (b) Compute the fitted main effects and inter-
          actions from the nine combination sample                                        y¯ 22 = 4.111 kg
          means. Use these to make individual 98%                                         s22 = .1030 kg
          confidence intervals for all of the main effects
          and interactions in this 3 × 3 factorial study.                                     3 wide
          What do these indicate about the detectability                                      (5.5 mm)
          of the various effects?
                                                                                          y¯ 13 = 8.001 kg
     (c) Use Tukey's method for simultaneous com-                                         s13 = .8556 kg
          parison of main effects and give simultaneous
          99% confidence intervals for all differences in                                 y¯ 23 = 6.346 kg
          Can Type main effects. Then use the same                                        s23 = .1924 kg
          method and give simultaneous 99% confi-
          dence intervals for all differences in Temper-    (a) Compute sP for the rubber band strength data.
          ature main effects.                                    What is this supposed to measure?

13. Consider again the 24 factorial data set in Chapter     (b) Make an interaction plot of sample means.
     Exercise 20 of Chapter 4. (Paper airplane flight            Use error bars for the means calculated from
     distances collected by K. Fellows were studied              95% two-sided individual confidence limits.
     there.) As a means of making the evaluation of              (Make use of your value of sP.)
     which of the fitted effects produced by the Yates
     algorithm appear to be detectable, normal-plot the     (c) Based on your plot from (b), which fac-
     fitted effects. Interpret the plot.                         torial effects appear to be distinguishable
                                                                 from background noise? (Brand main effects?
14. Boston, Franzen, and Hoefer conducted a 2 × 3                Width main effects? Brand × Width interac-
     factorial study of the strengths of rubber bands.           tions?)
     Two different brands of bands were studied. From
     both companies, bands of three different widths        (d) Compute all of the fitted factorial effects for
     were used. For each Brand/Width combination,                the strength data. (Find the ai 's, the bj 's, and
     the strengths of m = 5 bands of length 18-20 cm             the abi j 's defined in Section 4.3.)
     were determined by loading the bands till fail-
                                                            (e) To find individual 95% confidence intervals
                                                                 for the interactions i j , intervals of the form
          abi j ± are appropriate. Find . Based on                                                Chapter 8 Exercises 639
          this value, are there statistically detectable
                                                                           effects (in comparison to the other four, for
          interactions here? How does this conclusion                      example).
                                                                      (d) Why might it be well argued that the choice
          compare with your more qualitative answer                        D  ABC is superior to the choice D 
                                                                           AB?
          to part (c)?
                                                                 16. p = 5 factors A, B, C, D, and E are to be stud-
     (f) To compare Width main effects, confidence                    ied in a 25-2 fractional factorial study. The two
                                                                      generators D  AB and E  AC are to be used
         intervals for the differences j - j are in                   in choosing the eight ABCDE combinations to be
         order. Find individual 95% two-sided con-                    included in the study.
                                                                      (a) Give the list of eight different combinations
          fidence intervals for 1 - 2, 1 - 3, and                          of levels of the factors that will be included
          2 - 3. Based on these, are there statis-                         in the study. (Use the convention of naming,
          tically detectable Width main effects here?                      for each sample, those factors that should be
                                                                           set at their high levels.)
          How does this compare with your answer to                   (b) Give the list of all effects aliased with the
                                                                           A main effect if this experimental plan is
          part (c)?                                                        adopted.

     (g) Redo part (f), this time using simultaneous             17. The following are eight sample means listed in
                                                                      Yates standard order (left to right), considering
          95% two-sided confidence intervals.                         levels of three two-level factors A, B, and C:

15.  In Section 8.3, you were advised to choose       1  frac-                    70, 61, 72, 59, 68, 64, 69, 69
                                                      2
     tions of 2p factorials by using the generator                    (a) Use the Yates algorithm here to compute eight
                                                                           estimates of effects from the sample means.
          last factor  product of all other factors
                                                                      (b) Temporarily suppose that no value for sP is
     For  example,   this  means  that  in  choosing  1  of  24            available. Make a plot appropriate to identify-
                                                      2                    ing those estimates from (a) that are likely to
     possible combinations of levels of factors A, B, C,                   represent something more than background
                                                                           noise. Based on the appearance of your plot,
     and D, you were advised to use the generator D                        which if any of the estimated effects are
                                                                           clearly representing something more than
     ABC. There are other possibilities. For example,                      background noise?

     you could use the generator D  AB.                               (c) As it turned out, sP = .9, based on m = 2
                                                                           observations at each of the eight different
     (a) Using this alternative plan (specified by D                       sets of conditions. Based on 95% individual
                                                                           two-sided confidence intervals for the under-
          AB), what eight different combinations of                        lying effects estimated from the eight y¯ 's,
                                                                           which estimated effects are clearly represent-
          factor levels would be run? (Use the standard                    ing something other than background noise?
                                                                           (If confidence intervals E^ ± were to be
          naming convention, listing for each of the                       made, show the calculation of and state
                                                                           which estimated effects are clearly represent-
          eight sets of experimental conditions to be run                  ing more than noise.)

          those factors appearing at their high levels.)

     (b) For the alternative plan specified by D  AB,

          list all eight pairs of effects of factors A, B,

          C, and D that would be aliased. (You may,

          if you wish, list eight sums of the effects

          µ...., 2, 2, 22, 2, . . . etc. that can be esti-
          mated.)

     (c) Suppose that in an analysis of data from an

          experiment run according to the alternative

          plan (with D  AB), the Yates algorithm is

          used with y¯ 's listed according to Yates stan-

          dard order for factors A, B, and C. Give four

          equally plausible interpretations of the even-

          tuality that the first four lines of the Yates

          calculations produce large estimated sums of
640 Chapter 8 Inference for Full and Fractional Factorial Studies

     Still considering the eight sample means, hence-        were studied. (The actual levels of the factors em-
     forth suppose that by some criteria, only the es-       ployed were not given in the article.) The combi-
     timates ending up on the first, second, and sixth       nations studied and the values of y that resulted
     lines of the Yates calculations are considered to       are given next.
     be both statistically detectable and of practical
     importance.                                             Combination y            Combination y
     (d) If in fact the eight y¯ 's came from a (4-factor)
                                                             (1)             .037      de               .351
          24-1 experiment with generator D  ABC,
          how would one typically interpret the result       a               .040      ade              .360
          that the first, second, and sixth lines of the
          Yates calculations (for means in standard or-      b               .014      bde              .329
          der for factors A, B, and C) give statistically
          detectable and practically important values?       ab              .042      abde             .173
     (e) If in fact the eight y¯ 's came from a (5-factor)
          25-2 experiment with generators D  ABC             ce              .063      cd               .372
          and E  AC, how would one typically inter-
          pret the result that the first, second, and sixth  ace             .100      acd              .184
          lines of the Yates calculations (for means in
          standard order for factors A, B, and C) give       bce             .067      bcd              .158
          statistically detectable and practically impor-
          tant values?                                       abce            .026      abcd             .131

18. A production engineer who wishes to study six            Kenett and Vogel were apparently called in after
     two-level factors in eight experimental runs de-
     cides to use the generators D  AB, E  AC,               the fact of experimentation to help analyze this
     and F  BC in planning a 26-3 fractional facto-
     rial experiment.                                        nonstandard  1  fraction  of  the    full  25  factorial.
     (a) What eight combinations of levels of the six                     2
          factors will be run? (Name them using the          The recommendations of Section 8.3 were not
          usual convention of prescribing for each run
          which of the factors will appear at their high     followed in choosing which 16 of the 32 possible
          levels.)
     (b) What seven other effects will be aliased with       combinations of levels of factors A through E to
          the A main effect in the engineer's study?
                                                             include in the wave soldering study. In fact, the
19. The article "Going Beyond Main-Effect Plots" by
     Kenett and Vogel (Quality Progress, 1991) out-          generator E  -CD was apparently employed.
     lines the results of a 25-1 fractional factorial in-
     dustrial experiment concerned with the improve-         (a) Verify that the combinations listed above are
     ment of the operation of a wave soldering ma-
     chine. The effects of the five factors Conveyor            in fact those prescribed by the relationship
     Speed (A), Preheat Temperature (B), Solder Tem-
     perature (C), Conveyor Angle (D), and Flux Con-            E  -CD. (For example, with all of A
     centration (E) on the variable
                                                                through D at their low levels, note that the
      y = the number of faults per 100 solder joints
            (computed from inspection of 12                     low level of E is indicated by multiplying
            circuit boards)
                                                                minus signs for C and D by another minus

                                                                sign. Thus, combination (1) is one of the 16

                                                                prescribed by the generator.)

                                                             (b) Write the defining relation for the experiment.

                                                                What is the resolution of the design chosen by

                                                                the authors? What resolution does the stan-

                                                                dard  choice  of   1  fraction    provide?      Unless
                                                                                   2
                                                                there were some unspecified extenuating cir-

                                                                cumstances   that  dictated  the  choice    of  1  frac-
                                                                                                                2
                                                                tion, why does it seem to be an unwise one?

                                                             (c) Write out the 16 different differences of ef-

                                                                fects that can be estimated based on the data

                                                                  given. (For example, one of these is µ..... -
                                                                    222, another is 2 -   2222, etc.)
                                                             (d) Notice that the combinations listed here are in

                                                                Yates standard order as regards levels of fac-

                                                                tors A through D. Use the four-cycle Yates
                                                                               Chapter 8 Exercises 641

          algorithm and find the fitted differences of ef-        y2 is a measure of uniformity of the epitaxial
                                                                  thickness, and y1 is (clearly) a measure of the
          fects. Normal-plot these and identify any sta-          magnitude of the thickness. The authors reported
                                                                  results from the experiment as shown in the ac-
          tistically detectable differences. Notice that          companying table.

          by  virtue  of  the  choice  of  1  fraction  made  by
                                           2
          the engineers, the most obviously statistically

          significant difference is that of a main effect

          and a 2-factor interaction.                             Combination  y1 (µm)      y2

20. The article "Robust Design: A Cost-Effective                     (1)       14.821    -.4425
     Method for Improving Manufacturing Processes"                   afgh      14.888   -1.1989
     by Kacker and Shoemaker (AT&T Technical Jour-                   begh      14.037   -1.4307
     nal, 1986) discusses the use of a 28-4 fractional               abef      13.880    -.6505
     factorial experiment in the improvement of the                  cefh      14.165   -1.4230
     performance of a step in an integrated circuit fab-             aceg      13.860    -.4969
     rication process. The initial step in fabricating sil-          bcfg      14.757    -.3267
     icon wafers for IC devices is to grow an epitaxial              abch      14.921    -.6270
     layer of sufficient (and, ideally, uniform) thick-              defg      13.972    -.3467
     ness on polished wafers. The engineers involved                 adeh      14.032    -.8563
     in running this part of the production process                  bdfh      14.843    -.4369
     considered the effects of eight factors (listed in              abdg      14.415    -.3131
     the accompanying table) on the properties of the                cdgh      14.878    -.6154
     deposited epitaxial layer.                                      acdf      14.932    -.2292
                                                                    bcde       13.907    -.1190
Factor A  Arsenic Flow Rate            55% (-) vs. 59% (+)          abcdefgh   13.914    -.8625
Factor B  Deposition Temperature       1210C (-)
                                       vs. 1220C (+)              It is possible to verify that the combinations listed
Factor C  Code of Wafers                                          here come from the use of the four generators E 
                                       668G4 (-)                  BCD, F  ACD, G  ABD, and H  ABC.
Factor D  Susceptor Rotation           vs. 678G4 (+)              (a) Write out the whole defining relation for this

Factor E  Deposition Time              continuous (-)                  experiment. (The grand mean will have 15
Factor F  HC1 Etch Temperature         vs. oscillating (+)             aliases.) What is the resolution of the design?
                                                                  (b) Consider first the response y2, the measure
Factor G  HC1 Flow Rate                high (-) vs. low (+)            of uniformity of the epitaxial layer. Use the
Factor H  Nozzle Position              1180C (-)                       Yates algorithm and normal- and/or half
                                       vs. 1215C (+)                   normal-plotting (see Exercise 9) to identify
                                                                       statistically detectable fitted sums of effects.
                                       10% (-) vs. 14% (+)             Suppose that only the two largest (in magni-
                                                                       tude) of these are judged to be both statisti-
                                       2 (-) vs. 6 (+)                 cally significant and of practical importance.
                                                                       What is suggested about how levels of the
   A batch of 14 wafers is processed at one time,                      factors might henceforth be set in order to
and the experimenters measured thickness at five                       minimize y2? From the limited description of
locations on each of the wafers processed during                       the process above, does it appear that these
one experimental run. These 14 × 5 = 70 mea-                           settings require any extra manufacturing ex-
surements from each run of the process were then                       pense?
reduced to two response variables:

y1 = the mean of the 70 thickness measurements

y2 = the logarithm of the variance of the 70
      thickness measurements
642 Chapter 8 Inference for Full and Fractional Factorial Studies

     (c) Turn now to the response y1. Again use the        dividing by another so as to disguise the original
          Yates algorithm and normal- and/or half          responses without destroying their basic structure.
          normal-plotting to identify statistically de-    You may think of these values as output measured
          tectable sums of effects. Which of the factors   in numbers of some undisclosed units above an
          seems to be most important in determining        undisclosed baseline value.)
          the average epitaxial thickness? In fact, the
          target thickness for this deposition process             Combination        y
          was 14.5 µm. Does it appear that by appro-
          priately choosing a level of this variable it               ef            13.99
          may be possible to get the mean thickness on                a             6.76
          target? Explain. (As it turns out, the thought              bf            20.71
          process outlined here allowed the engineers                 abe       11.11, 11.13
          to significantly reduce the variability in epi-             ce            19.61
          taxial thickness while getting the mean on                  acf           15.73
          target, improving on previously standard pro-               bc            23.45
          cess operating methods.)                                    abcef         20.00
                                                                      def           24.94
21. Arndt, Cahill, and Hovey worked with a plastics                   ad        24.03, 25.03
     manufacturer and experimented on an extrusion                    bdf           24.97
     process. They conducted a 26-2 fractional facto-                 abde          24.29
     rial study with some partial "replication" (the rea-             cde       24.94, 25.21
     son for the quote marks will be discussed later).                acdf      24.32, 24.48
     The experimental factors in their study were as                  bcd           30.00
     follows:                                                         abcdef        33.08

Factor A  Bulk Density, a measure of the weight per        (a) The students who planned this experiment
Factor B  unit volume of the raw material used
Factor C                                                        hadn't been exposed to the concept of design
Factor D  Moisture, the amount of water added to the            resolution. What does Table 8.35 indicate is
Factor E  raw material mix                                      the best possible resolution for a 26-2 frac-
Factor F                                                        tional factorial experiment? What is the res-
          Crammer Current, the amperage supplied to
          the crammer-auger                                     olution of the one that the students planned?
                                                                Why would they have been better off with a
          Extruder Screw Speed
                                                                different plan than the one specified by the
          Front-End Temperature, a temperature controlled       generators E AB and F  AC?
          by heaters on the front end of the extruder      (b) Find a choice of generators E  (some prod-
                                                                uct of letters A through D) and F  (some
          Back-End Temperature, a temperature controlled        other product of letters A through D) that
          by heaters on the back end of the extruder            provides maximum resolution for a 26-2 ex-
                                                                periment.
Physically low and high levels of these factors
were identified. Using the two generators E                (c) The combinations here are listed in Yates
AB and F  AC, 16 different combinations of                      standard order as regards factors A through
levels of the factors were chosen for inclusion in              D. Compute y¯ 's and then use the (four-cycle)
a plant experiment, where the response of primary               Yates algorithm and compute 16 estimated
interest was the output of the extrusion process in             sums of 26 factorial effects.
terms of pounds of useful product per hour. A
coded version of the data the students obtained is
given in the accompanying table. (The data have
been rescaled by subtracting a particular value and
                                                                                Chapter 8 Exercises 643

(d) When the extrusion process is operating, many                  next, and what would you be planning to do
     pieces of product can be produced in an hour,                 with them?
     but the entire data collection process lead-
     ing to the data here took over eight hours.         22. The article "The Successful Use of the Taguchi
     (Note, for example, that changing tempera-               Method to Increase Manufacturing Process Capa-
     tures on industrial equipment requires time              bility" by S. Shina (Quality Engineering, 1991)
     for parts to heat up or cool down, changing              discusses the use of a 28-3 fractional factorial
     formulas of raw material means that one must             experiment to improve the operation of a wave
     let one batch clear the system, etc.) The re-            soldering process for through-hole printed circuit
     peat observations above were obtained from               boards. The experimental factors and levels stud-
     two consecutive pieces of product, made min-             ied were as shown in the accompanying table.
     utes apart, without any change in the extruder
     setup in between their manufacture. With this       Factor A  Preheat Temperature  180 (-) vs. 220 (+)
     in mind, discuss why a pooled standard de-          Factor B  Solder Wave height   .250 (-) vs. .400 (+)
     viation based on these four "samples of size        Factor C  Wave Temperature     490 (-) vs. 510 (+)
     2" is quite likely to underrepresent the level      Factor D  Conveyor Angle       5.0 (-) vs. 6.1 (+)
     of "baseline" variability in the output of this     Factor E  Flux Type            A857 (-) vs. K192 (+)
     process under a fixed combination of levels of      Factor F  Direction of Boards  0 (-) vs. 90 (+)
     factors A through F. Argue that it would have       Factor G  Wave Width           2.25 (-) vs. 3.00 (+)
     been extremely valuable to have (for exam-          Factor H  Conveyor Speed       3.5 (-) vs. 6.0 (+)
     ple) rerun one or more of the combinations
     tested early in the study again late in the study.  The generators F  -CD, G  -AD, and H 
                                                         -ABCD were used to pick 32 different com-
(e) Use the pooled sample standard deviation             binations of levels of these factors to run. For
     from the repeat observations and compute            each combination, four special test printed cir-
     (using the p = 4 version of formula (8.12) in       cuit boards were soldered, and the lead shorts per
     Section 8.2) the plus-or-minus part of 90%          board, y1, and touch shorts per board, y2, were
     two-sided confidence limits for the 16 sums         counted, giving the accompanying data. (The data
     of effects estimated in part (c), acting as if the  here and on page 644 are exactly as given in the
     value of sP were a legitimate estimate of back-     article, and we have no explanation for the fact
     ground variability. Which sums of effects are       that some of the numbers do not seem to have
     statistically detectable by this standard? How      come from division of a raw count by 4.)
     do you interpret this in light of the informa-
     tion in part (d)?                                             Combination    y1      y2

(f) As an alternative to the analysis in part (e),                    (1)        6.00   13.00
     make a normal plot of the last 15 of the 16                      agh       10.00   26.00
     estimated sums of effects you computed in                        bh        10.00   12.00
     part (c). Which sums of effects appear to be                     abg               14.00
     statistically detectable? What is the simplest                   cfh        8.50   18.75
     interpretation of your findings in the context                   acfg       1.50   16.25
     of the industrial problem? (What has been                        bcf          .25  25.75
     learned about how to run the extruding pro-                      abcfgh     1.75   18.50
     cess?)                                                           dfgh       4.25    6.50
                                                                                 6.50
(g) Briefly discuss where to go from here if it
     is your job to optimize the extrusion process
     (maximize y). What data would you collect

                                                                                        (continued )
644 Chapter 8 Inference for Full and Fractional Factorial Studies

Combination    y1      y2                                 (c) Note that the 32 combinations of the 8 factors
                                                               above are listed in Yates standard order as re-
   adf          .75     .00                                    gards Factors A through E (ignoring F, G, and
   bdfg       3.50    1.00                                     H). By some means (using a statistical anal-
   abdfh      3.25    6.50                                     ysis package like MINITAB, implementing
   cdg        6.00    7.25                                     spreadsheet calculations, or doing the 5-cycle
   acdh       9.50   11.25                                     Yates algorithm "by hand") find the estimated
   bcdgh      6.25   10.00                                     sums of effects for the response y1. Normal-
   abcd       6.75   12.50                                     plot the last 31 of these. You should find that
   e         20.00   29.25                                     the largest of these would be the CD 2-factor
   aegh      16.50   31.25                                     interaction, the E main effect, and the CDE
   beh       17.25   28.75                                     3-factor interaction if only 5 factors were in-
   abeg      19.50   41.25                                     volved (instead of 8). These are all positive
   cefh       9.67   21.33                                     and clearly larger in magnitude than the other
   acefg      2.00   10.75                                     estimates. If possible, give a simple interpre-
   bcef       5.67   28.67                                     tation of this in light of the alias structure
   abcefgh    3.75   35.75                                     specified by the defining relation you found
   defgh      6.00   22.70                                     in part (b).
   adef       7.30   25.70
   bdefg      8.70   30.00                                (d) Now find and normal-plot the estimated sums
   abdefh     9.00   29.70                                     of effects for the response y2. (Normal-plot 31
   cdeg      19.30   32.70                                     estimates.) You should find the estimate cor-
   acdeh     26.70   25.70                                     responding to the E main effect plus aliases to
   bcdegh    17.70   45.30                                     be positive, larger in magnitude than the rest,
   abcde     10.30   37.00                                     and detectably nonzero.

(a) Verify that the 32 combinations of levels of          (e) In light of your answers to (c) and (d), the
     the factors A through H listed here are those             signs of the fitted linear combinations of ef-
     that are prescribed by the choice of genera-              fects, and a desire to reduce both y1 and y2 to
     tors. (For each combination of levels of the              the minimum values possible, what combina-
     factors A through E, determine what levels of             tion of levels of the factors do you tentatively
     F, G, and H are prescribed by the generators              recommend here? Is the combination of levels
     and check that such a combination is listed.)             that you see as promising one that is among
                                                               the 32 tested? If it is not, how would you rec-
(b) Use the generators given here and write out                ommend proceeding in the real manufactur-
     the whole defining relation for this study.               ing scenario? (Would you, for example, order
     (You will end with I aliased with seven other             that any permanent process changes neces-
     strings of letters.) What is the resolution of            sary to the use of your promising combination
     the design used in this study? According to               be adopted immediately?)
     Table 8.35, what was possible in terms of res-
     olution for a 28-3 study? Could the engineers        The original article reported a decrease in solder
     in charge here have done better at containing        defects by nearly a factor of 10 in this process as a
     the ambiguity that unavoidably follows from          result of what was learned from this experiment.
     use of a fractional factorial study?
                                                     23. In the situation of Exercise 22, the 32 different
                                                          combinations of levels of factors A through H
                                                          were run in the order listed. In fact, the first 16
                                                          runs were made by one shift of workers, and the
                                                          last 16 were made by a second shift.
                                                                      Chapter 8 Exercises 645

     (a) In light of the material in Chapter 2 on ex-         Feed Ratio 6. The response variable was
          periment planning and the formal notion of
          confounding, what risk of a serious logical         y = percent conversion of butane
          flaw did the engineers run in the execution of
          their experiment? (How would possible shift-        and the data in the accompanying table were col-
          to-shift differences show up in the data from       lected.
          an experiment run like this? One of the main
          things learned from the experiment was that               Feed Wall Feed
          factor E was very important. Did the engi-       Day Flow Temp. Ratio Combination y
          neers run the risk of clouding their view of
          this important fact?) Explain.                   1 115 495  6  --                            78

     (b) Devise an alternative plan that could have        1  50 470  4  (1)                           99
          been used to collect data in the situation of
          Exercise 22 without completely confounding       1 180 520  8  abc                           87
          the effects of Flux and Shift. Continue to use
          the 32 combinations of the original factors      2  50 520  4  b                             98
          listed in Exercise 22, but give a better as-
          signment of 16 of them to each shift. (Hint:     2 180 470  8  ac                            18
          Think of Shift as a ninth factor, pick a sensi-
          ble generator, and use it to put half of the 32  2 115 495  6  --                            87
          combinations in each shift. There are a variety
          of possibilities here.)                          3  50 520  8  bc                            95

     (c) Discuss in qualitative terms how you would        3 180 470  4  a                             59
          do data analysis if your suggestion in (b) were
          to be followed.                                  3 115 495  6  --                            90

24. The article "Computer Control of a Butane Hy-          4  50 470  8  c                             76
     drogenolysis Reactor" by Tremblay and Wright
     (The Canadian Journal of Chemical Engineering,        4 180 520  4  ab                            92
     1974) contains an interesting data set concerned
     with the effects of p = 3 process variables on the    4 115 495  6  --                            89
     performance of a chemical reactor. The factors
     and their levels were as follows:                        (a) Suppose that to begin with, you ignore the
                                                                   fact that these data were collected over a pe-
Factor A  Total Feed Flow (cc/sec at STP)  50 (-)                  riod of four days and simply treat the data as
Factor B  Reactor Wall Temperature (F)     vs. 180 (+)             a complete 23 factorial augmented with a re-
Factor C  Feed Ratio (Hydrogen/Butane)                             peated center point. Analyze these data using
                                           470 (-)                 the methods of this chapter. (Compute sP from
                                           vs. 520 (+)             four center points. Use the Yates algorithm
                                                                   and the eight corner points to compute fitted
                                           4 (-)                   23 factorial effects. Then judge the statistical
                                           vs. 8 (+)               significance of these using appropriate 95%
                                                                   two-sided confidence limits based on sP.) Is
The data had to be collected over a four-day pe-                   any simple interpretation of the experimental
riod, and two combinations of the levels of fac-                   results in terms of factorial effects obvious?
tors A, B, and C above were run each day along
with a center point--a data point with Total Feed             According to the authors, there was the possibility
Flow 115, Reactor Wall Temperature 495, and                   of "process drift" during the period of experimen-
                                                              tation. The one-per-day center points were added
                                                              to the 23 factorial at least in part to provide some
                                                              check on that possibility, and the allocation of two
                                                              ABC combinations to each day was very carefully
                                                              done in order to try to minimize the possible con-
                                                              founding introduced by any Day/Block effects.
                                                              The rest of this problem considers analyses that
646 Chapter 8 Inference for Full and Fractional Factorial Studies

might be performed on the experimenters' data in                  normal-plot (or half normal-plot) the fitted
recognition of the possibility of process drift.                  effects from (a). Do so. Interpret your plot,
(b) Plot the four center points against the num-                  supposing that there were no interactions with
                                                                  Days in the reactor study. How do your con-
     ber of the day on which they were collected.                 clusions differ (if at all) from those in (a)?
     What possibility is at least suggested by your           (f) One possible way of dealing with the possi-
     plot? Would the plot be particularly troubling               bility of Day effects in this particular study
     if your experience with this reactor told you                is to use the center point on each day as a
     that a standard deviation of around 5(%) was                 sort of baseline and express each other re-
     to be expected for values of y from consecu-                 sponse as a deviation from that baseline. (If
     tive runs of the reactor under fixed operating               on day i there is a Day effect i , and on day
     conditions on a given day? Would the plot be                 i the mean response for any combination of
     troubling if your experience with this reactor               levels of factors A through C is µcomb + i ,
     told you that a standard deviation of around                 the mean of the difference ycomb - ycenter is
     1(%) was to be expected for values of y from                 µcomb - µcenter; one can therefore hope to see
     consecutive runs of the reactor under fixed                  23 factorial effects uncontaminated by ad-
     operating conditions on a given day?                         ditive Day effects using such differences in
(c) The four-level factor Day can be formally                     place of the original responses.) For each of
     thought of in terms of two extra two-level                   the four days, subtract the response at the cen-
     factors--say, D and E. Consider the choice of                ter point from the other two responses and
     generators D  AB and E  BC for a 25-2                        apply the Yates algorithm to the eight differ-
     fractional factorial. Verify that the eight com-             ences. Normal-plot the fitted effects on the
     binations of levels of A through E prescribed                (difference from the center point mean) re-
     by these generators divide the eight possible                sponse. Is there any substantial difference be-
     combinations of levels of A through C up into                tween the result of this analysis and that for
     the four groups of two corresponding to the                  the others suggested in this problem?
     four days of experimentation. (To begin with,
     note that both A low, B low, C low and A           25. The article "Including Residual Analysis in De-
     high, B high, C high correspond to D high               signed Experiments: Case Studies" by W. H.
     and E high. That is, the first level of Day             Collins and C. B. Collins (Quality Engineering,
     can be thought of as the D high and E high              1994) contains discussions of several machining
     combination.)                                           experiments concerned with surface finish. Given
(d) The choice of generators in (c) produces the             here are the factors and levels studied in (part of)
     defining relation I  ABD  BCE                           one of those experiments on a particular lathe.
     ACDE. Write out, on the basis of this defining
     relation, the list of eight groups of aliased 25   Factor     Levels
     factorial effects. Any effect involving factors
     A, B, or C with either of the letters D () or E    A Speed    2500 RPM (-)
     ( ) in its name represents some kind of inter-                vs. 4500 RPM (+)
     action with Days. Explain what it means for
     there to be no interactions with Days. Make        B Feed     .003 in/rev (-)
     out a list of eight smaller groups of aliased ef-             vs. .009 in/rev (+)
     fects that are appropriate supposing that there
     are no interactions with Days.                     C Tool Condition New (-)
(e) Allowing for the possibility of Day (Block)                                       vs. Used (after 250 parts) (+)
     effects, it does not make sense to use the cen-
     ter points to compute sP. However, one might       m = 2 parts were turned on the lathe for each of
                                                        the 23 different combinations of levels of the 3
                                                                      Chapter 8 Exercises 647

factors, and surface finish measurements, y, were       26. Below are 24 factorial data for two response vari-
made on these. (y is a measurement of the verti-             ables taken from the article "Chemical Vapor De-
cal distance traveled by a probe as it moves hor-            position of Tungsten Step Coverage and Thick-
izontally across a particular 1 inch section of the          ness Uniformity Experiments" by J. Chang (Thin
part.) Next are some summary statistics from the             Solid Films, 1992). The experiment concerned the
experiment.                                                  blanket chemical vapor deposition of tungsten in
                                                             the manufacture of integrated circuit chips. The
Combination y¯  s Combination y¯  s                          factors studied were as follows:

(1)  33.0 0.0   c    35.5 6.4                           A Chamber Pressure 8 (-) vs. 9 (+)

a    45.5 7.8   ac   44.0 7.1                           B H2 Flow     500 (-) vs. 1000 (+)
                                                        C SiH4 Flow   15 (-) vs. 25 (+)
b    222.5 4.9  bc   216.5 6.4                          D WF6 Flow    50 (-) vs. 60 (+)

ab   241.5 4.9  abc  216.5 0.7

(a) Find sP and its degrees of freedom. What does       The pressure is measured in Torr and the flows
     this quantity intend to measure?                   are measured in standard cm3/min. The response

(b) 95% individual two-sided confidence limits          variable y1 is the "percent step coverage," 100
     for the mean surface finish measurement for        times the ratio of tungsten film thickness at the
     a part turned under a given set of conditions
     are of the form y¯ i jk ± . Based on the value     top of the side wall to the bottom of the side wall
     of sP found above, find .
                                                        (large is good). The response variable y2 is an
(c) Would you say that the mean surface finish          "average sheet resistance" (measured in m ).
     measurements for parts of types "(1)" and
     "a" are detectably different? Why or why not?      Combination y1 y2 Combination y1 y2
     (Show appropriate calculations.)
                                                        (1)  73 646   d                     83 666
(d) 95% individual two-sided individual confi-
     dence limits for the 23 factorial effects in this  a    60 623   ad                    80 597
     study are of the form E^ ± . Find .
                                                        b    77 714   bd                    100 718
(e) Compute the 23 factorial fitted effects for the
     "all high" combination (abc).                      ab   90 643   abd                   85 661

(f) Based on your answers to parts (d) and (e),         c    67 360   cd                    77 304
     which of the main effects and/or interactions
     do you judge to be statistically detectable?       ac   78 359   acd                   90 309
     Explain.
                                                        bc   100 335  bcd                   70 360
(g) Give the practical implications of your answer
     to part (f). (How do you suggest running the       abc  77 318   abcd                  75 318
     lathe if small y and minimum machining cost
     are desirable?)                                    (a) Make a normal plot of the 15 fitted effects
                                                             a2, b2, . . . , abcd2222 as a means of judging
(h) Suppose you were to judge only the B main                the statistical detectability of the effects on the
     effect to be both statistically detectable and          response, y1. Interpret this plot and say what
     of practical importance in this study. What             is indicated about producing good "percent
     surface finish value would you then predict             step coverage."
     for a part made at a 2500 RPM speed and a
     .009 in/rev feed rate using a new tool?            (b) Repeat part (a) for the response variable y2.
                                                        Now suppose that instead of a full factorial study,
                                                        only the half fraction with defining relation
                                                        D  ABC had been conducted.
648 Chapter 8 Inference for Full and Fractional Factorial Studies

     (c) Which 8 of the 16 treatment combinations             factorial experiment. The experimental factors
          would have been run? List these combina-            and their levels in the study were:
          tions in Yates standard order as regards fac-
          tors A, B, and C and use the (3-cycle Yates         A Method of Preparation Usual (-) vs. Modified (+)
          algorithm) to compute the 8 estimated sums
          of effects that it is possible to derive from       B Sugar Content        50% (-) vs. 60% (+)
          these 8 treatment combinations for response
          y2. Verify that each of these 8 estimates is        C Antibiotic Level     8% (-) vs. 16% (+)
          the sum of two of your fitted effects from
          part (b). (For example, you should find that        D Aerosol              .4% (-) vs. .6% (+)
          the first estimated sum here is y¯ .... + abcd2222
          from part (b).)                                     E CMC                  .2% (-) vs. .4% (+)

     (d) Normal-plot the last 7 of the estimated sums         The response variable was
          from (c). Interpret this plot. If you had only
          the data from this 24-1 fractional factorial,       y = separated clear volume (%)
          would your subject-matter conclusions be the             for a suspension of antibiotic after 45 days
          same as those reached in part (b), based on
          the full 24 data set?                               and the manufacturer hoped to find a way to make
                                                              y small. The experimenters failed to follow the
27. An engineer wishes to study seven experimental            recommendation in Section 8.3 for choosing a
     factors, A, B, C, D, E, F and G, each at 2 levels,       best half fraction of the factorial and used the
     using only 16 combinations of factor levels. He          generator E  ABC (instead of the better one
     plans initially to use generators E  ABCD, F             E  ABCD).
     ABC, and G  BCD.                                         (a) In what sense was the experimental plan used
     (a) With this initial choice of generators, what
          16 combinations of levels of the seven factors           in the study inferior to the one prescribed in
          will be run?                                             Section 8.3? (How is the one from Section 8.3
     (b) In a 27-3 fractional factorial, each effect is            "better"?)
          aliased with 7 other effects. Starting from         The Yates algorithm applied to the 16 responses
          the engineer's choice of generators, find the       given in the paper produced the 16 fitted sums of
          defining relation for his study. (You will need     effects:
          not only to consider products of pairs but also
          a product of a triple.)                             mean + alias = 37.563      D + alias = -7.437
     (c) An alternative choice of generators is               A + alias = .187           AD + alias = .937
          E  ABC, F  BCD, G  ABD. This                        B + alias = 2.437          BD + alias = .678
          choice yields the defining relation                 AB + alias = .312          ABD + alias = .812
                                                              C + alias = -1.062         CD + alias = 1.438
           I  ABCE  BCDF  ABDG                                AC + alias = .312          ACD + alias = .062
                                                              BC + alias = -1.187        BCD + alias = .062
              ADEF  CDEG  ACFG  BEFG                          ABC + alias = -2.063       ABCD + alias = -.062

          Which is preferable, the defining relation in       (a) Make a normal plot of the last 15 of these
          part (b), or the one here? Why?                          fitted sums.

28. The article "Establishing Optimum Process Lev-            (b) If you had to guess (based on the results of this
     els of Suspending Agents for a Suspension Prod-               experiment) the order of the magnitudes of
     uct" by A. Gupta (Quality Engineering,                        the five main effects (A, B, C, D and E) from
     1997-1998) discussed an unreplicated fractional               smallest to largest, what would you guess?
                                                                   Explain.
                                                                 Chapter 8 Exercises 649

     (c) Based on the normal plot in (b), which sums       (b) Following are the mean thicknesses measured
          of effects do you judge to be statistically de-       for the combinations studied, listed in Yates
          tectable? Explain.                                    standard order as regards levels of factors A,
                                                                B, and C. Use the Yates algorithm and find
     (d) Based on your answers to (c) and (d), how do           eight estimated (sums of) effects.
          you suggest that suspensions of this antibiotic
          be made in order to produce small y? What        A  B  C  y¯
          mean y do you predict if your recommenda-
          tions are followed?                              -  -  -  .98

     (e) Actually, the company that ran this study         +  -  -  1.58
          planned to make suspensions using both high
          and low levels of antibiotic (factor C). Does    -  +  -  1.13
          your answer to (d) suggest that the company
          needs to use different product formulations      +  +  -  1.74
          for the two levels of antibiotic? Explain.
                                                           -  -  +  1.49
29. The paper "Achieving a Target Value for a Manu-
     facturing Process," by Eibl, Kess, and Pukelsheim     +  -  +  .84
     (Journal of Quality Technology, 1992) describes
     a series of experiments intended to guide the ad-     -  +  +  2.18
     justment of a paint coating process. The first of
     these was a 26-3 fractional factorial study. The ex-  +  +  +  1.45
     perimental factors studied were as follows (exact
     levels of these factors are not given in the paper,   (c) Two-sided confidence limits based on the es-
     presumably due to corporate security considera-            timated (sums of) effects calculated in part (b)
     tions):                                                    are of the form E^ ± . Find if (individual)
                                                                95% confidence is desired.
A  Tube Height          low (-) vs. high (+)
                                                           (d) Based on your answer to (c), list those esti-
B  Tube Width           low (-) vs. high (+)                    mates from part (b) that represent statistically
                                                                detectable (sums of) effects.
C  Paint Viscosity      low (-) vs. high (+)
                                                           In fact, the experimental plan used by the investi-
D  Belt Speed           low (-) vs. high (+)               gators had generators D  AC, E  BC, and F
                                                            ABC.
E  Pump Pressure        low (-) vs. high (+)               (e) Specify the combinations (of levels of the ex-

F  Heating Temperature  low (-) vs. high (+)                    perimental factors A, B, C, D, E and F) that
                                                                were included in the experiment.
   The response variable was a paint coating thick-        (f) Write out the whole defining relation for this
   ness measurement, y, whose units are mm. m = 4               study. (You will need to consider here not only
                                                                products of pairs but a product of a triple as
   workpieces were painted and measured for each                well. The grand mean is aliased with seven
   of the r = 8 combinations of levels of the fac-              other effects.)
   tors studied. The r = 8 samples of size m = 4           (g) In light of your answers to part (d) and the
   produced a value of sP = .118 mm.                            aliasing pattern here, what is the simplest pos-
   (a) Suppose that you wish to attach a precision              sible potential interpretation of the results of
                                                                this experiment?
        to one of the r = 8 sample means obtained in

        this study. This can be done using 95% two-
        sided confidence limits of the form y¯ ± .

        Find .
  9q q q q q q q q q q q q q q q q q q q q q q q q q q q q

               Regression
               Analysis--Inference
               for Curve- and
               Surface-Fitting

               The two previous chapters began a study of inference methods for multisample

                                  studies by considering first those which make no explicit use of structure relating
                                  several samples and then discussing some directed at the analysis of factorial struc-
                                  ture. The discussion in this chapter will primarily consider inference methods for
                                  multisample studies where factors involved are inherently quantitative and it is rea-
                                  sonable to believe that some approximate functional relationship holds between the
                                  values of the system/input/independent variables and observed system responses.
                                  That is, this chapter introduces and applies inference methods for the curve- and
                                  surface-fitting contexts discussed in Sections 4.1 and 4.2.

                                       The chapter begins with a discussion of the simplest situation of this type--
                                  namely, where a response variable y is approximately linearly related to a single
                                  quantitative input variable x. In this specific context, it is possible to give explicit
                                  formulas and illustrate in concrete terms what is possible in the way of inference
                                  methods for surface-fitting analyses. The second section then treats the general
                                  problem of statistical inferences in multiple regression (curve- and surface-fitting)
                                  analyses. In the general case, it is not expedient to produce many computational for-
                                  mulas. So the exposition relies instead on summary measures commonly appearing
                                  on multiple regression printouts from statistical packages. A final section further
                                  illustrates the broad utility of the multiple regression methods by applying them to
                                  "response surface," and then factorial, analyses.

650
9.1 Inference Methods Related to the Least Squares Fitting of a Line (Simple Linear Regression) 651

               qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

   9.1 Inference Methods Related to the
          Least Squares Fitting of a Line
          (Simple Linear Regression)

                         This section considers inference methods that are applicable where a response y is
                         approximately linearly related to an input/system variable x. It begins by introducing
                         the (normal) simple linear regression model and discussing how to estimate response
                         variance in this context. Next there is a look at standardized residuals. Then inference
                         for the rate of change ( y/ x) is considered, along with inference for the average
                         response at a given x. There follows a discussion of prediction and tolerance intervals
                         for responses at a given setting of x. Next is an exposition of ANOVA ideas in the
                         present situation. The section then closes with an illustration of how statistical
                         software expedites the calculations introduced in the section.

9.1.1  The Simple Linear Regression Model, Corresponding
       Variance Estimate, and Standardized Residuals

       Chapter 7 introduced the one-way (equal variances, normal distributions) model as
       the most common probability basis of inference methods for multisample studies.
       It was represented in symbols as

       yi j = µi + i j                                    (9.1)

       where the means µ1, µ2, . . . , µr were treated as r unrestricted parameters. In Chap-
       ter 8, it was convenient (for example) to rewrite equation (9.1) in two-way con-
       texts as

       yi jk = µi j + i jk (= µ.. + i + j + i j + i jk )  (9.2)

       where the µi j are still unrestricted, and to consider restrictions/simplifications of
       model (9.2) such as

       yi jk = µ.. + i + j + i jk                         (9.3)

       Model (9.3) really differs from model (9.2) or (9.1) only in the fact that it postulates

       a special form or restriction for the means µi j . Expression (9.3) says that the means
       must satisfy a parallelism relationship.

            Turning now to the matter of inference based on data pairs (x1, y1), (x2, y2), . . . ,
       (xn, yn) exhibiting an approximately linear scatterplot, one once again proceeds by
       imposing a restriction on the one-way model (9.1). In words, the model assumptions

       will be that there are underlying normal distributions for the response y with a
652 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

                          common variance  2 but means µy|x that change linearly in x. In symbols, it is
                          typical to write that for i = 1, 2, . . . , n,

The (normal) simple          yi = 0 + 1xi + i                                         (9.4)
    linear regression
                   model

                          where the i are (unobservable) iid normal (0,  2) random variables, the xi are known
                          constants, and 0, 1, and  2 are unknown model parameters (fixed constants).
                          Model (9.4) is commonly known as the (normal) simple linear regression model.
                          If one thinks of the different values of x in an (x, y) data set as separating it into
                          various samples of y's, expression (9.4) is the specialization of model (9.1) where the
                          (previously unrestricted) means of y satisfy the linear relationship µy|x = 0 + 1x.
                          Figure 9.1 is a pictorial representation of the "constant variance, normal, linear (in x)
                          mean" model.

                               Inferences about quantities involving those x values represented in the data (like
                          the mean response at a single x or the difference between mean responses at two
                          different values of x) will typically be sharper when methods based on model (9.4)
                          can be used in place of the general methods of Chapter 7. And to the extent that model
                          (9.4) describes system behavior for values of x not included in the data, a model
                          like (9.4) provides for inferences involving limited interpolation and extrapolation
                          on x.

                               Section 4.1 contains an extensive discussion of the use of least squares in the
                          fitting of the approximately linear relation

                             y  0 + 1x                                                (9.5)

                          to a set of (x, y) data. Rather than redoing that discussion, it is most sensible simply
                          to observe that Section 4.1 can be thought of as an exposition of fitting and the
                          use of residuals in model checking for the simple linear regression model (9.4). In

                          y

                                                                 µy|x = 0 + 1x

                                                             Distributions of y
                                                             for various x

                                                                                   x
                          Figure 9.1 Graphical representation of the
                          simple linear regression model
9.1 Inference Methods Related to the Least Squares Fitting of a Line (Simple Linear Regression) 653

                    particular, associated with the simple linear regression model are the estimates of
                    1 and 0

Estimator of 1,                               (x - x)(y - y)
         the slope                 b1 = (x - x)2 (9.6)

                    and

Estimator of 0,                                      b0 = y¯ - b1x¯         (9.7)
    the intercept
                    and corresponding fitted values
Fitted values for
    simple linear                                    y^ i = b0 + b1xi       (9.8)
        regression
                    and residuals
    Residuals for
    simple linear                                    ei = yi - y^ i         (9.9)

        regression

                         Further, the residuals (9.9) can be used to make up an estimate of  2. As
                    always, a sum of squared residuals is divided by an appropriate number of degrees
                    of freedom. That is, there is the following definition of a (simple linear regression
                    or) line-fitting sample variance.

Definition 1        For a set of data pairs (x1, y1), (x2, y2), . . . , (xn, yn) where least squares fitting
                    of a line produces fitted values (9.8) and residuals (9.9),

                                   sLF 2 = 1 n - 2 (y - y^ )2 = 1 n - 2 e2  (9.10)

                    will be called a line-fitting sample variance. Associated with it are  = n - 2
                    degrees of freedom and an estimated standard deviation of response, sLF =

                      sLF 2 .

                         sLF 2 estimates the level of basic background variation,  2, whenever the model
                    (9.4) is an adequate description of the system under study. When it is not, sLF will
                    tend to overestimate  . So comparing sLF to sP is another way of investigating
                    the appropriateness of model (9.4). (sLF much larger than sP suggests the linear
                    regression model is a poor one.)
654 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

              Example 1  Inference in the Ceramic Powder Pressing Study
(Example 1, Chapter 4,   The main example in this section will be the pressure/density study of Benson,
                         Locher, and Watkins (used extensively in Section 4.1 to illustrate the descriptive
  revisited--page 124)   analysis of (x, y) data). Table 9.1 lists again those n = 15 data pairs (x, y) (first
                         presented in Table 4.1) representing

                                                    x = the pressure setting used (psi)
                                                    y = the density obtained (g/cc)

                         in the dry pressing of a ceramic compound into cylinders, and Figure 9.2 is a
                         scatterplot of the data.

                              Recall further from the calculation of R2 in Example 1 of Chapter 4 that the
                         data of Table 4.1 produce fitted values in Table 4.2 and then

                                                               (y - y^ )2 = .005153

                         So for the pressure/density data, one has (via formula (9.10)) that

                                               sLF 2 = 1 15 - 2 (.005153) = .000396 (g/cc)2
                         so

                                                              
                                                      sLF = .000396 = .0199 g/cc

                         If one accepts the appropriateness of model (9.4) in this powder pressing example,
                         for any fixed pressure the standard deviation of densities associated with many
                         cylinders made at that pressure would be approximately .02 g/cc.

                         Table 9.1
                         Pressing Pressures and Resultant Specimen Densities

                                x,              y,              x,                   y,
                         Pressure (psi)  Density (g/cc)  Pressure (psi)       Density (g/cc)

                         2,000           2.486            6,000               2.653
                         2,000           2.479            8,000               2.724
                         2,000           2.472            8,000               2.774
                         4,000           2.558            8,000               2.808
                         4,000           2.570           10,000               2.861
                         4,000           2.580           10,000               2.879
                         6,000           2.646           10,000               2.858
                         6,000           2.657
9.1 Inference Methods Related to the Least Squares Fitting of a Line (Simple Linear Regression) 655

Density (g/cc)  2.900
                2.800
                2.700
                2.600
                2.500

                2,000 4,000 6,000 8,000 10,000
                             Pressure (psi)

Figure 9.2 Scatterplot of density versus pressing
pressure

Table 9.2
Sample Means and Standard Deviations of Densities for Five
Different Pressing Pressures

       x,                     y¯ ,                 s,
Pressure (psi)         Sample Mean  Sample Standard Deviation

 2,000                 2.479        .0070
 4,000                 2.569        .0110
 6,000                 2.652        .0056
 8,000                 2.769        .0423
10,000                 2.866        .0114

     The original data in this example can be thought of as organized into r = 5
separate samples of size m = 3, one for each of the pressures 2,000 psi, 4,000
psi, 6,000 psi, 8,000 psi, and 10,000 psi. It is instructive to consider what this
thinking leads to for an alternative estimate of  --namely, sP. Table 9.2 gives y¯
and s values for the five samples.

     The sample standard deviations in Table 9.2 can be employed in the usual
way to calculate sP. That is, exactly as in Definition 1 of Chapter 7

         2 (3 - 1)(.0070)2 + (3 - 1)(.0110)2 + · · · + (3 - 1)(.0114)2
        sP =

                              (3 - 1) + (3 - 1) + · · · + (3 - 1)

           = .000424 (g/cc)2
656 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

Example 1       from which
(continued )

                            sP = sP2 = .0206 g/cc

                   Comparing sLF and sP, there is no indication of poor fit carried by these values.

                     Section 4.1 includes some plotting of the residuals (9.9) for the pressure/density
                data (in particular, a normal plot that appears as Figure 4.7). Although the (raw)
                residuals (9.9) are most easily calculated, most commercially available regression
                programs provide standardized residuals as well as, or even in preference to, the
                raw residuals. (At this point, the reader should review the discussion concerning
                standardized residuals surrounding Definition 2 of Chapter 7.) In curve- and surface-
                fitting analyses, the variances of the residuals depend on the corresponding x's.
                Standardizing before plotting is a way to prevent mistaking a pattern on a residual
                plot that is explainable on the basis of these different variances for one that is
                indicative of problems with the basic model. Under model (9.4), for a given x with
                corresponding response y,

                                        2  1 (x - x¯ )2
                            Var(y - y^ ) =  1 - -                               (9.11)
                                           n        (x - x¯ )                2

                So using formula (9.11) and Definition 7.2, corresponding to the data pair (xi , yi ) is
                the standardized residual for simple linear regression

Standardized                ei =                ei  (xi - x¯ )2                 (9.12)
 residuals for                     sLF  1- 1 -       (x - x¯ )2
simple linear
                                             n
    regression

                The more sophisticated method of examining residuals under model (9.4) is thus to
                make plots of the values (9.12) instead of plotting the raw residuals (9.9).

Example 1       Consider how the standardized residuals for the pressure/density data set are
(continued )    related to the raw residuals. Recalling that

                            (x - x¯ )2 = 120,000,000

                and that the xi values in the original data included only the pressures 2,000 psi,
                4,000 psi, 6,000 psi, 8,000 psi, and 10,000 psi, it is easy to obtain the necessary
                values of the radical in the denominator of expression (9.12). These are collected
                in Table 9.3.
9.1 Inference Methods Related to the Least Squares Fitting of a Line (Simple Linear Regression) 657

        Table 9.3
        Calculations for Standardized Residuals
        in the Pressure/Density Study

           x       1 - 1 - (x - 6,000)2
                        15 120,000,000
         2,000
         4,000               .894
         6,000               .949
         8,000               .966
        10,000               .949
                             .894

     The entries in Table 9.3 show, for example, that one should expect residuals
corresponding to x = 6,000 psi to be (on average) about .966/.894 = 1.08 times
as large as residuals corresponding to x = 10,000 psi. Division of raw residuals
by sLF times the appropriate entry of the second column of Table 9.3 then puts
them all on equal footing, so to speak. Table 9.4 shows both the raw residuals
(taken from Table 4.5) and their standardized counterparts.

     In the present case, since the values .894, .949, and .966 are roughly com-
parable, standardization via formula (9.12) doesn't materially affect conclusions
about model adequacy. For example, Figures 9.3 and 9.4 are normal plots of (re-
spectively) raw residuals and standardized residuals. For all intents and purposes,
they are identical. So any conclusions (like those made in Section 4.1 based on
Figure 4.7) about model adequacy supported by Figure 9.3 are equally supported
by Figure 9.4, and vice versa.

     In other situations, however (especially those where a data set contains a
few very extreme x values), standardization can involve more widely varying
denominators for formula (9.12) than those implied by Table 9.3 and thereby
affect the results of a residual analysis.

Table 9.4
Residuals and Standardized Residuals for the Pressure/Density Study

x               e               Standardized Residual

 2,000  .0137, .0067, -.0003    .77, .38, -.02
 4,000  -.0117, .0003, .0103    -.62, .02, .55
 6,000  -.0210, -.0100, -.0140  -1.09, -.52, -.73
 8,000  -.0403, .0097, .0437    -2.13, .51, 2.31
10,000  -.0007, .0173, -.0037   -.04, .97, -.21
658 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

Example 1

(continued )       Standard normal quantile  2.4

                                             1.2

                                             0.0

                                             -1.2

                                      -.03 -.02 -.01 0 .01 .02 .03
                                                Residual quantile

                   Figure 9.3 Normal plot of residuals for a linear fit to
                   the pressure/density data

                         2.4Standard normal quantile

                         1.2

                         0.0

                       -1.2

                                    -2.0 -1.0 0 1.0 2.0
                                        Standardized residual quantile

                   Figure 9.4 Normal plot of standardized residuals for
                   a linear fit to the pressure/density data

9.1.2         Inference for the Slope Parameter

              Especially in applications of the simple linear regression model (9.4) where x
              represents a variable that can be physically manipulated by the engineer, the slope
              parameter 1 is of fundamental interest. It is the rate of change of average response
              with respect to x, and it governs the impact of a change in x on the system output.
              Inference for 1 is fairly simple, because of the distributional properties that b1 (the
              slope of the least squares line) inherits from the model. That is, under model (9.4),
              b1 has a normal distribution with

                                                   Eb1 = 1

              and

                                                   Var b1 =    2             (9.13)
                                                             (x - x¯ )2
9.1 Inference Methods Related to the Least Squares Fitting of a Line (Simple Linear Regression) 659

                    which in turn imply that

                                              Z = b1 - 1
                                                           
                                                          (x - x¯ )2

                    is standard normal. In a manner similar to many of the arguments in Chapters 6
                    and 7, this motivates the fact that the quantity

                                              T=      b1 - 1               (9.14)
                                                        sLF

                                                      (x - x¯ )2

                    has a tn-2 distribution. The standard arguments of Chapter 6 applied to expression
                    (9.14) then show that

                                                               H0 : 1 = #  (9.15)
                    can be tested using the test statistic

Test statistic for                            T = b1 - #                   (9.16)
       H0 : 1 = #                                          sLF
                                                          (x - x¯ )2

                    and a tn-2 reference distribution. More importantly, under the simple linear re-
                    gression model (9.4), a two-sided confidence interval for 1 can be made using
                    endpoints

Confidence limits                             b1 ± t  sLF                  (9.17)
 for the slope, 1                                     (x - x¯ )2

                    where the associated confidence is the probability assigned to the interval between
                    -t and t by the tn-2 distribution. A one-sided interval is made in the usual way,
                    based on one endpoint from formula (9.17).

Example 1           In the context of the powder pressing study, Section 4.1 showed that the slope of
(continued )        the least squares line through the pressure/density data is

                                                     b1 = .0000486¯ (g/cc)/psi
660 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

Example 1          Then, for example, a 95% two-sided confidence interval for 1 can be made using
(continued )       the .975 quantile of the t13 distribution in formula (9.17). That is, one can use
                   endpoints

                                                 .0000486 ± 2.160 ¯             .0199

                                                                             120,000,000

                   that is,

                                                           .0000486¯ ± .0000039

                   that is,

                                      .0000448 (g/cc)/psi and .0000526 (g/cc)/psi

                        A confidence interval like this one for 1 can be translated into a confidence
                   interval for a difference in mean responses for two different values of x. Ac-
                   cording to model (9.4), two different values of x differing by x have mean
                   responses differing by 1 x. One then simply multiplies endpoints of a confi-
                   dence interval for 1 by x to obtain a confidence interval for the difference
                   in mean responses. For example, since 8,000 - 6,000 = 2,000, the difference
                   between mean densities at 8,000 psi and 6,000 psi levels has a 95% confidence
                   interval with endpoints

                                      2,000(.0000448) g/cc and 2,000(.0000526) g/cc

                   that is,

                                                     .0896 g/cc and .1052 g/cc

Considerations     Formula (9.17) allows a kind of precision to be attached to the slope of the
in the selection
                   least squares line. It is useful to consider how that precision is related to study
      of x values
                   characteristics that are potentially under an investigator's control. Notice that both
                   formulas (9.13) and (9.17) indicate that the larger (x - x¯ )2 is (i.e., the more spread

                   out the xi values are), the more precision b1 offers as an estimator of the underlying
                   slope 1. Thus, as far as the estimation of 1 is concerned, in studies where x
                   represents the value of a system variable under the control of an experimenter, he or

                   she should choose settings of x with the largest possible sample variance. (In fact,

                   if one has n observations to spend and can choose values of x anywhere in some

                   interval  [a, b],  taking  n  of  them  at  x  =  a  and  n  at  x  =  b  produces  the  best  possible
                                              2                              2
                   precision for estimating the slope 1.)
                   However, this advice (to spread the xi 's out) must be taken with a grain of salt.
                   The approximately linear relationship (9.4) may hold over only a limited range of

                   possible x values. Choosing experimental values of x beyond the limits where it is

                   reasonable to expect formula (9.4) to hold, hoping thereby to obtain a good estimate
9.1 Inference Methods Related to the Least Squares Fitting of a Line (Simple Linear Regression) 661

                         of slope, is of course nonsensical. And it is also important to recognize that precise
                         estimation of 1 under the assumptions of model (9.4) is not the only consideration
                         when planning data collection. It is usually also important to be in a position to tell
                         when the linear form of (9.4) is inappropriate. That dictates that data be collected
                         at a number of different settings of x, not simply at the smallest and largest values
                         possible.

9.1.3            Inference for the Mean System Response
                 for a Particular Value of x

                 Chapters 7 and 8 repeatedly considered the problem of estimating the mean of y
                 under a particular one (or combination) of the levels of the factor (or factors) of
                 interest. In the present context, the analog is the problem of estimating the mean
                 response for a fixed value of the system variable x,

                      µy|x = 0 + 1x            (9.18)

                 The natural data-based approximation of the mean in formula (9.18) is the corre-
                 sponding y value taken from the least squares line. The notation

   Estimator of       y^ = b0 + b1x            (9.19)
µy|x = 0 + 1x

                 will be used for this value on the least squares lines. (This is in spite of the fact that
                 the value in formula (9.19) may not be a fitted value in the sense that the phrase
                 has most often been used to this point. x need not be equal to any of x1, x2, . . . , xn
                 for both expressions (9.18) and (9.19) to make sense.) The simple linear regression
                 model (9.4) leads to simple distributional properties for y^ that then produce inference
                 methods for µy|x .

                      Under model (9.4), y^ has a normal distribution with

                      E y^ = µy|x = 0 + 1x

                 and

                      2 1 (x - x¯ )2
                      Var y^ =  +              (9.20)
                      n            (x - x¯ )2

                 (In expression (9.20), notation is being abused somewhat. The i subscripts and
                 indices of summation in (x - x¯ )2 have been suppressed. This summation runs

                 over the n values xi included in the original data set. On the other hand, in the
                 (x - x¯ )2 term appearing as a numerator in expression (9.20), the x involved is not
662 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

                           necessarily equal to any of x1, x2, . . . , xn. Rather, it is simply the value of the system
                           variable at which the mean response is to be estimated.) Then

                           Z=                                         y^ - µy|x
                                 
                                                                   1 (x - x¯ )2
                                                                      +
                                                                   n     (x - x¯ )2

                           has a standard normal distribution. This in turn motivates the fact that

                           T=                                         y^ - µy|x                      (9.21)
                                 sLF
                                                                   1 (x - x¯ )2
                                                                      +
                                                                   n     (x - x¯ )   2

                           has a tn-2 distribution. The standard arguments of Chapter 6 applied to expression
                           (9.21) then show that

                           H0 : µy|x = #                                                             (9.22)

                           can be tested using the test statistic

                           T=                                         y^ - #                         (9.23)
                                 sLF
Test statistic for                                                 1 (x - x¯ )2
     H0 : µy|x = #                                                    +
                                                                   n     (x - x¯ )   2

                           and a tn-2 reference distribution. Further, under the simple linear regression model
                           (9.4), a two-sided individual confidence interval for µy|x can be made using end-
                           points

        Confidence limits  y^ ± tsLF                               1 (x - x¯ )2
for the mean repsonse,                                                +                              (9.24)
                                                                   n     (x - x¯ )   2
            µy|x = 0 + 1x

                           where the associated confidence is the probability assigned to the interval between
                           -t and t by the tn-2 distribution. A one-sided interval is made in the usual way
                           based on one endpoint from formula (9.24).

Example 1                  Returning again to the pressure/density study, consider making individual 95%
(continued )               confidence intervals for the mean densities of cylinders produced first at 4,000
                           psi and then at 5,000 psi.
9.1 Inference Methods Related to the Least Squares Fitting of a Line (Simple Linear Regression) 663

                                 Treating first the 4,000 psi condition, the corresponding estimate of mean
                            density is

                                                 y^ = 2.375 + .0000486¯(4,000) = 2.5697 g/cc

                            Further, from formula (9.24) and the fact that the .975 quantile of the t13 distri-
                            bution is 2.160, a precision of plus-or-minus

                                            2.160(.0199) 1 + (4,000 - 6,000)2 = .0136 g/cc
                                                               15 120,000,000

                            can be attached to the 2.5697 g/cc figure. That is, endpoints of a two-sided 95%
                            confidence interval for the mean density under the 4,000 psi condition are

                                                         2.5561 g/cc and 2.5833 g/cc

                                 Under the x = 5,000 psi condition, the corresponding estimate of mean
                            density is

                                                 y^ = 2.375 + .0000486¯(5,000) = 2.6183 g/cc

                            Using formula (9.24), a precision of plus-or-minus

                                            2.160(.0199) 1 + (5,000 - 6,000)2 = .0118 g/cc
                                                               15 120,000,000

                            can be attached to the 2.6183 g/cc figure. That is, endpoints of a two-sided 95%
                            confidence interval for the mean density under the 5,000 psi condition are

                                                         2.6065 g/cc and 2.6301 g/cc

                                 The reader should compare the plus-or-minus parts of the two confidence
                            intervals found here. The interval for x = 5,000 psi is shorter and therefore more
                            informative than the interval for x = 4,000 psi. The origin of this discrepancy
                            should be clear, at least upon scrutiny of formula (9.24). For the students' data,
                            x¯ = 6,000 psi. x = 5,000 psi is closer to x¯ than is x = 4,000 psi, so the (x - x¯ )2
                            term (and thus the interval length) is smaller for x = 5,000 psi than for x =
                            4,000 psi.

                              The phenomenon noted in the preceding example--that the length of a confi-
                         dence interval for µy|x increases as one moves away from x¯ --is an important one.
                         And it has an intuitively plausible implication for the planning of experiments where
                         an approximately linear relationship between y and x is expected, and x is under
664 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

    Inference for        the investigator's control. If there is an interval of values of x over which one wants
the intercept, 0
                         good precision in estimating mean responses, it is only sensible to center one's data

                         collection efforts in that interval.

                              Proper use of displays (9.22), (9.23), and (9.24) gives inference methods for the

                         parameter 0 in model (9.4). 0 is the y intercept of the linear relationship (9.18). So
                         by setting x = 0 in displays (9.22), (9.23), and (9.24), tests and confidence intervals
                         for 0 are obtained. However, unless x = 0 is a feasible value for the input variable
                         and the region where the linear relationship (9.18) is a sensible description of
                         physical reality includes x = 0, inference for 0 alone is rarely of practical interest.

                              The confidence intervals represented by formula (9.24) carry individual associ-

                         ated confidence levels. Section 7.3 showed that it is possible (using the P-R method)

                         to give simultaneous confidence intervals for r possibly different means, µi . This
                         comes about essentially by appropriately increasing the t multiplier used in the plus-

                         or-minus part of the formula for individual confidence limits. Here it is possible, by

                         replacing t in formula (9.24) with a larger value, to give simultaneous confidence

                         intervals for all means µy|x . That is, under model (9.4), simultaneous two-sided
                         confidence intervals for all mean responses µy|x can be made using respective end-
                         points

Simultaneous two-                              1 + (x - x¯ )2                            (9.25)
  sided confidence       (b0 + b1x) ± 2 f sLF  n                             (x - x¯ )2
         limits for all
          means, µy|x

                         where for positive f , the associated simultaneous confidence is the F2,n-2 probability
                         assigned to the interval (0, f ).

                              Of course, the practical meaning of the phrase "for all means µy|x " is more
                         like "for all mean responses in an interval where the simple linear regression model
                         (9.4) is a workable description of the relationship between x and y." As is always
                         the case in curve- and surface-fitting situations, extrapolation outside of the range
                         of x values where one has data (and even to some extent interpolation inside that
                         range) is risky business. When it is done, it should be supported by subject-matter
                         expertise to the effect that it is justifiable.

                              It may be somewhat difficult to grasp the meaning of a simultaneous confidence
                         figure applicable to all possible intervals of the form (9.25). To this point, the
                         confidence levels considered have been for finite sets of intervals. Probably the
                         best way to understand the theoretically infinite set of intervals given by formula
                         (9.25) is as defining a region in the (x, y)-plane thought likely to contain the line
                         µy|x = 0 + 1x. Figure 9.5 is a sketch of a typical confidence region represented
                         by formula (9.25). There is a region indicated about the least squares line whose
                         vertical extent increases with distance from x¯ and which has the stated confidence
                         in covering the line describing the relationship between x and µy|x .
9.1 Inference Methods Related to the Least Squares Fitting of a Line (Simple Linear Regression) 665
                                                         y

              Least
              squares
              line

Example 1                                                       Simultaneous
(continued )                                                    confidence region
                                                                for all mean responses

                                                                                                   x

                                    Figure 9.5 Region in the (x, y)-plane defined
                                    by simultaneous confidence intervals for all values
                                    of µy|x

              It is instructive to compare what the P-R method of Section 7.3 and formula
              (9.25) give for simultaneous 95% confidence intervals for mean cylinder densities
              produced under the five conditions actually used by the students in their study.

                   First, formula (7.28) of Section 7.3 shows that with n - r = 15 - 5 = 10
              degrees of freedom for sP and r = 5 conditions under study, 95% simultaneous
              two-sided confidence limits for all five mean densities are of the form

                                                                   sP
                                                     y¯ i ± 3.103 

                                                                     ni
              which in the present context is

                                                                 .0206
                                                    y¯ i ± 3.103 

                                                                     3
              that is,

                                                     y¯ i ± .0369 g/cc

                   Then, since 1 = 2 and 2 = 13 degrees of freedom are involved in the use
              of formula (9.25), simultaneous limits of the form

                                      y^ ± 2(3.81) sLF 1 + (x - 6,000)2
                                                              15 120,000,000
666 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

Example 1     Table 9.5
(continued )  Simultaneous (and Individual) 95% Confidence Intervals for Mean Cylinder Densities

                               µy|x               µy|x                          µy|x
                          (P-R Method)
                 x,                            (from formula (9.25)) (from formula (9.24))
              Pressure    Mean Density
                                               Mean Density                     Mean Density

               2,000 psi  2.4790 ± .0369 g/cc  2.4723 ± .0246 g/cc              2.4723 ± .0136 g/cc
               4,000 psi  2.5693 ± .0369 g/cc  2.5697 ± .0174 g/cc              2.5697 ± .0118 g/cc
               6,000 psi  2.6520 ± .0369 g/cc  2.6670 ± .0142 g/cc              2.6670 ± .0111 g/cc
               8,000 psi  2.7687 ± .0369 g/cc  2.7643 ± .0174 g/cc              2.7643 ± .0118 g/cc
              10,000 psi  2.8660 ± .0369 g/cc  2.8617 ± .0246 g/cc              2.8617 ± .0136 g/cc

              are indicated. Table 9.5 shows the five intervals that result from the use of each
              of the two simultaneous confidence methods, together with individual intervals
              (9.24).

                   Two points are evident from Table 9.5. First, the intervals that result from
              formula (9.25) are somewhat wider than the corresponding individual intervals
              given by formula (9.24). But it is also clear that the use of the simple linear
              regression model assumptions in preference to the more general one-way as-
              sumptions of Chapter 7 can lead to shorter simultaneous confidence intervals and
              correspondingly sharper real-world engineering inferences.

9.1.4         Prediction and Tolerance Intervals (Optional )

              Inference for µy|x is one kind of answer to the qualitative question, "If I hold
              the input variable x at some particular level, what can I expect in terms of a
              system response?" It is an answer in terms of mean or long-run average response.
              Sometimes an answer in terms of individual responses is of more practical use.
              And in such cases it is helpful to know that the simple linear regression model
              assumptions (9.4) lead to their own specialized formulas for prediction and tolerance
              intervals.

                   The basic fact that makes possible prediction intervals under assumptions (9.4) is
              that if yn+1 is one additional observation, coming from the distribution of responses
              corresponding to a particular x, and y^ is the corresponding fitted value at that x
              (based on the original n data pairs), then

                          T=                   yn+1 - y^
                                sLF
                                               1 (x - x¯ )2
                                               1+ +
                                               n          (x - x¯ )          2
9.1 Inference Methods Related to the Least Squares Fitting of a Line (Simple Linear Regression) 667
                         has a tn-2 distribution. This fact leads in the usual way to the conclusion that under
                         model (9.4) the two-sided interval with endpoints

         Simple linear          y^ ± tsLF  1+ 1 +  (x - x¯ )2   (9.26)
             regression                         n   (x - x¯ )2

prediction limits for
an additional y at a

                 given x

                           can be used as a prediction interval for an additional observation y at a particular
                           value of the input variable x. The associated prediction confidence is the probability
                           that the tn-2 distribution assigns to the interval between -t and t. One-sided intervals
                           are made in the usual way, by employing only one of the endpoints (9.26) and
                           adjusting the confidence level appropriately.

                                It is possible not only to derive prediction interval formulas from the simple
                           linear regression model assumptions but also to develop relatively simple formulas
                           for approximate one-sided tolerance bounds. That is, the intervals

A one-sided tolerance                      (y^ -  sLF, )        (9.27)
       interval for the y
        distribution at x

                           and

    Another one-sided           (-, y^ +  sLF)                  (9.28)
tolerance interval for
the y distribution at x

                           can be used as one-sided tolerance intervals for a fraction p of the underlying
                           distribution of responses corresponding to a particular value of the system variable
                           x, provided  is appropriately chosen (depending upon the data, p, x, and the desired
                           confidence level).

                                In order to write down a reasonably clean formula for  , the notation

           The ratio of                    1 (x - x¯ )2
Var y^ to  for simple           A= +                            (9.29)
                                           n  (x - x¯ )   2
     linear regression

                           will be adopted for the multiplier that is used (e.g., in formula (9.24)) to go from an
                           estimate of  to an estimate of the standard deviation of y^ . Then, for approximate
668 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

                                   level confidence in locating a fraction p of the responses y at the x of interest, 
                                  appropriate for use in interval (9.27) or (9.28) is

                                 Qz( p) + AQz( )         1+ 1             Q2z ( p)  2- Qz ( )
                           =                                  2(n - 2)
     Multiplier to use in                                                 A2
interval (9.27) or (9.28)                                1 - Q2z ( )
                                                              2(n - 2)                         (9.30)

Example 1                  To illustrate the use of prediction and tolerance interval formulas in the simple
(continued )               linear regression context, consider a 90% lower prediction bound for a single
                           additional density in powder pressing, if a pressure of 4,000 psi is employed.
                           Then, additionally consider finding a 95% lower tolerance bound for 90% of
                           many additional cylinder densities if that pressure is used.

                                Treating first the prediction problem, formula (9.26) shows that an appropri-
                           ate prediction bound is

                               2.5697 - 1.350(.0199) 1 + 1 + (4,000 - 6,000)2 = 2.5796 - .0282
                                                                   15 120,000,000

                           that is,

                                                                    2.5514 g/cc

                                If, rather than predicting a single additional density for x = 4,000 psi, it is
                           of interest to locate 90% of additional densities corresponding to a 4,000 psi
                           pressure, a tolerance bound is in order. First use formula (9.29) and find that

                                                   A = 1 + (4,000 - 6,000)2 = .3162
                                                            15 120,000,000

                           Next, for 95% confidence, applying formula (9.30),

                                 1.282 + (.3162)(1.645)   1+ 1            (1.282)2 - (1.645)2
                           =                                   2(15 - 2)  (.3162)2

                                                         1 - (1.645)2                          = 2.149
                                                              2(15 - 2)
9.1 Inference Methods Related to the Least Squares Fitting of a Line (Simple Linear Regression) 669

                            So finally, an approximately 95% lower tolerance bound for 90% of densities
                            produced using a pressure of 4,000 psi is (via formula (9.27))

                                 2.5697 - 2.149(.0199) = 2.5697 - .0428

                       that is,

                                 2.5269 g/cc

    Cautions about          The fact that curve-fitting facilitates interpolation and extrapolation makes it
     prediction and    imperative that care be taken in the interpretation of prediction and tolerance in-
tolerance intervals    tervals. All of the warnings regarding the interpretation of prediction and tolerance
                       intervals raised in Section 6.6 apply equally to the present situation. But the new
        in regression  element here (that formally, the intervals can be made for values of x where one
                       has absolutely no data) requires additional caution. If one is to use formulas (9.26),
                       (9.27), and (9.28) at a value of x not represented among x1, x2, . . . , xn, it must
                       be plausible that model (9.4) not only describes system behavior at those x values
                       where one has data, but at the additional value of x as well. And even when this is
                       "plausible" the application of formulas (9.26), (9.27), and (9.28) to new values of
                       x should be treated with a good dose of care. Should one's (unverified) judgment
                       prove wrong, the nominal confidence level has unknown practical relevance.

9.1.5                  Simple Linear Regression and ANOVA

                       Section 7.4 illustrates how, for unstructured studies, partition of the total sum of
                       squares into interpretable pieces provides both (1) intuition and quantification re-
                       garding the origin of observed variation and also (2) the basis for an F test of "no
                       differences between mean responses." It turns out that something similar is possible
                       in simple linear regression contexts.

                            In the unstructured context of Section 7.4, it was useful to name the difference
                       between SSTot and SSE. The corresponding convention for curve- and surface-fitting
                       situations is stated next in definition form.

Definition 2 In curve- and surface-fitting analyses of multisample studies, the difference
                                                         SSR = SSTot - SSE

                      will be called the regression sum of squares.
670 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

                                    It is not obvious, but the difference referred to in Definition 2 in general has the
                               form of a sum of squares of appropriate quantities. In the present context of fitting
                               a line by least squares,

                                                     n

                                       SSR = (y^ i - y¯ )2

                                                    i =1

                                    Without using the particular terminology of Definition 2, this text has already
                               made fairly extensive use of SSR = SSTot - SSE. A review of Definition 3 in Chap-
                               ter 4 (page 130), and Definitions 4 and 6 in Chapter 7 (page 484) will show that in
                               curve- and surface-fitting contexts,

        The coefficient of             R2 = SSR                                                        (9.31)
       determination for                      SSTot
simple linear regression
        in sum of squares

                     notation

                               That is, SSR is the numerator of the coefficient of determination defined first in
                               Definition 3 (Chapter 4). It is commonly thought of as the part of the raw variability
                               in y that is accounted for in the curve- or surface-fitting process.

                                    SSR and SSE not only provide an appealing partition of SSTot but also form the
                               raw material for an F test of

                                       H0 : 1 = 0                                                      (9.32)

                               versus

                                       Ha : 1 = 0                                                      (9.33)

                               Under model (9.4), hypothesis (9.32) can be tested using the statistic

 An F statistic for                          SSR/1  SSR/1                                              (9.34)
testing H0 : 1 = 0                     F= 2 =
                                       sLF S S E/(n - 2)

                               and an F1,n-2 reference distribution, where large observed values of the test statistic
                               constitute evidence against H0.

                                    Earlier in this section, the general null hypothesis H0 : 1 = # was tested using
                               the t statistic (9.16). It is thus reasonable to consider the relationship of the F

                               test indicated in displays (9.32), (9.33), and (9.34) to the earlier t test. The null
                               hypothesis H0 : 1 = 0 is a special form of hypothesis (9.15), H0 : 1 = #. It is the
                               most frequently tested version of hypothesis (9.15) because it can (within limits)

                               be interpreted as the null hypothesis that mean response doesn't depend on x.

                               This is because when hypothesis (9.32) is true within the simple linear regression
                               model (9.4), µy|x = 0 + 0 · x = 0, which doesn't depend on x. (Actually, a better
                               interpretation of a test of hypothesis (9.32) is as a test of whether a linear term in
9.1 Inference Methods Related to the Least Squares Fitting of a Line (Simple Linear Regression) 671

                         x adds significantly to one's ability to model the response y after accounting for an
                         overall mean response.)

                              If one then considers testing hypotheses (9.32) and (9.33), it might appear that
                         the # = 0 version of formula (9.16) and formula (9.34) represent two different testing
                         methods. But they are equivalent. The statistic (9.34) turns out to be the square of
                         the # = 0 version of statistic (9.16), and (two-sided) observed significance levels
                         based on statistic (9.16) and the tn-2 distribution turn out to be the same as observed
                         significance levels based on statistic (9.34) and the F1,n-2 distribution. So, from one
                         point of view, the F test specified here is redundant, given the earlier discussion. But
                         it is introduced here because of its relationship to the ANOVA ideas of Section 7.4,
                         and because it has an important natural generalization to more complex curve- and
                         surface-fitting contexts. (This generalization is discussed in Section 9.2 and cannot
                         be made equivalent to a t test.)

                              The partition of SSTot into its parts, SSR and SSE, and the calculation of the
                         statistic (9.34) can be organized in ANOVA table format. Table 9.6 shows the general
                         format that this book will use in the simple linear regression context.

                     Table 9.6
                     General Form of the ANOVA Table for Simple Linear Regression

                             ANOVA Table (for testing H0 : 1 = 0)

                     Source  SS                df   MS           F

                     Regression SSR            1    SSR/1        MSR/MSE
                                               n-2  SSE/(n - 2)
                     Error   SSE

                     Total   SSTot n - 1

Example 1     Recall again from the discussion of the pressure/density example in Section 4.1
(continued )  that

                                          SSTot = (y - y¯ )2 = .289366

              Also, from page 654 recall that

                             SSE = (y - y^ )2 = .005153

              Thus,

                     SSR = SSTot - SSE = .289366 - .005153 = .284213

              and the specific version of Table 9.6 for the present example is given as Table 9.7.
672 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

Example 1          Then the observed level of significance for testing H0 : 1 = 0 is
(continued )
                                    P[an F1,13 random variable > 717] < .001

              and one has very strong evidence against the possibility that 1 = 0. A linear term
              in Pressure is an important contributor to one's ability to describe the behavior
              of Cylinder Density. This is, of course, completely consistent with the earlier
              interval-oriented analysis that produced 95% confidence limits for 1 of

                                 .0000448 (g/cc)/psi and .0000526 (g/cc)/psi

              that do not bracket 0.
                   The value of R2 = .9822 (found first in Section 4.1) can also be easily

              derived, using the entries of Table 9.7 and the relationship (9.31).

              Table 9.7
              ANOVA Table for the Pressure/Density Data

              ANOVA Table (for testing H0 : 1 = 0)

              Source  SS                         d f MS                      F

              Regression .284213 1 .284213 717

              Error   .005153 13 .000396

              Total   .289366 14

9.1.6         Simple Linear Regression and Statistical Software

              Many of the calculations needed for the methods of this section are made easier
              by statistical software packages. None of the methods of this section are so com-
              putationally intensive that they absolutely require the use of such software, but it
              is worthwhile to consider its use in the simple linear regression context. Learning
              where on a typical printout to find the various summary statistics corresponding
              to calculations made in this section helps in locating important summary statistics
              for the more complicated curve- and surface-fitting analyses of the next section.
              Printout 1 is from a MINITAB analysis of the pressure/density data.

WWW           Printout 1 Simple Linear Regression for the Pressure/Density Data (Example 1)

              Regression Analysis

              The regression equation is
              density = 2.38 +0.000049 pressure
9.1 Inference Methods Related to the Least Squares Fitting of a Line (Simple Linear Regression) 673

Predictor            Coef         StDev          T           P
Constant        2.37500        0.01206    197.01       0.000
pressure    0.00004867     0.00000182                  0.000
                                           26.78

S = 0.01991      R-Sq = 98.2%         R-Sq(adj) = 98.1%

Analysis of Variance

Source            DF              SS            MS            F            P
                           0.28421       0.28421       717.06        0.000
Regression         1       0.00515       0.00040
                           0.28937
Residual Error 13

Total             14

Obs pressure      density            Fit      StDev Fit   Residual            St Resid
                  2.48600      2.47233           0.00890   0.01367                  0.77
1          2000   2.47900      2.47233           0.00890   0.00667                  0.37
                  2.47200      2.47233           0.00890
2          2000   2.55800      2.56967           0.00630  -0.00033                -0.02
                  2.57000      2.56967           0.00630  -0.01167                -0.62
3          2000   2.58000      2.56967           0.00630
                  2.64600      2.66700           0.00514   0.00033                  0.02
4          4000   2.65700      2.66700           0.00514   0.01033                  0.55
                  2.65300      2.66700           0.00514  -0.02100                -1.09
5          4000   2.72400      2.76433           0.00630  -0.01000                -0.52
                  2.77400      2.76433           0.00630  -0.01400                -0.73
6          4000   2.80800      2.76433           0.00630  -0.04033                -2.14R
                  2.86100      2.86167           0.00890   0.00967                  0.51
7          6000   2.87900      2.86167           0.00890   0.04367                  2.31R
                  2.85800      2.86167           0.00890  -0.00067                -0.04
8          6000                                            0.01733                  0.97
                                                          -0.00367                -0.21
9          6000

10         8000

11         8000

12         8000

13      10000

14      10000

15      10000

R denotes an observation with a large standardized residual

Predicted Values

    Fit StDev Fit          95.0% CI                    95.0% PI

2.61833 0.00545 ( 2.60655, 2.63011) ( 2.57374, 2.66293)

     Printout 1 is typical of summaries of regression analyses printed by commer-
cially available statistical packages. The most basic piece of information on the
printout is, of course, the fitted equation. Immediately below it is a table giving (to
more significant digits) the estimated coefficients (b0 and b1), their estimated stan-
dard deviations, and the t ratios (appropriate for testing whether coefficients  are
0) made up as the quotients. The printout includes the values of sLF and R2 and an
ANOVA table much like Table 9.7. For the several observed values of test statistics
printed out (including the observed value of F from formula (9.34)), MINITAB
gives observed levels of significance. The ANOVA table is followed by a table of
values of y, fitted y,

                           "StDev Fit" = sLF  1 (x - x¯ )2
                                                    +
                                              n           (x - x¯ )2
674 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

                                  and residual, and standardized residual corresponding to the n data points. MINI-
                                  TAB's regression program has an option that allows one to request fitted values,
                                  confidence intervals for µy|x , and prediction intervals for x values of interest, and
                                  Printout 1 finishes with this information for the value x = 5,000.

                                       The reader is encouraged to compare the information on Printout 1 with the
                                  various results obtained in Example 1 and verify that everything on the printout
                                  (except the "adjusted R2" value) is indeed familiar.

Section 1 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Return to the situation of Exercise 3 of Section      (a) Find estimates of the parameters 0, 1, and 
   4.1 and the polymer molecular weight study of R.           in the simple linear regression model y = 0 +
   Harris.                                                    1x + . How does your estimate of  based
    (a) Find sLF for these data. What does this intend        on the simple linear regression model compare
        to measure in the context of the engineering          with the pooled sample standard deviation, sP?
        problem?
   (b) Plot both residuals versus x and the standard-    (b) Compute residuals and standardized residuals.
        ized residuals versus x. How much difference          Plot both against x and y^ and normal-plot them.
        is there in the appearance of these two plots?        How much do the appearances of the plots of
    (c) Give a 90% two-sided confidence interval for          the standardized residuals differ from those of
        the increase in mean average molecular weight         the raw residuals?
        that accompanies a 1C increase in temperature
        here.                                            (c) Make a 90% two-sided confidence interval for
   (d) Give individual 90% two-sided confidence in-           the increase in mean compressive strength that
        tervals for the mean average molecular weight         accompanies a .1 increase in the water/cement
        at 212C and also at 250C.                             ratio. (This is .11).
    (e) Give simultaneous 90% two-sided confidence
        intervals for the two means indicated in part    (d) Test the hypothesis that the mean compressive
        (d).                                                  strength doesn't depend on the water/cement
    (f) Give 90% lower prediction bounds for the next         ratio. What is the p-value?
        average molecular weight, first at 212C and
        then at 250C.                                    (e) Make a 95% two-sided confidence interval for
   (g) Give approximately 95% lower tolerance                 the mean strength of specimens with the wa-
        bounds for 90% of average molecular weights,          ter/cement ratio .5 (based on the simple linear
        first at 212C and then at 250C.                       regression model).
   (h) Make an ANOVA table for testing H0 : 1 = 0
        in the simple linear regression model. What is   (f) Make a 95% two-sided prediction interval for
        the p-value here for a two-sided test of this         the strength of an additional specimen with
        hypothesis?                                           the water/cement ratio .5 (based on the simple
                                                              linear regression model).
2. Return to the situation of Chapter Exercise 1 of
   Chapter 4 and the concrete strength study of Nichol-  (g) Make an approximately 95% lower tolerance
   son and Bartle.                                            bound for the strengths of 90% of additional
                                                              specimens with the water/cement ratio .5
                                                              (based on the simple linear regression model).
9.2 Inference Methods for General Least Squares Curve- and Surface-Fitting (Multiple Linear Regression) 675

                                                qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

         9.2 Inference Methods for General Least
                Squares Curve- and Surface-Fitting
                (Multiple Linear Regression)

                                  The previous section presented formal inference methods available under the (nor-
                                  mal) simple linear regression model. Confidence interval estimation, hypothesis
                                  testing, prediction and tolerance intervals, and ANOVA were all seen to have sim-
                                  ple linear regression versions. This section makes a parallel study of more gen-
                                  eral curve- and surface-fitting contexts. First, the multiple linear regression model
                                  and its corresponding variance estimate and standardized residuals are introduced.
                                  Then, in turn, there are discussions of how multiple linear regression computer
                                  programs can (1) facilitate inference for rate of change parameters in the model,
                                  (2) make possible inference for the mean system response at a given combination
                                  of values for the input/system variables and the making of prediction and toler-
                                  ance intervals, and (3) allow the use of ANOVA methods in multiple regression
                                  contexts.

9.2.1  The Multiple Linear Regression Model, Corresponding
       Variance Estimate, and Standardized Residuals

       This section considers situations like those treated on a descriptive level in Section
       4.2, where for k system variables x1, x2, . . . , xk and a response y, an approximate
       relationship like

       y  0 + 1x1 + 2x2 + · · · + k xk  (9.35)

       holds. As in Section 4.2, the form (9.35) not only covers those circumstances where
       x1, x2, . . . , xk all represent physically different variables but also describes contexts
       where some of the variables are functions of others. For example, the relationship

                                           y  0 + 1x1 + 2x12

       can be thought of as a k = 2 version of formula (9.35), where x2 is a deterministic
       function of x1, x2 = x12.

            As in Section 4.2, a double subscript notation will be used for the values of the in-
       put variables. Thus, the problem considered is that of inference based on the data vec-
       tors (x11, x21, . . . , xk1, y1), (x12, x22, . . . , xk2, y2), . . . , (x1n, x2n, . . . , xkn, yn). As
       always, a probability model is needed to support formal inferences for such data,
       and the one considered here is an appropriate specialization of the general one-way
       normal model of Section 7.1. That is, the standard assumptions of the multiple linear
       regression model are that there are underlying normal distributions for the response
676 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

                            y with a common variance  2 but means µy|x ,x ,...,x that change linearly with each

                                                                                                                      12 k

                            of x1, x2, . . . , xk. In symbols, it is typical to write that for i = 1, 2, . . . , n,

The (normal) multiple       yi = 0 + 1x1i + 2x2i + · · · + k xki + i                                   (9.36)
       linear regression
                     model

                            where the i are (unobservable) iid normal (0,  2) random variables, the x1i , x2i , . . . ,
                            xki are known constants, and 0, 1, 2, . . . , k and  2 are unknown model param-
                            eters (fixed constants). This is the specialization of the general one-way model

                                    yi j = µi + i j

                            to the situation where the means µy|x1,x2,...,xk satisfy the relationship

                            µy|x1,x2,...,xk = 0 + 1x1 + 2x2 + · · · + k xk                             (9.37)

                            If one thinks of formula (9.37) as defining a surface in (k + 1)-dimensional space,
                            then the model equation (9.36) simply says that responses y differ from correspond-
                            ing values on that surface by mean 0, variance  2 random noise. Figure 9.6 illustrates
                            this point for the simple k = 2 case (where x1 and x2 are not functionally related).

                                 Inferences about quantities involving those (x1, x2, . . . , xk) combinations repre-
                            sented in the data, like the mean response at a single (x1, x2, . . . , xk) or the difference
                            between two such mean responses, will typically be sharper when methods based

                            on model (9.36) can be used in place of the general methods of Chapter 7. And as

                            was true for simple linear regression, to the extent that it is sensible to assume that

                            model (9.36) describes system behavior for values of x1, x2, . . . , xk not included

                                 y
                            0

                                    Surface defined by
                                    µ y|x1, x2 = 0 + 1x1 +  2x2

                                                                 Distributions of y x1
                                                                 for 2 different
                                                                 (x1, x2) pairs

                             x2

                            Figure 9.6 Graphical representation of the multiple linear
                            regression model y = 0 + 1x1 + 2x2 +
9.2 Inference Methods for General Least Squares Curve- and Surface-Fitting (Multiple Linear Regression) 677

  Estimators of the    in the data, it provides the basis for inferences involving limited interpolation and
   coefficients  in    extrapolation on the system variables x1, x2, . . . , xk.

the multiple linear         Section 4.2 contains a discussion of using statistical software in the least squares
  regression model     fitting of the approximate relationship (9.35) to a set of (x1, x2, . . . , xk, y) data.
                       That discussion can be thought of as covering the fitting and use of residuals in
   Fitted values for   model checking for the multiple linear regression model (9.36). Section 4.2 did
the multiple linear    not produce explicit formulas for b0, b1, b2, . . . , bk, the (least squares) estimates of
                       0, 1, 2, . . . , k. Instead it relied on the software to produce those estimates. Of
  regression model     course, once one has estimates of the 's, corresponding fitted values immediately
                       become
        Residuals for
the multiple linear                    y^ i = b0 + b1x1i + b2x2i + · · · + bk xki  (9.38)

  regression model     with residuals

                                       ei = yi - y^ i                              (9.39)

                            The residuals (9.39) can be used to make up an estimate of  2. One divides
                       a sum of squared residuals by an appropriate number of degrees of freedom. That
                       is, one can make the following definition of a (multiple linear regression or)
                       surface-fitting sample variance.

Definition 3           For a set of n data vectors (x11, x21, . . . , xk1, y1), (x12, x22, . . . , xk2, y2), . . . ,
                       (x1n, x2n, . . . , xkn, yn) where least squares fitting produces fitted values given
                       by formula (9.38) and residuals (9.39),

                       sSF 2 = 1 n - k - 1 (y - y^ )2 = 1 n - k - 1 e2             (9.40)

                       will be called a surface-fitting sample variance. Associated with it are  =
                       n - k - 1 degrees of freedom and an estimated standard deviation of response,

                       sSF = sSF 2 .

                            Compare Definitions 1 and 3 and notice that the k = 1 version of sSF 2 is just sLF 2
                       from simple linear regression. sSF estimates the level of basic background variation,
                        , whenever the model (9.36) is an adequate description of the system under study.

                       When it is not, sSF will tend to overestimate . So comparing sSF to sP is another
                       way of investigating the appropriateness of that description. (sSF much larger than
                       sP suggests that model (9.36) is a poor one.)
678 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

              Example 2  Inference in the Nitrogen Plant Study
(Example 5, Chapter 4,
                         The main example in this section will be the nitrogen plant data set given in Table
  revisited--page 150)   4.8. Recall that in the discussion of the example, with

                                              x1 = a measure of air flow
                                              x2 = the cooling water inlet temperature
                                               y = a measure of stack loss

                         the fitted equation

                                              y^ = -15.409 - .069x1 + .528x2 + .007x12

                         appeared to be a sensible data summary. Accordingly, consider the making of
                         inferences based on the k = 3 version of model (9.36),

                                              yi = 0 + 1x1i + 2x2i + 3x1i2 + i                (9.41)

                              Printout 2 is from a MINITAB analysis of the data of Table 4.8. Among

                         many other things, it gives the values of the residuals from the fitted version of
                         formula (9.41) for all n = 17 data points. It is then possible to apply Definition
                         3 and produce a surface-fitting estimate of the parameter  2 in the model (9.41).

                         That is,

                         sS2F = 1 17 - 3 - 1 (.053)2 + (-.125)2 + · · · + (.265)2 + (2.343)2
                             = 1.26

                         so a corresponding estimate of  is
                                                                         

                                                                 sSF = 1.26
                                                                      = 1.125

                         (The units of y--and therefore sSF--are .1% of incoming ammonia escaping
                         unabsorbed.)

                              In routine practice it is a waste to do even these calculations, since multiple
                         regression programs typically output sSF as part of their analysis. The reader
                         should take time to locate the value sSF = 1.125 on Printout 2. If one accepts
                         the relevance of model (9.41), for fixed values of airflow and inlet temperature
                         (and therefore airflow squared), the standard deviation associated with many
                         days' stack losses produced under those conditions would then be expected to be
                         approximately .1125%.
9.2 Inference Methods for General Least Squares Curve- and Surface-Fitting (Multiple Linear Regression) 679

Printout 2 Multiple Linear Regression for the Stack Loss Data (Example 2)

Regression Analysis

The regression equation is
y = - 15.4 - 0.069 x1 + 0.528 x2 + 0.00682 x1**2

Predictor          Coef         StDev         T         P
Constant        -15.41          12.60   -1.22     0.243
x1            -0.0691          0.3984   -0.17     0.865
x2              0.5278         0.1501             0.004
x1**2        0.006818       0.003178     3.52     0.051
                                         2.15

S = 1.125        R-Sq = 98.0%         R-Sq(adj) = 97.5%

Analysis of Variance

Source            DF              SS          MS         F        P
                            799.80      266.60    210.81    0.000
Regression        3
                             16.44         1.26
Residual Error 13           816.24

Total             16

Source       DF       Seq SS
x1
x2           1        775.48
x1**2
             1           18.49

             1           5.82

Obs          x1          y             Fit StDev Fit Residual St Resid

1          80.0   37.000        36.947    1.121             0.053    0.57 X

2          62.0   18.000        18.125    0.407             -0.125   -0.12

3          62.0   18.000        18.653    0.462             -0.653   -0.64

4          62.0   19.000        19.181    0.553             -0.181   -0.18

5          62.0   20.000        19.181    0.553             0.819    0.84

6          58.0   15.000        15.657    0.513             -0.657   -0.66

7          58.0   14.000        13.018    0.475             0.982    0.96

8          58.0   14.000        13.018    0.475             0.982    0.96

9          58.0   13.000        12.490    0.595             0.510    0.53

10         58.0   11.000        13.018    0.475             -2.018   -1.98

11         58.0   12.000        13.546    0.378             -1.546   -1.46

12         50.0      8.000      7.680     0.493             0.320    0.32

13         50.0      7.000      7.680     0.493             -0.680   -0.67

14         50.0      8.000      8.208     0.499             -0.208   -0.21

15         50.0      8.000      8.208     0.499             -0.208   -0.21

16         50.0      9.000      8.735     0.548             0.265    0.27

17         56.0   15.000        12.657    0.298             2.343    2.16R

R denotes an observation with a large standardized residual
X denotes an observation whose X value gives it large influence.

Predicted Values

        Fit StDev Fit           95.0% CI                 95.0% PI

     15.544       0.383 ( 14.717, 16.372) ( 12.978, 18.111)
680 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

Example 2     Among the 17 data points in Table 4.8, there are only 12 different airflow/inlet
(continued )
              temperature combinations (and therefore 12 different (x1, x2, x12) vectors). The
              original data can be thought of as organized into r = 12 separate samples, one

              for  each  different  (  x  1  ,  x  2  ,  x  2  )  vector  and  there  is  thus  an  estimate  of    that
                                                            1

              doesn't depend for its validity on the appropriateness of the assumption that

              µy|x ,x = 0 + 1x1 + 2x2 + 3x12. That is, sP can be computed and compared

                      12

              it to sSF as a check on the appropriateness of model (9.41). Table 9.8 organizes
              the calculation of that pooled estimate of  .

                         Table 9.8
                         Twelve Sample Means and Four Sample Variances
                         for the Stack Loss Data

                          x1,                  x2,                          y,            y¯    s2
                          Air                 Inlet                       Stack
                                                                          Loss
                         Flow             Temperature

                         50                              18               8, 7            7.5 .5

                         50                              19               8, 8            8.0 0.0

                         50                              20               9               9.0 --

                         56                              20               15          15.0 --

                         58                              17               13          13.0 --

                         58                              18       14, 14, 11 13.0 3.0

                         58                              19               12          12.0 --

                         58                              23               15          15.0 --

                         62                              22               18          18.0 --

                         62                              23               18          18.0 --

                         62                              24               19, 20 19.5 .5

                         80                              27               37          37.0 --

                   Then
                  sP2 = 1 17 - 12 ((2 - 1)(.5) + (2 - 1)(0.0) + (3 - 1)(3.0) + (2 - 1)(.5))

                     = 1.40

              so
                                                            

                                             sP = sP2 = 1.40 = 1.183
9.2 Inference Methods for General Least Squares Curve- and Surface-Fitting (Multiple Linear Regression) 681

                       The fact that sSF = 1.125 and sP = 1.183 are in substantial agreement is
                  consistent with the work in Example 5 of Chapter 4, which found the equation

                                        y^ = -15.409 - .069x1 + .528x2 + .007x12
                  to be a good summarization of the nitrogen plant data.

                       sSF is basic to all of formal statistical inference based on the multiple lin-
                  ear regression model. But before using it to make statistical intervals and do
                  significance testing, note also that it is useful for producing standardized resid-
                  uals for the multiple linear regression model. That is, it is possible to find pos-
                  itive constants a1, a2, . . . , an (which are each complicated functions of all of
                  x11, x21, . . . , xk1, x12, x22, . . . , xk2, . . . , x1n, x2n, . . . , xkn) such that the i th residual
                  ei = yi - y^ i has

                  Var(yi - y^ i ) = ai  2

                  Then, recalling Definition 2 in Chapter 7 (page 458), corresponding to the data point
                  (x1i , x2i , . . . , xki , yi ) is the standardized residual for multiple linear regression

  Standardized      ei
   residuals for  ei =                     (9.42)
multiple linear     sSF ai

      regression

                  It is not possible to include here a simple formula for the ai that are needed to
                  compute standardized residuals. (They are of interest only as building blocks in
                  formula (9.42) anyway.) But it is easy to read the standardized residuals (9.42) off a
                  typical multiple regression printout and to plot them in the usual ways as means of
                  checking the apparent appropriateness of a candidate version of model (9.36) fit to
                  a set of n data points (x1, x2, . . . , xk, y).

Example 2         As an illustration of the use of standardized residuals, consider again Printout 2
(continued )      on page 679. The annotations on that printout locate the columns of residuals and
                  standardized residuals for model (9.41). Figure 9.7 depicts normal probability
                  plots, first of the raw residuals and then of the standardized residuals.

                       There are only the most minor differences between the appearances of the
                  two plots in Figure 9.7, suggesting that decisions concerning the appropriateness
                  of model (9.41) based on raw residuals will not be much altered by the more
                  sophisticated consideration of standardized residuals instead.
682 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

Standard normal quantile2.4                                                                                   2.4
                                                                                    Standard normal quantile
1.2                                                                                                           1.2

 0.0                                                                                                           0.0

                                      2                                                                                                             2

-1.2                                                                                                          -1.2

     -1.60 -0.80 0.00 0.80 1.60 2.40                                                                               -1.60 -0.80 0.00 0.80 1.60 2.40
                       Residual quantile                                                                                   Standardized residual quantile

     Figure 9.7 Normal plots of residuals and standardized residuals for the stack loss data (Example 2)

     9.2.2                               Inference for the Parameters 0, 1, 2, . . . , k

                                         Section 9.1 considered inference for the slope parameter 1 in simple linear regres-
                                         sion, treating it as a rate of change (of average y as a function of x). In the multiple
                                         regression context, if x1, x2, . . . , xk are all physically different system variables, the
                                         coefficients 1, 2, . . . , k can again be thought of as rates of change of average
                                         response with respect to x1, x2, . . . , xk, respectively. (They are partial derivatives
                                         of µy|x1,x2,...,xk with respect to the x's.) On the other hand, when some x's are
                                         functionally related to others (for instance, if k = 2 and µy|x = 0 + 1x + 2x2),
                                         individual interpretation of the 's can be less straightforward. In any case, the 's
                                         do determine the nature of the surface represented by

                                                              µy|x1,x2,...,xk = 0 + 1x1 + 2x2 + · · · + k xk

                                         and it is possible to do formal inference for 0, 1, . . . , k individually. In many
                                         instances, important physical interpretations can be found for such inferences. (For
                                         example, beginning with µy|x = 0 + 1x + 2x2, an inference that 2 is positive
                                         says that the mean response is concave up as a function of x and has a minimum
                                         value.)

                                              The key to formal inference for the 's is that under model (9.36), there are
                                         positive constants d0, d1, d2, . . . , dk (which are each complicated functions of all of
                                         x11, . . . , xk1, x12, . . . , xk2 . . . , x1n, . . . , xkn) such that the least squares coefficients
                                         b0, b1, . . . , bk are normally distributed with

                                                                                     Ebl = l

                                         and

                                                                                                              Var bl = dl  2
9.2 Inference Methods for General Least Squares Curve- and Surface-Fitting (Multiple Linear Regression) 683

                       This in turn makes it plausible that for l = 0, 1, 2, . . . , k, the quantity

Estimated standard                                             sSF dl                                 (9.43)
      deviation of bl

                       is an estimate of the standard deviation of bl and that                        (9.44)
                                                                 T = bl - l
                                                                        sSF dl

                       has a tn-k-1 distribution.
                            There is no simple way to write down formulas for the constants dl, but the

                       estimated standard deviations of the coefficients, sSF dl, are a typical part of the
                       output from multiple linear regression programs.

                            The usual arguments of Chapter 6 applied to expression (9.44) then show that

                                                               H0 : l = #                             (9.45)

                       can be tested using the test statistic

 Test statistic                                                T = bl - #                             (9.46)
for H0 : l = #                                                       sSF dl

                       and a tn-k-1 reference distribution. More importantly, under the multiple linear
                       regression model (9.36), a two-sided individual confidence interval for l can be
                       made using endpoints

Confidence limits      bl ± t sSF dl                                                                  (9.47)
                for l

                       where the associated confidence is the probability assigned to the interval between
                       -t and t by the tn-k-1 distribution. Appropriate use of only one of the endpoints
                       (9.47) gives a one-sided interval for l.

Example 2              Looking again at Printout 2 (see page 679), note that MINITAB's multiple re-
(continued )
                       gression output includes a table of estimated coefficients (bl ) and (estimated)
                       standard deviations (sSF dl). These are collected in Table 9.9.
684 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

Example 2     Table 9.9
(continued )  Fitted Coefficients and Estimates of Their Standard Deviations
              for the Stack Loss Data

              Estimated Coefficient  (Estimated) Standard Deviation
                                     of the Estimate

              b0 = -15.41            sSF d0 = 12.60
              b1 = -.0691            sSF d1 = .3984
              b2 = .5278             sSF d2 = .1501
              b3 = .006818           sSF d3 = .003178

                   Then since the upper .05 point of the t13 distribution is 1.771, from formula
              (9.47) a two-sided 90% confidence interval for 2 in model (9.41) has endpoints

                                                 .5278 ± 1.771(.1501)

              that is,

                 .2620 (.1% nitrogen loss/degree) and .7936 (.1% nitrogen loss/degree)

              This interval establishes that there is an increase in mean stack loss y with
              increased inlet temperature x2 (the interval contains only positive values). It
              further gives a way of assessing the likely impact on y of various changes in x2.
              For example, if x1 (and therefore x3 = x12) is held constant but x2 is increased by
              2, one can anticipate an increase in mean stack loss of between

                         .5240 (.1% nitrogen loss) and 1.5873 (.1% nitrogen loss)

                   As a second example of the use of formula (9.47), note that a 90% two-sided
              confidence interval for 3 has endpoints

                                              .006818 ± 1.771(.003178)

              that is,

                                                  .0012 and .0124

              3 controls the amount and direction of curvature (in the variable x1) possessed by
              the surface specified by µy|x ,x = 0 + 1x1 + 2x2 + 3x12. Since the interval

                                                                       12

              contains only positive values, it shows that at the 90% confidence level, there is
              some important concave-up curvature in the airflow variable needed to describe
              the stack loss variable. This is consistent with the picture of fitted mean response
              given previously in Figure 4.15 (see page 155).
9.2 Inference Methods for General Least Squares Curve- and Surface-Fitting (Multiple Linear Regression) 685

                              However, check that if 95% confidence is used in the calculation of the two-
                         sided interval for 3, the resulting confidence interval contains values on both
                         sides of 0. If this higher level of confidence is needed, the data in hand are not
                         adequate to establish definitively the nature of any curvature in mean stack loss
                         as a function of airflow. Any real curvature appears weak enough in comparison
                         to the basic background variation that more data are needed to decide whether
                         the surface is concave up, linear, or concave down in the variable x1.

                           Very often multiple regression programs output not only the estimated standard
                      deviations of fitted coefficients (9.43) but also the ratios

                                                                 t = bl
                                                                      sSF dl

                      and associated two-sided p-values for testing

                                                                 H0 : l = 0

                      Review Printout 2 and note that, for example, the two-sided p-value for testing
                      H0 : 3 = 0 in model (9.41) is slightly larger than .05. This is completely consistent
                      with the preceding discussion regarding the interpretation of interval estimates
                      of 3.

9.2.3                 Inference for the Mean System Response for a Particular
                      Set of Values for x1, x2, . . . , xk

                      Inference methods for the parameters 0, 1, . . . , k provide insight into the nature
                      of the relationships between x1, x2, . . . , xk and the mean response y. But other
                      methods are needed to answer the important engineering question, "What can be
                      expected in terms of system response if I use a particular combination of levels of the
                      system variables x1, x2, . . . , xk?" An answer to this question will first be phrased
                      in terms of inference methods for the mean system response µy|x1,x2,...,xk .

                           In a manner similar to what was done in Section 9.1, the notation

Estimator of          y^ = b0 + b1x1 + b2x2 + · · · + bk xk  (9.48)
  µy |x1 ,x2 ,...,xk

                      will here be used for the value produced by the least squares equation when a
                      particular set of numbers x1, x2, . . . , xk is plugged into it. (y^ may not be a fitted
                      value in the strict sense of the phrase, as the vector (x1, x2, . . . , xk) may not match
                      any data vector (x1i , x2i , . . . , xki ) used to produce the least squares coefficients
                      b0, b1, . . . , bk.) As it turns out, the multiple linear regression model (9.36) leads to
                      simple distributional properties for y^ , which then produce inference methods for
                      µy|x1,x2,...,xk .
686 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

                                          Under model (9.36), it is possible to find a positive constant A depending in
                                     a complicated way upon x1, x2, . . . , xk and all of x11, . . . , xk1, x12, . . . , xk2, . . . ,
                                     x1n, . . . , xkn (the locations at which inference is desired and at which the original
                                     data points were collected) so that y^ has a normal distribution with

                                          E y^ = µy|x1,x2,...,xk = 0 + 1x1 + · · · + k xk

                                     and

A = Var y^ /                              Var y^ =  2 A2                                            (9.49)

                                     In view of formula (9.49), it is thus plausible that

Estimated standard                                                           sSF · A                (9.50)
       deviation of y^

                                     can be used as an estimated standard deviation for y^ and that inference methods for
                                     the mean system response can be based on the fact that

                                                                         y^ - µ T = y|x1,x2,...,xk  (9.51)
                                                                                      sSF · A

                                     has a tn-k-1 distribution. That is,
                                                                          H0 : µy|x1,x2,...,xk = #

                                     can be tested using the test statistic

   Test statistic for                                                        T = y^ - #             (9.52)
H0 : µy|x ,x ,...,x = #                                                            sSF · A

                 12 k

                                     and a tn-k-1 reference distribution. Further, under the multiple linear regression
                                     model (9.36), a two-sided confidence interval for µy|x1,x2,...,xk can be made using
                                     endpoints

       Confidence limits                                                     y^ ± tsSF · A          (9.53)
for the mean response
                                     where the associated confidence is the probability assigned to the interval between
                 µy |x1 ,x2 ,...,xk  -t and t by the tn-k-1 distribution. One-sided intervals based on formula (9.53) are
                                     made in the usual way.
               Finding the
                    factor A              The practical obstacle to be overcome in the use of these methods is the compu-
                                     tation of A. Although it is not possible to give a simple formula for A, most multiple
                                     regression programs provide A for (x1, x2, . . . , xk) vectors of interest. MINITAB,
                                     for example, will fairly automatically produce values of sSF · A corresponding to
9.2 Inference Methods for General Least Squares Curve- and Surface-Fitting (Multiple Linear Regression) 687

Example 2     each data point (x1i , x2i , . . . , xki , yi ), labeled as (the estimated) standard deviation
(continued )  (of the) fit. And an option makes it possible to obtain similar information for any
              user-specified choice of (x1, x2, . . . , xk). (Division of this by sSF then produces A.)

                 Consider the problem of estimating the mean stack loss if the nitrogen plant
                 of Example 5 in Chapter 4 is operated consistently with x1 = 58 and x2 = 19.
                 (Notice that this means that x3 = x12 = 3,364 is involved.) Now the conditions
                 x1 = 58, x2 = 19, and x3 = 3,364 match perfectly those of data point number
                 11 on Printout 2 (see page 679). Thus, y^ and sSF · A for these conditions may
                 be read directly from the printout as 13.546 and .378, respectively. Then, for
                 example, from formula (9.53), a 90% two-sided confidence interval for the mean
                 stack loss corresponding to an airflow of 58 and water inlet temperature of 19
                 has endpoints

                                                    13.546 ± 1.771(.378)

              that is,

                        12.88 (.1% nitrogen loss) and 14.22 (.1% nitrogen loss)

                   As a second illustration of the use of formula (9.53), suppose that setting
              plant operating conditions at an airflow of x1 = 60 and a water inlet temperature
              of x2 = 20 is contemplated and it is desireable to have an interval estimate for the
              mean stack loss implied by those conditions. Notice that the x1 = 60, x2 = 20,
              and x3 = x12 = 3,600 vector does not exactly match that of any of the n = 17
              data points available. Therefore, some interpolation/extrapolation is required to

              make the desired interval. And it will not be possible to simply read appropriate
              values of y^ and sSF · A off Printout 2 as related to one of the data points used to
              fit the equation.

                   Location of the point with coordinates x1 = 60 and x2 = 20 on a scatterplot
              of (x1, x2) values for the original n = 17 data points (like Figure 4.19) reveals
              that the candidate operating conditions are not wildly different from those used

              to develop the fitted equation. So there is hope that the use of formula (9.53)

              will provide an inference of some practical relevance. Accordingly, the coordi-
              nates x1 = 60, x2 = 20, and x3 = x12 = 3,600 were input into MINITAB and a
              "prediction" request made, resulting in the final section of Printout 2. Reading
              from that final section of the printout, y^ = 15.544 and sSF · A = .383, so a 90%
              two-sided confidence interval for the mean stack loss has endpoints

                        15.544 ± 1.771(.383)

              that is,

                        14.86 (.1% nitrogen loss) and 16.22 (.1% nitrogen loss)

              (Of course, endpoints of a 95% interval can be read directly from the printout.)
688 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

Example 2                                   It is impossible to overemphasize the fact that the preceding two intervals are
(continued )                           dependent for their practical relevance on that of model (9.41) for not only those
                                       (x1, x2) pairs in the original data but (in the second case) also for the x1 = 60 and
                                       x2 = 20 set of conditions. Formulas like (9.53) always allow for imprecision due
                                       to statistical fluctuations/background noise in the data. They do not, however,
                                       allow for discrepancies related to the application of a model in a regime over
                                       which it is not appropriate. Formula (9.53) is an important and useful formula.
                                       But it should be used thoughtfully, with no expectation that it will magically do
                                       more than help quantify the precision provided by the data in the context of a
                                       particular set of model assumptions.

Simultaneous two-sided                      Lacking a simple explicit formula for A, it is difficult to be very concrete about
confidence limits for all              how this quantity varies. In qualitative terms, it does change with the (x1, x2, . . . , xk)
                                       vector under consideration. It is smallest when this vector is near the center of the
           mean repsonses              cloud of points (x1i , x2i , . . . , xki ) in k-dimensional space corresponding to the n
                   µy |x1 ,x2 ,...,xk  data points used to fit model (9.36). The fact that it can vary substantially is obvious
                                       from Printout 2. There for the nitrogen plant case, the estimated standard deviation
                                       of y^ given in display (9.50) varies from .298 to 1.121, indicating that A for data
                                       point 1 is about 3.8 times the size of A for data point 17 ( .298 1.121  3.8). That is, the
                                       precision with which a mean response is determined can vary widely over the region
                                       where it is sensible to use a fitted equation.

                                            Formula (9.53) provides individual confidence intervals for mean responses.
                                       Simultaneous intervals are also easily obtained by a modification of formula (9.53)
                                       similar to the one provided for simple linear regression. That is, under the multiple
                                       linear regression model, simultaneous two-sided confidence intervals for all mean
                                       responses µy|x1,x2,...,xk can be made using respective endpoints

                                                                             (9.54)
                                       y^ ± (k + 1) f sSF · A

                                       where for positive f , the associated confidence is the Fk+1,n-k-1 probability as-
                                       signed to the interval (0, f ). Formula (9.54) is related to formula (9.53) through
                                       the replacement of the multiplier t by the (larger for a given nominal confidence)
                                       multiplier (k + 1) f . When it is applied only to (x1, x2, . . . , xk) vectors found in
                                       the original n data points, formula (9.54) is an alternative to the P-R method of
                                       simultaneous intervals for means, appropriate to surface-fitting problems. When the
                                       multiple linear regression model is indeed appropriate, formula (9.54) will usually
                                       give shorter simultaneous intervals than the P-R method.

Example 2                              For making simultaneous 90% confidence intervals for the mean stack losses
(continued )
                                       at the 12 different sets of plant conditions represented in the original data set,
                                       one can use formula (9.54) with k = 3, f = 2.43 (the .9 quantile of the F4,13
                                       distribution) and the y^ and corresponding sSF · A values appearing on Printout 2
                                       (see page 679). For example, considering the x1 = 80 and x2 = 27 conditions of
9.2 Inference Methods for General Least Squares Curve- and Surface-Fitting (Multiple Linear Regression) 689

                                      observation 1 on the printout, sSF · A = 1.121 and one of the simultaneous 90%
                                      confidence intervals associated with these conditions has endpoints

                                                                  36.947 ± (3 + 1)(2.43)(1.121)
                                      or

                                                 33.452 (.1% nitrogen loss) and 40.442 (.1% nitrogen loss)

                9.2.4         Prediction and Tolerance Intervals (Optional )

Multiple regression           The second kind of answer that statistical theory can provide to the question, "What is
prediction limits for         to be expected in terms of system response if one uses a particular (x1, x2, . . . , xk)?",
                              has to do with individual responses rather than mean responses. That is, the same
  an additional y at          factor A referred to in making confidence intervals for mean responses can be used
        (x1, x2, . . . , xk)  to develop prediction and tolerance intervals for surface-fitting situations.

                                   In the first place, under model (9.36), the two-sided interval with endpoints

                                                     y^ ± tsSF 1 + A2                         (9.55)

                              can be used as a prediction interval for an additional observation at a particular
                              combination of levels of the variables x1, x2, . . . , xk. The associated prediction
                              confidence is the probability that the tn-k-1 distribution assigns to the interval
                              between -t and t. One-sided intervals are made in the usual way, by employing
                              only one of the endpoints (9.55) and adjusting the confidence level appropriately.

                                   In order to use formula (9.55), sSF · A and sSF can be taken from a multiple
                              regression printout and A obtained via division. Equivalently, it is possible to use a
                              small amount of algebra to rewrite formula (9.55) as

  An alternative                                     y^ ± t sSF 2 + (sSF · A)2                (9.56)
      formula for

prediction limits

                              and substitute sSF and sSF · A directly into formula (9.56).
                                   In order to find one-sided tolerance bounds in the surface-fitting context, begin

                              with the value of A corresponding to a particular (x1, x2, . . . , xk). If a confidence
                              level of  is desired in locating a fraction p of the underlying distribution of
                              responses, compute

    Multiplier to use               Qz( p) + AQz( )  1+ 1               Q2z ( p)    - Qz ( )2
in making tolerance           =                           2(n - k - 1)
                                                                                A2
            intervals in                             1 - Q2z ( )
 multiple regression                                      2(n - k - 1)                        (9.57)
690 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

                                 Then, the interval

A one-sided tolerance                                         (y^ -  sSF, )                            (9.58)
       interval for the y
          distribution at        or
           (x1, x2, . . . , xk)
                                                              (-, y^ +  sSF)                           (9.59)
    Another one-sided
      tolerance interval
 for the y distribution

        at (x1, x2, . . . , xk)

                                 can be used as an approximately  level one-sided tolerance interval for a fraction
                                 p of the underlying distribution of responses corresponding to (x1, x2, . . . , xk).

Example 2                            Returning to the nitrogen plant example, consider first the calculation of a 90%
(continued )                         lower prediction bound for a single additional stack loss y, if airflow of x1 = 58
                                     and water inlet temperature of x2 = 19 are used. Then consider also a 95% lower
                                     tolerance bound for 90% of many additional stack loss values if the plant is run

                                     under those conditions.
                                          Treating the prediction interval problem, recall that for x1 = 58 and x2 = 19,

                                     y^ = 13.546 and sSF · A = .378. Since sSF = 1.125 and the .9 quantile of the t13
                                     distribution is 1.350, formula (9.56) shows that the desired 90% lower prediction

                                     bound for an additional stack loss under such plant operating conditions is

                                                               13.546 - 1.350 (1.125)2 + (.378)2

                                     that is, approximately

                                                                      11.94 (.1% nitrogen loss)

                                          To not predict a single additional stack loss, but rather to locate 90% of many
                                     additional stack losses with 95% confidence, expression (9.57) is the place to
                                     begin. Note that for x1 = 58 and x2 = 19,

                                                                       A = .378/1.125 = .336

                                     so, using expression (9.57),

                                       1.282 + (.378)(1.645)  1+ 1                (1.282)2 - (1.645)2
                                 =                                 2(17 - 3 - 1)  (.378)2

                                                              1 - (1.645)2                             = 2.234
                                                                   2(17 - 3 - 1)
9.2 Inference Methods for General Least Squares Curve- and Surface-Fitting (Multiple Linear Regression) 691

                                      So finally, a 95% lower tolerance bound for 90% of stack losses produced under
                                      operating conditions of x1 = 58 and x2 = 19 is, via display (9.58),

                                                            13.546 - 2.234(1.125) = 13.546 - 2.513

                                      that is,

                                                                      11.033 (.1% nitrogen loss)

                                       The warnings raised in the previous section concerning prediction and tolerance
                                  intervals in simple regression all apply equally to the present case of multiple
                                  regression. So do points similar to those made in Example 2 (page 688) in reference
                                  to confidence intervals for the mean system response. Although they are extremely
                                  useful engineering tools, statistical intervals are never any better than the models on
                                  which they are based.

9.2.5                           Multiple Regression and ANOVA

                                Formal inference in curve- and surface-fitting contexts can (and typically should)
                                be carried out primarily using interval-oriented methods. Nevertheless, testing and
                                ANOVA methods do have their place. So the discussion now turns to the matter of
                                what ANOVA ideas provide in multiple regression.

                                     As always, SSTot will stand for (y - y¯ )2 and SSE for (y - y^ )2. Remember
                                also that Definition 2 introduced the notation SSR for the difference SSTot - SSE.
                                As remarked following Definition 2, the coefficient of determination can be written
                                in terms of SSR and SSTot as

                                        R2 = SSTot - SSE = SSR
                                        SSTot                                           SSTot

                                Further, under model (9.36), these sums of squares (SSTot, SSE, and SSR) form the
                                basis of an F test of the hypothesis

                                        H0 : 1 = 2 = · · · = k = 0                             (9.60)

                                versus

                                                                           Ha : not H0         (9.61)
                                Hypothesis (9.60) can be tested using the statistic

       F statistic for testing          F = SSR/k                                              (9.62)
H0 : 1 = 2 = · · · = k = 0                    SSE/(n - k - 1)
692 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

                                  and an Fk,n-k-1 reference distribution, where large observed values of the test statis-
                                  tic constitute evidence against H0. (The denominator of statistic (9.62) is another
                                  way of writing sS2F.)

                                       Hypothesis (9.60) in the context of the multiple linear regression model implies
                                  that the mean response doesn't depend on any of the process variables x1, x2, . . . , xk.
                                  That is, if all of 1 through k are 0, model statement (9.36) reduces to

                                                  yi = 0 + i

      Interpreting a test of  So a test of hypothesis (9.60) is often interpreted as a test of whether the mean
H0 : 1 = 2 = · · · = k = 0    response is related to any of the input variables under consideration. The calculations
                              leading to statistic (9.62) are most often organized in a table quite similar to the
                              one discussed in Section 9.1 for testing H0 : 1 = 0 in simple linear regression. The
                              general form of that table is given as Table 9.10.

                              Table 9.10

                              General Form of the ANOVA Table for Testing H0 : 1 = 2 = · · · = k = 0
                              in Multiple Regression

                                     ANOVA Table (for testing H0 : 1 = 2 = · · · = k = 0)

                              Source  SS      df     MS               F

                              Regression SSR  k      SSR/ k           MSR/MSE
                                              n-k-1  SSE/(n - k - 1)
                              Error   SSE

                              Total   SSTot n - 1

Example 2                     Once again turning to the analysis of the nitrogen plant data under the model yi =
(continued )                  0 + 1x1i + 2x2i + 3x1i2 + i , consider testing H0 : 1 = 2 = 3 = 0--that
                              is, mean stack loss doesn't depend on airflow (or its square) or water inlet
                              temperature. Printout 2 (see page 679) includes an ANOVA table for testing this
                              hypothesis, which is essentially reproduced here as Table 9.11.

                                   From Table 9.11, the observed value of the F statistic is 210.81, which is to be
                              compared to F3,13 quantiles in order to produce an observed level of significance.
                              As indicated in Printout 2, the F3,13 probability to the right of the value 210.81
                              is 0 (to three decimal places). This is definitive evidence that not all of 1, 2,
                              and 3 can be 0. Taken as a group, the variables x1, x2, and x3 = x12 definitely
                              enhance one's ability to predict stack loss.
9.2 Inference Methods for General Least Squares Curve- and Surface-Fitting (Multiple Linear Regression) 693

                        Table 9.11

                        ANOVA Table for Testing H0 : 1 = 2 = 3 = 0 for the Stack Loss
                        Data

                               ANOVA Table (for testing H0 : 1 = 2 = 3 = 0)

                        Source  SS  d f MS                            F

                        Regression (on x1, x2, x12) 799.80 3 266.60 210.81

                        Error   16.44 13                        1.26

                        Total   816.24 16

                             Note also that the value of the coefficient of determination here can be
                        calculated using sums of squares given in Table 9.11 as

                                                     R2 = SSR = 799.80 = .980
                                                             SSTot 816.24

                        This is the value for R2 advertised long ago in Example 5 in Chapter 4. Also,
                        the error mean square, MSE = 1.26, is (as expected) exactly the value of sSF 2
                        calculated earlier in this example.

                             It is a matter of simple algebra to verify that R2 and the F statistic (9.62) are
                        equivalent in the sense that

  An expression for             F = (1 - R2 R2/k )/(n - k - 1)                         (9.63)

the F statistic (9.62)
        in terms of R2

                        so the F test of hypothesis (9.60) can be thought of in terms of attaching a p-value
                        to the statistic R2. This is a valuable development, but it should be remembered
                        that it is R2 (rather than F) that has the direct interpretation as a measure of what

                        fraction of raw variability the fitted equation accounts for. F and its associated
                        p-value take account of the sample size n in a way that R2 doesn't. They really

                        measure statistical detectability rather than variation accounted for. This means that

                        an equation that accounts for a fraction of observed variation that is relatively small

                        by most standards can produce a very impressive (small) p-value. If this point is not
                        clear, try using formula (9.63) to find the p-value for a situation where n = 1,000,
                        k = 4, and R2 = .1.

                             From Section 4.2 on, R2 values have been used in this book for informal

                        comparisons of various potential summary equations for a single data set. It turns

                        out that it is sometimes possible to attach p-values to such comparisons through the

                        use of the corresponding regression sums of squares and another F test.
694 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

                              Suppose that there are two different regression models for describing a data
                         set--the first of the usual form (9.36) for k input variables x1, x2, . . . , xk,

                                                yi = 0 + 1x1i + 2x2i + · · · + k xki + i

                         and the second being a specialization of the first where some p of the coefficients
                          (say, l1, l2, . . . , lp ) are all 0 (i.e., a specialization not involving input variables
                         xl1, xl2 , . . . , xlp ). The first of these models will be called the full regression model
                         and the second a reduced regression model. When one informally compares R2
                         values for two such models, the comparison is essentially between SSR values, since
                         the two R2 values share the same denominator, SSTot. The two SSR values can be
                         used to produce an observed level of significance for the comparison.

                              Under model the full model (9.36), the hypothesis

                                                        H0 : l1 = l2 = · · · = lp = 0               (9.64)
                         (that the reduced model holds) can be tested against

                                                             Ha : not H0                            (9.65)

                         using the test statistic

F statistic for testing                                   F = (SSRf - SSRr)/ p
                                                                SSEf/(n - k - 1)
H0 : l = · · · = l = 0                                                                              (9.66)
1           p

in multiple regression

                         and an Fp,n-k-1 reference distribution, where large observed values of the test
                         statistic constitute evidence against H0 in favor of Ha. In expression (9.66), the
                         "f" and "r" subscripts refer to the full and reduced regressions. The calculation of

                         statistic (9.66) can be facilitated by expanding the basic ANOVA table for the full

                         model (Table 9.10). Table 9.12 shows one form this can take.

Table 9.12

Expanded ANOVA Table for Testing H0 : l = l = · · · = l = 0 in Multiple Regression
                                    1              2      p

                         ANOVA Table (for testing H0 : l1 = l2 = · · · = lp = 0)

Source                          SS                    df     MS                        F

Regression (full)               SSRf                  k      (SSRf - SSRr)/ p          MSRf|r/MSEf
   Regression (reduced)         SSRr                  k-p    SSEf/(n - k - 1)
   Regression (full | reduced)  SSRf - SSRr           p
                                SSEf                  n-k-1
Error
                                SSTot                 n-1
Total
9.2 Inference Methods for General Least Squares Curve- and Surface-Fitting (Multiple Linear Regression) 695

Example 2     In the nitrogen plant example, consider the comparison of the two possible
(continued )  descriptions of stack loss

                                        y  0 + 1x1                                 (9.67)

              (stack loss is approximately a linear function of airflow only) and

                      y  0 + 1x1 + 2x2 + 3x12                                      (9.68)

              (the description of stack loss that has been used throughout this section). Although

              a printout won't be included here to show it, it is a simple matter to verify that the

              fitting of expression (9.67) to the nitrogen plant data produces SSR = 775.48 and
              therefore R2 = .950. Fitting expression (9.68), on the other hand, gives SSR =
              799.80 and R2 = .980. Since expression (9.67) is the specialization/reduction of
              expression (9.68) obtained by dropping the last p = 2 terms, the comparison of
              these two SSR (or R2) values can be formalized with a p-value. A test of

                                   H0 : 2 = 3 = 0

              can be made in the (full) model (9.68). Table 9.13 organizes the calculation of
              the observed value of the statistic (9.66) for this problem. That is,

                      f = (799.80 - 775.48)/2 = 9.7
                                   16.44/13

                   When compared with tabled F2,13 percentage points, the observed value of
              9.7 is seen to produce a p-value between .01 and .001. There is strong evidence
              in the nitrogen plant data that an explanation of mean response in terms of
              expression (9.68) (pictured, for example, in Figure 4.15) is superior to one in
              terms of expression (9.67) (which could be pictured as a single linear mean
              response in x1 for all x2).

              Table 9.13

              ANOVA Table for Testing H0 : 2 = 3 = 0 in Model (9.68)
              for the Stack Loss Data

                     ANOVA Table (for testing H0 : 2 = 3 = 0)

              Source                    SS d f MS F

              Regression (x1, x2, x12)  799.80 3

              Regression (x1)           775.48 1

              Regression (x2, x12 | x1) 24.32 2 12.16 9.7
              Error (x1, x2, x12)
                                        16.44 13 1.26

              Total                     816.24 16
696 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

                        The F statistic (9.66) can be written in terms of R2 values as

Alternative form                          F = (Rf2 - Rr2)/ p
                                                (1 - Rf2 )/(n - k - 1)
of the F statistic                                                                      (9.69)

   for testing

H0 : l = · · · = l = 0
1  p

Interpreting full       so that the test of hypothesis (9.64) is indeed a way of attaching a p-value to the
and reduced R2's        comparison of two R2's. However, just as was remarked earlier concerning the test
                        of hypothesis (9.60), it is the R2's themselves that indicate how much additional
    and the F test
                        variation a full model accounts for over a reduced model. The observed F value
   p tests that single
    coefficients are 0  or associated p-value measures the extent to which that increase is distinguishable
 versus a test that p
coefficients are all 0  from background noise.

                             To conclude this section, something needs to be said about the relationship
                        between the tests of hypotheses (9.45) (with # = 0), mentioned earlier, and the tests
                        of hypothesis (9.64) based on the F statistic (9.66). When p = 1 (the full model
                        contains only one more term than the reduced model), observed levels of significance

                        based on statistic (9.66) are in fact equal to two-sided observed levels of significance
                        based on # = 0 versions of statistic (9.46). But for cases where p  2, the tests of
                        the hypotheses that individual 's are 0 (one at a time) are not an adequate substitute

                        for the tests of hypothesis (9.64). For example, in the full model

                                          y = 0 + 1x1 + 2x2 + 3x3 +                     (9.70)

                        testing

                                          H0 : 2 = 0                                    (9.71)

                        and then testing

                                          H0 : 3 = 0                                    (9.72)

                        need not be at all equivalent to making a single test of

                                          H0 : 2 = 3 = 0                                (9.73)

                        This fact may at first seem paradoxical. But should the variables x2 and x3 be
                        reasonably highly correlated in the data set, it is possible to get large p-values
                        for tests of both hypothesis (9.71) and (9.72) and yet a tiny p-value for a test of
                        hypothesis (9.73). The message carried by such an outcome is that (due to the fact
                        that the variables x2 and x3 appear in the data set to be more or less equivalent) in
                        the presence of x1 and x2, x3 is not needed to model y. And in the presence of x1
                        and x3, x2 is not needed to model y. But one or the other of the two variables x2 and
                        x3 is needed to help model y even in the presence of x1. So, the F test of hypothesis
                        (9.64) is more than just a fancy version of several tests of hypotheses H0 : l = 0. It
                        is an important addition to an engineer's curve- and surface-fitting tool kit.
9.3 Application of Multiple Regression in Response Surface Problems and Factorial Analyses 697

Section 2 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Return to the situation of Chapter Exercise 2 of                  in the context of the study and the quadratic
   Chapter 4 and the carburetion study of Griffith and               model?
   Tesdall. Consider an analysis of these data based
   on the model y = 0 + 1x + 2x2 + .                         2. Return to the situation of Exercise 2 of Section
    (a) Find sSF for these data. What does this intend          4.2, and the chemithermomechanical pulp study of
        to measure in the context of the engineering            Miller, Shankar, and Peterson. Consider an analysis
        problem?                                                of the data there based on the model y = 0 +
   (b) Plot both residuals versus x and the standard-           1x1 + 2x2 + .
        ized residuals versus x. How much difference             (a) Find sSF. What does this intend to measure in
        is there in the appearance of these two plots?               the context of the engineering problem?
    (c) Give 90% individual two-sided confidence in-            (b) Plot both residuals and standardized residuals
        tervals for each of 0, 1, and 2.                             versus x1, x2, and y^ . How much difference is
   (d) Give individual 90% two-sided confidence in-                  there in the appearance of these pairs of plots?
        tervals for the mean elapsed time with a carbu-          (c) Give 90% individual two-sided confidence in-
        retor jetting size of 70 and then with a jetting             tervals for all of 0, 1, and 2.
        size of 76.                                             (d) Give individual 90% two-sided confidence in-
    (e) Give simultaneous 90% two-sided confidence                   tervals for the mean specific surface area, first
        intervals for the two means indicated in                     when x1 = 9.0 and x2 = 60 and then when
        part (d).                                                     x1 = 10.0 and x2 = 70.
    (f) Give 90% lower prediction bounds for an ad-              (e) Give simultaneous 90% two-sided confidence
        ditional elapsed time with a carburetor jetting              intervals for the two means indicated in part
        size of 70 and also with a jetting size of 76.               (d).
   (g) Give approximate 95% lower tolerance bounds               (f) Give 90% lower prediction bounds for the next
        for 90% of additional elapsed times, first with              specific surface area, first when x1 = 9.0 and
        a carburetor jetting size of 70 and then with a               x2 = 60 and then when x1 = 10.0 and x2 = 70.
        jetting size of 76.                                     (g) Give approximate 95% lower tolerance bounds
   (h) Make an ANOVA table for testing H0 : 1 =                      for 90% of specific surface areas, first when
        2 = 0 in the model y = 0 + 1x + 2x2 +                         x1 = 9.0 and x2 = 60 and then when x1 = 10.0
          . What is the meaning of this hypothesis in the            and x2 = 70.
        context of the study and the quadratic model?           (h) Make an ANOVA table for testing H0 : 1 =
        What is the p-value?                                         2 = 0 in the model y = 0 + 1x1 + 2x2 +
    (i) Use a t statistic and test the null hypothesis H0 :            . What is the p-value?
        2 = 0. What is the meaning of this hypothesis

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

9.3 Application of Multiple Regression
       in Response Surface Problems
       and Factorial Analyses

                    The discussions in Sections 4.1, 4.2, 9.1, and 9.2 have, we hope, given you a growing
                    appreciation of the wide utility of regression methods in engineering. The purpose
                    of this final section is to further expand your range of experience with multiple
698 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

                            regression by illustrating its usefulness in two additional contexts. First there is
                            an illustration of how surface fitting is used in "response surface" (or response
                            optimization) problems. Then there is a look at how regression has its applications
                            even in factorial analyses.

               9.3.1        Surface-Fitting and Response Surface Studies

    Fitted linear and       Engineers are often called upon to address the following generic problem. A response
quadratic functions         or responses y are known to depend upon system variables x1, x2, . . . , xk. No
as empirical models         simple physical theory is available for describing the dependence. Nevertheless, the
                            variables x1, x2, . . . , xk need adjustment to get good system behavior (as measured
                            by the variables y). Multiple regression analysis and some specialized "response
                            surface" considerations often prove effective in such problems.

                                 For one thing, linear and quadratic functions of x1, x2, . . . , xk are often useful
                            empirical descriptions of a relationship between x1, x2, . . . , xk and y. The material in
                            Sections 4.2 and 9.2 directly addresses fitting and inference for a linear approximate
                            relationship like

                            y  0 + 1x1 + 2x2 + · · · + k xk                      (9.74)

                            Response surfaces specified by equation (9.74) are "planar" (see again Figure 9.6
                            in this regard). When such surfaces fail to capture the nature of dependence of
                            y on x1, x2, . . . , xk because of their "lack of curvature," quadratic approximate
                            relationships often prove effective. The general version of a quadratic equation for
                            y in k variables x has k linear terms, k quadratic terms, and cross product terms
                            for all pairs of x variables. For example, the general 3-variable quadratic response
                            surface is specified by

                            y  0 + 1x1 + 2x2 + 3x3 + 4x12 + 5x22 + 6x32 + 7x1x2  (9.75)
                                 + 8x1x3 + 9x2x3

Gathering adequate               One issue in using the k-variable version of quadratic function (9.75) is that of
                      data  collecting adequate data to support the enterprise. 2k factorial data are not sufficient.

                            This is easy to see by considering the k = 1 case. Having data for only two different
                            values of x1, say x1 = 0 and x1 = 1, would not be adequate to support the fitting of

                            y  0 + 1x1 + 2x12                                    (9.76)

                            There are, as an arbitrary example, many different versions of equation (9.76) with
                            y = 5 for x1 = 0 and y = 7 for x1 = 1, including

                                                                 y  5 + 2x1 + 0x12
                                                                 y  5 - 8x1 + 10x12
                                                                 y  5 + 10x1 - 8x12
9.3 Application of Multiple Regression in Response Surface Problems and Factorial Analyses 699

               y
           8

           7      y = 5 + 10x1 - 8x12

           6

           5      y = 5 + 2 x1  y = 5 - 8x1 + 10 x12

              0                        1  x1

           Figure 9.8 Plots of three different quadratic
           functions passing through the points
           (x1, y) = (0, 5) and (x1, y) = (1, 7)

           These three equations have plots with quite different shapes. The first is linear, the
           second is concave up with a minimum at x1 = .4, and the third is concave down
           with a maximum at x1 = .625. This is illustrated in Figure 9.8. The point is that
           data from at least three different x1 values are needed in order to fit a one-variable
           quadratic equation.

                What would happen if a regression program were used to fit equation (9.76)
           to a set of (x1, y) data having only two different x1 values in it? The program
           will typically refuse the user's request, perhaps fitting instead the simpler equation
           y  0 + 1x1.

                Exactly what is needed in the way of data in order to fit a k-variable quadratic
           equation is not easy to describe in elementary terms. 3k factorial data are sufficient
           but for large k are really much more than are absolutely necessary. Statisticians have
           invested substantial effort in identifying patterns of (x1, x2, . . . , xk) combinations
           that are both small (in terms of number of different combinations) and effective (in
           terms of facilitating precise estimation of the coefficients in a quadratic response
           function). See, for example, Section 7.2.2 of Statistical Quality Assurance Methods
           for Engineers by Vardeman and Jobe for a discussion of "central composite" plans
           often employed to gather data adequate to fit a quadratic. An early successful
           application of such a plan is described next.

Example 3  A Central Composite Study for Optimizing Bread Wrapper Seal Strength

           The article "Sealing Strength of Wax-Polyethylene Blends" by Brown, Turner,
           and Smith (Tappi, 1958) contains an interesting central composite data set. The
           effects of the three process variables Seal Temperature, Cooling Bar Temperature,
           and % Polyethylene Additive on the seal strength y of a bread wrapper stock were
           studied. With the coding of the process variables indicated in Table 9.14, the data
700 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

Example 3     Table 9.14
(continued )  Coding of Three Process Variables in a Seal Strength Study

              Factor                                Variable

              A Seal Temperature          x1 = t1 - 255                      where t1 is in F
              B Cooling Bar Temperature             30                       where t2 is in F
              C Polyethylene Content                                         where c is in %
                                          x2 = t2 - 55
                                                    9

                                          x3 = c - 1.1
                                                    .6

                      Table 9.15
                      Seal Strengths Produced under 15 Different Sets
                      of Process Conditions

                          x1       x2        x3     Seal Strength,
                                                       y (g/in.)
                           -1        -1        -1
                             1       -1        -1          6.6
                                               -1          6.9
                           -1          1       -1          7.9
                             1         1                   6.1
                                     -1          1         9.2
                           -1        -1          1         6.8
                             1         1         1       10.4
                                       1         1         7.3
                           -1          0         0       10.1
                             1         0         0         9.9
                             0         0         0       12.2
                             0         0         0         9.7
                             0         0         0         9.7
                             0         0         0         9.6
                             0         0         0         9.8
                             0         0         0         5.0
                                -1.682           0         6.9
                      -1.682      1.682          0         6.3
                        1.682          0  -1.682           4.0
                             0         0    1.682          8.6
                             0
                             0
                             0
9.3 Application of Multiple Regression in Response Surface Problems and Factorial Analyses 701

                    in Table 9.15 were obtained. Notice that there are fewer than 33 = 27 different
                    (x1, x2, x3) vectors in these data. (The central composite plan involves only 15
                    different combinations.)

                         If one fits a first-order (linear) model

                    y = 0 + 1x1 + 2x2 + 3x3 +                                        (9.77)

                    to the data points listed in Table 9.15, a coefficient of determination of only R2 =
                    .38 is obtained, along with sSF = 1.79. The pooled sample standard deviation
                    (coming from the six points with x1 = 0, x2 = 0, and x3 = 0) is quite a bit
                    smaller than sSF--namely, sP = 1.00. Between the small value of R2 and the
                    moderate difference between sSF and sP, there is already some indication that
                    model (9.77) may be a poor description of the data. A residual analysis like those
                    done in Section 4.2 would further confirm this.

                         On the other hand, fitting the expression (9.75) to the data in Table 9.15
                    produces the equation

                    y^ = 10.165 - 1.104x1 + .0872x2 + 1.020x3 - .7596x12 - 1.042x22

                    - 1.148x32 - .3500x1x2 - .5000x1x3 + .1500x2x3                   (9.78)

                    with a coefficient of determination of R2 = .86 and sSF = 1.09. At least on the
                    basis of the two measures R2 and sSF, this quadratic description of seal strength
                    seems much superior to a first-order description.

         Plots and       For small values of k, the interpretation of a fitted quadratic response function
  interpreting a    can be facilitated through the use of various plots. One possibility is to plot y^ versus
fitted quadratic    a particular system variable x, with values of any other system variables held fixed.
                    This was the method used in Figure 4.15 for the nitrogen plant data, in Figure 4.16
                    (see page 158) for the lift/drag ratio data of Burris, and in Figure 9.8 of this section
                    for the hypothetical one-variable quadratics. (It is also worth noting that in light of
                    the inference material presented in Section 9.2, one can enhance such plots of y^ by
                    adding error bars based on confidence limits for the means µy|x1,x2,...,xk .)

                         A second kind of plot that can help in understanding a fitted quadratic function is
                    the contour plot. A contour plot is essentially a topographic map. For a given pair of
                    system variables (say x1 and x2) one can, for fixed values of all other input variables,
                    sketch out the loci of points in the (x1, x2)-plane that produce several particular
                    values of y^ . Most statistical packages and engineering mathematics packages will
                    make contour plots.

Example 3           Figure 9.9 shows a series of five contour plots made using the fitted equation
(continued )        (9.78) for seal strength. These correspond to x3 = -2, -1, 0, 1, and 2. The figure
                    suggests that optimum predicted seal strength may be achievable for x3 between
                    0 and 1, with x1 between -2 and -1, and x2 between 0 and 1.
702 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

x3 = -2      x2                     x3 = -1     x2                             x3 = 0     x2
          2                                  2                                         2

          1                                         1                                       1
                                           y = 8.12                               y = 10.59

 -2 -1               1 2 x1         -2 -1 y = 8 1 y = 7 2 x1                 -2 -1             1 2 x1
y = 3.54       y=3 y=1                            -1 y = 6                        y = 10 y = 9 y = 8
          -1 y = 2                                -2                                   -1

          -2                                                                           -2

                 x3 = 1         x2                     x3 = 2              x2
                             2                                          2

                 y = 10.97 1                           y = 9.26         1

                                                                 y = 9

                 -2 -1 y = 10 y = 9 1      2 x1        -2 -1 y = 8             1       2 x1
                                 -1 y = 8
                                                               y = 7 -1
                                 -2
                                                                 -2

                          Figure 9.9 A series of contour plots for seal strength

Analytic interpretation        Plotting is helpful in understanding a fitted quadratic primarily for small k. So
   of a fitted quadratic
                          it is important that there are also analytical tools that can be employed. To illustrate
                          their character, consider the simple case of k = 1. The basic nature of the quadratic
                          equation

                                                       y^ = b0 + b1x1 + b2x12

                          is governed by b2. For b2 > 0 it describes a parabola opening up. For b2 < 0 it
                          describes a parabola opening down. And for b2 = 0 it describes a line. Provided
                          b2 = 0 the value

                                                               x1 = - b1
                                                                        2b2

                          produces the minimum (b2 > 0) or maximum (b2 < 0) value of y^ . Something like
                          this story is also true for k > 1.

                               It is necessary to use some matrix notation to say what happens for k > 1.
                          Temporarily modify the way the b's are subscripted as follows. The meaning of
                          b0 will remain unchanged. b1 through bk will be the coefficients for the k system
9.3 Application of Multiple Regression in Response Surface Problems and Factorial Analyses 703

                         variables x1 through xk. b11 through bkk will be the coefficients for the k squares x12
                         through xk2. And for each i = j , bi j will be the coefficient of the xi xj cross product.
                         One can define a k × 1 vector b and a k × k matrix B as

                                      b1 
                                      b2 
                               b  =     ..       
                                                 
                                       .
     Vector of linear
    coefficients and                   bk
matrix of quadratic
                                                       1b ··· 1b 
           coefficients                  b11           2 12         2 1k

                                                                          
                                      1b b ··· 1b 
                               B =  2 12 22                         2 2k 
                                             ..         ..           ..   
                                            .          .            .     

                                       12 b 1 1k 2 b2k · · · bkk

                         With

                                                        x1 
                                                        x2 
                                                 x  =        ..   
                                                                  
                                                               .

                                                            xk

                         Provided the matrix B is nonsingular, the corresponding k-variable quadratic then
                         has a stationary point (i.e., a point at which first partial derivatives with respect to
                         x1, x2, . . . , xk are all 0) where

       Location of a                             x = - 12 B-1b              (9.79)
   stationary point
    for a k-variable     And depending upon the nature of B, the stationary point will be either a minimum,
    fitted quadratic     a maximum, or a saddle point of the fitted response. (Moving away from a saddle
                         point in some directions produces an increase in y^ , while moving away in other
   Equation solved       directions produces a decrease.)
by the eigenvalues
                              It is the eigenvalues of B that are critical in determining the shape of the fitted
   of the matrix B       quadratic surface. The eigenvalues of B are the k solutions of the equation (in )

                                            det(B - I) = 0                  (9.80)

                         where I is the identity matrix. (Most statistical analysis packages and engineering
                         mathematics packages will compute eigenvalues quite painlessly.)

                              When all solutions to equation (9.80) are positive, a fitted quadratic is bowl-
                         shaped up and has a minimum at the point (9.79). When all solutions to equation
704 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

                                  (9.80) are negative, a fitted quadratic is bowl-shaped down and has a maximum at
                                  the point (9.79). When some solutions to equation (9.80) are positive and some are
                                  negative, the fitted quadratic surface has neither a maximum nor minimum (unless
                                  one restricts attention to some bounded region of x vectors).

              Printout 3 Analysis of the Fitted Quadratic for the Bread Wrapper Data
                               (Example 3)

              MTB >  Read 3 3 M1.
              DATA>  -.7596 -.175 -.250
              DATA>  -.175 -1.042 .075
              DATA>  -.250 .075 -1.148
                     3 rows read.
              MTB >  Read 3 1 M2.
              DATA>  -1.104
              DATA>  .0872
              DATA>  1.020
                     3 rows read.
              MTB >  Eigen M1 C1.
              MTB >  Print C1.

              Data Display

              C1
                 -1.27090 -1.11680 -0.56190

              MTB > Invert M1 M3.
              MTB > Multiply M3 M2 M4.
              MTB > Multiply M4 -.5 M5.
              MTB > Print M5.

              Data Display

               Matrix M5

              -1.01104
                 0.26069
                 0.68146

Example 3     Printout 3 illustrates the use of MINITAB in the analytic investigation of the
(continued )  nature of the fitted surface (9.78) in the bread wrapper seal strength study. The
              printout shows the three eigenvalues of B to be negative. The fitted seal strength
              therefore has a maximum. This maximum is predicted to occur at the combination
              of values x1 = -1.01, x2 = .26, and x3 = .68. (The MINITAB matrix functions
              used to make the printout are under the "Calc/Matrices" menu, and the display
              routine is under the "Manip/Display Data" menu.)
9.3 Application of Multiple Regression in Response Surface Problems and Factorial Analyses 705

                          The discussion of response surface studies in this subsection isn't intended to
                     be complete. Whole books, like, for example, Box and Draper's Empirical Model-
                     Building and Response Surfaces, have been written on the subject. (Section 9.3 of
                     Vardeman's Statistics for Engineering Problem Solving contains a more complete
                     discussion than the present one, is still short of a book-length treatment.) We hope,
                     however, this brief look at the topic suffices to indicate its importance to engineering
                     practice.

9.3.2  Regression and Factorial Analyses

       Many of the factorial inference methods discussed in this book are applicable only
       in balanced-data situations. For example, remember that the use of the reverse Yates
       algorithm to fit few-effects 2p factorial models and the methods of interval-oriented
       inference for 2p studies under few-effects models discussed in Section 8.2 are limited
       to balanced-data applications.

            But by accident if not by design, an engineer will eventually face the analysis
       of unbalanced factorial data. Happily enough, this can be accomplished through use
       of the multiple regression formulas provided in Section 9.2. This subsection shows
       how factorial analyses can be thought of in multiple regression terms. It begins with
       a discussion of two-way factorial cases and then considers three-way (and higher)
       situations.

            The basic multiple regression model equation used in Section 9.2,

       yi = 0 + 1x1i + 2x2i + · · · + k xki + i  (9.81)

       looks deceptively simple. With proper choice of the inputs x, versions of it can

       be used in a wide variety of contexts, including factorial analyses. For purposes
       of illustration, consider the case of a complete two-way factorial study with I = 3
       levels of factor A and J = 3 levels of factor B. In the usual two-way factorial

       notation introduced in Definitions 1 and 2 of Chapter 8, the basic constraints on the
       main effects and two-factor interactions are i i = 0, j j = 0, and i i j =

          j i j = 0. These imply that the I · J = 3 · 3 = 9 different mean responses in
       such a study,

       µi j = µ.. + i + j + i j                  (9.82)

       can be written as displayed in Table 9.16.

            At first glance, the advantage of writing out these mean responses in terms of
       only effects corresponding to the first 2 (= I - 1) levels of A and first 2 (= J - 1)
       levels of B is not obvious. But doing so expresses the 9 (= I · J ) different means in
       terms of only as many different parameters as there are means, and helps one find a

       regression-type analog of expression (9.82).
            Notice first that µ.. appears in each mean response listed and therefore plays

       a role much like that of the intercept term 0 in a regression model. Further, the
       two A main effects, 1 and 2, appear with positive signs when (respectively) i = 1
706 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

Table 9.16
Mean Responses in a 32 Factorial Study

     i,          j,        Mean Response
Level of A  Level of B

1           1              µ.. + 1 + 1 + 11

1           2              µ.. + 1 + 2 + 12

1           3              µ.. + 1 - 1 - 2 - 11 - 12

2           1              µ.. + 2 + 1 + 21

2           2              µ.. + 2 + 2 + 22

2           3              µ.. + 2 - 1 - 2 - 21 - 22

3           1              µ.. - 1 - 2 + 1 - 11 - 21

3           2              µ.. - 1 - 2 + 2 - 12 - 22

3           3              µ.. - 1 - 2 - 1 - 2 + 11 + 12 + 21 + 22

or 2 but with negative signs when i = 3 (= I ). In a similar manner, the first two
B main effects, 1 and 2, appear with positive signs when (respectively) j = 1
or 2 but with negative signs when j = 3 (= J ). If one thinks of the four A and B

main effects used in Table 9.16 in terms of coefficients  in a regression model,

it soon becomes clear how to invent "system variables" x to make the regression

coefficients  appear with correct signs in the expressions for means µi j . That is,
define four dummy variables

                           if the response y is from level 1 of A
                   1       if the response y is from level 3 of A
            x1A = -1       otherwise

                        0  if the response y is from level 2 of A
                           if the response y is from level 3 of A
                   1       otherwise
            x2A = -1
                           if the response y is from level 1 of B
                        0  if the response y is from level 3 of B
                           otherwise
                   1
            x1B = -1       if the response y is from level 2 of B
                           if the response y is from level 3 of B
                        0  otherwise
                   
                   1
            x2B = -1

                        0

Then, making the correspondences indicated in Table 9.17, µ.. + i + j can be
written in regression notation as

               0 + 1x1A + 2x2A + 3x1B + 4x2B
9.3 Application of Multiple Regression in Response Surface Problems and Factorial Analyses 707

Table 9.17

Correspondences between Regression Coefficients and the Grand
Mean and Main Effects in a 32 Factorial Study

Regression Coefficient Corresponding 3 × 3 Factorial Effect

0  µ..

1  1

2  2

3  1

4  2

     What is more, since the x's used here take only the values -1, 0, and 1, so
also do their products. And taken in pairs (one xA variable with one xB variable),
their products produce the correct (-1, 0, or 1) multipliers for the 2-factor inter-

actions 11, 12, 21, and 22 appearing in Table 9.16. That is, if one thinks
of the interactions i j in terms of regression coefficients , with the additional
correspondences listed in Table 9.18, the entire expression (9.82) can be written in

regression notation as

µy|xA,xA,xB,xB = 0 + 1x1A + 2x2A + 3x1B + 4x2B + 5x1Ax1B         (9.83)

        1212

                    + 6x1Ax2B + 7x2Ax1B + 8x2Ax2B

     By rewriting the factorial-type expression (9.82) as a regression-type expression
(9.83) it is then obvious how to fit few-effects models and do inference under those
models even for unbalanced data. Nowhere in Section 9.2 was there any requirement
that the data set be balanced. So the methods there can be used (employing properly
constructed x variables and properly interpreting a corresponding regression print-
out) to fit reduced versions of model (9.83) and make confidence, prediction, and
tolerance intervals under those reduced models.

Table 9.18

Correspondence between Regression Coefficients and Interactions
in a 32 Factorial Study

Regression Coefficient Corresponding 3 × 3 Factorial Effect

5  11

6  12

7  21

8  22
708 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

                                   The general I × J two-way factorial version of this story is similar. One defines

                             I  -  1  factor  A  dummy  variables  x1A,  x2A,  .  .  .  ,    A     according  to

                                                                                           x I -1

I - 1 dummy                                                  if the response y is from level i of A                    (9.84)
variables for                                           1    if the response y is from level I of A
                                                 xiA = -1    otherwise
      factor A
                                                          0

                             and J - 1 factor B dummy variables x1B, x2B, . . . , xJB-1 according to

J - 1 dummy                                                  if the response y is from level j of B                    (9.85)
variables for                                           1    if the response y is from level J of B
                                                 xjB = -1    otherwise
      factor B
                                                          0

Multiple regression          and uses a regression program to do the computations. Estimated regression coeffi-
        and two-way          cients of xiA or xjB variables alone are estimated main effects, while those for xiAxjB
                             cross products are estimated 2-factor interactions.
   factorial analyses

                Example 4       A Factorial Analysis of Unbalanced Wood Joint Strength
(Examples 7, Chapter 4,         Data Using a Regression Program

       and 1, Chapter 8,        Consider again the wood joint strength study of Kotlers, MacFarland, and Tom-
    revisited--see pages        linson. The discussion in Section 8.1 showed that if only the wood types pine
                                and oak are considered, a no-interaction description of joint strength for butt,
                 163, 547 )     beveled, and lap joints might be appropriate. The corresponding part of the (orig-
                                inally 3 × 3 factorial) data of Kotlers, MacFarland, and Tomlinson is given here
                                in Table 9.19.

                                      Table 9.19
                                      Strengths of 11 Wood Joints

                                                                                                   B Wood Type

                                                                                           1 (Pine)           2 (Oak)

                                              A Joint Type   1 (Butt)                   829, 596     1169
                                                             2 (Beveled)                1348, 1207   1518, 1927
                                                             3 (Lap)                    1000, 859    1295, 1561
9.3 Application of Multiple Regression in Response Surface Problems and Factorial Analyses 709

Table 9.20
Joint Strength Data Prepared for a Factorial Analysis Using
a Regression Program

i,      j,

Joint Type Wood Type x1A x2A x1B               y

1       1                      1 0 1 829, 596

1       2                      1 0 -1 1169

2       1                      0 1 1 1348, 1207

2       2                      0 1 -1 1518, 1927

3       1                      -1 -1 1 1000, 859

3       2                      -1 -1 -1 1295, 1561

     Notice that because these data are unbalanced (due to the unfortunate loss
of one butt/oak response), it is not possible to fit a no-interaction model to these
data by simply adding together fitted effects (defined in Section 4.3) or to use
anything said in Chapter 8 to make inferences based on such a model. But it is
possible to use the dummy variable regression approach based on formulas (9.84)
and (9.85) to do so.

     Consider the regression-data-set version of Table 9.19 given in Table 9.20.
Printouts 4 and 5 show the results of fitting the two regression models

y = 0 + 1x1A + 2x2A + 3x1B + 4x1Ax1B + 5x2Ax1B +                            (9.86)
y = 0 + 1x1A + 2x2A + 3x1B +                                                (9.87)

to the data of Table 9.20. Printout 4 corresponding to model (9.86) is the full model
or µi j = µ.. + i + j + i j description of the data. For that regression run, the
reader should verify the correspondences between fitted regression coefficients

b and fitted effects (defined in Section 4.3), listed in Table 9.21. (For example,

Table 9.21
Correspondence between Fitted Regression Coefficients and Fitted Factorial
Effects for the Wood Joint Strength Data

Fitted Regression Coefficient  Value Corresponding Fitted Effect

    b0                         1206.5   y¯ ..

    b1                         -265.75  a1

    b2                         293.50   a2

    b3                         -233.33  b1

    b4                         5.08     ab11

    b5                         10.83    ab21
710 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

Example 4     y¯ .. = 1206.5 and y¯ 1. = 940.75, so a1 = 940.75 - 1206.5 = -265.75, which is
(continued )  the value of the fitted regression coefficient b1.)

                   Model (9.86), like the two-way model (8.4) of Section 8.1, represents no
              restriction or simplification of the basic one-way model. So least squares estimates

              of parameters that are linear combinations of underlying means are simply the
              same linear combinations of sample means. Further, the fitted y values are (as
              expected) simply the sample means y¯ i j .

                   Printout 5 corresponding to model (9.87) is the µi j = µ.. + i + j descrip-
              tion of the data. The fitted regression coefficients b for model (9.87) are not

              equal to the (full-model) fitted factorial effects defined in Section 4.3. (The b's
              are least squares estimates of the underlying effects for the no-interaction model.

              When factorial data are unbalanced, these are not necessarily equal to the quan-
              tities defined in Section 4.3. For example, b1 from Printout 5 is -264.48, which
              is the least squares estimate of 1 in a no-interaction model but differs from
              a1 = -264.75.) In a similar vein, the fitted responses are neither sample means
              nor sums of y¯ .. plus the full-model fitted main effects defined in Section 4.3. (Of
              course, since the x variables take only values -1, 0, and 1, the fitted responses are
              sums and differences of the least squares estimates of the underlying parameters

              µ.., 1, 2, 1 in the no-interaction model.)
                   Inference under model (9.86) is simply inference under the usual one-way

              normal model, and all of Sections 7.1 through 7.4 and 8.1 can be used. It is then
              reassuring that on Printout 4, sSF = sP = 182.2 and that (for example) for butt
              joints and pine wood (levels 1 of both A and B), the estimated standard deviation
              for y^ = y¯ 11 is

                                                 sP 182.2
                        128.9 = sSF · A =  = 
                        n11                                                  2

                   To illustrate how inference under a no-interaction model would proceed for
              the unbalanced 3 × 2 factorial joint strength data, consider making a 95% two-
              sided confidence interval for the mean strength of butt/pine joints and then a

              90% lower prediction bound for the strength of a single joint of the same kind.
              Note that for data point 1 (a butt/pine observation) on Printout 5, y^ = 708.7 and
              sSF · A = 94.8, where sSF = 154.7 has seven associated degrees of freedom. So
              from formula (9.53) of Section 9.2 (page 686), two-sided 95% confidence limits

              for mean butt/pine joint strength are

                        708.7 ± 2.365(94.8)

              that is,

                        484.5 psi and 932.9 psi
9.3 Application of Multiple Regression in Response Surface Problems and Factorial Analyses 711

                        Similarly, using formula (9.56) on page 689, a 90% lower prediction limit for a
                        single additional butt/pine joint strength is

                                            708.7 - 1.415 (154.7)2 + (94.8)2 = 452.0 psi

                              From these two calculations, it should be clear that other methods from
                        Section 9.2 could be used here as well. The reader should have no trouble finding
                        and using residuals and standardized residuals for the no-interaction model based
                        on formulas (9.39) and (9.42), giving simultaneous confidence intervals for all
                        six mean responses under the no-interaction model using formula (9.54) or giving
                        one-sided tolerance bounds for certain joint/wood combinations under the no-
                        interaction model using formula (9.58) or (9.59).

WWW  Printout 4 Multiple Regression Version of the With-Interactions Factorial Analysis
                      of Joint Strength (Example 4)

     Data Display

     Row xa1 xa2 xb1             y

     1  1       0         1   829
                              596
     2  1       0         1  1169
                             1348
     3  1       0 -1         1207
                             1518
     4  0       1         1  1927
                             1000
     5  0       1         1   859
                             1295
     6  0       1 -1         1561

     7  0       1 -1

     8 -1 -1              1

     9 -1 -1              1

     10 -1 -1 -1

     11 -1 -1 -1

     Regression Analysis

     The regression equation is
     y = 1207 - 266 xa1 + 294 xa2 - 233 xb1 + 5.1 xa1*xb1 + 10.8 xa2*xb1

     Predictor      Coef     StDev        T        P
     Constant   1206.50      56.82  21.23    0.000
     xa1        -265.75      85.91  -3.09    0.027
     xa2                     77.43           0.013
     xb1         293.50      56.82   3.79    0.009
     xa1*xb1    -233.33      85.91  -4.11    0.955
     xa2*xb1                 77.43           0.894
                    5.08             0.06
                   10.83             0.14

     S = 182.2     R-Sq = 88.5%     R-Sq(adj) = 77.1%
712 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

     Analysis of Variance

     Source           DF          SS              MS      F                        P
                           1283527          256705    7.73                   0.021
     Regression       5
                            166044           33209
     Residual Error   5    1449571

     Total            10

     Source      DF        Seq SS

     xa1         1         120144

     xa2         1         577927

     xb1         1         583908

     xa1*xb1     1         897

     xa2*xb1     1         650

     Obs         xa1       y              Fit StDev Fit Residual St Resid

     1          1.00      829.0     712.5    128.9                           116.5    0.90

     2          1.00      596.0     712.5    128.9                           -116.5   -0.90

     3          1.00  1169.0        1169.0   182.2                           -0.0     *X

     4          0.00  1348.0        1277.5   128.9                           70.5     0.55

     5          0.00  1207.0        1277.5   128.9                           -70.5    -0.55

     6          0.00  1518.0        1722.5   128.9                           -204.5   -1.59

     7          0.00  1927.0        1722.5   128.9                           204.5    1.59

     8        -1.00   1000.0        929.5    128.9                           70.5     0.55

     9        -1.00   859.0         929.5    128.9                           -70.5    -0.55

     10       -1.00   1295.0        1428.0   128.9                           -133.0   -1.03

     11       -1.00   1561.0        1428.0   128.9                           133.0    1.03

     X denotes an observation whose X value gives it large influence.

WWW  Printout 5 Multiple Regression Version of the No-Interactions Factorial Analysis
                      of Joint Strength (Example 4)

     Regression Analysis

     The regression equation is
     y = 1207 - 264 xa1 + 293 xa2 - 234 xb1

     Predictor       Coef          StDev           T        P
     Constant    1207.14           47.38     25.48    0.000
     xa1         -264.48           70.62     -3.74    0.007
     xa2                           65.11              0.003
     xb1          292.86           47.38      4.50    0.002
                 -233.97                     -4.94

     S = 154.7        R-Sq = 88.4%    R-Sq(adj) = 83.5%

     Analysis of Variance

     Source           DF          SS              MS        F                      P
                           1281980          427327    17.85                  0.001
     Regression       3
                            167591           23942
     Residual Error   7    1449571

     Total            10

     Source      DF        Seq SS
     xa1
     xa2         1         120144
     xb1
                 1         577927

                 1         583908
9.3 Application of Multiple Regression in Response Surface Problems and Factorial Analyses 713

                         Obs          xa1       y                   Fit StDev Fit Residual St Resid

                         1            1.00      829.0            708.7                     94.8            120.3                         0.98

                         2            1.00      596.0            708.7                     94.8         -112.7                     -0.92

                         3            1.00      1169.0        1176.6                       109.4           -7.6                    -0.07

                         4            0.00      1348.0        1266.0                       90.7            82.0                          0.65

                         5            0.00      1207.0        1266.0                       90.7            -59.0                   -0.47

                         6            0.00      1518.0        1734.0                       90.7         -216.0                     -1.72

                         7            0.00      1927.0        1734.0                       90.7            193.0                         1.54

                         8           -1.00      1000.0           944.8                     90.7            55.2                          0.44

                         9           -1.00      859.0            944.8                     90.7            -85.8                   -0.68

                         10          -1.00      1295.0        1412.7                       90.7         -117.7                     -0.94

                         11          -1.00      1561.0        1412.7                       90.7            148.3                         1.18

                              The pattern of analysis set out for two-way factorials carries over quite nat-

                         urally to three-way and higher factorials. To use a multiple regression program

                         to fit and make inferences based on simplified versions of the p-way factorial

 Dummy variables         model,    proceed  as  follows.  I  -1        dummy       variables      x  A  ,  x  A  ,  .  .  .  ,  x  A     are   defined
      for regression                                                                                 1        2                    I -1
                         (as before) to carry information about I levels of factor A, J - 1 dummy variables
  analysis of p-way
             factorials  x1B, x2B, . . . , xJB-1 are defined (as before) to carry information about J levels of factor
                                                        x1C,     C                 xKC -1
Alternative choice       B,  K  -  1  dummy  variables        x  2  ,  .  .  .  ,          are  defined  to   carry          information       about
  of x variables for
                         K levels of factor C, . . . , etc. Products of pairs of these, one each from the groups
regression analysis
     of 2p factorials    representing two different factors, carry information about 2-factor interactions of

                         the factors. Products of triples of these, one each from the groups representing

                         three different factors, carry information about 3-factor interactions of the factors.

                         And so on.

                              When something short of the largest possible regression model is fitted to

                         an unbalanced factorial data set, the estimated coefficients b that result are the

                         least squares estimates of the underlying factorial effects in the few-effects model.

                         (Usually, these differ somewhat from the (full-model) fitted effects defined in Section

                         4.3.) All of the regression machinery of Section 9.2 can be applied to create fitted

                         values, residuals, and standardized residuals; to plot these to do model checking; to

                         make confidence intervals for mean responses; and to create prediction and tolerance

                         intervals.

                              When the regression with dummy variables approach is used as just described,

                         the fitted coefficients b correspond to fitted effects for the levels 1 through I - 1,

                         J - 1, K - 1, etc. of the factors. For two-level factorials, this means that the fitted

                         coefficients are estimated factorial effects for the "all low" treatment combination.

                         However, because of extensive use of the Yates algorithm in this text, you will
                         probably think first in terms of the 2p factorial effects for the "all high" treatment

                         combination.

                              Two sensible courses of action then suggest themselves for the analysis of
                         unbalanced 2p factorial data. You can proceed exactly as just indicated, using
                         dummy variables x1A, x1B, x1C, etc. and various products of the same, taking care
                         to remember to interpret b's as "all low" fitted effects and subsequently to switch

                         signs as appropriate to get "all high" fitted effects. The other possibility is to depart
                         slightly from the program laid out for general p-way factorials in 2p cases: Instead
714 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

                                  of using the variables x1A, x1B, x1C, etc. and their products when doing regression, one
                                  may use the variables

                                             x2A = -x1A = 1 if the response y is from the high level of A -1 if the response y is from the low level of A
                                             x2B = -x1B = 1 if the response y is from the high level of B -1 if the response y is from the low level of B
                                             x2C = -x1C = 1 if the response y is from the high level of C -1 if the response y is from the low level of C

                                  etc. and their products when doing regression. When the variables x2A, x2B, x2C, etc.
                                  are used, the fitted b's are the estimated "all high" 2p factorial effects.

              Example 5  A Factorial Analysis of Unbalanced 23 Power
(Example 4, Chapter 8,   Requirement Data Using Regression

  revisited--page 569)   Return to the situation of the 23 metalworking power requirement study of Miller.
                         The original data set (given in Table 8.8) is balanced, with the common sample
                         size being m = 4. For the sake of illustrating how regression with dummy vari-
                         ables can be used in the analysis of unbalanced higher-way factorial data, consider
                         artificially unbalancing Miller's data by supposing that the first data point ap-
                         pearing in Table 8.8 has gotten lost. The portion of Miller's data that will be used
                         here is then given in Table 9.22.

                         Table 9.22
                         Dynamometer Readings for 23 Treatment Combinations

                         Tool Type  Bevel Angle  Type of Cut  y, Dynamometer Reading (mm)

                              1     15           continuous   26.5, 30.5, 27.0
                              2     15           continuous   28.0, 28.5, 28.0, 25.0
                              1     30           continuous   28.5, 28.5, 30.0, 32.5
                              2     30           continuous   29.5, 32.0, 29.0, 28.0
                              1     15           interrupted  28.0, 25.0, 26.5, 26.5
                              2     15           interrupted  24.5, 25.0, 28.0, 26.0
                              1     30           interrupted  27.0, 29.0, 27.5, 27.5
                              2     30           interrupted  27.5, 28.0, 27.0, 26.0
9.3 Application of Multiple Regression in Response Surface Problems and Factorial Analyses 715

     For this slightly altered data set, the Yates algorithm produces the fitted
effects

a2 = -.2656      ab22 = .0469   abc222 = -.0469
b2 = .8281       ac22 = -.0469
c2 = -.9531      bc22 = -.2031

and sP = 1.51 with  = 23 associated degrees of freedom. Formula (8.12)
(page 575) of Section 8.2 then shows that (say) two-sided 90% confidence
intervals for effects have plus-and-minus parts

                                             17 1
                           ±1.714(1.51) 3 + = ±.47

                                             243

Just as in Example 4 in Chapter 8, where all n = 32 data points were used,
one might thus judge only the B and C main effects to be clearly larger than
background noise.

     Printout 6 supports exactly these conclusions. This regression run was made
using all seven of the terms

                 x A2 , x B2 , x C2 , x A2 x B2 , x A2 x C2 , x B2 x C2 , and x2Ax2Bx2C

(i.e., using the full model in regression terminology and the unrestricted 23
factorial model in the terminology of Section 8.2). On Printout 6, one can identify
the fitted regression coefficients b with the fitted factorial effects in the pairs
indicated in Table 9.23.

Table 9.23
Correspondence Between Fitted Regression Coefficients
and Fitted Factorial Effects for the Regression Run
of Printout 6

Fitted Regression Coefficient Fitted Factorial Effect

             b0                 y¯ ···

             b1                 a2

             b2                 b2

             b3                 c2

             b4                 ab22

             b5                 ac22

             b6                 bc22

             b7                 abc222
716 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

Example 5          Analysis of the data of Table 9.22 based on a full factorial model
(continued )
                      yi jkl = µ... + i + j + k + i j + ik + jk + i jk + i jkl

              that is,

                      yi = 0 + 1x2iA + 2x2iB + 3x2iC + 4x2iA x2iB + 5x2iA x2iC + 6x2iB x2iC
                            + 7x2iA x2iB x2iC + i

              is a logical first step. Based on that step, it seems desirable to fit and draw
              inferences based on a "B and C main effects only" description of y. Since the
              data in Table 9.22 are unbalanced, the naive use of the reverse Yates algorithm
              with the (full-model) fitted effects will not produce appropriate fitted values. y¯ ...,
              b2, and c2 are simply not the least squares estimates of µ..., 2, and 2 for the "B
              and C main effects only" model in this unbalanced data situation.

                   However, what can be done is to fit the reduced regression model

                                            yi = 0 + 2x2iB + 3x2iC + i

              to the data. Printout 7 represents the use of this technique. Locate on that printout
              the (reduced-model) estimates of the factorial effects µ..., 2, and 2 and note
              that they differ somewhat from y¯ ..., b2, and c2 as defined in Section 4.3 and
              displayed on Printout 6. Note also that the four different possible fitted mean
              responses, along with their estimated standard deviations, are as given in Table
              9.24.

                   The values in Table 9.24 can be used in the formulas of Section 9.2 to produce
              confidence intervals for the four mean responses, prediction intervals, tolerance
              intervals, and so on based on the "B and C main effects only" model. All of this
              can be done despite the fact that the data of Table 9.22 are unbalanced.

              Table 9.24
              Fitted Values and Their Estimated Standard Deviations for a "B
              and C Main Effects Only" Analysis of the Unbalanced Power
              Requirement Data

              Bevel Angle x2B Type of Cut x2C                                y^  sSF · A

              15  -1 continuous -1 27.88 .46

              30  1 continuous -1 29.54 .44

              15  -1 interrupted  1 25.98 .44

              30  1 interrupted   1 27.64 .44
9.3 Application of Multiple Regression in Response Surface Problems and Factorial Analyses 717

Printout 6 Multiple Regression Version of the With-Interactions Factorial Analysis
                 of Power Requirement (Example 5)

Regression Analysis

The regression equation is
y = 27.8 - 0.266 xa2 + 0.828 xb2 - 0.953 xc2 + 0.047 xa*xb - 0.047 xa*xc

                - 0.203 xb*xc - 0.047 xa*xb*xc

Predictor       Coef      StDev               T        P
Constant    27.7656      0.2731        101.68    0.000
xa2         -0.2656      0.2731                  0.341
xb2                      0.2731         -0.97    0.006
xc2          0.8281      0.2731           3.03   0.002
xa*xb       -0.9531      0.2731                  0.865
xa*xc                    0.2731         -3.49    0.865
xb*xc        0.0469      0.2731           0.17   0.465
xa*xb*xc    -0.0469      0.2731                  0.865
            -0.2031                     -0.17
            -0.0469                     -0.74
                                        -0.17

S = 1.514        R-Sq = 51.0%       R-Sq(adj) = 36.0%

Analysis of Variance

Source           DF             SS         MS        F          P
                          54.748       7.821     3.41     0.012
Regression       7        52.687       2.291
                         107.435
Residual Error 23

Total            30

Source      DF        Seq SS
                       2.202
xa2         1
                      22.645
xb2         1         28.398

xc2         1          0.091
                       0.051
xa*xb       1          1.293
                       0.068
xa*xc       1

xb*xc       1

xa*xb*xc    1

Obs         xa2       y             Fit StDev Fit Residual St Resid

1         -1.00  26.500        28.000  0.874              -1.500   -1.21

2         -1.00  30.500        28.000  0.874              2.500    2.02R

3         -1.00  27.000        28.000  0.874              -1.000   -0.81

4          1.00  28.000        27.375  0.757              0.625    0.48

5          1.00  28.500        27.375  0.757              1.125    0.86

6          1.00  28.000        27.375  0.757              0.625    0.48

7          1.00  25.000        27.375  0.757              -2.375   -1.81

8         -1.00  28.500        29.875  0.757              -1.375   -1.05

9         -1.00  28.500        29.875  0.757              -1.375   -1.05

10        -1.00  30.000        29.875  0.757              0.125    0.10

11        -1.00  32.500        29.875  0.757              2.625    2.00R

12         1.00  29.500        29.625  0.757              -0.125   -0.10

13         1.00  32.000        29.625  0.757              2.375    1.81

14         1.00  29.000        29.625  0.757              -0.625   -0.48

15         1.00  28.000        29.625  0.757              -1.625   -1.24

16        -1.00  28.000        26.500  0.757              1.500    1.14

17        -1.00  25.000        26.500  0.757              -1.500   -1.14

18        -1.00  26.500        26.500  0.757              -0.000   -0.00

19        -1.00  26.500        26.500  0.757              -0.000   -0.00

20         1.00  24.500        25.875  0.757              -1.375   -1.05

21         1.00  25.000        25.875  0.757              -0.875   -0.67

22         1.00  28.000        25.875  0.757              2.125    1.62

23         1.00  26.000        25.875  0.757              0.125    0.10
718 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

24      -1.00    27.000        27.750  0.757                                 -0.750   -0.57
                                       0.757                                  1.250    0.95
25      -1.00    29.000        27.750  0.757
                                       0.757                                 -0.250   -0.19
26      -1.00    27.500        27.750  0.757                                 -0.250   -0.19
                                       0.757
27      -1.00    27.500        27.750  0.757                                  0.375    0.29
                                       0.757                                  0.875    0.67
28         1.00  27.500        27.125                                        -0.125   -0.10
                                                                             -1.125   -0.86
29         1.00  28.000        27.125

30         1.00  27.000        27.125

31         1.00  26.000        27.125

R denotes an observation with a large standardized residual

Printout 7 Multiple Regression Version of a "B and C Main Effects Only" Analysis
                 of Power Requirement (Example 5)

Regression Analysis

The regression equation is
y = 27.8 + 0.832 xb2 - 0.949 xc2

Predictor       Coef      StDev               T        P
Constant    27.7619      0.2553        108.73    0.000
xb2                      0.2553                  0.003
xc2          0.8319      0.2553           3.26   0.001
            -0.9494                     -3.72

S = 1.420        R-Sq = 47.4%     R-Sq(adj) = 43.7%

Analysis of Variance

Source           DF          SS              MS        F                           P
                       50.972          25.486    12.64                       0.000
Regression       2     56.463
                      107.435           2.017
Residual Error 28

Total            30

Source      DF        Seq SS
xb2
xc2         1         23.093

            1         27.879

Obs         xb2       y           Fit StDev Fit Residual St Resid

1       -1.00    26.500        27.879  0.457                                 -1.379   -1.03

2       -1.00    30.500        27.879  0.457                                 2.621    1.95

3       -1.00    27.000        27.879  0.457                                 -0.879   -0.65

4       -1.00    28.000        27.879  0.457                                 0.121    0.09

5       -1.00    28.500        27.879  0.457                                 0.621    0.46

6       -1.00    28.000        27.879  0.457                                 0.121    0.09

7       -1.00    25.000        27.879  0.457                                 -2.879   -2.14R

8          1.00  28.500        29.543  0.437                                 -1.043   -0.77

9          1.00  28.500        29.543  0.437                                 -1.043   -0.77

10         1.00  30.000        29.543  0.437                                 0.457    0.34

11         1.00  32.500        29.543  0.437                                 2.957    2.19R

12         1.00  29.500        29.543  0.437                                 -0.043   -0.03

13         1.00  32.000        29.543  0.437                                 2.457    1.82

14         1.00  29.000        29.543  0.437                                 -0.543   -0.40

15         1.00  28.000        29.543  0.437                                 -1.543   -1.14

16      -1.00    28.000        25.981  0.437                                 2.019    1.49

17      -1.00    25.000        25.981  0.437                                 -0.981   -0.73

18      -1.00    26.500        25.981  0.437                                 0.519    0.38

19      -1.00    26.500        25.981  0.437                                 0.519    0.38

20      -1.00    24.500        25.981  0.437                                 -1.481   -1.10

21      -1.00    25.000        25.981  0.437                                 -0.981   -0.73

22      -1.00    28.000        25.981  0.437                                 2.019    1.49

23      -1.00    26.000        25.981  0.437                                 0.019    0.01
9.3 Application of Multiple Regression in Response Surface Problems and Factorial Analyses 719

          24            1.00         27.000              27.644    0.437  -0.644  -0.48
                                                                                   1.00
          25            1.00         29.000              27.644    0.437  1.356
                                                                                  -0.11
          26            1.00         27.500              27.644    0.437  -0.144  -0.11
                                                                                  -0.11
          27            1.00         27.500              27.644    0.437  -0.144
                                                                                   0.26
          28            1.00         27.500              27.644    0.437  -0.144  -0.48
                                                                                  -1.22
          29            1.00         28.000              27.644    0.437  0.356

          30            1.00         27.000              27.644    0.437  -0.644

          31            1.00         26.000              27.644    0.437  -1.644

          R denotes an observation with a large standardized residual

               Example 5 has been treated as if the lack of balance in the data came about
          by misfortune. And the lack of balance in Example 4 did come about in such a
          way. But lack of balance in p-way factorial data can also be the result of careful
          planning. Consider, for example, a 24 factorial situation where the budget can
          support collection of 20 observations but not as many as 32. In such a case, complete
          replication of the 16 combinations of two levels of four factors in order to achieve
          balance is not possible. But it makes far more sense to replicate four of the 16
          combinations (and thus be able to calculate sP and honestly assess the size of
          background variation) than to achieve balance by using no replication. By now
          it should be obvious how to subsequently go about the analysis of the resulting
          partially replicated (and thus unbalanced) factorial data.

Section 3 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Flood and Shankwitz reported the results of a met-     Time,    Temperature,   Increase in
   allurgical engineering design project involving the   x1 (min)     x2 (F)     Hardness, y
   tempering response of a certain grade of stainless
   steel. Slugs of this steel were preprocessed to rea-    150           800        4, 2, -2
   sonably uniform hardnesses, which were measured         150           900     -1, -1, -2
   and recorded. The slugs were then tempered at var-      150         1000      -4, -5, -7
   ious temperatures for various lengths of time. The      150         1100      -7, -5, -8
   hardnesses were then remeasured and the change          500           800
   in hardness computed. The data in the accompa-          500           900        1, -3, 0
   nying tables were obtained in this replicated 4 × 4     500         1000      -2, -8, -2
   factorial study.                                        500         1100      -8, -7, -7
                                                                                 -11, -9, -5

 Time,    Temperature,  Increase in                      (a) Fit the quadratic model
x1 (min)     x2 (F)     Hardness, y
                                                               y = 0 + 1 ln(x1) + 2x2 + 3 ln(x1) 2 +
     5          800       0, 0, -1                                  4x22 + 5x2 ln(x1) +
     5          900      -3, -2, 1
     5        1000       -1, -1, 0                           to these data. What fraction of the observed
     5        1100        -4, 1, 3                           variability in hardness increase is accounted for
    50          800       3, 4, -1                           in the fitting of the quadratic response surface?
    50          900      -3, -1, 1                           What is your estimate of the standard deviation
    50        1000      -4, -1, -3                           of hardness changes that would be experienced
    50        1100      -4, -4, -2
720 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

     at any fixed combination of time and tempera-                 (iii) give an approximate 95% lower tolerance
     ture? How does this estimate compare with sP?                 bound for the hardness increases of 90% of
     Does there appear to be enough difference be-                 such slugs undergoing tempering.
     tween the two values to cast serious doubt on
     the appropriateness of the regression model?          2. Return to the situation of Chapter Exercise 10 of
(b) There was some concern on the project group's             Chapter 8 and the chemical product impurity study.
     part that the 5-minute time was completely un-           The analysis suggested in that exercise leads to the
     like the other times and should not be consid-           conclusion that only the A and B main effects are
     ered in the same analysis as the longer times.           detectably nonzero. The data are unbalanced, so it
     Temporarily delete the 12 slugs treated only 5           is not possible to use the reverse Yates algorithm
     minutes from consideration, refit the quadratic          to fit the "A and B main effects only" model to the
     model, and compare fitted values for the 36              data.
     slugs tempered longer than 5 minutes for this             (a) Use the dummy variable regression techniques
     regression to those from part (a). How different              to fit the "A and B main effects only" model.
     are these two sets of values?                                 (You should be able to pattern what you do
Henceforth consider the quadratic model fitted to                  after Example 5.) How do A and B main
all 48 data points.                                                effects estimated on the basis of this few-
(c) Make a contour plot showing how y varies with                  effects/simplified description of the pattern of
     ln(x1) and x2. In particular, use it to identify the          response compare with what you obtained for
     region of ln(x1) and x2 values where the tem-                 fitted effects using the Yates algorithm?
     pering seems to provide an increase in hard-             (b) Compute and plot standardized residuals for
     ness. Sketch the corresponding region in the                  the few-effects model. (Plot against levels of
     (x1, x2)-plane.                                               A, B, and C, against y^ , and normal-plot them.)
(d) For the x1 = 50 and x2 = 800 set of conditions,                Do any of these plots indicate any problems
     (i) give a 95% two-sided confidence interval                  with the few-effects model?
     for the mean increase in hardness provided by             (c) How does sFE (which you can read directly off
     tempering.                                                    your printout as sSF) compare with sP in this
     (ii) give a 95% two-sided prediction interval                 situation? Do the two values carry any strong
     for the increase in hardness produced by tem-                 suggestion of lack of fit?
     pering an additional slug.

Chapter 9 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Return to the situation of Chapter Exercise 3 of        (b) Make a plot of the observed y's versus the cor-
   Chapter 4 and the grain growth study of Huda and             responding ln(x2)'s. On this plot, sketch the lin-
   Ralph. Consider an analysis of the researchers' data         ear fitted response functions (y^ versus ln(x2))
   based on the model                                           for x1 = 1443, 1493, and 1543. Notice that the
                                                                fit to the researchers' data is excellent. How-
      y = 0 + 1x1 + +2 ln(x2) + 3x1 ln(x2) +                    ever, notice also that the model has four 's and
                                                                was fit based on only nine data points. What
    (a) Fit this model to the data given in Chapter 4.          possibility therefore needs to be kept in mind
        Based on this fit, what is your estimate of the         when making predictions based on this model?
        standard deviation of grain size, y, associated
        with different specimens treated using a fixed
        temperature and time?
                                                                             Chapter 9 Exercises 721

    (c) Make a 95% two-sided confidence interval           x1  x2   x3  x4   y1  y2  y3
        for the mean y when a temperature of x1 =
        1493K and a time of x2 = 120 minutes are           (W) (mTorr) (cm) (sccm) (A° /min) (%) (SiN/poly)
        used.
                                                           275 450  0.8 125  1075 2.7 1.63
   (d) Make a 95% two-sided prediction interval for        275 500  1.0 160   633 4.9 1.37
        an additional grain size, y, when a tempera-       275 550  1.2 200   406 4.6 1.10
        ture of x1 = 1493K and a time of x2 = 120          300 450  1.0 200   860 3.4 1.58
        minutes are used.                                  300 500  1.2 125   561 4.6 1.26
                                                           275 450  0.8 125
    (e) Find a 95% two-sided confidence interval for       300 550  0.8 160  1052 1.7 1.72
        the mean y when a temperature of x1 =1500K         325 450  1.2 160   868 4.6 1.65
        and a time of x2 = 500 minutes are used. (This     325 500  0.8 200   669 5.0 1.42
        is not a set of conditions in the original data    325 550  1.0 125
        set. So you will need to inform your regression    275 450  0.8 125  1138 2.9 1.69
        program of where you wish to predict.)                                749 5.6 1.54

    (f) What does the hypothesis H0 : 1 = 2 =                                1037 2.6 1.72
        3 = 0 mean in the context of this study and
        the model being used in this exercise? Find            The data are listed in the order in which they were
        the p-value associated with an F test of this          actually collected. Notice that the conditions un-
        hypothesis.                                            der which the first, sixth, and eleventh data points
                                                               were collected are the same--that is, there is some
   (g) What does the hypothesis H0 : 3 = 0 mean in             replication in this fractional factorial data set.
        the context of this study and the model being          (a) The fact that the first, sixth, and last data points
        used in this exercise? Find the p-value associ-
        ated with a two-sided t test of this hypothesis.            were collected under the same set of process
                                                                    conditions provides some check on the con-
2. The article "Orthogonal Design for Process Opti-                 sistency of experimental results across time in
   mization and its Application in Plasma Etching"                  this study. What else might (should) have been
   by Yin and Jillie (Solid State Technology, 1987)                 done in this study to try to make sure that time
   discusses a 4-factor experiment intended to guide                trends in an extraneous variable don't get con-
   optimization of a nitride etch process on a single               fused with the effects of the experimental vari-
   wafer plasma etcher. Data were collected at only                 ables (in particular, the effect of x1, as the ex-
   nine out of 34 = 81 possible combinations of three               periment was run)? (Consider again the ideas
   levels of each of the four factors (making up a so-              of Section 2.3.)
   called orthogonal array). The factors involved in           (b) Fit a linear model in all of x1, x2, x3, and x4
   the experimentation were the Power applied to the                to each of the three response variables. Notice
   cathode x1, the Pressure in the reaction chamber x2,             that although such a model appears to provide
   the spacing or Gap between the anode and the cath-               a good fit to the y3 data, the situations for y1
   ode x3, and the Flow of the reactant gas C2F6, x4.               and y2 are not quite so appealing. (Compare
   Three different responses were measured, an etch                 sSF to sP for y1 and note that R2 for the second
   rate for SiN y1, a uniformity for SiN y2, and a se-              variable is relatively low, at least compared to
   lectivity of the process (for silicon nitride) between           what one can achieve for y3.)
   silicon nitride and polysilicon y3. Eight of the nine       (c) In search of better-fitting equations for the y2
   different combinations were run once, while one                  (or y1) data, one might consider fitting a full
   combination was run three times. The researchers                 quadratic equation in x1, x2, x3, and x4 to the
   reported the data given in the accompanying table.               data. What happens when you attempt to do
                                                                    this using a regression package? (The problem
                                                                    is that the data given here are not adequate to
722 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

        distinguish between various possible quadratic            bonder. The effects of the variables Force, Ultra-
        response surfaces in four variables.)                     sonic Power, Temperature, and Time on the final
   (d) In light of the difficulty experienced in (c),             ball bond shear strength were studied. The accom-
        a natural thing to do might be to try to fit              panying table gives data like those collected by the
        quadratic surfaces involving only some of all             authors. (The original data were not given in the
        possible second-order terms. Fit the two mod-             paper, but enough information was given to pro-
        els for y2 including (i) x1, x2, x3, x4, x12, x22,        duce these simulated values that have structure like
         x32, and x42 terms, and (ii) x1, x2, x4, x12, x22, x42,  the original data.)
         x1x2, and x2x4 terms. How do these two fitted
        equations compare in terms of y^ 2 values for             Force,   Power,   Temp.,  Time,    Strength,
        (x1, x2, x3, x4) combinations in the data set?            x1 (gm)  x2 (mw)  x  3 C  x4 (ms)   y (gm)
        How do y^ 2 values compare for the two fitted
        equations when x1 = 325, x2 = 550, x3 = 1.2,                 30       60     175      15       26.2
        and x4 = 200? (Notice that although this last                40       60     175      15       26.3
        combination is not in the data set, there are                30       90     175      15       39.8
        values of the individual variables in the data               40       90     175      15       39.7
        set matching these.) What is the practical engi-             30       60     225      15       38.6
        neering difficulty faced in a situation like this,           40       60     225      15       35.5
        where there is not enough data available to fit              30       90     225      15       48.8
        a full quadratic model but it doesn't seem that              40       90     225      15       37.8
        a model linear in the variables is an adequate               30       60     175      25       26.6
        description of the response?                                 40       60     175      25       23.4
   Henceforth, confine attention to y3 and consider an               30       90     175      25       38.6
   analysis based on a model linear in all of x1, x2, x3,            40       90     175      25       52.1
   and x4.                                                           30       60     225      25       39.5
    (e) Give a 90% two-sided individual confidence                   40       60     225      25       32.3
        interval for the increase in mean selectivity ra-            30       90     225      25       43.0
        tio that accompanies a 1 watt increase in power.             40       90     225      25       56.0
    (f) What appear to be the optimal (large y3) set-                25       75     200      20       35.2
        tings of the variables x1, x2, x3, and x4 (within            45       75     200      20       46.9
        their respective ranges of experimentation)?                 35       45     200      20       22.7
        Refer to the coefficients of your fitted equa-               35      105     200      20       58.7
        tion from (b).                                               35       75     150      20       34.5
   (g) Give a 90% two-sided confidence interval for                  35       75     250      20       44.0
        the mean selectivity ratio at the combination of             35       75     200      10       35.7
        settings that you identified in (f). What cautions           35       75     200      30       41.8
        would you include in a report in which this                  35       75     200      20       36.5
        interval is to appear? (Under what conditions                35       75     200      20       37.6
        is your calculated interval going to have real-              35       75     200      20       40.3
        world meaning?)                                              35       75     200      20       46.0
                                                                     35       75     200      20       27.8
3. The article "How to Optimize and Control the Wire                 35       75     200      20       40.3
   Bonding Process: Part II" by Scheaffer and Levine
   (Solid State Technology, 1991) discusses the use
   of a k = 4 factor central composite design in the
   improvement of the operation of the K&S 1484XQ
                                                           Chapter 9 Exercises 723

    (a) Fit both the full quadratic response surface and      and an Fr-k-1,n-r reference distribution, where
        the simpler linear response surface to these          large values of F count as evidence against H0.
        data. On the basis of simple examination of           (If sSF is much larger than sP, the difference in the
        the R2 values, does it appear that the quadratic      numerator of F will be large, producing a large
        surface is enough better as a data summary            sample value and a small observed level of signifi-
        to make it worthwhile to suffer the increased         cance.)
        complexity that it brings with it? How do the          (a) It is not possible to use the lack of fit test in
        sSF values for the two fitted models compare
        to sP computed from the final six data points              any of Exercise 3 of Section 4.1, Exercise 2
        listed here?                                               of Section 4.2, or Chapter Exercises 2 or 3 of
                                                                   Chapter 4. Why?
   (b) Conduct a formal test (in the full quadratic           (b) For the situation of Exercise 2 of Section 9.1,
        model) of the hypothesis that the linear model             conduct a formal test of lack of fit of the linear
         y = 0 + 1x1 + 2x2 + 3x3 + 4x4 + is                        relationship µy|x = 0 + 1x to the concrete
        an adequate description of the response. Does              strength data.
        your p-value support your qualitative judg-            (c) For the situation of Exercise 1 of Section 9.3,
        ment from part (a)?                                        conduct a formal test of lack of fit of the full
                                                                   quadratic relationship
    (c) In the linear model y = 0 + 1x1 + 2x2 +
        3x3 + 4x4 + , give a 90% confidence inter-                 µy|x ,x = 0 + 1 ln(x1) + 2x2 + 3 ln(x1) 2
        val for 2. Interpret this interval in the context
        of the original engineering problem. (What is                             12
        2 supposed to measure?) Would you expect
        the p-value from a test of H0 : 2 = 0 to be                             + 4x22 + 5x2 ln(x1)
        large or to be small?
                                                                   to the hardness increase data.
   (d) Use the linear model and find an approximate           (d) For the situation of Chapter Exercise 3, con-
        95% lower tolerance bound for 98% of bond
        shear strengths at the center point x1 = 35,               duct a formal test of lack of fit of the linear
         x2 = 75, x3 = 200, and x4 = 20.                           relationship

4. (Testing for "Lack of Fit" to a Regression Model)                       µy|x1,x2,x3,x4 = 0 + 1x1 + 2x2
   In curve- and surface-fitting problems where there                                         + 3x3 + 4x4
   is some replication, this text has used the informal
   comparison of sSF (or sLF) to sP as a means of de-              to the ball bond shear strength data.
   tecting poor fit of a regression model. It is actually
   possible to use these values to conduct a formal        5. Return to the situation of Chapter Exercises 18 and
   significance test for lack of fit. That is, under the      19 of Chapter 4 and the ore refining study of S.
   one-way normal model of Chapter 7, it is possible          Osoka. In that study, the object was to discover set-
   to test                                                    tings of the process variables x1 and x2 that would
                                                              simultaneously maximize y1 and minimize y2.
   H0 : µy|x1,x2,...,xk = 0 + 1x1 + 2x2 + · · · + k xk         (a) Fit full quadratic response functions for y1 and
                                                                    y2 to the data given in Chapter 4. Compute
   using the test statistic                                        and plot standardized residuals for these two
                                                                   fitted equations. Comment on the appearance
                    (n - k - 1)sS2F - (n - r )sP2                  of these plots and what they indicate about the
              F = 2 r - k - 1                                      appropriateness of the fitted response surfaces.
                                                              (b) One useful rule of thumb in response surface
                                    sP                             studies (suggested by Box, Hunter, and Hunter
                                                                   in their book Statistics for Experimenters) is to
724 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

     check that for a fitted surface involving a total       6. Return to the concrete strength testing situation of
     of l coefficients b (including b0),                        Chapter Exercise 16 of Chapter 4.
                                                                 (a) Find estimates of the parameters 0, 1, and
                max y^ - min y^ > 4 l · sS2F                          in the simple linear regression model y =
                                             n                       0 + 1x + .
                                                                (b) Compute standardized residuals and plot them
     before trying to make decisions based on its na-                in the same ways that you were asked to plot
     ture (bowl-shape up or down, saddle, etc.) or                   the ordinary residuals in part (g) of the problem
     do even limited interpolation or extrapolation.                 in Chapter 4. How much do the appearances of
     This criterion is a comparison of the movement                  the new plots differ from the earlier ones?
     of the fitted surface across those n data points            (c) Make a 95% two-sided confidence interval for
     in hand, to four times an estimate of the root of               the increase in mean compressive strength that
     the average variance associated with the n fit-                 accompanies a 5 psi increase in splitting tensile
     ted values y^ . If the criterion is not satisfied, the          strength. (Note: This is 51.)
     interpretation is that the fitted surface is so flat       (d) Make a 90% two-sided confidence interval for
     (relative to the precision with which it is deter-              the mean strength of specimens with splitting
     mined) as to make it impossible to tell with any                tensile strength 300 psi (based on the simple
     certainty the true nature of how mean response                  linear regression model).
     varies as a function of the system variables.               (e) Make a 90% two-sided prediction interval for
Judge the usefulness of the surfaces fitted in part (a)              the strength of an additional specimen with
against this criterion. Do the response surfaces ap-                 splitting tensile strength 300 psi (based on the
pear to be determined adequately to support further                  simple linear regression model).
analysis (involving optimization, for example)?                  (f) Find an approximate 95% lower tolerance
(c) Use the analytic method discussed in Section                     bound for the strengths of 90% of additional
     9.3 to investigate the nature of the response sur-              specimens with splitting tensile strength 300
     faces fitted in part (a). According to the signs                psi (based on the simple linear regression model).
     of the eigenvalues, what kinds of surfaces were
     fitted to y1 and y2, respectively?                      7. Wiltse, Blandin, and Schiesel experimented with
(d) Make contour plots of the fitted y1 and y2 re-              a grain thresher built for an agricultural engineer-
     sponse surfaces from (a) on a single set of                ing design project. They ran efficiency tests on the
     (x1, x2)-axes. Use these to help locate (at least          cleaning chamber of the machine. This part of the
     approximately) a point (x1, x2) with maximum               machine sucks air through threshed material, draw-
     predicted y1, subject to a constraint that pre-            ing light (nonseed) material out an exhaust port,
     dicted y2 be no larger than 55.                            while the heavier seeds fall into a collection tray.
(e) For the point identified in part (d), give 90%              Airflow is governed by the spacing of an air relief
     two-sided prediction intervals for the next val-           door. The following are the weights, y (in grams),
     ues of y1 and y2 that would be produced by                 of the portions of 14 gram samples of pure oat seeds
     this refining process. Also give an approximate            run through the cleaning chamber that ended up in
     95% lower tolerance bound for 90% of ad-                   the collection tray. Four different door spacings x
     ditional pyrite recoveries and an approximate              were used, and 20 trials were made at each door
     95% upper tolerance bound for 90% of addi-                 spacing.
     tional kaolin recoveries at this combination of
     x1 and x2 settings.
                                                                Chapter 9 Exercises 725

.500 in. Spacing                                                    (e) Give a 90% lower prediction bound for the next
                                                                        weight of the part of such a sample that would
12.00, 12.30, 12.45, 12.45, 12.50, 12.50, 12.50, 12.60, 12.65,          wind up in the collection tray using a 1.000 in.
12.70, 12.70, 12.80, 12.90, 12.90, 13.00, 13.00, 13.00, 13.10,          door spacing.
13.20, 13.20
                                                                    (f) Give an approximate 95% lower tolerance for
.875 in. Spacing                                                        90% of the weights of all such samples that
                                                                        would wind up in the collection tray using a
12.40, 12.80, 12.80, 12.90, 12.90, 12.90, 12.90, 13.00, 13.00,          1.000 in. door spacing.
13.00, 13.00, 13.20, 13.20, 13.20, 13.30, 13.40, 13.40, 13.45,
13.45, 13.70                                                    8. Return to the armor testing context of Chapter Ex-
                                                                   ercise 21 of Chapter 4. In what follows, base your
1.000 in. Spacing                                                  answers on the model y = 0 + 1x1 + 2x2 + .
                                                                    (a) Based on this model, what is your estimate
12.00, 12.80, 12.80, 12.90, 12.90, 13.00, 13.00, 13.00, 13.15,          of the standard deviation of ballistic limit, y,
13.20, 13.20, 13.30, 13.40, 13.40, 13.45, 13.50, 13.60, 13.60,          associated with different specimens of a given
13.60, 13.70                                                            thickness and Brinell hardness?
                                                                   (b) Find and plot the standardized residuals. (Plot
1.250 in. Spacing                                                       them versus x1, versus x2, and versus y^ and
                                                                        normal-plot them.) Comment on the appear-
12.10, 12.20, 12.25, 12.25, 12.30, 12.30, 12.30, 12.40, 12.50,          ance of your plots.
12.50, 12.50, 12.60, 12.60, 12.85, 12.90, 12.90, 13.00, 13.10,      (c) Make 90% two-sided confidence intervals for
13.15, 13.25                                                            1 and for 2. Based on the second of these,
                                                                        what increase in mean ballistic limit would you
   Use the quadratic model y = 0 + 1x + 2x2 +                           expect to accompany a 20-unit increase in the
   and do the following.                                                Brinell hardness number?
    (a) Find an estimate of  in the model above. What              (d) Make a 95% two-sided confidence interval for
                                                                        the mean ballistic limit when a thickness of
        is this supposed to measure? How does your                       x1 = 258 (.001 in.) and a Brinell hardness of
        estimate compare to sP here? What does this                      x2 = 391 are involved.
        comparison suggest to you?                                  (e) Make a 95% two-sided prediction interval for
   (b) Use an F statistic and test the null hypothesis                  an additional ballistic limit when a thickness
        H0 : 1 = 2 = 0. (You may take values off a                      of x1 = 258 (.001 in.) and a Brinell hardness
        printout to do this but show the whole five-step                of x2 = 391 are involved.
        significance-testing format.) What is the mean-             (f) Find an approximate 95% lower tolerance
        ing of this hypothesis in the present context?                  bound for 98% of additional ballistic limits
    (c) Use a t statistic and test the null hypothesis                  when a thickness of x1 = 258 (.001 in.) and a
        H0 : 2 = 0. (Again, you may take values off a                   Brinell hardness of x2 = 391 are involved.
        printout to do this but show the whole five-step           (g) Find a 95% two-sided confidence interval for
        significance-testing format.) What is the mean-                 the mean ballistic limit when a thickness of
        ing of this hypothesis in the present context?                   x1 = 260 (.001 in.) and a Brinell hardness of
   (d) Give a 90% lower confidence bound for the                         x2 = 380 are involved.
        mean weight of the part of such samples that               (h) What does the hypothesis H0 : 1 = 2 = 0
        would wind up in the collection tray using a                    mean in the context of this study and the model
        1.000 in. door spacing.                                         being used in this exercise? Find the p-value
                                                                        associated with an F test of this hypothesis.
726 Chapter 9 Regression Analysis--Inference for Curve- and Surface-Fitting

      (i) What does the hypothesis H0 : 1 = 0 mean                  nificance testing format.) What is the meaning
          in the context of this study and the model be-            of this hypothesis in the present context?
          ing used in this exercise? Find the p-value          (c) Use a t statistic and test the hypothesis H0 :
          associated with a two-sided t test of this hy-            2 = 0 in the quadratic model. (Again, show
          pothesis.                                                 the whole five-step significance testing for-
                                                                    mat.) What is the meaning of this hypothesis
 9. Return to the PETN density/detonation velocity                  in the present context?
     data of Chapter Exercise 23 of Chapter 4.                 (d) Give a 95% two-sided confidence interval for
     (a) Find estimates of the parameters 0, 1, and                 the mean torque at failure for a thread engage-
           in the simple linear regression model y =                ment of 40 (in the units of the problem) using
          0 + 1x + . How does your estimate of                      the quadratic model.
          compare to sP? What does this comparison             (e) Give a 95% two-sided prediction interval for
          suggest about the reasonableness of the re-               an additional torque at failure for a thread
          gression model for the data in hand?                      engagement of 40 using the quadratic model.
     (b) Compute standardized residuals and plot                (f) Give an approximate 99% lower tolerance
          them in the same ways that you plotted the                bound for 95% of torques at failure for studs
          residuals in part (g) of Chapter Exercise 23              having thread engagements of 40 using the
          of Chapter 4. How much do the appearances                 quadratic model.
          of the new plots differ from the earlier ones?
     (c) Make a 90% two-sided confidence interval         11. Return to the situation of Chapter Exercise 28 of
          for the increase in mean detonation velocity         Chapter 4 and the metal cutting experiment of
          that accompanies a 1 g/cc increase in PETN           Mielnick. Consider an analysis of the torque data
          density.                                             based on the model y1 = 0 + 1x1 + 2x2 + .
     (d) Make a 90% two-sided confidence interval              (a) Make a 90% two-sided confidence interval
          for the mean detonation velocity of charges               for the coefficient 1.
          with PETN density 0.65 g/cc.                         (b) Make a 90% two-sided confidence interval
     (e) Make a 90% two-sided prediction interval for               for the mean log torque when a .318 in drill
          the next detonation velocity of a charge with             and a feed rate of .005 in./rev are used.
          PETN density 0.65 g/cc.                              (c) Make a 95% two-sided prediction interval for
      (f) Make an approximate 99% lower tolerance                   an additional log torque when a .318 in drill
          bound for the detonation velocities of 95% of             and a feed rate of .005 in./rev are used. Expo-
          charges having a PETN density of 0.65 g/cc.               nentiate the endpoints of this interval to get
                                                                    a prediction interval for a raw torque under
10. Return to the thread stripping problem of Chapter               these conditions.
     Exercise 24 of Chapter 4.                                 (d) Find a 95% two-sided confidence interval for
     (a) Find estimates of the parameters 0, 1, 2,                  the mean log torque for x1 = .300 in and x2 =
          and  in the model y = 0 + 1x + 2x2 +                      .010 in./rev.
            . How does your estimate of  compare to
          sP? What does this comparison suggest about     12. Return to Chapter Exercise 25 of Chapter 4 and
          the reasonableness of the quadratic model for        the tire grip force study.
          the data in hand? What is your estimate of           (a) Find estimates of the parameters 0, 1, and 
          supposed to be measuring?                                 in the simple linear regression model ln(y) =
     (b) Use an F statistic and test the null hypothe-              0 + 1x + .
          sis H0 : 1 = 2 = 0 for the quadratic model.          (b) Compute standardized residuals and plot
          (You may take values off a printout to help               them in the same ways you plotted the resid-
          you do this but show the whole five-step sig-             uals in part (h) of Chapter Exercise 25 of
                                                             Chapter 9 Exercises 727

          Chapter 4. How much do the appearances of               (e) Give a 90% two-sided prediction interval for
          the new plots differ from the earlier ones?                  the next permeability measured on a specimen
     (c) Make a 90% two-sided confidence interval                      of this type having a 6.5% asphalt content.
          for the increase in mean log grip force that
          accompanies an increase in drag of 10% (e.g.,            (f) Find an approximate 95% lower tolerance
          from 30% drag to 40% drag). Note that this                   bound for the permeability of 90% of the
          is 101.                                                      specimens of this type having a 6.5% asphalt
     (d) Make a 95% two-sided confidence interval                      content.
          for the mean log grip force of a tire of this
          type under 30% drag (based on the simple           14. Consider again the axial breaking strength data
          linear regression model).                               of Koh, Morden, and Ogbourne given in Chapter
     (e) Make a 95% two-sided prediction interval for             Exercise 27 of Chapter 4. At one point in that
          the raw grip force of another tire of this design       exercise, it is argued that perhaps the variable
          under 30% drag. (Hint: Begin by making an               x3 = x12/x2 is the principal determiner of axial
          interval for log grip force of such a tire.)            breaking strength, y.
      (f) Find an approximate 95% lower tolerance                 (a) Plot the 36 pairs (x3, y) corresponding to the
          bound for the grip forces of 90% of tires of                 data given in Chapter 4. Note that a constant
          this design under 30% drag (based on the sim-                 assumption is probably not a good one over
          ple linear regression model for ln(y)).                      the whole range of x3's in the students' data.
                                                                  In light of the point raised in part (a), for purposes
13. Consider again the asphalt permeability data of               of simple linear regression analysis, henceforth
     Woelfl, Wei, Faulstich, and Litwack given in                 restrict attention to those 27 data pairs with x3 >
     Chapter Exercise 26 of Chapter 4. Use the qua-               .004.
     dratic model y = 0 + 1x + 2x2 + and do                       (b) Find estimates of the parameters 0, 1, and
     the following:                                                     in the simple linear regression model y =
     (a) Find an estimate of  in the quadratic model.                  0 + 1x3 + . How does your estimate of 
          What is this supposed to measure? How does                   based on the simple linear regression model
          your estimate compare to sP here? What does                  compare to sP? What does this comparison
          this comparison suggest to you?                              suggest about the reasonableness of the re-
     (b) Use an F statistic and test the null hypothe-                 gression model for the data in hand?
          sis H0 : 1 = 2 = 0 for the quadratic model.             (c) Make a 98% two-sided confidence interval for
          (You may take values off a printout to help                  the mean axial breaking strength of .250 in.
          you do this, but show the whole five-step sig-               dowels 8 in. in length based on the regression
          nificance testing format.) What is the meaning               analysis. How does this interval compare with
          of this hypothesis in the present context?                   the use of formula (6.20) and the four mea-
     (c) Use a t statistic and test the null hypothesis                surements on dowels of this type contained in
          H0 : 2 = 0 in the quadratic model. Again,                    the data set?
          show the whole five-step significance testing           (d) Make a 98% two-sided prediction interval for
          format. What is the meaning of this hypothe-                 the axial breaking strength of a single addi-
          sis in the present context?                                  tional .250 in. dowel 8 in. in length. Do the
     (d) Give a 90% two-sided confidence interval for                  same if the dowel is only 6 in. in length.
          the mean permeability of specimens of this              (e) Make an approximate 95% lower tolerance
          type with a 6.5% asphalt content.                            bound for the breaking strengths of 98% of
                                                                       .250 in. dowels 8 in. in length.
   APPENDIX

  Aq q q q q q q q q q q q q q q q q q q q q q q q q q q q q

               More on Probability
               and Model Fitting

               The introduction to probability theory in Chapter 5 was relatively brief. There

                                  are, of course, important engineering applications of probability that require more
                                  background in the subject. So this appendix gives a few more details and discusses
                                  some additional uses of the theory that are reasonably elementary, particularly in
                                  the contexts of reliability analysis and life data analysis.

                                       The appendix begins by discussing the formal/axiomatic basis for the math-
                                  ematics of probability and several of the most useful simple consequences of the
                                  basic axioms. It then applies those simple theorems of probability to the prediction
                                  of reliability for series, parallel, and combination series-parallel systems. A brief
                                  section treats principles of counting (permutations and combinations) that are some-
                                  times useful in engineering applications of probability. There follows a section on
                                  special probability concepts used with life-length (or time-to-failure) variables. The
                                  appendix concludes with a discussion of maximum likelihood methods for model
                                  fitting and drawing inferences.

                                                qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

         A.1 More Elementary Probability

                                  Like any other mathematical theory or system, probability theory is built on a few
                                  basic definitions and some "rules of the game" called axioms. Logic is applied to
                                  determine what consequences (or theorems) follow from the definitions and axioms.
                                  These, in turn, can be helpful guides as an engineer seeks to understand and predict
                                  the behavior of physical systems that involve chance.

                                       For the sake of logical completeness, this section gives the formal axiomatic
                                  basis for probability theory and states and then illustrates the use of some simple
                                  theorems that follow from this base. Conditional probability and the independence
                                  of events are then defined, and a simple theorem related to these concepts is stated
                                  and its use illustrated.

728
              A.1 More Elementary Probability 729

A.1.1         Basic Definitions and Axioms

              As was illustrated informally in Chapter 5, the practical usefulness of probability
              theory is in assigning sensible likelihoods of occurrence to possible happenings in
              chance situations. The basic, irreducible, potential results in such a chance situation
              are called outcomes belonging to a sample space.

Definition 1  A single potential result of a chance situation is called an outcome. All
              outcomes of a chance situation taken together make up a sample space for the
              situation. A script capital S is often used to stand for a sample space.

                   Mathematically, outcomes are points in a universal set that is the sample space.
              And notions of simple set theory become relevant. For one thing, subsets of S
              containing more than one outcome can be of interest.

Definition 2  A collection of outcomes (a subset of S) is called an event. Capital letters
              near the beginning of the alphabet are sometimes used as symbols for events,
              as are English phrases describing the events.

                   Once one has defined events, the standard set-theoretic operations of comple-

              mentation, union, and intersection can be applied to them. However, rather than
              using the typical "c," "," and "" mathematical notation for these operations, it is
              common in probability theory to substitute the use of the words not, or, and and,

              respectively.

Definition 3  For event A and event B, subsets of some sample space S,

                   1. notA is an event consisting of all outcomes not belonging to A;
                   2. AorB is an event consisting of all outcomes belonging to one, the

                       other, or both of the two events; and
                   3. AandB is an event consisting of all outcomes belonging simultane-

                       ously to the two events.

Example 1     A Redundant Inspection System for Detecting Metal Fatigue Cracks

              Consider a redundant inspection system for the detection of fatigue cracks in metal
              specimens. Suppose the system involves the making of a fluorescent penetrant
              inspection (FPI) and also a (magnetic) eddy current inspection (ECI). When a
730 Appendix A More on Probability and Model Fitting

Example 1     metal specimen is to be tested using this two-detector system, a potential sample
(continued )  space consists of four outcomes corresponding to the possible combinations of
              what can happen at each detector. That is, a possible sample space is specified in
              a kind of set notation as

              S = {(FPI signal and ECI signal), (no FPI signal and ECI signal),   (A.1)

              (FPI signal and no ECI signal), (no FPI signal and no ECI signal)}

              and in tabular and pictorial forms as in Table A.1 and Figure A.1. Notice that
              Figure A.1 can be treated as a kind of Venn diagram--the big square standing for
              S and the four smaller squares making up S standing for events that each consist
              of one of the four different possible outcomes.

                   Using this four-outcome sample space to describe experience with a metal
              specimen, one can define several events of potential interest and illustrate the use
              of the notation described in Definition 3. That is, let

              A = {(FPI signal and ECI signal), (FPI signal and no ECI signal)} (A.2)
              B = {(FPI signal and ECI signal), (no FPI signal and ECI signal)} (A.3)

              Table A.1
              A List of the Possible Outcomes for Two Inspections

              Possible Outcome FPI Detection Signal? ECI Detection Signal?

              1                                       yes              yes

              2                                       no               yes

              3                                       yes              no

              4                                       no               no

                                                           ECI signal

                                                           Yes  No

                               Yes
                 FPI signal

                                No

                 Figure A.1 Graphical represen-
                 tation of four outcomes of two
                 inspections
                                                                   A.1 More Elementary Probability 731

                 Then in words,

                                                A = the FPI detector signals
                                                B = the ECI detector signals

                 Part 1 of Definition 3 means, for example, that using notations () and (A.2),
                   not A = {(no FPI signal and ECI signal), (no FPI signal and no ECI signal)}
                          = the FPI detector doesn't signal

                 Part 2 of Definition 3 means, for example, that using notations (A.2) and (A.3),
                       AorB = {(FPI signal and ECI signal), (FPI signal and no ECI signal),
                                                (no FPI signal and ECI signal)}
                              = at least one of the two detectors' signals

                 And Part 3 of Definition 3 means that again using (A.2) and (A.3), one has
                                       AandB = {(FPI signal and ECI signal)}
                                                 = both of the two detectors' signals

                 notA, AorB, and AandB are shown in Venn diagram fashion in Figure A.2.

                   Elementary set theory allows the possibility that a set can be empty--that is,
              have no elements. Such a concept is also needed in probability theory.

Definition 4  The empty event is an event containing no outcomes. The symbol  is typically
              used to stand for the empty event.

               has the interpretation that none of the possible outcomes of a chance situation occur.
              The way in which  is most useful in probability is in describing the relationship

              between two events that have no outcomes in common, and thus cannot both occur.
              There is special terminology for this eventuality (that AandB = ).

Definition 5  If event A and event B have no outcomes in common (i.e., AandB = ), then
              the two events are called disjoint or mutually exclusive.
732 Appendix A More on Probability and Model Fitting

                                 ECI signal                              ECI signal

              A                  Yes  No              B                  Yes  No

                            Yes                                     Yes
              FPI signal                              FPI signal

                             No                                      No

                                 ECI signal                              ECI signal

                                 Yes  No              AorB               Yes  No

                            Yes                                     Yes
              FPI signal                              FPI signal

                             No                                      No

              notA

                                 ECI signal

              AandB              Yes  No

                            Yes
              FPI signal

                             No

Example 1             Figure A.2 Graphical representations of A, B, notA, AorB, and AandB
(continued )
              From Figure A.2 it is quite clear that, for example, the event A and the event
              notA are disjoint. And the event AandB and the event not(AorB), for example,
              are also mutually exclusive events.

                   Manipulation of events using complementation, union, intersection, etc. is nec-
              essary background, but it is hardly the ultimate goal of probability theory. The goal is
              assignment of likelihoods to events. In order to guarantee that such assignments are
              internally coherent, probabilists have devised what seem to be intuitively sensible
              axioms (or rules of operation) for probability models. Assignment of likelihoods
              in conformance to those rules guarantees that (at a minimum) the assignment is
                                                                   A.1 More Elementary Probability 733

              logically consistent. (Whether it is realistic or useful is a separate question.) The
              axioms of probability are laid out next.

Definition 6  A system of probabilities is an assignment of numbers (probabilities) P[ A]
              to events A in such a way that

                   1. for each event A, 0  P[A]  1,
                   2. P[S] = 1 and P[] = 0, and
                   3. for mutually exclusive events A1, A2, A3, . . . ,

                               P[ A1or A2or A3or . . .] = P[ A1] + P[ A2] + P[ A3] + · · ·

                   The relationships (1), (2), and (3) are the axioms of probability theory.

                   Definition 6 is meant to be in agreement with the ways that empirical relative
              frequencies behave. Axiom (1) says that, as in the case of relative frequencies, only
              probabilities between 0 and 1 make sense. Axiom (2) says that if one interprets a
              probability of 1 as sure occurrence and a probability of 0 as no chance of occurrence,
              it is certain that one of the outcomes in S will occur. Axiom (3) says that if an event
              can be made up of smaller nonoverlapping pieces, the probability assigned to that
              event must be equal to the sum of the probabilities assigned to the pieces.

                   Although it was not introduced in any formal way, the third axiom of probability
              was put to good use in Chapter 5. For example, when concluding that for a Poisson
              random variable X

                       P[2  X  5] = P[X = 2] + P[X = 3] + P[X = 4] + P[X = 5]

                                         = f (2) + f (3) + f (4) + f (5)

              one is really using the third axiom with

                                                       A1 = {X = 2}
                                                       A2 = {X = 3}
                                                       A3 = {X = 4}
                                                       A4 = {X = 5}

                   It is only in very simple situations that one would ever try to make use of
              Definition 6 by checking that an entire candidate set of probabilities satisfies the
              axioms of probability. It is more common to assign probabilities (totaling to 1) to
              individual outcomes and then simply declare that the third axiom of Definition 6
734 Appendix A More on Probability and Model Fitting

                                  will be followed in making up any other probabilities. (This strategy guarantees that
                                  subsequent probability assignments will be logically consistent.)

Example 2     A System of Probabilities for Describing
              a Single Inspection of a Metal Part
              As an extremely simple illustration, consider the result of a single inspection of a
              metal part for fatigue cracks using fluoride penetrant technology. With a sample
              space

                                     S = {crack signaled, crack not signaled}

              there are only four events:

                                                  S
                                                  {crack signaled}
                                                  {no crack signaled}
                                                  

                   An assignment of probabilities that can be seen to conform to Definition 6 is

                                              P[S] = 1
                                              P[crack signaled] = .3
                                              P[no crack signaled] = .7
                                              P[] = 0

              Since they conform to Definition 6, these values make up a mathematically valid
              system of probabilities. Whether or not they constitute a realistic or useful model
              is a separate question that can really be answered only on the basis of empirical
              evidence.

Example 1     Returning to the situation of redundant inspection of metal parts using both
(continued )  fluoride penetrant and eddy current technologies, suppose that via extensive
              testing it is possible to verify that for cracks of depth .005 in., the following four
              values are sensible:

              P[FPI signal and ECI signal] = .48     (A.4)
              P[FPI signal and no ECI signal] = .02  (A.5)
            A.1 More Elementary Probability 735

P[no FPI signal and ECI signal] = .32     (A.6)
P[no FPI signal and no ECI signal] = .18  (A.7)

This assignment of probabilities to the basic outcomes in S is illustrated in
Figure A.3. Since these four potential probabilities do total to 1, one can adopt

them together with provision (3) of Definition 6 and have a mathematically

consistent assignment. Then simple addition gives appropriate probabilities for
all other events. For example, with event A and event B as defined earlier (A =
the FPI detector signals and B = the ECI detector signals),

P[A] = P[the FPI detector signals]
       = P[FPI signal and ECI signal] + P[FPI signal and no ECI signal]
       = .48 + .02
       = .50

And further,

 P[ AorB] = P[at least one of the two detectors signals]
             = P[FPI signal and ECI signal] + P[FPI signal and no ECI signal]
                 + P[no FPI signal and ECI signal]
             = .48 + .02 + .32
             = .82

It is clear that to find the two values, one simply adds the numbers that appear in
Figure A.3 in the regions that are shaded in Figure A.2 delimiting the events in
question.

            ECI signal

            Yes      No

            Yes .48  .02

FPI signal

            No .32   .18

Figure A.3 An assignment of
probabilities to four possible
outcomes of two inspections
736 Appendix A More on Probability and Model Fitting

A.1.2         Simple Theorems of Probability Theory

              The preceding discussion is typical of probability analyses, in that the probabilities
              for all possible events are not explicitly written down. Rather, probabilities for some
              events, together with logic and the basic rules of the game (the probability axioms),
              are used to deduce appropriate values for probabilities of other events that are of
              particular interest. This enterprise is often facilitated by the existence of a number
              of simple theorems. These are general statements that are logical consequences of
              the axioms in Definition 6 and thus govern the assigning of probabilities for all
              probability models.

                   One such simple theorem concerns the relationship between P[ A] and P[not A].

Proposition 1 For any event A,

                                P[not A] = 1 - P[A]

              This fact is again one that was used freely in Chapter 5 without explicit reference.

              For example, in the context of independent, identical success-failure trials, the
              fact that the probability of at least one success (i.e., P[X  1] for a binomial
              random variable X) is 1 minus the probability of 0 successes (i.e., 1 - P[X = 0] =
              1 - f (0)) is really a consequence of Proposition 1.

Example 1     Upon learning, via the addition of probabilities for individual outcomes given in
(continued )  displays (A.4) through (A.7), that the assignment

                                        P[ A] = P[the FPI detector signals]
                                                = .50

              is appropriate, Proposition 1 immediately implies that

                                  P[not A] = P[the FPI detector doesn't signal]
                                             = 1 - P[A]
                                             = 1 - .50
                                             = .50

              is also appropriate. (Of course, if the point here weren't to illustrate the use
              of Proposition 1, this value could just as well have been gotten by adding .32
              and .18.)
                                                                                A.1 More Elementary Probability 737

                                A second simple theorem of probability theory is a variation on axiom (3) of
                           Definition 6 for two events that are not necessarily disjoint. It is sometimes called
                           the addition rule of probability.

        Proposition 2      For any two events, event A and event B,                 (A.8)
(The Addition Rule of                            P[ AorB] = P[A] + P[B] - P[AandB]

            Probability )

                           Note that when dealing with mutually exclusive events, the last term in equation (A.8)
                           is P[] = 0. Therefore, equation (A.8) simplifies to a two-event version of part (3)
                           of Definition 6. When the event A and the event B are not mutually exclusive,
                           the simple addition P[ A] + P[B] (so to speak) counts P[ AandB] twice, and the
                           subtraction in equation (A.8) corrects for this in the computing of P[AorB].

                                The practical usefulness of an equation like (A.8) is that when furnished with
                           any three of the four terms appearing in it, the fourth can be gotten by using simple
                           arithmetic.

Example 3                  Describing the Dual Inspection of a Single Cracked Part
                           Suppose that two different inspectors, both using a fluoride penetrant inspection
                           technique, are to inspect a metal part actually possessing a crack .007 in. deep.
                           Suppose further that some relevant probabilities in this context are

                                              P[inspector 1 detects the crack] = .50
                                              P[inspector 2 detects the crack] = .45
                                              P[at least one inspector detects the crack] = .55

                           Then using equation (A.8),

                            P[at least one inspector detects the crack] = P[inspector 1 detects the crack]
                                  + P[inspector 2 detects the crack] - P[both inspectors detect the crack]

                           Thus,

                                          .55 = .50 + .45 - P[both inspectors detect the crack]

                           so

                                                  P[both inspectors detect the crack] = .40
738 Appendix A More on Probability and Model Fitting

Example 3      Of course, the .40 value is only as good as the three others used to produce it.
(continued )   But it is at least logically consistent with the given probabilities, and if they have
               practical relevance, so does the .40 value.

                    A third simple theorem of probability concerns cases where the basic outcomes
               in a sample space are judged to be equally likely.

Proposition 3  If the outcomes in a finite sample space S all have the same probability, then
               for any event A,

               P[ A] = the number of outcomes in A
                         the number of outcomes in S

               Proposition 3 shows that if one is clever or fortunate enough to be able to conceive
               of a sample space where an equally likely outcomes assignment of probabilities
               is sensible, the assessment of probabilities can be reduced to a simple counting
               problem. This fact is particularly useful in enumerative contexts (see again Definition
               4 in Chapter 1 for this terminology) where one is drawing random samples from a
               finite population.

Example 4      Equally Likely Outcomes in a Random Sampling Scenario

               Suppose that a storeroom holds, among other things, four integrated circuit chips
               of a particular type and that two of these are needed in the fabrication of a
               prototype of an advanced electronic instrument. Suppose further that one of these
               chips is defective. Consider assigning a probability that both of two chips selected
               on the first trip to the storeroom are good chips. One way to find such a value
               (there are others) is to use Proposition 3. Naming the three good chips G1, G2,
               and G3 and the single defective chip D, one can invent a sample space made up
               of ordered pairs, the first entry naming the first chip selected and the second entry
               naming the second chip selected. This is given in set notation as follows:

                  S = {(G1, G2), (G1, G3), (G1, D), (G2, G1), (G2, G3), (G2, D), (G3, G1),
                        (G3, G2), (G3, D), (D, G1), (D, G2), (D, G3)}

               A pictorial representation of S is given in Figure A.4.
                                                                  A.1 More Elementary Probability 739

                                                                      Second chip selected
                                                                   G1 G2 G3 D
                                                          G1

                                                          G2
                                   First chip selected

                                                          G3

                                                            D

                                   Figure A.4 Graphical representation of 12 possible
                                   outcomes when selecting two of four IC chips

              Then, noting that the 12 outcomes in this sample space are reasonably thought
              of as equally likely and that 6 of them do not have D listed either first or second,
              Proposition 3 suggests the assessment

                                           P[two good chips] = 6 = .50
                                                                      12

A.1.3         Conditional Probability and the Independence of Events

              Chapter 5 discussed the notion of independence for random variables. In that dis-
              cussion, the idea of assigning probabilities for one variable conditional on the value
              of another was essential. The concept of conditional assignment of probabilities of
              events is spelled out next.

Definition 7  For event A and event B, provided event B has nonzero probability, the
              conditional probability of A given B is

                                                 P[ A | B] = P[AandB]                              (A.9)
                                                                    P[B]

                   The ratio (A.9) ought to make reasonable intuitive sense. If, for example,

              P[ AandB] = .3 and P[B] = .5, one might reason that "B occurs only 50% of

              the  time,  but  of  those  times  B  occurs,  A  also  occurs  .3  =  60%  of  the  time.  So  .6  is
                                                                              .5
              a sensible assessment of the likelihood of A knowing that indeed B occurs."
740 Appendix A More on Probability and Model Fitting

Example 4     Return to the situation of selecting two integrated circuit chips at random from
(continued )  four residing in a storeroom, one of which is defective. Consider using expres-
              sion (A.9) and evaluating

                    P[the second chip selected is defective | the first chip selected is good]

              Simple counting in the 12-outcome sample space leads to the assignments

                      P[the first chip selected is good] = 9 = .75
                                                                  12

                      P[first chip selected is good and second is defective] = 3 = .25
                                                                                           12

              So using Definition 7,

                                                                                                     31
              P[the second chip selected is defective | the first selected is good] =    12          =
                                                                                         9
                                                                                         12 3

              Of the 9 equally likely outcomes in S for which the first chip selected is good,

              there are 3 for which the second chip selected is defective. If one thinks of the 9

              outcomes for which the first chip selected is good as a kind of reduced sample

              space (brought about by the partial restriction that the first chip selected is good),

              then  the  3  figure  above  is  a  perfectly  plausible  value  for  the  likelihood  that  the
                         9
              second chip is defective.

                   There are sometimes circumstances that make it obvious how a conditional
              probability ought to be assigned. For example, in the context of Example 4, one
              might argue that it is obvious that

                     P[the second chip selected is defective | the first selected is good] = 1
                                                                                                         3

              because if the first is good, when the second is to be selected, the storeroom will
              contain three chips, one of which is defective.

                   When one does have a natural value for P[A | B], the relationship between this
              and the probabilities P[AandB] and P[B] can sometimes be exploited to evaluate
              one or the other of them. This notion is important enough that the relationship (A.9)
              is often rewritten by multiplying both sides by the quantity P[B] and calling the
              result the multiplication rule of probability.
                              A.1 More Elementary Probability 741

           Proposition 4      Provided P[B] > 0, so that P[A | B] is defined,          (A.10)
(The Multiplication Rule                                  P[ AandB] = P[A | B] · P[B]

            of Probability )

Example 5                     The Multiplication Rule of Probability and a Probabilistic Risk Assessment

                              A probabilistic risk assessment of the solid rocket motor field joints used in
                              space shuttles prior to the Challenger disaster was made in "Risk Analysis of the
                              Space Shuttle: Pre-Challenger Prediction of Failure" (Journal of the American
                              Statistical Association, 1989) by Dalal, Fowlkes, and Hoadley. They estimated
                              that for each field joint (at 31 and 200 psi),

                                          P[primary O-ring erosion] = .95
                                          P[primary O-ring blow-by | primary O-ring erosion] = .29

                              Combining these two values according to rule (A.10), one then sees that the
                              authors' assessment of the failure probability for each primary O-ring was

                                          P[primary O-ring erosion and blow-by] = (.29)(.95) = .28

                                   Typically, the numerical values of P[ A | B] and P[A] are different. The dif-
                              ference can be thought of as reflecting the change in one's assessed likelihood of
                              occurrence of A brought about by knowing that B's occurrence is certain. In cases
                              where there is no difference, the terminology of independence is used.

Definition 8                  If event A and event B are such that
                                                                 P[A | B] = P[A]

                              they are said to be independent. Otherwise, they are called dependent.

Example 1                     Consider again the example of redundant fatigue crack inspection with probabil-
(continued )                  ities given in Figure A.3. Since

                                                      P[ECI signal] = .80
                                                      P[ECI signal | FPI signal] = .48 = .96

                                                                                          .50

                              the events {ECI signal} and {FPI signal} are dependent events.
742 Appendix A More on Probability and Model Fitting

Example 1                                             ECI signal
(continued )
                                                      Yes     No

                                                      Yes .4  .1

                    FPI signal

                                                      No .4   .1

                    Figure A.5 A second assignment
                    of probabilities to four possible
                    outcomes of two inspections

                         Of course, different probabilities assigned to individual outcomes in this
                    example can lead to the conclusion that the two events are independent. For
                    example, the probabilities in Figure A.5 give

                                          P[ECI signal] = .4 + .4 = .8
                                          P[ECI signal | FPI signal] = .4 = .8

                                                                               .4 + .1

                    so with these probabilities, the two events would be independent.

The multiplication       Independence is the mathematical formalization of the qualitative notion of un-
rule when A and B   relatedness. One way in which it is used in engineering applications is in conjunction
                    with the multiplication rule. If one has values for P[A] and P[B] and judges the
  are independent   event A and the event B to be unrelated, then independence allows one to replace
                    P[A | B] with P[ A] in formula (A.10) and evaluate P[ AandB] as P[ A] · P[B].
                    (This fact was behind the scenes in Section 5.1 when sequences of independent
                    identical success-failure trials and the binomial and geometric distributions were
                    discussed.)

Example 5           In their probabilistic risk assessment of the pre-Challenger space shuttle solid
(continued )        rocket motor field joints, Dalal, Fowlkes, and Hoadley arrived at the figure

                                                         P[failure] = .023

                    for a single field joint in a shuttle launch at 31F. A shuttle's two solid rocket
                    motors have a total of six such field joints, and it is perhaps plausible to think of
                    their failures as independent events.
                                                 A.1 More Elementary Probability 743

     If a model of independence is adopted, it is possible to calculate as follows:

       P[ joint 1 and joint 2 are both effective] = P[ joint 1 is effective] ×
                                                           P[ joint 2 is effective]

                                                        = (1 - .023)(1 - .023)
                                                        = .95

And in fact, considering all six joints,

            P[at least one joint fails] = 1 - P[all 6 joints are effective]
                                           = 1 - (1 - .023)6
                                           = .13

Section 1 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. Return to the situation of Chapter Exercise 30 of       2.  A bin of nuts is mixed, containing 30%                     1    in. nuts
   Chapter 5, where measured diameters of a turned                                                                        2
   metal part were coded as Green, Yellow, or Red,                             9                                                  1
   depending upon how close they were to a mid-                and  70%        16  in.  nuts.    A  bin   of  bolts  has  40%     2  in.
   specification. Suppose that the probabilities that a
   given diameter falls into the various zones are .6247       bolts  and      60%      9   in.  bolts.   Suppose    that    one  bolt
   for the Green Zone, .3023 for the Yellow Zone, and                                   16
   .0730 for the Red Zone. Suppose further (as in the          and one nut are selected (independently and at ran-
   problem in Chapter 5) that the lathe turning the
   parts is checked once per hour according to the fol-        dom) from the two bins.
   lowing rules: One diameter is measured, and if it is
   in the Green Zone, no further action is needed that         (a) What is the probability that the nut and bolt
   hour. If it is in the Red Zone, the process is immedi-
   ately stopped. If it is in the Yellow Zone, a second             match?
   diameter is measured. If the second diameter is
   in the Green Zone, no further action is necessary,          (b) What is the conditional probability that the nut
   but if it is not, the process is stopped immediately.
   Suppose further that the lathe is physically stable,             is  a  9   in.  nut,    given   that  the  nut  and  bolt  match?
   so that it makes sense to think of successive color                     16
   codes as independent.
    (a) Show that the probability that the process is      3. A physics student is presented with six unmarked
        stopped in a given hour is .1865.
   (b) Given that the process is stopped, what is the          specimens of radioactive material. She knows that
        conditional probability that the first diameter
        was in the Yellow Zone?                                two are of substance A and four are of substance B.

                                                               Further, she knows that when tested with a Geiger

                                                               counter, substance A will produce an average of

                                                               three counts per second, while substance B will

                                                               produce an average of four counts per second. (Use

                                                               Poisson models for the counts per time period.)

                                                               (a) Suppose the student selects a sample at random

                                                                    and makes a one-second check of radioactiv-

                                                                    ity. If one count is observed, how should the

                                                                    student assess the (conditional) probability that

                                                                    the specimen is of substance A?

                                                               (b) Suppose the student selects a sample at random

                                                                    and makes a ten-second check of radioactivity.
744 Appendix A More on Probability and Model Fitting

        If ten counts are observed, how should the stu-      (e) Describe any two mutually exclusive events in
        dent assess the (conditional) probability that           this situation.
        the specimen is of substance A?
    (c) Are your answers to (a) and (b) the same? How    6. A lot of machine parts is checked piece by piece
        should this be understood?                          for Brinell hardness and diameter, with the result-
                                                            ing counts as shown in the accompanying table. A
4. At final inspection of certain integrated circuit        single part is selected at random from this lot.
   chips, 20% of the chips are in fact defective. An         (a) What is the probability that it is more than
   automatic testing device does the final inspection.           1.005 in. in diameter?
   Its characteristics are such that 95% of good chips      (b) What is the probability that it is more than
   test as good. Also, 10% of the defective chips test           1.005 in. in diameter and has Brinell hardness
   as good.                                                      of more than 210?
    (a) What is the probability that the next chip is
        good and tests as good?                                                Diameter
   (b) What is the probability that the next chip tests
        as good?                                                                           1.000 to
    (c) What is the (conditional) probability that the                    < 1.000 in. 1.005 in. > 1.005 in.
        next chip that tests as good is in fact good?
                                                         < 190            154  98        48
5. In the process of producing piston rings, the rings
   are subjected to a first grind. Those rings whose     Brinell 190-210  94   307       99
   thicknesses remain above an upper specification       Hardness > 210 33 72 95
   are reground. The history of the grinding process
   has been that on the first grind,                         (c) What is the probability that it is more than
                                                                 1.005 in. in diameter or has Brinell hardness of
         60% of the rings meet specifications (and are           more than 210?
         done processing)
                                                            (d) What is the conditional probability that it has
         25% of the rings are above the upper specifi-           a diameter over 1.005 in., given that its Brinell
         cation (and are reground)                               hardness is over 210?

         15% of the rings are below the lower specifi-       (e) Are the events {Brinell hardness over 210} and
         cation (and are scrapped)                               {diameter over 1.005 in.} independent? Ex-
                                                                 plain.
   The history has been that after the second grind,
                                                             (f) Name any two mutually exclusive events in this
         80% of the reground rings meet specifications           situation.

         20% of the reground rings are below the lower   7. Widgets produced in a factory can be classified as
         specification                                      defective, marginal, or good. At present, a machine
                                                            is producing about 5% defective, 15% marginal,
   A ring enters the grinding process today.                and 80% good widgets. An engineer plans the fol-
    (a) Evaluate P[the ring is ground only once].           lowing method of checking on the machine's ad-
   (b) Evaluate P[the ring meets specifications].           justment: Two widgets will be sampled initially,
    (c) Evaluate P[the ring is ground only once | the       and if either is defective, the machine will be im-
                                                            mediately adjusted. If both are good, testing will
        ring meets specifications].                         cease without adjustment. If neither of these first
   (d) Are the events {the ring is ground only once}        two possibilities occurs, an additional three wid-
                                                            gets will be sampled. If all three of these are good,
        and {the ring meets specifications} indepen-        or two are good and one is marginal, testing will
        dent events? Explain.
                                                           A.1 More Elementary Probability 745

   cease without machine adjustment. Otherwise, the        Suppose first that the single pair {00} is transmitted.
   machine will be adjusted.
    (a) Evaluate P[only two widgets are sampled and             (a) Find the probability that the pair is correctly
                                                                     received.
        no adjustment is made].
   (b) Evaluate P[only two widgets are sampled].                (b) Find the probability that what is received has
    (c) Evaluate P[no adjustment is made].                           obviously been corrupted.
   (d) Evaluate P[no adjustment is made | only two
                                                                (c) Find the conditional probability that the pair
        widgets are sampled].                                        is correctly received given that it is not obvi-
    (e) Are the events {only two widgets are sam-                    ously corrupted.

        pled} and {no adjustment is made} indepen-         Suppose now that the "doubled string" {00 00 11 11} is
        dent events? Explain.                              transmitted and that the string received is not obviously
    (f) Describe any two mutually exclusive events in      corrupted.
        this situation.
                                                                (d) What is then a reasonable assignment of the
8. Glass vials of a certain type are conforming, blem-               "chance" that the correct message string
   ished (but usable), or defective. Two large lots of               (namely {0 0 1 1}) is received? (Hint: Use
   these vials have the following compositions.                      your answer to part c).)

         Lot 1: 70% conforming, 20% blemished, and         10. Figure A.6 is a Venn diagram with some proba-
         10% defective                                          bilities of events marked on it. In addition to the
                                                                values marked on the diagram, it is the case that,
         Lot 2: 80% conforming, 10% blemished, and              P[B] = .4 and P[C | A] = .8.
         10% defective
                                                           A                         B
   Lot 1 is three times the size of Lot 2 and these two
   lots have been mixed in a storeroom. Suppose that                             .1
   a vial from the storeroom is selected at random to
   use in a chemical analysis.                                    .1
    (a) What is the probability that the vial is from Lot
                                                              .3             .2
        1 and not defective?
   (b) What is the probability that the vial is blem-                    .1
                                                              C
        ished?
    (c) What is the conditional probability that the vial          Figure A.6 Figure for Exercise 10

        is from Lot 1 given that it is blemished?          (a) Finish filling in the probabilities on the di-
                                                                agram. That is, evaluate the three probabili-
9. A digital communications system transmits infor-             ties P[ AandB and notC], P[Aand notB and
   mation encoded as strings of 0's and 1's. As a means         notC] and P[not( AorBorC)] = P[not A and
   of reducing transmission errors, each digit in a mes-        notB and notC].
   sage string is repeated twice. Hence the message
   string {0 1 1 0} would (ideally) be transmitted as      (b) Use the probabilities on the diagram (and your
   {00 11 11 00} and if digits received in a given pair         answers to (a)) and evaluate P[AandB].
   don't match, one can be sure that the pair has been
   corrupted in transmission. When each individual         (c) Use the probabilities on the diagram and eval-
   digit in a "doubled string" like {00 11 11 00} is            uate P[B | C].
   transmitted, there is a probability p of transmis-
   sion error. Further, whether or not a particular digit  (d) Based on the information provided here, are
   is correctly transferred is independent of whether           the events B, C independent events? Explain.
   any other one is correctly transferred.
746 Appendix A More on Probability and Model Fitting

                                                qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

         A.2 Applications of Simple Probability
                 to System Reliability Prediction

                                  Sometimes engineering systems are made up of identifiable components or subsys-
                                  tems that operate reasonably autonomously and for which fairly accurate reliability
                                  information is available. In such cases, it is sometimes of interest to try to predict
                                  overall system reliability from the available component reliabilities. This section
                                  considers how the simple probability material from Section A.1 can be used to help
                                  do this for series, parallel, and combination series-parallel systems.

A.2.1 Series Systems

Definition 9  A system consisting of components C1, C2, C3, . . . , Ck is called a series sys-
              tem if its proper functioning requires the functioning of all k components.

                   Figure A.7 is a representation of a series system made up of k = 3 components.
              The interpretation to be attached to a diagram like Figure A.7 is that the system will
              function provided there is a path from point 1 to point 2 that crosses no failed
              component. (It is tempting, but not a good idea, to interpret a system diagram as a
              flow diagram or like an electrical circuit schematic. The flow diagram interpretation
              is often inappropriate because there need be no sequencing, time progression, com-
              munication, or other such relationship between components in a real series system.
              The circuit schematic notion often fails to be relevant, and even when it might seem
              to be, the independence assumptions typically used in arriving at a system reliability
              figure are of questionable practical appropriateness for electrical circuits.)

                   If it is sensible to model the functioning of the individual system components as
              independent, then the overall system reliability is easily deduced from component
              reliabilities via simple multiplication. For example, for a two-component series
              system,

                  P[the system functions] = P[C1 functions and C2 functions]

                                                = P[C2 functions | C1 functions] · P[C1 functions]

                                                = P[C2 functions] · P[C1 functions]

              1       C1  C2  C3                                2

                      Figure A.7 Three-component series system
                        A.2 Applications of Simple Probability to System Reliability Prediction 747

                        where the last step depends on the independence assumption. And in general, if the
                        reliability of component Ci (i.e., P[Ci functions]) is ri , then assuming that the k
                        components in a series system behave independently, the (series) system reliability
                        (say, RS), becomes

Series system           RS = r1 · r2 · r3 · · · · · rk  (A.11)
reliability for
independent
 components

            Example 6   Space Shuttle Solid Rocket Motor Field Joints as a Series System
(Example 5 revisited )
                        The probabilistic risk assessment of Dalal, Fowlkes, and Hoadley put the relia-
                        bility (at 31F) of pre-Challenger solid rocket motor field joints at .977 apiece.
                        Since the proper functioning of six such joints is necessary for the safe operation
                        of the solid rocket motors, assuming independence of the joints, the reliability of
                        the system of joints is then

                                          RS = (.977)(.977)(.977)(.977)(.977)(.977) = .87

                        as in Example 5. (The .87 figure might well be considered optimistic with regard
                        to the entire solid rocket motor system, as it doesn't take into account any potential
                        problems other than those involving field joints.)

                Since typically each ri is less than 1.0, formula (A.11) shows (as intuitively it
           should) that system reliability decreases as components are added to a series system.
           And system reliability is no better (larger) than the worst (smallest) component
           reliability.

A.2.2 Parallel Systems
           In contrast to series system structure is parallel system structure.

Definition 10           A system consisting of components C1, C2, C3, . . . , Ck is called a parallel
                        system if its proper functioning requires only the functioning of at least one
                        component.

                        Figure A.8 is a representation of a parallel system made up of k = 3 components.
                        This diagram is interpreted in a manner similar to Figure A.7 (i.e., the system will
                        function provided there is a path from point 1 to point 2 that crosses no failed
                        component).
748 Appendix A More on Probability and Model Fitting
                                                                                                C1

                         1  C2                                                                      2

                            C3

                         Figure A.8 Three-component parallel
                         system

                The fact that made it easy to develop formula (A.11) for the reliability of a
           series system is that for a series system to function, all components must function.
           The corresponding fact for a parallel system is that for a parallel system to fail, all
           components must fail. So if it is sensible to model the functioning of the individual
           components in a parallel system as independent, if ri is the reliability of component
           i, and if RP is the (parallel) system reliability,

                     or  1 - RP = P[the system fails]                                                  (A.12)
                                  = P[all components fail]
Parallel system                   = (1 - r1)(1 - r2)(1 - r3) · · · (1 - rk)
  reliability for
  independent            RP = 1 - (1 - r1)(1 - r2)(1 - r3) · · · (1 - rk)
   components

Example 7                Parallel Redundancy and Critical Safety Systems

                         The principle of parallel redundancy is often employed to improve the reliability
                         of critical safety systems. For example, two physically separate automatic shut-
                         down subsystems might be called for in the design of a nuclear power plant. The
                         hope would be that in a rare overheating emergency, at least one of the two would
                         successfully trigger reactor shutdown.

                              In such a case, if the shutdown subsystems are truly physically separate
                         (so that independence could reasonably be used in a model of their emergency
                         operation), relationship (A.12) might well describe the reliability of the overall
                         safety system. And if, for example, each subsystem is 90% reliable, the overall
                         reliability becomes

                                                  RP = 1 - (.10)(.10) = 1 - .01 = .99
                             A.2 Applications of Simple Probability to System Reliability Prediction 749

                                  Expression (A.12) is perhaps a bit harder to absorb than expression (A.11).
                             But the formula functions the way one would intuitively expect. System reliability
                             increases as components are added to a parallel system and is no worse (smaller)
                             than the best (largest) component reliability.

                                  One useful type of calculation that is sometimes done using expression (A.12)
                             is to determine how many equally reliable components of a given reliability r are
                             needed in order to obtain a desired system reliability, RP. Substitution of r for each
                             ri in formula (A.12) gives

                                                                    RP = 1 - (1 - r )k

                             and this can be solved for an approximate number of components required, giving

Approximate number           k  ln(1 - RP)     (A.13)
  of components with               ln(1 - r )
  individual reliability
 r needed to produce         Using (for the sake of example) the values r = .80 and RP = .98, expression (A.13)
         parallel system     gives k  2.4, so rounding up to an integer, 3 components of individual 80% relia-
             reliability RP  bility will be required to give a parallel system reliability of at least 98%.

A.2.3                        Combination Series-Parallel Systems

                             Real engineering systems rarely have purely series or purely parallel structure.
                             However, it is sometimes possible to conceive of system structure as a combination
                             of these two basic types. When this is the case, formulas (A.11) and (A.12) can
                             be used to analyze successively larger subsystems until finally an overall reliability
                             prediction is obtained.

Example 8                    Predicting Reliability for a System with a Combination
                             of Series and Parallel Structure

                             In order for an electronic mail message from individual A to reach individual
                             B, the main computers at both A's site and B's site must be functioning, and at
                             least one of three separate switching devices at a communications hub must be
                             working. If the reliabilities for A's computer, B's computer, and each switching
                             device are (respectively) .95, .99, and .90, a plausible figure for the reliability of
                             the A-to-B electronic mail system can be determined as follows.

                                  An appropriate system diagram is given in Figure A.9, with CA, CB, C1,
                             C2, and C3 standing (respectively) for the A site computer, the B site computer,
                             and the three switching devices. Although this system is neither purely series
                             nor purely parallel, mentally replacing components C1, C2, and C3 with a single
750 Appendix A More on Probability and Model Fitting

Example 8                                                 Switching
(continued )
                                                          subsystem
                                                                                                 C1

              1  CA                                       CB  C2                                     2

                                                                                    C3

                              Figure A.9 System diagram for an electronic mail system

              "switching subsystem" block, there would be a purely series structure. So,
                    C1, C2, and C3 parallel subsystem reliability = 1 - (1 - .90)3 = .999

              via formula (A.12). Then using formula (A.11),
                                   System reliability = (.95)(.99)(.999) = .94

              It is clear that the weak link in this communications system is at site A, rather
              than at B or at the communications hub.

Section 2 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. A series system is to consist of k = 5 independent     Engineering design requirements are that the en-
   components with comparable individual reliabili-       tire system have overall reliability at least .99. Two
   ties. How reliable must each be if the system re-      kinds of components are available. Type A compo-
   liability is to be at least .999? Suppose that it is   nents cost $8 apiece and have reliability .98. Type B
   your job to guarantee components have this kind of     components cost $5 apiece and have reliability .90.
   individual reliability. Do you see any difficulty in   (a) If only type A components are used, what will
   empirically demonstrating this level of component
   performance? Explain.                                       be the minimum system cost? If only type B
                                                               components are used, what will be the mini-
2. A parallel system is to consist of k identical inde-        mum system cost?
   pendent components. Design requirements are that       (b) Find a system design meeting engineering re-
   system reliability be at least .99. Individual com-         quirements that uses some components of each
   ponent reliability is thought to be at least .90. How       type and is cheaper than the best option in
   large must k be?                                            part (a).

3. A combination series-parallel system is to consist
   of k = 3 parallel subsystems, themselves in series.
                                                                                                   A.3 Counting 751

qqqqqqqqqqqqqqqqqqqqqqq

A.3 Counting

                    Proposition 3 and Example 4 illustrate that using a model for a chance situation
                    that consists of a finite sample space S with outcomes judged to be equally likely,
                    the computation of probabilities for events of interest is conceptually a very simple
                    matter. The number of outcomes in the event are simply counted up and divided by
                    the total number of outcomes in the whole sample space. However, in most realistic
                    applications of this simple idea, the process of writing down all outcomes in S and
                    doing the counting involved would be most tedious indeed, and often completely
                    impractical. Fortunately, there are some simple principles of counting that can often
                    be applied to shortcut the process, allowing outcomes to be counted mentally. The
                    purpose of this section is to present those counting techniques.

                         This section presents a multiplication principle of counting, the notion of per-
                    mutations and how to count them, and the idea of combinations and how to count
                    them, along with a few examples. This material is on the very fringe of what is
                    appropriate for inclusion in this book. It is not statistics, nor even really probability,
                    but rather a piece of discrete mathematics that has some engineering implications.
                    It is included here for two reasons. First is the matter of tradition. Counting has tra-
                    ditionally been part of most elementary expositions of probability, because games
                    of chance (cards, coins, and dice) are often assumed to be fair and thus describable
                    in terms of sample spaces with equally likely outcomes. And for better or worse,
                    games of chance have been a principal source of examples in elementary probability.
                    A second and perhaps more appealing reason for including the material is that it
                    does have engineering applications (regardless of whether they are central to the
                    particular mission of this text). Ultimately, the reader should take this short section
                    for what it is: a digression from the book's main story that can on occasion be quite
                    helpful in engineering problems.

A.3.1                 A Multiplication Principle, Permutations,
                      and Combinations

                      The fundamental insight of this section is a multiplication principle that is simply
                      stated but wide-ranging in its implications. To emphasize the principle, it will be
                      stated in the form of a proposition.

  Proposition 5       Suppose a complex action can be thought of as composed of r component
(A Multiplication     actions, the first of which can be performed in n1 different ways, the second
                      of which can subsequently be performed in n2 different ways, the third of
         Principle )  which can subsequently be performed in n3 different ways, etc. Then the total
                      number of ways in which the complex action can be performed is

                                                        n = n1 · n2 · · · · · nr
752 Appendix A More on Probability and Model Fitting

                                       In graphical terms, this proposition is just a statement that a tree diagram that
                                  has n1 first-level nodes, each of which leads to n2 second-level nodes, and so on,
                                  must end up having a total of n1 · n2 · · · · · nr r th-level nodes.

Example 9   The Multiplication Principle and Counting the Number
            of Treatment Combinations in a Full Factorial

            The familiar calculation of the number of different possible treatment combina-
            tions in a full factorial statistical study is an example of the use of Proposition 5.
            Consider a 3 × 4 × 2 study in the factors A, B, and C. One may think of the
            process of writing down a combination of levels for A, B, and C as consisting
            of r = 3 component actions. There are n1 = 3 different ways of choosing a level
            for A, subsequently n2 = 4 different ways of choosing a level for B, and then
            subsequently n3 = 2 different ways of choosing a level for C. There are thus

                                           n1 · n2 · n3 = 3 · 4 · 2 = 24

            different ABC combinations.

Example 10  The Multiplication Principle and Counting the Number of Ways
            of Assigning 4 Out of 100 Pistons to Four Cylinders

            Suppose that 4 out of a production run of 100 pistons are to be installed in a
            particular engine block. Consider the number of possible placements of (dis-
            tinguishable) pistons into the four (distinguishable) cylinders. One may think
            of the installation process as composed of r = 4 component actions. There are
            n1 = 100 different ways of choosing a piston for installation into cylinder 1,
            subsequently n2 = 99 different ways of choosing a piston for installation into
            cylinder 2, subsequently n3 = 98 different ways of choosing a piston for installa-
            tion into cylinder 3, and finally, subsequently n4 = 97 different ways of choosing
            a piston for installation into cylinder 4. There are thus

            100 · 99 · 98 · 97 = 94,109,400

            different ways of placing 4 of 100 (distinguishable) pistons into four (distin-
            guishable) cylinders. (Notice that the job of actually making a list of the different
            possibilities is not one that is practically doable.)

                 Example 10 is a generic type of enough importance that there is some special
            terminology and notation associated with it. The general problem is that of choosing
            an ordering for r out of n distinguishable objects, or equivalently, placing r out
            of n distinguishable objects into r distinguishable positions. The application of
                                                                    A.3 Counting 753

                       Proposition 5 shows that the number of different ways in which this placement can
                       be accomplished is

                                 n(n - 1)(n - 2) · · · (n - r + 1)                             (A.14)

                       since at each stage of sequentially placing objects into positions, there is one less
                       object available for placement. The special terminology and notation for this are
                       next.

Definition 11          If r out of n distinguishable objects are to be placed in an order 1 to r (or
                       equivalently, placed into r distinguishable positions), each such potential ar-
                       rangement is called a permutation. The number of such permutations possible
                       will be symbolized as Pn,r , read "the number of permutations of n things r
                       at a time."

                       In the notation of Definition 11, one has (from expression (A.14) that

                                 Pn,r = n(n - 1)(n - 2) · · · (n - r + 1)

                       that is,

    Formula for the                          n!                                                (A.15)
            number of            Pn,r =

    permutations of                       (n - r )!
n things r at a time

Example 10             In the special permutation notation, the number of different ways of installing
 (continued )          the four pistons is

                                            100!
                                 P100,4 =

                                            96!

Example 11             Permutations and Counting the Number of Possible
                       Circular Arrangements of 12 Turbine Blades

                       The permutation idea of Definition 11 can be used not only in straightforward
                       ways, as in the previous example, but in slightly more subtle ways as well. To
                       illustrate, consider a situation where 12 distinguishable turbine blades are to be
                       placed into a central disk or hub at successive 30 angles, as sketched in Figure
                       A.10. Notice that if one of the slots into which these blades fit is marked on the
754 Appendix A More on Probability and Model Fitting

                Example 11
                 (continued )

                                                       Figure A.10 Hub with 12
                                                       slots for blade installation

                  front face of the hub (and one therefore thinks of the blade positions as completely
                  distinguishable), there are

                                               P12,12 = 12 · 11 · 10 · · · · · 2 · 1

                  different possible arrangements of the blades.
                       But now also consider the problem of counting all possible (circular) ar-

                  rangements of the 12 blades if relative position is taken into account but absolute
                  position is not. (Moving each blade 30 counterclockwise after first installing
                  them would not create an arrangement different from the first, with this under-
                  standing.) The permutation idea can be used here as well, thinking as follows.
                  Placing blade 1 anywhere in the hub essentially establishes a point of reference
                  and makes the remaining 11 positions distinguishable (relative to the point of
                  reference). One then has 11 distinguishable blades to place in 11 distinguishable
                  positions. Thus, there must be

                                                P11,11 = 11 · 10 · 9 · · · · · 2 · 1

                  such circular arrangements of the 12 blades, where only relative position is
                  considered.

                    A second generic counting problem is related to the permutation idea and is
               particularly relevant in describing simple random sampling. That is the problem of
               choosing an unordered collection of r out of n distinguishable objects. The special
               terminology and notation associated with this generic problem are as follows.

Definition 12  If an unordered collection of r out of n distinguishable objects is to be made,
               each such potential collection is called a combination. The number of such
                                                                                                   A.3 Counting 755

                           combinations possible will be symbolized as              n  ,  read  "the  number  of  combi-
                                                                                    r
                           nations of n things r at a time."

                           There is in Definition 12 a slight conflict in terminology with other usage in this text.

                           The "combination" in Definition 12 is not the same as the "treatment combination"

                           terminology used in connection with multifactor statistical studies to describe a

                           set of conditions under which a sample is taken. (The "treatment combination"

                           terminology has been used in this very section in Example 9.) But this conflict

                           rarely causes problems, since the intended meaning of the word combination is

                           essentially always clear from context.

                           Appropriate use of Proposition 5 and formula (A.15) makes it possible to
                                                  n
                           develop a formula for  r      as follows. A permutation of r out of n distinguishable

                           objects can be created through a two-step process. First a combination of r out of

                           the n objects is selected and then those selected objects are placed in an order. This

                           thinking suggests that Pn,r can be written as

                                                                  Pn,r =  n   · Pr,r

                                                                          r

                           But this means that

                                                                n! = n r!
                                                             (n - r )! r 0!

                           that is,

    Formula for the                                               n = n!                                              (A.16)
            number of                                             r r ! (n - r )!

    combinations of
n things r at a time

                           The ratio in equation (A.16) ought to look familiar to readers who have studied
                           Section 5.1. The multiplier of px (1 - p)n-x in the binomial probability function is
                                         n
                           of the form   x  ,  counting  up  the  number  of  ways  of    placing  x  successes   in  a  series

                           of n trials.

              Example 12   Combinations and Counting the Numbers of Possible Samples
(Example 10, Chapter 3,    of Cable Connectors with Prescribed Defect Class Compositions

    revisited--page 105 )  In the cable connector inspection scenario of Delva, Lynch, and Stephany, 3,000
                           inspections of connectors produced 2,985 connectors classified as having no de-
                           fects, 1 connector classified as having only minor defects, and 14 others classified
                           as having moderately serious, serious, or very serious defects. Suppose that in an
                           effort to audit the work of the inspectors, a sample of 100 of the 3,000 previously
                           inspected connectors is to be reinspected.
756 Appendix A More on Probability and Model Fitting

Example 12     Then notice that directly from expression (A.16), there are in fact
 (continued )

                                                      3000 = 3000!
                                                      100 100! 2900!

               different (unordered) possible samples for reinspection. Further, there are

                                                      2985 = 2985!
                                                      100 100! 2885!

               different possible samples of size 100 made up of only connectors judged to
               be defect-free. If (for some reason) the connectors to be reinspected were to be
               chosen as a simple random sample of the 3,000, the ratio

                                                          2985
                                                          100
                                                          3000
                                                          100

               would then be a sensible figure to use for the probability that the sample is
               composed entirely of connectors initially judged to be defect-free.

                    It is instructive to take this example one step further and combine the use
               of Definition 12 and Proposition 5. So consider the problem of counting up the
               number of different samples containing 96 connectors initially judged defect-free,
               1 judged to have only minor defects, and 3 judged to have moderately serious,
               serious, or very serious defects. To solve this problem, the creation of such a
               sample can be considered as a three-step process. In the first, 96 nominally defect-
               free connectors are chosen from 2,985. In the second, 1 connector nominally
               having minor defects only is chosen from 1. And finally, 3 connectors are chosen
               from the remaining 14. There are thus

                                                      2985 · 1 · 14
                                                      96  13

               different possible samples of this rather specialized type.

Example 13     An Application of Counting Principles to the Calculation
               of a Probability in a Scenario of Equally Likely Outcomes

               As a final example in this section, most of the ideas discussed here can be applied
               to the computation of a probability in another situation of equally likely outcomes
               where writing out a list of the possible outcomes is impractical.

                    Consider a hypothetical situation where 15 manufactured devices of a par-
               ticular kind are to be sent 5 apiece to three different testing labs. Suppose further
                                                                       A.3 Counting 757

that 3 of the seemingly identical devices are defective. Consider the probability
that each lab receives 1 defective device, if the assignment of devices to labs is
done at random.

     The total number of possible assignments of devices to labs can be computed
by thinking first of choosing 5 of 15 to send to Lab A, then 5 of the remaining 10
to send to Lab B, then sending the remaining 5 to Lab C. There are thus

                                                          15 · 10 · 5
                                                          5  55

such possible assignments of devices to labs.

On the other hand, if each lab is to receive 1 defective device, there are
                                                                       12                                    8     4
P3,3 ways to assign defective devices to labs and then subsequently    4                                  ·  4  ·  4

possible ways of completing the three shipments. So ultimately, an appropriate

probability assignment for the event that each lab receives 1 defective device is

        12 8 4
P3,3 ·  ··
        4 44                                                 = 3 · 2 · 1 · 12! · 8! · 5! · 10! · 5! · 5!
                                                                      15! · 10! · 4! · 8! · 4! · 4!
15 · 10 · 5
5       55                                                   = 3·2·1·5·5·5
                                                                   15 · 14 · 13

                                                             = .27

Section 3 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. A lot of 100 machine parts contains 10 with diam-              the high side, and 7 parts with diameters that
   eters that are out of specifications on the low side,          are in specifications?
   20 with diameters that are out of specifications on
   the high side, and 70 that are in specifications.      2. The lengths of bolts produced in a factory are
    (a) How many different possible samples of n =           checked with two "go-no go" gauges and the bolts
        10 of these parts are there?                         sorted into piles of short, OK, and long bolts. Sup-
   (b) How many different possible samples of size           pose that of the bolts produced, about 20% are
        n = 10 are there that each contain exactly 1         short, 30% are long, and 50% are OK.
        part with diameter out of specifications on the       (a) Find the probability that among the next ten
        low side, 2 parts with diameters out of spec-             bolts checked, the first three are too short, the
        ifications on the high side, and 7 parts with             next three are OK, and the last four are too
        diameters that are in specifications?                     long.
    (c) Based on your answers to (a) and (b), what is        (b) Find the probability that among the next ten
        the probability that a simple random sample of            bolts checked, there are three that are too short,
        n = 10 of these contains exactly 1 part with              three that are OK, and four that are too long.
        diameter out of specifications on the low side,           (Hint: In how many ways it is possible to
        2 parts with diameters out of specifications on           choose three of the group to be short, three
758 Appendix A More on Probability and Model Fitting

        to be OK, and four to be long? Then use your          (b) What is the probability that all three digits in
        answer to (a).)                                            her number are different?

3. User names on a computer system consist of three            (c) What is the probability that her number uses
   letters A through Z, followed by two digits 0                   three different digits and lists them in either
   through 9. (Letters and digits may appear more                  ascending or descending order?
   than once in a name.)
    (a) How many user names of this type are there?        6. When ready to configure a PC order, a consumer
   (b) Suppose that Joe has user name TPK66, but              must choose a Processor Chip, a MotherBoard, a
        unfortunately he's forgotten it. Joe remembers        Drive Controller and a Hard Drive. The choices
        only the format of the user names and that the        are:
        letters K, P, and T appear in his name. If he
        picks a name at random from those consistent       Processor            Mother-   Drive     Hard
        with his memory, what's the probability that he    Chip                 Board
        selects his own?                                                                  Controller Drive
    (c) If Joe in part (b) also remembers that his digits
        match, what's the probability that he selects his  Fast New Generation  Premium   Premium   Premium
        own user name?                                     Slow New Generation  Standard  Standard  Standard
                                                           Fast Old Generation            Economy   Economy
4. A lot contains ten pH meters, three of which are        Slow Old Generation
   miscalibrated. A technician selects these meters
   one at a time, at random without replacement, and       (a) Suppose initially that all components are com-
   checks their calibration.                                    patible with all components. How many differ-
    (a) What is the probability that among the first four       ent configurations are possible?
        meters selected, exactly one is miscalibrated?
   (b) What is the probability that the technician dis-    Suppose henceforth that:
        covers his second miscalibrated meter when         (i) a Premium MotherBoard is needed to run a New
        checking his fifth one?                            Generation Processor,
                                                           (ii) a Premium MotherBoard is needed to run a
5. A student decides to use the random digit function      Premium Drive Controller, and
   on her calculator to select a three-digit PIN number    (iii) a Premium Drive Controller is needed to run a
   for use with her new ATM card. (Assume that all         Premium Hard Drive.
   numbers 000 through 999 are then equally likely to      (b) How many permissible configurations are there
   be chosen.)
    (a) What is the probability that her number uses            with a Standard MotherBoard?
        only odd digits?                                   (c) How many permissible configurations are there

                                                                total? Explain carefully.

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

A.4 Probabilistic Concepts Useful
       in Survival Analysis

                    Section A.2 is meant to provide only the most elementary insights into how proba-
                    bility might prove useful in the context of reliability modeling and prediction. The
                    ideas discussed in that section are of an essentially "static" nature. They are most
                    appropriate when considering the likelihood of a system performing adequately at
                    a single point in time--for example, at its installation, or at the end of its warranty
                    period.
                                          A.4 Probabilistic Concepts Useful in Survival Analysis 759

                    Reliability engineers also concern themselves with matters possessing a more
               dynamic flavor, having to do with the modeling and prediction of life-length variables
               associated with engineering systems and their components. It is outside the intended
               scope of this text to provide anything like a serious introduction to the large body of
               methods available for probability modeling and subsequent formal inference for such
               variables. But what will be done here is to provide some material (supplementary
               to that found in Section 5.2) that is part of the everyday jargon and intellectual
               framework of reliability engineering. This section will consider several descriptions
               and constructs related to continuous random variables that, like system or component
               life lengths, take only positive values.

A.4.1          Survivorship and Force-of-Mortality Functions

               In this section, T will stand for a continuous random variable taking only nonneg-
               ative values. The reader may think of T as the time till failure of an engineering
               component. In Section 5.2, continuous random variables X (or more properly, their
               distributions) were described through their probability densities f (x) and cumula-
               tive probability functions F(x). In the present context of lifetime random variables,
               there are several other, more popular ways of conveying the information carried by
               f (t) or F(t). Two of these devices are introduced next.

Definition 13  The survivorship function for a nonnegative random variable T is the function
                                           S(t) = P[T > t] = 1 - F(t)

               The survivorship function is also sometimes known as the reliability function. It
               specifies the probability that the component being described survives beyond time t.

Example 14     The Survivorship Function and Diesel Engine Fan Blades

               Data on 70 diesel engines of a single type (given in Table 1.1 of Nelson's Applied
               Life Data Analysis) indicate that lifetimes in hours of a certain fan on such engines
               could be modeled using an exponential distribution with mean   27,800. So
               from Definition 17 in Chapter 5, to describe a fan lifetime T , one could use the
               density

                                       if t < 0
                       0               if t > 0
               f (t ) =  1 e-t/27,800

                          27,800
760 Appendix A More on Probability and Model Fitting

Example 14     or the cumulative probability function
 (continued )

                       F(t) =                         0               if t  0
                                                      1 - e-t/27,800  if t > 0

               or from Definition 13, the survivorship function

                       S(t) =                         1               if t  0
                                                      e-t /27,800     if t > 0

               The probability of a fan surviving at least 10,000 hours is then
                                      S(10,000) = e-10,000/27,800 = .70

                    A second way of specifying the distribution of a life-length variable (unlike any-
               thing discussed in Section 5.2) is through a function giving a kind of "instantaneous
               rate of death of survivors."

Definition 14  The force-of-mortality function for a nonnegative continuous random vari-
               able T is, for t > 0, the function

                                                      h(t) = f (t)
                                                               S (t )

               h(t) is sometimes called the hazard function for T, but such usage tends to perpetu-
               ate unfortunate confusion with the entirely different concept of "hazard rate" for re-
               pairable systems. (The important difference between the two concepts is admirably
               explained in the paper "On the Foundations of Reliability" by W. A. Thompson
               (Technometrics, 1981) and in the book Repairable Systems Reliability by Ascher
               and Feingold.) This book will thus stick to the term force of mortality.

                    The force-of-mortality function can be thought of heuristically as

               h(t) = f (t) = lim P[t < T < t + ]/                 = lim P[t < T < t +  | t < T]

               S(t) 0  P[t < T ]                                          0

               which is indeed a sort of "death rate of survivors at time t."

Example 14     The force-of-mortality function for the diesel engine fan example is, for t > 0,
 (continued )

                       f (t ) 27,800 1 e-t/27,800 1
               h(t) =                                 = -t/27,800 =
                       S (t )                          e              27,800
                                       A.4 Probabilistic Concepts Useful in Survival Analysis 761

                  The exponential (mean  = 27,800) model for fan life implies a constant     1
                                                                                          27,800

                  force of mortality.

Constant force         The property of the fan-life model shown in the previous example is characteris-
 of mortality is  tic of exponential distributions. That is, a distribution has constant force of mortality
 equivalent to    exactly when that distribution is exponential. So having a constant force of mortality
                  is equivalent to possessing the memoryless property of the exponential distributions
   exponential    discussed in Section 5.2. If the lifetime of an engineering component is described
    distribution  using a constant force of mortality, there is no (mathematical) reason to replace such
                  a component before it fails. The distribution of its remaining life from any point in
                  time is the same as the distribution of the time till failure of a new component of the
                  same type.

                       Potential probability models for lifetime random variables are often classified
                  according to the nature of their force-of-mortality functions, and these classifi-
                  cations are taken into account when selecting models for reliability engineering
                  applications. If h(t) is increasing in t, the corresponding distribution is called
                  an increasing force-of-mortality (IFM) distribution, and if h(t) is decreasing
                  in t, the corresponding distribution is called a decreasing force-of-mortality
                  (DFM) distribution. The reliability engineering implications of an IFM distri-
                  bution being appropriate for modeling the lifetimes of a particular type of com-
                  ponent are often that (as a form of preventative maintenance) such components
                  are retired from service once they reach a particular age, even if they have not
                  failed.

Example 15        The Weibull Distributions and Their Force-of-Mortality Functions

                  The Weibull family of distributions discussed in Section 5.2 is commonly used
                  in reliability engineering contexts. Using formulas (5.26) and (5.27) of Section
                  5.2 for the Weibull cumulative probability function and probability density, the
                  Weibull force-of-mortality function for shape parameter  and scale parameter 
                  is, for t > 0

                                          f (t) f (t)   t-1e- t (t/) -1 
                               h(t) = S(t) = 1 - F(t) = e-(t/) = 

                  For  = 1 (the exponential distribution case) this is constant. For  < 1, this is
                  decreasing in t, and the Weibull distributions with  < 1 are DFM distributions.
                  For  > 1, this is increasing in t, and the Weibull distributions with  > 1 are
                  IFM distributions.
762 Appendix A More on Probability and Model Fitting

Example 16  Force-of-Mortality Function for a Uniform Distribution

            As an artificial but instructive example, consider the use of a uniform distribution
            on the interval (0, 1) as a life-length model. With

                           1                             if 0 < t < 1
                 f (t) =                                 otherwise

                           0

            the survivorship function is                 if t < 0
                                                         if 0  t < 1
                                                  1      if 1  t

                                         S(t) = 1 - t    if 0 < t < 1
                                                    0

            so
                                         h(t) = 1
                                                   1-t

                 f (t)

            1.0
             .5

                         0                               1             t
                  h(t)
            3.0
            2.0
            1.0

                                                      0  1             t

            Figure A.11 Probability density and
            force-of-mortality function for a
            uniform distribution
                           A.4 Probabilistic Concepts Useful in Survival Analysis 763

   Figure A.11 shows plots of both f (t) and h(t) for the uniform model. h(t) is
   clearly increasing for 0 < t < 1 (quite drastically so, in fact, as one approaches
   t = 1). And well it should be. Knowing that (according to the uniform model) life
   will certainly end by t = 1, nervousness about impending death should skyrocket
   as one nears t = 1.

     Conventional wisdom in reliability engineering is that many kinds of manufac-
tured devices have life distributions that ought to be described by force-of-mortality
functions qualitatively similar to the hypothetical one sketched in Figure A.12.

                              h(t)

0                                 t

   Figure A.12 A "bathtub curve"
   force-of-mortality function

     The shape in Figure A.12 is often referred to as the bathtub curve shape. It
includes an early region of decreasing force of mortality, a long central period of
relatively constant force of mortality, and a late period of rapidly increasing force
of mortality. Devices with lifetimes describable as in Figure A.12 are sometimes
subjected to a burn-in period to eliminate the devices that will fail in the early
period of decreasing force of mortality, and then sold with the recommendation
that they be replaced before the onset of the late period of increasing force of
mortality or wear-out. Although this story is intuitively appealing, the most tractable
models for life length do not, in fact, have force-of-mortality functions with shapes
like that in Figure A.12. For a further discussion of this matter and references to
papers presenting models with bathtub-shaped force-of-mortality functions, refer to
Chapter 2 of Nelson's Applied Life Data Analysis.

     The functions f (t), F(t), S(t), and h(t) all carry the same information about
a life distribution. They simply express it in different terms. Given one of them,
the derivation of the others is (at least in theory) straightforward. Some of the
764 Appendix A More on Probability and Model Fitting

                       relationships that exist among the four different characterizations are collected here
                       for the reader's convenience. For t > 0,

      Relationships                      t
between F(t), f(t),
                       F(t) = f (x) dx
       S(t), and h(t)
                                       0

                       f (t) = d F(t)
                                dt

                       S(t) = 1 - F(t)

                       h(t) = f (t)
                                S (t )

                                                      t

                       S(t) = exp - h(x) dx

                                                    0

                                                              t

                       f (t) = h(t) exp - h(x) dx

                                                            0

Section 4 Exercises q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q

1. An engineer begins a series of presentations to his           of-mortality function? Is it constant like those
   corporate management with a working bulb in his               of X and Y ?
   slide projector and (an inferior-quality) Brand W
   replacement bulb in his briefcase. Suppose that the   2. A common modeling device in reliability applica-
   random variables                                         tions is to assume that the (natural) logarithm of a
                                                            lifetime variable, T , has a normal distribution. That
       X = the number of hours of service given by          is, one might suppose that for some parameters µ
             the bulb in the projector                      and  , if t > 0

       Y = the number of hours of service given by       F(t) = P[T  t] =  ln t - µ
             the spare bulb                                                   

may be modeled as independent exponential ran-           Consider the µ = 0 and  = 1 version of this.
                                                         (a) Plot F(t) versus t.
dom variables with respective means 15 and 5. The        (b) Plot S(t) versus t.
                                                         (c) Compute and plot f (t) versus t.
number of hours that the engineer may operate            (d) Compute and plot h(t) versus t.
without disaster is X + Y .                              (e) Is this distribution for T an IFM distribution,
(a) Find the mean and standard deviation of X + Y
                                                              a DFM distribution, or neither? What implica-
     using Proposition 1 in Chapter 5.                        tion does your answer have for in-service re-
(b) Find, for t > 0, P[X + Y  t].                             placement of devices possessing this lifetime
(c) Use your answer to (b) and find the probability           distribution?

     density for T = X + Y .
(d) Find the survivorship and force-of-mortality

     functions for T . What is the nature of the force-
      A.5 Maximum Likelihood Fitting of Probability Models and Related Inference Methods 765

qqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

A.5 Maximum Likelihood Fitting of Probability
       Models and Related Inference Methods

                    The model-fitting and inference methods discussed in this text are, for the most
                    part, methods for independent, normally distributed observations. This is in spite
                    of the fact that there are strong hints in Chapter 5 and this appendix that other
                    kinds of probability models often prove useful in engineering problem solving.
                    (For example, binomial, geometric, Poisson, exponential, and Weibull distribu-
                    tions have been discussed, and parts of Sections A.1 and A.2 should suggest that
                    probability models not even necessarily involving these standard distributions will
                    often be helpful.) It thus seems wise to present at least a brief introduction to
                    some principles of probability-model fitting and inference that can be applied
                    more generally than to only scenarios involving independent, normal observa-
                    tions. This will be done to give at least the flavor of what is possible, as well
                    as an idea of some kinds of things likely to be found in the engineering statistics
                    literature.

                         This section considers the use of likelihood functions in the fitting of para-
                    metric probability models and in large-sample inference for the model parameters.
                    It begins by discussing the idea of a likelihood function and maximum likelihood
                    model fitting for discrete data. Similar discussions are then conducted for con-
                    tinuous and mixed data. Finally, there is a discussion of how for large samples,
                    approximate confidence regions and tests can often be developed using likelihood
                    functions.

A.5.1                 Likelihood Functions for Discrete Data
                      and Maximum Likelihood Model Fitting

                      To begin, consider scenarios where the outcome of a chance situation can be de-
                      scribed in terms of a data vector of jointly discrete random variables (or a single
                      discrete random variable) Y, whose probability function f depends on some (un-
                      known) vector of parameters (or single parameter) . To make the dependence of
                      f on explicit, this section will use the notation

                      f ( y)

                      for the (joint) probability function of Y.
                           Chapter 5 made heavy use of particular parametric probability functions, pri-

                      marily thinking of them as functions of y. In this section, it will be very helpful to
                      shift perspective. With data Y = y in hand, think of

     A discrete data  f ( y)  (A.17)
likelihood function
766 Appendix A More on Probability and Model Fitting

                         or (often more conveniently) its natural logarithm

A discrete data                 L( ) = ln f ( y)                                                    (A.18)
 log likelihood
         function

                         as functions of , specifying for various possible vectors of parameters "how likely"
                         it would be to observe the particular data in hand. With this perspective, the function
                         (A.17) is often called the likelihood function and function (A.18) the log likelihood
                         function for the problem under consideration.

             Example 17  The Log Likelihood Function for the Number Conforming
(Example 4, Chapter 5,   in a Sample of Hexamine Pellets

  revisited--page 235 )  In the pelletizing machine example used in Chapter 5 and earlier, it is possible to
                         argue that under stable conditions,

                         X = the number of conforming pellets produced in a batch of 100 pellets

                         might well be modeled using a binomial distribution with n = 100 and p some
                         unknown parameter. The corresponding probability function is thus

                         
                          100!                            100-x
                         f (x) = x! (100 - x)! p (1 - p)x                    x = 0, 1, . . . , 100
                                                                             otherwise
                           0

                              Should one observe X = 66 conforming pellets be observed in a batch, the
                         material just introduced says that the function of p

                         L( p) = ln( f (66)) = ln(100!) - ln(66!) - ln(34!)                         (A.19)
                                                   + 66 ln( p) + 34 ln(1 - p)

                         is an appropriate log likelihood function. Figure A.13 is a sketch of L( p) for
                         this problem. Notice that (in an intuitively appealing fashion) the value of p
                         maximizing L( p) is

                                                                p^ = 66 = .66
                                                                      100

                         That is, p = .66 makes the chance of observing the particular data in hand
                         (X = 66) as large as possible.
A.5 Maximum Likelihood Fitting of Probability Models and Related Inference Methods 767
                                                  L( p)

                                              0
                                            -5
                                           -10
                                           -15

                  .4 .5 .6 .7 .8                                              p

                                                      p = .66

            Figure A.13 Plot of the log likelihood function
            based on 66 conforming tablets out of 100

Example 18  The Log Likelihood Function for n Independent Poisson Observations

            As a second, somewhat more abstract, example of the idea of a likelihood function,
            suppose that X1, X2, . . . , Xn are independent Poisson random variables, Xi with
            mean ki  for k1, k2, . . . , kn known constants, and  an unknown parameter. Such
            a model might, for example, be appropriate in a quality monitoring context, where
            at time i, ki standard-size units of product are inspected, Xi defects are observed,
            and  is a constant mean defects per unit.

                 The joint probability function for X1, X2, . . . , Xn is

                                     n  e-ki (ki )xi        for each xi a nonnegative integer
            f (x1, x2, . . . , xn) = i=1 xi !               otherwise

                                    0

            The log likelihood function in the present context is thus

            n     n                                   n                 n

            L() = - ki + xi ln(ki ) + xi ln() - ln(xi !)                         (A.20)

            i =1  i =1                                i =1              i =1

                 The likelihood functions in Examples 17 and 18 are for individual (univariate)
            parameters. The next example involves two parameters.
768 Appendix A More on Probability and Model Fitting

Example 19  A Log Likelihood Function Based on Pre-Challenger
            Space Shuttle O-Ring Failure Data

            Table A.2 contains pre-Challenger data on field joint primary O-ring failures
            on 23 (out of 24) space shuttle flights. (On one flight, the rocket motors were
            lost at sea, so no data are available.) The failure counts x1, x2, . . . , x23 are the
            numbers (out of 6 possible) of primary O-rings showing evidence of erosion or
            blow-by in postflight inspections of the solid rocket motors, and t1, t2, . . . , t23 are
            the corresponding temperatures at the times of launch.

            Table A.2
            Pre-Challenger Field Joint Primary O-Ring Failure Data

            Flight Date                x,                               t,
                           Number of Field Joint         Temperature at Launch (F)
                         Primary O-Ring Incidents

            4/12/81                                   0             66

            11/12/81                                  1             70

            3/22/82                                   0             69

            11/11/82                                  0             68

            4/4/83                                    0             67

            6/18/83                                   0             72

            8/30/83                                   0             73

            11/28/83                                  0             70

            2/3/84                                    1             57

            4/6/84                                    1             63

            8/30/84                                   1             70

            10/5/84                                   0             78

            11/8/84                                   0             67

            1/24/85                                   2             53

            4/12/85                                   0             67

            4/29/85                                   0             75

            6/17/85                                   0             70

            7/29/85                                   0             81

            8/27/85                                   0             76

            10/3/85                                   0             79

            10/30/85                                  2             75

            11/26/85                                  0             76

            1/12/86                                   1             58
A.5 Maximum Likelihood Fitting of Probability Models and Related Inference Methods 769

     In "Risk Analysis of the Space Shuttle: Pre-Challenger Prediction of Failure"
(Journal of the American Statistical Association, 1989), Dalal, Fowlkes, and
Hoadley considered several analyses of the data in Table A.2 (and other pre-
Challenger shuttle failure data). In one of their analyses of the data given here,
Dalal, Fowlkes, and Hoadley used a likelihood approach based on the observations

        1                          if xi  1
yi = 0                             if xi = 0

that indicate which flights experienced primary O-ring incidents. (They also
considered a likelihood approach based on the counts xi themselves. But here
only the slightly simpler analysis based on the yi 's will be discussed.) The
authors modeled Y1, Y2, . . . , Y23 as a priori independent variables and treated the
probability of at least one O-ring incident on flight i,

pi = P[Yi = 1] = P[Xi  1]

as a function of (temperature) ti . The particular form of dependence of pi on ti
used by the authors was a "linear (in t) log odds" form

ln p =  + t                                         (A.21)
     1- p

for  and  some unknown parameters. Equation (A.21) can be solved for p to
produce the function of t

p(t) =                             1+e  1           (A.22)

                                        -(+ t )

From either equation (A.21) or (A.22), it is possible to see that if  > 0, the
probability of at least one O-ring incident is increasing in t (low-temperature
launches are best). On the other hand, if  < 0, p is decreasing in t (high-
temperature launches are best).

     The joint probability function for Y1, Y2, . . . , Y23 employed by Dalal,
Fowlkes, and Hoadley was then

                                   1 - p(ti ) 1-yi  for each yi = 0 or 1
                          23                        otherwise
                          p(t )yi
f (y1, y2, . . . , y23) = i=1 i
                         

                           0
770 Appendix A More on Probability and Model Fitting

Example 19            
 (continued )  -.16

               -.20                                                Point of maximum

                                                                         L(, )

               -.24 L(, ) = -10.2

                                                                   L(, ) = -12.2

               -.28     L(, ) = -14.2

                        14.0                          15.0         16.0         17.0 

               Figure A.14 Contour plot of the Dalal, Fowlkes, and
               Hoadley log likelihood function

               The log likelihood function is then (using equations (A.21) and (A.22))

                                   23                   p(ti )          23
                                                      1 - p(ti )
               L(, ) = yi ln                                       + ln 1 - p(ti )

                                  i =1                                 i =1

                        23                                  23             e-(+ti )
                                                                         1 + e-(+ti )
                        = yi ( + ti ) + ln

                        i =1                                i =1

                        = 7 + (70 + 57 + 63 + 70 + 53 + 75 + 58)

                        + ln                            e-(+66)    + ln    e-(+70)
                                                      1 + e-(+66)        1 + e-(+70)

                        + · · · + ln                    e-(+58)                         (A.23)
                                                      1 + e-(+58)

               where the sum abbreviated in equation (A.23) is over all 23 ti 's. Figure A.14 is a
               contour plot of L(, ) given in equation (A.23).

                    It is interesting (and sadly, of great engineering importance) that the region
               of (, ) pairs making the data of Table A.2 most likely is in the  < 0 part of
               the (, )-plane--that is, where p(t) is decreasing in t (i.e., increases as t falls).
               (Remember that the tragic Challenger launch was made at t = 31.)

                    The binomial and Poisson examples of discrete-data likelihoods given thus
               far have arisen from situations that are most naturally thought of as intrinsically
               discrete. However, the details of how engineering data are collected sometimes
               lead to the production of essentially discrete data from intrinsically continuous
A.5 Maximum Likelihood Fitting of Probability Models and Related Inference Methods 771

              variables. For example, consider a life test of some electrical components, where
              a technician begins a test by connecting 50 devices to a power source, goes away,
              and then returns every ten hours to note which devices are still functioning. The
              details of data collection produce only discrete data (which ten-hour period produces
              failure) from the intrinsically continuous life lengths of the 50 devices. The next
              example shows how the likelihood idea might be used in another situation where
              the underlying phenomenon is continuous.

Example 20  A Log Likelihood Function for a Crudely Gauged Normally
            Distributed Dimension of Five Machined Metal Parts

            In many contexts where industrial process monitoring involves relatively stable
            processes and relatively crude gauging, intrinsically continuous product char-
            acteristics are measured and recorded as essentially discrete data. For example,
            Table A.3 gives values (in units of .0001 in. over nominal) of a critical dimension
            measured on a sample of n = 5 consecutive metal parts produced by a CNC
            lathe.

                 It might make sense to model underlying values of this critical dimension as
            normal, with some (unknown) mean µ and some (unknown) standard deviation
             , but nonetheless to want to explicitly recognize the discreteness of the recorded
            data. One way of doing so in this context is to think of the observed values
            as arising (after coding) from rounding normally distributed dimensions to the
            nearest integer. For a single metal part, this would mean that for any integer y,

            P[the value recorded is y] = P[the actual dimension is between
                                               y - .5 and y + .5]

               = y + .5 - µ - y - .5 - µ                                    (A.24)
                                           

            Table A.3
            Measurements of a Critical
            Dimension on Five Metal Parts
            Produced on a CNC Lathe

            Part Measured Dimension, y

            1  4

            2  3

            3  3

            4  2

            5  3
772 Appendix A More on Probability and Model Fitting

Example 20                                                      Point of maximum
 (continued )                2.0                                      L(µ, )

                             1.0

                                    2.0                    3.0      4.0     µ

                             Figure A.15 Contour plot of the "rounded
                             normal data" log likelihood for the data of
                             Table A.3

               So treating n = 5 consecutive recorded dimensions as independent, equation
               (A.24) leads to the joint probability function

                                                   5       yi + .5 - µ -    yi - .5 - µ
                                                                                 
               f (y1, y2, . . . , y5) =

                                                  i =1

               and log likelihood function for the data in Table A.3

                                  2 + .5 - µ                    2 - .5 - µ        
                                                                                  
               L(µ,  ) = ln                             -                         
                                                                                  
                                                                                  
                                    3 + .5 - µ - 3 - .5 - µ
               + 3 ln                                                                    (A.25)

               + ln 4 + .5 - µ - 4 - .5 - µ 
                                                                  

               Figure A.15 is a contour plot of L(µ,  ).

                    Consideration of a likelihood function f ( y) or its log version L( ) can be
               thought of as a way of assessing how compatible various probability models indexed
               by are with the data in hand, Y = y. Different parameter vectors having the
               same value of L( ) can be viewed as equally compatible with data in hand. A
               value of maximizing L( ) might then be considered to be as compatible with the
               observed data as is possible. This value is often termed the maximum likelihood
               estimate of the parameter vector . Finding maximum likelihood estimates of
               parameters is a very common method of fitting probability models to data. In
               simple situations, calculus can sometimes be employed to see how to maximize
               L( ), but in most nonstandard situations, numerical or graphical methods are
               required.
A.5 Maximum Likelihood Fitting of Probability Models and Related Inference Methods 773

Example 17     In the pelletizing example, simple investigation of Figure A.13 shows
 (continued )
                                                          p^ = 66
                                                                100

               to maximize L( p) given in display (A.19) and thus to be the maximum likelihood
               estimate of p. The reader is encouraged to verify that by differentiating L( p)
               with respect to p, setting the result equal to 0, and solving for p, this maximizing
               value can also be found analytically.

Example 18     Differentiating the log likelihood (A.20) with respect to , one obtains
 (continued )

               d n 1n
               d L() = - ki +  xi
               i =1               i =1

               Setting this derivative equal to 0 and solving for  produces

                        i=1 n xi
                = n = u^

                        i=1 ki

               which is the total number of defects observed divided by the total number of units

               inspected. Since the second derivative of L() is easily seen to be negative for all
               , u^ is the unique maximizer of L()--that is, the maximum likelihood estimate
               of .

Example 19     Careful examination of contour plots like Figure A.14, or use of a numerical
 (continued )  search method for the (, ) pair maximizing L(, ), produces maximum like-
               lihood estimates

                                                       ^ = 15.043
                                                       ^ = -.2322

               based on the pre-Challenger data. Figure A.16 is a plot of p(t) given in display
               (A.22) for these values of  and . Notice the disconcerting fact that the cor-
               responding estimate of p(31) (the probability of at least one O-ring failure in a
               31 launch) exceeds .99. (t = 31 is clearly a huge extrapolation away from any t
               values in Table A.2, but even so, this kind of analysis conducted before the Chal-
               lenger launch could well have helped cast legitimate doubt on the advisability of
               a low-temperature launch.)
774 Appendix A More on Probability and Model Fitting

Example 19           p(t)
 (continued )  1.0

               .8

               .6

               .4

               .2

                     30 40 50 60 70 80 t

               Figure A.16 Plot of fitted probability of at
               least one O-ring failure as a function of shuttle
               launch temperature

Example 20     Examination of the contour plot in Figure A.15 shows maximum likelihood
 (continued )  estimates of µ and  based on the rounded normal data model and the data in
               Table A.3 to be approximately

                                                          µ^ = 3.0
                                                          ^ = .55

               It is worth noting that for these data, s = .71, which is noticeably larger than
               ^ . This illustrates a well-established piece of statistical folklore. It is fairly well
               known that to ignore rounding of intrinsically continuous data will typically
               have the effect of inappropriately inflating the apparent spread of the underlying
               distribution.

A.5.2          Likelihood Functions for Continuous and Mixed Data
               and Maximum Likelihood Model Fitting

               The likelihood function ideas discussed thus far depend on treating the probability
               of discrete data in hand, Y = y, as a function of . When analyzing data using con-
               tinuous distributions, a slight logical snag is therefore encountered: If a continuous
               model is employed, the probability associated with observing any particular exact
               realization y is always 0, for every .
A.5 Maximum Likelihood Fitting of Probability Models and Related Inference Methods 775

                            To understand how to employ likelihood methods in continuous models, it is
                       then useful to consider the probability of observing a value of Y "near" y as a
                       function of . That is, suppose that

                       f ( y)

                       is a joint probability density for Y depending on an unknown parameter vector .
                       Then in rough terms, if is a small positive number and y = (y1, y2, . . . , yn),

                       P[each Yi is within 2 of yi ]  f ( y) n             (A.26)

                       But in expression (A.26), n doesn't depend on --that is, the approximate prob-
                       ability is proportional to the function of , f ( y). It is therefore plausible to use
                       the joint density with data plugged in,

 A continuous data     f ( y)                                              (A.27)
likelihood function

                       as a likelihood function and to use its logarithm,

A continuous data      L( ) = ln( f ( y))                                  (A.28)
      log likelihood
             function

                       as a log likelihood for data modeled as jointly continuous. (Formulas (A.27)
                       and (A.28) are formally identical to formulas (A.17) and (A.18), but they in-
                       volve a different type of data.) Contemplation of formula (A.27) or (A.28) can
                       be thought of as a way of assessing the consonance of different parameter vec-
                       tors, , with continuous data, y. And as for the discrete case, a vector max-
                       imizing L( ) is often termed a maximum likelihood estimate of the parameter
                       vector.

Example 21             Maximum Likelihood Estimation Based on iid Exponential Data

                       The exponential distribution is a popular model for life-length variables. The
                       following are hypothetical life lengths (in hours) for n = 4 nominally identical
                       electrical components, which will be assumed to have been a priori adequately
                       described as iid exponential variables with mean ,

                       75.4, 39.4, 3.7, 4.5                                (A.29)
776 Appendix A More on Probability and Model Fitting

Example 21              L()
 (continued )  -17.7

               -17.8

               -17.9

               -18.0

               -18.1

               -18.2 20                               25 30         35 40     45 
                                                                     = 30.75

               Figure A.17 Plot of a log likelihood based on four
               iid exponential observations

                    If Y1, Y2, Y3, and Y4 are iid exponential variables with means , an appropriate
               joint probability density is

                             
                             4
                                                         1
                                                            e-yi /  for each yi > 0
                                                                    otherwise
                      f ( y) = i=1 
                             
                                                      0

               So with the data of display (A.29) in hand, the log likelihood function becomes

               L() = -4 ln() - 1 (75.4 + 39.4 + 3.7 + 4.5)                           (A.30)
                                       

               It is easy to verify (using calculus and/or simply looking at the plot of L() in
               Figure A.17) that L() is maximized for

                                     ^ = 30.75 = 75.4 + 39.4 + 3.7 + 4.5 = y¯
                                                                    4

               This fact is a particular instance of the general result that the maximum likelihood
               estimate of an exponential mean is the sample average of the observations.

   Maximum          Example 21 is fairly simple, in that only one parameter is involved and calculus
   likelihood  can be used to find an explicit formula for the maximum likelihood estimator. The
 and normal    reader might be interested in working through the somewhat more complicated
observations   (two-parameter) situation involving n iid normal random variables with means µ
               and standard deviations  . Two-variable calculus can be used to show that maximum
A.5 Maximum Likelihood Fitting of Probability Models and Related Inference Methods 777

              likelihood estimates of the parameters based on observations x1, x2, . . . , xn turn out
              to be, respectively,

                                                       µ^ = x¯
                                                       ^ = n - 1 s

                                                                   n

                   The next example concerns an important continuous situation where no explicit
              formulas for maximum likelihood estimates seem to exist.

Example 22  Maximum Likelihood Estimation Based on iid Weibull
            Steel Specimen Failure Times

            The data in Table A.4 are n = 10 ordered failure times for hardened steel speci-
            mens that were subjected to a particular rolling fatigue test. These data appeared
            originally in the paper of J. I. McCool, "Confidence Limits for Weibull Regres-
            sion With Censored Data" (IEEE Transactions on Reliability, 1980). The Weibull
            probability plot of these data in Figure A.18 suggests the appropriateness of fit-
            ting a Weibull model to them (and indicates that  near 2 and  near .25 may be
            appropriate parameters for such a fitted model).

                 Notice that the joint density function of n = 10 iid Weibull random variables
            Y1, Y2, . . . , Y10 with parameters  and  is

                                      for each yi > 0
                      10  -1 -(yi /)  otherwise
            f ( y) = i=1  yi e
                     
                     

                        0

            So using the data of Table A.4, the log likelihood

            L(, ) = 10 ln() - 10 ln() + ( - 1)(ln(.073) + ln(.098) + · · · + ln(.456))
                          -  1 ((.073) + (.098) + · · · + (.456) )
                             

                      = 10 ln() - 10 ln() - 16.267( - 1) -  1 ((.073) + (.098)
                                                                            

                          + · · · + (.456) )

            is indicated. Figure A.19 shows a contour plot of L(, ) and indicates that
            maximum likelihood estimates of  and  are indeed in the vicinity of ^ = 2.0
            and ^ = .26.
778 Appendix A More on Probability and Model Fitting

Example 22     Table A.4
 (continued )  Ten Ordered Failure Times of Steel Specimens

                .073, .098, .117, .135, .175, .262, .270, .350, .386, .456

                     ln(-ln(1-p))
               1.0

               0.0

               -1.0                                                  Slope  2

               -2.0 Line eye-fit to data

                                                      About -1.4

               -3.0                                   (e -1.4  .25)

                     -3.0                             -2.0           -1.0

                                                      ln(life)

               Figure A.18 Weibull probability plot of McCool's steel
               specimen failure times

                         
                     .5

                     .4 L(, ) = 4.2
                                                            L(, ) = 5.2

                     .3

                                                          L(, ) = 6.2
                     .2 L(, ) = 7.2

                     .1

                           1.0 2.0 3.0 4.0                                     

                     Figure A.19 Contour plot of a Weibull
                     log likelihood for McCool's steel specimen
                     failure times
A.5 Maximum Likelihood Fitting of Probability Models and Related Inference Methods 779

                          Analytical attempts to locate the maximum likelihood estimates for this kind
                     of iid Weibull data situation are only partially fruitful. Setting partial derivatives
                     of L(, ) equal to 0, followed by some algebra, does lead to the two equations

                     =                                  yi ln(yi )               -1
                                                          -
                                                                    ln(yi )

                                                        yi          n

                     =                                   yi 1/
                                                        n

                     which maximum likelihood estimates must satisfy, but these must be solved
                     numerically.

                          Discrete and continuous likelihood methods have thus far been discussed sep-
                     arately. However, particularly in life-data analysis contexts, statistical engineering
                     studies occasionally yield data that are mixed--in the sense that some parts are
                     discrete, while other parts are continuous. If it is sensible to think of the two parts
                     as independent, a combination of things already said here can lead to an appropri-
                     ate likelihood function and then, for example, to maximum likelihood parameter
                     estimates.

                          That is, suppose that one has available discrete data, Y1 = y1, and continuous
                     data, Y2 = y2, which can be thought of as independently generated--Y1 from a
                     discrete joint distribution with joint probability function

                                                        f (1)( y1)

                     and Y2 from a continuous joint distribution with joint probability density

                                                        f (2)( y2)

                     Then a sensible likelihood function becomes

       A mixed-data     f (1)( y1) · f (2)( y2)                                                  (A.31)
likelihood function

                     with corresponding log likelihood

A mixed-data         L( ) = ln f (1)( y1) + ln f (2)( y2)                                        (A.32)
log likelihood

       function

                     Armed with equation (A.31) or (A.32), assessments of the compatibility of different
                     parameter vectors with the data in hand and maximum likelihood model fitting
                     can proceed just as for purely discrete or purely continuous cases.
780 Appendix A More on Probability and Model Fitting

Example 23  Maximum Likelihood Estimation of a Mean Insulating
            Fluid Breakdown Time Using Censored Data

            Table 2.1 of Nelson's Applied Life Data Analysis gives some data on times to
            breakdown (in seconds) of an insulating fluid at several different voltages. The
            results of n = 12 tests made at 30 kV are repeated below in Table A.5. The last
            two entries in Table A.5 mean that two tests were terminated at (respectively)
            29,200 seconds and 86,100 seconds without failures having been observed. In
            common statistical jargon, these last two data values are censored (at the times
            29,200 and 86,100, respectively).

                 Nelson remarks in his book that exponential distributions are often used
            to model life length for such fluids. Therefore, consider fitting an exponential
            distribution with mean  to the data of Table A.5. Notice that the first ten pieces
            of data in Table A.5 are continuous "exact" failure times, while the last two are
            essentially discrete pieces of information. Considering first the discrete part of
            the overall likelihood, the probability that two independent exponential variables
            exceed 29,200 and 86,100, respectively, is

            f(1)( y1) = e-29,200/ · e-86,100/

            Then considering the continuous part of the likelihood, the joint density of ten
            independent exponential variables with mean  is

                                                      for each yi > 0
                          1 e- yi /                   otherwise
            f (2)  ( y2) =  10
                         0

            Putting these two pieces together via equation (A.32), the log likelihood function
            appropriate here is

            L() = -10 ln() - 1 (50 + 134 + 187 + · · · +               (A.33)
                                      

                      + 15,800 + 29,200 + 86,100)
                  = -10 ln  - 1 (144,673)

                                    

            Table A.5
            12 Insulating Fluid Breakdown Times

             50, 134, 187, 882, 1450, 1470, 2290, 2930, 4180, 15800, > 29200, > 86100
A.5 Maximum Likelihood Fitting of Probability Models and Related Inference Methods 781

                  This function of  is easily seen via elementary calculus to be maximized at
                                                ^ = 144,673 = 14,467.3 sec
                                                          10

                  which has the intuitively appealing interpretation of the ratio of the total time on
                  test to the number of failures observed during testing.

A.5.3                  Likelihood-Based Large-Sample Inference Methods

                       One of the appealing things about the likelihood function idea is that in many
                       situations, it is possible to base large-sample significance testing and confidence
                       region methods on the likelihood function. Intuitively, it would seem that those
                       parameter vectors "most compatible" with the data in hand ought to form a
                       sensible confidence set for . And in significance-testing terms, if a hypothesized
                       value of (say, 0) has a corresponding value of the likelihood function far smaller
                       than the maximum possible, that circumstance ought to produce a small p-value--
                       that is, strong evidence against H0 : = 0.

                            To make this thinking precise, let

 The maximum of        L = max L( )
the log-likelihood
                       that is, L is the largest possible value of the log likelihood. (If ^ is a maximum
             function  likelihood estimate of , then L = L( ^ ).) An intuitively appealing way to make

       A likelihood-   a confidence set for the parameter vector  is to use the set of all 's with log
based confidence       likelihood not too far below L,

            set for    | L( ) > L - c                                                    (A.34)

                       for an appropriate number c. And a plausible way of deriving a p-value for testing

                       H0 : = 0                                                          (A.35)

                       is by trying to identify a sensible probability distribution for

                       L - L( 0)                                                         (A.36)

                       when H0 holds, and using the upper-tail probability beyond an observed value of
                       variable (A.36) as a p-value.

                            The practical gaps in this thinking are two: how to choose c in display (A.34)
                       to get a desired confidence level and what kind of distribution to use to describe
                       variable (A.36) under hypothesis (A.35). There are no general exact answers to these
782 Appendix A More on Probability and Model Fitting

                             questions, but statistical theory does provide at least some indication of approximate
                             answers that are often adequate for practical purposes when large samples are
                             involved. That is, statistical theory suggests that in many large-sample situations, if

                                is of dimension k, choosing

  Constant producing                                  c = 12 U                           (A.37)
          (large sample)
         approximate         for U the  quantile of the k2 distribution, produces a confidence set (A.34) of
                             confidence level roughly  . And similar reasoning suggests that in many large-
  level confidence for
         | L( ) > L - c      sample situations, if is of dimension k, the hypothesis (A.35) can be tested using

           A test statistic  the test statistic
         for H0 : = 0
                                                      2 L - L( 0)                        (A.38)
                    with an
      approximately k2       and a k2 approximate reference distribution, where large values of the test statistic
reference distribution       (A.38) count as evidence against H0.

Example 23                   Consider the problem of setting confidence limits on the mean time till break-
 (continued )                down of Nelson's insulating fluid tested at 30 kV. In this problem, is k = 1-

                             dimensional. So, for example, making use of the facts that the .9 quantile of
                             the 12 distribution is 2.706 and that the maximum likelihood estimate of  is
                             14,467.3, displays (A.33), (A.34), and (A.37) suggest that those  with

                                       L() > -10 ln(14,467.3) - 1 (144,673) - 1 (2.706)
                                                      14,467.3     2

                             that is,

                                                      -10 ln() - 1 (144,673) > -107.15
                                                                       

                             form an approximate 90% confidence set for . Figure A.20 shows a plot of the
                             log likelihood (A.33) cut at the level -107.15 and the corresponding interval of
                             's. Numerical solution of the equation

                                                      -10 ln() - 1 (144,673) = -107.15
                                                                       

                             shows the interval for mean time till breakdown to extend from 8,963 sec to
                             25,572 sec.)
A.5 Maximum Likelihood Fitting of Probability Models and Related Inference Methods 783

                                   L( )  Approximate 90%
                             L*          confidence interval
                        -106.0
                                                 for 
                        -106.5

                        -107.0
                       -107.15

                        -107.5

                        -108.0

                                         10,000  20,000       30,000  40,000 

                       Figure A.20 Plot of the log likelihood for Nelson's insulating fluid
                       breakdown time data and approximate confidence limits for 

                            The n = 12 pieces of data in Table A.5 do not constitute an especially large
                       sample, so the 90% approximate confidence level associated with the interval
                       (8,963, 25,572) should be treated as very approximate. But even so, this interval
                       does give one some feeling about the precision with which  is known based on
                       the data of Table A.5. There is clearly substantial uncertainty associated with the
                       estimate ^ = 14,467.3.

Cautions concerning         It is not a trivial matter to verify that the k2 approximations suggested here
     the large-sample  are adequate for a particular nonstandard probability model. In engineering sit-
     likelihood-based
                       uations where fairly exact confidence levels and/or p-values are critical, readers
  inference methods
                       should seek genuinely expert statistical advice before placing too much faith in the
                       k2 approximations. But for purposes of engineering problem solving requiring a
                       rough, working quantification of uncertainty associated with parameter estimates,
                       the use of the k2 approximation is certainly preferable to operating without any such
                       quantification.

                            The insulating fluid example involved only a single parameter. As an example of
                       a k = 2-parameter application, consider once again the space shuttle O-ring failure

                       example.

Example 19             Again use the log likelihood (A.23) and the fact that maximum likelihood esti-
 (continued )          mates of  and  in equation (A.21) or (A.22) are ^ = 15.043 and ^ = -.2322.

                       These produce corresponding log likelihood -10.158. This, together with the
784 Appendix A More on Probability and Model Fitting

Example 19                      
 (continued )               0
                         -.15
                                                      Approximate 90% confidence
                                                      region for (, )

                         -.30

                         -.45

                         -.60      8 16 24 32 40 
                                0

                         Figure A.21 Likelihood-based approximate
                         confidence region for the parameters of the O-ring
                         failure model

               fact that the .9 quantile of the 22 distribution is 4.605, gives one (from displays
               (A.34) and (A.37)), that the set of (, ) pairs with

                                   L(, ) > -10.158 - 1 (4.605)
                                                               2

               that is,

                                                      L(, ) > -12.4605

               constitutes an approximate 90% confidence region for (, ). This set of possible
               parameter vectors is shown in the plot in Figure A.21. Notice that one message
               conveyed by the contour plot is that  is pretty clearly negative. Low-temperature
               launches are more prone to O-ring failure than moderate- to high-temperature
               launches.

                    The approximate inference methods represented in displays (A.34) through
               (A.38) concern the entire parameter vector in cases where it is multidimensional.
               It is reasonably common, however, to desire inferences only for particular parameters
               individually. (For example, in the case of the O-rings, it is the parameter  that
               determines whether p(t) is increasing, constant, or decreasing in t, and for many
               purposes  is of primary interest.) It is thus worth mentioning that the likelihood
               ideas discussed here can be adapted to provide inference methods for a part of a
               parameter vector of individual interest. An exposition of these adaptations will
               not be attempted here, but be aware of their existence. For details, refer to more
               complete expositions of likelihood methods (such as that in Meeker and Escobar's
               Statistical Methods for Reliability Data text).
APPENDIX

 Bq q q q q q q q q q q q q

             Tables

Table B.1
Random Digits

12159  66144   05091  13446  45653  13684  66024  91410  51351  22772
30156  90519   95785  47544  66735  35754  11088  67310  19720  08379
59069  01722   53338  41942  65118  71236  01932  70343  25812  62275
54107  58081   82470  59407  13475  95872  16268  78436  39251  64247
99681  81295   06315  28212  45029  57701  96327  85436  33614  29070

27252  37875   53679  01889  35714  63534  63791  76342  47717  73684
93259  74585   11863  78985  03881  46567  93696  93521  54970  37601
84068  43759   75814  32261  12728  09636  22336  75629  01017  45503
68582  97054   28251  63787  57285  18854  35006  16343  51867  67979
60646  11298   19680  10087  66391  70853  24423  73007  74958  29020

97437  52922   80739  59178  50628  61017  51652  40915  94696  67843
58009  20681   98823  50979  01237  70152  13711  73916  87902  84759
77211  70110   93803  60135  22881  13423  30999  07104  27400  25414
54256  84591   65302  99257  92970  28924  36632  54044  91798  78018
36493  69330   94069  39544  14050  03476  25804  49350  92525  87941

87569  22661   55970  52623  35419  76660  42394  63210  62626  00581
22896  62237   39635  63725  10463  87944  92075  90914  30599  35671
02697  33230   64527  97210  41359  79399  13941  88378  68503  33609
20080  15652   37216  00679  02088  34138  13953  68939  05630  27653
20550  95151   60557  57449  77115  87372  02574  07851  22128  39189

72771  11672   67492  42904  64647  94354  45994  42538  54885  15983
38472  43379   76295  69406  96510  16529  83500  28590  49787  29822
24511  56510   72654  13277  45031  42235  96502  25567  23653  36707
01054  06674   58283  82831  97048  42983  06471  12350  49990  04809
94437  94907   95274  26487  60496  78222  43032  04276  70800  17378

                                                         (continued )
                                                                              785
786 Appendix B Tables

Table B.1
Random Digits (continued )

97842  69095           25982  03484  25173  05982  14624  31653  17170  92785
53047  13486           69712  33567  82313  87631  03197  02438  12374  40329
40770  47013           63306  48154  80970  87976  04939  21233  20572  31013
52733  66251           69661  58387  72096  21355  51659  19003  75556  33095
41749  46502           18378  83141  63920  85516  75743  66317  45428  45940

10271  85184           46468  38860  24039  80949  51211  35411  40470  16070
98791  48848           68129  51024  53044  55039  71290  26484  70682  56255
30196  09295           47685  56768  29285  06272  98789  47188  35063  24158
99373  64343           92433  06388  65713  35386  43370  19254  55014  98621
27768  27552           42156  23239  46823  91077  06306  17756  84459  92513

67791  35910           56921  51976  78475  15336  92544  82601  17996  72268
64018  44004           08136  56129  77024  82650  18163  29158  33935  94262
79715  33859           10835  94936  02857  87486  70613  41909  80667  52176
20190  40737           82688  07099  65255  52767  65930  45861  32575  93731
82421  01208           49762  66360  00231  87540  88302  62686  38456  25872

Reprinted from A Million Random Digits with 100,000 Normal Deviates, RAND (New York: The Free Press, 1955).
Copyright c 1955 and 1983 by RAND. Used by permission.
                                                                           Appendix B Tables 787

Table B.2
Control Chart Constants

m d2  d3                 c4  A2          A3     B3     B4     B5     B6     D1     D2     D3     D4

2 1.128 0.853 0.7979 1.880              2.659  0.030  3.267  0.029  2.606  0.205  3.686  0.076  3.267
3 1.693 0.888 0.8862 1.023              1.954  0.118  2.568  0.113  2.276  0.388  4.358  0.136  2.575
4 2.059 0.880 0.9213 0.729              1.628  0.185  2.266  0.179  2.088  0.547  4.698  0.184  2.282
5 2.326 0.864 0.9400 0.577              1.427  0.239  2.089  0.232  1.964  0.686  4.918  0.223  2.114
                                               0.284         0.276         0.811         0.256
 6 2.534 0.848 0.9515 0.483             1.287  0.321  1.970  0.313  1.874  0.923  5.079  0.283  2.004
 7 2.704 0.833 0.9594 0.419             1.182  0.354  1.882  0.346  1.806  1.025  5.204  0.307  1.924
 8 2.847 0.820 0.9650 0.373             1.099  0.382  1.815  0.374  1.751  1.118  5.307  0.328  1.864
 9 2.970 0.808 0.9693 0.337             1.032  0.406  1.761  0.399  1.707  1.203  5.394  0.347  1.816
10 3.078 0.797 0.9727 0.308             0.975  0.428  1.716  0.421  1.669  1.549  5.469  0.415  1.777
                                               0.510         0.504         1.806         0.459
11 3.173 0.787 0.9754 0.285             0.927  0.565  1.679  0.559  1.637         5.535         1.744
12 3.258 0.778 0.9776 0.266             0.886         1.646         1.610         5.594         1.717
13 3.336 0.770 0.9794 0.249             0.850         1.618         1.585         5.647         1.693
14 3.407 0.763 0.9810 0.235             0.817         1.594         1.563         5.696         1.672
15 3.472 0.756 0.9823 0.223             0.789         1.572         1.544         5.740         1.653

20 3.735 0.729 0.9869 0.180             0.680         1.490         1.470         5.921         1.585

25 3.931 0.708 0.9896 0.153             0.606         1.435         1.420         6.056         1.541

This table was computed using Mathcad.
     Table B.3
     Standard Normal Cumulative Probabilities

                                               z1     t2
                                (z) =  exp - dt
                                               - 2    2

      z     .00    .01    .02    .03            .04    .05    .06    .07    .08    .09

     -3.4  .0003  .0003  .0003  .0003          .0003  .0003  .0003  .0003  .0003  .0002
     -3.3  .0005  .0005  .0005  .0004          .0004  .0004  .0004  .0004  .0004  .0003
     -3.2  .0007  .0007  .0006  .0006          .0006  .0006  .0006  .0005  .0005  .0005
     -3.1  .0010  .0009  .0009  .0009          .0008  .0008  .0008  .0008  .0007  .0007
     -3.0  .0013  .0013  .0013  .0012          .0012  .0011  .0011  .0011  .0010  .0010

     -2.9  .0019  .0018  .0018  .0017          .0016  .0016  .0015  .0015  .0014  .0014
     -2.8  .0026  .0025  .0024  .0023          .0023  .0022  .0021  .0021  .0020  .0019
     -2.7  .0035  .0034  .0033  .0032          .0031  .0030  .0029  .0028  .0027  .0026
     -2.6  .0047  .0045  .0044  .0043          .0041  .0040  .0039  .0038  .0037  .0036
     -2.5  .0062  .0060  .0059  .0057          .0055  .0054  .0052  .0051  .0049  .0048

     -2.4  .0082  .0080  .0078  .0075          .0073  .0071  .0069  .0068  .0066  .0064
     -2.3  .0107  .0104  .0102  .0099          .0096  .0094  .0091  .0089  .0087  .0084
     -2.2  .0139  .0136  .0132  .0129          .0125  .0122  .0119  .0116  .0113  .0110
     -2.1  .0179  .0174  .0170  .0166          .0162  .0158  .0154  .0150  .0146  .0143
     -2.0  .0228  .0222  .0217  .0212          .0207  .0202  .0197  .0192  .0188  .0183

     -1.9  .0287  .0281  .0274  .0268          .0262  .0256  .0250  .0244  .0239  .0233
     -1.8  .0359  .0351  .0344  .0336          .0329  .0322  .0314  .0307  .0301  .0294
     -1.7  .0446  .0436  .0427  .0418          .0409  .0401  .0392  .0384  .0375  .0367
     -1.6  .0548  .0537  .0526  .0516          .0505  .0495  .0485  .0475  .0465  .0455
     -1.5  .0668  .0655  .0643  .0630          .0618  .0606  .0594  .0582  .0571  .0559

     -1.4  .0808  .0793  .0778  .0764          .0749  .0735  .0721  .0708  .0694  .0681
     -1.3  .0968  .0951  .0934  .0918          .0901  .0885  .0869  .0853  .0838  .0823
     -1.2  .1151  .1131  .1112  .1093          .1075  .1056  .1038  .1020  .1003  .0985
     -1.1  .1357  .1335  .1314  .1292          .1271  .1251  .1230  .1210  .1190  .1170
     -1.0  .1587  .1562  .1539  .1515          .1492  .1469  .1446  .1423  .1401  .1379

     -0.9  .1841  .1814  .1788  .1762          .1736  .1711  .1685  .1660  .1635  .1611
     -0.8  .2119  .2090  .2061  .2033          .2005  .1977  .1949  .1922  .1894  .1867
     -0.7  .2420  .2389  .2358  .2327          .2297  .2266  .2236  .2206  .2177  .2148
     -0.6  .2743  .2709  .2676  .2643          .2611  .2578  .2546  .2514  .2483  .2451
     -0.5  .3085  .3050  .3015  .2981          .2946  .2912  .2877  .2843  .2810  .2776

     -0.4  .3446  .3409  .3372  .3336          .3300  .3264  .3228  .3192  .3156  .3121
     -0.3  .3821  .3783  .3745  .3707          .3669  .3632  .3594  .3557  .3520  .3483
     -0.2  .4207  .4168  .4129  .4090          .4052  .4013  .3974  .3936  .3897  .3859
     -0.1  .4602  .4562  .4522  .4483          .4443  .4404  .4364  .4325  .4286  .4247
     -0.0  .5000  .4960  .4920  .4880          .4840  .4801  .4761  .4721  .4681  .4641

788
Table B.3                                               .06    .07    .08    .09
Standard Normal Cumulative Probabilities (continued)
                                                       .5239  .5279  .5319  .5359
 z .00 .01 .02 .03 .04 .05                             .5636  .5675  .5714  .5753
                                                       .6026  .6064  .6103  .6141
0.0 .5000 .5040 .5080 .5120 .5160 .5199                .6406  .6443  .6480  .6517
0.1 .5398 .5438 .5478 .5517 .5557 .5596                .6772  .6808  .6844  .6879
0.2 .5793 .5832 .5871 .5910 .5948 .5987
0.3 .6179 .6217 .6255 .6293 .6331 .6368                .7123  .7157  .7190  .7224
0.4 .6554 .6591 .6628 .6664 .6700 .6736                .7454  .7486  .7517  .7549
                                                       .7764  .7794  .7823  .7852
0.5 .6915 .6950 .6985 .7019 .7054 .7088                .8051  .8078  .8106  .8133
0.6 .7257 .7291 .7324 .7357 .7389 .7422                .8315  .8340  .8365  .8389
0.7 .7580 .7611 .7642 .7673 .7704 .7734
0.8 .7881 .7910 .7939 .7967 .7995 .8023                .8554  .8577  .8599  .8621
0.9 .8159 .8186 .8212 .8238 .8264 .8289                .8770  .8790  .8810  .8830
                                                       .8962  .8980  .8997  .9015
1.0 .8413 .8438 .8461 .8485 .8508 .8531                .9131  .9147  .9162  .9177
1.1 .8643 .8665 .8686 .8708 .8729 .8749                .9279  .9292  .9306  .9319
1.2 .8849 .8869 .8888 .8907 .8925 .8944
1.3 .9032 .9049 .9066 .9082 .9099 .9115
1.4 .9192 .9207 .9222 .9236 .9251 .9265

1.5 .9332 .9345 .9357 .9370              .9382  .9394  .9406  .9418  .9429  .9441
1.6 .9452 .9463 .9474 .9484              .9495  .9505  .9515  .9525  .9535  .9545
1.7 .9554 .9564 .9573 .9582              .9591  .9599  .9608  .9616  .9625  .9633
1.8 .9641 .9649 .9656 .9664              .9671  .9678  .9686  .9693  .9699  .9706
1.9 .9713 .9719 .9726 .9732              .9738  .9744  .9750  .9756  .9761  .9767

2.0 .9773 .9778 .9783 .9788              .9793  .9798  .9803  .9808  .9812  .9817
2.1 .9821 .9826 .9830 .9834              .9838  .9842  .9846  .9850  .9854  .9857
2.2 .9861 .9864 .9868 .9871              .9875  .9878  .9881  .9884  .9887  .9890
2.3 .9893 .9896 .9898 .9901              .9904  .9906  .9909  .9911  .9913  .9916
2.4 .9918 .9920 .9922 .9925              .9927  .9929  .9931  .9932  .9934  .9936

2.5 .9938 .9940 .9941 .9943              .9945  .9946  .9948  .9949  .9951  .9952
2.6 .9953 .9955 .9956 .9957              .9959  .9960  .9961  .9962  .9963  .9964
2.7 .9965 .9966 .9967 .9968              .9969  .9970  .9971  .9972  .9973  .9974
2.8 .9974 .9975 .9976 .9977              .9977  .9978  .9979  .9979  .9980  .9981
2.9 .9981 .9982 .9983 .9983              .9984  .9984  .9985  .9985  .9986  .9986

3.0 .9987 .9987 .9987 .9988              .9988  .9989  .9989  .9989  .9990  .9990
3.1 .9990 .9991 .9991 .9991              .9992  .9992  .9992  .9992  .9993  .9993
3.2 .9993 .9993 .9994 .9994              .9994  .9994  .9994  .9995  .9995  .9995
3.3 .9995 .9995 .9996 .9996              .9996  .9996  .9996  .9996  .9996  .9997
3.4 .9997 .9997 .9997 .9997              .9997  .9997  .9997  .9997  .9997  .9998

This table was generated using MINITAB.

                                                                                   789
     Table B.4
     t Distribution Quantiles

      Q(.9) Q(.95)             Q(.975)        Q(.99)  Q(.995)   Q(.999)   Q(.9995)

     1 3.078    6.314           12.706        31.821   63.657   318.317    636.607
     2 1.886    2.920             4.303        6.965     9.925   22.327      31.598
     3 1.638    2.353             3.182        4.541     5.841   10.215      12.924
     4 1.533    2.132             2.776        3.747     4.604     7.173      8.610
     5 1.476    2.015             2.571        3.365     4.032     5.893      6.869

      6 1.440   1.943          2.447          3.143   3.707     5.208     5.959
      7 1.415   1.895          2.365          2.998   3.499     4.785     5.408
      8 1.397   1.860          2.306          2.896   3.355     4.501     5.041
      9 1.383   1.833          2.262          2.821   3.250     4.297     4.781
     10 1.372   1.812          2.228          2.764   3.169     4.144     4.587

     11 1.363   1.796          2.201          2.718   3.106     4.025     4.437
     12 1.356   1.782          2.179          2.681   3.055     3.930     4.318
     13 1.350   1.771          2.160          2.650   3.012     3.852     4.221
     14 1.345   1.761          2.145          2.624   2.977     3.787     4.140
     15 1.341   1.753          2.131          2.602   2.947     3.733     4.073

     16 1.337   1.746          2.120          2.583   2.921     3.686     4.015
     17 1.333   1.740          2.110          2.567   2.898     3.646     3.965
     18 1.330   1.734          2.101          2.552   2.878     3.610     3.922
     19 1.328   1.729          2.093          2.539   2.861     3.579     3.883
     20 1.325   1.725          2.086          2.528   2.845     3.552     3.849

     21 1.323   1.721          2.080          2.518   2.831     3.527     3.819
     22 1.321   1.717          2.074          2.508   2.819     3.505     3.792
     23 1.319   1.714          2.069          2.500   2.807     3.485     3.768
     24 1.318   1.711          2.064          2.492   2.797     3.467     3.745
     25 1.316   1.708          2.060          2.485   2.787     3.450     3.725

     26 1.315   1.706          2.056          2.479   2.779     3.435     3.707
     27 1.314   1.703          2.052          2.473   2.771     3.421     3.690
     28 1.313   1.701          2.048          2.467   2.763     3.408     3.674
     29 1.311   1.699          2.045          2.462   2.756     3.396     3.659
     30 1.310   1.697          2.042          2.457   2.750     3.385     3.646

      40 1.303  1.684          2.021          2.423   2.704     3.307     3.551
      60 1.296  1.671          2.000          2.390   2.660     3.232     3.460
     120 1.289  1.658          1.980          2.358   2.617     3.160     3.373
       1.282    1.645          1.960          2.326   2.576     3.090     3.291

     This table was generated using MINITAB.

790
Table B.5
Chi-Square Distribution Quantiles

 Q(.005) Q(.01) Q(.025) Q(.05) Q(.1) Q(.9) Q(.95)                      Q(.975)   Q(.99)  Q(.995)

1   0.000 0.000    0.001 0.004 0.016 2.706 3.841                          5.024   6.635     7.879
                                                                          7.378   9.210   10.597
2   0.010 0.020    0.051 0.103 0.211 4.605 5.991                          9.348  11.345   12.838
                                                                        11.143   13.277   14.860
3   0.072 0.115    0.216 0.352 0.584 6.251 7.815                        12.833   15.086   16.750

4   0.207 0.297    0.484 0.711 1.064 7.779 9.488                        14.449   16.812   18.548
                                                                        16.013   18.475   20.278
5   0.412 0.554    0.831 1.145 1.610 9.236 11.070                       17.535   20.090   21.955
                                                                        19.023   21.666   23.589
6   0.676 0.872    1.237 1.635 2.204 10.645 12.592                      20.483   23.209   25.188

7   0.989 1.239    1.690 2.167 2.833 12.017 14.067                      21.920   24.725   26.757
                                                                        23.337   26.217   28.300
8   1.344 1.646    2.180 2.733 3.490 13.362 15.507                      24.736   27.688   29.819
                                                                        26.119   29.141   31.319
9   1.735 2.088    2.700 3.325 4.168 14.684 16.919                      27.488   30.578   32.801

10  2.156 2.558    3.247 3.940 4.865 15.987 18.307                      28.845   32.000   34.267
                                                                        30.191   33.409   35.718
11  2.603 3.053    3.816 4.575 5.578 17.275 19.675                      31.526   34.805   37.156
                                                                        32.852   36.191   38.582
12  3.074 3.571    4.404 5.226 6.304 18.549 21.026                      34.170   37.566   39.997

13  3.565 4.107    5.009 5.892 7.042 19.812 22.362                      35.479   38.932   41.401
                                                                        36.781   40.290   42.796
14  4.075 4.660    5.629 6.571 7.790 21.064 23.685                      38.076   41.638   44.181
                                                                        39.364   42.980   45.559
15  4.601 5.229    6.262 7.261 8.547 22.307 24.996                      40.647   44.314   46.928

16  5.142 5.812    6.908 7.962 9.312 23.542 26.296                      41.923   45.642   48.290
                                                                        43.195   46.963   49.645
17  5.697 6.408    7.564 8.672 10.085 24.769 27.587                     44.461   48.278   50.994
                                                                        45.722   49.588   52.336
18  6.265 7.015    8.231 9.390 10.865 25.989 28.869                     46.979   50.892   53.672

19  6.844 7.633    8.907 10.117 11.651 27.204 30.143                    48.232   52.192   55.003
                                                                        49.480   53.486   56.328
20  7.434 8.260    9.591 10.851 12.443 28.412 31.410                    50.725   54.775   57.648
                                                                        51.966   56.061   58.964
21  8.034 8.897 10.283 11.591 13.240 29.615 32.671                      53.204   57.342   60.275

22  8.643 9.542 10.982 12.338 14.041 30.813 33.924                      54.437   58.619   61.581
                                                                        55.668   59.893   62.885
23  9.260 10.196 11.689 13.091 14.848 32.007 35.172                     56.896   61.163   64.183
                                                                        58.120   62.429   65.477
24  9.886 10.856 12.401 13.848 15.659 33.196 36.415                     59.342   63.691   66.767

25  10.520 11.524  13.120 14.611 16.473 34.382 37.653                                            791

26  11.160 12.198  13.844 15.379 17.292 35.563 38.885

27  11.808 12.879  14.573 16.151 18.114 36.741 40.113

28  12.461 13.565  15.308 16.928 18.939 37.916 41.337

29  13.121 14.256  16.047 17.708 19.768 39.087 42.557

30  13.787 14.953  16.791 18.493 20.599 40.256 43.773

31  14.458 15.655  17.539 19.281 21.434 41.422 44.985

32  15.134 16.362  18.291 20.072 22.271 42.585 46.194

33  15.815 17.074  19.047 20.867 23.110 43.745 47.400

34  16.501 17.789  19.806 21.664 23.952 44.903 48.602

35  17.192 18.509  20.569 22.465 24.797 46.059 49.802

36  17.887 19.233  21.336 23.269 25.643 47.212 50.998

37  18.586 19.960  22.106 24.075 26.492 48.364 52.192

38  19.289 20.691  22.878 24.884 27.343 49.513 53.384

39  19.996 21.426  23.654 25.695 28.196 50.660 54.572

40  20.707 22.164  24.433 26.509 29.051 51.805 55.759

This table was generated using MINITAB.

                                         2         2  3

For  > 40, the approximation Q( p)   1 - + Qz( p)        can be used.
                                         9         9
Table B.6A
F Distribution .75 Quantiles

       2
(Denominator

Degrees of                               1 (Numerator Degrees of Freedom)

Freedom) 1 2 3 4 5 6 7 8 9 10 12 15 20 24 30 40 60 120 

            1 5.83 7.50 8.20 8.58 8.82 8.98 9.10 9.19 9.26 9.32 9.41 9.49 9.58 9.63 9.67 9.71 9.76 9.80 9.85
            2 2.57 3.00 3.15 3.23 3.28 3.31 3.34 3.35 3.37 3.38 3.39 3.41 3.43 3.43 3.44 3.45 3.46 3.47 3.48
            3 2.02 2.28 2.36 2.39 2.41 2.42 2.43 2.44 2.44 2.44 2.45 2.46 2.46 2.46 2.47 2.47 2.47 2.47 2.47
            4 1.81 2.00 2.05 2.06 2.07 2.08 2.08 2.08 2.08 2.08 2.08 2.08 2.08 2.08 2.08 2.08 2.08 2.08 2.08
            5 1.69 1.85 1.88 1.89 1.89 1.89 1.89 1.89 1.89 1.89 1.89 1.89 1.88 1.88 1.88 1.88 1.87 1.87 1.87

      6 1.62 1.76 1.78 1.79 1.79 1.78 1.78 1.78 1.77 1.77 1.77 1.76 1.76 1.75 1.75 1.75 1.74 1.74 1.74
      7 1.57 1.70 1.72 1.72 1.71 1.71 1.70 1.70 1.69 1.69 1.68 1.68 1.67 1.67 1.66 1.66 1.65 1.65 1.65
      8 1.54 1.66 1.67 1.66 1.66 1.65 1.64 1.64 1.64 1.63 1.62 1.62 1.61 1.60 1.60 1.59 1.59 1.58 1.58
      9 1.51 1.62 1.63 1.63 1.62 1.61 1.60 1.60 1.59 1.59 1.58 1.57 1.56 1.56 1.55 1.54 1.54 1.53 1.53
     10 1.49 1.60 1.60 1.59 1.59 1.58 1.57 1.56 1.56 1.55 1.54 1.53 1.52 1.52 1.51 1.51 1.50 1.49 1.48

     11 1.47 1.58 1.58 1.57 1.56 1.55 1.54 1.53 1.53 1.52 1.51 1.50 1.49 1.49 1.48 1.47 1.47 1.46 1.45
     12 1.46 1.56 1.56 1.55 1.54 1.53 1.52 1.51 1.51 1.50 1.49 1.48 1.47 1.46 1.45 1.45 1.44 1.43 1.42
     13 1.45 1.55 1.55 1.53 1.52 1.51 1.50 1.49 1.49 1.48 1.47 1.46 1.45 1.44 1.43 1.42 1.42 1.41 1.40
     14 1.44 1.53 1.53 1.52 1.51 1.50 1.49 1.48 1.47 1.46 1.45 1.44 1.43 1.42 1.41 1.41 1.40 1.39 1.38
     15 1.43 1.52 1.52 1.51 1.49 1.48 1.47 1.46 1.46 1.45 1.44 1.43 1.41 1.41 1.40 1.39 1.38 1.37 1.36

     16 1.42 1.51 1.51 1.50 1.48 1.47 1.46 1.45 1.44 1.44 1.43 1.41 1.40 1.39 1.38 1.37 1.36 1.35 1.34
     17 1.42 1.51 1.50 1.49 1.47 1.46 1.45 1.44 1.43 1.43 1.41 1.40 1.39 1.38 1.37 1.36 1.35 1.34 1.33
     18 1.41 1.50 1.49 1.48 1.46 1.45 1.44 1.43 1.42 1.42 1.40 1.39 1.38 1.37 1.36 1.35 1.34 1.33 1.32
     19 1.41 1.49 1.49 1.47 1.46 1.44 1.43 1.42 1.41 1.41 1.40 1.38 1.37 1.36 1.35 1.34 1.33 1.32 1.30
     20 1.40 1.49 1.48 1.47 1.45 1.44 1.43 1.42 1.41 1.40 1.39 1.37 1.36 1.35 1.34 1.33 1.32 1.31 1.29

     21 1.40 1.48 1.48 1.46 1.44 1.43 1.42 1.41 1.40 1.39 1.38 1.37 1.35 1.34 1.33 1.32 1.31 1.30 1.28
     22 1.40 1.48 1.47 1.45 1.44 1.42 1.41 1.40 1.39 1.39 1.37 1.36 1.34 1.33 1.32 1.31 1.30 1.29 1.28
     23 1.39 1.47 1.47 1.45 1.43 1.42 1.41 1.40 1.39 1.38 1.37 1.35 1.34 1.33 1.32 1.31 1.30 1.28 1.27
     24 1.39 1.47 1.46 1.44 1.43 1.41 1.40 1.39 1.38 1.38 1.36 1.35 1.33 1.32 1.31 1.30 1.29 1.28 1.26
     25 1.39 1.47 1.46 1.44 1.42 1.41 1.40 1.39 1.38 1.37 1.36 1.34 1.33 1.32 1.31 1.29 1.28 1.27 1.25

     26 1.38 1.46 1.45 1.44 1.42 1.41 1.39 1.38 1.37 1.37 1.35 1.34 1.32 1.31 1.30 1.29 1.28 1.26 1.25
     27 1.38 1.46 1.45 1.43 1.42 1.40 1.39 1.38 1.37 1.36 1.35 1.33 1.32 1.31 1.30 1.28 1.27 1.26 1.24
     28 1.38 1.46 1.45 1.43 1.41 1.40 1.39 1.38 1.37 1.36 1.34 1.33 1.31 1.30 1.29 1.28 1.27 1.25 1.24
     29 1.38 1.45 1.45 1.43 1.41 1.40 1.38 1.37 1.36 1.35 1.34 1.32 1.31 1.30 1.29 1.27 1.26 1.25 1.23
     30 1.38 1.45 1.44 1.42 1.41 1.39 1.38 1.37 1.36 1.35 1.34 1.32 1.30 1.29 1.28 1.27 1.26 1.24 1.23

      40 1.36 1.44 1.42 1.40 1.39 1.37 1.36 1.35 1.34 1.33 1.31 1.30 1.28 1.26 1.25 1.24 1.22 1.21 1.19
      60 1.35 1.42 1.41 1.38 1.37 1.35 1.33 1.32 1.31 1.30 1.29 1.27 1.25 1.24 1.22 1.21 1.19 1.17 1.15
     120 1.34 1.40 1.39 1.37 1.35 1.33 1.31 1.30 1.29 1.28 1.26 1.24 1.22 1.21 1.19 1.18 1.16 1.13 1.10
       1.32 1.39 1.37 1.35 1.33 1.31 1.29 1.28 1.27 1.25 1.24 1.22 1.19 1.18 1.16 1.14 1.12 1.08 1.00

This table was generated using MINITAB.

792
Table B.6B
F Distribution .90 Quantiles

       2
(Denominator

Degrees of                    1 (Numerator Degrees of Freedom)

Freedom) 1 2 3 4 5 6 7 8 9 10 12 15 20 24 30 40 60 120 

1 39.86 49.50 53.59 55.84 57.24 58.20 58.90 59.44 59.85 60.20 60.70 61.22 61.74 62.00 62.27 62.53 62.79 63.05 63.33
2 8.53 9.00 9.16 9.24 9.29 9.33 9.35 9.37 9.38 9.39 9.41 9.42 9.44 9.45 9.46 9.47 9.47 9.48 9.49
3 5.54 5.46 5.39 5.34 5.31 5.28 5.27 5.25 5.24 5.23 5.22 5.20 5.18 5.18 5.17 5.16 5.15 5.14 5.13
4 4.54 4.32 4.19 4.11 4.05 4.01 3.98 3.95 3.94 3.92 3.90 3.87 3.84 3.83 3.82 3.80 3.79 3.78 3.76
5 4.06 3.78 3.62 3.52 3.45 3.40 3.37 3.34 3.32 3.30 3.27 3.24 3.21 3.19 3.17 3.16 3.14 3.12 3.10

 6 3.78 3.46 3.29 3.18 3.11 3.05 3.01 2.98 2.96 2.94 2.90 2.87 2.84 2.82 2.80 2.78 2.76 2.74 2.72
 7 3.59 3.26 3.07 2.96 2.88 2.83 2.78 2.75 2.72 2.70 2.67 2.63 2.59 2.58 2.56 2.54 2.51 2.49 2.47
 8 3.46 3.11 2.92 2.81 2.73 2.67 2.62 2.59 2.56 2.54 2.50 2.46 2.42 2.40 2.38 2.36 2.34 2.32 2.29
 9 3.36 3.01 2.81 2.69 2.61 2.55 2.51 2.47 2.44 2.42 2.38 2.34 2.30 2.28 2.25 2.23 2.21 2.18 2.16
10 3.28 2.92 2.73 2.61 2.52 2.46 2.41 2.38 2.35 2.32 2.28 2.24 2.20 2.18 2.16 2.13 2.11 2.08 2.06

11 3.23 2.86 2.66 2.54 2.45 2.39 2.34 2.30 2.27 2.25 2.21 2.17 2.12 2.10 2.08 2.05 2.03 2.00 1.97
12 3.18 2.81 2.61 2.48 2.39 2.33 2.28 2.24 2.21 2.19 2.15 2.10 2.06 2.04 2.01 1.99 1.96 1.93 1.90
13 3.14 2.76 2.56 2.43 2.35 2.28 2.23 2.20 2.16 2.14 2.10 2.05 2.01 1.98 1.96 1.93 1.90 1.88 1.85
14 3.10 2.73 2.52 2.39 2.31 2.24 2.19 2.15 2.12 2.10 2.05 2.01 1.96 1.94 1.91 1.89 1.86 1.83 1.80
15 3.07 2.70 2.49 2.36 2.27 2.21 2.16 2.12 2.09 2.06 2.02 1.97 1.92 1.90 1.87 1.85 1.82 1.79 1.76

16 3.05 2.67 2.46 2.33 2.24 2.18 2.13 2.09 2.06 2.03 1.99 1.94 1.89 1.87 1.84 1.81 1.78 1.75 1.72
17 3.03 2.64 2.44 2.31 2.22 2.15 2.10 2.06 2.03 2.00 1.96 1.91 1.86 1.84 1.81 1.78 1.75 1.72 1.69
18 3.01 2.62 2.42 2.29 2.20 2.13 2.08 2.04 2.00 1.98 1.93 1.89 1.84 1.81 1.78 1.75 1.72 1.69 1.66
19 2.99 2.61 2.40 2.27 2.18 2.11 2.06 2.02 1.98 1.96 1.91 1.86 1.81 1.79 1.76 1.73 1.70 1.67 1.63
20 2.97 2.59 2.38 2.25 2.16 2.09 2.04 2.00 1.96 1.94 1.89 1.84 1.79 1.77 1.74 1.71 1.68 1.64 1.61

21 2.96 2.57 2.36 2.23 2.14 2.08 2.02 1.98 1.95 1.92 1.87 1.83 1.78 1.75 1.72 1.69 1.66 1.62 1.59
22 2.95 2.56 2.35 2.22 2.13 2.06 2.01 1.97 1.93 1.90 1.86 1.81 1.76 1.73 1.70 1.67 1.64 1.60 1.57
23 2.94 2.55 2.34 2.21 2.11 2.05 1.99 1.95 1.92 1.89 1.84 1.80 1.74 1.72 1.69 1.66 1.62 1.59 1.55
24 2.93 2.54 2.33 2.19 2.10 2.04 1.98 1.94 1.91 1.88 1.83 1.78 1.73 1.70 1.67 1.64 1.61 1.57 1.53
25 2.92 2.53 2.32 2.18 2.09 2.02 1.97 1.93 1.89 1.87 1.82 1.77 1.72 1.69 1.66 1.63 1.59 1.56 1.52

26 2.91 2.52 2.31 2.17 2.08 2.01 1.96 1.92 1.88 1.86 1.81 1.76 1.71 1.68 1.65 1.61 1.58 1.54 1.50
27 2.90 2.51 2.30 2.17 2.07 2.00 1.95 1.91 1.87 1.85 1.80 1.75 1.70 1.67 1.64 1.60 1.57 1.53 1.49
28 2.89 2.50 2.29 2.16 2.06 2.00 1.94 1.90 1.87 1.84 1.79 1.74 1.69 1.66 1.63 1.59 1.56 1.52 1.48
29 2.89 2.50 2.28 2.15 2.06 1.99 1.93 1.89 1.86 1.83 1.78 1.73 1.68 1.65 1.62 1.58 1.55 1.51 1.47
30 2.88 2.49 2.28 2.14 2.05 1.98 1.93 1.88 1.85 1.82 1.77 1.72 1.67 1.64 1.61 1.57 1.54 1.50 1.46

            40 2.84 2.44 2.23 2.09 2.00 1.93 1.87 1.83 1.79 1.76 1.71 1.66 1.61 1.57 1.54 1.51 1.47 1.42 1.38
            60 2.79 2.39 2.18 2.04 1.95 1.87 1.82 1.77 1.74 1.71 1.66 1.60 1.54 1.51 1.48 1.44 1.40 1.35 1.29
          120 2.75 2.35 2.13 1.99 1.90 1.82 1.77 1.72 1.68 1.65 1.60 1.55 1.48 1.45 1.41 1.37 1.32 1.26 1.19
             2.71 2.30 2.08 1.94 1.85 1.77 1.72 1.67 1.63 1.60 1.55 1.49 1.42 1.38 1.34 1.30 1.24 1.17 1.00

This table was generated using MINITAB.

                                                                793
Table B.6C
F Distribution .95 Quantiles

       2
(Denominator

Degrees of                                1 (Numerator Degrees of Freedom)

     Freedom)  1              2     3     4        5        6        7           8        9       10

               1 161.44 199.50   215.69   224.57   230.16   233.98   236.78   238.89   240.55   241.89
               2 18.51 19.00      19.16    19.25    19.30    19.33    19.35    19.37    19.39    19.40
               3 10.13 9.55         9.28     9.12     9.01     8.94     8.89     8.85     8.81     8.79
               4 7.71 6.94          6.59     6.39     6.26     6.16     6.09     6.04     6.00     5.96
               5 6.61 5.79          5.41     5.19     5.05     4.95     4.88     4.82     4.77     4.74

      6 5.99 5.14                   4.76  4.53 4.39 4.28 4.21                    4.15     4.10     4.06
      7 5.59 4.74                   4.35  4.12 3.97 3.87 3.79                    3.73     3.68     3.64
      8 5.32 4.46                   4.07  3.84 3.69 3.58 3.50                    3.44     3.39     3.35
      9 5.12 4.26                   3.86  3.63 3.48 3.37 3.29                    3.23     3.18     3.14
     10 4.96 4.10                   3.71  3.48 3.33 3.22 3.14                    3.07     3.02     2.98

     11 4.84 3.98                   3.59  3.36 3.20 3.09 3.01                    2.95     2.90     2.85
     12 4.75 3.89                   3.49  3.26 3.11 3.00 2.91                    2.85     2.80     2.75
     13 4.67 3.81                   3.41  3.18 3.03 2.92 2.83                    2.77     2.71     2.67
     14 4.60 3.74                   3.34  3.11 2.96 2.85 2.76                    2.70     2.65     2.60
     15 4.54 3.68                   3.29  3.06 2.90 2.79 2.71                    2.64     2.59     2.54

     16 4.49 3.63                   3.24  3.01 2.85 2.74 2.66                    2.59     2.54     2.49
     17 4.45 3.59                   3.20  2.96 2.81 2.70 2.61                    2.55     2.49     2.45
     18 4.41 3.55                   3.16  2.93 2.77 2.66 2.58                    2.51     2.46     2.41
     19 4.38 3.52                   3.13  2.90 2.74 2.63 2.54                    2.48     2.42     2.38
     20 4.35 3.49                   3.10  2.87 2.71 2.60 2.51                    2.45     2.39     2.35

     21 4.32 3.47                   3.07  2.84 2.68 2.57 2.49                    2.42     2.37     2.32
     22 4.30 3.44                   3.05  2.82 2.66 2.55 2.46                    2.40     2.34     2.30
     23 4.28 3.42                   3.03  2.80 2.64 2.53 2.44                    2.37     2.32     2.27
     24 4.26 3.40                   3.01  2.78 2.62 2.51 2.42                    2.36     2.30     2.25
     25 4.24 3.39                   2.99  2.76 2.60 2.49 2.40                    2.34     2.28     2.24

     26 4.23 3.37                   2.98  2.74 2.59 2.47 2.39                    2.32     2.27     2.22
     27 4.21 3.35                   2.96  2.73 2.57 2.46 2.37                    2.31     2.25     2.20
     28 4.20 3.34                   2.95  2.71 2.56 2.45 2.36                    2.29     2.24     2.19
     29 4.18 3.33                   2.93  2.70 2.55 2.43 2.35                    2.28     2.22     2.18
     30 4.17 3.32                   2.92  2.69 2.53 2.42 2.33                    2.27     2.21     2.16

      40 4.08 3.23                  2.84  2.61 2.45 2.34 2.25                    2.18     2.12     2.08
      60 4.00 3.15                  2.76  2.53 2.37 2.25 2.17                    2.10     2.04     1.99
     120 3.92 3.07                  2.68  2.45 2.29 2.18 2.09                    2.02     1.96     1.91
       3.84 3.00                    2.60  2.37 2.21 2.10 2.01                    1.94     1.88     1.83

794
Table B.6C
F Distribution of .95 Quantiles (continued )

       2
(Denominator

Degrees of                                        1 (Numerator Degrees of Freedom)

Freedom)       12    15                       20    24    30    40                  60     120       

            1 243.91 245.97 248.02 249.04 250.07 251.13 252.18                            253.27   254.31
                                                                                           19.49    19.50
            2 19.41 19.43 19.45 19.45 19.46 19.47 19.48                                      8.55     8.53
                                                                                             5.66     5.63
            3  8.74  8.70                     8.66  8.64  8.62  8.59                8.57     4.40     4.36

            4  5.91  5.86                     5.80  5.77  5.75  5.72                5.69     3.70     3.67
                                                                                             3.27     3.23
            5  4.68  4.62                     4.56  4.53  4.50  4.46                4.43     2.97     2.93
                                                                                             2.75     2.71
            6  4.00  3.94                     3.87  3.84  3.81  3.77                3.74     2.58     2.54

            7  3.57  3.51                     3.44  3.41  3.38  3.34                3.30     2.45     2.40
                                                                                             2.34     2.30
            8  3.28  3.22                     3.15  3.12  3.08  3.04                3.01     2.25     2.21
                                                                                             2.18     2.13
            9  3.07  3.01                     2.94  2.90  2.86  2.83                2.79     2.11     2.07

10             2.91  2.85                     2.77  2.74  2.70  2.66                2.62     2.06     2.01
                                                                                             2.01     1.96
11             2.79  2.72                     2.65  2.61  2.57  2.53                2.49     1.97     1.92
                                                                                             1.93     1.88
12             2.69  2.62                     2.54  2.51  2.47  2.43                2.38     1.90     1.84

13             2.60  2.53                     2.46  2.42  2.38  2.34                2.30     1.87     1.81
                                                                                             1.84     1.78
14             2.53  2.46                     2.39  2.35  2.31  2.27                2.22     1.81     1.76
                                                                                             1.79     1.73
15             2.48  2.40                     2.33  2.29  2.25  2.20                2.16     1.77     1.71

16             2.42  2.35                     2.28  2.24  2.19  2.15                2.11     1.75     1.69
                                                                                             1.73     1.67
17             2.38  2.31                     2.23  2.19  2.15  2.10                2.06     1.71     1.65
                                                                                             1.70     1.64
18             2.34  2.27                     2.19  2.15  2.11  2.06                2.02     1.68     1.62

19             2.31  2.23                     2.16  2.11  2.07  2.03                1.98     1.58     1.51
                                                                                             1.47     1.39
20             2.28  2.20                     2.12  2.08  2.04  1.99                1.95     1.35     1.25
                                                                                             1.22     1.00
21             2.25  2.18                     2.10  2.05  2.01  1.96                1.92
                                                                                                          795
22             2.23  2.15                     2.07  2.03  1.98  1.94                1.89

23             2.20  2.13                     2.05  2.01  1.96  1.91                1.86

24             2.18  2.11                     2.03  1.98  1.94  1.89                1.84

25             2.16  2.09                     2.01  1.96  1.92  1.87                1.82

26             2.15  2.07                     1.99  1.95  1.90  1.85                1.80

27             2.13  2.06                     1.97  1.93  1.88  1.84                1.79

28             2.12  2.04                     1.96  1.91  1.87  1.82                1.77

29             2.10  2.03                     1.94  1.90  1.85  1.81                1.75

30             2.09  2.01                     1.93  1.89  1.84  1.79                1.74

40             2.00  1.92                     1.84  1.79  1.74  1.69                1.64

60             1.92  1.84                     1.75  1.70  1.65  1.59                1.53

120            1.83  1.75                     1.66  1.61  1.55  1.50                1.43

               1.75  1.67                     1.57  1.52  1.46  1.39                1.32

This table was generated using MINITAB.
Table B.6D
F Distribution .99 Quantiles

            2
     (Denominator

     Degrees of                         1 (Numerator Degrees of Freedom)

     Freedom)      1          2  3      4      5      6      7            8        9     10

                 1 4052 4999     5403   5625   5764   5859   5929         5981   6023   6055
                 2 98.51 99.00   99.17  99.25  99.30  99.33  99.35        99.38  99.39  99.40
                 3 34.12 30.82   29.46  28.71  28.24  27.91  27.67        27.49  27.35  27.23
                 4 21.20 18.00   16.69  15.98  15.52  15.21  14.98        14.80  14.66  14.55
                 5 16.26 13.27   12.06  11.39  10.97  10.67  10.46        10.29  10.16  10.05

      6 13.75 10.92              9.78 9.15 8.75 8.47 8.26 8.10                    7.98   7.87
      7 12.25 9.55               8.45 7.85 7.46 7.19 6.99 6.84                    6.72   6.62
      8 11.26 8.65               7.59 7.01 6.63 6.37 6.18 6.03                    5.91   5.81
      9 10.56 8.02               6.99 6.42 6.06 5.80 5.61 5.47                    5.35   5.26
     10 10.04 7.56               6.55 5.99 5.64 5.39 5.20 5.06                    4.94   4.85

     11 9.65 7.21                6.22 5.67 5.32 5.07 4.89 4.74                    4.63   4.54
     12 9.33 6.93                5.95 5.41 5.06 4.82 4.64 4.50                    4.39   4.30
     13 9.07 6.70                5.74 5.21 4.86 4.62 4.44 4.30                    4.19   4.10
     14 8.86 6.51                5.56 5.04 4.69 4.46 4.28 4.14                    4.03   3.94
     15 8.68 6.36                5.42 4.89 4.56 4.32 4.14 4.00                    3.89   3.80

     16 8.53 6.23                5.29 4.77 4.44 4.20 4.03 3.89                    3.78   3.69
     17 8.40 6.11                5.18 4.67 4.34 4.10 3.93 3.79                    3.68   3.59
     18 8.29 6.01                5.09 4.58 4.25 4.01 3.84 3.71                    3.60   3.51
     19 8.19 5.93                5.01 4.50 4.17 3.94 3.77 3.63                    3.52   3.43
     20 8.10 5.85                4.94 4.43 4.10 3.87 3.70 3.56                    3.46   3.37

     21 8.02 5.78                4.87 4.37 4.04 3.81 3.64 3.51                    3.40   3.31
     22 7.95 5.72                4.82 4.31 3.99 3.76 3.59 3.45                    3.35   3.26
     23 7.88 5.66                4.76 4.26 3.94 3.71 3.54 3.41                    3.30   3.21
     24 7.82 5.61                4.72 4.22 3.90 3.67 3.50 3.36                    3.26   3.17
     25 7.77 5.57                4.68 4.18 3.85 3.63 3.46 3.32                    3.22   3.13

     26 7.72 5.53                4.64 4.14 3.82 3.59 3.42 3.29                    3.18   3.09
     27 7.68 5.49                4.60 4.11 3.78 3.56 3.39 3.26                    3.15   3.06
     28 7.64 5.45                4.57 4.07 3.75 3.53 3.36 3.23                    3.12   3.03
     29 7.60 5.42                4.54 4.04 3.73 3.50 3.33 3.20                    3.09   3.00
     30 7.56 5.39                4.51 4.02 3.70 3.47 3.30 3.17                    3.07   2.98

      40 7.31 5.18               4.31 3.83 3.51 3.29 3.12 2.99                    2.89   2.80
      60 7.08 4.98               4.13 3.65 3.34 3.12 2.95 2.82                    2.72   2.63
     120 6.85 4.79               3.95 3.48 3.17 2.96 2.79 2.66                    2.56   2.47
       6.63 4.61                 3.78 3.32 3.02 2.80 2.64 2.51                    2.41   2.32

796
Table B.6D
F Distribution of .99 Quantiles (continued )

       2
(Denominator

Degrees of                               1 (Numerator Degrees of Freedom)

Freedom)      12  15                     20   24  30  40  60                120     

            1 6107 6157 6209 6235 6260 6287 6312                           6339   6366
            2 99.41 99.43 99.44 99.45 99.47 99.47 99.48                    99.49  99.50
            3 27.05 26.87 26.69 26.60 26.51 26.41 26.32                    26.22  26.13
            4 14.37 14.20 14.02 13.93 13.84 13.75 13.65                    13.56  13.46
            5 9.89 9.72 9.55 9.47 9.38 9.29 9.20
                                                                            9.11   9.02
 6 7.72 7.56 7.40 7.31 7.23 7.14 7.06
 7 6.47 6.31 6.16 6.07 5.99 5.91 5.82                                       6.97   6.88
 8 5.67 5.52 5.36 5.28 5.20 5.12 5.03                                       5.74   5.65
 9 5.11 4.96 4.81 4.73 4.65 4.57 4.48                                       4.95   4.86
10 4.71 4.56 4.41 4.33 4.25 4.17 4.08                                       4.40   4.31
                                                                            4.00   3.91
11 4.40 4.25 4.10 4.02 3.94 3.86 3.78
12 4.16 4.01 3.86 3.78 3.70 3.62 3.54                                       3.69   3.60
13 3.96 3.82 3.66 3.59 3.51 3.43 3.34                                       3.45   3.36
14 3.80 3.66 3.51 3.43 3.35 3.27 3.18                                       3.25   3.17
15 3.67 3.52 3.37 3.29 3.21 3.13 3.05                                       3.09   3.00
                                                                            2.96   2.87
16 3.55 3.41 3.26 3.18 3.10 3.02 2.93
17 3.46 3.31 3.16 3.08 3.00 2.92 2.83                                       2.84   2.75
18 3.37 3.23 3.08 3.00 2.92 2.84 2.75                                       2.75   2.65
19 3.30 3.15 3.00 2.92 2.84 2.76 2.67                                       2.66   2.57
20 3.23 3.09 2.94 2.86 2.78 2.69 2.61                                       2.58   2.49
                                                                            2.52   2.42
21 3.17 3.03 2.88 2.80 2.72 2.64 2.55
22 3.12 2.98 2.83 2.75 2.67 2.58 2.50                                       2.46   2.36
23 3.07 2.93 2.78 2.70 2.62 2.54 2.45                                       2.40   2.31
24 3.03 2.89 2.74 2.66 2.58 2.49 2.40                                       2.35   2.26
25 2.99 2.85 2.70 2.62 2.54 2.45 2.36                                       2.31   2.21
                                                                            2.27   2.17
26 2.96 2.81 2.66 2.58 2.50 2.42 2.33
27 2.93 2.78 2.63 2.55 2.47 2.38 2.29                                       2.23   2.13
28 2.90 2.75 2.60 2.52 2.44 2.35 2.26                                       2.20   2.10
29 2.87 2.73 2.57 2.49 2.41 2.33 2.23                                       2.17   2.06
30 2.84 2.70 2.55 2.47 2.39 2.30 2.21                                       2.14   2.03
                                                                            2.11   2.01
 40 2.66 2.52 2.37 2.29 2.20 2.11 2.02
 60 2.50 2.35 2.20 2.12 2.03 1.94 1.84                                      1.92   1.80
120 2.34 2.19 2.03 1.95 1.86 1.76 1.66                                      1.73   1.60
 2.18 2.04 1.88 1.79 1.70 1.59 1.47                                         1.53   1.38
                                                                            1.32   1.00

This table was generated using MINITAB.

                                                                                         797
Table B.6E
F Distribution .999 Quantiles

       2
(Denominator

Degrees of                           1 (Numerator Degrees of Freedom)

     Freedom)  1               2  3  4  5  6  7                        8  9  10

               1 405261 499996 540349 562463 576409 585904 592890 598185 602359 605671
               2 998.55 999.01 999.23 999.26 999.29 999.38 999.40 999.35 999.45 999.41
               3 167.03 148.50 141.11 137.10 134.58 132.85 131.58 130.62 129.86 129.25
               4 74.14 61.25 56.18 53.44 51.71 50.53 49.66 49.00 48.48 48.05
               5 47.18 37.12 33.20 31.08 29.75 28.83 28.16 27.65 27.24 26.92

      6 35.51 27.00 23.70 21.92 20.80 20.03 19.46 19.03 18.69 18.41
      7 29.24 21.69 18.77 17.20 16.21 15.52 15.02 14.63 14.33 14.08
      8 25.41 18.49 15.83 14.39 13.48 12.86 12.40 12.05 11.77 11.54
      9 22.86 16.39 13.90 12.56 11.71 11.13 10.70 10.37 10.11 9.89
     10 21.04 14.91 12.55 11.28 10.48 9.93 9.52 9.20 8.96 8.75

     11 19.69 13.81 11.56 10.35 9.58 9.05 8.66 8.35 8.12 7.92
     12 18.64 12.97 10.80 9.63 8.89 8.38 8.00 7.71 7.48 7.29
     13 17.82 12.31 10.21 9.07 8.35 7.86 7.49 7.21 6.98 6.80
     14 17.14 11.78 9.73 8.62 7.92 7.44 7.08 6.80 6.58 6.40
     15 16.59 11.34 9.34 8.25 7.57 7.09 6.74 6.47 6.26 6.08

     16 16.12 10.97 9.01 7.94 7.27 6.80 6.46 6.19 5.98 5.81
     17 15.72 10.66 8.73 7.68 7.02 6.56 6.22 5.96 5.75 5.58
     18 15.38 10.39 8.49 7.46 6.81 6.35 6.02 5.76 5.56 5.39
     19 15.08 10.16 8.28 7.27 6.62 6.18 5.85 5.59 5.39 5.22
     20 14.82 9.95 8.10 7.10 6.46 6.02 5.69 5.44 5.24 5.08

     21 14.59 9.77 7.94 6.95 6.32 5.88 5.56 5.31 5.11 4.95
     22 14.38 9.61 7.80 6.81 6.19 5.76 5.44 5.19 4.99 4.83
     23 14.20 9.47 7.67 6.70 6.08 5.65 5.33 5.09 4.89 4.73
     24 14.03 9.34 7.55 6.59 5.98 5.55 5.23 4.99 4.80 4.64
     25 13.88 9.22 7.45 6.49 5.89 5.46 5.15 4.91 4.71 4.56

     26 13.74 9.12 7.36 6.41 5.80 5.38 5.07 4.83 4.64 4.48
     27 13.61 9.02 7.27 6.33 5.73 5.31 5.00 4.76 4.57 4.41
     28 13.50 8.93 7.19 6.25 5.66 5.24 4.93 4.69 4.50 4.35
     29 13.39 8.85 7.12 6.19 5.59 5.18 4.87 4.64 4.45 4.29
     30 13.29 8.77 7.05 6.12 5.53 5.12 4.82 4.58 4.39 4.24

      40 12.61 8.25 6.59 5.70 5.13 4.73 4.44 4.21 4.02 3.87
      60 11.97 7.77 6.17 5.31 4.76 4.37 4.09 3.86 3.69 3.54
     120 11.38 7.32 5.78 4.95 4.42 4.04 3.77 3.55 3.38 3.24
       10.83 6.91 5.42 4.62 4.10 3.74 3.47 3.27 3.10 2.96

798
Table B.6E
F Distribution .999 Quantiles (continued )

       2
(Denominator

Degrees of                                      1 (Numerator Degrees of Freedom)

Freedom)       12    15                     20    24    30    40                  60      120       

            1 610644 615766 620884 623544 626117 628724 631381                          634002   636619
            2 999.46 999.40 999.44 999.45 999.47 999.49 999.50                          999.52   999.50
            3 128.32 127.37 126.42 125.94 125.45 124.96 124.47                          123.97   123.47
            4 47.41 46.76 46.10 45.77 45.43 45.09 44.75
            5 26.42 25.91 25.40 25.13 24.87 24.60 24.33                                   44.40    44.05
                                                                                          24.06    23.79
            6 17.99 17.56 17.12 16.90 16.67 16.44 16.21
                                                                                          15.98    15.75
            7 13.71 13.32 12.93 12.73 12.53 12.33 12.12                                   11.91    11.70

            8 11.19 10.84 10.48 10.30 10.11                   9.92                9.73     9.53     9.33
                                                                                           8.00     7.81
            9  9.57  9.24                   8.90  8.72  8.55  8.37                8.19     6.94     6.76

10             8.45  8.13                   7.80  7.64  7.47  7.30                7.12     6.18     6.00
                                                                                           5.59     5.42
11             7.63  7.32                   7.01  6.85  6.68  6.52                6.35     5.14     4.97
                                                                                           4.77     4.60
12             7.00  6.71                   6.40  6.25  6.09  5.93                5.76     4.47     4.31

13             6.52  6.23                   5.93  5.78  5.63  5.47                5.30     4.23     4.06
                                                                                           4.02     3.85
14             6.13  5.85                   5.56  5.41  5.25  5.10                4.94     3.84     3.67
                                                                                           3.68     3.51
15             5.81  5.54                   5.25  5.10  4.95  4.80                4.64     3.54     3.38

16             5.55  5.27                   4.99  4.85  4.70  4.54                4.39     3.42     3.26
                                                                                           3.32     3.15
17             5.32  5.05                   4.78  4.63  4.48  4.33                4.18     3.22     3.05
                                                                                           3.14     2.97
18             5.13  4.87                   4.59  4.45  4.30  4.15                4.00     3.06     2.89

19             4.97  4.70                   4.43  4.29  4.14  3.99                3.84     2.99     2.82
                                                                                           2.92     2.75
20             4.82  4.56                   4.29  4.15  4.01  3.86                3.70     2.86     2.69
                                                                                           2.81     2.64
21             4.70  4.44                   4.17  4.03  3.88  3.74                3.58     2.76     2.59

22             4.58  4.33                   4.06  3.92  3.78  3.63                3.48     2.41     2.23
                                                                                           2.08     1.89
23             4.48  4.23                   3.96  3.82  3.68  3.53                3.38     1.77     1.54
                                                                                           1.45     1.00
24             4.39  4.14                   3.87  3.74  3.59  3.45                3.29

25             4.31  4.06                   3.79  3.66  3.52  3.37                3.22

26             4.24  3.99                   3.72  3.59  3.44  3.30                3.15

27             4.17  3.92                   3.66  3.52  3.38  3.23                3.08

28             4.11  3.86                   3.60  3.46  3.32  3.18                3.02

29             4.05  3.80                   3.54  3.41  3.27  3.12                2.97

30             4.00  3.75                   3.49  3.36  3.22  3.07                2.92

40             3.64  3.40                   3.14  3.01  2.87  2.73                2.57

60             3.32  3.08                   2.83  2.69  2.55  2.41                2.25

120            3.02  2.78                   2.53  2.40  2.26  2.11                1.95

               2.74  2.51                   2.27  2.13  1.99  1.84                1.66

This table was generated using MINITAB.

                                                                                                 799
800 Appendix B Tables

                       Table B.7A
                       Factors for Two-Sided Tolerance Intervals for Normal Distributions

                             95% Confidence                            99% Confidence

                       n     p = .90 p = .95 p = .99 p = .90 p = .95 p = .99

                          2  36.519 46.944                             182.720             234.877
                                                                        22.131              28.586
                          3 8.306 9.789 12.647                 18.782   11.118              14.405
                                                                9.416                       10.220
                          4 5.368 6.341 8.221                   6.655     7.870

                          5 4.291 5.077 6.598

                           6 3.733 4.422 5.758                 5.383   6.373               8.292
                           7 3.390 4.020 5.241                 4.658   5.520               7.191
                           8 3.156 3.746 4.889                 4.189   4.968               6.479
                           9 2.986 3.546 4.633                 3.860   4.581               5.980
                          10 2.856 3.393 4.437                 3.617   4.294               5.610

                          11 2.754 3.273 4.282                 3.429   4.073               5.324
                          12 2.670 3.175 4.156                 3.279   3.896               5.096
                          13 2.601 3.093 4.051                 3.156   3.751               4.909
                          14 2.542 3.024 3.962                 3.054   3.631               4.753
                          15 2.492 2.965 3.885                 2.967   3.529               4.621

                          16 2.449 2.913 3.819                 2.893   3.441               4.507
                          17 2.410 2.868 3.761                 2.828   3.364               4.408
                          18 2.376 2.828 3.709                 2.771   3.297               4.321
                          19 2.346 2.793 3.663                 2.720   3.237               4.244
                          20 2.319 2.760 3.621                 2.675   3.184               4.175

                          25 2.215 2.638 3.462                 2.506   2.984               3.915
                          30 2.145 2.555 3.355                 2.394   2.851               3.742
                          35 2.094 2.495 3.276                 2.314   2.756               3.618
                          40 2.055 2.448 3.216                 2.253   2.684               3.524
                          50 1.999 2.382 3.129                 2.166   2.580               3.390

                        60 1.960 2.335 3.068                   2.106   2.509               3.297
                        80 1.908 2.274 2.987                   2.028   2.416               3.175
                       100 1.875 2.234 2.936                   1.978   2.357               3.098
                       150 1.826 2.176 2.859                   1.906   2.271               2.985
                       200 1.798 2.143 2.816                   1.866   2.223               2.921

                        500 1.737 2.070 2.721                  1.777   2.117               2.783
                       1000 1.709 2.036 2.676                  1.736   2.068               2.718
                                                               1.645   1.960               2.576
                          1.645 1.960 2.576

                       This table was computed using Mathcad.
                                        Appendix B Tables 801

Table B.7B
Factors for One-Sided Tolerance Intervals for Normal Distributions

   95% Confidence                               99% Confidence

n  p = .90 p = .95 p = .99 p = .90 p = .95 p = .99

   2                                    14.006  17.372              23.896
   3 6.155 7.656 10.553                  7.380   9.083              12.388
   4 4.162 5.144 7.042                   5.362   6.578               8.939
   5 3.407 4.203 5.741

    6 3.006 3.708 5.062                 4.411   5.406               7.335
    7 2.755 3.399 4.642                 3.859   4.728               6.412
    8 2.582 3.187 4.354                 3.497   4.285               5.812
    9 2.454 3.031 4.143                 3.240   3.972               5.389
   10 2.355 2.911 3.981                 3.048   3.738               5.074

   11 2.275 2.815 3.852                 2.898   3.556               4.829
   12 2.210 2.736 3.747                 2.777   3.410               4.633
   13 2.155 2.671 3.659                 2.677   3.290               4.472
   14 2.109 2.614 3.585                 2.593   3.189               4.337
   15 2.068 2.566 3.520                 2.521   3.102               4.222

   16 2.033 2.524 3.464                 2.459   3.028               4.123
   17 2.002 2.486 3.414                 2.405   2.963               4.037
   18 1.974 2.453 3.370                 2.357   2.905               3.960
   19 1.949 2.423 3.331                 2.314   2.854               3.892
   20 1.926 2.396 3.295                 2.276   2.808               3.832

   25 1.838 2.292 3.158                 2.129   2.633               3.601
   30 1.777 2.220 3.064                 2.030   2.515               3.447
   35 1.732 2.167 2.995                 1.957   2.430               3.334
   40 1.697 2.125 2.941                 1.902   2.364               3.249
   50 1.646 2.065 2.862                 1.821   2.269               3.125

 60 1.609 2.022 2.807                   1.764   2.202               3.038
 80 1.559 1.964 2.733                   1.688   2.114               2.924
100 1.527 1.927 2.684                   1.639   2.056               2.850
150 1.478 1.870 2.611                   1.566   1.971               2.740
200 1.450 1.837 2.570                   1.524   1.923               2.679

 500 1.385 1.763 2.475                  1.430   1.814               2.540
1000 1.354 1.727 2.430                  1.385   1.762               2.475
                                        1.282   1.645               2.326
   1.282 1.645 2.326

This table was computed using Mathcad.
802 Appendix B Tables

Table B.8A
Factors for Simultaneous 95% Two-Sided Confidence Limits for Several Means

                                Number of Means

1  2  3                4  5  6  7                                   8  9 10 12 14 16 32

2 4.303 5.571 6.340 6.886 7.306 7.645 7.929 8.172 8.385 8.573 8.894 9.162 9.390 10.529
3 3.182 3.960 4.430 4.764 5.023 5.233 5.410 5.562 5.694 5.812 6.015 6.184 6.328 7.055
4 2.776 3.382 3.745 4.003 4.203 4.366 4.503 4.621 4.725 4.817 4.975 5.107 5.221 5.794
5 2.571 3.091 3.399 3.619 3.789 3.928 4.044 4.145 4.233 4.312 4.447 4.560 4.657 5.150

 6 2.447 2.916 3.193 3.389 3.541 3.664 3.769 3.858 3.937 4.008 4.129 4.230 4.317   4.760
 7 2.365 2.800 3.055 3.236 3.376 3.489 3.585 3.668 3.740 3.805 3.916 4.009 4.090   4.498
 8 2.306 2.718 2.958 3.127 3.258 3.365 3.454 3.532 3.600 3.660 3.764 3.852 3.927   4.310
 9 2.262 2.657 2.885 3.046 3.171 3.272 3.357 3.430 3.494 3.552 3.650 3.733 3.805   4.169
10 2.228 2.609 2.829 2.983 3.103 3.199 3.281 3.351 3.412 3.467 3.562 3.641 3.710   4.058

11 2.201 2.571 2.784 2.933 3.048 3.142 3.220 3.288 3.347 3.400 3.491 3.568 3.634   3.969
12 2.179 2.540 2.747 2.892 3.004 3.095 3.171 3.236 3.294 3.345 3.433 3.507 3.571   3.897
13 2.160 2.514 2.717 2.858 2.967 3.055 3.129 3.193 3.249 3.299 3.385 3.457 3.519   3.836
14 2.145 2.493 2.691 2.830 2.936 3.022 3.095 3.157 3.212 3.260 3.344 3.415 3.475   3.784
15 2.131 2.474 2.669 2.805 2.909 2.994 3.065 3.126 3.180 3.227 3.309 3.378 3.438   3.740

16 2.120 2.458 2.650 2.784 2.886 2.969 3.039 3.099 3.152 3.199 3.279 3.347 3.405   3.701
17 2.110 2.444 2.633 2.765 2.866 2.948 3.017 3.076 3.127 3.173 3.253 3.319 3.376   3.668
18 2.101 2.432 2.619 2.749 2.849 2.929 2.997 3.055 3.106 3.151 3.229 3.295 3.351   3.638
19 2.093 2.421 2.606 2.734 2.833 2.912 2.979 3.037 3.087 3.132 3.209 3.273 3.329   3.611
20 2.086 2.411 2.594 2.721 2.819 2.897 2.963 3.020 3.070 3.114 3.190 3.254 3.308   3.587

24 2.064 2.380 2.558 2.681 2.775 2.851 2.914 2.969 3.016 3.059 3.132 3.193 3.246   3.513
30 2.042 2.350 2.522 2.641 2.732 2.805 2.866 2.918 2.964 3.005 3.075 3.133 3.184   3.439
36 2.028 2.331 2.499 2.615 2.704 2.775 2.834 2.885 2.930 2.970 3.038 3.094 3.143   3.391
40 2.021 2.321 2.488 2.602 2.690 2.760 2.819 2.869 2.913 2.952 3.019 3.075 3.123   3.367

 60 2.000 2.292 2.454 2.564 2.649 2.716 2.772 2.821 2.863 2.900 2.964 3.018 3.064  3.295
120 1.980 2.264 2.420 2.527 2.608 2.673 2.727 2.773 2.814 2.849 2.910 2.961 3.005  3.225
144 1.977 2.259 2.415 2.521 2.602 2.666 2.720 2.766 2.806 2.841 2.902 2.952 2.996  3.214
  1.960 2.237 2.388 2.491 2.569 2.631 2.683 2.727 2.766 2.800 2.858 2.906 2.948    3.156

This table was prepared using a program written by Daniel L. Rose.
                                                                            Appendix B Tables 803

Table B.8B
Factors for Simultaneous 95% One-Sided Confidence Limits for Several Means

                  Number of Means

1  2  3  4  5  6  7                                                 8  9 10 12 14 16 32

2 2.920 4.075 4.834 5.397 5.842 6.208 6.516 6.781 7.014 7.220 7.573 7.867 8.118 9.364
3 2.353 3.090 3.551 3.888 4.154 4.372 4.557 4.717 4.858 4.983 5.199 5.380 5.535 6.315
4 2.132 2.722 3.080 3.340 3.544 3.711 3.852 3.974 4.082 4.179 4.345 4.484 4.604 5.212
5 2.015 2.532 2.840 3.062 3.234 3.376 3.495 3.599 3.690 3.772 3.912 4.031 4.132 4.650

 6 1.943 2.417 2.696 2.894 3.049 3.175 3.282 3.374 3.455 3.528 3.653 3.758 3.849 4.312
 7 1.895 2.340 2.599 2.783 2.925 3.041 3.139 3.224 3.299 3.365 3.480 3.577 3.660 4.085
 8 1.860 2.285 2.530 2.703 2.837 2.946 3.038 3.117 3.187 3.250 3.357 3.447 3.525 3.923
 9 1.833 2.243 2.479 2.644 2.772 2.875 2.962 3.038 3.104 3.163 3.265 3.351 3.424 3.801
10 1.812 2.211 2.439 2.598 2.720 2.820 2.904 2.976 3.039 3.096 3.193 3.275 3.346 3.707

11 1.796 2.186 2.407 2.561 2.680 2.776 2.857 2.927 2.988 3.042 3.136 3.215 3.283 3.631
12 1.782 2.164 2.380 2.531 2.647 2.740 2.819 2.886 2.946 2.999 3.090 3.166 3.232 3.569
13 1.771 2.147 2.359 2.506 2.619 2.710 2.787 2.853 2.911 2.962 3.051 3.126 3.190 3.517
14 1.761 2.132 2.340 2.485 2.596 2.685 2.760 2.825 2.881 2.932 3.018 3.091 3.154 3.473
15 1.753 2.119 2.324 2.467 2.576 2.663 2.737 2.800 2.856 2.905 2.990 3.062 3.123 3.436

16 1.746 2.108 2.311 2.451 2.558 2.645 2.717 2.779 2.834 2.883 2.966 3.036 3.096 3.403
17 1.740 2.099 2.299 2.437 2.543 2.628 2.700 2.761 2.815 2.863 2.945 3.014 3.073 3.375
18 1.734 2.090 2.288 2.425 2.530 2.614 2.684 2.745 2.798 2.845 2.926 2.994 3.052 3.349
19 1.729 2.083 2.279 2.415 2.518 2.601 2.671 2.731 2.783 2.830 2.910 2.977 3.034 3.327
20 1.725 2.076 2.271 2.405 2.507 2.590 2.659 2.718 2.770 2.816 2.895 2.961 3.018 3.307

24 1.711 2.055 2.245 2.375 2.474 2.554 2.621 2.678 2.728 2.772 2.848 2.912 2.967 3.244
30 1.697 2.034 2.219 2.346 2.442 2.519 2.584 2.639 2.687 2.730 2.803 2.864 2.917 3.183
36 1.688 2.020 2.202 2.327 2.421 2.496 2.559 2.613 2.660 2.702 2.773 2.833 2.884 3.142
40 1.684 2.014 2.194 2.317 2.410 2.485 2.547 2.600 2.647 2.688 2.758 2.817 2.868 3.122

 60 1.671 1.993 2.169 2.289 2.379 2.451 2.511 2.563 2.607 2.647 2.715 2.771 2.820 3.063
120 1.658 1.974 2.145 2.261 2.349 2.418 2.476 2.526 2.569 2.607 2.672 2.726 2.773 3.005
144 1.656 1.971 2.141 2.257 2.344 2.413 2.471 2.520 2.563 2.601 2.665 2.719 2.765 2.995
  1.645 1.955 2.121 2.234 2.319 2.386 2.442 2.490 2.531 2.568 2.630 2.682 2.727 2.948

This table was prepared using a program written by Daniel L. Rose.
804 Appendix B Tables

Table B.9A
.95 Quantiles of the Studentized Range Distribution

                                           Number of Means to Be Compared

  2  3  4              5                6  7         8  9 10 11 12 13 15 20

  5 3.64 4.60 5.22 5.67 6.03 6.33 6.58 6.80 6.99 7.17 7.32 7.47 7.72 8.21

 6 3.46 4.34 4.90 5.30 5.63 5.90 6.12 6.32 6.49 6.65 6.79 6.92 7.14 7.59
 7 3.34 4.16 4.68 5.06 5.36 5.61 5.82 6.00 6.16 6.30 6.43 6.55 6.76 7.17
 8 3.26 4.04 4.53 4.89 5.17 5.40 5.60 5.77 5.92 6.05 6.18 6.29 6.48 6.87
 9 3.20 3.95 4.41 4.76 5.02 5.24 5.43 5.59 5.74 5.87 5.98 6.09 6.28 6.64
10 3.15 3.88 4.33 4.65 4.91 5.12 5.30 5.46 5.60 5.72 5.83 5.93 6.11 6.47

11 3.11 3.82 4.26 4.57 4.82 5.03 5.20 5.35 5.49 5.61 5.71 5.81 5.98 6.33
12 3.08 3.77 4.20 4.51 4.75 4.95 5.12 5.27 5.39 5.51 5.61 5.71 5.88 6.21
13 3.06 3.73 4.15 4.45 4.69 4.88 5.05 5.19 5.32 5.43 5.53 5.63 5.79 6.11
14 3.03 3.70 4.11 4.41 4.64 4.83 4.99 5.13 5.25 5.36 5.46 5.55 5.71 6.03
15 3.01 3.67 4.08 4.37 4.59 4.78 4.94 5.08 5.20 5.31 5.40 5.49 5.65 5.96

16 3.00 3.65 4.05 4.33 4.56 4.74 4.90 5.03 5.15 5.26 5.35 5.44 5.59 5.90
17 2.98 3.63 4.02 4.30 4.52 4.70 4.86 4.99 5.11 5.21 5.31 5.39 5.54 5.84
18 2.97 3.61 4.00 4.28 4.49 4.67 4.82 4.96 5.07 5.17 5.27 5.35 5.50 5.79
19 2.96 3.59 3.98 4.25 4.47 4.65 4.79 4.92 5.04 5.14 5.23 5.31 5.46 5.75
20 2.95 3.58 3.96 4.23 4.45 4.62 4.77 4.90 5.01 5.11 5.20 5.28 5.43 5.71

 24 2.92 3.53 3.90 4.17 4.37 4.54 4.68 4.81 4.92 5.01 5.10 5.18 5.32 5.59
 30 2.89 3.49 3.85 4.10 4.30 4.46 4.60 4.72 4.82 4.92 5.00 5.08 5.21 5.47
 40 2.86 3.44 3.79 4.04 4.23 4.39 4.52 4.63 4.73 4.82 4.90 4.98 5.11 5.36
 60 2.83 3.40 3.74 3.98 4.16 4.31 4.44 4.55 4.65 4.73 4.81 4.88 5.00 5.24
120 2.80 3.36 3.68 3.92 4.10 4.24 4.36 4.47 4.56 4.64 4.71 4.78 4.90 5.13

 2.77 3.31 3.63 3.86 4.03 4.17 4.29 4.39 4.47 4.55 4.62 4.68 4.80 5.01

This table was computed using Mathcad.
                                                                           Appendix B Tables 805

Table B.9B
.99 Quantiles of the Studentized Range Distribution

                                           Number of Means to Be Compared

  2  3  4  5                            6  7         8  9  10  11           12     13     15     20

  5 5.70 6.98 7.80 8.42 8.91 9.32 9.67 9.97 10.24 10.48                    10.70  10.89  11.24  11.93

 6 5.24 6.33 7.03 7.56 7.97 8.32 8.61 8.87 9.10 9.30                        9.48   9.65   9.95  10.54
 7 4.95 5.92 6.54 7.00 7.37 7.68 7.94 8.17 8.37 8.55                        8.71   8.86   9.12   9.65
 8 4.75 5.64 6.20 6.62 6.96 7.24 7.47 7.68 7.86 8.03                        8.18   8.31   8.55   9.03
 9 4.60 5.43 5.96 6.35 6.66 6.91 7.13 7.33 7.49 7.65                        7.78   7.91   8.13   8.57
10 4.48 5.27 5.77 6.14 6.43 6.67 6.87 7.05 7.21 7.36                        7.49   7.60   7.81   8.23

11 4.39 5.15 5.62 5.97 6.25 6.48 6.67 6.84 6.99 7.13                        7.25   7.36   7.56   7.95
12 4.32 5.05 5.50 5.84 6.10 6.32 6.51 6.67 6.81 6.94                        7.06   7.17   7.36   7.73
13 4.26 4.96 5.40 5.73 5.98 6.19 6.37 6.53 6.67 6.79                        6.90   7.01   7.19   7.55
14 4.21 4.89 5.32 5.63 5.88 6.08 6.26 6.41 6.54 6.66                        6.77   6.87   7.05   7.39
15 4.17 4.84 5.25 5.56 5.80 5.99 6.16 6.31 6.44 6.55                        6.66   6.76   6.93   7.26

16 4.13 4.79 5.19 5.49 5.72 5.92 6.08 6.22 6.35 6.46                        6.56   6.66   6.82   7.15
17 4.10 4.74 5.14 5.43 5.66 5.85 6.01 6.15 6.27 6.38                        6.48   6.57   6.73   7.05
18 4.07 4.70 5.09 5.38 5.60 5.79 5.94 6.08 6.20 6.31                        6.41   6.50   6.65   6.97
19 4.05 4.67 5.05 5.33 5.55 5.73 5.89 6.02 6.14 6.25                        6.34   6.43   6.58   6.89
20 4.02 4.64 5.02 5.29 5.51 5.69 5.84 5.97 6.09 6.19                        6.28   6.37   6.52   6.82

 24 3.96 4.55 4.91 5.17 5.37 5.54 5.69 5.81 5.92 6.02                       6.11   6.19   6.33   6.61
 30 3.89 4.45 4.80 5.05 5.24 5.40 5.54 5.65 5.76 5.85                       5.93   6.01   6.14   6.41
 40 3.82 4.37 4.70 4.93 5.11 5.26 5.39 5.50 5.60 5.69                       5.76   5.83   5.96   6.21
 60 3.76 4.28 4.59 4.82 4.99 5.13 5.25 5.36 5.45 5.53                       5.60   5.67   5.78   6.01
120 3.70 4.20 4.50 4.71 4.87 5.01 5.12 5.21 5.30 5.37                       5.44   5.50   5.61   5.83

 3.64 4.12 4.40 4.60 4.76 4.88 4.99 5.08 5.16 5.23                          5.29   5.35   5.45   5.65

This table was computed using Mathcad.
qqqqqqqqqqqqqqqqqqqqqqqqqqqqqq

               Answers to
               Section Exercises

Chapter 1                                                Section 2

Section 1                                                1. Observational study--you might be interested in
                                                            assessing the job satisfaction of a large number
1. Designing and improving complex products and             of manufacturing workers; you could administer
   systems often leads to situations where there is no      a survey to measure various dimensions of job
   known theory that can guide decisions. Engineers         satisfaction. Experimental study--you might want
   are then forced to experiment and collect data to        to compare several different job routing schemes
   find out how a system works, usually under time          to see which one achieves the greatest throughput
   and monetary constraints. Engineers also collect         in a job shop.
   data in order to monitor the quality of products and
   services. Statistical principles and methods can be   2. Qualitative data--rating the quality of batches of
   used to find effective and efficient ways to collect     ice cream as either poor, fair, good, or exceptional.
   and analyze such data.                                   Quantitative data--measuring the time (in hours)
                                                            it takes for each of 1,000 integrated circuit chips
2. The physical world is filled with variability. It        to fail in a high-stress environment.
   comes from differences in raw materials, machin-
   ery, operators, environment, measuring devices,       3. Any relationships between the variables x and y
   and other uncontrollable variables that change over      can only be derived from a bivariate sample.
   time. This produces variability in engineering data,
   at least some of which is impossible to completely    4. You might want to compare two laboratories in
   eliminate. Statistics must therefore address the re-     their ability to determine percent impurities in rare
   ality of variability in data.                            metal specimens. Each specimen could be divided
                                                            in two, with each half going to a different lab. Since
3. Descriptive statistics provides a way of summariz-       each specimen is being measured twice for percent
   ing patterns and major features of data. Inferential     impurity, the data would be paired (according to
   statistics uses a probability model to describe the      specimen).
   process from which the data were obtained; data
   are then used to draw conclusions about the pro-
   cess by estimating parameters in the model and
   making predictions based on the model.

806
5. Full factorial data structure--tests are performed                          Answers to Section Exercises 807
   for all factor-level combinations:
                                                              produces too much variability (and this cannot be
Design  Paper         Loading Condition                       corrected by calibration). If a measurement sys-
                                                              tem is valid and precise, but inaccurate, it might
delta   construction  with clip                               be easy to make it accurate (and thus useful) by
t-wing  construction  with clip                               calibrating it to a standard.
delta   typing        with clip
t-wing  typing        with clip                            2. If the measurement system is not valid, then tak-
delta   construction  without clip                            ing an average will still produce a measurement
t-wing  construction  without clip                            that is invalid. If the individual measurements are
delta   typing        without clip                            inaccurate, then the average will be inaccurate. Av-
t-wing  typing        without clip                            eraging many measurements only improves preci-
                                                              sion. Suppose that the long-run average yield of
Fractional factorial data structure--tests are per-           the process is stable over time. Imagine making
formed for only some of the possible factor-level             5 yield measurements every hour, for 24 hours.
combinations. One possibility is to choose the fol-           This produces 120 individual measurements, and
lowing "half fraction":                                       24 averages. Since the averages are "pulled" to the
                                                              center, there will be less variability in the 24 aver-
Design  Paper         Loading Condition                       ages than in the 120 individual measurements, so
                                                              averaging improves precision.
delta   construction  without clip
t-wing  construction  with clip                            3. Unstable measurement systems (e.g., instrument
delta   typing        with clip                               drift, multiple inconsistent devices) can lead to
t-wing  typing        without clip                            differences or changes in validity, precision, and
                                                              accuracy. In a statistical engineering study, it is
6. Variables can be manipulated in an experiment.             important to obtain valid, precise, and accurate
   If changes in the response coincide with changes           measurements throughout the study. Changes or
   in factor levels, it is usually safe to infer that the     differences may create excessive variability, mak-
   changes in the factor caused the changes in the            ing it hard to draw conclusions. Changes or differ-
   response (as long as other factors have been con-          ences can also bias results by causing patterns in
   trolled and there is no source of bias). There is          data that might incorrectly be attributed to factors
   no control or manipulation in an observational             in the experiment.
   study. Changes in the response may coincide with
   changes in another variable, but there is always        Section 4
   the possibility that a third variable is causing the
   correlation. It is therefore risky to infer a cause-    1. Mathematical models can help engineers describe
   and-effect relationship between any variable and           (in a relatively simple and concise way) how phys-
   the response in an observational study.                    ical systems behave, or will behave. They are an
                                                              integral part of designing and improving products
Section 3                                                     and processes.

1. Even if a measurement system is accurate and pre-       Chapter 2
   cise, if it is not truly measuring the desired dimen-
   sion or characteristic, then the measurements are       Section 1
   useless. If a measurement system is valid and ac-
   curate, but imprecise, it may be useless because it     1. Flight distance might be defined as the horizontal
                                                              distance that a plane travels after being launched
                                                              from a mechanical slingshot. Specifically, the hor-
                                                              izontal distance might be measured from the point
                                                              on the floor directly below the slingshot to the
808 Answers to Section Exercises                             the n items chosen will be "extreme" members of
                                                             the population.
   point on the floor where any part of the plane first
   touches.                                               Section 3

2. If all operators are trained to use measuring equip-   1. Possible controlled variables: operator, launch an-
   ment in the same consistent way, this will result         gle, launch force, paper clip size, paper manu-
   in better repeatability and reproducibility of mea-       facturer, plane constructor, distance measurer, and
   surements. The measurements will be more repeat-          wind. The response is Flight Distance and the ex-
   able because individual operators will use the same       perimental variables are Design, Paper Type, and
   technique from measurement to measurement, re-            Loading Condition. Concomitant variables might
   sulting in small variability among measurements           be wind speed and direction (if these cannot be
   of the same item by the same operator. The mea-           controlled), ambient temperature, humidity, and
   surements will be more reproducible because all           atmospheric pressure.
   operators will be trained to use the same technique,
   resulting in small variability among measurements      2. Advantage: may reduce baseline variation (back-
   made by different operators.                              ground noise) in the response, making it easier to
                                                             see the effects of factors. Disadvantage: the vari-
3. This scheme will tend to "over-sample" larger lots        able may fluctuate in the real world, so controlling
   and "under-sample" smaller lots, since the amount         it makes the experiment more artificial--it will be
   of information obtained about a large population          harder to generalize conclusions from the experi-
   from a particular sample size does not depend on          ment to the real world.
   the size of the population. To obtain the same
   amount of information from each lot, you should        3. Treat "distance measurer" as an experimental
   use an absolute (fixed) sample size instead of a          (blocking) variable with 2 levels. For each level
   relative one.                                             (team member), perform a full factorial experi-
                                                             ment using the 3 primary factors. If there are differ-
4. If the response variable is poorly defined, the data      ences in the way team members measure distance,
   collected may not properly describe the character-        then it will still be possible to unambiguously as-
   istic of interest. Even if they do, operators may         sess the effects of the primary factors within each
   not be consistent in the way that they measure the        "sub-experiment" (block).
   response, resulting in more variation.
                                                          4. List the tests for Mary in the same order given for
Section 2                                                    Exercise 5 of Section 1.2. Then list the tests for
                                                             Tom after Mary, again in the same order. Label
1. Label the 38 runout values consecutively, 1-38, in        the tests consecutively 1-16, in the order listed.
   the order given in Table 1.1 (smallest to largest).       Let the digits 01-05 refer to test 1, 06-10 to test
   First sample labels: {12, 15, 5, 9, 11}; First sample     2, . . . , and 76-80 to test 16. Move through Table
   runout values: {11, 11, 9, 10, 11}. Second sample         B.1 choosing two digits at a time. Ignore previ-
   labels: {34, 31, 36, 2, 14}; Second sample runout         ously chosen test labels or numbers between 81
   values: {17, 15, 18, 8, 11}. Third sample labels:         and 00. Order the tests in the same order that their
   {10, 35, 12, 27, 30}; Third sample runout values:         corresponding two-digit numbers are chosen from
   {10, 17, 11, 14, 15}. Fourth sample labels: {15, 5,       the table. Using this method (and starting from the
   19, 11, 8}; Fourth sample runout values: {11, 9,          upper-left of the table), the test labeled 3 (Mary,
   12, 11, 10}. The samples are not identical. Note:         delta, typing, with clip) would be first, followed
   the population mean is 12.63; the sample means            by the tests labeled 13, 9, 1, 2, 7, 10, 8, 14, 11, 6,
   are 10.4, 13.8, 13.4, and 10.6.                           15, 4, 16, 12, and 5.

3. A simple random sample is not guaranteed to be
   representative of the population from which it is
   drawn. It gives every set of n items an equal chance
   of being selected, so there is always a chance that
                                                                      Answers to Section Exercises 809

5. For the delta/construction/with clip condition (for       decisions to be made in the middle of the study.
   example), flying the same plane twice would pro-          Although some may think that this is improper
   vide information about flight-to-flight variability       from a scientific/statistical point of view, it is only
   for that particular plane. This would be useful           practical to base the design of later stages on the
   if you are only interested in making conclusions          results of earlier stages.
   about that particular plane. If you are interested in
   generalizing your conclusions to all delta design      Section 4
   planes made with construction paper and loaded
   with a paper clip, then reflying the same airplane     1. If you regard student as a blocking variable, then
   does not provide much more information. But               this would be a randomized complete block ex-
   making and flying two planes for this condition           periment. Otherwise, it would just be a completely
   would give you some idea of variability among             randomized experiment (with a full factorial struc-
   different planes of this type, and would therefore        ture).
   validate any general conclusions made from the
   study. This argument would be true for all 8 con-      2. (a) Label the 24 runs as follows:
   ditions, and would also apply to comparisons made
   among the 8 conditions.                                Labels      Level of A  Level of B  Level of C

6. Random sampling is used in enumerative studies.        1, 2, 3          1           1           1
   Its purpose is to choose a representative sample       4, 5, 6          2           1           1
   from some population of items. Randomization           7, 8, 9          1           2           1
   is used in analytical/experimental studies. Its pur-   10, 11, 12       2           2           1
   pose is to assign units to experimental conditions     13, 14, 15       1           1           2
   in an unbiased way, and to order procedures to         16, 17, 18       2           1           2
   prevent bias from unsupervised variables that may      19, 20, 21       1           2           2
   change over time.                                      22, 23, 24       2           2           2

7. Blocking is a way of controlling an extraneous         Use the following coding for the test labels: ta-
   variable: within each block, there may be less base-   ble number 01-04 for test label 1, table number
   line variation (background noise) in the response      05-08 for test label 2, . . . , table number 93-96 for
   than there would be if the variable were not con-      test number 24. Move through Table B.1 choosing
   trolled. This makes it easier to see the effects of    two digits at a time, ignoring numbers between 97
   the factors of interest within each block. Any ef-     and 00 and those corresponding to test labels that
   fects of the extraneous variable can be isolated and   have already been picked. Order the tests in the
   distinguished from the effects of the factors of in-   same order that their corresponding two-digit num-
   terest. Compared to holding the variable constant      bers are picked from the table. Using this method,
   throughout the experiment, blocking also results       and starting from the upper-left corner of the ta-
   in a more realistic experiment.                        ble, the order would be 3, 4, 24, 16, 11, 2, 9, 12,
                                                          17, 8, 21, 1, 13, 7, 18, 5, 20, 14, 19, 15, 22, 23,
8. Replication is used to estimate the magnitude of       6, 10. (b) Treat day as a blocking variable, and
   baseline variation (background noise, experimen-       run each of the 8 factor-level combinations once on
   tal error) in the response, and thus helps sharpen     each day. Blocking allows comparisons among the
   and validate conclusions drawn from data. It pro-      factor-level combinations to be made within each
   vides verification that results are repeatable and     day. If blocking were not used, differences among
   establishes the limits of that repeatability.          days might cause variation in the response which
                                                          would cloud comparisons among the factor-level
9. It is not necessary to know exactly how the entire
   budget will be spent. Experimentation in engineer-
   ing is usually sequential, and this requires some
810 Answers to Section Exercises

combinations. (c) List the 8 factor-level combi-         Day Level of A Level of B Level of C
nations separately for each day. For each day, label
the runs as follows:                                     3        1                1                1

                                                         3        1                2                1

Label  Level of A  Level of B     Level of C             3        2                1                1

   1        1           1              1                 3        1                2                2
  2         2           1              1
  3         1           2              1                 3        2                2                1
  4         2           2              1
  5         1           1              2                 3        1                1                2
  6         2           1              2
  7         1           2              2                 3        2                2                2
  8         2           2              2
                                                         3        2                1                2

                                                            Part (a) randomized all 24 runs together; here, each
                                                            block of 8 runs is randomized separately.

                                                         3. The factor Person is the "block" variable.

For each day, move through Table B.1 one digit                    Block    Design     Paper
at a time ignoring the digits 9 and 0 and any that
have already been picked. Order the 8 runs in the                 Tom      delta      construction
same order that the numbers were picked from the                  Tom      t-wing     typing
table. Starting from where I left off in part (a), the            Juanita  delta      typing
order for day 1 is 5, 3, 8, 4, 1, 2, 6 (which implies             Juanita  t-wing     construction
that run 7 goes last). For day 2, the order is 5, 1, 8,
7, 2, 3, 6 (which implies that run 4 goes last). For     4. Focusing on Design, you would want each per-
day 3, the order is 1, 3, 2, 7, 4, 5, 8, (which implies     son to test two delta-wing planes and two t-wing
that run 6 goes last).                                      planes; this would allow you to clearly compare the
                                                            two designs. You could separately compare the de-
   The plan is summarized below:                            signs "within" each person. If possible, you would
                                                            want a plan such that this is true for all three pri-
Day Level of A Level of B Level of C                        mary factors, simultaneously. This is possible by
                                                            using the same pattern that is used in Table 2.6:
1      1           1              2

1      1           2              1

1      2           2              2

1      2           2              1                      Person   Design   Paper         Loading Condition

1      1           1              1                      Juanita  delta    construction  with clip
                                                         Tom      t-wing   construction  with clip
1      2           1              1                      Tom      delta    typing        with clip
                                                         Juanita  t-wing   typing        with clip
1      2           1              2                      Tom      delta    construction  without clip
                                                         Juanita  t-wing   construction  without clip
1      1           2              2                      Juanita  delta    typing        without clip
                                                         Tom      t-wing   typing        without clip
2      1           1              2

2      1           1              1

2      2           2              2

2      1           2              2

2      2           1              1

2      1           2              1                      This design also allows each person to test each
                                                         Design/Paper combination once, each Design/
2      2           1              2

2      2           2              1
   Loading combination once, and each Paper/Load-                               Answers to Section Exercises 811
   ing combination once.
                                                               value (outlier). (c) The nonlinearity of the Q-
5. This is an incomplete block experiment.                      Q plot indicates that the overall shapes of these
                                                               two data sets are not the same. The lengthwise
Section 5                                                      cuts had an unusually large data point ("long right
                                                               tail"), whereas the crosswise cuts had an unusu-
1. A cause-and-effect diagram may be useful for rep-           ally small data point ("long left tail"). Without
   resenting a complex system in a relatively sim-             these two outliers, the data sets would have simi-
   ple and visual way. It enables people to see how            lar shapes, since the rest of the Q-Q plot is fairly
   the components of the system interact, and may              linear.
   help identify areas which need the most atten-
   tion/improvement.                                        2. Use the (i - .5)/n quantiles for the smaller data
                                                               set. The plot coordinates are: (.370, .907), (.520,
Chapter 3                                                      1.22), (.650, 1.47), (.920, 1.70), (2.89, 2.45), (3.62,
                                                               5.89).
Section 1
                                                            3. The first 3 plot coordinates are: (65.6, -2.33),
1. One choice of intervals for the frequency table and         (65.6, -1.75), (66.2, -1.55). The normal plot is
   histogram is 65.5-66.4, 66.5-67.4, . . . , 73.5-74.4.       quite linear, indicating that the data are very bell-
   For this choice, the frequencies are 3, 2, 9, 5, 8, 6,      shaped.
   2, 3, 2; the relative frequencies are .075, .05, .225,
   .125, .2, .15, .05, .075, .05; the cumulative relative   4. Theoretical Q-Q plotting allows you to roughly
   frequencies are .075, .125, .35, .475, .675, .825,          check to see if a data set has a shape that is similar
   .875, .95, 1. The plots reveal a fairly symmetric,          to some theoretical distribution. This can be use-
   bell-shaped distribution.                                   ful in identifying a theoretical (probability) model
                                                               to represent how the process is generating data.
2. The plots show that the depths for 200 grain bullets        Such a model can then be used to make inferences
   are larger and have less variability than those for         (conclusions) about the process.
   the 230 grain bullets.
                                                            Section 3
3. (a) There are no obvious patterns. (b) The dif-
   ferences are -15, 0, -20, 0, -5, 0, -5, 0, -5,           1. For the lengthwise cuts: x¯ = .919, Median = .895,
   20, -25, -5, -10, -20, and 0. The dot diagram                R = .310, IQR = .060, s = .088. For the cross-
   shows that most of the differences are negative             wise cuts: x¯ = .743, Median = .775, R = .430,
   and "truncated" at zero. The exception is the tenth         IQR = .110, s = .120. The sample means and me-
   piece of equipment, with a difference of 20. This           dians show that the center of the distribution for
   point does not fit in with the shape of the rest of         lengthwise cuts is higher than the center for cross-
   the differences, so it is an outlier. Since most of the     wise cuts. The sample ranges, interquartile ranges,
   differences are negative, the bottom bolt generally         and sample standard deviations show that there
   required more torque than the top bolt.                     is less spread in the lengthwise data than in the
                                                               crosswise data.
Section 2
                                                            2. These values are statistics. They are summariza-
1. (a) For the lengthwise sample: Median = .895,               tions of two samples of data, and do not represent
    Q(.25) = .870, Q(.75) = .930, Q(.37) = .880.               exact summarizations of larger populations or the-
   For the crosswise sample: Median = .775,                    oretical (long-run) distributions.
    Q(.25) = .690, Q(.75) = .800, Q(.37) = .738.
   (b) On the whole, the impact strengths are larger        4. In the first case, the sample mean and median in-
   and more consistent for lengthwise cuts. Each               crease by 1.3, but none of the measures of spread
   method also produced an unusual impact strength             change; in the second case, all of the measures
                                                               double.
812 Answers to Section Exercises

Section 4                                                             The implied relationship between x and y is y =
                                                                      e0 x 1 .
1. p^ = the proportion of part orders that are delivered
   on time to the factory floor. u^ = number of defects            Section 2
   per shift produced on an assembly line. A mea-
   sured value of 65% yield for a run of a chemical                1. y^ = -1315 + 5.6x + .04212x2. R2 = .996. For
   process is of neither form.                                        the quadratic model, at x = 200 C, y^ = 1487.2,
                                                                      which is relatively close to 1525.1 from part (e) of
2.  p^ Laid  =  6   =  .158.  p^ Hung  =  24  =  .615.  Most  en-     Exercise 3 of Section 1.
                38                        39
    gineering situations call for minimizing variation.            2. (a) y^ = 6.0483 + .14167x1 - .016944x2. b1 =
                                                                      .14167 means that as x1 increases by 1% (holding
    The p^ values do not give any indication of how                   x2 constant), y increases by roughly .142 cm3/g.
                                                                      b2 = -.016944 means that as x2 increases by
    much spread there is in each set of data, and would               one minute (holding x1 constant), y decreases by
                                                                      roughly .017 cm3/g. R2 = .807. (b) The resid-
    not be helpful in comparing the two methods with                  uals are -.015, .143, .492, -.595, -.457, -.188,
                                                                      .695, .143, -.218. (c) For x2 = 30, the equa-
    respect to variation.                                             tion is y^ = 5.53998 + .14167x1. For x2 = 60, the
                                                                      equation is y^ = 5.03166 + .14167x1. For x2 = 90,
3. Neither type. These rates represent continuous                     the equation is y^ = 4.52334 + .14167x1. The fit-
   measurements on each specimen; there is no                         ted responses do not match up well, because the
   "counting" involved.                                               relationship between y and x1 is not linear for
                                                                      any of the x2 values. (d) At x1 = 10% and x2 =
Chapter 4                                                             70 minutes, y^ = 6.279 cm3/g. It would not be
                                                                      wise to make a similar prediction at x1 = 10%
Section 1                                                             and x2 = 120 minutes because there is no evi-
                                                                      dence that the fitted relationship is correct un-
1. (a) y^ = 9.4 - 1.0x (b) r = -.945 (c) r =                          der these conditions. Some data should be ob-
   .945. This is the negative of the r in part (b), since             tained around x1 = 10% and x2 = 120 minutes.
   the y^ 's are perfectly negatively correlated with the             (e) y^ = 4.98 + .260x1 + .00081x2 - .00197x1x2,
   x's. (d) R2 = .893 = r 2 from both (b) and (c).                    and R2 = .876. The increase in R2 from .807 to
   (e) -.4, .6, -.4, .6, -.4. These are the vertical                  .876 is not very large; using the more compli-
   distances from each data point to the least squares                cated equation may not be desirable (this is sub-
   line.                                                              jective). (f) For x2 = 30, the equation is y^ =
                                                                      5.0076 + .20084x1. For x2 = 60, the equation is
3. (a) R2 = .994 (b) y^ = -3174.6 + 23.50x. 23.5                      y^ = 5.0319 + .14168x1. For x2 = 90, the equa-
   (c) Residuals: 105.36, -21.13, -60.11, -97.58,                     tion is y^ = 5.0562 + .08252x1. The new model
   16.95, 14.48, 42.00, .02. (d) There is no replica-                 allows there to be a different slope for different
   tion (multiple experimental runs at a particular pot               values of x2, so these lines fit the data better
   temperature). (e) For x = 188 C, y^ = 1243.1.                      than the lines in part (c). But they still do not
   For x = 200 C, y^ = 1525.1. It would not be wise                   account for the nonlinearity between x1 and y. An
   to make a similar prediction at x = 70 C because                   x12 term should be added to the model. (g) There
   there is no evidence that the fitted relationship is               is no replication (multiple experimental runs at a
   correct for pot temperatures as low as x = 70 C.                   particular NaOH/Time combination). (h) These
   Some data should be obtained around x = 70 C.                      data have a complete (full) factorial structure.
                                                                      The straight-line least squares equation for x1 is
4. (a) The scatterplot is not linear, so the given
   straight-line relationship does not seem appro-
   priate. R2 = .723. (b) This scatterplot is much
   more linear, and a straight-line relationship seems
   appropriate for the transformed variables. R2 =
   .965. (c) ln y = 34.344 - 5.1857 ln x. For x =
   550, ln y = 1.6229 so y^ = e1.6229 = 5.07 minutes.
   y^ = 5.0317 + .14167x1 with a corresponding R2                               Answers to Section Exercises 813
   of .594. The straight-line least squares equation for
   x2 is y^ = 7.3233 - .01694x2 with a correspond-             are the only effects on Time. (b) y¯ ··· = 2.699,
   ing R2 of .212. The slopes in these one-variable            a2 = .006, b2 = -.766, ab22 = -.003, c2 = .271,
   linear equations are the same as the corresponding          ac22 = -.003, bc22 = -.130, abc222 = .007. Yes,
   slopes in the two variable equation from (a). The           but the Diameter × Fluid interaction still seems
    R2 value in (a) is the sum of the R2 values from           to be important. (c) In standard order, the fitted
   the two one-variable linear equations.                      values are 3.19, 3.19, 1.66, 1.66, 3.74, 3.74, 2.20,
                                                               2.20. R2 = .974. For a model with all factorial
Section 3                                                      effects (ln yi jk = ln yi jk), R2 = .995. (d) b1 -
                                                               b2 = 1.532 ln (sec) decrease; divide the .188 raw
1. (a) Labeling x1 as A and x2 as B, a1 = -.643, a2 =          drain time by e1.532 to get the .314 drain time. This
   -.413, a3 = 1.057, b1 = .537, b2 = -.057, b3 =              suggests that (.188 drain time/.314 drain time) =
   -.480, ab11 = -.250, ab12 = -.007, ab13 =                   e1.532 = 4.63; the theory predicts this ratio to be
   .257, ab21 = -.210, ab22 = .013, ab23 = .197,               7.78.
   ab31 = .460, ab32 = -.007, ab33 = -.453. The
   fitted interactions ab31 and ab33 are large (rela-       3. Interpolation, and possibly some cautious extrapo-
   tive to fitted main effects) indicating that the effect     lation, is only possible using surface-fitting meth-
   on y of changing NaOH from 9% to 15% de-                    ods. In many engineering situations, an "optimal"
   pends on the Time (non-parallelism in the plot).            setting of quantitative factors is sought. This can
   It would not be wise to use the fitted main effects         be facilitated by interpolation (or extrapolation)
   alone to summarize the data, since there may be             using a surface-fitting model.
   an importantly large interaction. (b) y^ 11 = 6.20,
   y^ 12 = 5.61, y^ 13 = 5.18, y^ 21 = 6.43, y^ 22 = 5.84,  Section 4
   y^ 23 = 5.41, y^ 31 = 7.90, y^ 32 = 7.31, y^ 33 = 6.88.
   Like the plot in part (c) and unlike the plot in         1. Transforming data can sometimes make relation-
   (f) of Exercise 2 in Section 4.2, the fitted val-           ships among variables simpler. Sometimes nonlin-
   ues for each level of B (x2) must produce parallel          ear relationships can be made linear, or factors and
   plots; no interactions are allowed. However, un-            response can be transformed so that there are no in-
   like parts (c) and (f) of that exercise, the current        teractions among the factors. Transformations can
   model allows these fitted values to be nonlinear            also potentially make the shape of a distribution
   in x1 (factorial models are generally more flexible         simpler, allowing the use of statistical models that
   than lines, curves, and surfaces). (c) R2 = .914.           assume a particular distributional shape (such as
   The plots of residuals versus Time and residuals            the bell-shaped normal distribution).
   versus y^ i both have patterns; these show that the
   "main effects only" model is not accounting for          2. In terms of the raw response, there will be interac-
   the apparent interaction between the two factors.           tions, since x1 and x2 are multiplied together in the
   Even though R2 is higher than both of the models            power law. The suggested plot of raw y versus x1
   in Exercise 2 of Section 4.2, this model does not           will have different slopes for different values of x2.
   seem to be adequate.                                        This means that the effect of changing x1depends
                                                               on the setting of x2, which is one way to define an
2. (a) y¯ ··· = 20.792, a2 = .113, b2 = -13.807,               interaction.
   ab22 = -.086, c2 = 7.081, ac22 = -.090, bc22 =                 In terms of the log of y, there will not be inter-
   -6.101, abc222 = .118. Other fitted effects can             actions, since x1 and x2 appear additively in the
   be obtained by appropriately changing the signs             equation for ln y. Therefore, the suggested plot of
   of the above. The simplest possible interpreta-             ln y versus x1 will have the same slope for all val-
   tion is that Diameter, Fluid, and their interaction         ues of x2. This means that the effect of changing
                                                               x1 does not depend on the setting of x2 (there are
                                                               no interactions).
814 Answers to Section Exercises

Section 5                                                                 First Second

1. A deterministic model is used to describe a sit-                       Item Item     x¯  s2 Probability

uation where the outcome can be almost exactly

predicted if certain variables are known. A stochas-                      2       3     2.5 .5   1
                                                                                                 15
tic/probabilistic model is used in situations where
                                                                          2       41    3.0 2.0  1
it is not possible to predict the exact outcome.                                                 15

This may happen when important variables are                              2       42    3.0 2.0  1
                                                                                                 15

unknown, or when no known deterministic the-                              2       5     3.5 4.5  1
                                                                                                 15

ory can describe the situation. An example of a                           2       6     4.0 8.0  1
                                                                                                 15
deterministic model is the classical Economic Or-
                                                                          3       41    3.5 .5   1
der Quantity (EOQ) model for inventory control.                                                  15

Given constant rate of demand R, order quantity                           3       42    3.5 .5   1
                                                                                                 15

X , ordering cost P, and per unit holding cost C, the                     3       5     4.0 2.0  1
                                                                                                 15
                                                  R          2X .
total cost per time period is Y = P               X  +C                   3       6     4.5 4.5  1
                                                                                                 15

Chapter 5                                                                 41      42    4.0 0    1
                                                                                                 15

                                                                          41      5     4.5 .5   1
                                                                                                 15
Section 1
                                                                          41      6     5.0 2.0  1
1. (b) 4.1; 1.136.                                                                               15

                                                                          42      5     4.5 .5   1
                                                                                                 15
2. (a) X has a binomial distribution with n = 10 and
                                                                          42      6     5.0 2.0  1
   =  1                                        =             =  31 .                             15
p     3    .  U  se  equation  (5.3)  with  n     10 and  p
                                                                          5       6     5.5 .5   1
f (0)- f (10) are .0173, .0867, .1951, .2601, .2276,                                             15

.1366, .0569, .0163, .0030, .0003, .0000. (b) As-                        Using the above table, the probability distribu-
                                                                      tion for X is:
suming that they are just guessing, the chance that

7 (or more) out of 10 subjects would be correct

is P(X  7) = .0197. Under the hypothesis that                         x¯          2.5 3 3.5 4 4.5 5 5.5

they are only guessing, this kind of extreme out-

come would only happen about 1 in 50 times, so                        P (X = x¯ ) 15 15 15 15 15 15 15 1 2 3 3 3 2 1

the outcome is strong evidence that they are not

just guessing.                                                        Using equations (5.1) and (5.2), E X = 4 and
                                                                      VarX = 32 . As might be expected, the mean of
3. (a) Using equations (3.4) and (3.5), µ = 4,
    2 = 35 , and  = 1.291.                                            X is the same as the mean of X , and the variance
   (b)                                                                is smaller. The probability distribution for S2 is

                 x   23456                                                    s2        0 .5 2 4.5 8

           P (X = x ) 1 1 2 1 1 6 6 6 6 6                                 P (S2 = s2) 15 15 15 15 15 1 6 5 2 1

Since all members of the population are equally                       4. For p = .1, f (0)- f (5) are .59, .33, .07, .01, .00,
likely to be chosen, the probability histogram for                       .00; µ = np = .5;  = np(1 - p) = .67. For
X is the same as the population relative frequency                        p = .3, f (0)- f (5) are .17, .36, .31, .13, .03, .00;
distribution. Using equations (5.1) and (5.2),                           µ = 1.5;  = 1.02. For p = .5, f (0)- f (5) are
EX = 4 and VarX = 35 . (c) Label the values 2, 3,                        .03, .16, .31, .31, .16, .03; µ = 2.5;  = 1.12. For
41, 42, 5, 6.
                                                                               Answers to Section Exercises 815

    p = .7, f (0)- f (5) are .00, .03, .13, .31, .36, .17;            is not a subfield of probability just as engineering
   µ = 3.5;  = 1.02. For p = .9, f (0)- f (5) are                     is not a subfield of calculus; many simple statisti-
   .00, .00, .01, .07, .33, .59; µ = 4.5;  = .67.                     cal methods do not require the use of probability,
                                                                      and many engineering techniques do not require
 5. Binomial distribution: n = 8, p = .20. (a) .147                   calculus.
     (b) .797 (c) np = 1.6 (d) np(1 - p) = 1.28
     (e) 1.13                                                    11. A relative frequency distribution is based on data.
                                                                      A probability distribution is based on a theoreti-
 6. Geometric distribution: p = .20. (a) .08 (b) .59                  cal model for probabilities. Since probability can
     (c) 1/ p = 5 (d) (1 - p)/ p2 = 20 (e) 4.47                       be interpreted as long-run relative frequency, a
                                                                      relative frequency distribution approximates the
 7. For  = .5, f (0), f (1), . . . are .61, .30, .08, .01,            underlying probability distribution, with the ap-
     .00, .00, . . . ; µ =  = .5;  =  = .71. For                      proximation getting better as the amount of data
      = 1.0, f (0), f (1), . . . are .37, .37, .18, .06,              increases.
     .02, .00, .00, . . . ; µ = 1.0;  = 1.0. For  = 2.0,
     f (0), f (1), . . . are .14, .27, .27, .18, .09, .04, .01,  Section 2                  for x  0
     .00, .00, . . . ; µ = 2.0;  = 1.41. For  = 4.0,                                        for 0 < x < 1
     f (0), f (1), . . . are .02, .07, .15, .20, .20, .16, .10,  1. (a) 2/9 (c) .5          for x  1
     .06, .03, .01, .00, .00, . . . ; µ = 4.0;  = 2.0.                             
                                                                                   0
 8. (a) .323 (b) .368                                                              

 9. (a) .0067 (b) Y  Binomial(n = 4, p = .0067);                    (d) F (x ) = 10x-x2  9
     .00027                                                                        1

10. Probability is a mathematical system used to de-                (e) 13/27; .288
     scribe random phenomena. It is based on a set of
     axioms, and all the theory is deduced from the              2. (a) .2676  (b) .1446    (c) .3393 (d) .3616
     axioms. Once a model is specified, probability                 (e) .3524  (f) .9974    (g) 1.28 (h) 1.645
     provides a deductive process that enables predic-              (i) 2.17
     tions to be made based on the theoretical model.
        Statistics uses probability theory to describe           3. (a) .7291 (b) .3594 (c) .2794 (d) .4246
     the source of variation seen in data. Statistics tries         (e) .6384 (f) 48.922 (g) 44.872. (h) 7.056
     to create realistic probability models that have
     (unknown) parameters with meaningful interpre-              4. (a) .4938 (b) Set µ to the midpoint of the speci-
     tations. Then, based on observed data, statistical             fications: µ = 2.0000; .7888 (c) .0002551
     methods try to estimate the unknown parame-
     ters as accurately and precisely as possible. This          5. (a) P(X < 500) = .3934; P(X > 2000) = .1353
     means that statistics is inductive, using data to              (b) Q(.05) = 51.29; Q(.90) = 2,302.58
     draw conclusions about the process or popula-
     tion from which the data came.                              6. (b) Median = 68.21 × 106 (c) Q(.05) =
        Neither is a subfield of the other. Just as engi-           21.99 × 106; Q(.95) = 128.9 × 106
     neering uses calculus and differential equations
     to model physical systems, statistics uses proba-           Section 3
     bility to model variation in data. In each case the
     mathematics can stand alone as theory, so calcu-            1. Data that are being generated from a particular dis-
     lus is not a subfield of engineering and probability           tribution will have roughly the same shape as the
     is not a subfield of statistics. Conversely, statistics        density of the distribution, and this is more true
                                                                    for larger samples. Probability plotting provides
                                                                    a sensitive graphical way of deciding if the data
                                                                    have the same shape as a theoretical probability
816 Answers to Section Exercises                                    3. (a) For y = 1, 2, 3, 4, fY |X (y | 0) = 0, 0, 0, 1
                                                                       and fY |X (y | 1) = .25, .25, .25, .25. f (0, 1) =
   distribution. If a distribution can be found that ac-                f (0, 2) = f (0, 3) = 0, f (0, 4) = p, f (1, 1) =
   curately describes the data generating process, one                  f (1, 2) = f (1, 3) = f (1, 4) = .25(1 - p).
   can then estimate probabilities and quantiles and                   (b) 2.5 + 1.5 p (c) p > .143
   make predictions about future process behavior
   based on the model.                                              4. (a) Since X and Y are independent, f (x, y) =

2. Fit a line (by eye or some other method) through                 fX (x) fY (y) (Definition 27),
   the points on the plot. The x-intercept is an approx-
   imate mean, and an approximate standard devia-                               1 1            for x  (1.97, 2.02) and
   tion is   slope 1 = yx = std. normal quantiles data quantiles .               .05 .06       y  (2.00, 2.06)
                                                                    f (x, y) = 
3. (b) µ  69.5;   1/slope = 2.1                                                                otherwise
                                                                                   0
4. (a) First 3 coordinates of the normal plot of the
   raw data: (17.88, -2.05), (28.92, -1.48), (33.00,                                           for x  (1.97, 2.02) and
   -1.23). The normal plot is not linear, so a Gaus-                             333.33        y  (2.00, 2.06)
   sian (normal) distribution does not seem to fit                            =
   these data. First 3 coordinates of the normal plot                                          otherwise
   of the natural log of the data: (2.884, -2.05),                                 0
   (3.365, -1.48), (3.497, -1.23). This normal plot
   is fairly linear, indicating that a lognormal distri-            (b) Find the volume below this density over the re-
   bution fits the data well. µ  4.1504,   .5334.                   gion in which 2.00 < y < x and 1.97 < x < 2.02.
   3.273; 26.391. (b) The first 3 coordinates of the                This is .0667. (Using calculus, this is
   Weibull plot are (2.88, -3.82), (3.36, -2.70),
   (3.50, -2.16). The Weibull plot is fairly linear, in-            2.02 x
   dicating that a Weibull distribution might be used
   to describe bearing load life.   81.12,   2.3;                           333.33 dy dx.)
   22.31.
                                                                    2.00 2.00
5. (b) The exponential plot is fairly linear, indicating
   that an exponential distribution fits the data well.             5. (a)                     for 0  x  1
   Since a line on the plot indicates that Q(0)  0, no                                     2x                    ;
   need for a threshold parameter greater than zero is
   indicated.                                                                 fX(x) = 0        otherwise

Section 4                                                           fY (y) =              2(1 - y)  for 0  y  1
                                                                                          0                           ;
1. If X and Y are independent, then observing the ac-
   tual value of X does not in any way change proba-                                                otherwise
   bility assessments about the yet-to-be-observed Y ,
   or vice-versa. Independence provides great mathe-                µ = E X = 2/3.                                  (c) .7083
   matical simplicity in the description of the behav-              (b) Yes, since f (x, y) = fX (x) fY (y).
   ior of X and Y .                                                 (d) E(X |Y = .5) = 2/3

2. (a) For x = 0, 1, 2, fX (x) = .5, .4, .1. For y =                6. (a)                e-x e-y   if x  0 and y  0
   0, 1, 2, 3, 4, fY (y) = .21, .19, .26, .21, .13.                           f (x, y) =  0         otherwise
   (b) No, since f (x, y) = fX (x) fY (y). (c) .6; .44
   (d) 1.86; 1.74 (e) For y = 0, 1, 2, 3, 4, fY |X (y |                  fX(x) fY (y) =
   0) = .3, .2, .2, .2, .1; 1.6.
                                                                    (b) e-2t  (c) fT (t) =     2e-2t  for t  0
                                                                                               0                   .

                                                                                                      otherwise

                                                                    This is an exponential distribution with mean .5.
                                                                    (d) (1 - e-t )2.
(e) fT (t) =  2e-t (1 - e-t )  for t  0;                                     Answers to Section Exercises 817
E(T ) = 1.5   0                              .
                                                         2. (a) [111.0, 174.4] (b) [105.0, 180.4] (c) 167.4
                               otherwise                    (d) 174.4 (e) [111.0, 174.4] ppm is a set of plau-
                                                            sible values for the mean aluminum content of
Section 5                                                   samples of recycled PET plastic from the recycling
                                                            pilot plant at Rutgers University. The method used
1. mean = .75 in.; standard deviation = .0037.              to construct this interval correctly contains means
                                                            in 90% of repeated applications. This particular in-
2. (a) Propagation of error formula gives 1.4159 ×          terval either contains the mean or it doesn't (there
   10-6. (b) The lengths.                                   is no probability involved). However, because the
                                                            method is correct 90% of the time, we might say
3. (a) 13/27; .0576 (b) X  Normal with mean                 that we have 90% confidence that it was correct
   13/27 and standard deviation .0576. (c) .3745            this time.
   (d) .2736 (e) 13/27, .0288; X  Normal with
   mean 13/27 and standard deviation .0288; .2611;       3. n = 66
   .5098.
                                                         4. (a) x¯ = 4.6858 and s = .02900317 (b) =
4. .7888, .9876, 1.0000                                     [4.676, 4.695] mm (c) [4.675, 4.696] mm. This
                                                            interval is wider than the one in (b). To increase
5. Rearrange the relationship in terms of g to get          the confidence that µ is in the interval, you need to
   g = 2 42L . Take the given length and period to be       make the interval wider. (d) The lower bound is
                                                            4.677 mm. This is larger than the lower endpoint
                                                            of the interval in (b). Since the upper endpoint here
                                                            is set to , the lower endpoint must be increased
   approximately equal to the means of these input          to keep the confidence level the same. (e) To
   random variables. To use the propagation of error        make a 99% one-sided interval, construct a 98%
   formula, the partial derivatives need to be eval-        two-sided interval and use the lower endpoint. This
   uated at the means of the input random variables         was done in part (a), and the resulting lower bound
   and  L g =  2 42 = 6.418837 and g =  3 -82 L =           is 4.676. This is smaller than the value in (d); to
   -25.8824089. Then applying equation (5.59),              increase the confidence, the interval must be made
   Var(g)  (6.418837)2(.0208)2 +(-25.8824089)2              "wider." (f) [4.676, 4.695] ppm is a set of plau-
   × (.1)2 = 6.7168 ft2/sec4 so the approximate stan-       sible values for the mean diameter of this type
   dard deviation of g is 6.7168 = 2.592 ft/sec2.           of screw as measured by this student with these
   The precision in the period measurement is the           calipers. The method used to construct this inter-
   principal limitation on the precision of the derived     val correctly contains means in 98% of repeated
   g because its term (variance × squared partial           applications. This particular interval either con-
   derivative) contributes much more to the propaga-        tains the mean or it doesn't (there is no probability
   tion of error formula than the length's term.            involved). However, because the method is correct
                                                            98% of the time, we might say that we have 98%
Chapter 6                                                   confidence that it was correct this time.

Section 1                                                Section 2

1. [6.3, 7.9] ppm is a set of plausible values for the   1. H0 : µ = 200; Ha : µ > 200; z = -2.98; p-
   mean. The method used to construct this interval                 .
   correctly contains the true mean in 95% of re-
   peated applications. This particular interval either     value = .9986. There is no evidence that the mean
   contains the mean or it doesn't (there is no prob-       aluminum content for samples of recycled plastic
   ability involved). However, because the method is        is greater than 200 ppm.
   correct 95% of the time, we might say that we have
   95% confidence that it was correct this time.
818 Answers to Section Exercises                             strong evidence that the mean torque is not 100
                                                             ft lb. (c) [104.45, 117.55] (d) Independence
2. (a) H0 : µ = .500; Ha : µ = .500; z = 1.55; p-            among assemblies; normal distribution for differ-
           .                                                 ences. (e) H0: µd = 0; Ha: µd < 0 (where dif-
                                                             ferences are Top - Bottom); t = -2.10 on 14 df;
   value = .1212. There is some (weak) evidence that         .025 < p-value < .05. (f) [-13.49, 1.49]
   the mean punch height is not .500 in. (The rounded
   x¯ and s given produce a z that is quite a bit dif-    3. (a) [-0.0023, .0031] mm (b) H0: µd = 0; Ha:
   ferent from what the exact values produce. x¯ =           µd = 0 ; z = .24; p-value = .8104. There is no ev-
   .005002395 and s = .002604151, computed from              idence of a systematic difference between calipers.
   the raw data, produce z = 1.85, and a p-value of          (c) The confidence interval in part (a) contains
   2(.0322) = .0644.) (b) [.49990, .50050] (c) If            zero; in fact, zero is near the middle of the inter-
   uniformity of stamps on the same piece of material        val. This means that zero is a very plausible value
   is important, then the standard deviation (spread)        for the mean difference--there is no evidence that
   of the distribution of punch heights will be impor-       the mean is not equal to zero. This is reflected by
   tant (in addition to the mean).                           the large p-value in part (b).

3. The mean of the punch heights is almost cer-           4. (a) The data within each sample must be iid nor-
   tainly not exactly equal to .50000000 inches. Given       mal, and the two distributions must have the same
   enough data, a hypothesis test would detect this as       variance  2. One way to check these assumptions
   a "statistically significant" difference (and produce     is to normal plot both data sets on the same axes.
   a small p-value). What is practically important is        For such small sample sizes, it is difficult to defini-
   whether the mean is "close enough" to .500 inches.        tively verify the assumptions. But the plots are
   The confidence interval in part (b) answers this          roughly linear with no outliers, indicating that the
   more practical question.                                  normal part of the assumption may be reasonable.
                                                             The slopes are similar, indicating that the common
4. H0 : µ = 4.70; Ha : µ = 4.70 ; z = -3.46; p-              variance assumption may be reasonable. (b) La-
           .                                                 bel the Treaded data Sample 1 and the Smooth
                                                             data Sample 2. H0 : µ1 - µ2 = 0; Ha : µ1 - µ2 =
   value = .0006. There is very strong evidence that         0; t = 2.49; p-value is between .02 and .05. This
   the mean measured diameter differs from nominal.          is strong evidence of a difference in mean skid
                                                             lengths. (c) [2.65, 47.35] (d) [2.3, 47.7]
5. Although there is evidence that the mean is not
   equal to nominal, the test does not say anything       Section 4
   about how far the mean is from nominal. It may
   be "significantly" different from nominal, but the     1. (a) [9.60, 37.73] (b) 57.58 (c) H0 : 2 T2 = 1
   difference may be practically unimportant. A con-
   fidence interval is what is needed for determining                                                                                    S
   how far the mean is from nominal.
                                                             Ha : 2 T2 = 1; f = .64 on 5,5 df; p-value > .50
Section 3
                                                                       S
1. The normal distribution is bell-shaped and sym-
   metric, with fairly "short" tails. The confidence         (d) [.36, 1.80]
   interval methods depend on this regularity. If the
   distribution is skewed or prone to outliers/extreme    2. (a) [7.437, ) (b) [44.662, ) (c) Top and
   observations, the normal-theory methods will not          bottom bolt torques for a given piece are probably
   properly take this into account. The result is an         not sensibly modeled as independent.
   interval whose real confidence level is different
   from the nominal value (and often lower than the       Section 5
   nominal value).
                                                          1. (a) Conservative method: [.562, .758]; .578. Other
2. (a) Independence among assemblies; normal dis-            method: [.567, .753]; .582. (b) H0 : p = .55; Ha :
   tribution for top-bolt torques. (b) H0: µ = 100;
                                            .
   Ha: µ = 100 ; t = 4.4; p-value = .001. There is
    p > .55; z = 2.21; p-value = .0136. (c) Con-                              Answers to Section Exercises 819
   servative method: [-.009, .269]. Other method:
   [-.005, .265]. (d) H0 : pS - pL = 0; Ha : pS -         Chapter 7
    pL = 0; z = 1.87; p-value = .0614.
                                                          Section 1
2. 9604
                                                          1. (a) The plot reveals two outliers. The assumptions
3. Conservative method: [.22, .35]. Other method:            of the one-way normal model appear to be less
   [.23, .34].                                               than perfectly met in this problem. (Both of the
                                                             outliers come from the 8,000 psi condition. This
4. H0 : p1 - p2 = 0; Ha : p1 - p2 = 0; z = -.97; p-          is an indication that the common  part of the
   value = .3320.                                            one-way normal model may be less than perfect.)
                                                             (b) .02057. This measures the magnitude of base-
Section 6                                                    line variation in any of the five treatments, assum-
                                                             ing it is the same for all five treatments; [.01521,
1. A consumer about to purchase a single auto would          .03277].
   be most interested in a prediction bound, because
   the single auto that the consumer will purchase is     2. (a) The plot reveals one outlier/unusual residual
   likely to have mileage above the bound. This is not       (the 1.010 value from Van #1 produces the residual
   true for a confidence bound for the mean. That may        -.0094). One should proceed under the one-way
   be more useful for the EPA official, since this per-      model assumptions only with caution. (b) The
   son wants to be sure that the manufacturer is pro-        standardized residuals tell the same story told in
   ducing cars that exceed some minimum average              part (a). (c) sp = .0036 measures the (suppos-
   mileage. The design engineer would be most inter-         edly common) variation in tilt angle for repeated
   ested in a lower tolerance bound for most mileages,       measurement of a particular van; [.0026, .0058].
   to be sure that a high percentage of the cars pro-
   duced are able to cruise for at least 350 miles. A     Section 2
   confidence for the mean or prediction bound does
   not answer this question.                              1. (a) .02646; 75% (b) .03742 (c) [-.0724,
                                                             .0572] provides no convincing evidence of non-
2. (a) [132.543, 297.656] (b) [92.455, 337.745]              linearity over the range from 2,000 to 6,000, as it
   (c) The tolerance interval is much wider than the         includes 0.
   prediction interval. The interval in (b) is meant to
   bracket 90% of all observations, while the the one     2. (a) The intervals in numerical order of the four
   from (a) is meant only to bracket a single addi-          vans are: [1.0875, 1.0984], [.9608, 9716], [1.0145,
   tional observation. (d) The confidence interval           1.0242], [.9968, 1.0076]; at least 96% simulta-
   for mean lifetime is smaller than both the predic-        neous confidence. (b) = .0077; = .0073
   tion interval and the tolerance interval. It is meant     (c) [.013516, .02408]
   only to bracket the mean/center of the population,
   not additional observation(s). (e) [152.811, )         3. Before the data are collected,the probability is .05
   (f) [113.969, )                                           that an individual 95% confidence interval will be
                                                             in error--that it will not contain the quantity that it
3. (a) [3.42, 6.38]; [30.6, 589.1] (b) [3.87, 5.93];         is supposed to contain. If several of these individ-
   [48.1, 375.0] (c) The intervals in (a) are wider          ual intervals are made, then the probability that at
   than those in (b). This is usually true when apply-       least one of the intervals is in error is greater than
   ing tolerance intervals and prediction intervals in       .05. (If each interval has a .05 chance of failing,
   the same situation.                                       then the overall chance of at least one failure is
                                                             greater than .05.) When making several intervals,
4. 92.6%; 74.9%                                              most people would like the overall or simultane-
                                                             ous error probability to be small. In order to make
                                                             sure, for example, that the overall error probability
820 Answers to Section Exercises

    is .05, the error probability associated with the in-     4. (a) Unstructured multisample data could also be
    dividual intervals must be made smaller than .05.            thought of as data from one factor with r levels.
    This is equivalent to increasing the individual con-         In many situations, the specific levels of the fac-
    fidences (above 95%), which makes the intervals              tor included in the study are the levels of interest.
    wider.                                                       For example, in comparing three drugs, the fac-
                                                                 tor might be called"Treatment." It might have four
Section 3                                                        levels: Drug 1, Drug 2, Drug 3, and Control. The
                                                                 experimenter is interested in comparing the spe-
1. (a) .03682; it is larger. (b) .05522; it is larger.           cific drugs used in the study to each other and to
                                                                 the control. Sometimes the specific levels of the
2.  (a)  k     = 2.88 so the intervals in numerical order of     factor are not of interest in and of themselves,
            2                                                    but only because they may represent (perhaps they
                                                                 are a random sample of) many different possible
    the four vans are: [1.0878, 1.0982], [.9610, 9714],          levels that could have been used in the study. A
                                                                 random effects analysis is appropriate in this situ-
    [1.0147, 1.0240], [.9970, 1.0074]. (b) =                     ation. For an example, see part (b). (b) If there
                                                                 are many technicians, and five of these were ran-
    .0097; = .0092. These are larger than the earlier            domly chosen to be in the study, then interest is
                                                                 in the variation among all technicians, not just the
    's. The confidence level here is a simultaneous              five chosen for the study. (c) ^ = .00155 in.;
                                                                 ^  = .00071 in.
    one while the earlier level was an individual one.

    The intervals here are doing a more ambitious job

    and must therefore be wider.

Section 4                                                     Section 5

1. (a) Small, since some means differ by more than            1. (a) Center linex¯ = 21.0, UCLx¯ = 22.73, LCLx¯ =
   the there. (b) SSTr = .285135, MSTr =                         19.27. Center lineR = 1.693, UCLR = 4.358, no
   .071284, df = 4; SSE = .00423, MSE = .000423,                 LCLR. (b) Center lines = .8862, UCLs = 2.276,
   df = 10; SSTot = .289365, df = 14; f = 168.52                 no LCLs. (c) 1.3585; 1.3654; sp = 1.32.
   on 4,10 df; p-value < .001. R2 = .985.                        (d) Center linex¯ = 21.26, UCLx¯ = 23.61, LCLx¯ =
                                                                 18.91. Center lineR = 2.3, UCLR = 5.9202, no
2. (a) Small, since some sample means differ by more             LCLR. (e) Center linex¯ = 21.26, UCLx¯ = 23.62,
   than the 's there. (b) SSTr = .034134, MSTr =                 LCLx¯ = 18.90. Center lines = 1.21, UCLs =
   .011378, df = 3; SSE = .000175, MSE = .000013,                3.10728, no LCLs.
   df = 13; SSTot = .034308, df = 16; f = 847 on
   3,13 df; p-value < .001.                                   2. (a) Rd2 = 4.052632 2.326 = 1.742318 × .001 in.; s¯c4 =
                                                              1.732632   =            ×
3. (a) To check that the µi 's are normal, make a nor-          .9400       1.843226     .001  in.  (b) For the  R  chart
   mal plot of the y¯ i 's. To check that the i 's are
   normal, make a normal plot of the residuals. (Nor-         Center LineR = 2.326(1.843226) = 4.287344 ×
   mal plotting each sample individually will not             .001 in., UCLR = 4.918(1.843226) = 9.064985 ×
   be very helpful because the sample sizes are so            .001 in. and there is no lower control limit. For
   small.) Both plots are roughly linear, giving no ev-
   idence that the one-way random effects model as-           the s chart Center Lines = 1.732632 × .001 in.
   sumptions are unreasonable. (b) SSTr = 9310.5,             UCLs = 2.089(1.732632) = 3.619468 × .001 in.
   MSTr = 1862.1, df = 5; SSE = 194.0, MSE =                  and there is again no lower control limit. Neither
   16.2, df = 12; SSTot = 9504.5, df = 17; f =
   115.18 on 5,12 df; p-value < .001. ^ = 4.025               chart indicates that the short-term variability of the
   measures variation in y from repeated measure-
   ments of the same rail; ^  = 24.805 measures               process (as measured by  ) was unstable. (c) Use
   the variation in y from differences among rails.
   (c) [3.46, 13.38]                                          Center Linex¯ = 11.17895 × .001 in. above nom-
                                                              inal, LCLx¯ = 11.17895 - 3 1.843226 5 = 8.706 × .001
                                                                      Answers to Section Exercises 821

   in. above nominal and UCLx¯ = 11.17895 +                   causes and taking action to eliminate them. Reduc-
   3 1.843226 = 13.65189 × .001 in. above nominal.            ing variability increases the quality of the process
                                                              output.
              5
                                                           4. Shewhart control charts do not physically control
   x¯ from sample 16 comes close to the upper con-            a process in the sense of guiding or adjusting it.
   trol limit, but overall the process mean seems to          They only monitor the process, trying to detect
   have been stable over the time period. (d) The             process instability. There is an entirely different
   x¯ 's from samples 9 and 16 seem to have "jumped"          field dedicated to "engineering control"; this field
   from the previous x¯ . The coil change may be caus-        uses feedback techniques that manipulate process
   ing this jump, but it could also be explained by           variables to guide some response. Shewhart con-
   common cause variation. It may be something                trol charts simply monitor a response, and are not
   worth investigating. (e) Assuming that the mean            intended to be used to make "real time" adjust-
   could be adjusted (down), you need to look at              ments.
   one of the estimates of  to answer this question
   about individual thread lengths. (You should not        5. Out-of-control points should be investigated. If the
   use control limits to answer this question!) If µ          causes of such points can be determined and elim-
   could be made equal to zero, then (assuming nor-           inated, this will reduce long-term variation from
   mally distributed thread lengths), almost all of the       the process. There must be an active effort among
   thread lengths would fall in the interval ±3 . Us-         those involved with the process to improve the
   ing the estimate of  based on s¯ from part (a), this       quality; otherwise, control charts will do nothing
   can be approximated by 3(1.843226) = 5.53×                 to improve the process.
   .001 in. It does seem that the equipment is ca-
   pable of producing thread lengths within .01 in.        6. Control limits for an x¯ chart are set so that, un-
   of nominal. If the equipment were not capable              der the assumption that the process is stable, it
   of meeting the given requirements, the company             would be very unusual for an x¯ to plot outside the
   could invest in better equipment. This would "per-         control limits. The chart recognizes that there will
   manently" solve the problem, but it might not              be some variation in the x¯ 's even if the process
   be feasible from a financial standpoint. A sec-            is stable, and prevents overadjustment by allow-
   ond option is to inspect the bolts and remove the          ing the x¯ 's to vary "randomly" within the control
   ones that are not within .01 in. of nominal. This          limits. If the process mean or standard deviation
   might be cheaper than investing in new equip-              changes, x¯ 's will be more likely to plot outside of
   ment, but it will do nothing to improve the quality        the control limits, and sooner or later the alarm will
   of the process in the long run. A third option is          sound. This provides an opportunity to investigate
   to study the process (through experimentation) to          the cause of the change, and hopefully take steps
   see if there might be some way of reducing the             to prevent it from happening again. In the long run,
   variability without making a large capital invest-         such troubleshooting may improve the process by
   ment.                                                      making it less variable.

3. Control charting is used to monitor a process and       Section 6
   detect changes (lack of stability) in a process. The
   focus is on detecting changes in a meaningful pa-       1. (a) Center linep^ = .02, UCLp^ = .0438, no LCLp^ .
   rameter such as µ, , p, or . Points that plot out of       (b) Center linep^ =.0234, UCLp^ =.0491, no LCLp^ .
   control are a signal that the process is not stable at
   the standard parameter value (for a standards given     2. Center lineu^i = .138 for all i , UCLu^i = .138 +
   chart) or was not stable at any parameter value (for
   a retrospective chart). The overall goal is to re-      i3 .138 k , no LCLu^ for all i .i
   duce process variability by identifying assignable
822 Answers to Section Exercises

3. (a) Center lineu^i = .714 for all i, UCLu^i = .714 +                                             Chapter 8

    3 .714 k , no LCLu^ for all i . The process seems to beii                                       Section 1

    stable. (b) (i) if ki = 1, .0078; if ki = 2, .0033.                                             1. (a) Error bars: y¯ i j ± 23.54. (b) a1 = 21.78, a2 =
    (ii) if ki = 1, .0959; if ki = 2, .1133.                                                           -21.78, b1 = -41.61, b2 = 16.06, b3 = 25.56,
                                                                                                       ab11 = -1.94, ab12 = 1.39, ab13 = .56, ab21 =
4.  p^  =  18   =  .072,  so  Center Linep^  i  = .072.  The                                           1.94, ab22 = -1.39, ab23 = -.56. Interactions:
           250                                                                                         abi j ± 9.52. A main effects: ai ± 6.73. B main
                                                                                                       effects: bj ± 9.52. Interactions are not detectable,
    control limits depend on the sample size ni . For                                                  but main effects for both A and B are. (c) y¯ · j -
                                                                                                       y¯ · j ± 20.18
    ni = 20, .072 - 3 20 .072(1-.072) = -.101399 < 0,
    so there is no lower control limit, while UCLp^ =                                               2. (a) sp = 33.25 measures baseline variation in y for
                                                                                                       each factor-level combination, assuming it is the
                                                                                                 i     same for all factor-level combinations. (b) Error
                                                                                                       bars: y¯ i j ± 27.36. (d) a1 = -2.77, a2 = -17.4,
    .072 + 3 20 .072(1-.072) = .245399. For ni = 30,                                                   a3 = 20.17, b1 = -13.33, b2 = -1.20, b3 =
                                                                                                       14.53, ab11 = .033, ab12 = -5.40, ab13 = 5.37,
    .072 - 3 .072(1-.072) 30 = -.06957966 < 0, so there                                                ab21 = -2.13, ab22 = -.567, ab23 = 2.70,
    is no lower control limit, while UCLp^ = .072 +                                                    ab31 = 2.104, ab32 = 5.97, ab33 = -8.07.
                                                                                                       (e) 18.24. No. (f) Use (ai - ai ) ± 22.35. (g) Use
                                                                                  i                    (ai - ai ) ± 26.88.

    3 .072(1-.072) 30 = .2135797. For ni = 40, .072 -                                               Section 2

    3 .072(1-.072) 40 = -.05061158 < 0, so there is no                                              1. (a) E^ ± .014. B and C main effects, BC inter-
    lower control limit, while UCLp^ = .072 +                                                          action. (b) sFE = .0314 with 20 df; close to
                                                                                                       sp = .0329. (c) Using few effects model: [3.037,
                                                                                  i                    3.091]. Using general method: [3.005, 3.085].

    3 .072(1-.072) 40 = .1946116. There is no evidence                                              2. (a) Only the main effect for A plots "off the line."
    that the process fraction nonconforming was un-                                                    (b) Since the D main effect is almost as big (in
                                                                                                       absolute value) as the main effect for A, you might
    stable (changing) over the time period studied.                                                    choose to include it. For this model, the fitted val-
                                                                                                       ues are (in standard order): 16.375, 39.375, 16.375,
5. If different data collectors have different ideas of                                                39.375, 16.375, 39.375, 16.375, 39.375, -4.125,
   exactly what a "nonconformance" is, then the data                                                   18.875, -4.125, 18.875, -4.125, 18.875, -4.125,
   collected will not be consistent. A stable process                                                  18.875. (c) Set A low (unglazed) and D high (no
   may look unstable (according to the c chart) be-                                                    clean). [0, 9.09].
   cause of these inconsistencies.
                                                                                                    3. (a) y¯ ···· = 3.594, a2 = -.806, b2 = .156, ab22 =
6. It may indicate that the chart was not applied prop-                                                -.219, c2 = -.056, ac22 = -.031, bc22 = .081,
   erly. For example, if hourly samples of size m = 4                                                  abc222 = .031, d2 = -.056, ad22 = -.156,
   are collected, it may or may not be reasonable to                                                   bd22 = .006, abd222 = -.119, cd22 = -.031,
   use a retrospective x¯ chart with m = 4. If the 4                                                   acd222 = -.056, bcd222 = -.044, abcd2222 =
   items sampled are from 4 different machines, 3 of                                                   .006. (b) It appears that only the main effect for
   which are stable at some mean and the 4th stable at                                                 A is detectably larger than the rest of the effects,
   a different mean, then the sample ranges and stan-                                                  since the point for a2 is far away from the rest of
   dard deviations will be inflated. This will make the
   control limits on the x¯ chart too wide. Also, the x¯ 's
   will show very little variation about a center line
   somewhere between the two means. This is all a
   result of the fact that each sample is really com-
   ing from four different processes. Four different
   control charts should be used.
   the fitted effects. (c) To minimize y, use A(+)                             Answers to Section Exercises 823
   (monks cloth) and B(+) (treatment Y).
                                                              estimates from the Yates algorithm (excluding the
Section 3                                                     one that includes the grand mean) to be bell-shaped
                                                              around zero. A normal plot of these estimates
1. Since A  BCDE, if both are large but opposite              would then be roughly linear. However, if there
   in sign, their estimated sum will be small.                are effects (or sums of effects) that are relatively
                                                              far from zero, the corresponding estimates will plot
2. (a) 8.23, .369, .256, -.056, .344, -.069, -.081,           away from the rest (off the line), and may be con-
   -.093, -.406, .181, .269, -.344, -.094, -.156,             sidered more than just random noise. The principle
   -.069, .019. (b) .312. The sums 2 +   2222,                of "sparsity of effects" says that in most situations,
   2 +  2222, 2 +  2222, and 222 +  22                        only a few of the many effects in a factorial exper-
   are detectable. Simplest explanation: A, C, D main         iment are dominant, and their estimates will then
   effects and CE interaction are responsible for these       plot off the line on a normal plot.
   large sums. (c) A (+), C (+), D (-), and E (-).
   The abc combination, which did have the largest         4. (a) I  ABCDF  ABCEG  DEFG (b) ABDF,
   observed bond strength.                                    ABEG, CDEFG (c) +, +; -, - (d) That only
                                                              A, F, and their interaction are important in describ-
3. (b) (1), ad, bd, ab, cd, ac, bc, abcd. Estimated           ing y.
   sums of effects: 3.600, -.850, .100, -.250, -.175,
   -.025, -, 075, -.025. (c) The estimate of 2 +           5. 3.264
    222 plots off the line. Still, one might conclude
   that this is due to the main effect for A, but the      6. (a) I  ABCE  BCDE  ADEF (b) -, -;
   conclusion here would be a little more tentative.          +, - (c) .489

Section 4                                                  Chapter 9

1. The advantage of fractional factorial experiments       Section 1
   is that the same number of factors can be stud-
   ied using less experimental runs. This is important     1. (a) sLF = 67.01 measures the baseline variation
   when there are a large number of factors, and/or           in Average Molecular Weight for any particular
   experimental runs are expensive. The disadvantage          Pot Temperature, assuming this variation is the
   is that there will be ambiguity in the results; only       same for all Pot Temperatures. (b) Standard-
   sums of effects can be estimated. The advantage            ized residuals: 2.0131, -.3719, -.9998, -1.562,
   of using a complete factorial experiment is that           .2715, .2394, .7450, .0004 (c) [22.08, 24.91]
   all means can be estimated, so all effects can be          (d) [1761, 1853], [2630, 2770] (e) [1745, 1869],
   estimated.                                                 [2605, 2795] (f) 1705; 2590 (g) 1627; 2503
                                                              (h) SSR = 4,676,798, MSR = 4,676,798, df = 1;
2. It will be impossible to separate main effects from        SSE = 26,941, MSE = 4490, df = 6; SSTot =
   two-factor interactions. You would hope that any           4,703,739, df = 7; f = 1041.58 on 1,6 df; p-value
   interactions are small compared to main effects;           < .001
   the results of the experiment can then be (tenta-
   tively) summarized in terms of main effects. (If all    2. (a) b0 = 4345.9, b1 = -3160.0, sLF = 26.76
   interactions are really zero, then it is possible to       (close to sp = 26.89) (b) Standardized resid-
   estimate all of the main effects.) Looking at Ta-          uals: 1.32, -.48, -.04, -.91, .52, -1.07, 1.94,
   ble 8.35, the best possible resolution is 3 (at most).     -.04, -1.09. (c) [-357.4, -274.64] (d) t =
                                                              -14.47 on 7 df, p-value < .001; or f = 209.24
3. Those effects (or sums of effects) that are nearly         on 1,7 df, p-value < .001. (e) [2744.8, 2787.0]
   zero will have corresponding estimates that are            (f) [2699.2, 2832.6] (g) 2698.5
   "randomly" scattered about zero. If all of the ef-
   fects are nearly zero, then one might expect the
824 Answers to Section Exercises                       is some hint of a pattern in the plot of Standard-

Section 2                                              ized Residuals versus levels of C, indicating that

1. (a) sSF = .04677 measures variation in Elapsed      the amount of additive may be having a small ef-
   Time for any particular Jetting Size, assuming
   this variation is the same for all Jetting Sizes.   fect that the model is not accounting for. Other-
   (b) Standardized residuals: -.181, .649, -.794,
   -.747, 1.55, -1.26. (c) [81.32, 126.66]; [-3.17,    wise, the residuals do not provide any evidence
   -1.89]; [.01344, .02245] (d) [14.462, 14.596];
   [14.945, 15.145] (e) [14.415, 14.644]; [14.875,     that the model is inadequate.        (c) sFE = .09623.
   15.215] (f) 14.440; 14.942 (g) 14.323; 14.816       sp = .12247. No; sFE < sp.
   (h) SSR = .20639, MSR = .01319, df = 2; SSE =
   .00656, MSE = .00219, df = 3; SSTot = .21295,       Appendix A (selected answers only)
   df = 5; f = 42.17 on 2,3 df; p-value = .005. H0
   means that Elapsed Time is not related to Jet-      Section 1
   ting Size. (i) t = 9.38; p-value = .003. H0 : y 
   0 + 1x + 0; i.e., Elapsed Time is related to Jet-   1. (a) .1865 (b) .6083
   ting Size only linearly (no curvature).
                                                       2. (a) .54 (b) .78
2. (a) sSF = .4851 measures baseline variation in
   y for any (x1, x2) combination, assuming this       3. (a) .505 (b) .998
   variation is the same for all (x1, x2) combina-
   tions. (b) Standardized residuals: -.041, .348,     4. (a) .76 (b) .78 (c) .974
   1.36, -1.44, -1.00, -.457, 1.92, .348, -.604.
   (c) [5.036, 7.060]; [.0775, .2058]; [-.0298,        5. (a) .75 (b) .80 (c) .75 (d) Yes, since the
   -.0041] (d) [5.992, 6.622]; [5.933, 6.625]             answers to parts (a) and (c) are the same. (e) One
   (e) [5.798, 6.816]; [5.720, 6.838] (f) 5.571;          such pair is "ring meets spec.s on first grind" and
   5.535 (g) 5.017; 4.970 (h) SSR = 5.8854,               "ring is ground twice."
   MSR = 2.9427, df = 2; SSE = 1.4118, MSE =
   .2353, df = 6; SSTot = 7.2972, df = 8; f = 12.51    Section 2
   on 2,6 df; p-value = .007.
                                                       1. r = .99979
Section 3
                                                       2. k = 2
1. (a) y^ = 31.40 + 7.430 ln x1 - .08101x2 - .2760
   (ln x1)2 + .00004792x22 - .006596x2 ln x1. R2 =     Section 3
   .724. sSF = 1.947. sp = 2.136, which is greater
   than sSF, so there is no indication that the model  1. (a) 1.7310 × 1013 (b) 2.2777 × 1012 (c) .1316
   is inappropriate. (b) Factor-level combinations     2. (a) .0000081 (b) .03402
   have fitted values that differ by as much as .77.   3. (a) 1,757,600 (b) .00167 (c) .0167
   (d) (i) [.128, 2.781]. (ii) [-2.693, 5.601]. (iii)  4. (a) .5 (b) .167
   -2.332.
                                                       Section 4
2. (a) Estimate of µ··· = .67407; estimate of 2 =
   .12407; estimate of 2 = -.30926. (b) There          1. (a) 20; 15.81 (b) 1 - 32 exp - t15 + 12 exp - t5

                                                       (c)  fT (t) =  1      exp  - 15t  - exp   - 5t
                                                                      10

                                                       (d)  ST (t)    =   3  exp  t   -  1  exp  - t5 ,  hT (t) =
                                                                          2       15     2

                                                       1 exp (- 1t5 )-exp (- 5t )
                                                       5 3 exp (- t15 )-exp (- t5 ) . hT (t ) is not constant. It

                                                       starts at 0, and increases to an asymptote of 1/15.
qqqqqqqqqqqqqqqqq

               Index

                                      Note: boldface page numbers indicate definitions.

2-factor interaction of factors, 553, 573         inference for variance components,     Block of experimental units, 41
                                                        491- 495                         Blocking variables, 40
2p factorial studies                                                                     Bonferroni inequality, 470, 471- 472
   balanced, 580 -587                          simple linear regression, 669-672, 673    Book paper thickness, measurements, 16
   confidence intervals for, 587-590        Analytical study, 6                          Boxplots, 81
   special devices for, 187-190             ANOVA, see Analysis of variance              Brittleness, measuring, 14
   without replication, 577-580                                                          Brownlee's stack loss data, 150
                                                  (ANOVA)                                Bunching, 530
2p-1 fractional factorials                  Arithmetic mean, 93                          Burn-in period, 763
   choosing, 596 -597                       "As past data" Shewhart control charts,
   data analysis for, 603-607                                                            Calibration, 17
   determining the alias structure of,            500                                    Capability, 95
         600 -601                           Assignable causes, 498                       Capability of a process, 389
                                            Attributes data                              Carryover effects, 290
2p-q fractional factorials                                                               Categorical data, 8-9 (see also qualitative
   choosing, 613-614                           bar charts and plots for, 107-112
   data analysis for, 618-620                  numerical summarization of, 104 -107            data)
   determining the alias structure of, 614  Axioms, 728                                  Causality, 5
                                            Axioms of probability theory, 729-735,       Cause-and-effect diagram, 60
3-factor interaction, 573                                                                Census, 33
                                                  733                                    Center of mass, 93
Accelerated life test, 62                                                                Central limit effect, 316 -321
Accelerated life testing, 210               Balanced 2p factorial studies,               Central Limit Theorem, 316
Accompanying variable, 39                      confidence intervals for, 587-590         Changes in level, 530
Accurate measurement, 17                       fitting and checking simplified models,   Charts for demerits, 538
Alias structure, 601                                 580 -587                            Chebyschev's Theorem, 97
Allocation of resources, 46                                                              Chi-squared distribution, 386
Alternative hypothesis, 347                 Balanced data, 172                           Coefficient of determination, 130 -132,
Analysis of variance (ANOVA)                Balanced data confidence limits
                                                                                               143, 173, 486 - 487
   multiple linear regression, 691-696         for a 2p effect, 587                         simple linear regression, sum of
   one-way, 478                                for one-way random effects model,
                                                                                                  squares, 670
      F test, 479- 482                               491, 493                            Coefficients for a quadratic
      identity and table, 482- 487          Baseline variation, 44, 498
   random effects models and analyses,      Bathtub curve, 763                              matrix of quadratic, 703
                                            Bell-shaped histogram, 73                       vector of linear, 703
         487- 491                           Bimodal histogram, 72                        Combination, 754 -757
      estimator of the treatment variance,  Binomial distribution, 233-236               Comparative study, 43- 44, 374

            492                                mean of, 236                                                                   825
                                               variance of, 236
                                            Bivariate data, 11
                                               check sheet, 30 -31
826 Index                                  Contour plot, 701-702                         types of, 8-11
                                           Control chart patterns, 527-531 (see also     univariate, 11
Complete block plans, 630 -631                                                           variables, 104
Complete randomization, 48- 49                   Shewhart control charts)             Data analysis, 19-23
Completely randomized experiments,         Control charts, see Shewhart control       Data collection
                                                                                         physical preparation, 63
      47-50                                      charts                                  problem definition, 57-60
Conceptual population, 8                   Control limits, 498                           recording, 30 -32
Concomitant variable, 39                                                                 sampling, 28-30
Conditional densities, geometry of, 298       setting, 499                               study definition, 60 -63
Conditional distributions, for continuous  Controlled variables, 38, 40               Data structures, types of, 11-14
                                           Correlation vs. causation, 137             Data vectors, influence of in regression,
      random variables, 297-300            Correlations
Conditional probability, 739-743                                                            159
Conditional probability density function,     sample, 129-130, 137                    Decreasing force-of-mortality (DFM)
                                              squared, 131
      for continuous random variables,     Count data, descriptive statistics for,          distribution, 761
      297                                                                             Defining relation, 602, 615
Conditional probability function, for            104 -112 (see also attributes data)  Definition of effects, 552-554, 572-575
      discrete random variables, 284 -285  Count variables, 27                        Descriptive statistics, 104 -112
Confidence intervals, 335                  Cumulative probability functions,          Design resolution, 620 -625, 621
   factorial effects, 554 -562                                                        Deterministic models, 202-203
   interpretation of, 342                        226 -228, 247-249                    Diagram, cause-and-effect, 60
   large-sample, 335-344                   Curve fitting by least squares, 149-158    Diagrams
   P-R method of simultaneous, 472         Cyclical patterns, 147, 528
   Tukey method, 474 - 477                                                               dot, 66 -68, 74, 76
Confidence intervals for means, 461- 464   Daniel, Cuthbert, 577-578                     Ishikawa, 61
Confidence levels                          Data                                          Pareto, 58
   individual and simultaneous, 469- 470                                              Direct measure, 26
   interpretation, 341-342                    attributes                              Discrete data
   of prediction intervals, 424                  bar charts and plots for, 107-112       likelihood function, 765-774
   of tolerance intervals, 424 - 425             numerical summarization of,             log likelihood function, 766
Confidence limits                                      104 -107                       Discrete probability distributions,
   effects in a 2p factorial, 575
   mean system response, 662, 686             analysis for 2p-1 fractional studies,         228-232
   one-way model, 461- 462                          603-607                              binomial, 232-237
   one-way random effects model, 491,                                                    geometric, 237-240
                                              balanced, 172                           Discrete probability functions, 223-226
         493                                  bivariate, 11, 123                      Discrete probability models, 221
   simultaneous (in regression), 664, 688     Brownlee's stack loss, 150              Discrete random variable, 222
   slope parameter, 659                       categorical, 8                             conditional distributions of, 283-284
   Tukey simultaneous for main effects,       continuous                                 conditional probability function,

         562-563                                 likelihood function, 774 -781                 284 -285
   variance of one-way model, 457                log likelihood function, 775            expected value of, 228
Continuous data                               count, 104 -112 (see also attributes       independence of, 289
   likelihood function, 774 -781                                                         mean of, 228
   log likelihood function, 775                     data)                                standard deviation of, 230
Continuous distributions, means and           discrete                                   variance of, 230
                                                                                      Disjoint events, 731
      variances for, 249-250                     likelihood function, 765-774         Distribution
Continuous random variable, 222,                 log likelihood function, 776            center of mass, 93
                                              engineering                                first moment, 93
      244 -263, 292-300                          collection of, 26 -32                Distributional shapes
   conditional distributions, 297-300            preparing to collect, 56 -64            engineering interpretations of, 72-73
   conditional probability density            measurement, 104                           terminology for, 73
                                              mixed                                   Distributions
         function, 297                           likelihood function, 779                binomial, 233-236
   independent, 299                              log likelihood function, 779            chi-squared, 386
   joint probability density, 292             multivariate, 11
   marginal probability density, 295          numerical, 9
   mean or expected value of, 249             overfitting of, 160
   standard deviation of, 250                 paired, 11
   variance of, 250                           qualitative, 8-9, 104 -112 (see also
Continuous variables, 9
                                                    attributes data)
                                              quantitative, 9
                                              repeated measures, 11
   decreasing force-of-mortality (DFM),        mutually exclusive, 731                                            Index 827
         761                                Experimental study, 5
                                            Experimental variables, 38                     three-way and higher factorials,
   exponential, 257-260                     Experiments, completely randomized,                  178-184
   Gaussian, 251
   geometric, 237-239                             47-50                                 Fitted interaction of factors, 169, 182, 183
   increasing force-of-mortality (IFM),     Exponential distributions, 257-260, 258     Fitted main effect of factors, 166, 182
                                            Extraneous variables, 40                    Fitted quadratics, interpreting, 701-702
         761                                                                            Fitted value, 129
   joint, 279                                  blocking, 40                             Flowcharts, 58
   marginal, 282                               control of, 40                           Force-of-mortality function, 760 -764
   normal, 251                                 randomization, 40                        Formal inference, methods of, 361
   null, 348                                Extrapolation, caution concerning,          Fractional factorial experimentation, 591
   Poisson, 240 -243                                                                    Fractional factorial studies
   probability, 222, 251-257                      158-159
   reference, 348                                                                          aliases, 601
   Snedecor F, 391                          Factorial effects, individual confidence       blocks, 625-631
   standard normal, 88                            intervals for, 554 -562                  complete block plans, 630 -631
   Studentized extreme deviate, 472                                                        design resolution, 620 -625
   Studentized range, 475                   Factorial inference methods, 705               experiment size, 631
   Weibull, 260 -263, 761                   Factorial interactions, interpretation of,     fundamental issues, 596 -597
Documentation, 31-32                                                                       generator, 601, 614
Dot diagram, 66 -68, 74, 76, 81, 94               183-184                                  observations about, 592-596
Dummy variables, 706                        Factorial notation, special 2p, 187-188     Fractional factorials, saturated, 622
   regression analysis, 713                 Factorial study                             Frequency histogram, 72
                                                                                        Frequency table, 70 -71, 74
Effect sparsity, 577                           2p                                       Functions
Effective experimentation, principles for,        confidence intervals for, 587-590        conditional probability, 284 -285
                                                  special devices for, 187-190             conditional probability density, 297
      38- 47                                      without replication, 577-580             cumulative probability, 226 -228,
Eigenvalues, 703
Empirical models, 161                          balanced 2p factorial studies                     247-249
Empty event, 731                                  confidence intervals for, 587-590        discrete probability, 223-226
Engineering data                                  fitting and checking simplified          force-of-mortality, 760 -764
                                                        models, 580 -587                   geometric cumulative probability
   collection of, 26 -32
   preparing to collect, 56 -64                complete, 12                                   relationship for, 237
Engineering data-generating process,           fractional, 13                              hazard (see force-of-mortality)
                                            Factorials, importance of two-level, 190       joint probability, 279
      stability of, 496                     Factors                                        likelihood
Engineering statistics, 2                      2-factor interaction of in three-way
Enumerative studies                                                                           continuous and mixed data,
                                                     factorial studies, 573                         774 -781
   judgment-based method, 33                   3-factor interaction of in three-way
   sampling, 33-37                                                                            discrete data, 765-774
   systematic method, 33                             factorial studies, 573                   mixed, 779
Enumerative study, 6                           fitted interaction of, 169, 182             linear, 698
Equal variances, 651                           fitted main effect of, 166, 182             log likelihood
Equations                                      interaction of in p-way factorials, 553        continuous data, 775
   choice and interpretation of                levels, 12                                     discrete data, 766
                                               main effect of in p-way factorials, 552        mixed, 779
         appropriate, 151                      main effect of in three-way factorial       marginal probability, 282
   normal, 126, 141                                                                        probability density, 245-247
   polynomial, 141                                   studies, 572                             conditional, 297
Error sum of squares, 484                   Few-effects model, confidence intervals        probability, 223-228
Estimation of all r individual mean                                                           conditional, 284 -285
                                                  for balanced 2p studies, 587-590            cumulative, 226 -228, 247-249
      responses, 471                        Few-effects sample variance, 582                  discrete, 223-226
Events, 729                                                                                   geometric cumulative, 237
                                               alternative formula for, 583                   joint, 279
   dependent, 741                           First (or lower) quartile, 80                     marginal, 282
   disjoint, 731                            First moment, 93                                  mathematically valid, 225
   empty, 731                               Fishbone diagram, see cause-and-effect            standard normal cumulative, 252
   independence of, 741-743
                                                  and Ishikawa diagrams
                                            Fitted effects, normal-plotting of,

                                                  577-580
                                            Fitted factorial effects, 162-190

                                               2-factor studies, 163-171
828 Index                                  Independent discrete random variables,     Least squares
                                                 289                                     curve fitting by, 141-149
   quadratic, 698                                                                        fitting a line by, 123-136, 651
   quantile, standard normal, 254          Independent identical success-failure         principle of, 124 -129, 125
   reliability (see survivorship)                trials, 232, 400                        surface fitting by, 149-158
   standard normal cumulative
                                           Independent and identically distributed    Left-skewed histogram, 73
         probability, 252                        (idd) random variables, 291          Likelihood estimate, maximum, 772
   standard normal quantile, 254                                                      Likelihood functions
   survivorship, 759-760                   Individual confidence intervals for
                                                 factorial effects, 554 -562             continuous data, 774 -781
Gauge R and R studies, 27                                                                discrete data, 765-774
Gaussian probability distribution, 251     Individual confidence levels, 562             mixed data, 774 -781
                                           Industrial process improvement, 515-516    Likelihood-based large-sample inference
      (see also normal distribution)       Inference
General linear combinations of means,                                                       methods, 781-784
                                              single proportion, 400 - 407            Linear combination, confidence limits for
      intervals for, 464 - 469                two proportions, 407- 413
Generators, 601, 614                       Inference methods                                two-way factorial means, 556
                                              definition of effects, 552-554          Linear combination of means, confidence
   defining relation, 615 Geometric           likelihood-based large-sample,
         cumulative probability function,                                                   limits for, 465
         relationship for, 237 Geometric            781-784                           Linear combinations of random variables,
         distribution, 237-239                one-way in two-way factorials,
                                                                                            307-310
   mean of, 239                                     547-551                           Linear functions, 698
   variance of, 239                           p-way factorials, 568-590               Linear regression model
Geometry of conditional densities, 298        two way factorial notation, 551-554
Grouping, 530                              Inference methods for individual values,      multiple, 675-682
                                                                                            fitted values for, 677
Half fraction of a 2p factorial                  440                                        graphical representation of, 676
   aliasing, 600 -603                      Inference methods for one and two                residuals for, 677
   best, 597                                                                                standardized residuals for, 682
   defining relation for, 602                    means, 441
                                           Inference methods for proportions, 442        simple, 651-658
Half normal plot, 577                      Inference methods for variances, 442             fitted values for, 653
Hazard function, see force-of-mortality    Inference for specified regression               graphical representation of, 652
                                                                                            residuals for, 653
      function                                   parameters, 682-685                        standardized residuals for, 656
Histograms, 71-73, 74, 81, 85              Inferences for variances, caveats about,
                                                                                      Line-fitting sample variance, 653 (see
   bell-shaped, 72                               398                                        also simple linear regression model)
   bimodal, 72                             Inferring causality, 5
   frequency, 72                           Instability, 101, 528                      Logarithmic transformation, 193
   guidelines for making, 72               Instrument drift, 18                       Long-term variation, 498
   left-skewed, 73                         Interaction of factors, p-way factorial    Lurking variables, 5
   multimodal, 72
   probability, 228                              studies, 553                         Main effect of factors
   relative frequency, 72                  Interaction plot, 165                         three-way factorial studies, 572
   right-skewed, 73                        Interquartile range, 81                       two-way factorial studies, 552
   truncated, 73                           Ishikawa diagram, 61 (see also
   uniform, 73                                                                        Managed variable, 38
Hypothesis testing, 347 (see also                cause-and-effect and fishbone        Marginal distribution, 282
                                                 diagrams)                            Marginal probability densities, for
      significance testing)
                                           JMP, 103                                         continuous random variables, 295
iid, see independent and identically       Joint probability density, for continuous  Marginal probability function, 282
      distributed random variables                                                    Mathematical models, 19-23
                                                 random variables, 292
Incomplete block experiments, 53-55        Joint probability function, 279               predictive ability, 19-20
Increasing force-of-mortality (IFM)        Jointly continuous random variables,          simplicity, 19
                                                                                      Maximum likelihood estimate, 772
      distribution, 761                          292-297                              Means
Independence of events, 741-743            Jointly discrete random variables,            arithmetic, 93
Independent continuous random                                                            binomial distribution, 236
                                                 279-283                                 confidence intervals for, 461- 464
      variables, 299                                                                     continuous distributions, 249-250
                                           Lack of fit, testing for, 723                 continuous random variables, 249
                                           Large-sample likelihood-based inference

                                                 methods, 781-784
   discrete random variables, 228          Multiple linear regression model,                                      Index 829
   general linear combinations                   675-682
                                                                                           assumptions, 447
      intervals for, 464 - 469                fitted values for, 677                       confidence limits for, 461- 462
   geometric distribution, 239                residuals for, 677                           confidence limits for variance of, 457
   inference methods for, 441                 standardized residuals for, 682              fitted values for, 448
   linear combinations, 464                Multiple linear regression program, 141         residuals for, 449
                                           Multiple regression                             standardized residuals for, 459
      confidence limits for, 465              common residual plots in, 155                statement in symbols, 447
      confidence limits for 2-way             goal of, 152                              One-way random effects model, 488
                                              interpreting fitted coefficients from,       balanced data confidence limits, 491,
            factorial, 556
   paired differences                               151                                          493
                                           Multiple regression methods, 650             Operating characteristic curve, 331
      inference for, 368-374               Multiple regression model, factorial         Operational definitions, 27
   Poisson distributions, 241                                                           Outcomes, 729
   population, 98                                analyses, 705-719
   process, 101                            Multiplication principle, 751-752            p charts, 518-523 (see also Shewhart
   random variables                        Multiplication rule of probability, 740,           control charts)

      linear combinations of, 307-310            742                                    Paired data, 11
   sample, 163, 178                        Multisample studies, 478- 479                Paired differences, inference for the mean

      linear combination of, 464              notational convention for, 480                  of, 368-374
   simultaneous two-sided confidence          pooled estimate of variance for,          Paired distortion measurements, 11
                                                                                        Parallel systems, 747-749
         limits for, 664, 688                       455- 457                            Parallel traces, 168
   Weibull, 260                            Multivariate data, 11                        Parameters, 98
Mean system response                       Mutually exclusive events, 731
   confidence limits for, 662, 686                                                         fitting or estimating, 20
   estimate of all r individual, 471       Nonrandom variation, 498                     Pareto diagram, 58
   inference for, 661-666, 685-689         Normal distribution                          Permutations, 753-754
Measurement                                                                             Peterson, Dr. Frank, 20
   accuracy, 15, 17                           inference for the variance of, 386 -391   Physical preparation, 63
   blind, 29                                  prediction intervals, 414 - 419           Pillai-Ramachandran method, 471- 474
   calibration of a system, 17             Normal distributions, 651
   methods of, 14 -19                         with a common variance, 378                     (see also P-R method)
   precision, 15, 16 -17                   Normal equations, 126                        Pilot plants, 62
   unbiased, 17                            Normal plot, 88 (see also probability plot)  Plots
   validity, 15                            Normal probability distributions,
   variation/error, 15                                                                     attributes data, 107-112
Measurement data, 104 (see also                  251-257                                   boxplots, 81-85
                                           Normal probability paper, 90                    common residual, 155
      variables data)                      Normal probability plots, 264 -269              contour, 701-702
Measures of location, 92-95                Normal-plotting of fitted effects, 577-580      cube, 180 -181
Measures of spread, 95-98                                                                  cycles in, 101
Median, 80                                    interpreting, 579                            cyclical pattern of, 147
Memoryless property, 259                   Null distribution, 348 (see also reference      exponential probability, 270 -273
Methods of formal inference, 361                                                           half normal, 577
MINITAB, 102-103, 138-139, 142-143,              distribution)                             interaction, 165
                                           Null hypothesis, 347                            interpreting fitted quadratic, 701-702
      150 -151, 156, 170, 306, 402, 486,   Numerical data, 9 (see also quantitative        multiple regression
      561, 672-674, 704
Mixed data                                       data)                                        common residual, 155
   likelihood function, 779                   discrete, 9                                  normal, 88
   log likelihood function, 779            Numerical summary measures, 92-104              normal probability, 264 -269
Multimodal histogram, 72                                                                   probability, 88
Multiple linear regression                 Observational study, 5                          Q-Q, 85-92, 86
   ANOVA, 691-696                          Observed level of significance, 349             quantile, 80 -81
   prediction intervals, 689-691           One-way methods in p-way factorials,            residual, 135-136
   prediction limits, alternative formula                                                  scatterplots, 74 -75
                                                 569-571                                   steam-and-leaf, 68-70, 74
         for, 689                          One-way methods in two-way factorials,          summary statistics, 99-101
   tolerance intervals, 689-691                                                            theoretical Q-Q, 88
                                                 547-551 (see also inference
                                                 methods)
                                           One-way model, 447
830 Index                                     Probability theory                            Reality, 19-23
                                                 axioms of, 729-735, 733                    Recording, 30 -32
   Weibull distribution, 273-277                 simple theorems of, 736 -739
Plots against process variables, 101-102                                                       documentation, 31
Plots against time, 99-101                    Problem definition, 57-60                     Reference distribution, 348
                                              Process capability, 268                       Regression program, 138
   patterns on, 101                           Process mean, changes in level of, 101        Regression sum of squares, 669
Plotting, 137                                 Process variables, plots against, 101-102     Relative frequency histogram, 72
Poisson distributions, 240 -243               Propagation of error formulas, 310 -315       Reliability function, see survivorship
                                              p-value, 349
   mean of, 241                               p-way factorial                                     function
   variance of, 241                                                                         Repeatability, 27
Poisson observations, independent, 767           definitions of effects, 572-575            Repeatability and reproducibility studies,
Pooled sample standard deviation, 380,           notation, 571-572
                                                                                                  see gauge R and R studies
      455                                     Q-Q plots, 85-92, 86                          Repeated measures data, 11
Pooling, 379                                  Quadratic functions, 698                      Replication, 44 - 46
Population, 7-8                               Qualitative and count data, Shewhart          Reproducibility, 28
                                                                                            Reproducibility of results, 44
   conceptual, 8                                    control charts for, 518-533             Residual analysis, 580
Population mean, 98                           Qualitative data, 8-9 (see also categorical   Residuals, 132-136, 173
Population means, linear combination of,
                                                    data)                                      linear regression model
      464                                        descriptive statistics for, 104 -112 (see        multiple, 677
Population standard deviation, 99                                                                 simple, 653
Population variance, 99                                also attributes data)
Power laws, 194                               Qualitative variables, 27                        normal plotting of, 135-136
P-R method of simultaneous confidence         Quantile function, standard normal, 254          patterns on plots of, 135
                                              Quantile plots, 80 -81                           standardized, 457- 460, 458, 656, 682
      intervals, 472                          Quantile-quantile plot, see Q-Q plots         Resources, allocation of, 46
Precise measurement, 16 -17                   Quantiles, 78-81                              Response surface, 650
Predicted value, 129                                                                        Response surface studies, 698-705
Prediction interval, 416                         standard normal, 89                        Response variable, 38
                                              Quantitative data, 9 (see also numerical      Retrospective contexts, 500
   cautions concerning, 418                                                                 Retrospective x¯ Shewhart control charts,
   interpretation of, 419                           data)
Prediction intervals                          Quartiles, 80                                       504 -509
   cautions about, 669                                                                      Reverse Yates algorithm, 189
   confidence levels of, 424                  Random digit table, 35                        Right-skewed histogram, 73
   multiple linear regression, 689-691        Random effects model, assumptions, 488        Run charts, 75-77
   normal distribution, 414 - 419             Random effects models and analyses,           Runs, 530
   simple linear regression, 666 -669
Principle of least squares, 125                     487- 491                                Sample, 7-8
Probabilistic models, see stochastic          Random variables, 221-223, 222                Sample (linear) correlation, 129-130
                                                                                            Sample means
      models                                     conditional distributions of discrete,
Probability, 22                                        283-284                                 linear combination of, 464
                                                                                               notation for, 163, 178
   multiplication rule of, 740, 742              continuous, 222, 249, 292-300              Sample space, 729
Probability density, mechanics analogy           discrete, 222                              Sample standard deviation, 96
                                                 discrete independent, 289                  Sample variance, 96
      for, 245                                   distribution of a function of, 302-304     Sampling, 28-30
Probability density functions, 245-247           independent and identically                   mechanical methods, 34
Probability distribution, 222                                                                  random digit methods, 34
Probability distributions, see distributions           distributed, 291                        simple random, 34
Probability functions, 223-228                   jointly continuous, 292-297                SAS, 103
                                                 jointly discrete, 279-283                  Satterthwaite, 383
   conditional, 284 -285                         linear combinations of, 307-310            Saturated fractional factorials, 622
   cumulative, 226 -228, 247-249                 means and variances for linear             Scatterplots, 74 -75
   discrete, 223-226                                                                        Sequential investigations, 46
   geometric cumulative, 237                           combinations of, 307-310             Series systems, 746 -747
   joint, 279                                 Random variation, 498                         Series-parallel systems, 749-750
   marginal, 282                              Randomization, 40 - 41
   mathematically valid, 225                  Randomized complete block
   standard normal cumulative, 252
Probability histogram, 228                          experiments, 50 -53
Probability paper, 90                         Range, 95
Probability plot, 88
Shewhart control charts                       Simple random sampling, 34                                            Index 831
   bunching, 530                                 objective method, 37
   common chart patterns, 527-531                probability, 37                             analytical, 6
      changes in level, 530                                                                  balanced 2p factorial
      cyclical, 528                           Simultaneous confidence levels, 562-563
   generalities about, 496 -500               Simultaneous or joint confidence, 470             confidence intervals for, 587-590
   grouping, 530                              Simultaneous two-sided confidence limits          fitting and checking simplified
   instability of, 528
   measurements and industrial process              for all means, 664, 688                           models, 580 -587
         improvement, 515-516                 Single proportion, inference for,              comparative, 43- 46, 374
   p charts, 518-523                                                                         data collection
      pooled estimator, 520                         400 - 407
      retrospective control limits, 521       Slope parameter                                   definition, 60 -63
      "standards given" control limits, 519                                                  enumerative, 6
   "as past data," 500                           confidence limits for, 659                  experimental, 5
   R charts, 509-512                             inference for, 658-661                      factorial
      retrospective control limits, 511       Small-sample inference, 362-368
      "standards given" control limits, 510   Snedecor F distribution, 391                      2p, 187-190, 577-580, 587-590
   retrospective, 500, 504 -509               Software, 138-139                                 complete, 12
   runs, 530                                     statistical or spreadsheet, 36                 fractional, 13
   special checks, 531-533                       summary statistics, 102-104                 observational, 5
   s charts, 512-515                          Special causes, 498                            statistical engineering
      retrospective control limits, 513       SPLUS, 103                                        planning, 56 -57
      "standards given" control limits, 514   SPSS, 103                                   Study definition, 60 -63
   "standards given," 500 -504                Stable processes, 528                       Sum of squares
      control limits for R, 510               Standard deviation, 95                         error, 484
      control limits for s, 512                  population, 99                              total, 484
   systematic differences on, 529                sample, 96                                  treatment, 484
   u charts, 523-527                          Standard normal cumulative probability      Summary statistics, plots of, 99-101
      pooled estimator, 524                                                               Supervised variable, 38
      retrospective control limits, 524             function, 252                         Surface fitting, 698-705
      "standards given" control limits, 524   Standard normal distribution, 88, 252          replication, 160
   x¯ charts, 500 -509                        Standard normal quantile function, 254      Surface fitting by least squares, 149-158
      retrospective control limits, 508, 509  Standard normal quantiles, 89               Surface-fitting analyses, 650
      "standards given" control limits, 502   Standardized residuals, 457- 460, 458,      Surface-fitting sample variance, 653 (see
                                                                                                also multiple linear regression
Shewhart monitoring chart, see Shewhart             651-658, 675-682                            model)
      control charts                          Standards given, 499                        Survivorship function, 759-760
                                                                                          SYSTAT, 103
Shewhart's partition of process variation,       charting and hypothesis testing, 500     System of formal multiplication, 601
      498                                     Stationary point, 703                          conventions for, 602
                                              Statistical engineering study, planning,    System of probabilities, 733
Short-term variation, 498                                                                 Systematic variations, 101
Significance level, 354                             56 -57                                Systems
Significance testing, 347, 478- 479           Statistical models                             combination series-parallel, 749-750
                                                                                             parallel, 747-749
   goal of, 345                                  deterministic, 202-203                      series, 746 -747
Simple linear regression                         stochastic (or probabilistic), 203
                                              Statistical significance vs. practical      t distribution, see Student t distribution
   ANOVA, 669-672, 673                                                                    Taxonomy of variables, 38-39
   prediction intervals, 666 -669                   importance, 358                       Test statistic, 348
   prediction limits, 667                     Statistical software, simple linear         Theoretical Q-Q plot, 88
   tolerance intervals, 666 -669                                                          Third (or upper quartile), 80
Simple linear regression model, 651-658             regression, 672-674                   Three-way data sets, 184 -187
                                              Statistical or spreadsheet software, 36     Tolerance intervals, 420 - 426
      (see also Least squares)                Statistical studies, types of, 5-8
   fitted values for, 653                     Statistical tolerance interval, 420            cautions about, 669
   graphical representation of, 652           Statistics, 98, see Engineering statistics     confidence levels of, 424 - 425
   residuals for, 653                         Stem-and-leaf plots, 68-70, 74                 interpretation of, 422
   standardized residuals for, 656                                                           multiple linear regression, 689-691
Simple random samples, 290                       back-to-back, 69-70
                                              Stochastic models, 203                            multiplier to use, 689
                                              Student t, 362                                 simple linear regression, 666 -669
                                              Studentized extreme deviate

                                                    distributions, 472
                                              Studentized range distributions, 475
                                              Studies,
832 Index                                  Variables                              Variables vs. attributes control charting,
                                              accompanying, 39                          538
Total sum of squares, 484                     behavior of, 75
Transformation                                blocking, 40                        Variables data, 104
                                              concomitant, 39                     Variance, 95
   logarithmic, 193                           continuous, 9
   power, 193                                 controlled, 38, 40                     population, 99
Transformations                               count, 27                              sample, 96
   multifactor studies, 194 -202              dummy, 706                             transforming to stabilize, 194
   multiple samples, 193-194                     for regression analysis, 713     Variances
   single sample, 192-193                     experimental, 38                       equal, 651
Transmission of variance formulas, 311        extraneous, 40                         estimate for multiple linear regression
                                              factors, 12
      (see also propagation of error)         handling extraneous, 40 - 43                 model, 675-682
Treatment sum of squares, 484                 iid random, 291                        estimate for simple linear regression
Trend charts, 75-77 (see also run charts)     independent continuous random, 299
Truncated histogram, 73                       independent discrete random, 289             model, 651-658
Tukey's method, 474 - 477, 479                jointly continuous random, 292-297  Variation, 498
                                              jointly discrete random, 279-283
   comparing main effects, 562-567            linear combinations of random,      Wear-out, 763
Two proportions, inference for the                  307-310                       Weibull distributions, 260 -263, 761
                                              lurking, 5
      difference between, 407- 413            managed, 38                            mean of, 260
Two-level factorials, standard fractions      plots against process, 101-102         median of, 261
                                              qualitative, 27                        variance of, 260
      of, 591-611                             random, 221-223, 222                Weibull paper, 276 -277
Two-way factorial notation, 551-554           response, 38                        Weibull probability density, 260
Type I error, 353                             supervised, 38                      Whiskers, 82
                                              taxonomy of, 38-39                  Wood joint strength, measuring, 15
   probability, 354
Type II error, 353                                                                Yates algorithm, 188-189
                                                                                     reverse, 189
   probability, 354
                                                                                  Yates standard order, 188
u charts, 523-527 (see also Shewhart
      control charts)

Uniform histogram, 73
Univariate data, 11
                                           IMPORTANT
                        If the CD-ROM packaging has been opened,
                   the purchaser cannot return the book for a refund!
                         The CD-ROM is subject to this agreement!

                     LICENSING AND WARRANTY AGREEMENT

Notice to Users: Do not install or use the CD-ROM until you have read and agreed to
this agreement. You will be bound by the terms of this agreement if you install or use the
CD-ROM or otherwise signify acceptance of this agreement. If you do not agree to the
terms contained in this agreement, do not install or use any portion of this CD-ROM.

License: The material in the CD-ROM (the "Software") is copyrighted and is pro-
tected by United States copyright laws and international treaty provisions. All rights
are reserved to the respective copyright holders. No part of the Software may be re-
produced, stored in a retrieval system, distributed (including but not limited to over
the www/Internet), decompiled, reverse engineered, reconfigured, transmitted, or tran-
scribed, in any form or by any means--electronic, mechanical, photocopying, record-
ing, or otherwise--without the prior written permission of Duxbury Press, an imprint
of Brooks/Cole (the "Publisher"). Adopters of Vardeman and Jobe's Basic Engineering
Data Collection and Analysis may place the Software on the adopting school's network
during the specific period of adoption for classroom purposes only in support of that
text. The Software may not, under any circumstances, be reproduced and/or downloaded
for sale. For further permission and information, contact Brooks/Cole, 511 Forest Lodge
Road, Pacific Grove, CA 93950.

U.S. Government Restricted Rights: The enclosed Software and associated documen-
tation are provided with RESTRICTED RIGHTS. Use, duplication, or disclosure by the
Government is subject to restrictions as set forth in subdivision (c)(1)(ii) of the Rights in
Technical Data and Computer Software clause at DFARS 252.277.7013 for DoD con-
tracts, paragraphs(c)(1) and (2) of the Commercial Computer Software-Restricted Rights
clause in the FAR (48 CFR 52.227-19) for civilian agencies, or in other comparable
agency clauses. The proprietor of the enclosed software and associated documentation
is Brooks/Cole, 511 Forest Lodge, Pacific Grove, CA 93950.

Limited Warranty: The warranty for the media on which the Software is provided
is for ninety (90) days from the original purchase and valid only if the packaging for
the Software was purchased unopened. If, during that time, you find defects in the
workmanship or material, the Publisher will replace the defective media. The Publisher
provides no other warranties, expressed or implied, including the implied warranties
of merchantability or fitness for a particular purpose, and shall not be liable for any
damages, including direct, special, indirect, incidental, consequential, or otherwise.

                                     For Technical Support:
                   Voice: 1-800-423-0563 E-mail: support@kdc.com.
