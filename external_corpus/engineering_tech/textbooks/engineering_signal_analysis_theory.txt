Engineering Signal Analysis  Christian Tiberius
                                    Max Mulder
from Fourier to filtering

                             Theory
Engineering Signal Analysis

           - from Fourier to filtering -

               Theory

        Christian Tiberius & Max Mulder
ii
Cover design by CYANETICA (cyanetica.com), Sebastiaan de Stigter

Engineering Signal Analysis - Theory
Christian Tiberius, Max Mulder
January 2026, first edition
Publisher:
TU Delft OPEN Publishing TU Delft Open Textbooks
Delft University of Technology - The Netherlands
keywords:
Fourier series, Fourier transform, sampling, Discrete Fourier Transform, spectral estimation,
linear systems, filtering
ISBN (softback/paperback): 978-94-6384-901-2
ISBN (e-book): 978-94-6518-235-3
DOI: https://doi.org/10.59490/mt.247

This textbook is licensed under a Creative Commons Attribution-BY 4.0 License (CC BY 4.0)
unless otherwise stated.

The text has been typeset using the MikTex 2.9 implementation of LATEX. Graphs have been
created in Python, and drawings and diagrams with Inkscape.
                              Contents

Preface                       ix

I Introduction                1

1. Introduction               3

1.1 Signals and systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4

1.2 Signals in time and frequency . . . . . . . . . . . . . . . . . . . . . . . . . . 4

         1.2.1 Spectral analysis principles . . . . . . . . . . . . . . . . . . . . . . . 5

         1.2.2 Spectral analysis applications . . . . . . . . . . . . . . . . . . . . . 7

1.3 Systems in time and frequency . . . . . . . . . . . . . . . . . . . . . . . . . . 8

1.4 This book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10

         1.4.1 Definitions and notations . . . . . . . . . . . . . . . . . . . . . . . . 10

         1.4.2 Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10

2. Signal preliminaries       13

2.1 Signal models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13

         2.1.1 Deterministic and random signals. . . . . . . . . . . . . . . . . . . 13

         2.1.2 Continuous and discrete signals . . . . . . . . . . . . . . . . . . . . 14

         2.1.3 Periodic and aperiodic signals . . . . . . . . . . . . . . . . . . . . . 15

2.2 Signal symmetry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

         2.2.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

         2.2.2 Even and odd signal parts . . . . . . . . . . . . . . . . . . . . . . . . 19

2.3 Sinusoid building block. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19

         2.3.1 Phasor description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

         2.3.2 Single-sided spectrum . . . . . . . . . . . . . . . . . . . . . . . . . . 21

         2.3.3 Double-sided spectrum . . . . . . . . . . . . . . . . . . . . . . . . . . 21

2.4 Signal operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23

         2.4.1 Rules. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23

         2.4.2 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

         2.4.3 Constructing signals using building blocks. . . . . . . . . . . . . 25

2.5 Energy and power . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26

         2.5.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26

         2.5.2 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28

2.6 Practical implications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29

II Continuous time            31

3. Real Fourier series        33

3.1 Rationale and definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33

3.2 Derivation Fourier series coefficients . . . . . . . . . . . . . . . . . . . . . . 34

3.3 Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

3.4 Example: Fourier series of a square wave . . . . . . . . . . . . . . . . . . . 36

3.5 Effects of symmetry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38

                         iii
iv                                     Contents

    3.6 Single-sided spectrum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
            3.6.1 Derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
            3.6.2 Alternative definition of real Fourier series . . . . . . . . . . . . . 40
            3.6.3 Diagram of coefficients . . . . . . . . . . . . . . . . . . . . . . . . . . 42

    3.7 Parseval: real Fourier series. . . . . . . . . . . . . . . . . . . . . . . . . . . . 43

4. Complex exponential Fourier series  45

    4.1 Derivation of the complex exponential Fourier series . . . . . . . . . . . 45

    4.2 Interpretation, polar form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46

    4.3 Effects of symmetry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47

    4.4 Double-sided spectrum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47

    4.4.1 Diagram of coefficients . . . . . . . . . . . . . . . . . . . . . . . . . . 48

    4.4.2 Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48

    4.5 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49

    4.6 Two Fourier series theorems . . . . . . . . . . . . . . . . . . . . . . . . . . . 52

    4.6.1 Time shift theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52

    4.6.2 Differentiation theorem . . . . . . . . . . . . . . . . . . . . . . . . . . 53

    4.7 Parseval: complex exponential Fourier series . . . . . . . . . . . . . . . . 54

5. Fourier transform                   55

    5.1 Rationale: development towards aperiodic signals . . . . . . . . . . . . . 55

    5.2 Derivation of the Fourier transform . . . . . . . . . . . . . . . . . . . . . . . 56

    5.3 Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57

    5.4 Interpretation, polar form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58

    5.5 Effects of symmetry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59

    5.6 Double-sided spectrum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59

    5.7 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60

    5.8 Fourier transform in-the-limit . . . . . . . . . . . . . . . . . . . . . . . . . . 62

    5.8.1 Dirac delta function and constant . . . . . . . . . . . . . . . . . . . 63

    5.8.2 Cosine and sine. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63

    5.8.3 Relation with the complex exponential Fourier series . . . . . . 64

    5.8.4 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65

    5.9 Parseval: Fourier transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67

6. Fourier transform theorems          69

    6.1 Linearity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69

    6.2 Time shift . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70

    6.3 Time scale change . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70

    6.4 Time reversal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71

    6.5 Duality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71

    6.6 Frequency translation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72

    6.7 Modulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72

    6.8 Convolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72

    6.9 Multiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73

    6.10 Differentiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74

    6.11 Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74

    6.12 Relation with the complex exponential Fourier series . . . . . . . . . . . 75

    6.13 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
Contents                                          v

7. Convolution                                    77

7.1 Definition and rationale . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77

7.2 Properties of convolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79

7.3 Convolution with Dirac pulse . . . . . . . . . . . . . . . . . . . . . . . . . . . 81

7.4 Convolution in frequency. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81

7.5 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82

8. Finite signal duration, leakage and windowing  85

8.1 Finite signal duration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85

8.2 Leakage of harmonics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87

8.3 Identifying neighboring frequencies . . . . . . . . . . . . . . . . . . . . . . . 88

8.4 Consequences for energy and power . . . . . . . . . . . . . . . . . . . . . . 89

8.5 Mitigating leakage: windowing . . . . . . . . . . . . . . . . . . . . . . . . . . 90

III Discrete time                                 93

9. Sampling                                       95

9.1 Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95

9.2 Impulse train sampling model . . . . . . . . . . . . . . . . . . . . . . . . . . 97

9.3 Derivation of Fourier transform of sampled signal . . . . . . . . . . . . . 99

9.4 Sampling theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100

9.5 Aliasing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102

10. Signal reconstruction                         105

10.1 Ideal signal reconstruction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105

10.2 Zero-order hold reconstruction . . . . . . . . . . . . . . . . . . . . . . . . . . 109

11. Discrete-Time Fourier Transform               115

11.1 Derivation of the DTFT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115

11.2 Alternative derivation and formulation . . . . . . . . . . . . . . . . . . . . . 118

11.3 Inverse DTFT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118

11.4 DTFT properties. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119

11.5 Discrete-time complex exponential . . . . . . . . . . . . . . . . . . . . . . . 120

12. Discrete Fourier Transform                    123

12.1 Finite length - discrete frequency . . . . . . . . . . . . . . . . . . . . . . . . 123

12.2 Definition of the DFT and inverse DFT . . . . . . . . . . . . . . . . . . . . . 125

          12.2.1 DFT derivation and definition. . . . . . . . . . . . . . . . . . . . . . 125

          12.2.2 Inverse DFT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126

          12.2.3 Alternative definition . . . . . . . . . . . . . . . . . . . . . . . . . . . 126

          12.2.4 Fast Fourier transform (FFT) . . . . . . . . . . . . . . . . . . . . . . 127

12.3 DFT frequencies, properties, and diagram. . . . . . . . . . . . . . . . . . . 127

          12.3.1 DFT frequencies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127

          12.3.2 DFT properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128

          12.3.3 DFT diagram of coefficients . . . . . . . . . . . . . . . . . . . . . . . 129

12.4 DFT example ( = 8) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129

12.5 Two more examples: DFT of cosine . . . . . . . . . . . . . . . . . . . . . . . 131

12.6 Window and leakage - revisited . . . . . . . . . . . . . . . . . . . . . . . . . 133

12.7 Zero-padding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
vi                                     Contents

IV Spectral estimation                 137

13. Energy and power spectral density  139

    13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139

    13.2 Continuous-time signals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140

    13.2.1 Power spectrum and power spectral density . . . . . . . . . . . . 140

    13.2.2 Energy spectral density. . . . . . . . . . . . . . . . . . . . . . . . . . 141

    13.2.3 Finite-duration signals . . . . . . . . . . . . . . . . . . . . . . . . . . 143

    13.3 Discrete-time signals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144

    13.3.1 Energy spectral density. . . . . . . . . . . . . . . . . . . . . . . . . . 144

    13.3.2 Power spectral density . . . . . . . . . . . . . . . . . . . . . . . . . . 145

14. Spectral estimation                147

    14.1 Assumptions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147

    14.2 Power spectral density . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147

    14.3 Periodogram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148

    14.4 Statistical properties of the periodogram . . . . . . . . . . . . . . . . . . . 151

    14.5 Welch periodogram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152

15. Spectrogram                        155

V Linear systems                       159

16. Linear time-invariant systems      161

    16.1 System properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161

    16.1.1 Instantaneous and dynamic systems . . . . . . . . . . . . . . . . . 162

    16.1.2 Time-invariant and time-varying systems . . . . . . . . . . . . . . 163

    16.1.3 Linear and nonlinear systems . . . . . . . . . . . . . . . . . . . . . 163

    16.1.4 Causal and non-causal systems . . . . . . . . . . . . . . . . . . . . 164

    16.1.5 Continuous-time and discrete-time systems . . . . . . . . . . . . 164

    16.1.6 Linear time-invariant systems . . . . . . . . . . . . . . . . . . . . . 164

    16.2 System response . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164

    16.3 Impulse response function. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166

    16.4 Frequency response function . . . . . . . . . . . . . . . . . . . . . . . . . . . 166

    16.4.1 System response: magnitude . . . . . . . . . . . . . . . . . . . . . . 167

    16.4.2 System response: phase . . . . . . . . . . . . . . . . . . . . . . . . . 167

    16.4.3 Bode plot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168

17. Measuring signals                  169

    17.1 Sensor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169

    17.2 Frequency response . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170

    17.3 Impulse response. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171

    17.4 Example. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172

    17.5 Sensor and object . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173

18. Filters                            175

    18.1 Ideal filters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175

    18.1.1 Causality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176

    18.1.2 Impact of ideal filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
Contents                                          vii

18.2 Practical filters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
        18.2.1 Cut-off frequency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
        18.2.2 Frequency response function - phase revisited . . . . . . . . . . 180
        18.2.3 Low-pass filter analogy: mechanical system . . . . . . . . . . . . 180
        18.2.4 FIR and IIR. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181

18.3 Digital filters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
        18.3.1 Digital Butterworth filter . . . . . . . . . . . . . . . . . . . . . . . . . 182
        18.3.2 Moving-average filter . . . . . . . . . . . . . . . . . . . . . . . . . . . 183

Appendices                                        185

A. Common mathematical formulas                   187

A.1 Trigonometric identities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187

A.2 Orthogonality of sines and cosines . . . . . . . . . . . . . . . . . . . . . . . 187

A.3 Definite integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188

A.4 Indefinite integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188

A.5 Series expansions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188

B. Elementary functions                           189

B.1 Unit pulse function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189

B.2 Unit triangular function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189

B.3 Sine cardinal function (sinc) . . . . . . . . . . . . . . . . . . . . . . . . . . . 189

B.4 Singularity functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190

          B.4.1 Unit step function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190

          B.4.2 Unit ramp function . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191

          B.4.3 Dirac delta function . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191

C. Complex algebra                                193

C.1 Complex plane . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193

C.2 Euler's formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194

C.3 Complex conjugate. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194

C.4 Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195

C.5 Phasor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195

D. Quantization                                   197

E. Fourier transform pairs                        199

F. Discrete convolution                           201

G. DTFT addendum                                  203

G.1 Addition to proof of DTFT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203

G.2 Proof of inverse DTFT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204

G.3 DTFT pairs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204

H. DFT addendum                                   207

H.1 Proof of inverse DFT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207

H.2 DFT matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208

H.3 DFT pairs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209

I. Spectral analysis in practice: full procedure  211

I.1 Window - time domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211

I.2 Sampling - time domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212

I.3 Evaluating Fourier transform at discrete frequencies . . . . . . . . . . . 213
viii                                                    Contents

J. Confidence interval of periodogram                   215

      J.1 Estimating amplitudes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215

      J.2 Confidence interval for periodogram . . . . . . . . . . . . . . . . . . . . . . 217

      J.3 Variance of periodogram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218

      J.4 Advanced spectral estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . 218

K. Correlation                                          219

      K.1 Deterministic signals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219

      K.1.1 Continuous-time signals . . . . . . . . . . . . . . . . . . . . . . . . . 219

      K.1.2 Discrete-time signals . . . . . . . . . . . . . . . . . . . . . . . . . . . 221

      K.2 Random signals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222

      K.2.1 Continuous-time signals . . . . . . . . . . . . . . . . . . . . . . . . . 223

      K.2.2 Discrete-time signals . . . . . . . . . . . . . . . . . . . . . . . . . . . 223

      K.2.3 Cross-correlation estimation in practice . . . . . . . . . . . . . . . 223

L. White noise                                          225

      L.1 Theoretical white noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225

      L.2 Band-limited white noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226

      L.3 Additive White Gaussian Noise (AWGN) . . . . . . . . . . . . . . . . . . . . 227

M. Solving first-order differential equation - example  229

      M.1 Homogeneous solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229

      M.2 Total solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229

      M.3 Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230

Bibliography                                            231
                                           Preface

This textbook provides an introduction to the subject of signal analysis in the frequency do-
main. Although many excellent textbooks already exist on this topic, there were a number of
reasons that motivated us to write our own book. We aim to provide an introductory volume:
concise, with a clear engineering-oriented storyline, and highlighting the practical use of the
theory. With 18 chapters, well under 200 pages, and written in an informal manner, the
book is explicitly meant as an introduction. The content is aimed to be taught at the bachelor
(undergraduate) level of an academic engineering curriculum. After studying this book, the
student should have an understanding of the basic concepts in signal analysis and be able to
apply them to actual signals.

    While providing a theoretical framework, the book's main focus is on explaining how to
perform signal analysis in practice. It focuses on analyzing continuous-time signals, as in
many engineering applications signals are indeed, in physical reality, continuous-time. Since
much of the measuring, processing and analyzing of signals takes place using a computer, in
discrete time, a significant part of the book explains the effects of sampling and working with
discrete-time signal representations of the continuous-time phenomena.

    The book has five parts. In Part I, the subject of signal analysis and the reasons to
study signals in the frequency domain are discussed, including a chapter with (mathemati-
cal) preliminaries and definitions relevant to signal analysis. In Part II the transformations
of continuous-time signals to the frequency domain are introduced: the Fourier series and
Fourier transform. The principle of signal convolution is explained, as well as effects of finite
signal duration - inevitable when measuring signals in practice, causing spectral leakage. In
Part III signal sampling and reconstruction are explained, the Discrete-Time Fourier Transform
is derived and the most common signal transformation applied in practice, the Discrete Fourier
Transform, is introduced. In Parts IV and V, respectively, spectral estimation and linear sys-
tems are introduced. A number of appendices contain supplementary material, to review prior
knowledge, offer further insight, and provide guidelines for practical application.

    Although this book, Theory, already includes many worked-out examples, it is best used
together with its companion book, Exercises. This second volume contains more exercises,
more (practical) examples and computer-based problems with Python.

    The authors welcome corrections and suggestions for improvement, as well as feedback
in general. Please e-mail ESAbook-TUD@tudelft.nl for this purpose.

Acknowledgments
First and foremost, our students deserve our gratitude. Teaching this material has been a
tremendous inspiration. Working with our students, the interaction with young minds, the
feedback received, and their many clever questions have made us better teachers, and hope-
fully has led to a book that will inspire future generations.

    Christian would like to acknowledge professor Roland Klees for sharing valuable insights
during several years of co-teaching classes on the subject of signal analysis. Many colleagues
of the faculty of Civil Engineering and Geosciences contributed indirectly, in particular from
the departments of Engineering Structures, Hydraulic Engineering, Water Management, Geo-
science and Engineering, and Geoscience and Remote Sensing.

    Max would like to thank his teachers, mentors and colleagues from the faculties of Aerospace
and Mechanical Engineering. Emeritus professors Bob Mulder and Henk Stassen for their in-

                                                            ix
x  Contents

spiring lectures, academic guidance and mentorship, which motivated him to spend his life
working on topics related to signals, systems and control. Ton van Lunteren, Hans van der
Vaart, Jan-Willem van Staveren, Qi Ping Chu and René van Paassen for expanding his knowl-
edge and insight and fostering his curiosity.

    The authors would like to thank Eric Verschuur and Rowenna Wijlens for reviewing major
parts of this book and providing valuable feedback. All remaining errors are our own.

    Thank you, Sebastiaan de Stigter (cyanetica.com), for designing such an inspiring book cover.
    Thank you, Saskia Roselaar (Roselaar Tekstadvies), for your meticulous proofreading.
    Finally, we have appreciated all support from the TU Delft OPEN Publishing team: thank
you to Jacqueline Michielen-van de Riet, Michiel Munnik and Kees Moerman.

Christian Tiberius, Max Mulder
Delft, January 2026
     I

         Introduction

1
            1

                                    Introduction

Signals are everywhere around us. Signals, and the associated systems which operate on
these signals, are of paramount importance in science, engineering and society, see Fig-
ure 1.1. In engineering and technology, examples are countless, ranging from smartphones,
communication, radar and sonar, noise and acoustics, monitoring and maintenance, remote
sensing, media and entertainment, robotics, vehicles on earth, in the air and in space, to the
transition to clean energy. Also in the biological, neuroscience, medical and sociological do-
mains, the capability to dissect the matter at hand (cells, brain, body, groups of people) into
smaller components, sub-systems, and to identify and measure the signals with which these
components interact in various command, control and communication hierarchies in order to
achieve a particular goal, is crucial. The `signals and systems perspective' is perhaps the most
versatile model of the world that exists in science. This book provides a brief introduction to
this broad topic, focusing in particular on the analysis of signals in engineering applications.

Figure 1.1: Applications of signals and systems are everywhere. All images from [1], public domain or open
license all attributions and licenses are summarized on page 232.

                                                             3
4  1. Introduction

1.1 Signals and systems

A generic signals and systems view on the world is illustrated in Figure 1.2 in its most basic
form. In engineering, a system is often considered as some device or algorithm which performs
an operation on a signal, the input signal (), to create an output signal, (). Systems can be
connected to create larger systems, with signals passing to and from the system components.

    Systems can exist in physical reality (mechanical, electrical or computerized, and combina-
tions of these) or as just a script running on a computer. In most cases, signals are functions
of time, either in continuous time, () or, when computers or other discrete-time devices
are involved, in discrete time, . While systems often have multiple inputs and outputs, this
book limits the discussion to single-input single-output systems, i.e., SISO systems.

Figure 1.2: A system turns input signal () into output signal ().

    The operation { } that the system performs on the input signal can represent many
possible actions or phenomena. Systems can be designed to perform a specific operation,
such as in a filter, where the system operates on the frequency content of the input signal,
e.g., to create an output signal in which the high-frequency components of the input signal
have been removed, as illustrated in Figure 1.2. But we could also apply the same perspective
to investigate the dynamic characteristics of a device or structure that exists in physical reality,
by studying the response of that device to a cleverly designed input signal. For instance, in
a wind tunnel we can study how the blade of a wind turbine (the system studied) bends (the
output of the system) as the result of applying variable wind loads on that blade (the input to
the system), revealing the dynamic response of this wind turbine blade.

    Whereas we can measure, characterize and analyze system input and output signals, and
often also the operation performed by the system, in the time domain, it turns out that this
analysis is generally much easier and provides more and more useful information when per-
forming it in the frequency domain. Both signals (functions of time) and systems (their dy-
namic response in time) can be transformed to the frequency domain.

1.2 Signals in time and frequency

The analysis of signals, i.e., functions of time, in the time domain is often the first thing we
do. For instance, after performing an experiment we plot the measured signal as a func-
tion of time, with time on the horizontal axis, and signal value on the vertical axis, to `see
what the signal looks like'. In many applications, however, such as in analyzing speech or
other vibration-related signals, the signal may be difficult to interpret in time. Effects of mea-
surement noise and the complexity of the signal studied may be prohibitive in detecting the
underlying phenomenon. For example, periodic or oscillatory behavior (such as heart rhythms
or machinery vibrations) may be hidden within seemingly random signals.

    This is where spectral 1 analysis, that is, studying the signal in the frequency domain, has
proven to be extremely useful. Spectral analysis involves the decomposition of a signal into
its constituent frequency components, as if we `build' the signal using basic building blocks.
Compare this operation to placing a triangular glass prism (the system) into a beam of white

1From the Latin noun spectrum (plural: spectra), meaning `an appearance, an image' the noun comes from the
 Latin verb specere, meaning `to look at, to view'.
1.2 Signals in time and frequency                                5

light (the input to the system): the refraction of the prism (its output) will be a triangular beam
of light with all visible colors separated, like a rainbow. This is because white light `contains'
all visible colors (wavelengths and hence frequencies), and the prism, through frequency-
dependent refraction, splits the white light into its constituent frequency components, the
individual colors. The prism splits the input signal and thus performs a spectral decomposition,
allowing for spectral analysis. The prism allows us to `see' what white light is composed of.

1.2.1 Spectral analysis principles

Sinusoids act as our gateway between the time domain and frequency domain. Sinusoid func-

tions are very convenient: they are simple, they represent a pure oscillation with a specific

frequency, they are continuous functions of time (without discontinuities) and they differenti-

ate and integrate again into sinusoid functions.

    In spectral analysis, requiring the transformation of signals from the time to the frequency

domain, the waveform that acts as the main building block is the cosine function () =
 cos(20 + ). This function is characterized by three parameters: its frequency 0 in
Hertz, its amplitude  and its phase  in radians.

    Plotting this building block as a function of time  will show an oscillation, cosine-shaped,
ranging between values + and -, i.e., a periodic function with period 0 in seconds equal
to 1 . Plotting this building block as a function of frequency  requires two graphs: one that

     0

shows the amplitude of the signal as a function of frequency (), and one that shows the
phase of the signal as a function of frequency (). For the cosine function, the amplitude and
phase functions are zero for all frequencies, except at the frequency 0 where the amplitude
function equals , (0) = , and where the phase function equals , (0) = .

    An arbitrary periodic signal (), with period 0 and corresponding fundamental frequency
0, can be built by adding cosine function building blocks with different frequencies, amplitudes
and phases, resulting in a `model' of this signal, (). The frequency of each building block
must be an integer multiple of the fundamental frequency 0 for that building block to be
periodic with 0. They are all harmonic frequencies, with 0 also known as the `first harmonic'.
As will be discussed in Chapter 3, this also means that the building blocks are orthogonal. The

model () of signal () can be written as:

() = 0 + 1 cos(210 + 1) + 2 cos(220 + 2) + 3 cos(230 + 3) + ...

                           -1

= 0 +   cos(20 + )                 (1.1)

                =1

The value of constant 0 (representing the average of ()) and the amplitudes and phases
of the other  - 1 building blocks () =  cos(20 + ), for  = 1, ... ,  - 1, need to
be chosen in such a way that the model () represents the signal () in the best possible
way (e.g., through minimizing the mean square of error signal () = () - ()).

    Each th building block is a component of the signal, with amplitude  and phase ,
at frequency 0. Constructing a model of signal () is equivalent to decomposing that
signal into its constituent frequency components. It allows us to `see' what components are

important to build the signal, i.e., at what frequencies the signal `is happening with how much

amplitude' in other words, we `see' the signal spectrum.

    This process of signal (de)composition is illustrated in Figure 1.3. The top figure shows

signal () as a black dashed line, a square wave which alternates between 1 and -1 from
 = - to  with a period of 2 seconds. Obviously, this signal also repeats itself every 4 s
and 6 s et cetera, but the smallest repetition interval is 2 s, referred to as the fundamental
period 0 of this waveform. Its fundamental frequency 0 then equals 0 = 10 = 12 Hz.
6                                                                                                                                1. Introduction

              x(t), ~x(t)   1                     x(t)

                             0 x~(t)

                            -1

                                 0             1                                                          2              3            4

                                                                                                          t [s]

   magnitude

   phase

          9f0                                                                                                                                        x~(t)
                   8f0                                                                                                   x(t)
                             7f0
                                      6f0                                                                                                         T0
                                                5f0
                                                          4f0
                                                                     3f0
                                                                               2f0
                                                                                          f0
                                                                                                    0

                                                  f [Hz]                                                          t [s]

              magnitude Ak  2

                            1

                            0 k=0 1 2 3 4 5 6 7 8 9

                                    0  0.5  1     1.5                                                  2     2.5  3      3.5  4  4.5

                                                                                                          f [Hz]

              phase k       0

                            - 2

                            -

                                    0  0.5  1     1.5                                                  2     2.5  3      3.5  4  4.5

                                                                                                          f [Hz]

Figure 1.3: Transforming a periodic square wave () (period 0 = 2 s) from the time domain (top) to the
frequency domain (below). The middle figure (inspired by [2]) shows, in front, the square wave (black dashed

curve) assembled as the sum () (light blue curve) of ten cosine `building blocks'  cos(20 + ) with
 = 0, 1, 2, ... , 9. The cosine waves are harmonically related as their frequencies are integer multiples of 0 = 10 =
1 Hz. At these frequencies, the cosine waves can have different amplitudes  (magnitude) and phases , as

2

indicated by the stems on the left. The figure below shows the spectrum of ().

    The middle figure in Figure 1.3 shows in a three-dimensional plot how the same signal
can be considered as a function of time  in [s], shown on the right axis, and as a function
of frequency  in [Hz], on the left axis. The signal in time is decomposed into cosine building
blocks, together forming the signal model (). The frequency of each building block is an
integer multiple of the fundamental frequency 0 of the periodic waveform they are all har-
monics. The amplitude of each building block depends on how much of this building block is
needed to construct signal (). The phase of each building block depends on how much the
building block must be shifted forward or backward in time to help construct signal ().

    The signal model () thus constructed uses  = 10 building blocks,  = 0, 1, 2, ... , 9, with
frequencies equal to integer multiples of 0, as indicated by the frequency axis  in [Hz]. Five
of these building blocks are zero. Adding the five non-zero building blocks results in the light
blue signal shown in the top and middle graphs. The signal model alternates in a similar way
as the square wave but is clearly not exactly the same. Using more building blocks, with
1.2 Signals in time and frequency                                                          7

higher frequencies (and typically smaller amplitudes) will increase the accuracy of our model.

The bottom figure in Figure 1.3 shows the amplitude (or magnitude) and phase (in radians)

of the signal model () as a function of frequency. Given that we are modeling a periodic

signal, with fundamental frequency 0, both functions of frequency can only have non-zero
values at integer multiples of this frequency. The signal model is relatively simple and although

it can be further improved we `see' at a glance how the square wave is constructed.

Apparently, for this particular square wave, the building blocks for all even-indexed integer

multiples of 0 (00, 20, 40, 60, 80) are zero. All non-zero amplitude building blocks have a
phase equal to -  . This is because for this signal, switching to +1 at  = 0, we exclusively

                                2

need sine functions to construct it, and a sine function can be described as a cosine func-

tion having phase -  : sin  = cos ( -  ). These latter two findings are the result of the
2                                  2
symmetry properties of this square wave and will be further explained in Chapter 3.

The spectral decomposition just performed (at a basic level) allows for spectral analysis

and provides insight into what the dominant frequencies of this waveform are, i.e., what `main

components' signal () consists of. For aperiodic signals, which do not repeat themselves in

time, we follow the same reasoning but let the period 0 go to infinity, as if the signal does re-
peat itself after an infinite number of seconds. Then the building block fundamental frequency

becomes infinitesimally small, the building block components at the different frequencies will

lie infinitesimally close to each other in the frequency domain, ultimately forming a continuous

function of frequency, known as the Fourier transform () of signal ().

In Part II the theory of signal analysis will be introduced for periodic signals, as we have

done in this subsection, but also for aperiodic signals, using the Fourier transform. Signal

analysis theory will be explained first for the continuous-time case, and then in Part III for

discrete-time signals, the most common application, using the Discrete Fourier Transform.

1.2.2 Spectral analysis applications

A plethora of operations and applications opens up once a signal has been transformed to the
frequency domain, but only three will be discussed here.

    The spectral representation of a signal is often (much) more compact and meaningful than
the time-domain representation, especially for a signal that has repeating patterns. By just
focusing on the dominant frequency components of a signal, spectral analysis allows for easier
interpretation, operation, compression, storage and transmission. For instance, in communi-
cation, when sending a measured signal from one place to another, the communicated signal
could either be composed using the measured signal values in time or we could first perform
a frequency transform and then send the signal spectrum, that is, the values of the signal in
frequency. In many cases, the latter will require far less communication bandwidth. When
compressing a signal, we could first transform it to the frequency domain and then cut off
the higher-frequency components, thereby not losing much of the signal, given that for most
signals the higher-frequency components will indeed become smaller and smaller in ampli-
tude, as illustrated in Figure 1.3. Signal compression, such as composing a JPEG image, is
indeed a common application of signal analysis, illustrated in Figure 1.4. Despite a significant
compression, by a factor of 32, the object in the photo can still be recognized.

    Measurements often contain some form of randomness caused by measurement noise,
which often manifests itself at high frequencies. After transforming the signal to the fre-
quency domain, it becomes easier to identify, isolate and remove the unwanted noise or in-
terference from the desired signal. Filters can be designed to target the elimination of certain
frequency components of a signal. For instance, a so-called low-pass filter will pass the lower
frequency band of its input signal and block the higher frequency band of this same signal. In
entertainment applications, such as music or cinema, an equalizer can be used to boost the
8                                                                                          1. Introduction

Figure 1.4: Photo of TU Delft's laboratory aircraft PH-LAB, operated together with the Netherlands Aerospace
Center NLR, compressed to 1 (center) and 1 (right) of the original (copyrighted) at left.
   4  32

bass in a song or movie soundtrack through increasing the amplitudes of the lower-frequency
components while keeping the existing amplitudes of the higher-frequency components.

    Spectral analysis enables us to better understand and interpret the underlying structure
of signals by examining their frequency components, which is - apart from mostly trivial
cases - impossible in the time domain. A signal's dominant frequencies, possible harmonics
and distribution in frequency of the signal component amplitudes can be studied. This allows
potentially harmful resonant frequencies in mechanical systems to be identified and anomalies
in electrical circuits to be detected. Spectral analysis of vibrations or acoustics can reveal
information about machinery health and enables fault diagnosis, often in a non-destructive
manner. Specific patterns in the frequency domain and changes in these patterns can indicate
wear, imbalance, defects or other issues which may be impossible to `see' in the time domain.

1.3 Systems in time and frequency

While the focus of this book is on signal analysis in the frequency domain, in Part V we
introduce systems. Modeling and understanding a system's input-output relation, in time and
frequency, is a key competence of an engineer and is briefly explained by a simple example.

Figure 1.5: Mass-spring-damper system: a proof mass (in blue) is suspended by a spring and connected to a
dashpot damper it is positioned on a flat, perfectly horizontal table, such that effects of gravity can be ignored.
This system turns external motion, input acceleration , into output displacement  of the proof mass.

    Consider the single-degree-of-freedom mass-spring-damper system illustrated in Figure 1.5,
an elementary physical dynamic system. Its behavior is governed by a second-order differen-
tial equation:

   () + 20() + 02() = -()                                                                  (1.2)

with both the input  and output  axis positive pointing right. In this equation  is the
damping ratio (which is dimensionless), and equals  =  2 , 0 is the undamped natural

angular frequency in [rad/s], 0 =   , with  the mass in [kg],  the damper constant in
[kg/s], and  the spring constant in [N/m].

    Through solving (1.2) we find that the response of this system to a so-called impulse input
at  = 0 (such as hitting it with a hammer on one of its sides) equals a damped harmonic
1.3 Systems in time and frequency                                                                              9

0.02                                                                         0.006

                                                                  0.004
0.00

                                                                  0.002
h(t)
                                                                    |H(f )|

-0.02                                                                        0.000

       0.0  0.5     1.0            1.5  2.0                                         0  2  4          6  8  10

                    t [s]                                                                    f [Hz]

Figure 1.6: At left: impulse response () of mass-spring-damper system as a function of time , with mass
 = 1.0 kg, spring constant  = 2100 N/m and damping ratio  = 0.05. At right: corresponding FRF () as a
function of frequency , with  = 2. The dotted vertical line marks the damped natural frequency  = 7.28 Hz.
The dotted blue curve shows the FRF for a system with damping ratio that is twice as large:  = 0.10.

described as a function of time :                                                                          (1.3)
       () = - 1 -0 sin()
                   

for   0, with  = 01 - 2 the damped natural angular frequency in [rad/s]. This
response to an impulse input, (), is referred to as the impulse response function it is shown
in Figure 1.6 at left. The impulse response function fully characterizes the system response
in the time domain. When () is known, we can compute the response () of this system
to any input signal () through the procedure of convolution: () = ()  (), as will be
explained in Chapter 7.

    Having mastered the theory on signal analysis discussed in Part II, we will learn how to
also characterize the system response as a function of frequency, known as the Frequency
Response Function (FRF). Fourier-transforming the impulse response function () leads to
the FRF (). A crucial advantage of working with systems in the frequency domain rela-
tive to the time domain is that the tedious procedure of convolution, required to compute
the system's response to an input, becomes a simple multiplication in the frequency domain:
() = ()(), with () and () the Fourier transforms of the output and input signals,
respectively. In most cases, we calculate () by first moving the entire problem to the fre-
quency domain, using the Fourier transform, solve it there, and then inverse Fourier-transform
() back to ().

    The fact that signals and systems can both be transformed to the frequency domain, where
the interplay of signals and systems can be described using algebraic equations, is extremely
valuable. It allows for a quick and easy analysis of this interplay which is (apart from trivial
cases) not possible in the time domain. Again, for a particular class of systems, the impulse
response function () and its frequency domain equivalent, the FRF (), fully characterize
the dynamic behavior of that system and allow engineers to compute its response to arbitrary
input signals for instance, to predict the behavior of a structure (e.g., a wind turbine blade,
a car suspension system, a bridge deck, et cetera) under a dynamic load.

    In particular the FRF provides clear access to a structure's natural frequencies, which is of
crucial importance as this allows to prevent excessive and possibly devastating resonance of
the structure. The modulus of the FRF, relating input acceleration to output displacement of
the mass-spring-damper system, is shown in Figure 1.6 at right as a function of frequency 
in [Hz], and can be computed as:

() = 2           1                                                                                         (1.4)

             - 20 - 0      2
10  1. Introduction

Figure 1.7: The Tacoma Narrows bridge, a suspension bridge in the US state of Washington that spanned Puget
Sound, opened to traffic on July 1st, 1940, at left, and dramatically collapsed into Puget Sound on November 7th
of the same year, at right, because of variable wind loads acting on the bridge deck. The Tacoma Narrows bridge
is a classical and popular example of resonance in engineering, though the actual cause of the collapse was more
intricate. Images taken from Wikimedia Commons and Wikimedia Commons [1], public domain.

with angular frequency  = 2, and  the imaginary unit with 2 = -1. The FRF shows
the response of the structure to a sinusoidal input over the entire range of frequencies it
gives a direct and clear view on how that structure amplifies or dampens vibration at different
frequencies. For the mass-spring-damper system this graph shows a clear peak for the natural
frequency 0 the dotted vertical line is positioned at the damped natural frequency . The
width of the peak with regard to its height is driven by damping ratio . For comparison, the
graph also shows the FRF for a damping ratio twice as large:  = 0.10. For the undamped case
with  = 0.0 the peak will go to infinity and the system will start to resonate when excited at
that frequency. For small frequencies,   0: |()|    for large frequencies, |()|  21 
for   , |()| approaches zero. The FRF is a key element in structural dynamics for system
identification, model validation and monitoring, with applications in aerospace engineering,
civil engineering (see Figure 1.7) and many other fields.

1.4 This book

1.4.1 Definitions and notations

In this book, a signal is by default a function of time  in seconds. The Fourier transform of
a signal is by default a function of frequency  in Hertz. Note that a signal can instead be a
function of spatial position, or another running variable. In this book we stick to time  in [s]
and frequency  in [Hz].

    Signals in the time domain are written using lowercase letters, like (), while that same
signal in the frequency domain (its Fourier transform) is written using the uppercase equiva-
lent, ().

    We focus on one-dimensional signals and their transforms. For transforms in two or more
dimensions, like in imaging applications, the reader is referred to other books, e.g., [3].

    The unit of a signal (when provided) is given in square brackets [ ], for instance [V] if the
signal is a voltage measured by an electrical sensor.

1.4.2 Structure

The book consists of five parts, see Figure 1.8.
1.4 This book                                            11

    In Part I the book is introduced (this chapter) and the main signal analysis definitions
and concepts are explained in Chapter 2. Appendices A-D present common mathematical
formulas, define elementary functions, and explain complex algebra and the procedure of
signal quantization, respectively.

I introduction

               1      2        A  B               C   D

II continuous time             III discrete time

               3      4        9                  10

               5      6E       11 G 12 H

               7F     8           I

IV spectral analysis

               13     14 J 15     K                   L

V linear systems

               16 M 17    18

Figure 1.8: The book consists of five parts (light blue rectangles), eighteen chapters (squares) and thirteen
appendices (circles). Boldface chapter numbers indicate essential material. The theory on continuous-time signals
forms the main thread of this book. When working with discrete-time signals, the analogy with the continuous-
time counterpart is maintained, for the reason of interpretation of the results and implications in physical reality.

    Parts II and III contain essential material for engineers to understand and apply signal
analysis for, respectively, continuous-time and discrete-time signals.

    In many engineering applications the physical phenomena studied and the resulting (mea-
surement) signals are continuous-time: (), with   . Part II forms the theoretical heart of
the book and focuses on the Fourier transform of continuous-time signals. Chapters 3 and 4
first discuss the real and complex exponential Fourier series models for periodic signals. Chap-
ters 5 and 6 then introduce the Fourier transform and its main theorems, for aperiodic signals
and, in-the-limit, for periodic signals. Chapter 7 explains the concept and properties of signal
convolution, in the time and frequency domains. In Chapter 8 the consequences of time-
windowing a signal on the Fourier transform of that signal are explained. Though typically not
extensively covered in other sources, this chapter is a major step from infinite-duration signal
analysis theory to finite-duration signal analysis practice. Some Fourier transform pairs are
listed in Appendix E the convolution of discrete-time sequences is discussed in Appendix F.

    Since nowadays, most, if not all, of measuring, processing and analyzing signals takes
12  1. Introduction

place on the computer, in discrete time, Part III discusses how to work with the discrete-time
representation of continuous-time signal (): the sequence of samples , with   . First
the effects of sampling a continuous-time signal - the analog-to-digital conversion - on the
Fourier transform of that signal are discussed in Chapter 9, using the impulse train sampling
model. Chapter 10 discusses the reverse process - the digital-to-analog conversion - using
ideal reconstruction and reconstruction in practice. Chapter 11 then derives the Fourier trans-
form of the infinite-duration discrete-time sequence , the Discrete-Time Fourier Transform
(DTFT). Evaluating the DTFT of a time-windowed, and therefore finite-duration discrete-time
sequence, at a limited number of frequencies, leads to the Discrete Fourier Transform (DFT),
the most common signal transformation applied in practice, discussed in Chapter 12. Com-
mon DTFT and DFT pairs are summarized in, respectively, Appendices G and H. The full
procedure, from the Fourier transform of a continuous-time infinite-duration signal to the DFT
of a discrete-time finite-duration sequence, is summarized in Appendix I.

    Ultimately, one of the main goals of signal analysis is to compute and analyze the spectrum
of a signal. Spectral analysis is the topic of Part IV. In this part, first the energy spectral den-
sity (ESD) and power spectral density (PSD) functions of signals are defined in Chapter 13, for
continuous-time signals () and discrete-time sequences . The estimation of signal spectra
for random but stationary, finite-duration discrete-time sequences is the topic of Chapter 14.
The estimation of signal spectra for potentially non-stationary, finite-duration discrete-time
sequences, the spectrogram, is briefly discussed in Chapter 15. Appendix J defines the con-
fidence interval of the spectral estimate and Appendices L and K discuss, very briefly, the
concepts of (cross-)correlation and white noise.

    While this book focuses on the analysis of signals, in Part V the analysis of continuous-time
systems is introduced. Chapter 16 defines system properties and an important class of sys-
tems, namely the linear time-invariant (LTI) system. The dynamic response of an LTI system
is fully characterized in the time domain by its impulse response function, and in the frequency
domain by the Fourier transform of this function, the frequency response function (FRF). The
signals and systems perspective on practical measurements is discussed in Chapter 17, after
which the book concludes with a chapter on filtering, the most common purpose of systems
from the perspective of signal analysis, covered in Chapter 18. Appendix M explains, for a sim-
ple example, how to obtain the impulse response function of an LTI system from the system's
differential equation.
                               2

                          Signal preliminaries

In this chapter we introduce models and definitions to characterize signals as functions of
time. Some mathematical signal symmetry properties are discussed. We show how the si-
nusoid signal building block can be easily studied in the frequency domain, using phasors,
to obtain the single-sided or double-sided spectrum. Then the so-called singularity functions
are introduced as basis functions. The rules of signal operation in time are discussed, and it
will be shown how new signals can be built using basis functions. We define the concepts of
energy and power of signals in time, and explain what it means to move from mathematical
definitions to real-life measurements: all signals become energy signals.

    This chapter is accompanied by Appendices B-D, which provide background on some of
the mathematical foundations and practical implications.

2.1 Signal models

We discuss deterministic and random (or stochastic) signals, and the continuous-ness and
discrete-ness of signals both in time and in amplitude. We define a crucial characteristic, that
is, whether a signal is periodic or aperiodic, and introduce the most important `building block'
in signal analysis: the sinusoidal signal.

2.1.1 Deterministic and random signals

3                                                                         3

x(t)0                                                                     0
                                                                    y(t)

-3                                                                        -3

    0  1  2         3  4  5                                                   0  1  2         3  4  5

             t [s]                                                                     t [s]

Figure 2.1: Deterministic (left) and random (right) signal one realization of the random signal is shown.

    Figure 2.1 illustrates an example of a deterministic and a random signal. A deterministic
signal () is a specified function of time . That is, when we know , we know exactly what
value  the signal has: (). Examples are mathematical expressions, e.g.,

       () = 32 + 2 - 4,

                          13
14                                                              2. Signal preliminaries

sinusoids, and the unit pulse function () (width of 1 s) as defined in Appendix B:

                     1  || < 1                                                                (B.1)
            () = 0.5
                                  2
                                  1
                     0
                        || = 2
                        || > 1

                                  2

    A random (or stochastic) signal has random values at any moment in time, and can only be
modeled in a probabilistic way. Random signal () in Figure 2.1 (at right) will look different
every time we measure it. In real life, randomness in signals (some random appearance)
occurs constantly, because of the inherent variable nature of the phenomenon being observed
and because of measurement noise. A large portion of this book focuses on deterministic
signals. In reality, most signals are random in nature, and we return to this in Chapter 14.

2.1.2 Continuous and discrete signals

A second important characterization of a signal is whether it is continuous or discrete, both in
terms of what values a signal can take, as well as in the independent variable time , that is,
at what times does the signal `exist'. This is illustrated in Figure 2.2.

                                     time -                     discrete
                        continuous
            3                                         3

    x(t)    0                                   xn    0                                value -
                                                                                    continuous
            -3                                        -3

                0  1    2            3  4    5            0  5  10 15 20 25

                           t [s]                                n

            3                                         3

    xq (t)  0                                   xq,n  0                             discrete

            -3                                        -3

                0  1    2            3  4    5            0  5  10 15 20 25

                           t [s]                                n

Figure 2.2: Signal continuity in time (continuous at left, discrete at right) and in value (continuous on top, discrete
at bottom).

    Most of our theory aims at analyzing signals that are continuous in time (they exist at all
times   ) and value (they can have all values in ), as shown in the top-left of Figure 2.2.
In real life, the signals (measurements) we process in our computers are sampled (we only
have their values at discrete moments in time) and quantized (the samples cannot take on any
value because we have a limited number of bits to represent them (here, 3, so 23 = 8 possible
values)). The signals we deal with in practice resemble the bottom right of Figure 2.2.

    The principle of quantization is explained in Appendix D. Because the number of bits can
be large (typically 14-16 bits or even more) the effects of quantization are ignored in this book.
This means that our theory considers a discrete-time signal as illustrated at the top right of
Figure 2.2. In other words, all signals in this book will, from now on, as an approximation, be
considered continuous in value: ()  () and ,  .
2.1 Signal models                                                                              15

    Regarding notation, a continuous-time signal is indicated as () its discrete-time equiva-
lent (the sampled version of ()) is written as . While  can have any value   ,  is the
index of the sample,   , or . In this sense, `time is lost', and only when multiplying the
index with the sample interval  (0.2 s in Figure 2.2) we retrieve time on the horizontal axis.

2.1.3 Periodic and aperiodic signals
A signal () is periodic if and only if:

( + 0) = () ,                - <  <                                                            (2.1)

with 0 the fundamental period. 0 marks the smallest value in time after which the signal
repeats itself. The fundamental frequency in Hz is then 0 = 10 .

    Any signal not satisfying (2.1) for any value of 0 is called aperiodic. An example of an
aperiodic signal is the unit pulse (B.1).

2                                                                2
                         t0                                      1
                                                                 0
1                                                              -1 T0
                                                               -2
x(t)0
                                                         y(t)          -2 -1 0 1 2
                                                                                        t [s]
-1                           T0
-2                           12

        -2 -1 0

                   t [s]

Figure 2.3: Periodic signals: sinusoid (left) and saw-tooth (right). 0 indicates the period.

    A special case is when the signal is a constant, i.e., () = 3. This signal is periodic for
any 0 hence, the fundamental period cannot be determined.

Sinusoidal signals
The sinusoidal signal, the quintessential periodic signal that acts as the main building block of
signal analysis, is defined as follows:

() =  cos (20 + ) ,          - <  <                                                            (2.2)

Here, , 0 and  are constants referred to as the amplitude, frequency (in Hz) and (relative)
phase (in radians), respectively. Amplitude  carries the unit of the signal, [unit]. Note that

the cosine is taken as our reference (for defining the phase), not the sine. A signal that is

constant (() = ) can be interpreted as a cosine with zero frequency 0 = 0 Hz.
    An example (=1, 0 = 0.5 Hz,  = 1 rad) is illustrated in Figure 2.3. The phase shift  (in

radians) can also be considered as a time shift 0 =  20 = 2 0 (in seconds):

() =  cos (20 + ) =  cos (20 ( +                                   )) =  cos (20( + 0))        (2.3)

                                                               20

    This book focuses on signal analysis sinusoids are mostly characterized in Hertz (Hz), the

linear (or ordinary) frequency 0. This number represents the number of periods (or, as we will
see, full rotations in the complex plane) per second. In books focusing on (dynamic) system

analysis, sinusoids are often characterized using the angular (or radial) frequency 0, in rad/s,
where 0 = 20.
        16                                                               2. Signal preliminaries

        Commensurability

        A continuous-time sinusoidal signal () with frequency 0 is periodic. One often constructs
        periodic signals using sinusoidal components, e.g.:

            () = 1 sin(21) + 2 sin(22)                                                      (2.4)

        with (1, 2) and (1, 2) the amplitudes and frequencies of the two components, respectively.

        While the individual components are both periodic, what can we say about the periodic-ness of

        their sum? The concept of commensurability provides the answer. Two frequencies 1 and 2

        are commensurable if they have a common measure. That is, there is a number 0 contained

        in each of them an integer number of times: 1 = 10 and 2 = 20, with 1, 2  . The

        highest number 0 for which this is valid is the fundamental frequency. Integers 1 and 2
        are then the smallest integers defining the ratio, and 0 = 10 is the fundamental period.

            In other words, for commensurable frequencies the ratio of the frequencies of the com-

        ponents 1 = 2 = 1 is a rational number, 1  . Only when the frequencies of the
            2        1        2                           2
        components (two or more) are commensurable, their summation will yield a periodic signal.

EX 2.1      Suppose in (2.4) 1 = 1.5 Hz and 2 = 2.0 Hz. What is the fundamental frequency?

            Solution Their ratio equals 1 = 1.5 which, when written using (the smallest possible)
                                          2         2

            integers, equals 34 , a rational number. Then: 1 2 = 34 = 1 2 = 10 20 . Hence, 1 = 10 =
            30 = 1.5 Hz, so 0 = 0.5 Hz. In conclusion, the sum of components is periodic and we

            obtain fundamental frequency 0 = 0.5 Hz and period 0 = 2 s.

               x(t)     3
                        0
                      -3               1               2 t [s] 3         4               5

                           0                                                f1 = 1.5 Hz
                     =
                                    1         2        3                 6
                     +                                                   8 f2 = 2.0 Hz
                                 1     2         3     4

                                          T0                 T0

            Figure 2.4: Finding the fundamental frequency and period when adding two harmonic components.

            Figure 2.4 shows the summation () in the top row, and the two components in the
            middle and bottom row. At  = 0 the two components are synchronized (same phase),
            while the first time they both complete an integer number of periods (that is, the
            first time they are `starting again' together) is after 2 seconds. At that moment the
            first component has completed 3 full periods (=1), the second component 4 periods
            (=2). After 4 s this pattern repeats again the first component has completed 6
            periods (=21), the second component 8 periods (=22). Etcetera. The first time
            the two components repeat themselves together corresponds with the period. The
            fundamental frequency is the highest common frequency 0, as this belongs to the
            smallest time 0, the first time the resulting signal repeats itself.
2.1 Signal models                                                                           17

Suppose 1 = 3.5 Hz and 2 = 6.1 Hz. What is the fundamental frequency?                                                    EX 2.2

Solution The highest common frequency equals 1 2 = 3.5 6.1 = 35 61 = 10 20 , so 1 =
3.5 = 350 and 2 = 6.1 = 610, hence 0 = 0.1 Hz, 0 = 10 s. At that time, the first
component repeated itself 35 times, the second component 61 times. It is the first time
this happens after 20 s (then 0 = 0.05 Hz), 40 s (then 0 = 0.025 Hz) etc. it happens
again. We need the greatest common divisor to find the fundamental frequency.

Assume 1 = 2 Hz and 2 =  Hz. What is the fundamental frequency?                                                          EX 2.3

Solution Their ratio equals 1 = 2 , an irrational number  , not  . While the
                      2  

components are periodic, their summation will not be periodic.

    Do the magnitudes of the components matter here, or their relative phases? Consider
Figure 2.4, and you can see that the answer is no. Only the frequencies matter.

Singularity functions

The sinusoidal signal, (2.2), is periodic, oscillates about zero and runs from  = - to +.
But how can we describe phenomena that happen once and last for instance just a second,
or switch from one value to another in some time interval? This is where the aperiodic singu-
larity functions, defined in Appendix B, come into play: the unit impulse () (Dirac, or delta
function), unit step () and unit ramp () functions, see Figure 2.5. Especially the sifting
property of the Dirac function, see (B.12) and Figure B.4, is important.

                                          

1.0                      1.0                                                           1.5

(t)                                                                                    1.0
                                        u(t)
0.5                                                                              r(t)0.5

                                                                                       0.5

0.0                              0.0                 0.0

     -1 0          1   ddt  -1 0 1 t [s]   ddt  -1 0 1 t [s]

     t [s]

Figure 2.5: Unit impulse (), unit step (), unit ramp () functions, and how they relate through integration
(from left to right) and differentiation (from right to left). Note that the value of a function with a sudden jump is,
in this book, defined to lie `half-way the jump', Appendix B. The dashed line and small circle at  = 0 s in () are
drawn here to accentuate this point, but will be replaced with a solid line in later chapters.

    More singularity functions exist, such as the unit parabolic function, the unit pulse function
() (B.1) and unit triangular function () (B.2). Note that, whereas they are introduced
here as functions of time, these same functions can also have frequency  as their `running
variable', or any running variable for that matter. They are `just' mathematical functions often
used in signal analysis, in time and frequency. To prevent confusion, it is important to stick to
the notation ( for step function, etc.) and use lowercase for functions of time (()) and, in
later chapters, uppercase for functions of frequency (()).
18                                    2. Signal preliminaries

2.2 Signal symmetry

2.2.1 Definitions

Signals can exhibit characteristics that show a particular behavior in time, the most important
of which in our context is whether a signal is even, odd, or half-wave odd. These symmetry
properties are illustrated in Figure 2.6 and are defined in mathematical terms in Table 2.1.

        x(t)                          y(t) = x(t) + a, a > 0

    2                     2

                                        a

    0                     0

        -3 -2 -1 0  1  2  3 -3 -2 -1 0                 1      2  3

    2                     2

    0                     0             a

    -2                    -2

        -3 -2 -1 0  1  2  3 -3 -2 -1 0                 1      2  3

    1 1.0 t + T20 1 a
    0                     0

    -1  0.1 t             -1

        -3 -2 -1 0  1  2  3 -3 -2 -1 0                 1      2  3

        t [s]                         t [s]

Figure 2.6: Even, odd and half-wave odd symmetry: pulse (top), doublet (middle), square wave (bottom).

    The left-hand side of Figure 2.6 shows the even, odd and half-wave odd signals ()
which correspond to the mathematical definitions of Table 2.1. Evenness and oddness express
symmetry with respect to  = 0. The half-wave odd property (bottom left) is strictly for periodic
signals, here 0 = 2 s. Imagine taking a sample from this signal at any arbitrary moment in
time  (the red dot in Figure 2.6), and then taking another sample half of a period later (or
earlier)  ± 0 yields the same signal value but with opposite sign (green dot).

                       2

    The right-hand side of Figure 2.6 shows signal (), constructed by adding a non-zero
constant  to signal (): () = ()+. Adding the constant does not `destroy' the evenness
of the function, () is even. The same cannot be said about the oddness and half-wave
oddness property, which do not hold anymore for (). Odd signals and half-wave odd signals
must have a zero average. When subtracting the mean () from the signal () we obtain
(). Hence, non-zero mean signals can have characteristics of oddness or half-wave oddness
`about their average'. This will be discussed further in Chapters 3 and 4 on Fourier series.

Table 2.1: Mathematical definitions of signal symmetry properties.

             even             odd       half-wave odd
        () = (-)       () = -(-)      ( ± 0 ) = -()
                        zero average
                                                  2

                                         zero average

                                         periodic (0)

    When considering the half-wave odd signal () in the bottom left of Figure 2.6, we see
that this signal is neither odd nor even. Shifting it a little bit (0.1 s) to the right would make
2.3 Sinusoid building block                                                                    19

it an even signal. Shifting it 0.4 s to the left would make it an odd signal. In both cases the
signal remains half-wave odd. In fact, no matter how much we shift this signal to the left or
right, it will maintain this property. Signals can therefore be odd and half-wave odd, or even
and half-wave odd these properties are independent.

2.2.2 Even and odd signal parts
Note that any signal () can be written as the sum:

() = () + ()                                                                                   (2.5)

of an even signal component () and an odd signal component (), which in turn can be
computed as follows:

              1                                                                                (2.6)
() = (() + (-))                                                                                (2.7)

              2
              1
() = (() - (-))
              2

This finding has important consequences, as discussed in the following chapters. When ()
is even, the odd component () is zero when () is odd, the even component () is zero.
Except these special cases, signals typically have both even and odd components.

    As a first example, consider the half-wave odd signal () at the bottom left of Figure 2.6.
Figure 2.7 shows how the even and odd components of this signal are constructed. Note how
the components () and () share the same amplitude and period as ().

1           1.0

x(t)0                                      =
     xe(t)
                                                                xo(t)-10.1

        -2            0         2

                 t [s]

    1                                                                  1

    0                                         +                        0

    -1                                                                 -1

            -2               0          2                                  -2        0      2

                         t [s]                                                       t [s]

Figure 2.7: Even and odd parts of half-wave odd signal of Figure 2.6: () = () + ().

    As a second example, take the sinusoidal signal () =  cos(20 + ) introduced in
Section 2.2. Using the trigonometric relation (A.7) we obtain:

 cos(20 + ) = (cos)cos(20) + (-sin)sin(20)                                                     (2.8)

                             even part                                     odd part

with a zero phase cosine and zero phase sine, respectively, as a function of time. Hence, a
plain cosine and sine together, with the same frequency, can create a cosine with any phase
. Figure 2.8 illustrates the even and odd components of this signal for four values of .

2.3 Sinusoid building block

The sinusoid (2.2) is the most important building block in signal analysis. In Chapter 3 it
will be shown that periodic signals can be `constructed' using a Fourier series, a summation
20                                                               2. Signal preliminaries

             2     =0               = /6              = /3              = /2

             0        0      2                 2                 2

    x(t)   -2                0                 0                 0
               -2
                             -2                -2                -2

                          2      -2  0      2      -2  0      2      -2                                  0      2

    xe(t)  2                 2                 2                 2

           0                 0                 0                 0

           -2                -2                -2                -2

               -2  0      2      -2  0      2      -2  0      2      -2                                  0      2

    xo(t)  2                 2                 2                 2

           0                 0                 0                 0

           -2                -2                -2                -2

               -2  0      2      -2  0      2      -2  0      2      -2                                  0      2

                   t [s]             t [s]             t [s]                                             t [s]

Figure 2.8: Even and odd parts of sinusoidal signal, (2.8), for four values of phase shift  ( = 32 , 0 = 0.5 Hz).

of sines and cosines that share a common frequency. Sinusoidal signals have many features
that make them easy to use, such as orthogonality, discussed in Appendix A. But another
useful property of sinusoidal signals is that they can be easily described in (in other words:
`transformed to...') the frequency domain. While in this book we only deal with real-valued
signals, it is customary to represent these real-valued signals in terms of complex quantities.

2.3.1 Phasor description
The common way to describe a sinusoidal signal in the frequency domain is to use phasors, as
discussed in Appendix C. In short, we use Euler's1 formula (C.1) to relate real-valued sinusoidal
signals to the complex-valued exponential basis function, as illustrated in Figure 2.9.

    The complex exponential basis function 20, a complex number representing a unit
vector in the complex plane, rotates counterclockwise in the complex plane (top left) as 
advances. Its magnitude is one its phase at  = 0 is zero. The frequency 0 defines how fast
it rotates when 0 = 1 Hz, it makes a full rotation of 2 radians in 1 second. The real part of
this function is the cosine cos(20) (bottom left), the imaginary part of this function is the
sine sin(20) (top right). When  = 0 = 10 a full rotation is completed and the function
starts all over again, it is periodic. The complex conjugate of this function, -20, rotates
clockwise in the complex plane the real part is the same cosine, the imaginary part is the

negative sine, Figure C.4. This representation is at the very heart of Fourier analysis.

    The phasor description appears when representing a sinusoidal signal using the real part

of the complex exponential function:

    () =  cos (20 + ) = Re ((20+)) = Re(  20)                                                                      (2.9)

                                                                                                 phasor

The phasor is (usually) a complex number, scaling the complex exponential basis function (a
unit vector) magnitude from 1 to , and setting its phase angle at  = 0 seconds, from 0 to .
In other words, the phasor is the initial position ( = 0) of the complex vector (20+) in
the complex plane. Together with the frequency 0 of the basis function, () is fully described
by the phasor, (2.9).

1After the Swiss mathematician L. Euler (1707-1783).
2.3 Sinusoid building block                                                              21

ej2f0t j 1 Im

                                                t=t

                                Re                                                t [s]
                                                                               3.0
-1                           1      0      1.0  2.0
          2f0t
                                    -1
             -j

-1               0           1

            1.0                         Euler:

    t=t                                 {eju = cos u + j sin u
                                            e-ju = cos u - j sin u
            2.0
                                                  here u = 2f0t
            3.0                                        f0 = 0.5 Hz, t = 1.8 s
                 t [s]

Figure 2.9: The complex exponential basis function 20, uniting the sine (top right) and cosine (bottom left)
time functions in a unit vector rotating in counterclockwise direction in the complex plane.

2.3.2 Single-sided spectrum

A real-valued signal can be described as the projection of the complex rotating exponential,
multiplied by the phasor, on the real axis. To obtain the single-sided spectrum, we simply
show at what frequency (or frequencies) a signal () is active, i.e., has a non-zero amplitude.
At that frequency (or frequencies) the signal has an amplitude and phase, resulting in an
amplitude `as a function of frequency', the amplitude spectrum, and a phase `as a function
of frequency', the phase spectrum. This is illustrated in the left column of Figure 2.10, for a

                                                                

sinusoidal signal with  = 1.5,  = 2 and 0 = 2 Hz.

2.3.3 Double-sided spectrum

A real-valued signal can also be described as the summation of the complex rotating expo-
nential, multiplied by the phasor, and its complex conjugate, divided by two:

() =  cos(20 + ) = 1 ((20+) + -(20+))
                                        2

                             =  20 +  --20                                               (2.10)
                                2       2

This representation, in terms of two conjugate, oppositely rotating complex exponential func-
tions is the most common one in signal analysis (Figure C.4). It can be considered as the sum
of two rotating complex vectors, one with a positive frequency and one with a negative fre-
quency. Of course, negative frequencies do not exist in physical reality. It is a mathematical
abstraction that results from describing real-valued signals (which occur on the real axis of
the complex plane) using complex conjugate exponential basis functions.

    The double-sided spectrum of the same signal as above ( = 1.5,  = 2 and 0 = 2 Hz) is
illustrated in the right column of Figure 2.10. Note that the amplitude spectrum has an even
symmetry, and the phase spectrum has an odd symmetry. This is simply the consequence of
        22                                                                    2. Signal preliminaries

                single-sided spectrum      double-sided spectrum

            2 amplitude                                   2 amplitude

                 1.5                                               1

            1                                                           0.75

            0          f [Hz]                                      0             f [Hz]
                         4
               0    2                  -4  -2                           0     2  4

             phase                                                  phase

                                                                   
              2                                                    2

             0         f [Hz]                                      0             f [Hz]
                0
                    2  4               -4  -2                           0     2  4
            - 2
                                                                   - 2

            -                                                      -

        Figure 2.10: Single-sided (left) and double-sided (right) amplitude (top) and phase (bottom) spectra of () =
        1.5 cos(4 +  ). The convention is to show the phase spectrum below the amplitude spectrum.

                                      2

        using two conjugate rotating phasors to obtain a real-valued signal. Their magnitudes must
        be the same and their phases must be equal, but opposite, in sign. This also follows directly
        from (2.10). See also Appendix C, (C.2) and (C.13).

            Clearly, these relations and findings are elementary. However, as we will see later in this
        book, they have massive consequences for signal analysis. The convention here is the use
        of the double-sided spectrum, although with the above-mentioned symmetries of amplitude
        and phase, we may only show one half, namely, the part for positive frequencies. A common
        mistake is then to name this half of the double-sided spectrum the single-sided spectrum,
        as only one side is shown. But as we can clearly see in Figure 2.10 the amplitudes of the
        double-sided spectrum are half of the amplitudes of the single-sided spectrum (the phase is
        identical). As a final note, for most periodic signals the amplitude and phase spectra will only
        have non-zero values at some frequencies, and these spectra are referred to as line spectra.

EX 2.4      Consider the following signal: () = 4 sin(30) + 3 cos(70 +  ). Construct the

                                                                                                                                        4

            single-sided and double-sided spectra of this signal.

            Solution This signal has two components with frequencies 1 = 15 and 2 = 35 Hz.

            When drawing the single-sided spectrum, we first need to write the sine component

            as a cosine, because the sine component of the rotating complex exponential is
            imaginary. Using sin  = cos( -  ), the first component becomes 4 cos(30 -  ).
                               2                                                         2

            The single-sided spectrum then quickly follows, from:
                   () = Re (4(30- 2 ) + 3(70+ 4 ))

            Similarly, for the double-sided spectrum we obtain:
            () = 2(30- 2 ) + 2-(30- 2 ) + 3 (70+ 4 ) + 3 -(70+ 4 )
                                       2                                2
2.4 Signal operations                                                                                                                    23

5 amplitude                                            5 amplitude

4                                                      4

3                                                      3

2                                                      2

1                      f [Hz]                          1                                                    f [Hz]

0                                                      0

   0 10 20 30 40                           -40 -30 -20 -10 0 10 20 30 40

 phase                                                  phase

                            f [Hz]                                                                                               f [Hz]
  2    10 20 30 40                                                               2                          10 20 30 40

 0                                                                    0
    0                                      -40 -30 -20 -10 0

- 2                                                                 - 2

-                                                      -

Figure 2.11: Single-sided (left) and double-sided (right) spectra of Example 2.4.

In drawing the signal spectra it is clear that, by convention, the cosine function is taken

as the reference: sine functions are `converted to' cosines, a zero-phase sine is a cosine with
phase of -  . More on these phase shifts or, equivalently, time shifts, in the next section.

                    2

The amplitudes are taken to be positive, negative amplitudes are converted to positive

amplitudes through adding (or subtracting)  to the phase. Note that the phase must always
lie in [-, +]. For example, () = -5 cos(2 +  ) is converted to () = +5 cos(2 - 3 ),
                                           4                                                                                             4
rather than () = 5 cos(2 + 5 ). Finally, note that the commensurability of the signal
                       4
components is not required to draw a signal spectrum.

2.4 Signal operations

While we focus on operating on the argument of functions of time, the same rules hold for
operated functions of frequency. Clever operations on building blocks allows us to construct
an infinite variety of new functions of time (or frequency). One must first define the `rules of
precedence' and apply these rules consistently. This can be compared to agreeing on, and
following the `order of operations' in mathematics. E.g., 4 + 2 × 3 equals 10 rather than 18.
This is because we defined the operation of multiplication to take precedence over addition.

2.4.1 Rules

Imagine that we have an arbitrary function of time (). Suppose we would perform some
operation on the time-argument, like  +  parameter  is dimensionless, parameter  has
unit [s]. Then what does signal () = ( + ), with argument  + , rather than just ,
look like? To answer this question, first rewrite ():2

                                                                                                                                         (2.11)
() = ( + ) =  ( ( +  ))

and then apply the following three rules.

2Note that we aim to obtain 1 again, i.e., ((1 +  )), similar as in (2.3).

                                                                                                          
        24                                                                            2. Signal preliminaries

            (1)  < 0 :      function is flipped around  = 0

            (2) ||  1 :     function is scaled in time:

                     || > 1 signal is compressed                                       (signal goes faster)
                                                                                      (signal goes slower)
                     || < 1 signal is expanded

            (3)   0  :      function is shifted in time, with 0 =  :

                     0 > 0  signal is shifted to the left                             (signal comes earlier, time advance)
                     0 < 0
                            signal is shifted to the right                            (signal comes later, time delay)

        The rules define the order of precedence, first (1), then (2), and then (3). Consistency in
        applying the rules (in time and frequency) is extremely important.

            Finally we note that a signal can also be transformed in terms its amplitude: () =
        ( + ), where the amplitude is scaled by a factor of .

EX 2.5  2.4.2 Examples

            What does signal () = (-3 - 6) look like?

            Solution This is an operation of the unit ramp function (). First re-write ():

            () = (-3 - 6) = ( - 3(  + 2))                                             (2.12)

                                                       = = 

                                                                                 

            First step: we see that  < 0, so the function () is mirrored about  = 0. Second

            step: || = 3, larger than 1, the signal compresses and runs three times as fast, i.e.,
            the () function rises more rapidly over time. Third step:  = +2, so 0 > 0, meaning
            that the signal shifts left by 2 seconds. The resulting signal is shown in Figure 2.12

            (left), with the blue circle added to check that () rises by +3 units per second.

EX 2.6      What does signal () = (-2 + 3) look like?

            Solution This is an operation of the unit pulse function (). First re-write ():

                                                        3                             (2.13)
            () = (-2 + 3) = ( - 2( - ))

                                            = 2

                                                                    =

                                                                                    

            First step: we see that  < 0, so the function () is mirrored about  = 0, which has

            no effect in this case, as () is even. Second step: || = 2, larger than 1, so the signal

            compresses, it runs twice as fast and the () function width shrinks to half a second.
            Third step:  = - 32 , so 0 < 0, meaning that the signal shifts right by 32 seconds. The
            resulting signal is shown in Figure 2.12 (right).

            To check one's answer regarding where on the time axis the operated signal appears, we
        set everything within parentheses to zero and check where the feature of the original signal
        at  = 0 shows up within the operated signal. For instance, in Example 2.5: (-3 - 6) = 0 
         = -2 seconds, where the `bend' of the operated () appears.

            In Example 2.6 the signal was compressed by a factor of 2, and the pulse width changed
2.4 Signal operations                                                                                                                     25

6                                                                        1.0
                                                                                                                             0.5
4
                                                                         0.5
2

0
 -5 -4 -3 -2 -1 0 1 2
                            t [s]
x(t)
                                                                   y(t)

                                                                         0.0
                                                                                                                    1.5

                                                                         -3 -2 -1 0              1                          2       3

                                                                                          t [s]

Figure 2.12: Illustrating () = (-3 - 6) and () = (-2 + 3) from Examples 2.5 and 2.6.

from 1 to half a second. An alternative way to define the pulse function is to write it as  (  ),
                                                                                                                                          
as then dimensionless scale factor  = 1 indicates that the pulse has a width of  seconds.
                                           
Then () =  (  ) is a unit pulse with a width of one second. In Example 2.6, () can be
  1
                       - 3
re-written as () =  (    2   ).  One  can  see  at  a  glance            that  ()  is  a  pulse  with                    a  width   of  half
                       1

                       2
a second, centered at  = 3 s. Similarly, the triangular function can be written as  (  ), again
                          2                                                                                                       
with  = 1 , where 2 is the width of the triangle, as it runs from  = - to + s, see the unit

triangular function in Figure B.1. Pulse and triangle widths are defined to be positive,  > 0.

Note that when these widths are defined using parameters that have a unit, the scaling

of the functions is performed using the parameter value only. This is because the widths

correspond with the scale factor, parameter  in (2.11), which is dimensionless.

2.4.3 Constructing signals using building blocks

With the rules established, we can build new signals using building blocks and even define
building blocks using others. E.g., the unit pulse with a width of  seconds can be expressed:

                                 
 (  ) = ( + 2 ) - ( - 2 )                                                                                                           (2.14)

using the unit-step function (). An important use of writing a signal using building blocks
will appear later, e.g., when we compute the Fourier transform of a signal (). Imagine that
we could construct () as the summation of two other signals () and (), i.e., () =
() + (). It could well be that, whereas the Fourier transform of () is tedious, the Fourier
transforms of () and () are relatively easy. In that case, it is easier to first write the signal
as the sum of components, Fourier transform the components, and (using the linearity property
of the Fourier transform discussed in Chapter 6), add the Fourier-transformed components.

What does signal () = 2(-) cos(2) look like?                                                                                                  EX 2.7

Solution This is an operation of the unit step function () multiplied by a 1 Hz cosine
function, multiplied by 2. First re-write (-):

(-) = (-1( + 0))                                                                                                                  (2.15)

First step:  < 0, so the function () is mirrored about  = 0 s. Second step: || = 1,
which has no effect. Third step:  = 0 = 0, which again has no effect. We multiply
the `flipped' unit step with the cosine, with amplitude equal to 2. The resulting signal

is shown in Figure 2.13 (left).
        26                                                                                              2. Signal preliminaries

EX 2.8      What does signal () = -2() + ( - 4) + ( + 4) look like?

            Solution This is a summation of three (operations of) unit ramp functions (). Apply
            the check stated above to determine at what times the three components have their
            `bend'. First component: at  = 0 s second component: at  = 4 s third component:
            at  = -4 s. Rewrite () to put them in their order of appearance:

            () = ( + 4) - 2() + ( - 4)                                                                                    (2.16)

            The first component is a ramp function that is zero before  = -4 s and then rises by 1
            unit every second. The second ramp function is zero before  = 0 s and then descends
            by 2 units every second. The third ramp function is zero before  = 4 s and then rises
            by 1 unit every second. The signal is shown in Figure 2.13 (right).

              2                                                                         6     r(t + 4)                 r(t - 4)
              1 u(-t)                                                                          -4 -2
              0                                      2 cos(2t)                          4                      -2r(t)
            -1
            -2                                       1  2       3                       2               0      2       4         6

                -3 -2 -1 0                                                              0
                                          t [s]                                       -2
                                                                                      -4
              2                                                                       -6
              1                                                                       -8
              0
            -1                                                                            -6
            -2
                                                                                                        t [s]
                -3 -2 -1 0
                                          t [s]                                         6
            x(t)                                                                        4
                                                                                y(t)    2
                                                                                        0
                                                     1  2       3                     -2                0      2       4         6
                                                                                      -4

                                                                                          -6 -4 -2

                                                                                                        t [s]

        Figure 2.13: Constructing () = 2(-) cos(2) (left) and () = -2() + ( - 4) + ( + 4) (right) using
        building blocks (top row), from Examples 2.7 and 2.8.

        2.5 Energy and power

        2.5.1 Definitions

        An important classification of signals used throughout this book is that of energy signals and
        power signals. For an arbitrary real-valued signal (), its (total) energy is defined as:

                                         +                                                                                       (2.17)

                                                  2

             = lim  2() d

                             - 

                                                 2

        Suppose the unit of the signal is [unit], then the energy of that signal has unit [unit2s].The
        average power of that signal is defined as:

                    1 + 2
             = lim   2() d                                                                                                       (2.18)
              
                    -2
2.5 Energy and power                                                                                                27

The unit of the power of a signal is [unit2].3
    Three signal classes can be identified, as listed in Table 2.2. A signal can be an energy

signal, a power signal, or neither an energy nor power signal, with an example of each shown
in Figure 2.14. In this figure (and unless noted otherwise, in all figures in this book), note
that only part of the time axis is shown. These signals range from  = - to +.

Table 2.2: Three signal classes.

                    energy signal             0<<                   =0
                                                                 0<<
                    power signal                 =

                    signal is neither energy nor power

2                                 2 t0                                                          4  t0
                      t0                                A = 1.5

1 A=1                             1
                                
x(t)                                                                                            2
                                             y(t)
                                                                                          z(t)

0                                 0                                                             0

   -2     0           2                       0         5                                          -4 -2         0  2

          t [s]                                  t [s]                                                    t [s]

Figure 2.14: Illustrating the three signal classes: energy (left), power (middle), neither energy nor power (right).
Here () =  ( -0  ), () = 32 ( - 0) and () = ( + 0), with 0 > 0 an arbitrary time shift.

Clearly, signal () in Figure 2.14 is an energy signal its energy equals 2 = , (2.17),

its average power is zero. Signal () is a power signal it has infinite energy and its average
power equals 2 = 9 , (2.18). Signal () is neither energy nor power, as both its energy,
       2         8
(2.17), and its power, (2.18), are infinite.

Four insights follow from these examples. First, energy and power scale with 2, so only

the amplitude matters. Second, shifting signals left or right has no effect on their energy or

power: the time-shift 0 in Figure 2.14 is irrelevant. Third, for signals to be either energy or
power signals they must be bounded in amplitude. Fourth, signals with a non-zero average,

over  = - to , cannot be energy signals.

Periodic signals are an important class of signals, Section 2.1. From the above definition, it

is clear that periodic signals are power signals: they repeat forever and their energy is infinite.

Then, since a periodic signal repeats every period of 0 seconds, the energy contained in each
period is the same, and the average power can be computed using one period:

                 0+0                                                                                                (2.19)

    = 1  2() d
         0

                  0

3In electrical engineering, where the concepts of energy and power originate, signals are often in [Volt] (or
 [Ampere]). For power signals, when normalized to unit resistance, and with Ohm's law, the power (2.18) is then
 expressed in [Watt]. For energy signals, when normalized to unit resistance, the energy (2.17) is expressed in
 [Joule]=[Watt s]. Section 13.1 provides a more extensive explanation.
         28                                           2. Signal preliminaries

         Decibel notation

         Energy and power values are often expressed in decibel, dB4. It is not a unit but a represen-

         tation of a numerical value as a (base-10) logarithmic value. To represent a signal's power 
         in dB, use  = 10 log10 . The other way around:  = 10/10. The same holds for signal
         energy. Expressing values in decibel is extremely useful when these values are either very
         small, or very large. E.g., when a voltage signal has power  = 0.0000000001 W (= 10-10
         W), we write:  = -100 dBW. The unit remains Watt, the number is expressed in decibel.

EX 2.9   2.5.2 Examples

             Consider signal () = -(), with  > 0, where  and  are constants. Determine
             to what class of signals () belongs, according to Table 2.2.

             Solution Note that it is often useful to first sketch the signal, if possible, see Fig-
             ure 2.15 (left). First compute the energy of the signal, using (2.17):

                              

              = lim  (-())2 d = 2 lim  -2 d = 2 2  lim (- - 1) = 2 2
                                    -2                                                                     2

             - 0
                     2

             Because energy is limited, () is an energy signal. When   0 we obtain signal
             () = (), with infinite energy, see middle graph of Figure 2.15. Its average power
             can be computed using (2.18):

                              

              1  = lim  (())2 d = 2 2 lim 1  12 d = 2 2 lim 1 (  - 0) = 2
                                      2               2
                     - 0
                        2

EX 2.10      Consider signal () =  cos(20 + ), where , 0 and  are constants. Determine
             to what class of signals () belongs, according to Table 2.2.

             Solution Signal () is periodic (see Figure 2.15 at right) hence it cannot be an energy
             signal as its energy is infinite. The average power can be computed using either (2.18)
             or (2.19). Using the latter, with 0 = 10 , and using cos2  = 12 (1 + cos(2)), (A.4):

                     0           0
              = 1 ( cos(20 + ))2 d = 1 2  1 (1 + cos(2(20 + ))) d
             0 0 0 0 2

             2 0 1            0  2 2
             = (+          sin(40 + 2)|0 ) = + (sin(4 + 2) - sin(2))
             0 2 80                 2 8

                  2        since sin(4 + 2) = sin(2)
             =2

             The average power of a sinusoidal signal only depends on its amplitude. Its phase 
             and frequency 0 do not matter in energy or power calculations.

         4Named after A.G. Bell, a Canadian-American engineer who patented the first practical telephone.
2.6 Practical implications                                                                                     29

       2 =1                                 2 =0                                      2
       1                                    1
x(t)                                 y(t)                                 z2(t) z(t)  0

       0                                    0                                       -2
                                                                                             -2
          -2  0         2                      -2             0      2                           0      2
                                                                                      4
              t [s]                                           t [s]                              t [s]
                                                                                      2
       4 =1                                 4 =0
                                                       level
x2(t)                                y2(t)                                                              level
                                            2
       2                area

       0                                    0                                         0

          -2  0         2                      -2             0      2                   -2      0      2

              t [s]                                           t [s]                              t [s]

Figure 2.15: Sketching the signals (top row) and their energy or power (bottom) of Example 2.9 and Example 2.10.
Here () = -() (with  = 2,  = 1), () = -() (with  = 2,  = 0) and () =  cos(20 + )
(with  = 2,0 = 12 and  = 0).

    Figure 2.15 illustrates signals (), () and () from Examples 2.9 and 2.10 (for  = 2,
 = 1 and  = 0, 0 = 0.5 Hz and  = 0). Again, in energy and power calculations it is
wise to sketch the signal first before doing any calculations. From the sketch it often quickly

becomes obvious to what class of signals the signal belongs, and what calculations (if any!)

are necessary.

    For instance, consider energy signal (), shown in Figure 2.14 at left. Suppose we define
a new signal, (), which equals () plus a non-zero (and possibly negative) constant . In
that case, () is a power signal with average power 2. Whereas () has a zero average,
() has a non-zero average, . Because of this, its energy becomes infinite its average
power is 2. The pulse becomes irrelevant, as can be seen in the next example.

Compute the average power of signal () =  + (  ), with  and  non-zero con-                                         EX 2.11

                                                                                                    

stants, and  > 0.

Solution         + +
     =        12                                      2
                                               1              2 2                        
          =               2
              lim   () d = lim  ( +  ( ) + 2( )) d
                                                                                         
                 - -
                     2                             2

                              +
                                  2
              lim 1 2 +  (2 + 2) d = 2 + lim 1 (2 + 2) = 2 + 0 = 2
                                                                         
                              
                              -2                         

2.6 Practical implications

The concepts defined in this chapter are fundamental to the analysis of signals in the time and
frequency domains. Before moving on to the foundations of signal analysis in the frequency
domain, the main topic of this book, it is important to briefly look ahead on how signal analysis
30                                                                                                     2. Signal preliminaries

is performed in `real life' and the consequences for the concepts considered.
    The most important difference between the mathematical view on signals and what signal

analysis entails in real life, is that measurements do not last forever. The observation of a
signal starts at some time 1 and ends some later time 2. Typically we set 1 to be equal to
0, and then 2 - 1 equals the measurement duration or time . What does this mean
for some of the signal characteristics we discussed up to now? For example, do even and
odd signals exist in real life? Since these characterize the potential symmetry properties of a
signal around  = 0, see Section 2.2, and we just agreed that measurements in practice start
at  = 0, it is clear that we do not have evenness or oddness of measured signals. These are
mathematical abstractions, which offer insight and are helpful in signal analysis, but in most
cases not relevant when processing measurements.

    Regarding periodicity, periodic signals are defined from  = - to +, (2.1), but mea-
surements do not last that long. The periodic-ness of signals disappears completely. The
measurement () that we have of a (potentially) periodic signal (), or any signal for that
matter, is a window on the signal, lasting  seconds. This is illustrated in Figure 2.16
for measuring a deterministic (top) and random (bottom) signal. The rectangular window is
shown in red in Figure 2.16 (top) it equals value 1 during the window, and 0 outside.

          3 -              xw (t)                                            window                       +
          00                                                                                                   0
    x(t)                                                                                            1

                                                            Tmeas
          -3

              -3 -2 -1  0          1                               2         3  4  5                   6  7       8

                                                                      t [s]

          3

    y(t)  0

                           yw (t)

          -3

              -3 -2 -1  0          1                               2         3  4  5                   6  7       8

                                                                      t [s]

Figure 2.16: The signals of Figure 2.1 considered as measurements of infinite-duration signals (for  = 5 s).

    Figure 2.16 illustrates that while signals () and () shown in Figure 2.1 (in theory) run
from  = - to +, after measuring these signals for  = 5 seconds, we work with ()
and (), the resulting signals after multiplication by the window. This has important conse-
quences: measurements are by definition energy signals, since their energy is limited, their
average power equals zero. Moving our mathematical concepts and techniques to practical
applications means that we have to account for the effects caused by the (inevitable) time
window. We return to the subject of finite signal duration in Chapter 8, and to energy and
power in this respect in Chapter 13.

    This chapter concludes Part I, which introduced the topic of signal analysis and some
signal models and definitions, all in the time domain. In Part II, we discuss how periodic and
aperiodic signals can be mathematically described in the frequency domain using the Fourier
series and Fourier transform, respectively.
    II

   Continuous time

31
            3

                          Real Fourier series

In this chapter we perform a signal de-composition, expressing a periodic real-valued signal
as a linear combination of orthogonal basis functions. The Fourier series expansion relies
on sinusoids as its basis function, and expresses a periodic signal as a sum of harmonic,
zero-phase cosines and sines. We first consider the real, or trigonometric, Fourier series in
Chapter 4 the same decomposition but then in terms of complex exponentials will be discussed,
the complex Fourier series. The real Fourier series allows us to easily obtain the single-
sided amplitude and phase spectra of periodic signals. Chapter 2 showed that obtaining
the spectrum of a sinusoid is straightforward. De-composing a periodic signal into a sum of
sinusoid components means that the spectrum of that signal is simply the sum of the spectra
of these components.

3.1 Rationale and definition

The Fourier series and Fourier transform have been named in honour of French mathematician
and physicist Jean-Baptiste Joseph Fourier (1768-1830). Several influential mathematicians
played a role in the development, and notably also Carl Friedrich Gauss (1777-1855) [4].

    Consider signal () which is periodic with period 0 and fundamental (linear) frequency
0 = 10 in Hz. The fundamental angular frequency is then 0 = 20 = 2 0 , in rad/s. The
rationale of the real Fourier series is to decompose this signal () into a series of  pure
sine wave components, each with its own amplitude and with a frequency that is an integer
multiple of the fundamental frequency 0. Adding these  components together `builds' the
signal (), as if we obtain a model () of signal () using sinusoidal building blocks:

                       -1

       () = ( cos(0) +  sin(0))

                       =0

Because cos 0 = 1 and sin 0 = 0, the 0 component is discarded and 0 is taken out of the
summation, yielding:

                                -1

       () = 0 + ( cos(0) +  sin(0))

                                =1

Then, when () adheres to certain conditions (Section 3.3), the model () will approximate
signal () for   . In what follows, the real Fourier series will be defined for this ultimate

                                                            33
34                                      3. Real Fourier series

case, yet in most examples and all practical applications of the real Fourier series we work
with a finite number of components , corresponding with 2 - 1 coefficients.

    The real, or trigonometric Fourier series of signal () (for   ) is written as:

       =                   =

    () = 0 +   cos(0) +   sin(0)                            (3.1)

       =1                  =1

Expanding the summations, the Fourier series becomes:

       () = 0 + 1 cos(0) + 2 cos(20) + ... + 1 sin(0) + 2 sin(20) + ... (3.2)

Periodic signal (), for - <  < , is built, or constructed from a sum of harmonically related
sines and cosines, with the coefficients given by 0 and {, } for   +. Harmonically
related means that the frequencies of the sinusoidal components are all integer multiples of
the fundamental (angular) frequency 0 the frequencies are commensurable and the resulting
signal is periodic with 0. The th cosine and sine components, that is, the th harmonic of
the fundamental frequency, fit an integer  times in the period 0. Regarding the unit of the
coefficients, from (3.1) it follows that they have the same unit [unit] as the signal.

3.2 Derivation Fourier series coefficients

In this section we derive expressions, in terms of periodic signal (), for the coefficients 0,
 and , for  = 1, 2, 3, ... in the real Fourier series.

    First, coefficient 0 is found by integrating all terms of (3.2) over one period:

    0 () d =  0 d +  1 cos(0) d +  2 cos(20) d + ...000

                        +  1 sin(0) d +  2 sin(20) d + ...00

Now, all terms on the right-hand side, except the first one, are zero, as for each integral term

we integrate a sine or cosine over one period 0. For any cos(0) and sin(0), with
integer , as much `area' appears above the time axis as below it, in one period 0, and the
integrals are zero. We arrive at  () d = 00, and therefore:

                                                                          0

    0 = 10  () d                                            (3.3)

                  0

Hence, 0 is simply the average value of the signal. Another interpretation of 0 is that it
represents a cosine with zero frequency, i.e., an ultimately slowly varying cosine signal. Note

that the rest of the terms in (3.1) are oscillations about this average.
    A general expression for the coefficients , for   +, is found by multiplying both sides

of (3.1) by cos(0), for   +, and then integrating over one period 0:

    0 () cos(0) d = 0  cos(0) d0

                              =

                        +  (   cos(0)) cos(0) d

                                0 =1

                              =

                        +  (   sin(0)) cos(0) d

                                0 =1

The first term on the right-hand side is zero. With the two other terms, we move the term
cos(0) inside the summation and integrate term-by-term. Essentially, we interchange the
3.3 Conditions                                            35

order of summation and integration as follows:

                =                                  =

 () cos(0) d =    cos(0) cos(0) d+    sin(0) cos(0) d
0                                0                     0
                =1                                 =1

Applying the orthogonality properties of integrals involving products of sines and cosines,

(A.11)-(A.13), results in each term in the second series on the right-hand side to be zero, for

all values of  and  (using integral 3, (A.13)). Each term in the first series on the right-hand
side is also zero, except for the one term where  equals , where the integral equals 0/2
(integral 2, (A.12)). We obtain, with  = :

    () cos(0) d =  () cos(0) d =  ( 0 )
   0                             0              2

which leads to our main result:

    = 20  () cos(0) d               with   +              (3.4)

                 0

    Similarly, the values for coefficients  are found, by multiplying the series (3.1) with
sin(0), integrating it over one period 0 and using the integral properties 1 and 3:

    = 20  () sin(0) d               with   +              (3.5)

                 0

    One way to interpret integrals (3.4) and (3.5), is that for each component (representing

the th harmonic of the fundamental frequency 0) we consider the commonality of signal
() with that component (harmonic). For instance, if signal () is dominated by the th

cosine component in the Fourier series, integral (3.4) and therefore the corresponding 
coefficient will be larger than other coefficients. Apparently, this particular building block is

essential to construct (). Conversely, when signal () has nothing in common with the

th sine component in the Fourier series, the corresponding  coefficient will be zero: this
building block is not needed to construct signal ().

3.3 Conditions

For expanding a signal () in a Fourier series, this signal must be absolutely integrable over
the finite interval (- 0 , 0 ), i.e., over one period:

                                      22

              0
               2

         |()| d < 

          - 0

                 2

In this case the coefficients  and  have finite values, see [5].
    If signal () has only a finite number of maxima and minima and a finite number of (finite

jump) discontinuities in the interval (- 0 , 0 ) (that is, signal () is of bounded variation),

                                                                         22

then the Fourier series, for   , converges to () for all values of , and to the `average'
of the two values forming a discontinuity, see [5]. In this case, i.e., for indices   , the
coefficients  and  will go to zero.

    These conditions are sufficient, implying that they are more restrictive than necessary.
Most practical periodic signals do not contain sudden jumps and the Fourier series gives an
adequate approximation of the signal when using a sufficient finite number of components .
        36                                                                3. Real Fourier series

EX 3.1  3.4 Example: Fourier series of a square wave

            Compute the real Fourier series of the periodic square wave signal (), given as:

                          1  for 0   < 0                                                (3.6)
            () = {           for 0 2   < 0

                        -1           2

            and periodically extended outside this interval, as shown in red in Figure 3.1 at left.

            Solution Expressing () as a Fourier series requires the calculation of coefficients
            0,  and . The first coefficient 0 is found with (3.3):

            1                   1        0              0
                                         2
            0 =  () d = ( 1 d +  (-1) d)
            0 0 0 0 02

            The integral is taken over period 0 we have full freedom to set the lower and upper
            integral bounds, as long as exactly one period is taken. Here we choose to integrate

            over the interval [0, 0]:

            1 02                0        1 0                     0
            0 = ([]0 + [-] 0 ) = ( - 0 - 0 + ) = 0                                      (3.7)
            0                            0 2                     2
                                2

            This is no surprise as the square wave () has a zero mean, see Figure 3.1.
                The coefficients  are found with (3.4):

               2                                  2        0                    0
                                                              2
             =  () cos(0) d = ( cos(0) d -  cos(0) d)
               0 0                                0 0                        0

                                                                             2

                                            0                       0
               21                           2        1

            = 0 ([ 0 0 sin(0)] - [ 0 sin(0)] 0 )
                                                                    2

            and with 0 = 2 0 we obtain:

             = 2 0 (sin( 2 0 ) - sin( 2 0) - sin( 2 0) + sin( 2 0 ))
               0 2              0 2                  0              0              0 2

            = 1 (sin() - sin 0 - sin(2) + sin()) = 0                                    (3.8)
                 

            All cosine coefficients  = 0, for   +. This is also not a surprise, since the square
            wave is odd-symmetric about  = 0, see Table 2.1, and hence an even function such

            as a cosine is of no use at all to construct this square wave.

                The coefficients  are found using (3.5):

               2                                  2        0                 0
                                                           2
             =  () sin(0) d = ( sin(0) d -  sin(0) d)
               0 0                                0 0                     0

                                                                          2

               2             1                 0                       0
                                               2        1

            = 0 ([- 0 0 cos(0)] + [ 0 cos(0)] 0 )
                                                                       2
3.4 Example: Fourier series of a square wave                                                            37

and again with 0 = 2 0 we obtain:

       = 2 0 (- cos( 2 0 ) + cos( 2 0) + cos( 2 0) - cos( 2 0 ))
          0 2                         0 2              0    0          0 2

          = 1 (- cos() + cos 0 + cos(2) - cos()) = 2 (1 - cos())
                                                                     

Note that cos() = 1 for integer  even, and cos() = -1 for integer  odd. Hence:

                     4  for  odd                                            (3.9)
                        for  even
       = { 
               0

with   +. All even-indexed coefficients are zero, a consequence of this signal being
half-wave odd symmetric. This will be discussed in more detail in Section 3.5.

With the above coefficients we can write the Fourier series for the square wave as:

          41                       1               1
      () =  ( 1 sin(0) + 3 sin(30) + 5 sin(50) + ...)                       (3.10)

The left-hand side of Figure 3.1 shows the square wave (in red), period 0 = 1 s, and
the real Fourier series approximation (in blue) for  = 5 (top) and  = 11 (bottom).

     Figure 3.1 at right shows the Fourier series coefficients 0, {, }, as a function
of  on the horizontal axis, representing angular frequency 0. In other words, the
horizontal axis represents frequency and the coefficients 0, {, } are shown as a
function of frequency. When adding more components, the signal approximation be-

comes better (compare the graph at bottom ( = 11) with the one on top ( = 5)).

Adding components (i.e., higher-frequency building blocks) has no effect on the coeffi-

cients of components already computed. Typically, the coefficients become smaller for

larger  (for this signal, see (3.9)), and reduce to 0 when   .

      1                               square wave  ak  0.0

                                      FS, K = 5        1.5 0 1 2 3 4 5 6 7 8 9 10

x(t)  0                                                1.0  no b0 !

                                                   bk  0.5

      -1                                               0.0
                                                            0 1 2 3 4 5 6 7 8 9 10
          -0.5 0.0 0.5 1.0 1.5                                                          k
                                 t [s]

      1                               square wave  ak  0.0

                                      FS, K = 11       1.5 0 1 2 3 4 5 6 7 8 9 10
                                                       1.0
x(t)  0                                                0.5
                                                       0.0
                                                   bk
                                                            0 1 2 3 4 5 6 7 8 9 10
      -1                                                                                k

              -0.5 0.0 0.5 1.0 1.5
                                     t [s]

Figure 3.1: Left: square wave signal () and its Fourier series approximation. Right: the 2 - 1 Fourier
series coefficients 0, {, } as a function of , for  = 5 (top row) and  = 11 (bottom row).
        38                                                                              3. Real Fourier series

        3.5 Effects of symmetry

        When considering the Fourier series coefficients of the square wave signal, Example 3.1, some
        patterns emerged. All cosine coefficients were shown to be zero, an effect caused by the
        oddness of the square wave. All even-indexed coefficients were zero as well. These patterns
        can be explained using the three symmetry properties introduced in Section 2.2.

            As shown in (2.5), all signals can be described as the summation of an even and odd signal
        part: () = () + (). The same holds for the real Fourier series, (3.1):

                          =                           =

                () = 0 +   cos(0) +   sin(0)

                     =1  =1

                          even part ()                odd part ()

        When considering the Fourier series of arbitrary periodic signals from this perspective, it is

        clear that for even signals all sine components are zero, whereas for odd signals all cosine
        components are zero. When signals are neither even nor odd, both sine and cosine compo-
        nents are required to construct the signal. Note that the signal average, 0, belongs to the
        even part as the odd part must be zero for  = 0, see Table 2.1.

           Finally, it can be shown (see Exercise .3-7) that for signals which have half-wave odd

        symmetry (Section 2.2) all the even-indexed coefficients ( = 2, 4, 6, ...) are zero.

EX 3.2      What would happen to the Fourier series coefficients of Example 3.1 if we were to shift
            the square wave () a quarter of a period ( 0 ) to the left?

                                                                                              4

            Solution From visual inspection of Figure 3.1, we learn that the shifted signal is even,
            and hence all sine coefficients will be zero. In this way the signal can be constructed
            with cosines. The signal remains half-wave odd, as time shifts have no effect on this
            symmetry property, and all even-indexed coefficients remain zero.

EX 3.3      What would happen to the Fourier series coefficients of Example 3.1 if we were to add

            a non-zero constant  to the square wave ()? That is, what are the Fourier series
            coefficients 0, { ,  } of periodic signal () = () + ?

            Solution Strictly mathematically, () is neither odd nor half-wave odd. When com-

            paring the real Fourier series coefficients of () and (), the only difference will occur

            in  the  average:       =  0 +     =  0+  =    .  All  other  coefficients  will  remain  the  same:

                                 0                +.

                =    and       =     ,  for           The  Fourier series  coefficients will  show    the  same

            patterns as those obtained for the original signal (): all cosine coefficients are zero, all

            even-indexed coefficients are zero. As discussed in Section 2.2, non-zero-mean signals

            can have characteristics of oddness and half-wave oddness `about their average'.

        3.6 Single-sided spectrum

        The real Fourier series gives an array of real numbers (0, and {, } for   +), which
        can be converted to single-sided amplitude and phase spectra.

        3.6.1 Derivation

        Consider the th harmonic of the Fourier series (  0), and suppose we want to express it
        as a single cosine function with amplitude  and phase :

                cos(0) +  sin(0) =  cos(0 + )
3.6 Single-sided spectrum                                                                                39

Applying cos( + ) = cos  cos  - sin  sin , (A.7), to the right-hand side yields:      (3.11)
        cos(0) +  sin(0) =  cos(0) cos  -  sin(0) sin 

Thus:
        =  cos  and  = - sin 

which allows us to obtain the amplitude  of the th harmonic:

    =  2  +  2   with   +                                                             (3.12)

               

and phase  of the th harmonic:1

 = arctan (-  )       with   +                                                        (3.13)
                    

For the signal average, the `zero-frequency' Fourier series component ( = 0): 0 = |0|.
When 0 > 0, the average lies on the positive real axis of the complex plane, 0 = 0. When
0 < 0, the average lies on the negative real axis, 0 = ±. When 0 = 0, 0 = 0.

Draw the single-sided spectrum of the square wave signal of Example 3.1.                                     EX 3.4

Solution Plotting the  amplitudes  and phases  as a function of , for all val-

ues of  = 0, 1, 2, ... ,  - 1, we obtain the single-sided amplitude and phase spectra,

respectively. Figure 3.2 at left illustrates 0 and coefficients  and  (for  = 17) at

right are shown the single-sided amplitude (, top) and phase (, bottom) spectra

of the square wave of Example 3.1. As only discrete frequencies occur in the spectra,

all integer multiples of 0, these spectra are known as line spectra.

    All even-indexed coefficients are zero, and so are their amplitudes and phases. The
odd-indexed  coefficients are non-zero (for finite ), their phases are all equal to - 2
as they represent `pure' sines. Remember from Chapter 2: the cosine is our reference,
a zero-phase sine is a `cosine with phase equal to -  ', sin  = cos( -  ).
                                                        2                          2

ak  1.0                                             Ak  1.5
                                                        1.0
bk  0.5                                             k   0.5
                                                        0.0
    0.0
                                                               0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
          0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
                                                                                         k
                                    k
                                                          0
    1.5                                                 - 4
    1.0                                                 - 2
    0.5
    0.0                                                        0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16

          0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16                                       k

                                    k

Figure 3.2: Signal () first  = 17 Fourier series coefficients (left column) single-sided amplitude and
phase spectra (right column).

1Numerically, e.g., in Python, you need to use the four-quadrant arctan function.
        40                                                                                           3. Real Fourier series

        3.6.2 Alternative definition of real Fourier series

        With  and  defined, an alternative expression for the real Fourier series, (3.1) reads:

                                   =                                                                                          (3.14)

             () = 0 +   cos(0 + )

                                   =1

        This expression shows the Fourier series directly as single-sided amplitude and phase spectra.

        It allows for easy analysis of what happens to a signal spectrum when shifting that signal in

        time. Define new signal () = ( + 0) with 0 > 0 some time shift. Substituting  + 0 for 
                                                                         
        in  (3.14)  yields  the  amplitudes              and    phases       of  the  single-sided   spectrum        of  ():
                                                                           

                    =       (unchanged)         and             =     +  00                                                   (3.15)

                                                             

        Clearly, the amplitudes remain unaffected by the time shift. Recall from Section 2.1 that, for

        a sinusoid with frequency 0, a time shift 0 corresponds with a phase shift  = 00, (2.3).
        Hence, the phase of the first harmonic, the fundamental frequency, increases with 00, and
        the phase of the th harmonic increases with  times this phase shift, 00. Using (3.11),
        the real Fourier series coefficients { ,  } of new signal () can be computed as:

                    =      cos(00)         +    sin(00)            and           =  -  sin(00)       +      cos(00)

                                                                               

        The  average     remains     unaffected  by        the  time  shift       =   0,       =  0  and       =     0.

                                                                               0            0               0

EX 3.5      Given (3.14), reconsider Example 3.2, in other words, what happens to the Fourier
            series coefficients and single-sided spectrum when shifting signal () by 0 to the left?

                                                                                                                                                  4

            Solution Since () = ( + 04 ), 0 = 04  with 0 = 2 0 , 00 =  2 0 0 4 =  2 . Hence:

                     =  and  =  +                          

                                                           2

            The amplitude spectrum remains the same the average remains zero. The even-

            indexed amplitudes and phases remain zero. The phase of the first harmonic ( = 1)

                                                                                                                                                         

            increases with 2 , a quarter of a wavelength, corresponding to 2 2 0 = 04 , a quarter of
            the period, see (2.3). The phases of the other odd-indexed th harmonics increase with
             times this phase shift:   . The coefficients of () become (recall, in Example 3.1

                                                             2

            we found for signal () that  = 0  ):

                                                                         
                     =  sin( ) and  =  cos( )
                                     2                                   2

            Since  = 0 for even indices , and cos(  ) = 0 for odd indices , we conclude that

              2

             = 0  : signal () is even, no sines are needed. Hence, using (3.9):

                            +4         for  = 1, 5, 9, ...
                       4               for  = 3, 7, 11, ...
                     = -               for  even

                            
                           0
                           

            The half-wave odd symmetry is maintained. Figure 3.3 shows the first  = 17 Fourier

            series coefficients (left) and single-sided amplitude and phase spectra (right) of ().

            The signal () is even, and all sines in () (3.10) become cosines in (), with positive

            coefficients        for     =  1, 5, 9, ...    and  negative    coefficients  for     =  3, 7, 11, ....

                              
3.6 Single-sided spectrum                                                                                                   41

        1.5                                                                 1.5
        1.0                                                                 1.0
        0.5                                                                 0.5
        0.0                                                                 0.0
      -0.5
      -1.0                                                                        0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16

               0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16                                                     k

                                        k                                     

        1.5                                                                    
        1.0                                                                    2
        0.5
        0.0                                                                   0

               0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16                           0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16

                                        k                                                                   k
  b ak k
                                                                        Ak k

Figure 3.3: Signal () = ( + 0 ) first  = 17 Fourier series coefficients (left column) single-sided

                                                                    4

amplitude and phase spectra (right column).

This is because for  = 1, 5, 9, ... the sines become positive cosines ( = 1: sin(10 +
                                                                                                                
2 ) = cos(10 + 2 - 2 ) = cos(0)  = 5: sin(50 + 5 2 ) = cos(50 + 5 2 - 2 ) =
cos(50 + 2) = cos(50)). For  = 3, 7, 11, ... the sines become negative cosines
( = 3: sin(30 + 3 2 ) = cos(30 + 3 2 - 2 ) = cos(30 + ), etc.).
Hence, in the phase spectrum, when limiting phases to [-, ], the phases of the odd-

indexed coefficients become either zero (for  = 1, 5, 9, ...) or  (for  = 3, 7, 11, ...).

Given  ()     from  Example    3.1,  elaborate           on  the              coefficients  {    ,      }  and  spectrum        EX 3.6
                                                                                               
                                                                                                      
{, } of signal () = ( + 03 ), which is neither odd nor even, Figure 3.4.

Similar to Figure 3.3, shifting a signal in time causes the coefficients {, } to change.
Considering the spectrum, the amplitude spectrum remains the same ( = ). Effects

of a time shift become visible only in the phase spectrum. For this signal, which is

neither  odd  nor  even,  the  ,      and                graphs               become  more  irregular,     which  is  the

                                             

default in practical applications. The signal remains half-wave odd.

  ak    1.5                                              Ak  1.5
        1.0
  bk    0.5                                              k   1.0
        0.0
      -0.5                                                   0.5

               0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16      0.0

                                        k                           0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16

        1.0                                                                                  k
        0.5
        0.0                                                    
      -0.5
      -1.0                                                      
                                                                 2
               0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
                                                               0
                                        k                    - 2
                                                             -

                                                                    0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16

                                                                                             k

Figure 3.4: Signal () = ( + 0 ) first  = 17 Fourier series coefficients (left column) single-sided

                                                                   3

amplitude and phase spectra (right column).
        42                                                                        3. Real Fourier series

EX 3.7      Figure 3.5 shows the Fourier series coefficients 0, {, } of periodic signal (). De-
            scribe () in mathematical terms.

                3 2.0 22                                 3
                2                                        2
                1                                        1                      1.0

            ak  0                                    bk  0
                -1                                       -1
                -2                                       -2                             
                    -3.0                                 -3 -2 2
                -3

                    0 1 2 3 4 5 6 7 8 9 10                    0 1 2 3 4 5 6 7 8 9 10

                                             k                                         k

            Figure 3.5: Fourier series coefficients of signal ().

            Solution The non-zero coefficients can be read from the figure: 0 = 2, 3 = -3,
            9 = 22, 4 = 1 and 9 = -22. The average of the signal is 2, we have a pure
            negative cosine with amplitude 3 at 30 (3 = 3, 3 = ), a pure sine with amplitude

                                                              

            1 at 40 (4 = 1, 4 = - 2 ) and a shifted cosine at 90. With (3.12) and (3.13),
            respectively, the amplitude and phase at the latter frequency yield 9 = 8 + 8 =
            4, 9 = arctan(1) = 4 . Signal () can be written as:

                                                                                                 
                   () = 2 + 3 cos(30 + ) + 1 sin(40) + 4 cos(90 + 4 )

            The sine component can also be written as 1 cos(40 - 2 ). A spectrum like Figure 3.5
            does not provide information on 0, which remains unknown.

EX 3.8      Figure 3.6 shows the amplitude and phase spectrum {, } of periodic signal ()
            that has period 1 s. Describe () in mathematical terms.

                                         6

                8 10 8 10                                                               

            Ak  6                                    k   + 2 /2

                44                                       0

                2               2                        - 2  -/3

                0                                        -

                    0 1 2 3 4 5 6 7 8 9 10 11 12 13           0 1 2 3 4 5 6 7 8 9 10 11 12 13

                           k                                                         k

            Figure 3.6: Single-sided amplitude and phase spectra of signal ().

            Solution The fundamental frequency 0 = 12 rad/s. Signal () can be written as:

                                                                                
            () = 4 cos(0 + ) + 8 cos(12 - 3 ) + 10 cos(60 + 2 ) + 2 cos(144 + )
                              
                    = -4 + 8 cos(12 - 3 ) - 10 sin(60) - 2 cos(144)

        3.6.3 Diagram of coefficients

        With reference to (3.12) and (3.13), the relation between the 2 - 1 real Fourier series coeffi-
        cients and the single-sided spectrum of the  components is shown in Figure 3.7. Throughout
        this book, similar diagrams, e.g., for the complex exponential Fourier series, will be included.
3.7 Parseval: real Fourier series                                                                            43

ak                                  ...                         Ak                             ...

        k=0 k=1 k=2                         k = K -1                    0         1     2           K -1

         AVG                        ...

bk                                             K -1             k                              ...

                 1            2                                            0      1     2           K -1
                                                                        AVG

                 2K - 1 coefficients                                                K components

Figure 3.7: Real Fourier series coefficients (left) and the single-sided amplitude and phase spectra (right), they
are related through (3.12) and (3.13) In this diagram, AVG means average.

3.7 Parseval: real Fourier series

Here we deal with periodic signals, which are by definition power signals, since their energy
is infinite. Power can be calculated in time, but also in frequency, using Parseval's theorem:2

              1  0+0                       21                           21      

                           2                              22                         2
    =              () d = 0 + ( + ) = 0 +                                                                    (3.16)
              0                               2                              2
                 0                                 =1                           =1

using (3.12) and computing the average power using one period 0, as in (2.19). The power
                                      2       2
of  the  signal  average      equals       =       and  the  power  of  the  th  harmonic  in  the  Fourier  series
                                        0       0
equals 12 (2 + 2) = 12 2. These follow from the discussion in Section 2.5, see Example 2.10.
    As shown in (3.16), only the amplitudes matter, and the phases and frequencies are ir-

relevant when computing signal power. The amplitude spectrum contains all the information

required to compute the signal power. Finally, note that to compute a signal's power, we need

to include all components of the Fourier series, i.e.,   , see the following example.

    Compute the average power of the square wave () of Example 3.1 using (3.16).                                     EX 3.9
    Assume that () is a voltage signal, its unit is [V].

    Solution The square wave average power can be easily obtained in time:

          = 1 ((1)2 0 + (-1)2 0 ) = 1 W
                 0               2            2

    Half of the period the signal value equals +1, and half of the period it equals -1. Cal-
    culating the average power in frequency using Parseval's theorem is more complicated.
    Time shifts do not affect the amplitude spectrum, the only thing that matters in the
    power calculation, so we can use the 0, {, } coefficients calculated in Example 3.1,
    with (3.12), to directly obtain the amplitude spectrum :

                        4     for  odd
                              for  even
          = { 
                   0

    with   + and 0 = 0 = 0. Then, using (3.16) we get:

                    21              2 2 1 16                 1          8 2

          = 0 +   = 0 + 2                                           2 = 2 =1W
                           2                  2  (2 - 1)  8
                              =1                      =1

2After the French mathematician M.-A. Parseval (1755-1836).
         44                              3. Real Fourier series

             Here, we first expressed the series in its odd-indexed coefficients only and then used
             (A.22), a result from power series [6]. Clearly, this signal's average power is easier to
             calculate in time. Given that time shifts do not matter in power calculations, the power
             of signals () and () in, respectively, Examples 3.5 and 3.6, is the same as .

                  With the Fourier series, however, we can compute which components contribute
             most to the signal power. Figure 3.8 shows the `cumulative build-up of power'  as
             we add more and more components in our calculation above. Signal component  = 1
             contributes 81% of the power, and after adding component  = 7 we already reach
             95% of the total signal power . Using a Fourier series model, we can `see' how the
             signal power is built up as a function of frequency.

                    1.00

             P, Px  0.75      P = 95%Px

                    0.50 81%

                    0.25

                    0.00

                         0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16

                                                                   k

             Figure 3.8: Power build-up: the more harmonic components are used in the Fourier series, the closer the
             power  of the series gets to the power  of signal () (horizontal dashed line). Here   16.

EX 3.10      Compute the average power of signals () and () from Examples 3.7 and 3.8. You
             can assume that both are voltage signals, unit [V].
             Solution For signal (), use (3.16):

                                    

               = 02+ 12 (2 +2) = (2)2+ 12 ((3)2 + (1)2 + (22)2 + (-22)2) = 4+13 = 17 W

                                   =1

             Similarly, for ():

                                                

                     = 02 + 12  2 = (4)2 + 12 ((8)2 + (10)2 + (2)2) = 16 + 84 = 100 W

                                              =1

             Both signals are a summation of sinusoids. Example 2.10 showed that the power of a
             sinusoid with amplitude  equals 2 . The power of () and () can be obtained by

                                                                            2

             simply adding up the powers of all components, and the power of the average.

             In the next chapter, the complex exponential Fourier series will be discussed, which is
         much more common in signal analysis than the real Fourier series. The complex exponential
         Fourier series forms a convenient stepping stone towards the Fourier transform.
           4

Complex exponential Fourier series

In Chapter 3 we decomposed a real-valued, periodic signal () (period 0) into a sum of
harmonic cosines and sines, known as the real or trigonometric Fourier series. In this chapter,
we perform the same signal decomposition but express it differently, using complex algebra,
yielding the complex exponential Fourier series. The complex Fourier series allows us to easily
obtain the double-sided amplitude and phase spectra of periodic signals. We will discuss the
relationships between the real and complex Fourier series, the effects of signal symmetry on
the complex Fourier series coefficients, and the time shift and differentiation theorems.

4.1 Derivation of the complex exponential Fourier series

Using Euler's formula (C.1) from Appendix C, we have:

cos(0) = 1 (0 + -0) and sin(0) =  (-0 - 0)
            2                           2

Substituting these in (3.1) we obtain (for an infinite number of components ):

            =                     =
() = 0 +   1  (0 + -0) +    (-0 - 0)
            =1 2                  =1 2

Collecting the terms associated with the positive and negative exponents, we get:

            =   1              =  1
                  (                 (
()  =  0 +           - ) 0  +          + ) -0                                      (4.1)
            =1 2               =1 2
                                     -

Here we define new coefficients,  and -, for   +. This is the complex exponential
form of the Fourier series:

            =        =

() = 0 +  0 +  --0

            =1       =1

Defining 0 = 0 (recall 00 = 1), and re-defining the index in the second summation,

            =        =-

() = 0 +  0 +  0

            =1       =-1

                               45
46                                                 4. Complex exponential Fourier series

yields a more common form of the complex Fourier series, the synthesis1 equation:

                  =                                                                (4.2)

    () =  0

                =-

The coefficients  can be computed using the analysis equation:

     = 1  ()-0 d                                                                   (4.3)

              0 0

with   , or, using the real Fourier series coefficients  and , (4.1):

    0 = 0 and  = 21 ( - ) and - = 21 ( + ) for   +                                 (4.4)

Hence, Re() =  2 and Im() = - 2 for   +. The complex exponential Fourier series
coefficients  and - are conjugate complex numbers,

    -  =                                                                           (4.5)

            

in a similar way as that the complex exponential function 0 is the complex conjugate of
complex exponential function -0. Hence, whereas the right-hand side of (4.2) consists of

the summation of complex numbers multiplied by the complex exponential basis function, the

result of this summation is the real-valued signal (). The unit of the coefficients  equals
the unit [unit] of the signal.

4.2 Interpretation, polar form

The building block of the complex exponential Fourier series is, as its name implies, the com-

plex exponential basis function. Its components have positive frequencies, 0, and negative
frequencies, -0, a convenient mathematical construct which stems from the sum of a com-
plex signal and its complex conjugate. The summation for  in (4.2) runs from  = - to

, referring to (angular) frequencies 0 from - to  (similarly, for linear frequencies 0,
with 0 = 20). The complex Fourier series is said to be double-sided, in line with the con-
struction of a real signal as the sum of a complex signal and the complex conjugate of that

signal, see (2.10) in Section 2.3.3 and Appendix C.

    The complex Fourier series coefficients  are, in general, complex-valued, and can there-
fore also be expressed in polar form, see Appendix C:

     = ||                                                                          (4.6)

The magnitude of the complex coefficient equals:

    || = Re2() + Im2() = 12 2 + 2 = 12  with   +                                   (4.7)

and the phase of the complex coefficient equals:2

     = arctan ( Im() ) = arctan (-  )  with   +                                    (4.8)
              Re( )  

1From Ancient Greek: súnthesis = `the combination of parts so as to form a whole' analusis = `separation of a

 whole into its component parts'.
2Numerically, e.g., in Python, you need to use the four-quadrant arctan function.
4.3 Effects of symmetry                                                                  47

Figure 4.1: Left: complex Fourier series coefficient  represented by a vector in the complex plane (for  > 0,
 > 0 and  > 0), with magnitude ||. Phase  is shown here as a negative angle. Right: complex Fourier
series coefficients  and - represented by vectors in the complex plane.

Note that 0 = 0 represents the signal's average, with |0| = |0| and 0 = 0 (0  0) or
                                    
0  =  ±  (0  <  0).  Because  -  =      it  follows  that  |- |  =  | |  and  -  =  - .
                                      

    The coefficient  is the phasor description, as explained in Section 2.3.1. It scales the
complex exponential basis function 0 magnitude from 1 to ||, and sets its phase angle

at  = 0 seconds from 0 to , see (4.6). Its conjugate, coefficient -, scales the complex
exponential basis function -0 magnitude from 1 to |-|, and sets its phase angle at
 = 0 seconds from 0 to -. The sum of the two complex conjugate phasor signals, 0
and --0, as they always occur in pairs in (4.2), yields the real-valued th harmonic

component. The coefficients  and -, for  > 0, are illustrated in Figure 4.1.

4.3 Effects of symmetry

Similar to the real Fourier series coefficients, the complex Fourier series coefficients show

particular patterns when the signal being constructed has a symmetry property. From (4.4)

we can directly derive that when a signal is even (only cosines are needed, all 's are zero)
the complex Fourier series coefficients are real-valued when a signal is odd (only sines are

needed, all 's are zero) the coefficients are imaginary, and when a signal is half-wave odd
(all even-indexed {, }'s are zero) the even-indexed coefficients are zero.

    This also follows from re-writing the analysis equation, (4.3), as follows:

         1                                  (-1)
       =  () cos(0) d +  (                            () sin(0) d)                       (4.9)
         00                                 0 0

                real part Re()              imaginary part Im()

When () is even, (-) = (), the imaginary part in (4.9) is zero, as it integrates the
multiplication of an even function with an odd function, which results in an odd function, the
integral of which over one period equals zero. Similarly, when () is odd, (-) = -(), the
real part in (4.9) is zero. In general, signals are neither even nor odd, and (some or all of)
their complex Fourier series coefficients are complex-valued.

4.4 Double-sided spectrum

The complex exponential Fourier series gives an array of complex numbers {}, for   ,
which can be directly converted to the double-sided amplitude and phase spectra. This is
because the complex Fourier series is already written as a sum of complex conjugate phasors,
(2.10). For instance, consider the th harmonic, with (4.6):

       0 + --0 = || 0 + |-|- -0
        48                                               4. Complex exponential Fourier series

        For the positive frequency 0 we have amplitude || and phase  for the negative fre-
        quency -0 we have amplitude |-| = || and phase - = -. Hence, the amplitude
        spectrum is an even function of frequency, while the phase spectrum is an odd function of

        frequency. Note that for the amplitude spectrum we use the modulus or magnitude of ,
        and hence, `magnitude spectrum' would be a more appropriate term.

            Comparing the magnitude and phases of the double-sided spectrum, (4.7) and (4.8), with

        the expressions obtained for the single-sided spectrum, (3.12) and (3.13), we see that, for

        positive frequencies, the phases are the same, and the amplitudes are halved in the double-

        sided spectrum. At the zero frequency,  = 0, i.e., the average, the value remains the same.

        4.4.1 Diagram of coefficients

        In practical applications, we have  components and the complex exponential Fourier series
        gives an array of 2 - 1 complex numbers {}, for  = -( - 1), ... , -1, 0, 1, ... ,  - 1. The
        relation between the 2 -1 complex Fourier series coefficients and the double-sided spectrum
        of the  components is illustrated in Figure 4.2. Compare this diagram with the diagram of
        the real Fourier series, Figure 3.7, and study the differences.

                                           X-k  =  X     (k  N+)

                                                      k

        Xk               ...                                         ...

               -(K - 1)       -2         -1        0     1        2           K -1

        |Xk|                                                      related to single-sided spectrum
                         ...                                         ...
                                                                                        X0 = A0
               -(K - 1)       -2         -1        0     1        2           K - 1 |Xk| = 21 Ak

            k            ...                                         . . . k  N+
                                                                              K - 1 k = k
               -(K - 1)       -2         -1        0     1        2

                                                AVG

               K - 1 negative frequencies                   K - 1 positive frequencies

        Figure 4.2: Complex Fourier series coefficients (top row) and the double-sided amplitude (middle) and phase
        (bottom) spectra, see (4.5), (4.7) and (4.8). In this diagram, AVG means average.

EX 4.1  4.4.2 Example

            Compute the complex Fourier series coefficients of the square wave of Example 3.1,
            using the real Fourier series coefficients. Show the complex coefficients as a function
            of frequency, together with the double-sided amplitude and phase spectra.

            Solution In Example 3.1, we found that 0 = 0,  = 0  ,  = 4 for odd  and
             = 0 for even . Using (4.4) we obtain 0 = 0 = 0 and, for   +:

                        - 2   for  odd                          + 2 for  odd            (4.10)
                = {           for  even      and - = { 

                        0                                       0 for  even

            One is advised to start with ,   +, then take the complex conjugate of  to
            obtain -. Figure 4.3 shows the real and imaginary parts of , the double-sided
            amplitude || and phase  spectra, for   , using  = 17 components. Compare
            this spectrum with its single-sided equivalent, Figure 3.2. To facilitate this comparison,

            Figure 4.3 shows the  values in light gray (row 3). Amplitude and phase spectra are,
            respectively, even and odd functions of frequency. They are referred to as line spectra

            because only discrete frequencies occur in them, namely integer multiples of 0.
4.5 Examples                                                                                              49

Re(Xk )  1.0
         0.5
         0.0

                  -16 -14 -12 -10 -8 -6 -4 -2 0 2 4 6 8 10 12 14 16

                                                                           k

Im(Xk )    1.0
           0.5
           0.0
         -0.5
         -1.0

                     -16 -14 -12 -10 -8 -6 -4 -2 0 2 4 6 8 10 12 14 16

                                                                             k

         1.5     Ak
         1.0
|Xk |    0.5

         0.0

              -16 -14 -12 -10 -8 -6 -4 -2 0 2 4 6 8 10 12 14 16

              k

k           
             2

           0
         - 2

                  -16 -14 -12 -10 -8 -6 -4 -2 0 2 4 6 8 10 12 14 16

                                                                           k

Figure 4.3: Signal (): real and imaginary part of the complex Fourier series coefficients (rows 1 and 2)
double-sided amplitude (row 3) and phase (row 4) spectra, for the first  = 17 components.

For the complex Fourier series, showing the amplitude and phase spectra is more com-
mon than showing the real and imaginary parts of the coefficients as a function of
frequency. When considering the Fourier series coefficients corresponding to the th
harmonic,  and -, the amplitude spectrum gives no indication about whether the
coefficient is real-, imaginary-, or complex-valued. This information is all contained in
the phase spectrum. Phases equal to 0 and ± indicate pure cosines (real), phases
equal to ±  indicate pure sines (imaginary), and any other phases indicate shifted

                    2

cosines (complex). In Figure 4.3, the phases reveal that all non-zero complex Fourier
series coefficients are imaginary-valued: they all represent pure sines.

4.5 Examples                                                                                                  EX 4.2

    Consider a periodic signal () which is composed of three cosines and two sines:

            () = 1 cos() + 2 cos(2) + 1 cos(4) + 1 sin() - 2 sin(3)

    Determine the fundamental frequency 0 and period 0. Compute the real Fourier series
    coefficients 0, {, } and, from these, the complex Fourier series coefficients .

    Solution The five components are commensurable and their highest common fre-
    quency equals 0 = 0.5 Hz this signal is periodic with period 0 = 2 s. The signal
    is already written as a Fourier series, so we can quickly obtain its real Fourier series
    coefficients through writing () like (3.2), with 0 = 20 =  rad/s:

            () = 1 cos(10) + 2 cos(20) + 1 cos(40) + 1 sin(10) - 2 sin(30)
        50                                             4. Complex exponential Fourier series

            Then: 0 = 0, 1 = 1, 2 = 2, 4 = 1, 1 = 1 and 3 = -2. All other coefficients are
            zero. The complex Fourier series coefficients can be computed using (4.4):

                  0 = 0, 1 = 12 (1 - ), 2 = 1, 3 = , 4 = 12

                                 1                                                 1
                          -1 = 2 (1 + ), -2 = 1, -3 = -, -4 = 2

            Figure 4.4 shows the signal, the real Fourier series coefficients and the double-sided
            amplitude and phase spectra. Verify their magnitudes and phases using (4.7) and (4.8).

                    5                            2                             2

            y(t)    0                                                          1

                  -5                      ak     1                         bk  0
                       0
                                                                               -1

                                                 0                             -2

                           1           2            012345                         012345
                          t [s]
                                                    k                                 k

                                          |Yk |  2                         k      
                                                                                   2
                                                 1                                
                                                                                   4
                                                 0
                                                                                  0
                                                   -5-4-3-2-1 0 1 2 3 4 5      - 4
                                                                               - 2
                                                                   k
                                                                                     -5-4-3-2-1 0 1 2 3 4 5

                                                                                                   k

            Figure 4.4: One period of signal () (left), its real Fourier series coefficients  and  (right, top) and
            its double-sided amplitude and phase spectra, || and  (right, bottom).

EX 4.3      Compute the complex exponential Fourier series coefficients  for the pulse train signal
            () illustrated in Figure 4.5.
                                 x(t)
                                                         1.0

                                                                                                                         1

                                                         0.5 4

                                       0.0
                                         -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5

                                                                  t [s]

            Figure 4.5: Pulse train signal (), with amplitude  = 1, period 0 = 1 s and pulse width  = 14 s.

            Solution Though not really needed, we first provide an expression for (). The
            building block of this signal is (), the unit pulse function, (B.1). We scale this pulse
            to a width of  seconds, by considering (  ), still centered at  = 0 s, and we set its

                                                                                           

            amplitude to . This pulse then repeats itself every 0 seconds:

                                =                                                        (4.11)

                                          - 0
                  () =   (  )

                              =-

            In Figure 4.5,  = 1, 0 = 1 s and  = 14 s. We compute the coefficients using the
            analysis equation, (4.3). We choose the integration interval to be symmetric about
4.5 Examples                                                                                                 51

 = 0 s, that is, from  = - 0 to 0 :
                                2        2

                 0                                       
    = 1  2 ()-0 d = 1  2 -0 d
        0 - 0                               0 - 
                 2                                       2

for all   . The integral is evaluated as:

                                   

         1 -0 2  -0 2 0 2
    = [-                           ]=                       (-+)
        0 0                        -  00
                                                                                          
                                      2 =2 sin(0 2 )

Using (C.3), and substituting 0 = 2 0 , we obtain:

              2                   
    = 00 sin(0 2 ) =  sin( 0 )

Multiplying the right-hand side by 1 =  0 and then re-arranging terms yields:

                                                                      0 

                                            
                     0              sin(  )                                            
    = sin( )  =
                                                      0
                                             = sinc ( )                                              (4.12)
                    0              0                        0                          0
                             0              0

with the sinc function defined in Appendix B, (B.3).

     In this case, with a real, even function of time (), all complex Fourier series

coefficients are real-valued their imaginary parts are zero. The complex Fourier series

coefficients represent a real, even, discrete, function of frequency 0, with   .
This follows from the symmetry properties discussed in Section 4.3.

     Figure 4.6 shows the double-sided magnitude (amplitude) spectrum ||, at left,
and the double-sided phase spectrum  = arg() at right, for the first  = 11
components, in blue. In the amplitude spectrum, it also shows the values of , in
gray, which is possible in this example because  only has a real part.

   0.3                                                                            

Xk, |Xk|                                                                          
                                                                     k = arg(Xk)
   0.2                                   Xk < 0                                   2

                     Xk < 0                                                       0

   0.1

   0.0 -8 -4                          4        8                                  - 2

-0.1                                                                              -

        -10 -5               0           5        10                                   -10 -5  0  5  10

                             k                                                                 k

Figure 4.6: At left, magnitude (amplitude) spectrum || (blue). It also shows the (here, real-valued)
complex Fourier series coefficients  (gray). At right, phase spectrum showing the angle (or argument)
of the coefficients  = arg(), in radians, for the pulse train in Figure 4.5.

For the signal average, we substitute  = 0 in (4.12) and obtain, since sinc(0) = 1,
0 =  0 = 14 . This makes sense, as the signal illustrated in Figure 4.5 equals 1 during
 = 1 s and 0 during 0 -  = 3 s, for every 0 = 1 s period.
4                                  4
        52                    4. Complex exponential Fourier series

            The sinc function sinc(  ) has its zero-crossings when  = ±1, ±2, ..., see Ap-
                  0                       0
            pendix B. When  fits an integer  number of times in 0,  = 0, then the complex

            Fourier series coefficients will indeed be zero for  = ±, ±2, ±3, .... In this exam-

            ple,  = 4, so  = 0 for  = ±4, ±8, ..., indicated by the red dots in Figure 4.6.

            In Figure 4.6, some  coefficients have negative values (e.g., 5). These coeffi-

            cients lie on the negative real axis in the complex plane their phase is ±. Here, we

            have chosen the phase of these coefficients for  > 0 to be +. Then, because of the

            oddness of the phase spectrum, the phase of these coefficients for  < 0 equals -.

            The phases of all positive-valued  coefficients are zero, as all these coefficients

            lie on the positive real axis in the complex plane. The phase of the average, 0, is zero
            as the average is a positive real number, 1 .

                                                                                       4

        4.6 Two Fourier series theorems

        The complex Fourier series comes with a number of theorems which can reduce the number
        of calculations needed to find the series expansion. Two theorems are discussed here.

        4.6.1 Time shift theorem

        Imagine that we have already obtained the complex Fourier series coefficients  of a periodic
        signal (). The time shift theorem of the complex Fourier series allows us to quickly calculate

        the complex Fourier series coefficients  of the time-shifted signal () = ( + 0):

             = 00 with                                                                       (4.13)

        Because of the time shift, the phase of the complex Fourier coefficients changes: arg() =
        arg() + 00 recall (3.15). Their magnitudes remain the same: || = ||. The complex
        Fourier series coefficients, the phasors, rotate in the complex plane.

EX 4.4      Using the time shift theorem, compute the complex Fourier series coefficients  of
            signal () = ( + 0 ), with () the square wave of Example 4.1.

                                                4

            Solution Coefficients  have been computed, (4.10). To obtain (), signal () is
            shifted by 0 = 04 s (0 > 0, to the left). Applying the time shift theorem, (4.13), yields:

                    =  0 4 =  0 4 =  0  2 0  2

            While this equation holds for   , one is advised to work with    and first obtain
            the coefficients at the positive frequencies, then take the complex conjugate to obtain
            the coefficients at the negative frequencies. Since  2 = (), once we know  we
            can readily compute . We can show this in a table, for   0:

            0 1 2 3 4 5 6 7 ...

              0 -2  0 -2 3 0 -2 5 0 -2 7
            () 1  -1 - 1  -1 -

             0 2  0 -2 0 2 0 -2
                  3        5  7

            As expected, all complex Fourier series coefficients  are real-valued, and signal ()
            is an even function. Taking the complex conjugate to calculate - leads to the same
4.6 Two Fourier series theorems                                                                                                   53

Fourier series coefficient in this case: - = . In other words:

         + 2             for  = ±1, ±5, ±9, ...                                        (4.14)
          ||             for  = ±3, ±7, ±11, ...
                         for  even
                      2

    =  - ||
         0
         

As expected, the magnitudes remain the same: || = ||. The complex coefficients
are rotated by  2 in the complex plane. When considering the coefficients  as a
function of frequency 0, we obtain a real and even function of frequency. This is
because () is real and even. One can check the complex Fourier series coefficients

in (4.14) using the real Fourier series coefficients computed in Example 3.5.

4.6.2 Differentiation theorem

Imagine that we have obtained the complex Fourier series coefficients  of a periodic signal

(). The differentiation theorem allows us to quickly calculate the complex Fourier series
                         d()
coefficients  of signal () =  (with    ), the th derivative of ():+

                                d

    = (0) with                                                                                 (4.15)

Compute the complex Fourier series coefficients  of signal (), defined as the first                                                   EX 4.5
derivative of () in Example 4.2: () = d ().

                                                                            d

Solution With the Fourier series coefficients  of (), and 0 =  rad/s, we obtain,
using (4.15) with  = 1, that  = (0)1 = (). For  = 0 we obtain 0 =

0(0) = 0: differentiating a constant with respect to time yields zero. That is, ()

will always have a zero average. Consider positive frequencies,  > 0: then, each

coefficient  is multiplied by , an imaginary number with magnitude  and phase
                                                                                       
2 . Hence, the magnitude || = ||, and the phase arg() = arg() + 2 . All
coefficients  are rotated counterclockwise in the complex plane by +  , and multiplied

                                                                                                                               2
by  to produce . For the negative frequencies,  < 0, the -s are rotated by - 2 ,
i.e., clockwise, and multiplied by ||. This can be shown in a table, for   0:

        0 1              234

         0 12 (1 - ) 1  12

    0  2 3 4

         0 2 (1 + ) 2 -3 2

We see that, e.g., 2 = 1 lies on the positive real axis and is rotated to the positive

imaginary axis (and multiplied by 2) to become 2 = 2. The cosine component with

magnitude 2 and frequency 20 = 2 rad/s becomes a sine component with magnitude
                   frequency.a                                              
4  and  the  same               Now,  of  course,  the  relationship  -  =      still  holds,  as
                                                                              

() remains a real-valued signal. We obtain:

                                  

   0 = 0, 1 = 2 (1 + ), 2 = 2, 3 = -3, 4 = 2

                                  

               -1 = 2 (1 - ), -2 = -2, -3 = -3, -4 = -2
        54                                                    4. Complex exponential Fourier series

            Figure 4.7 shows () and its double-sided amplitude and phase spectra. Verify their
            magnitudes and phases using (4.7) and (4.8).

              30                                                                                                    
              15
                                             3                                                                      
               0
            -15z(t)                                                                                                 2
            -30                                            |Zk |                                                    
                                                                                                       arg(Zk )
                  0                          2 04

                                                                                                          - 4
                                              - 2

                                             0                                                                   -

                            1        2          -5-4-3-2-1 0 1 2 3 4 5                                                 -5-4-3-2-1 0 1 2 3 4 5
                           t [s]
                                                           k                                                           k

            Figure 4.7: Signal () (left) and its double-sided amplitude (center) and phase spectra (right). All other
            coefficients, magnitudes and phases, are zero.

            aIn plain mathematics: d (2 cos(2)) = -4 sin(2) = -4 cos(2 -  ) = 4 cos(2 +  ).
                           d                                                                                     2        2

        4.7 Parseval: complex exponential Fourier series

        Similar to the real Fourier series, the average power of a periodic signal () can be computed
        with Parseval's theorem (Section 13.2.1) using the complex Fourier series coefficients:

                   1  0+0                                        
            =
            0              2() d =           | |2  =  2    +  2      | |2                                                    (4.16)

                                                        0

                      0              =-                          =1

        Again, only the amplitudes matter the phases and frequencies are irrelevant when computing
        signal power. The amplitude spectrum contains all the information needed to compute the
        signal power. We do need to include all components of the Fourier series, i.e.,   .

EX 4.6      Compute the average power of the square wave () of Example 4.1 using (4.16).
            Assume that () is a voltage signal, and its unit is [V].

            Solution The square wave average power was computed to be 1 W in Example 3.9.
            Using Example 4.1 the complex Fourier series coefficients magnitudes can be calcu-

                                                                                              2

            lated: |0| = 0, || = 0 for even , || = |  | for odd . Using (4.16) we get:

                                                4             1      8 2

             = 0 + 2  || = 0 + 2 2 22     2                      2 = 2 = 1 W,

                           =1                    (2 - 1)  8
                                                   =1

            where we applied the same reasoning as in Example 3.9.

        With the complex Fourier series covered, it is a small step to the Fourier transform, the

        quintessential tool in signal analysis, which can deal with aperiodic signals. The basic idea

        is that we increase the period 0 of the periodic signal () in fact, we make it infinitely large,
        and then look at what happens to the complex Fourier series coefficients . Chapter 5 will
        show that we get a continuous function of frequency ().
                           5

                       Fourier transform

Previously we covered the real Fourier series and the complex exponential Fourier series,
which are both signal decompositions of periodic signals. The Fourier transform applies to
aperiodic signals, and can be considered as to evolve from the complex exponential Fourier
series for a periodic signal, when the period 0 of that signal becomes infinitely large. In this
chapter, the Fourier transform will be derived, effects of signal symmetry on the transform
investigated, and the double-sided spectrum for aperiodic signals defined. The relationship
with the complex Fourier series will be established, and it will be shown that periodic signals
can also be Fourier-transformed, in-the-limit, yielding Dirac delta functions in frequency.

5.1 Rationale: development towards aperiodic signals

Recall Example 4.3, where we studied the complex exponential Fourier series for the pulse
train signal (), shown in Figure 4.5 (amplitude  = 1, period 0 = 1 second and pulse width
 = 14 s). The complex exponential Fourier series coefficients  of this periodic signal are:

  
 = 0 sinc ( 0 ) ,      (4.12)

Because signal () is real and even, the coefficients  represent a real, even, discrete
function of frequency 0 (or 0), with   . Now, suppose that the amplitude  and pulse
width  remain constant: what would happen to this function when the time between the
pulses, the signal period 0, increases?

    Figure 5.1 illustrates () for two periods, 0 = 1 s (top, left) and 5 s (top, right). At bottom,
for both periods, it shows 0 as a function of frequency  = 0 in [Hz], as stems, and the
continuous function () = sinc(), dashed. Clearly, the spacing of the Fourier series
coefficients along the frequency axis decreases, and the bottom-right graph becomes much
more dense. This is because when 0 increases, the fundamental frequency 0 decreases, and
since we show  as a function of 0, the coefficients move closer and closer in frequency.
For 0 = 1 s, the coefficients are 1 Hz apart for 0 = 5 s, they are 15 Hz apart.

    Eventually, for 0  , the complex Fourier series coefficients merge to become a continu-
ous function of frequency, and together form the dashed envelope function () in Figure 5.1:
0  ( = 0). As we will discover in Example 5.1, () is the Fourier transform of one
single pulse, here shown with amplitude  = 1, width  = 1 s, and its center at  = 0 s.

                                                                                                           4

                                                            55
56                                                                                              5. Fourier transform

             1.0                          1                          1.0                                1

x(t)         0.5 4 1 0.5 4 5                            x(t)

             0.0                                                     0.0

             -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5                          -7.5 -5.0 -2.5 0.0 2.5 5.0 7.5

                                  t [s]                                                         t [s]

             0.3                                                     0.3

T0Xk, X(f )  0.2                             X(f )      T0Xk, X(f )  0.2                                   X(f )

                               1                                                             1
             0.1                                                                             5

                                                                     0.1

             0.0                                                     0.0

    -0.1                                                 -0.1

             -10          -5      0          5      10               -10  -5                    0          5      10

                                  f [Hz]                                                        f [Hz]

Figure 5.1: Two pulse trains (top row) differing in their period 0 = 1 s (left column) and 0 = 5 s (right column)
pulse duration  = 1 s and amplitude  = 1. The bottom row shows complex Fourier series coefficients scaled

                                       4

by 0, i.e., 0, as a function of frequency  in [Hz], with  = 0 and the envelope function, (), the Fourier
transform of one single pulse with duration  = 1 s and amplitude  = 1 (dashed).

                                                                                                4

5.2 Derivation of the Fourier transform

To derive the Fourier transform from the complex exponential Fourier series in a heuristic
fashion, we return to the synthesis and analysis equations, (4.2) and (4.3):

                           =                                                                                      (4.2)

             () =  20

                         =-

where we substituted 0 = 20, and:

                     1  0/2
              =
                  0      ()-20 d                with                                                              (4.3)

                        -0/2

where the integral over one period 0 is conveniently chosen to be symmetrical about zero.
    With 0 = 10 we obtain, by substituting the second equation above into the first one:

                                  0/2

             () =  (0  ()-20 d) 20

                  =- -0/2

    Now, when 0  , then 0 becomes infinitesimally small: 0  d, and the product 0
approaches the continuous frequency variable . The summation from  = - to  = 

becomes an integration in  from  = - to  = :

                                  

             () =  (d  ()-2 d) 2

                  =-              =-
5.3 Conditions                                     57

Re-arranging the terms yields:

                

() =  (  ()-2 d) 2 d

=- =-
                  = ()

The integration within the parentheses, known as the Fourier integral, is defined as the
(continuous-time) Fourier transform of aperiodic signal ():

                                                   (5.1)

() = {()} =  ()-2 d

                                      =-

The inverse Fourier transform follows readily as:

() = -1{()} =                                      (5.2)

                                 ()2 d

                  =-

Similar to the complex exponential Fourier series, (5.1) is known as the analysis equation, and
(5.2) as the synthesis equation.

    The unit of the Fourier transform of a signal, (), equals [unit seconds] or, equivalently,
[unit/Hz], with [unit] the unit of signal (). Fourier transform () is a density function1
in the frequency domain. According to (5.2) we need to integrate (), multiplied by the
complex exponential 2, over frequency, from  = - to  in order to obtain the original
signal () in the time domain.

    The complex-valued coefficients  with the complex Fourier series for periodic signals,
pertaining to discrete frequencies equal to  times the fundamental frequency of that signal
(i.e., 0 or 0), have become a complex-valued, continuous function of frequency () with
the Fourier transform. Apparently, for aperiodic signals, we may need components with all
possible frequencies  to construct these signals using the complex exponential basis function.

    The continuous-time signal () and its continuous-frequency Fourier transform () are

                                                                                                               

referred to as a Fourier transform pair, and denoted by ()  (), or, as {(), ()}.
    An overview of frequently used Fourier transform pairs can be found in Appendix E. The

next chapter will elaborate on the most important Fourier transform theorems. One of these
theorems, linearity, states that the Fourier transform is a linear operation:

{11() + 22()} = 11() + 22()                        (6.1)

for arbitrary constants 1 and 2. This property will already be used throughout this chapter.

5.3 Conditions

Not all (aperiodic) signals can be Fourier-transformed. The conditions that guarantee that the
Fourier transform of a signal () exists, are called the Dirichlet2 conditions [3]:

   1. Signal () has a finite number of finite discontinuities,

1Similar to the probability density function in statistics, the `signal content' at a single discrete frequency 1, equals

                            =1

 zero, that is,  ()2d = 0.

                           =1

2After the German mathematician J.P.G. Lejeune Dirichlet (1805-1859).
58                                                                       5. Fourier transform

2. Signal () has a finite number of minima and maxima, and                           (5.3)

3. Signal () is absolutely integrable over the infinite interval (-, ), that is:

                   

            |()| d < 

                 -

Typically, but not invariably [5], these conditions mean that () will decay to zero as   ±.
    Conditions 1-3 above are sufficient, meaning that there are signals that violate at least one

of the conditions, but still have Fourier transforms, [7]. Several examples of these signals,
e.g., the Dirac function () and sinusoidal signals, will be shown in Section 5.8, where Fourier
transforms in-the-limit are discussed.

    While signals that are absolutely integrable, (5.3), have finite energy, the reverse is not
always true [3]. Hence, a weaker condition, relevant in this book, is that real signal () has
finite energy:

                                                                                     (5.4)

     =  2() d < 

                  -

Most practical signals, with finite energy, can be Fourier-transformed.

5.4 Interpretation, polar form

The Fourier transform has much in common with the complex exponential Fourier series. Both
have the complex exponential basis function as their building block, are evaluated at positive
and negative frequencies (i.e., are double-sided) and, for real signals, always consist of pairs
of complex conjugates.

    Similar to the complex Fourier series, the Fourier transform () of a signal is, in general,
complex-valued, and can be expressed in polar form:

    () = |()| ()                                                                     (5.5)

where for the phase () the notation with the angle symbol, (), is also often used, and
sometimes the argument arg(()).

    The magnitude of the complex-valued Fourier transform, at each frequency , equals:

    |()| = Re(())2 + Im(())2                                                         (5.6)

and the phase of the complex-valued Fourier transform, at each frequency , equals:3

                            Im(())                                                   (5.7)
    () = arctan ( Re(()) )

    The magnitude |()| is a density function. Suppose signal () has unit Volt [V], then
|()| has unit [V/Hz]. |()| represents the `level of signal magnitude per frequency '. To
compute the contribution, in terms of magnitude, of the harmonics in a band of frequencies
[1, 2] (in Hz) to signal (), we must integrate |()| from 1 to 2. This will be further
discussed in Section 5.9.

3Numerically, e.g., in Python, you need to use the four-quadrant arctan function.
5.5 Effects of symmetry                                                            59

Note that (0) does not represent the signal's average (substitute  = 0 in (5.1)):

                                                                                   (5.8)

(0) =  () d

              =-

This is an important difference with the complex exponential Fourier series, where 0 equals
the signal mean over one period. Note that energy signals must have zero mean, see Exam-

ple 2.11. For real-valued signals (), (0) lies on the real axis, its magnitude is |(0)|, and
its phase equals either 0 (when (0)  0) or ± (when (0) < 0).

5.5 Effects of symmetry

The Fourier transform, like the Fourier series, shows particular patterns when the signal be-
ing transformed has a symmetry property. Aperiodic signals have two possible symmetries,
evenness and oddness, see Table 2.1. All real-valued signals () can be written as the sum
of an even signal () and an odd signal (), (2.5). Then, {()} = {() + ()} =
{()} + {()}. Re-writing the analysis equation, (5.1), we obtain:

                                                             

() =   () cos(2) d             +  ((-1)  () sin(2) d)                              (5.9)
                                          -
            -
      real part Re(()) = {()}       imaginary part Im(()) = {()}

The real part of () is the Fourier transform of the even part of a signal, (). The imaginary
part of () is the Fourier transform of the odd part of that signal, (). This is because the
product of an even function and an odd function results in an odd function, and the integration

of an odd function symmetrically about zero yields zero.

5.6 Double-sided spectrum

The Fourier transform of signal () generally gives a complex-valued, continuous function of
frequency, (), which can be converted to the double-sided amplitude and phase spectra.
For real-valued signals (), the subject of this book, the value of its Fourier transform at any
frequency , and corresponding frequency -, are complex conjugate numbers:

(-) = ()                                                                           (5.10)

This result is similar to the complex Fourier series, (4.5).

Proof Eq. (5.10) can be proven easily, for real-valued signals ():

                         

() =  (()-2) d =  ()+2 d = (-)

      -                  -

    For real-valued signals it then follows that |(-)| = |()| and (-) = -(). The
amplitude spectrum is an even function of frequency, and the phase spectrum an odd function
of frequency. Note that for the amplitude spectrum we use the modulus or magnitude of (),
so that `magnitude spectrum' would be a more appropriate term.

    From (5.9) it follows that the Fourier transform of an even signal is a real-valued, even
function of frequency. Similarly, the Fourier transform of an odd signal is an imaginary-valued,
        60                                                                                       5. Fourier transform

        odd function of frequency. A signal that has no symmetry properties has a complex-valued
        Fourier transform. This, and more, will be explained in the following examples.

        5.7 Examples

EX 5.1      Compute the Fourier transform of the pulse function () =  (  ), (2.14).

                                                                                                                               

            Solution To compute () we simply substitute () in (5.1):

                     () =  { (  )} =   (   ) -2 d =  2 1-2 d 
                                        -                             -
                                                                      2

                        = [ 1 -2] 2  = - (  1 -2 2 -   2 2 )
                        -2              -2                2 

                                                                      =-2 sin(2  )

                                                                                              2

                             sin() 
                        =   = sinc()

            with the sinc function from Appendix B. We obtained our first Fourier transform pair:

                                                                                                                                 (5.11)

                         ( )  sinc()

                             

                   1.0                                             2

            x(t)                                       2  |X(f )|
              X(f )
                   0.5

                   0.0                                             0
                          -3 -2 -1 0 1 2 3                          -3 -2 -1 0 1 2 3
                                                 t [s]                                       f [Hz]

                     2                                             

                                                          X(f )    

                                        - + 20

                     0 - 2  0 

                                                                                                        -

                        -3 -2 -1 0 1 2 3                              -3 -2 -1 0 1 2 3

                                f [Hz]                                                           f [Hz]

            Figure 5.2: Left column: unit pulse function () =  (  ) ( = 2) (top) and its Fourier transform,

                                                                                                                                   2

            () = 2sinc(2) (bottom). Right column: double-sided amplitude (top) and phase spectra (bottom).

            Figure 5.2 shows () (top, left), for  = 2. Signal () is real and even, and so is its

            Fourier transform, see Figure 5.2 (bottom, left), for  = 2.

            Clearly, (0) = 2, while this signal has a zero average over the interval (-, ).
            The zero-crossings of sinc() occur at  = ± 1 , ± 2 , ..., i.e., with  = 2 at  =  1 Hz
                                                                                                                                 2
                 {0}.

            Figure 5.2 (at right) shows the amplitude (top) and phase (bottom) spectra, which

            are even and odd functions of frequency, respectively. When ()  0 (+), this value

            lies on the positive real axis, and its phase is 0. When () < 0 (-), its phase equals

            ±. Here, we choose () to be + when () < 0,  > 0. Then, () must be -

            when () < 0,  < 0.
5.7 Examples                                                                                      61

Compute the amplitude and phase specta of the unit doublet () = ( - 1 )-( + 1 ).                      EX 5.2
                                                                                  2     2

Solution To obtain the spectra, we first compute the Fourier transform of ():

                 0                1
            = (-1)-2 d +  1-2 d = 1 ([-2]0 - [-2]1)
        ()                                                   2     -1                0

                 -1               0

            = 1 (2 - 2 cos(2)) = - 4 sin2() = -2sinc2()                              (5.12)
                 2                           2

Because signal () is real and odd, its Fourier transform is imaginary-valued and odd.

        The magnitude equals 2||sinc2(), an even function of frequency. The phase is
arctan ( -2sinc2() ) which equals arctan(-) = -  for  > 0 and arctan(+) = + 
              0                                              2                              2

for  < 0. For  = 0, (0) = 0, and the phase is defined to be 0.

        Figure 5.3 shows () and () in the left column (with an imaginary-valued vertical

axis), and the spectra in the right column. Note that function () equals zero at all

integer multiples of 1 Hz at these frequencies its phase should be 0 (not shown).

        1                      1                |Y (f )|  2

y(t)    0                                                 0
                                                           -3 -2 -1 0 1 2 3
        -1                                                                          f [Hz]

               -3 -2 -1 0 1 2 3
                                      t [s]

        2j                                                

        j2
Y (f )                                          Y (f )
        0                                                 0

        -2j -j - 2

            -3 -2 -1 0 1 2 3                                 -3 -2 -1 0 1 2 3

                       f [Hz]                                      f [Hz]

Figure 5.3: Left column: unit doublet () = ( - 1 ) - ( + 1 ) (top) and its Fourier transform, ()
                                             2                  2
(bottom). Right column: double-sided amplitude (top) and phase (bottom) spectra.

Compute the Fourier transform of the exponential decay function () = -() for                          EX 5.3
 > 0 and with () the unit step function from Appendix B. Compute its real and
imaginary parts and show the amplitude and phase spectra.

Solution The Fourier transform () can be computed through direct substitution:

                                             

        () =  -()-2 d =  -(+2) d

                 -                           0

                                     
            = [- 1 -(+2)] = 1
                      + 2            0  + 2                                          (5.13)

where    yields -  0 (for  > 0), and -2 is bounded (its magnitude
is 1  ). Because () has no symmetry properties, its Fourier transform () is a
62                                                                         5. Fourier transform

    complex-valued function of . We compute the real and imaginary parts as follows:

                              1  - 2  - 2
               () =  + 2  - 2 = 2 + (2)2

    Then Re(()) = 2+(2)2  and Im(()) = 2+(2)2 -2 . The amplitude and phase follow
    from, respectively, (5.6) and (5.7):

                      1                                             -2
               |()| = 2 + (2)2 and () = arctan(  )

    Then |(0)| = 1 , |(±)| = 0 and (0) = 0, () = -  , (-) = +  .
                                                                        2  2

               1.0                                             1.0

    z(t)       0.5       =1                           z(t)     0.5         =3

               0.0                                             0.0
                      -1 0 1 2 3 4 5                                  -1 0 1 2 3 4 5
                                             t [s]                                           t [s]

    Re(Z(f ))  1.0                                             1.0

               0.5                                    |Z(f )|  0.5

               0.0                                             0.0
                      -3 -2 -1 0 1 2 3                                -3 -2 -1 0 1 2 3
                                           f [Hz]                                          f [Hz]

    Im(Z(f ))  0.5                                                
                                                                   2
               0.0                                    Z(f )       
                                                                   4
               -0.5
                         -3 -2 -1 0 1 2 3                         0
                                              f [Hz]           - 4
                                                               - 2

                                                                      -3 -2 -1 0 1 2 3

                                                                                           f [Hz]

    Figure 5.4: Top row: exponential decay function () = -() for  = 1 (left) and  = 3 (right).
    The middle and bottom rows show the real and imaginary parts of () (at left), and the double-sided
    amplitude and phase spectra (at right).

    Figure 5.4 shows () and (), for two values of  = 1 (blue) and  = 3 (dashed
    green). The top row shows the time signals the middle and bottom rows show the
    real and imaginary parts of () (at left), and the amplitude and phase spectra (at
    right). The real part of () is an even function of frequency the imaginary part is
    an odd function of frequency. When  increases, the magnitude of () is smaller for
    lower frequencies, but for higher frequencies the value of  does not make a difference.

5.8 Fourier transform in-the-limit

Some signals () fail to meet the Dirichlet conditions, yet have Fourier transforms. In this
section, we introduce a few basic and very useful signals belonging to this category.
5.8 Fourier transform in-the-limit                                                                                63

5.8.1 Dirac delta function and constant                                                                           (5.14)

                      

        ()  1

Both functions are even, and real-valued. The extremely `narrow' Dirac delta function in time,
when Fourier-transformed, becomes extremely `broad' in frequency, as () = 1  .

Proof Although the Dirac function has non-finite discontinuities, its Fourier transform is
easily found through direct substitution, then using the sifting property, (B.12):

                                                  

        () = {()} =  ()-2 d = 0 = 1

                                                -

                                                                                                                  (5.15)

1  ()

Both functions are even, and real-valued. The extremely `broad' constant function in time,
() = 1  , when Fourier-transformed, becomes extremely `narrow' in frequency.

Proof Signal () is clearly not absolutely integrable, yet has a Fourier transform. Again
we use the sifting property and now start with the inverse Fourier transform:

                                                      

        () = -1{()} =  ()2 d = 0 = 1

                                                    -

    We have obtained two important Fourier transform pairs: {(), 1} and {1, ()}. These
pairs follow when using the duality Fourier theorem, Section 6.5. As the Fourier transform is
linear, both sides of these pairs can, for instance, be multiplied by constant .

5.8.2 Cosine and sine                                                                                             (5.16)

The cosine function () =  cos(20) has as its Fourier transform:

                                                
        () = { cos(20)} = 2 (( + 0) + ( - 0))

Both functions are even, and real-valued.

Proof We prove (5.16) in two steps. First, a frequency-translated impulse 1() = ( +
0) is inverse Fourier-transformed:

                                    

1() =        1()2 d =                  (    +   )2     d  =   -20 

          -                         -           0

and similarly 2() = ( - 0):

                                    

2() =        2()2 d =                  (    -   )2     d  =   20 

          -                         -           0

We then add these two time signals together, multiply by  , and use (C.1) to obtain:

                                                                                                               2

()  =          +  2())  =           ( -20   +   20  )  =   cos(20)
         (1()
       2                     2
64                                                                                     5. Fourier transform

    1.0                                                                     1.0

    0.5

x(t)0.0                                                                     0.5
                                                                     X(f )

    -0.5

    -1.0                                                                    0.0

          -1     0                  1                                              -2  0                                                          2

                 t [s]                                                                 f [Hz]

Figure 5.5: Fourier transform in-the-limit.1At left signal () = cos(20), with 0 = 2 Hz, as a function of time,
and at right its Fourier transform () = 2 (( + 0) + ( - 0)), as a function of frequency.

    Similarly, adding the two Fourier transforms together and multiplying by  yields:

                                                                                                                                               2

                                    
          () = (1() + 2()) = (( + 0) + ( - 0))
              2                     2

    Properties of the Fourier transform, such as linearity and frequency-translation used above,
are discussed in detail in Chapter 6. Signal () and its Fourier transform () are shown in
Figure 5.5. () is an even, real-valued, continuous function of frequency. It equals zero for
all frequencies, except at ±0, where it equals a Dirac pulse multiplied by 2 (here  = 1).

    The Fourier transform of the sine function () =  sin(20) can be obtained similarly:

                                                                                                                                                     (5.17)
    () = { sin(20)} =  2 (( + 0) - ( - 0))

Both functions are odd, the Fourier transform is imaginary-valued.

5.8.3 Relation with the complex exponential Fourier series

In the discussion of Fourier transforms in-the-limit, three of the four example signals were in

fact periodic signals, for which the complex Fourier series coefficients can be (easily) obtained.

    First, consider the constant in time, function () = 1. Recall that this is indeed a periodic

function, Section 2.1. Its complex Fourier series has just one non-zero coefficient, 0 = 1,
the signal average. This coefficient belongs to  = 0, i.e., only at (discrete) frequency  = 0

we have a non-zero number. In contrast, the Fourier transform of the constant, (), is a

continuous function of frequency. It equals zero at all frequencies, except at  = 0, where it

has a Dirac pulse multiplied by 1.

    Second, consider the cosine function () =  cos(20). In the real Fourier series, it has

one non-zero coefficient: 1 = , at (discrete) frequency 10. The complex Fourier series has
                                                                                 
two non-zero coefficients: 1 = 2 (at 10) and -1 = 2 (at -10). In contrast, the Fourier
transform is a continuous function of frequency. It is zero at all frequencies, except at  = ±0,
where it has a Dirac pulse multiplied by  .
                                       2
    Third, consider the sine function () =  sin(20). In the real Fourier series, it has

one non-zero coefficient: 1 = , at (discrete) frequency 10. The complex Fourier series
                                                                                   
has two non-zero coefficients: 1 = - 2 (at 10) and -1 =  2 (at -10). In contrast, the
Fourier transform is a continuous function of frequency. It is zero at all frequencies, except at
 = ±0, where it has a Dirac pulse, multiplied by - 2 (at 10) or  2 (at -10).
5.8 Fourier transform in-the-limit                                                        65

    Fourier transforms in-the-limit of periodic time signals lead to the inclusion of Dirac delta
functions in the frequency domain. When Dirac delta functions appear in the Fourier transform
of a signal, that signal is a power signal. Section 6.12 will formalize the relation of the Fourier
transform of periodic functions with their complex exponential Fourier series.

5.8.4 Examples

The principle of performing a Fourier transform in-the-limit is illustrated using two exam-
ples. In the first example, the Dirac function () and the constant signal () = 1 will both
be expressed as limiting cases of a unit pulse function (see Appendix B), and then Fourier-
transformed. In the second example, a signal () which is not absolutely integrable will be
approximated by means of an auxiliary signal () = ()-||, for  > 0, which is absolutely
integrable. Whereas () cannot be computed directly, () can. We then take the limit
  0, where lim0 -|| = 1. Then () approximates () and () approximates ().

Express the Dirac function () as the limiting case of a scaled pulse function and show               EX 5.4
that, in-the-limit, its Fourier transform equals 1  . Repeat the analysis for a constant
function () = 1 and show that, in-the-limit, its transform equals ().

Solution Using (B.10) we can write () = lim0 1  (  ) = ().       Fourier-
transforming this equation yields:

1                                   1                   sin() 0
{()} = {lim  ( )} = lim sinc() = lim                    =
0      0                               0                   0

Using L'Hôpital's rule, we quickly find: {()} = 1  . This is a heuristic, alternative
way to derive (5.14).

    In a similar way, the Fourier transform of a constant can be found, using:

                           
       () = lim  ( ) = 1

                  

Fourier-transforming this equation yields, see (5.15):


{1} = {lim  ( )} = lim sinc()  ()
     

Figure 5.6 shows the transforms in-the-limit for both functions, for different values of
. The top row, for () = 1  (  ), shows that when  is smaller, the function in time

                                                  

approximates the Dirac function better it becomes narrower and higher, its Fourier

transform () = sinc() becomes broader and, in the limit,   0, approaches 1  .
     The bottom row, for () =  (  ) shows that when  is larger, the function in

                                                                    

time becomes broader and its Fourier transform () = sinc() narrower. At  = 0,

(0) = sinc(0) = , which becomes  when   . The zero-crossings of the sinc
function occur at  =  1 , with     {0}, and get closer and closer to  = 0 when
  . 

    We obtain a function that has similar properties as the Dirac delta function: zero  

except for  at  = 0, and its integral equal to 1. The latter point can be understood by

considering the, by approximation, triangular shape of the sinc function around  = 0
(bottom right plot, in red): its area equals 1 2  = 1.

                                                                              2
        66                                                                                    5. Fourier transform

            x(t)  5          1                          X(f )   11 1
                  4          25
                  3                                                 25 1 5
                  2                        1
                  1                                             0
                  0                   5 =1

                     -2  -1     0         1        2                  -5 -4 -3 -2 -1 0 1 2 3 4 5
                                                                                           f [Hz]
                  1 10       t [s]

            y(t)  0                             3       Y (f )  23 10 3
                     -2              1
                                                                1                                1

                                                                0

                         -1     0         1        2            -1
                                                                        -5 -4 -3 -2 -1 0 1 2 3 4 5

                             t [s]                                                            f [Hz]

            Figure 5.6: Left column: time signals () = 1  (  ) (top), and () =  (  ) (bottom). The right column
                                                                                  
            shows their Fourier transforms. Signal () and () are shown for  = 1 (blue), 1 (green, dashed) and
                                                                                              5
            1 (black, dotted). Signal () and () are shown for  = 1 (blue), 3 (green, dashed) and 10 (black,
            25
            dotted). The bottom right plot shows the triangle (red) referred to in the text.

EX 5.5      Compute the Fourier transform of signal () = - 1 (-) + 1 -().a Show that,
                                                                   2                 2
            in-the-limit, we can compute the Fourier transform of the unit step function () using
            the expression () = () + 1 .
                                     2

            aFor  = 0, signal sgn() = 2() is known as the sign or signum signal.

            Solution Recall Example 5.3 where a similar function () was Fourier-transformed,
            for  > 0. The Fourier transform theorems, discussed in Chapter 6, would allow for a
            quick calculation of () from () instead, we apply direct substitution here:

                             0                          
                  () = - 1  -2 d + 1  --2 d
                             2 -                   20

                             11           11                       -2
                         = - 2  - 2 + 2  + 2 = 2 + (2)2

            In the limit case, when   0, we get () = -  = 1 .
                                                                2 2
                  Now, it is given that () = () + 1 , so () = () + 1 (), and we obtain the
                                                   2                              2
            Fourier transform of the unit step function ():

                                  1           1
                  () = {()} = 2 () + 2                                                                (5.18)

            Compare this result with the Fourier transform of () = -() for  > 0, () =
               1 . Apparently, we cannot obtain the Fourier transform of () by simply substi-

            +2

            tuting  = 0 in the expression for ()! This is because () can be calculated in the

            manner performed in Example 5.3 only for  > 0. When  > 0, () and () are
            energy signals. When  = 0, () and () become power signals, with averages 1 and

                                                                                                                                                               2

            0, respectively. Clearly, we must be careful with Fourier transforms in-the-limit.
5.9 Parseval: Fourier transform                                                                                                     67

        0.5                                          4j -j 2f
        0.0                       =0     V (f )      2j
v(t)  -0.5                                           0
  u(t)        = 15                                   -2j     = 1
                                                                 5             -j
                                                     -4j
                                                                               2f

             -2     -1     0      1   2                      -0.2 -0.1 0.0 0.1 0.2

                           t [s]                                       f [Hz]

        1.0                              Im(U (f ))  4j      -j 1      2 (f )       1                  Re(U (f ))

                                                     2j      2f

        0.5                                          0                              0
                                                     -2j
                                                     -4j                       -j

        0.0                                                                    2f

             -2     -1     0      1   2                      -0.2 -0.1 0.0 0.1 0.2

                           t [s]                                       f [Hz]

Figure 5.7: Fourier transform in-the-limit of signal () = - 1 (-) + 1 -() (top row) and the unit
                                                          2      2
step () (bottom row). Signal () is shown for  = 0 (blue), and  = 1 (green). While () only has
                                                                    5
an imaginary part, () is complex-valued. Its real part is shown in black, its imaginary part in blue.

Figure 5.7 shows () and () at left and their Fourier transforms () and () at
right. Signal () is shown for two values of , namely  = 0 and  = 1 .

                                                                                                                                 5

5.9 Parseval: Fourier transform

The Fourier transform exists for signals that fulfill the Dirichlet conditions, Section 5.3. Gen-
erally, these signals are energy signals: energy is limited, the average power is zero. Energy
can be calculated in time, but also in frequency, using Parseval's theorem (Section 13.2.2):

                           

 =  2() d =  |()|2 d                                                                                   (5.19)

          -                -

For a signal with unit [unit], the energy has unit [unit2 s]. The fact that we integrate |()|2
over all frequencies  indicates that, like |()|, |()|2 is also a density function.

Compute the energy of signal (), from Example 5.3 its unit is [V].                                                                      EX 5.6

Solution Although computing this signal's energy is easier using (2.17), see Exam-
ple 2.9, here we compute it in the frequency domain, using (5.19):

                                                             
         =  |()|2 d =  2 + (2)2 1 d = 2 2  1 + ( 2 1 )2 d
                 -                -                          0         

                                

             = 2 2 2  1 + 2 1 d = 1 [arctan()]0 = 1   2 = 12 J

                               0

In the first line we used the fact that the integrand is an even function of frequency

(and so are all amplitude spectra of real-valued signals) and in the second line we used

 = 2 , so d =  d. When   0, energy becomes infinite and () is a power
                        2
signal. These results correspond with the answers obtained in Example 2.9 for  = 1.
        68                                            5. Fourier transform

EX 5.7      Compute the energy of () =  (  ) + 1 its unit is [V].

                                                                           2

            Solution The Fourier transform can be swiftly obtained from (5.11) and (5.15):

                   () = 2sinc(2) + ()

            where we used the property that the Fourier transform is a linear operation. A Dirac
            pulse appears in (): this signal has infinite energy, it is a power signal and its
            average power  is 12 = 1 W. The average of this signal equals 1 the pulse  ( 2 ) at
             = 0 is irrelevant. Recall Example 2.11, with here  = 1,  = 2 and  = 1.

EX 5.8      Compute the energy of unit pulse () =  (  ) (unit [V], with  = 1) in time and

                                                                                                  

            frequency. In frequency, compute the contribution to the energy of the unit pulse
            from: (a) all frequencies between ±1 Hz, and (b) at frequency 0.5 Hz.

            Solution The energy of a unit pulse () =  (  ) can be easily calculated in time:

                                                                                                       

             = (1)2 =  J. In frequency, we use (5.19):

                                    sin() 2

             =  |()|2 d =  |sinc()|2 d = 2  (  ) d
            -         -       -

               1  sin() 2     21 
            = 22   (  ) d() = 2  2 =  J
                   0

            Here, we used the fact that the sinc function is an even function of frequency, and
            (A.15). For  = 1, the energy is 1 J. Energy in time equals energy in frequency.

                 To calculate the contribution to the energy of the unit pulse from all frequencies
            between ±1 Hz (a), consider Figure 5.2 (bottom, left). When starting at  = 0 and
            moving left or right on the frequency axis, () has its first zero-crossings at  = ± 1 =

                                                                                                                                                                   

            ±1 Hz. Hence, we are requested to compute the contribution of the central lobe of
            (), centered at 0 Hz, to the total energy of (). We obtain:

               1 1 sin() 2
            |[-1,1] =  |()|2 d = 2  (  ) d  0.9028 J
               -1          0

            through numerical integration. The central lobe contains  90.3% of the pulse energy.

                To calculate the contribution to the energy of the pulse at 0.5 Hz (b), (5.19) shows
            that we integrate |(()|2 from  = 0.5 to 0.5, which results in 0 J.

                 This, and the previous calculation, illustrates that |()|2 is a density function.

            Only when a signal's Fourier transform () contains a Dirac pulse at some frequency

            0, which makes that signal a power signal, we obtain a non-zero contribution at that
            frequency 0 to the signal's average power.

            With the Fourier transform and its inverse defined, the following chapter will discuss the
        main Fourier transform theorems. These will prove to be essential tools in signal analysis.
        6

Fourier transform theorems

In the previous chapter, we introduced the Fourier transform, allowing a description of a
continuous-time signal () as a continuous-frequency function ():

                                                   (5.1)

() = {()} =  ()-2 d

                                     -

    Suppose we have the Fourier transform of signal () available, and we require the Fourier
transform of a signal (), which is obtained by applying some operation on (). Examples
are applying a time shift, i.e., () = ( - 0), or a time scaling, () = (). Then, is
it possible to obtain the Fourier transform of (), in an easy way, rather than having to
evaluate the Fourier integral again? Fortunately, this is indeed the case, and in this chapter
we demonstrate, for a collection of basic operations on a signal in the time domain, how these
operations carry over to the Fourier transform of that signal, in the frequency domain. As will
become clear in the remainder of this book, the Fourier transform theorems will prove to be
instrumental in signal analysis in continuous time as well as in discrete time.

6.1 Linearity                                      (6.1)

                                              

        11() + 22()  11() + 22()

    The Fourier transform is a linear operation. The Fourier transform of a linear combination

(also called superposition) of two (or more) signals 1() and 2(), with, respectively, Fourier
transforms 1() and 2(), is just this linear combination of the Fourier transforms.

Proof Fourier-transforming () = 11()+22() (with 1 and 2 arbitrary constants),
we obtain through direct substitution:

     

() =  ()-2 d =  (11() + 22()) -2 d
-    -

                                                

= 1  1()-2 d + 2  2()-2 d = 11() + 22()
  -                                             -

    We used this property in Chapter 5 several times, for instance while explaining the sym-
metry properties, in Section 5.5, where {()} = {() + ()} = {()} + {()}.

                                          69
70                                   6. Fourier transform theorems

6.2 Time shift

    ( + 0)    ()20                                                               (6.2)

            

    A time shift (a time delay 0 < 0, or a time advance 0 > 0) of signal () in time causes
its Fourier transform () to be multiplied by the complex exponential 20. Since the
magnitude of the latter is 1  , the magnitude of () is not affected, only its phase changes.

    Proof With () = ( + 0), the Fourier transform reads:

                               

    () =  ()-2 d =  ( + 0)-2 d
              -                -

    Substitute  =  + 0 then  =  - 0, d = d. When   - then   -, and when   
    then   . We obtain:

                                  

    () =  ()-2(-0) d =  ()-220 d

                       -          -

                       

            =  ()-2 d 20 = ()20

                       -

    where in the second line we used the fact that the complex exponential 20 does not
    depend on  and can therefore be taken out of the integral in .

    The magnitude is invariant under a time shift: |()| = |()|, but its phase has changed:

arg(()) = arg(()) + 20. The effect is referred to as a phase rotation. Compare with
(3.15) and (4.13).

6.3 Time scale change

                 1                                                               (6.3)

    ()  ( ) for   0

                       || 

    Changing the scale of the running variable (time ) inversely scales the running variable
(frequency ) of the Fourier transform. When || > 1 the signal gets compressed in time,
and expanded in frequency conversely, when || < 1 the signal gets expanded in time, and
compressed in frequency. Recall Section 2.4 and the rules of signal operations.

    Proof With () = (), the Fourier transform reads:

                               

    () =  ()-2 d =  ()-2 d

              -                -

    We distinguish two cases:  > 0 and  < 0. For positive , we apply a change of variable
     =  then  =  and d = 1 d:
                          

                       ()-2  1 d = 1   ()-2   d
                              -
    () = 

              =-

    which, with  the frequency variable in the Fourier transform of (), yields:

                           

              1
    () =  (  )
6.4 Time reversal                                                                             71

For negative , we recognize that  = -||:

                 

() =  (-||)-2 d

                -

Substitute  = -||, then  =  and d = - 1 d when   - then   + when
                        -||               ||

   then   -:

- () =  ()2 || (- 1 ) d = 1 =  ()-2 || -  d
                        || || =-
                   =

which, with -  the frequency variable in the Fourier transform of (), yields:

                          ||

                   1    1
() = || (- || ) = || (  )

as here  < 0. Combining the results for  > 0 and  < 0 leads to (6.3).

6.4 Time reversal

                                                                                (6.4)

(-)  (-)

This theorem is a special case of the scale change theorem, (6.3), with  = -1.

6.5 Duality

                                                                                (6.5)

()  (-)

    In other words, if {(), ()} is a Fourier transform pair, then {(), (-)} is also a Fourier
transform pair. For every transform that we find, duality gives us two transform pairs.

Proof The expression for the Fourier transform, (5.1), and the one for the inverse Fourier
transform, (5.2), are very similar. The main differences are the variable of integration and
the sign of the complex exponential function. This suggests some sort of symmetry in the
Fourier transform pair, referred to as duality. Once we have:

                  

() =  ()-2 d

                =-

we can change variables as  =  and  = , and the following relation holds:

                  

() =  ()-2 d

                =-

Now we change variables again, according to  =  and  = -, and we get:

                                          -   

() =                  (-)2 d(-) = -  (-)2 d =  (-)2 d

             -=-                          =   -

where the last expression on the right is the inverse Fourier transform of (-), the result
of which is () as a function of time.
72                                                 6. Fourier transform theorems

6.6 Frequency translation

    ()20    ( - 0)                                                                  (6.6)

          

    This theorem is also known as the frequency shift theorem. It forms a dual of the time

shift theorem, (6.2). Note that multiplying a real-valued signal (), as assumed in this book,
with 20, results in a complex-valued signal.

    Proof When we take the Fourier transform of () = ()20 we find:

                                          

    () =  ()-2 d =  ()20-2 d

                -                         -

                

          =  ()-2(-0) d = ( - 0)

                   -

    such that () has been frequency-shifted by 0.

6.7 Modulation

                1                      1
    () cos(20)  ( + 0) + ( - 0)                                                     (6.7)
                   2                   2

    Signal () is said to be (amplitude-)modulated onto the carrier signal cos(20).

    Proof With Euler's formula, (C.1), we obtain:

            cos(20) = 12 (20 + -20)

    The Fourier transform of () = () cos(20) can be found by applying the frequency
    translation theorem, (6.6), twice. This shifts one copy of () by 0 and shifts one copy of
    () by -0. Both copies are multiplied by 12 :

            () = 12 (( - 0) + ( + 0)) = 12 ( + 0) + 12 ( - 0)

6.8 Convolution                                                                     (6.8)

                                     

        1()  2()  1()2()

    The convolution of two signals () = 1()  2() is an operation defined by the following
integral (as will be explained in more detail in Chapter 7):

                          

       () =  1()2( - ) d

                         -

The convolution integral will play an important role in the analysis of linear time-invariant
systems, Chapter 16.

    Proof Starting with the convolution integral:

                                

            () =  1()2( - ) d

                               -
6.9 Multiplication                                                                     73

we substitute for 2( - ) the inverse Fourier transform of 2() (using  -  as the time
variable, rather than ):

                          

2( - ) =  2()2(-) d

                        -

We obtain:

                                                     

() =  1()  2()2(-) d d =   1()2()2-2 d d
            -         -         =- =-

where we moved the term 1() inside the integral in . Change the order of integration:

                                                              

() =   1()2()2-2 d d =  2()2  1()-2 d d
=- =-                                              -          -

where we moved the term 2()2 outside the integral in . The inner integral in  equals
the Fourier transform of 1():

                             

() =  2()21() d =  1()2()2 d
               =-            -

which presents the inverse Fourier transform of () = 1()2().

    We often explain this theorem as `a convolution in the time domain is equivalent to a
multiplication in the frequency domain'. Its dual is the multiplication theorem, discussed next.

6.9 Multiplication

                                                                     (6.9)

1()2()  1()  2()

    We explain this theorem as `a multiplication in the time domain is equivalent to a convolu-
tion in the frequency domain'.

Proof The convolution of two signals in the frequency domain reads:

                                                 

() = 1()  2() =  1()2( - ) d

                                               -

For 2( - ) we substitute the Fourier transform of 2() (using  -  as the frequency
variable, rather than ):

                           

2( - ) =  2()-2(-) d

                         -

We obtain:

                                                      

() =  1()  2()-2(-) d d =   1()2()-22 d d
            -         -         =- =-

where we moved the term 1() inside the integral in . Change the order of integration:

                                                              

() =   1()2()-22 d d =  2()-2  1()2 d d
=- =-                                              -          -
74                                                6. Fourier transform theorems

    where we moved the term 2()-2 outside the integral in . The inner integral in 
    equals the inverse Fourier transform of 1():

                                            

       () =  2()-21() d =  1()2()-2 d
              =-                            -

    which presents the Fourier transform of () = 1()2().

6.10 Differentiation

    d()          
        (2) ()                                                                            (6.10)
    d

This Fourier theorem holds, assuming that the derivative(s) exist(s).

    Proof With () = d() , its Fourier transform is found through:

                                                 d

       () =  ()-2   d =  d() -2 d
                                       - d
              -

    We then use integration by parts:

                                                                      

       () = [()-2] + 2  ()-2 d = (2)()
                                  - -
                                                        = ()

    The first term equals zero, because () has to be absolutely integrable (Dirichlet condition
    3, (5.3)), hence () has to go to zero both for   - and   , and -2 is bounded.

    Integration by parts can be applied repeatedly to prove (6.10) for  differentiations.

6.11 Integration

                  1 () + 1 (0)()

     () d     (2)           2                                                             (6.11)

    -

                                                      

    Note that when (0) = - ()d = 0, only the first part remains, implying the direct
counterpart of (6.10).

    Proof Integration of () from - to  can be represented as the convolution of ()
    with the unit step function ():

                                               

       () = ()  () =  ()( - ) d =  () d

                         -                     -

    Fourier-transforming this equation and using the convolution theorem, (6.8), yields:

          

       {  () d} = {()  ()} = ()()

           -

    In Example 5.5, using a Fourier transform in-the-limit, we derived ():

       () = {()} = 1 () + 1                                                               (5.18)
                      2        2
6.12 Relation with the complex exponential Fourier series                                       75

Multiplication by ():


{  () d} = ()() = () ( 1 () + 1 ) = (0) () + ()
                         2  2                                                      2  2

-

6.12 Relation with the complex exponential Fourier series

For periodic signals (), with fundamental frequency 0 and (finite) period 0, the following
relationship holds between the Fourier transform and the complex Fourier series:

                                                                                         (6.12)

()   ( - 0)

                =-

Here, the 's refer to the complex Fourier series coefficients of periodic signal (), (4.3).
This relation formalizes the discussion in Section 5.8.3.

Proof The Fourier series representation of periodic signal () equals:

                                                                                         (4.2)

() =  20

             =-

Each complex exponential basis function with frequency 0 can be written as an integral of
a Dirac delta function, positioned at  = 0, using the sifting property, (B.12):

                      

20 =  ( - 0)2 d

                   -

Substituting in the Fourier series representation, we obtain:

                            

() =    ( - 0)2 d =   ( - 0)2 d

   =- -                     =- -

Changing the order of summation and integration yields:

               

() =   ( - 0)2 d

            - =-

                                                                                 

which is the inverse Fourier transform of  ( - 0).

                                                                              =-

6.13 Examples

In this section two examples are presented on the use of the Fourier transform theorems.
        76                                                                                     6. Fourier transform theorems

EX 6.1      In Chapter 5, the Fourier transform of pulse function () =  (  ) was computed:

                                                                                                                                

                                                                                                        (5.11)
            () =  (  )  () = sinc()

            Using duality, obtain the Fourier transform of () = sinc().

            Solution We have the Fourier transform pair {(), ()}, then duality states that
            {(), (-)} is also a Fourier transform pair. Hence:

                                                                                         -
            () = sinc()  (-) =  (  )

            Because  ( - ) is even, (-) = (). The second pair is then:

                                   

                                                                                       
            () = sinc()  () =  (  )                                                                     (6.13)

            In words: `a pulse in time becomes a sinc in frequency a pulse in frequency becomes
            a sinc in time'. Figure 6.1 illustrates this fundamental result, for  = 2.

            1.0x(t)                                                                       2      2
              X (t)
            0.5                                                                 x(f )  123       1

            0.0                                                                   X(f ) 123      0
                   -3 -2 -1 0
                                          t [s]                                                    -3 -2 -1 0 1 2 3
                                                                                                                            f [Hz]
              2
              1                                                                                1.0
              0
                                                                                                                                   2
                -3 -2 -1 0
                                          t [s]                                                0.5

                                                                                               0.0
                                                                                                      -3 -2 -1 0 1 2 3
                                                                                                                            f [Hz]

            Figure 6.1: Duality of the Fourier transform pairs { (  ) , sinc()} and {sinc(),  (  )}.
                                                                                                      

EX 6.2      Recall Examples 5.3 and 5.5, where the Fourier transforms were obtained for, respec-
            tively, decay functions () = -() and () = - 1 (-) + 1 -(). When
                                                                                               2   2
            () = 1 , obtain () in a quick way. Assume  > 0.
            +2

            Solution Using the linearity and time reversal theorems, we immediately obtain:

                1    1                                                                 11      11  -2
            () = - 2 (-) + 2 () = - 2  - 2 + 2  + 2 = 2 + (2)2

            The Exercises book contains many more examples of how to use the Fourier transform
        theorems in a clever way these will prove to be instrumental in the remainder of this book.

            Before we discuss signal analysis in discrete time, the following two chapters will pro-
        vide background on the concept of convolution (in the time and frequency domains) and the
        consequences of finite signal duration on the Fourier transform.
                                                  7

                                                Convolution

Convolution, denoted by the `' symbol, is a mathematical operation on two functions ( and
) that produces a new function (  ). In the previous chapter, we have seen that Fourier-
transforming the product of two functions in time, {()()}, yields the convolution of the
Fourier transforms of the individual signals, (6.9). Its dual states that Fourier-transforming the
convolution of two functions in time, {()()}, yields the product of the Fourier transforms
of the individual signals, (6.8). But what exactly is convolution, what are its properties, and
why is it useful in signal analysis? These questions will be answered in this chapter.

7.1 Definition and rationale

This book discusses the analysis of signals in the time and frequency domain, and in both
domains the convolution operation can be applied. In this section, we first discuss the convo-
lution operation on signals in the time domain. Consider two signals, functions of time ()
and () their convolution ()  () yields another signal ():

                                                                       (7.1)

() = ()  () =  ()( - ) d

                                         =-

Here, we use  as the (arbitrary) running variable in the integral. With () and () running
from  = - to , their convolution yields a new signal () which also runs from  = -
to . An important example where convolution plays a major role is when computing the
response () of a so-called Linear Time-Invariant (LTI) system to an input signal (), where
the response of that system is completely characterized by its impulse response function ().
These will be explained in Chapter 16.

    The operation of convolution can be split into five steps:

1. Consider both signals as a function of time , here () and (),
2. Swap, or time-reverse signal () to yield (-),
3. Shift signal (-) by  to yield ( - ) = (-( - )),
4. Compute the integral of the product of functions () and ( - ), and
5. Repeat steps 3 and 4 for every value of , i.e., from  = - to .

This eventually results in a new signal, (), for all times   (-, ).

77
78                                                                 7. Convolution

    With two simple signals, we evaluate integral (7.1) and demonstrate step-by-step the con-
volution operation. Assume signal () to be the unit step function:

       () = ()

and signal () to be a pulse function:

                       -3
       () = 2 ( 2 )

with a duration of 2 seconds, centered at  = 3 s, and with amplitude equal to 2.
    In the first step, both signals are considered as functions of running variable  rather than

time . Figure 7.1 shows () = () in blue at left, and () = 2 ( -3 ) in green at right. To

                                                                                                                             2

illustrate convolution, we mark three points ((), () and ()) in the function (), which will
be swapped and shifted, with () at the center. Point () marks function () at  = 0.

           4                                  4
                                              2
    x( )   2d                       h( )      0                    a cb

           0                                         -4

              -4  -2  0     2  4                         -2  0     2  4

                       [s]                                    [s]

Figure 7.1: Unit step function () = () at left, and pulse function () = 2 ( -3 ) at right.

                                                                                                                                                                     2

    In the second step, we swap signal () about  = 0, resulting in (-) (time reversal,
Section 2.4), see Figure 7.2 at left. Note that this signal equals ( - ) for time shift  = 0 s.

                               t=0                                    t=1

           4                        h(t -  )  4          1
           2 bc a
    h(- )                                     2 bc a

           0                                  0

              -4  -2  0     2  4                 -4      -2  0     2  4

                       [s]                                    [s]

Figure 7.2: Swapped pulse function (-) = 2 ( --3 ) at left, and swapped and shifted pulse function ( - ) =

                                                                                                      2

2 ( --3 ), for shift  = 1 s, at right.

             2

    In the third step, we shift signal (-) by , resulting in signal ( - ) = (-( - ))
which is shown, for  = 1 s, in Figure 7.2 at right. The center of the pulse (), shifted by
, now occurs at  = -3 +  =  - 3. The edges of the pulse, shifted by , now occur at
 = -4 +  =  - 4 () and  = -2 +  =  - 2 (), see Figure 7.2 at right for  = 1.

    Now, the fourth step is broken into three parts. First, we note that for any  - 2 < 0, hence
 < 2, the product of () and ( - ) is everywhere equal to zero, and hence the integral of
this product is zero. This is because, for  < 2, point () lies to the left of  = 0, where ()
7.2 Properties of convolution                                                                             79

starts to get a non-zero value (point ()). Hence, the two functions () and ( - ) do not
have any overlap their product is zero, and so is the integral (7.1): () = 0 for  < 2.

    In the second part, for   2, pulse function ( - ) `enters' into overlap with the unit
step function. This is shown in Figure 7.3 at left for  = 3. This `entering' covers the range
from  - 2  0 (point () passes point () at  = 0) to  - 4  0 (point () passes point ()
at  = 0) hence 2    4 for shift . When  = 3, we integrate the product of 1 (from ())
and 2 (from ( - )) from  = 0 (when () is non-zero) to  =  - 2 = 3 - 2 = 1 (point ()).
For the `entering' range we obtain:

                          -2

       () =  2 d = [2]0-2 = 2( - 2)

                         0

for 2    4. Again, for shift  = 3, as we can see in Figure 7.3 at left, the area of the product
function equals 2, and ( = 3) = 2, see Figure 7.3 at right.

    In the third and last part, shift  satisfies  - 4 > 0 (point () has passed point () at  = 0)
and the pulse function ( - ) is fully overlapping the unit step function. The boundaries of
the integral of the product function are the left edge of the pulse at  =  - 4 (point ()) and
the right edge at  =  - 2 (point ()). The result becomes:

                          -2

       () =  2 d = [2]-4 -2 = 2( - 2) - 2( - 4) = 4

                         -4

for  > 4. In this way, we have also covered the fifth step of evaluating the integral of the
product function for every value of shift .

    The resulting function () is shown in Figure 7.3 at right, and is given by:

            0   for  < 2                                                                                  (7.2)
() = { 2( - 2)  for 2    4
                for  > 4
            4

                                    t=3
x( ), h(t -  )
                                                                    y(t)434                       t=3

2 bc a                                                                    2                 2
                              d

0                                                                         0

   -4  -2       0                2  4                                        -4  -2  0         2       4

                 [s]                                                                 t [s]

Figure 7.3: Swapped and shifted pulse function ( - ) = 2 ( --3 ), shown here for  = 3 in green, and unit

                                                                                                                                     2

step function () = () in blue, at left convolution output () in red at right.

    Figure 7.4 illustrates the process for these two signals once more. Basically, one function
in the convolution integral `remains where it is' (here () = ()). The other function (here
()) is `flipped around  = 0', shifted to - and then moved right, through changing ,
ultimately to +, and, for each time shift , the product ()( - ) is integrated over all
values of . The outcome of this integral for that value of  then becomes the value of the
new function  at that time , ().

7.2 Properties of convolution

The convolution operation has a number of properties, here stated in the time domain but
equally valid for convolutions in the frequency domain.
80                                                         7. Convolution

            t = -1

    2               h(t -  )           1 u( )

                                                                                                                        =0

            t=0

                                                                                                                           =0

                    t=1

                                                                                                                               =0

                               t=2

                                                                                                                        =0

                                   t=3

                                                                                                               =2

                                          t=4

                                                                                                         =4

                                                t=5

                                                              =4

    -5  -4  -3                 -2  -1  0     1  2  3    4  5

                                        [s]

Figure 7.4: Illustration of the convolution of () = () in blue and () = 2 ( -3 ) in green, for several shift

                                                                                                                                                                       2

times . The values of the convolution integral computed at these shift times  are indicated at right.

First of all, convolution is:

1. commutative: ()  () = ()  (),

2. distributive: (1() + 2())  () = 1()  () + 2()  (), and
3. associative: ()  (1()  2()) = (()  1())  2().

    The commutative property means that:

                                                      

    () = ()  () =  ()( - ) d = ()  () =  ()( - ) d                                                                                 (7.3)

                               -                   -

which can be easily proven by applying a change of variable. In words: when convolving two
functions, it does not matter which of the two functions `remains where it is', while the other
is reversed and shifted. For the two signals used in Section 7.1, this is illustrated in Figure 7.5.

    Furthermore, it can be proven that:

4. for any constant or factor : () = ()  () = ()  (), and

5. for any time shift 0: ( + 0) = ( + 0)  () = ()  ( + 0)

    From the properties of distributivity and associativity with a scalar, it follows that convolu-
tion is a linear operation.
7.3 Convolution with Dirac pulse                                                                            81

                          t = -1

    1       u(t -  )                           h( )           2
                                                                                      =0
                                  t=0
                                                                                      =0

                                       t=1

                                                                                                        =0

                                               t=2

                                                                                                      =0

                                                        t=3

                                                                                             =2

                                                           t=4

                                                                                     =4

                                                              t=5

                                                                 =4

-5  -4  -3  -2        -1          0         1        2  3  4  5

                                   [s]

Figure 7.5: Illustration of the commutative property of convolution, using () and () from Section 7.1.

7.3 Convolution with Dirac pulse

The convolution operation is, arguably, not an easy one, and can rapidly become difficult
for more complicated functions. There is, however, one notable exception, and that is when
one of the functions in the convolution integral is a Dirac delta function. Suppose we would
convolve an arbitrary signal () with a Dirac pulse positioned at  = 0, with 0 > 0:

() = ()  ( - 0) = ( - 0)                                                                                    (7.4)

Proof This can be easily proven by direct substitution in the convolution integral, (7.1):

                                                                    

        () = ()  ( - 0) =  ()( -  - 0) d = ( - 0)

                                                                  -

where we used the sifting property of the Dirac delta function, (B.12).

    Convolution with a Dirac pulse at  = 0 yields a signal which equals (), but which occurs
0 seconds later (because here we defined 0 as larger than 0). Convolving a function ()
with the Dirac pulse ( - 0) creates a copy of that function, where the function value at  = 0
gets positioned at the Dirac pulse position, here  = 0.

7.4 Convolution in frequency

Convolution is an operation on two functions which can be functions of time, of frequency,
or any other variable of interest. Hence, convolution in the frequency domain is exactly the
same operation, follows exactly the same principles, and has exactly the same properties as a
convolution in the time domain. For the sake of completeness, a convolution of two functions
        82                                                          7. Convolution

        () and () in frequency can be stated as:

                      

            () = ()  () =  ()( - ) d                                                       (7.5)

                                                         =-

        We use  as the running variable in the integral. With () and () running from  = - to
        , their convolution yields a new signal transform () which also runs from  = - to .

            Similar to the time domain, a special case is convolution with a Dirac pulse, that is when
        () equals ( - 0):

            () = ()  ( - 0) = ( - 0)                                                       (7.6)

        This relationship will show to be extremely useful, and can be used to quickly prove several
        of the Fourier transform theorems introduced in Chapter 6.

EX 7.1  7.5 Examples

            Compute the convolution of () = 4 (  ) with () = ( + 6): () = ()  ().

                                                                                       4

            Solution

                                                             +6
            () = ()  () = 4 ( 4 )  ( + 6) = 4 ( 4 )

            Pulse () is centered at  = 0 s, pulse () is centered at  = -6 s, the position of the
            Dirac pulse (). The shape of the pulse (amplitude and width) does not change.

EX 7.2      Prove the modulation theorem, (6.7), using the multiplication theorem, (6.9).
EX 7.3
            Solution Suppose () = () cos(20), with 0 the carrier frequency. Then, with
            () = cos(20), and () and () the respective Fourier transforms, we obtain:

                        1                                        1
            () = ()() = ()( (( + 0) + ( - 0)) ) = (( + 0) + ( - 0))
                        2                                        2

            Multiplication in time of a signal () with another signal, in this case a cosine function

            with frequency 0, is equivalent to a convolution in frequency of their Fourier transforms.
            Convolving () with a Dirac pulse at  = -0 creates a copy of () centered at
             = -0: ( + 0). Convolving () with a Dirac pulse at  = 0 yields ( - 0).

            Consider signal () = sinc() with Fourier transform () =  (  ), see Example 6.1.

                                                                                                                                    

            Obtain the Fourier transform () of signal () = ()().

            Solution Fourier-transforming a multiplication of two functions in time results in a
            convolution of the Fourier transforms of the individual signals in frequency, see (7.5):

                                                               -

            =- - () = ()  () =  ()( - ) d =   (  )  (  ) d

            The first pulse function is centered at  = 0, the second pulse function is centered at
             = . We consider four domains for :
7.5 Examples                                                                                                            83

   1. if  < -, then () = 0 as the two pulse functions do not overlap,

                                                              + 

   2. if - <  < 0, then () =   2 (1) d =  + ,

                                                              -2

                                                                         

   3. if 0 <  < , then () =  2  (1) d =  - , and

                                                          - 2

   4. if  > , then () = 0 as the two pulse functions do not overlap.

The first and fourth item can be summarized as () = 0 for || > , and with the
triangular function defined in Appendix B we have obtained:

                        
       () = (  )

which is a common Fourier transform pair:

              22                                             
() =  sinc ()  () =  ( )                                                                                         (7.7)
                                                             

The convolution of () with () is illustrated in Figure 7.6, for  = 2.

X(f - )       f = -3  X ( )                                    -  X()X(f - )d
                                                                             =0
                1
                                                                                   0
              f = -2                                                                     1

              f = -1

              f =0                                                                          2

                      f =1

                                                                                                 1

                                                f =2                                                      0

                                                       f =3                                                     0
                                                               -5 -4 -3 -2 -1 0 1 2 3 4 5
-5 -4 -3 -2 -1 0 1 2 3 4 5
                           [Hz]                                                          f [Hz]

Figure 7.6: Illustration of the convolution of () =  (  ) with itself (here  = 2).

                                                                                                               
        84                                                                                                   7. Convolution

EX 7.4      Evaluate the convolution of the unit step function () = () with the exponential
            decay function () = -(), with  > 0. Both functions are shown in Figure 7.7.

            1.0                                                                           3

            x(t)0.5                                                                       2           =3

                                          y(t)                                            1               =1

            0.0                                                                     h(t)  0

                 -2 -1 0  1       2  3     4                                                 -2 -1 0  1      2  3  4

                          t [s]                                                                       t [s]

            Figure 7.7: Unit step function () = () (left) and exponential decay function () = -() (right),
            for two values of  (1 and 3).

            Solution Because convolution is commutative we can write:

                                                                                       

                   () = ()  () = ()  () =  ()( - ) d

                                                                                     -

            We choose () to `remain where it is' and time-reverse and time-shift (). Then,
            the result after the third step is that we have the exponential decay as a function of ,
            () = -(), and the unit step function has been time-reversed and shifted by
             hence ( - ) = ( - ) as a function of . The exponential decay function ()
            equals zero for  < 0 due to the presence of (). The time-reversed and time-shifted
            unit step function equals zero, i.e., ( - ) = ( - ) = 0, for  -  < 0, i.e., for  > .
            The integration bounds become  = 0 and  = , with  > 0, as otherwise there is no
            overlap between () and ( - ), so () = 0.

                The fourth and fifth steps yield:

                                      

                   () =  - d = [--]0 = -- + 0 = 1 - -

                                     =0

            for  > 0. The solution can also be written as: () = (1 - -)(), for   .
                 The resulting signal is shown in Figure 7.8, for the two values of . We return to

            this example in Chapter 16.

                             1.0                                                             =1
                                    63%

                             0.5
                                       =3

                             0.0

                                 -2 -1 0 1 2 3 4

                                              t [s]

            Figure 7.8: Output () as the result of the convolution () = () and () = -(), for two values
            of . At  = 1 , () equals 63% (=1 - -1) of its final value, 1 see Chapter 16.

                                     

            This chapter discussed the convolution of continuous-time signals, in the time and fre-
        quency domains. The convolution of two discrete-time sequences is covered in Appendix F.

            Before starting with Part III, on discrete-time signals, first the effects of (inevitably) limited
        observation times will be discussed in the next chapter.
           8

Finite signal duration, leakage and
                                   windowing

With the Fourier transform in Chapter 5 we work, in general, with signals () in the time
domain, for - <  < , and hence with signals which extend to infinity, see (5.1). In
practice, however, a signal () will be available and measured only for a finite time duration
, Section 2.6. In this chapter we elaborate on the consequences of limiting the time
duration of a signal on spectral analysis: the Fourier transform of a limited-duration signal
may differ from the Fourier transform of the corresponding infinite-time signal, a phenomenon
called spectral leakage. We discuss ways to mitigate this inevitable practical problem.

8.1 Finite signal duration

Measurements do not last forever, and typically a recording of a signal () for only 
seconds is available. Limiting a signal's duration can mathematically be described by multi-

plication of the infinite-duration signal () by a (dimensionless) time-window function ().

                                                                                

Here we use a rectangular pulse () =  (  ) with width  = . During  seconds, from
 = -  to  s, this window function () equals 1, and it is 0 for all other times.

          22

    The windowed signal () equals the original signal () inside the window of duration
, and it equals 0 outside. In other words, we select only a part of the original signal:

() = ()()        (8.1)

The multiplication Fourier transform theorem, (6.9), states that the product of two time signals
corresponds to a convolution of their Fourier transforms:

() = ()  ()      (8.2)

The Fourier transform of () =  (  ) equals () = sinc(), (5.11). We deliberately set

                                                                 

the window to be symmetric about  = 0 for convenience, such that () is an even function,

and therefore () is a real (and even) function. Both functions are shown in Figure 8.1, for

three different values of window duration . Note that (0) =  and that the zero-crossings
of () occur at  =  1 for     {0}. Hence, the longer the window duration , the

                                             

higher and narrower the main lobe of the sinc function becomes.

    To demonstrate the effects of limiting a signal to a finite duration on its Fourier transform,
we continue with Example 7.3. Signal () = 2sinc2() and its Fourier transform () are
shown in Figure 8.2. The Fourier transform is a triangular function, () =  (  ), (7.7).

                                                                                                                                                 

             85
86                        8. Finite signal duration, leakage and windowing

      1.0                 T =1s             3                     T =1s
                          T =2s             2                     T =2s
w(t)                      T =3s     W (f )  1                     T =3s
                                            0
      0.5

      0.0

           -3 -2 -1 0  1  2      3             -3 -2 -1 0    1    2                                                                                                      3

           t [s]                                 f [Hz]

Figure 8.1: Rectangular window () with three different values for duration , at left, and the corresponding
Fourier transforms (), at right.

      1.0                                   1.0              1.0

x(t)  0.5                           X(f )   0.5

      0.0                                   0.0

           -3 -2 -1 0  1  2      3               -6 -4 -2 0  2    4                                                                                                      6

           t [s]                                 f [Hz]

Figure 8.2: Signal () = 2sinc2() at left, and its Fourier transform () =  (  ) at right, for  = 1.

                                                                                                                                                                       

    We apply a rectangular window () =  (  ), with a duration of  = 3 seconds, and

                                                                                        

obtain (), Figure 8.3 (left). The Fourier transform (), the result of the convolution
of () with (), shown in Figure 8.3 at right, looks a bit `wobbly'. Overall the shape of
function () is still quite similar to the triangular function of (), shown as the dotted
black shape, though the details are different. Through the width of the main peak of the sinc
function, in Figure 8.1 at right, we get a kind of `spreading' effect. The sharp peak of the
triangular function has become smoothed, rounded and smaller. The side lobes of the sinc
function cause scalloping, a `wobbly' effect visible in particular in the tails of ().

    Of course, differences in the Fourier transforms can be expected, as we truncated the
tails of the original signal () on both sides, ()  (). These effects are referred to as
spectral leakage: signal content in the frequency domain gets spread, or moved to, or leaks
into, neighboring frequencies.

    In practice, with a sufficiently long measurement duration , we hope to minimize leakage
and achieve ()  (). One can imagine that using a longer measurement duration  will
be better, having less impact on the Fourier transform, as the sinc function to convolve with,
will be narrower, see Figure 8.1. Ideally,   , and () would become by approximation
a Dirac delta function, the convolution with which would yield (): () = ()  () 
()  () = (), the original Fourier transform. Obviously, a window of infinite length has
no impact at all on the Fourier transform of a signal, that is, its spectral representation.

    Again note that, for convenience, both signal () and window () have been chosen,
on purpose, to be real and even functions, such that their Fourier transforms () and ()
are real and even. Defining () = 1 for  = [0, ] would mean that () is complex-valued.
Hence, in general, () and () are complex-valued and the modulus needs to be taken to
show these transforms in a graph.
8.2 Leakage of harmonics                                                                                                      87

1.0                                                                         1.0                               1.0

xw (t)0.5                                                                   0.5
                                                                    Xw(f )

0.0                                                                         0.0

     -3 -2 -1 0           1     2          3                                     -6 -4 -2 0                   2       4       6

               t [s]                                                                                  f [Hz]

Figure 8.3: Signal () = ()() at left, with time window function () =  (  ) shown in green dots
(for  = 3 s), and corresponding Fourier transform () = ()  () at right (obtained numerically, see
Appendix F).

8.2 Leakage of harmonics

When applying a time window to a cosine signal, the effects of leakage can be very clearly
demonstrated. The Fourier transform of a cosine signal only exists in-the-limit, (5.16), and its
spectral representation only consists of two Dirac pulses, Figure 5.5.

    The cosine function () =  cos(20) has as its Fourier transform () = 2 (( + 0) +
( - 0)). We realize the time window through multiplication of () by window function
() =  (  ). The Fourier transform of () is () = sinc(), and the Fourier transform

                   

of the time-windowed cosine is found through:

                                                                                                                         (8.3)
() = ()  () = sinc()  ( (( + 0) + ( - 0)) )

                                                        2
                
          = 2 (sinc(( + 0)) + sinc(( - 0)))

We obtain two sinc functions, both with amplitude 2 , one centered at -0, the other at 0,
see Figure 8.4. The spreading and scalloping effects are clearly visible, as are the widening

of peaks and the wobbles. Signal (), in red, is no longer periodic, and this is reflected
in its Fourier transform components over a continuous range of frequencies are needed to

construct it.

Finally, note that windowing a harmonic has yet another effect. The harmonic goes on for

ever (instead of fading out when   ±). Taking a longer window means that we get `more

signal'. The window duration  appears in the amplitude of the resulting sinc functions, see

(8.3). The amplitude in the frequency domain is amplified by the window duration.

1                                  x(t)                                     1.0                                       X(f )
                                                                                                                      Xw(f )
                                   xw (t)                                                       0.75

0x(t)                                                                       0.5
                                                                    X(f )
                                                                            0.0

-1

     -2 -1     0             1     2                                             -4 -2                0            2     4

               t [s]                                                                                  f [Hz]

Figure 8.4: Signal () =  cos(20) at left in blue, with 0 = 1 Hz and  = 1, and windowed () in red, with
 = 1.5 s. The corresponding Fourier transform () is shown at right in blue, and () in red.
88                                             8. Finite signal duration, leakage and windowing

          1.0                                                1.0

                       f2  =  f1  +  1                                    f2  =  f1  +  3  1
                                     T                                                  2  T
|Xw(f )|                                           |Xw(f )|
          0.5                                                0.5

          0.0              8         10    12  14            0.0              8         10    12  14
                    6                                                  6

                                  f [Hz]                                             f [Hz]

          1.0                                                1.0
                           f2 = f1 + 2 1T
                                                                          f2  =  f1  +  5  1
                                                                                        2  T
|Xw(f )|                                           |Xw(f )|
          0.5                                                0.5

          0.0              8         10    12  14            0.0              8         10    12  14
                    6                                                  6

                                  f [Hz]                                             f [Hz]

Figure 8.5: Magnitude spectrum of time-windowed signal () = ()(), with () the sum of two zero-phase
cosines 1() = 1 cos(21) and 2() = 2 cos(22), with 1 = 2 = 1, 1 = 10 Hz, and 2 slightly offset
from 1, as indicated window length  = 1 s (rectangular window ()). The magnitude spectra of the two
(time-windowed) cosines are shown separately in green and red curves, for, respectively, |()  1()| and
|()  2()| the resulting spectrum |()| is shown in blue.

8.3 Identifying neighboring frequencies

In this book, we generally present simple examples with signals consisting of a single harmonic
at a specific frequency. Theoretically, with infinite observation times, if we have a signal
consisting of multiple harmonics, even at slightly different frequencies, it would be possible
to identify each harmonic separately in the frequency domain. The Fourier transform (in-the-
limit) of a cosine (frequency 0) consists of two Dirac pulses: one at -0, and one at 0. For
a signal with  separate harmonics, we would see 2 Dirac pulses in the frequency domain,
for example in the magnitude spectrum.

    In practice, however, we deal with finite observation length  and consequently always
face the phenomenon of leakage. As shown in Figure 8.4 at right, signal amplitude `leaks
into' neighboring frequencies, and this may pose problems when the signal contains multiple
harmonics at closely separated frequencies: they will start to overlap in the frequency domain.
In this section we briefly elaborate on this problem, using two examples, and provide initial
guidelines with regard to the ability to identify (harmonics with) neighboring frequencies.

    Two cosines are considered with equal amplitude, for a window length of  = 1 s. The
frequency of the first cosine is fixed, 1 = 10 Hz, and the frequency of the second one, 2,
is varied, such that the separation 2 - 1 equals 1 , 3 1 2  , 2 1 , or 5 1 2  .1 Figure 8.5 shows, in
blue, the magnitude spectrum |()| of the time-windowed signal (double-sided spectrum,
although only relevant part of positive frequencies is shown). In green and red, the spectra
of the two individual (time-windowed) cosines are shown.

    When the two frequencies are close together (only 1 apart top left graph), they cannot be

                                                                                                   

separately identified their spectra merge into a single peak, shown in blue, halfway the two
individual frequencies. When they are separated by 3 1 (top right graph), two separate peaks

                                                                                               2

1In Chapter 12,  = 1 will be introduced as the frequency step of the Discrete Fourier Transform (DFT).

                                            
8.4 Consequences for energy and power                                                                 89

          1.0                                                1.0

                       f2  =  f1  +  1                                    f2  =  f1  +  3  1
                                     T                                                  2  T
|Xw(f )|                                           |Xw(f )|
          0.5                                                0.5

          0.0              8         10    12  14            0.0              8         10    12  14
                    6                                                  6

                                  f [Hz]                                             f [Hz]

          1.0                                                1.0
                           f2 = f1 + 2 1T
                                                                          f2  =  f1  +  5  1
                                                                                        2  T
|Xw(f )|                                           |Xw(f )|
          0.5                                                0.5

          0.0              8         10    12  14            0.0              8         10    12  14
                    6                                                  6

                                  f [Hz]                                             f [Hz]

Figure 8.6: Magnitude spectrum of time-windowed signal () = ()(), with () the sum of two zero-phase
cosines 1() = 1 cos(21) and 2() = 2 cos(22), with 1 = 1, 2 = 0.25, 1 = 10 Hz, and 2 slightly
offset from 1, as indicated window length  = 1 s (rectangular window ()). The magnitude spectra of the
two (time-windowed) cosines are shown separately in green and red curves, for, respectively, |()  1()| and
|()  2()| the resulting spectrum |()| is shown in blue.

start to become visible, and when they are 2 1 apart (bottom left graph), they both can be

                                                                                    

clearly identified, though the attentive reader will notice that the two peaks of the blue curve
are slightly offset (biased) from the actual frequencies, as indicated by the vertical lines.

    When we consider a weak harmonic near a strong one, the problem of identifying them
separately becomes even more challenging. We continue the example and keep the amplitude
of the first cosine at 1 = 1, and set the amplitude of the second one to 2 = 0.25. Figure 8.6
shows, in blue, the magnitude spectrum |()| of the time-windowed signal. In green and
red, the spectra of the two individual (time-windowed) cosines are shown.

    When the frequency separation is 1 (top left graph), the weak harmonic is not visible at

                                                                      

all in the resulting spectrum the strong harmonic masks the weak one. The latter introduces
only a slight bias in the peak of the blue curve toward the right. When the strong and weak
components are separated by 2 1 (bottom left graph), the weak harmonic becomes visible,

                                                           

although with considerable bias (the peak is off from the correct frequency of 12 Hz). Whereas
in this example the two harmonics can be clearly identified at a separation of 5 1 (bottom right

                                                                                                                                           2

graph), this finding depends on the amplitude ratio 2 of the two harmonics.

                                                                                               1

8.4 Consequences for energy and power

Note that windowing a cosine, or any function, periodic or aperiodic, has another effect as
well. The resulting windowed signal is always an energy signal.

    Example 2.10 showed that a cosine () is a power signal, with average power  = 2 2 .

                                                                                                                                                                 2

Windowing the cosine with window length  turns it into an energy signal (),  = 2 .
Recall that taking a longer window means that we get `more signal' and hence also more
energy. The average power of () is zero, however. To obtain its average power, calculated
90                                   8. Finite signal duration, leakage and windowing

for the finite window, we divide its energy by , i.e., the duration of the time window, yielding

            2

 = 2 for the cosine signal. The implicit assumption in this average power calculation is
that the original signal (), running from  = - to , `behaves the same' as the measured
signal (), before and beyond the time window of  seconds.

    We conclude that all windowed signals, and therefore all measured signals, are energy
signals. For an arbitrary signal, to obtain the average power during the measurement window,
we divide the energy accumulated during the window by the window time duration :

            1                                                                                      (8.4)
     =  

8.5 Mitigating leakage: windowing

Measurements have finite duration , so in all practical cases we need to be aware of potential
effects of leakage in the Fourier transform of the measured signal (). Fortunately, many
techniques exist to mitigate leakage effects, other than increasing measurement time. A
prominent category of techniques uses window functions, other than the rectangular window
() that we have used to model and demonstrate the effects of finite duration. Most, if not
all, of these window functions represent smooth curves which start and end at value zero.

    A large family of window functions is based on a raised cosine function. For instance, the
so-called Hann window reads:2

          11                       
    () =  (  ) ( 2 + 2 cos (2  ))                                                                  (8.5)

This function is shown in red in Figure 8.7 at left, for  = 2 s. It is based on a cosine with
a period of  seconds, cos(2  ), raised by adding 1 , and fit to the interval [0, 1] in terms of
                                                                            2
function value it is shown in blue in Figure 8.7. Only one period of the cosine is selected by

multiplication by the rectangular pulse in green (dotted). The Hann window, shown in red,

just like the rectangular window of Figure 8.1, limits the time duration of signal () to ,

through multiplication of () by (), (8.1).

    1.0                              w(t)                                   1.0                 W (f )

w(t)0.5                                                                     0.5
                                                                    W (f )

    0.0                                                                     0.0

         -3 -2 -1 0             1    2     3                                     -3 -2 -1 0  1  2       3

                       t [s]                                                     f [Hz]

Figure 8.7: Hann window () =  (  ) ( 1 + 1 cos(2  )) with duration  = 2 s in red at left, and corresponding
                              22            
Fourier transform () in red at right. At right, the black and gray dashed curves show, respectively, the first and

second part of (8.6).

    The essential difference with the rectangular window, is that the Hann window gradually
starts from zero and gradually returns to zero at the end, rather than an abrupt switch-on at
the beginning and switch-off at the end. In other words, the Hann window gradually `pushes'

2After the Austrian meteorologist J.F. von Hann (1839-1921).
8.5 Mitigating leakage: windowing                                                                                         91

        2  rectangular                                       2                   rectangular

           Hann                                    |W (f )|                      Hann

W (f )  1                                                    1

        0                                                    0

           -3 -2 -1 0           1  2  3                         -3 -2 -1 0    1  2                                        3

                        f [Hz]                                        f [Hz]

                                         |W (f )|  102

                                                                                                             rectangular

                                                   100 Hann

                                                   10-2

                                                   10-4

                                                                -3 -2 -1 0    1  2                                        3

                                                                      f [Hz]

Figure 8.8: Fourier transform () of rectangular window, (5.11), in blue and of Hann window, (8.6), in red (left)
window duration  = 2 s. Magnitude |()| (right) on a linear (top) and logarithmic (bottom) magnitude scale.

the beginning and end values of the windowed signal () to zero at  = ± 2 . Only then we
compute the Fourier transform of ().

    Figure 8.7 at right shows, in red, the corresponding Fourier transform ():

                                () 1        1                   1
           () = sinc()  ( 2 + 4 (( +  ) + ( -  )))

                                         1                         1
           = 2 sinc() + 4 (sinc(( +  )) + sinc(( -  )))                                (8.6)

found through applying the multiplication theorem, (6.9). The first sinc function,  sinc(),

                                                                                                                                                      2

and the sum of the latter two sinc functions are shown in, respectively, black and gray dashed
curves in Figure 8.7 at right. Since () is a real, even function of time, () is a real, even
function of frequency.

    Figure 8.8 compares the rectangular window and Hann window in the frequency domain,
showing () (left) and |()| (right) on linear (top) and logarithmic (bottom) magnitude
scales. The Hann window has a larger width of the main or central peak, twice as wide as
that of the rectangular window, but the level of the side peaks is (much) smaller. Practically,
the Hann window will introduce more spreading and smoothing than the rectangular window,
but it will introduce smaller `wobbles' in the spectral representation of the windowed signal.
This is illustrated in Figure 8.9, where we extend the discussion of Section 8.2 in two ways.

    First, we show the effects of using the Hann window (in green) versus the rectangular
window (in red). It is clear that the Hann window yields a broader transform () (i.e., more
smoothing) without much scalloping (i.e., fewer fluctuations). Second, we show the effects
of extending the window duration  from 1.5 s (top row) to 3.0 s (bottom row). Clearly, the
peaks of the Fourier transforms become higher, the sinc functions narrower, and, in case of
a rectangular window, more scalloping occurs. Longer windows will result in even narrower
and higher sinc functions, approximating Dirac delta functions when   , when ultimately
there will be no leakage.
92                          8. Finite signal duration, leakage and windowing

          x(t)   xw (t)  xw(t) (Hann)              X(f )                Xw(f )  Xw(f ) (Hann)

      1                                       1.0

x(t)  0                                X(f )                      0.75

                                              0.5

      -1                                      0.0

          -2 -1  0       1  2                      -4 -2                0       2  4

                 t [s]                                                  f [Hz]

      1                                       1.5  1.50

x(t)                                   X(f )  1.0

      0                                       0.5

      -1                                      0.0

          -2 -1  0       1  2                      -4 -2                0       2  4

                 t [s]                                                  f [Hz]

Figure 8.9: At left: signal () =  cos(20) with 0 = 1 Hz and  = 1, in blue, and windowed signal ()
(rectangular window: red, Hann window: green) with  = 1.5 s (top) and  = 3 s (bottom). At right, the

corresponding Fourier transforms () (blue) and () (rectangular window: red, Hann window: green).

    The Hann window is just one member of a family of raised cosine window functions. Many
alternative window functions to mitigate effects of leakage have been developed. There is
no window function that best serves all applications it generally comes down to a trade-off
between smoothing and scalloping.

    As a final note: when taking a measurement of some phenomenon in practice, the effects
of the rectangular time window, predominantly leakage, always occur. The rectangular window
represents our `model' of performing a finite-duration measurement it is always there. To
mitigate the effects of leakage, some opportunities exist, like the Hann window discussed
here, but their use is optional. In short, whereas the rectangular window is inevitable for any
practical measurement, applying additional time-windowing techniques is a choice.

    This chapter concludes Part II, which covered the analysis of continuous-time signals and
their representations in the frequency domain. The techniques discussed in this part form
the theoretical core of signal analysis, but need to be reconsidered before performing signal
analysis in real life. In any modern application of signal analysis, we work with algorithms
on a computer, most notably the Discrete Fourier Transform (DFT). Before applying these
algorithms, the continuous-time signals must be `put into the computer' first, a process referred
to as sampling and quantization, see Section 2.1.2. Part III of this book will focus on the
analysis of discrete-time signals and their representations in the frequency domain.
   III

        Discrete time

93
            9

                                        Sampling

In Chapter 5 we considered aperiodic, continuous-time signals (), of infinite duration, and
transformed them to the frequency domain by means of the Fourier transform. In practice, we
do not work with continuous-time signals but instead with discrete-time signals: signal ()
has been sampled, with sample frequency  Hz. In addition, measurements have a limited
duration  = , yielding a finite sequence of  samples in a computer, . A key question
then arises: in what way does the spectrum of these samples , calculated by the computer,
differ from the spectrum () of the original, infinite-duration, continuous-time signal ()?

    This question will be answered in three steps. First, in this chapter and Chapter 10,
we assume that we have an infinite-duration measurement and will, respectively, discuss
the effects of sampling and its reverse process, signal reconstruction, using a continuous-
time model. Second, in Chapter 11 we introduce the Fourier transform for infinite-duration,
discrete-time sampled signals, the Discrete-Time Fourier Transform (DTFT). In the third step,
Chapter 12, we discuss the effects of limited observation time, and introduce the corresponding
Fourier transform, the Discrete Fourier Transform (DFT).

9.1 Sampling

Consider Figure 9.1, which shows an infinite-duration continuous-time signal (), with Fourier
transform () (the latter not shown). All signal analysis theory and tools discussed in this
book so far apply to this category of signals. While in the past analog electrical circuits were
used to analyze a continuous-time signal in the frequency domain, nowadays computers are
used to process a discrete-time, sampled version of that signal.

    On a computer, all that we have is a sequence of numbers , with  the index of that
number. Computers are agnostic when it comes to the time a number `belongs to': time is lost.
As a consequence, they are also agnostic when it comes to the frequency of an oscillation:
frequency is lost. Time and frequency disappear altogether, and it is our job to restore time
and frequency at some point, e.g., when plotting a signal or its spectrum, or when performing
calculations on (or transformations of) signals on a computer.

    We define sampling to be performed at a fixed rate, the sampling frequency  in Hz. A
sample is then obtained every sampling interval  = 1 s. Since we assume infinite-time

                                                                                                       

duration signals (), we get an infinite number of samples , where   . While this is
not realistic, this assumption will be useful to obtain crucial insights about what happens when
you sample a signal. In Chapter 12 we abandon this assumption and consider the practical
case: a limited-duration signal, working with a limited number of samples.

                                                            95
96                                                                                                        9. Sampling

          3   t                                                                                                 t

              -                                        x(6t)                                                    

x(t)      0

                                         t

          -3

              -3 -2 -1          0        1          2                   3  4                  5  6        7        8

                                                       t [s]

          3   n                                                                                                 n

              -                                        x6                                                       

xn        0
                                                                    x7

          -3

                -9 -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24

                                                                n

Figure 9.1: Top: continuous-time signal (), of infinite duration,   . Bottom: the samples obtained from (),
with sampling interval , producing an infinite sequence of numbers  with   .

          1 f0 = 1 Hz                                                   1 f0 = 11 Hz
                                                                        0
x(t), xn  0                                            y(t), yn

          -1 n =          23    456         x(t)                        -1              23       456      78       9 10

                      01  0.25    0.50   7 8 9 10                                   01  0.25       0.50   0.75       1.00
                                  t [s]                                                            t [s]
               0.00                      0.75 1.00                           0.00

Figure 9.2: Left: continuous-time signal (), a cosine with frequency 0 = 1 Hz, sampled with  = 10 Hz,
yielding . Right: signal (), a cosine with frequency 0 = 11 Hz, also sampled with  = 10 Hz, yielding .
The horizontal axis shows time  in seconds, and index .

    At each moment  a sample is taken, i.e., at each sample time  = , the value of the
sample  equals the value of the continuous-time signal at that time:

           = ( = ) ,                                                                                               (9.1)

The effects of signal quantization (for which, see Section 2.1 and Appendix D) are ignored. As

illustrated in Figure 9.1, sample 6 equals (6), et cetera. From  we move to +1, and
in the same way from 6 to 7. In between the samples, the sampled discrete-time signal is
undefined. When having two samples on the computer, we do not know what the continuous-

time signal looks like in between the samples. Information is lost an inevitable consequence

of sampling. However, when signal () `behaves well' in between the samples - for instance
it does not oscillate rapidly or show a sudden peak - one could argue that its samples 
represent signal () `well enough' to analyze it using the samples.

    Figure 9.2 illustrates a `well-sampled' signal (left), and the ambiguity that results from
9.2 Impulse train sampling model                                                                        97

1.0                                                                        1.0

x(t)0.5                                                                    0.5              X(f ) = 0
                                                                    X(f )
                                                                                               |f |  1

0.0                                                                        0.0

     -3 -2 -1 0                   1  2  3                                       -6 -4 -2 0  2  4        6

     t [s]                                                                      f [Hz]

Figure 9.3: Aperiodic continuous-time signal () = sinc2() at left, and its Fourier transform () = () at
right. As signal () is even, the Fourier transform () is even and real-valued.

sampling a signal too slowly (right). From the samples  we might conclude that () = (),
since we cannot distinguish between () and (). Signal () is therefore called an alias of
(). In fact, an infinite number of signals exist (e.g., a cosine with frequency 0 = 9 or 21 Hz)
which, when improperly sampled, yield exactly the same sample values. Clearly, this must be
prevented, and it will be shown that we must sample `fast enough' to obtain a sequence of
samples that truly represent the signal we intend to analyze, and not an alias. With infinite
observation times, the requirement is that we can perfectly reconstruct a continuous-time
signal from its samples. This leads to the sampling theorem, discussed in Section 9.4. When
adhering to this theorem, no information is lost, a remarkable fact given that we do not know
the signal in between the samples. Signal reconstruction is the topic of Chapter 10.

    Throughout this and the following four chapters, we take signal () = sinc2() as an
example. Its Fourier transform () equals (), (7.7) (here  = 1) both are shown in
Figure 9.3. We deliberately use an even signal as an example, because then () is real-
valued, and we only need to show the real part of the Fourier transform. Of course, in general
() will not be even, and the Fourier transform () is complex-valued. Another useful
property of this signal, as we will see in Section 9.4, is that it is band-limited: its Fourier
transform equals 0 for frequencies greater than or equal to 1 Hz see Figure 9.3 at right.
band-limited signals are constrained in how rapidly they change in (an interval of) time, a
signal property that will play a crucial role.

    In the next section, we first develop a mathematical model for the sampling process, the
impulse train sampling model. This model enables us to obtain and understand the spectrum
of a sampled signal, while using the (continuous-time) Fourier transform.

9.2 Impulse train sampling model

A mathematical model for the sampling process is developed, as an ideal abstraction of reality:
the impulse train sampling model. We continue to work in continuous time, and construct a
conceptual signal (), which preserves the information in () at just the sample instants.
Considering signal () only at discrete, equitemporal time instants,  =  is materialized
through multiplication of that signal by the impulse train sampling function ():

        () = ()()                                                                                      (9.2)
                                                                                                       (9.3)
where:

                                

        () =   ( - )

                             =-
98                                                                                                     9. Sampling

                1.0
                                                                         - 2 2 t t

p(t)            0.5                                                                                    t

                0.0

                     -3  -2  -1  0                                                     1            2        3

                                 t [s]

Figure 9.4: Signal () =  =-  ( - ) is the impulse train, here  = 13 s.

fs xs(t), x(t)  1.0 x(t)                                                                  fs xs(t)
                0.5

                0.0  -9 -8 -7 -6 -5 -4 -3 -2 -1 0  1                                2  3  4  5      6     7  89

                     -3  -2  -1  0                                                     1            2        3

                                 t [s]

Figure 9.5: Signal () = sinc2() (dotted) and the model of the samples, () multiplied by  (stems, and zero
otherwise) sampling frequency  = 1 = 3 Hz.

with   . The impulse train, also known as a `Dirac comb', is a continuous-time, periodic
function with a Dirac delta impulse every  s its period is . It is zero at all times, except
at infinitesimal moments of time which coincide with the sampling times , see Figure 9.4.

    Signal () does not exist in real life but acts as a convenient `model of the samples', and
plays a crucial role in this and the following chapter. It is a continuous-time signal, and has a
value at all times   . In between the time of the samples, it equals zero. At the time of a
sample  = , the weight of the Dirac pulse at that time equals  times the value of ()
at that sample time, (). Because the height of the Dirac impulse itself at that time has
infinite value, an interpretation of (9.2), e.g., in terms of signal value, is troublesome.

    We used the Dirac function many times before in order to `put a number at a frequency',
for instance in the Fourier transform-in-the-limit of a sinusoidal signal, (5.16). In (), we
use a Dirac comb to `put' the sample values () at the right times, while for all other times
the signal is zero. Note that continuous-time signal () is not the same as the discrete-time
sequence of samples, , used in Chapters 11 and 12.

    The scaling with  in (9.3) may seem arbitrary. Other textbooks on signal analysis typically
do not include it in their impulse train sampling model, e.g. [3, 7, 8]. For multiple reasons, we
introduce the scaling here. As a start, recall that the Dirac delta impulse () has dimension
of one-over-time, see Appendix B. This means that through multiplication by  in (9.2) we
ensure that () and () share the same unit, [unit].

    Scaling () with  has many other advantages, but one disadvantage: when plotting
() and () together in one figure, () would, since  is typically (much) smaller than
1, lie well below (). To avoid this, we plot () in all figures showing (). For
our example, the resulting uniformly-sampled signal is shown in Figure 9.5, with the stems
representing () as the weights of the Dirac pulses.
9.3 Derivation of Fourier transform of sampled signal                    99

9.3 Derivation of Fourier transform of sampled signal

Because () is a continuous-time signal, we can compute its Fourier transform, (). We
first find the Fourier series of the periodic impulse train function (), and then develop an

expression for (). The impulse train function () is periodic with period  and its funda-
mental frequency equals  (instead of symbol 0 for the period and 0 for the fundamental
frequency, respectively, as used in Chapter 3). With (4.2) the complex exponential Fourier

series of the impulse train function () reads:

      =

() =       2                                                             (9.4)

            

      =-

with Fourier coefficients (4.3), for   :

                                               
 = 1  ()-2 d = 1  2 ()-2 d
                                        -                                (9.5)

                                               2

We chose the integration over one period  to be computed symmetrically about  = 0, see
Figure 9.4. In this interval only the Dirac delta at  = 0 (scaled by ) occurs:

      1                                        1 -20 = 1  

 =            2  ()-2 d                =                                 (9.6)

         
       -                                       
          2

With  = 1  , the impulse train function (), (9.4), can be expressed as:

              =                                                          (9.7)

() =  2

            =-

Substituting (9.7) into (9.2) yields:

                                            =       =                    (9.8)

() = ()() = ()  2                              =  ()2

                                           =-      =-

Fourier-transforming this equation results in:

                                                  

() =  {()} =  {  ()2} =   {()2}                                          (9.9)

                 =-                               =-

because the Fourier transform is a linear operator, (6.1). Using the frequency translation
property of the Fourier transform, (6.6), we obtain our final result:

                                                                         (9.10)

() =  ( - )

               =-

The Fourier transform () is a continuous function of frequency , with   . It is periodic
with period : ( + ) = ()    . Apparently, the result of impulse train sampling
a signal in time with sampling frequency  is that the Fourier transform of that signal is copied
to all integer multiples of that sampling frequency  with   .
100                                                                                9. Sampling

Xs(f )  1.0

                                                      - 2 2 fs fs

        0.5

        0.0 · · ·  -3fs       -2fs     -1fs     0fs                   1fs  2fs     3fs · · ·
           -12     -10   -8 -6
                                       -4 -2 0                     2  4    6    8  10 12

                                                f [Hz]

Figure 9.6: Fourier transform () of (). The sampling frequency  = 1 = 3 Hz.

    Returning to our example signal, the Fourier transform () of () is shown in Figure 9.6.

Compare this figure with the Fourier transform of the original continuous-time signal (),
(), Figure 9.3 at right. Clearly, for  = 0, (9.10) or, equivalently, between  = -  and

                                                                                                                                                              2

 = 2 , () equals the Fourier transform () of the original continuous-time signal ().
In addition, copies of () appear at all integer multiples of the sampling frequency, that is,

at  with  = -, ... , -1, 0, 1, 2, 3, ... , . This represents the ambiguity we discussed before,
with Figure 9.2.

Apparently, a consequence of sampling is that the infinite frequency axis is cut into equal

bands of frequency, all with a width of , where copies of the original spectrum () are
positioned. In Chapter 11 we analyze with more mathematical rigor why the Fourier transform

of a sampled signal repeats itself. In later chapters, by default we consider only one `period'

of the spectrum () of a sampled signal, either from [0, ) or from [- 2 , 2 ).

Scaling the Dirac comb in (9.3) with , as we do, avoids scaling of (). As mentioned

above, many other textbooks do not scale the Dirac comb with , but then (because 
would equal 1   in (9.6)) their expression of () is scaled by 1 . Again, a big advantage
of scaling the Dirac comb by  is that the spectrum of the samples, (), when properly
sampled, will equal () for -      .
                                    2        2

9.4 Sampling theorem

The result of sampling a continuous-time signal () is that its Fourier transform () gets

`copied' in the frequency domain. An infinite number of copies is created, centered at integer

multiples of the sampling frequency . Each copy is assigned a band of frequencies, with a
width of . But, what would happen when () is broader than this band, that is, when ()
is non-zero beyond ±  ?

                                        2

    Sampling faster, with higher , will assign larger frequency bands to each copy the copies
are positioned further apart, Figure 9.7. For all sampling frequencies, the copy put at 0 Hz,

will maintain its position, at 0. When changing the sampling frequency, all other copies will
move. Sampling slower, with lower , will assign smaller frequency bands to each copy they
are positioned closer together. When sampling too slowly, the copies will overlap, Figure 9.8.

Within each frequency band the energy in the copies will merge, and the Fourier transform

() of the sampled signal () will be different from the original Fourier transform (). This
is known as aliasing, and should be prevented. When it occurs, the original Fourier transform

() cannot be reconstructed from the Fourier transform of the sampled signal () . Instead
of reconstructing the original signal from the sampled signal, an alias is obtained, leading to

false conclusions about the measured signal.
9.4 Sampling theorem                                                           101

        1.0                          - 2fs                 fs

Xs(f )  0.5                    -6 -4                        2

                                                    fh           fs - fh

        0.0 · · ·        -1fs               0fs                           1fs  ···
           -12     -10 -8
                                            -2 0        2           4  6  8    10 12

                                            f [Hz]

Figure 9.7: Fourier transform () of () sampling frequency  = 8 Hz.

        1.0

Xs(f )  0.5

        0.0 · · · · · ·

        -12 -10 -8 -6 -4 -2 0                           2           4  6  8    10 12

                                            f [Hz]

Figure 9.8: Fourier transform () of () sampling frequency  = 1.5 Hz. The individual copies, occurring at
integer multiples of 1.5 Hz, are shown using dashed lines in blue. To prevent clutter, no further annotations are

shown in the figure.

    In our example, the highest frequency present in the signal, referred to as , is  = 1 Hz,
see Figure 9.7. From this figure, we can determine for what sampling frequency  the copy
positioned at 0 will `touch' the copy positioned at 1. This happens when:

         =  -    = 2                                                           (9.11)

No overlap occurs when  > 2, which leads to the sampling theorem:

    A band-limited signal (), having no frequency components beyond , i.e., ( > ) = 0,
is completely specified by its samples, when sampled at a uniform rate larger than 2. In
other words, the sampling frequency  should meet:

         > 2                                                                   (9.12)

with  the largest frequency occurring in the Fourier transform () of signal (). The value
2 is a characteristic of the signal to be sampled and is referred to as the Nyquist rate.1
Another, equivalent way to phrase requirement (9.12) is:

         >                                                                     (9.13)
        2

The value  is a characteristic of the sampling system and is referred to as the Nyquist

                     2

frequency. Whereas the Nyquist rate is a property of the signal, the Nyquist frequency is a

1This inequality, and also the sampling theorem itself, is named after the Swedish-American physicist and electronic
 engineer H.T. Nyquist (1889-1976), though be attributed also to British mathematician Sir E.T. Whittaker (1873-
 1956) and American mathematician C.E. Shannon (1916-2001).
        102                                          9. Sampling

        property of the sampler. As will become clear in the next chapter on signal reconstruction, the

        band of frequencies between -  and  , also referred to as `from negative Nyquist to positive
                             2  2
        Nyquist', will play a prominent role. Although in principle one could use any copy in ()

        to reconstruct a signal from the sampled signal (and account for the frequency translation of

        that copy relative to  = 0), typically the copy at  = 0 is used. This is also why this region
        is indicated using the vertical red dashed lines in Figures 9.6-9.8.

        Note that when (  ) = 0, i.e., () is zero at , the sampling theorem becomes:

               2 or, equivalently,                   (9.14)
                                                  2

        In our example, with  = 1 Hz, we could sample the signal () with sampling frequency 
        equal to 2 Hz, and still be able to perfectly reconstruct it from the sampled signal.

            Hence, two cases exist for setting the minimum required sampling frequency:

            · When ( = )  0: the minimum sampling frequency is strictly larger than 2

            · When ( = ) = 0: the minimum sampling frequency is equal to, or larger than 2.

        To decide on the choice for , we have to calculate (beforehand) the Fourier transform of the
        signal to be sampled, which in practice we may not know. In practice, we typically sample at
        (much) higher frequencies than the Nyquist rate, i.e., over-sampling.

            When the above requirement, stated as either (9.12) or (9.13), is not met, we are under-
        sampling the signal and aliasing will occur, as demonstrated in the next section.

        9.5 Aliasing

        In this section the effects of sampling, including aliasing, are shown using two examples in
        which we aim to sample a plain cosine signal:

             () = 2 cos(20)                          (9.15)

        with 0 = 11 Hz, and Fourier transform-in-the-limit () = ( + 11) + ( - 11), consisting of
        two Dirac pulses. Signal () is chosen to be even, such that its Fourier transform is real and

        even, and we can just show the real part. The highest frequency of the Fourier transform of

        this signal, , equals 11 Hz, and at this frequency we have a Dirac pulse, i.e., ( = )  0.
        We must apply a sampling frequency that is strictly higher than 2:  > 22 Hz.

EX 9.1  Sample signal () of (9.15) with  = 30 Hz. Show the Fourier transforms (), ()
        and () of, respectively, the original signal (), the sampled signal () and the
        reconstructed signal (). Assume that we use the band from  = - 2 to 2 Hz of
        () for ().

        Solution Figure 9.9 illustrates all Fourier transforms. The top image shows (), two
        Dirac pulses at ±11 Hz, with a weight equal to 1. When sampling () with  = 30 Hz,
        the Fourier transform of the sampled signal, (), will consist of copies of () at all
        integer multiples of , i.e. at 30 Hz, with   , see (9.10).
9.5 Aliasing                                                                       103

X(f )   1.0                     -11                       11
        0.5
        0.0                        -10               10

              -30          -20              0                     20  30
                                        copy at 0fs
             copy at -1fs                                             copy at 1fs

Xs(f )  1.0                - -15                              15

        0.5 -19 -11                                      11       19
        0.0

              -30          -20    -10   0            10           20  30

                                         used to
                                        reconstruct

Xr(f )  1.0
        0.5 -11 11
        0.0

              -30          -20    -10   0            10           20  30

                                        f [Hz]

Figure 9.9: Fourier transforms of original signal, () (top), sampled signal () (middle) and recon-
structed signal () (bottom). The original signal is a zero-phase cosine with amplitude 2 and frequency
0 = 11 Hz. The signal is sampled with  = 30 Hz.

In the second row of Figure 9.9, the first three copies are shown. The first copy is
positioned at  = 0 (for  = 0), with two Dirac pulses at ±11 Hz (blue). One copy
is positioned at  = 1 ( = 1), yielding Dirac pulses at 30 - 11 = 19 Hz (green) and
30 + 11 = 41 Hz (not shown). One copy is positioned at  = -1 ( = -1), yielding
Dirac pulses at -30 - 11 = -41 Hz (not shown) and -30 + 11 = -19 Hz (green).

    An infinite number of other copies occur, which are not shown, as the frequency axis
runs here from  = -35 to 35 Hz. For instance, the `next' copy at positive frequencies
would be placed at 2 = 60 Hz (for  = 2), resulting in Dirac pulses at 60 - 11 = 49 Hz
and at 60 + 11 = 71 Hz.

     In the bottom image we used () between ± 2 = ±15 Hz to reconstruct the
signal. What results is (), which is identical to () shown at the top. We have
sampled the signal properly, and so it can be perfectly reconstructed from its samples.
Signal reconstruction will be discussed in more detail in Chapter 10.

Repeat Example 9.1, but use sampling frequency  = 10 Hz. Explain the result.                            EX 9.2

Solution According to the sampling theorem, (9.12), the sampling frequency is too
low, and aliasing will occur. Figure 9.10 shows all Fourier transforms. Again, the top
image shows (), with two Dirac pulses at ±11 Hz, with weight 1. The second image,
for the sake of explanation, shows an intermediate result to the Fourier transform,
namely the Fourier transform of the sampled signal when copying () to  = 0
( = 0, blue), to  = 1 = 10 Hz ( = 1, green) and to -1 = -10 Hz ( = -1,
green). You will see that Dirac pulses appear at, respectively, ±11 Hz, at 10 - 11 = -1
and 10 + 11 = 21 Hz, and at -10 - 11 = -21 and -10 + 11 = 1 Hz.
104                                                             9. Sampling

     The third image shows the complete Fourier transform (), for the range of
frequencies in the figure, with different colors to distinguish between the copies (purple:

copies at ±2 = ±20 Hz ( = ±2), yellow: copies at ±3 = ±30 Hz ( = ±3), black:
copies at ±4 = ±40 Hz ( = ±4)) and the numbers above the Dirac pulse show the
`integer' of the integer multiple  in .

     X(f )   1.0

             0.5
             0.0 -11 11

                  -30  -20  -10  0            10      20          30

     Xs(f )  1.0 -1 0 1 -1 0                               1      30

             0.5 -5 5                                       21     42
                                                                29 31
             0.0 -21 -11 -1 1 11                      20
                                                                  30
                  -30  -20  -10  0            10       3
                                                                  30
     Xs(f )  1.0 -2 -4 -3 -2 2                      19 21
             0.5 -5 5
             0.0 -31 -29 -21 -19 -11 -9 -1 1  9 11    20

                  -30  -20  -10  0            10      20

     Xr(f )  1.0

             0.5
             0.0 -1 1

                  -30  -20  -10  0            10

                                 f [Hz]

Figure 9.10: Fourier transforms of original signal, () (top), sampled signal () (second and third
images) and reconstructed signal () (bottom). The original signal is a zero-phase cosine with amplitude
2 and frequency 0 = 11 Hz. The signal is sampled with  = 10 Hz.

On the bottom image we used () between ± 2 = ±5 Hz to reconstruct the signal.
What results is (), which is clearly different from () in the top row. We obtain
a zero-phase cosine with amplitude 2 and frequency 1 Hz, i.e., the situation shown in
Figure 9.2 (though with unit amplitude). We did not sample the signal properly, and so,
when we reconstruct it from the samples, we obtain instead an alias, not the original
signal.

    In this example we know the original continuous-time signal and its Fourier trans-
form, but in practice we typically do not, and therefore drawing conclusions based on
() could lead to serious mistakes.

    In these examples, and also in the rationale leading to the sampling theorem, we already

alluded to the possibility that, after obtaining a sequence of samples  of continuous-time
signal (), at some point it may be important to perform the inverse operation. This is known
as signal reconstruction and will be discussed in more detail in the next chapter.
                             10

                         Signal reconstruction

In Chapter 9 we discussed how a continuous-time signal (), for infinite measurement dura-
tion, can be sampled, ultimately yielding a discrete-time sequence  in a computer. In this
chapter, the reverse process, called signal reconstruction, is discussed. That is, is it possible
to reconstruct the original continuous-time signal () from the sampled signal? This may
seem an impossible task, as we do not know the value of () in between sample times. How-
ever, when we assume that the signal is band-limited and has been sampled with a sampling
frequency  according to the sampling theorem (Section 9.4) it is shown that the signal can
indeed be perfectly reconstructed from the sampled signal. This ideal signal reconstruction
is a limiting case in theory, and cannot be applied in real-time applications. Hence, several
other signal reconstruction procedures have been developed which can be used in real time,
such as the Zero-Order Hold (ZOH) technique.

10.1 Ideal signal reconstruction

The transformation of a sampled signal back to a continuous-time signal is called signal recon-

struction. In the following, we assume that a continuous-time signal () has been sampled

with a sufficiently high sampling frequency , such that no aliasing occurs. We will work with
the continuous-time model of our samples, (), to explain the principles, and continue with
example signal () = sinc2() from Figure 8.2, as it has a convenient (real and even) Fourier

transform. The reconstructed signal is referred to as () and its Fourier transform ().
    Figure 10.1 (top) illustrates the Fourier transform () of (), showing the (infinite)

repetition of the Fourier transform () at integer multiples of the sampling frequency ,
where each copy lies in a band of frequencies with a width of . A straightforward way to
obtain the desired () from () is to multiply (), (9.10), by a unit-amplitude pulse
() with width :

       () = ()()                                               (10.1)

with:

                                                               (10.2)
       () =  (  )

This operation is known as filtering and will be discussed in detail in Chapter 18. In fact, ()

is a so-called ideal filter: it perfectly passes a part of the spectrum (here, all frequencies be-

tween -  and +  ) and perfectly `blocks' all other parts, see Figure 10.1 (middle). Expressed
       2  2
as in (10.2), () is known as the ideal reconstruction filter.

                         105
106                                                         10. Signal reconstruction

Xs(f )         1.0

               0.5                     -fs/2          fs/2

               0.0 · · · -2fs    -1fs           0fs         1fs  2fs · · ·

                    -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7
                                                                           f [Hz]

Hr(f )         1.0
               0.5
               0.0

                       -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7
                                                                              f [Hz]

X(f ), Xr(f )  1.0
               0.5
               0.0

                       -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7
                                                                              f [Hz]

Figure 10.1: Top: Fourier transform of (), with sampling frequency  = 3 Hz. Middle: Ideal reconstruction
filter (). Bottom: Fourier transform (), (10.1), of the reconstructed signal () (green, dashed) and the
original, () (blue), with () = sinc2(). Note that all signal transforms shown here are real and even.

    Using the ideal reconstruction filter, for infinite signal duration, the original Fourier trans-
form () is reconstructed perfectly, see Figure 10.1 (bottom):

                                                                 (10.3)
               () =  (  )  ( - ) = ()

                                           =-

Because the Fourier transform () of the reconstructed signal () is identical to the Fourier
transform () of (), the reconstructed time signal () is identical to the original ().
Whereas in the frequency domain this is a trivial result, it is perhaps less trivial in order to

understand what happens in the time domain. After all, we only have the sampled signal and

do not know the values of () in between the samples.

    Recall that a multiplication of the Fourier transforms of two signals in the frequency domain

is equivalent to Fourier-transforming a convolution of these two signals in the time domain,

(6.8). Generally we try to avoid working with convolution integrals with one exception, and

that is when one of the signals being convolved is a (sum of) Dirac delta function(s), which is

the case here, as the model of our samples () is an infinite sum of Dirac functions, (9.2).
    Therefore, since () = ()(), then () = ()  (). We can obtain ()

through inverse Fourier-transforming ():

               () = -1{ (  )} = sinc() = 1 sinc (  )             (10.4)
                                                

This function is the so-called impulse response function of the ideal filter, and will be discussed

in more detail in Chapter 18. Figure 10.2 (at left) illustrates that (0) =  and that () has
its zero-crossings at all integer multiples of , except at  = 0:

               () = 0 for  = ,     {0}                           (10.5)
10.1 Ideal signal reconstruction                                                                                107

3                                                                                                     fs

2 fs = 1 t t                                                              1.0

1
hr (t)                                                                    0.5      -fs/2                  fs/2
                                                                  Hr(f )

0                                                                         0.0
                                      2t

-3 -2 -1 0      1                           2  3                               -5 -4 -3 -2 -1 0 1 2 3 4 5

       t [s]                                                                       f [Hz]

Figure 10.2: Left: impulse response of ideal filter () right: ideal filter () (sampling frequency  = 3 Hz).

Convolving () with () yields:

                                   -                                                                            (10.6)
() = ()  () =  ()sinc (  )

                                          =-

which is a summation of all samples () multiplied by the function  () located at the
position  =  of each individual sample on the continuous-time axis.

Proof                            1 

       With () = sinc ( ) and () = ()  ( - ) we obtain:
                                                                               =-

                1 
() =  sinc (  )  (()  ( - ))

                                                                =-

        - 
       =  (sinc (  ) ()  ( - )) d
       =-                                      =-

                                            -

       =   sinc (  ) ()( - ) d
       =- =-

                - 
       =  ()sinc (  )

              =-

Here we changed the order of integration and summation in the third step, and used the
sifting property of the Dirac delta function in the final step.

    Figures 10.3 and 10.4 illustrate the process of building () using an infinite summation
of sinc functions, (10.6). Figure 10.3 shows the multiplication of sample value () = ,
(9.1), by the sinc function for five individual samples,  = 0, ... , 4. Starting with the sample

at  = 0 (top), the sinc function (green curve) is scaled by the value of the sample at  = 0,
0. We obtain a continuous-time function which at the time of this sample,  = 0, exactly
equals the value of this sample, 0. In addition, because its zero-crossings occur at all integer
multiples of , (10.5), this sinc function will not affect the value of the reconstructed signal

at any other sample time. Repeating this procedure for the samples,  = 1, ... , 4, shown in the
other plots, we obtain a scaled sinc function at each sample time, with its maximum exactly

equal to the value of that sample, not affecting the value at any other sample time.

    Figure 10.4 illustrates, as part of the summation (10.6), the summation of the sinc functions

obtained for samples  = -1,  = 0 and  = 1 (top), starting from the left and assuming that
108                                                                                         10. Signal reconstruction

    1.0                                                                  x0                         multiplied by x0

xn  0.5          x0sinc( t-0 t )

    0.0 n = -8 -7 -6 -5 -4 -3 -2 -1 0 1                                             2      3     4  5      6       7      8

         -3      -2                              -1         0                            1                 2                  3

                                                            t [s]

    1.0                                                  n = 1 1.0                                                    n=2
                                          x1
xn                                                                   0.5
    0.5                                                                                                        x2

    0.0                                                              0.0

         -3 -2 -1 0                           1          2  3 -3 -2 -1 0                                      1       2       3

                         t [s]                                                                      t [s]

    1.0 n = 3 1.0                                                                                                        n=4
                                                                                                                   x4
xn  0.5                                                     0.5

    0.0                                       x3            0.0

         -3 -2 -1 0                           1          2  3 -3 -2 -1 0                                      1       2       3

                         t [s]                                                                      t [s]

Figure 10.3: Multiplication of the shifted sinc function (), (10.4), times  (green curve) by sample values 
(blue stems), for  = 0, ... , 4, to construct (), (10.6). Only the top plot shows indices  these are absent in
the other plots to prevent clutter.

     1.0                                         1.0                         ADD            1.0                       ADD

                     multiplied                                              multiplied                               multiplied

     0.5             by                          0.5                         by             0.5                       by

xn                   x-1                                                     x0                                       x1

     0.0                                         0.0                                        0.0

              -2 0          2                               -2 0                 2                  -2 0                  2
     1.0
xn                                            adding        x(t), xr(t)  1.0
     0.5                                      all green                           yields
                                              functions
                                                                         0.5

     0.0                                                                 0.0

             -2          0                            2                                -2           0                 2

                     t [s]                                                                          t [s]

Figure 10.4: Top: multiplication of shifted sinc function  () by -1 (left), then adding to that function the
multiplication of sinc function  () by 0 (center), then adding to that function the multiplication of shifted
sinc function  () by 1 (right). Bottom left: result of multiplication of shifted sinc functions  () by all
individual samples. Bottom right: adding all green functions in the bottom left figure yields (), (10.6), (dashed
green), the perfectly reconstructed () (blue). Note that the indices  are not shown to prevent clutter.
10.2 Zero-order hold reconstruction                                                                                109

the contribution of all samples before  = -1 is small (which is valid in this case). We see
that adding the sinc functions belonging to just these three samples (green curves) already
approximates this particular continuous-time function () quite well. The final result, obtained
when adding the convolutions for all samples (bottom left), for infinite signal duration, is shown
at the bottom right of Figure 10.4, where, ultimately, the original continuous-time signal ()
is obtained. Through (10.6), () has been perfectly reconstructed from the sampled signal:
() = ().

Recall Figure 9.8, which shows the Fourier transform () of the sampled version of                                       EX 10.1
signal () = sinc2() when sampling too slowly,  = 32 Hz < 2 Hz hence aliasing
occurs. Use the ideal reconstruction filter to reconstruct the signal, leading to ().
What does () look like?

Solution Multiplication of () in Figure 9.8 by () leads to () shown at right
in Figure 10.5. This real and even function can be described as:

     1 1
() = 2  ( 3 ) + 2  ( 1 )                                                                                      (10.7)

     2                               2

Inverse Fourier-transforming (10.7) results in ():

() = 3 sinc( 3 ) + 1 sinc2( 1 )                                                                               (10.8)
     4 24                               2

which is, clearly, not equal to the original signal () = sinc2(). We obtain an alias

of (), (), the samples of which are exactly equal to the samples of (). The
time-domain signal () and its Fourier transform () are illustrated in Figure 10.5.

1.0                                                                                1.0
x(t), xn, xr(t)
                                                                    X(f ), Xr(f )  0.5                Xr(f )
                                                                                               X(f )
0.5                                           xr (t)
               x(t)                                                                0.0
                                     1234
0.0                                                                                -4 -3 -2 -1 0 1 2 3 4
                                       123                                                                 f [Hz]
             -4 -3 -2 -1 0

    -3 -2 -1 0
                              t [s]

Figure 10.5: Left: () (gray), sampled with  = 1.5 Hz (blue), and reconstructed signal () (green).
Right: Fourier transforms () of () = sinc2() (gray), and () of reconstructed signal () (blue).

10.2 Zero-order hold reconstruction

Given that perfect signal reconstruction is possible, we may ask: is there a need for alternative
methods? The answer is yes, and lies in the fact that an ideal filter is a non-causal filter,
Chapter 16. For now, it is sufficient to know that non-causal filters need past, present and
future values of a signal to work properly. In practical, real-time applications we only know
the current and past values of a signal and need to apply a causal filter.

    The most elementary (and obvious) way to reconstruct a continuous-time signal from past
and present samples in practice is to hold the present sample value  up until the next sample,
110                                                    10. Signal reconstruction

+1, becomes available, also known as sample-and-hold:

       () =  for    < ( + 1)                           (10.9)

This process is known as Zero-Order Hold (ZOH) reconstruction, and can be modeled as the
convolution, in time, of the sampled signal () with a scaled pulse, with a width , and
shifted to the right by  :

                                          2

       () = ()  ()                                     (10.10)

with:

                  1  - 2                               (10.11)
       () =   (  )

Because the derivation is very similar to the ideal reconstruction case (Section 10.1) we
present, without proof, the formulation for the reconstructed signal () using ZOH:

                      -  -                             (10.12)
       () =  () ( 2  )

                     =-

which is a piecewise-continuous function of time.
    The process is illustrated in Figures 10.6 and 10.7, following the same reasoning as for

the ideal signal reconstruction (Figures 10.3 and 10.4). The ZOH reconstruction is more crude
than the ideal reconstruction, as it results in a staircase continuous-time function. Sampling
faster improves the reconstruction, as illustrated in Figure 10.8. This makes perfect sense:
when the sampling interval becomes smaller, the `step size' of the staircase becomes smaller
as well, thereby more closely approaching ().

    ZOH reconstruction is easy to understand in the time domain. Clearly, a loss of accuracy
results: the reconstructed signal will not be exactly equal to the original signal (). Hence,
its Fourier transform () will not equal ().

    To illustrate what happens in the frequency domain, recall that the Fourier transform of
a convolution of two time signals equals the multiplication of the Fourier transforms of those
signals, (6.8). Therefore, since () = ()  (), then () = ()(), as in (10.1).
We can obtain () through Fourier-transforming () from (10.11):

       () = sinc()-2 2                                 (10.13)

where we used (5.11) and (6.2). The ZOH reconstruction filter () is a complex-valued
function of frequency. The impulse response function of this filter, (), as well as the mag-
nitude of its Fourier transform, |()|, are illustrated in Figure 10.9. The ZOH reconstruction
filter approximates the ideal filter well for small frequencies (near the maximum of the sinc
function), but extends beyond the range [-  ,  ] which means that, when using this filter to

                                                                                22

reconstruct the signal from its samples, part of the (energy contained in the) copies of ()
ends up in ().

    This is illustrated in Figure 10.10. The left and right columns show the ZOH reconstruction

for sampling frequencies of 3 and 10 Hz, respectively. Sampling with 3 Hz creates copies at

integer multiples of 3 Hz, which are then multiplied by () to obtain ().
    The amplitude spectrum of the reconstructed signal, |()|, is similar to the amplitude

spectrum of the original signal, |()|, at lower frequencies, but contains scallops around
10.2 Zero-order hold reconstruction                                                                                   111

             1.0 t-0- 2 t x0                                                               multiplied by x0
                               x0( t )
xn           0.5

             0.0 n = -8 -7 -6 -5 -4 -3 -2 -1 0 1                          2      3      4  5      6     7          8

                  -3           -2            -1         0                    1                    2                   3

                                                        t [s]

             1.0 x1                                  n = 1 1.0                                                  n=2
             0.5                                                 0.5 x2
xn                                                               0.0

             0.0

                  -3 -2 -1 0              1          2  3 -3 -2 -1 0                                 1     2          3

                                   t [s]                                                   t [s]

             1.0 n = 3 1.0                                                                                    n=4
                                                                                                        x4
xn           0.5                                        0.5

             0.0                          x3            0.0

                  -3 -2 -1 0              1          2  3 -3 -2 -1 0                                 1     2          3

                                   t [s]                                                   t [s]

Figure 10.6: Multiplication of the shifted pulse (), (10.11), times  (red line) by sample values  (blue stems),
for  = 0, ... , 4. Only the top plot shows indices  these are absent in the other plots to prevent clutter.

             1.0                          adding        x(t), xr(t)  1.0                                xr (t)
             0.5                           all red                            yields
             0.0
                                          functions                  0.5 x(t)
                        -2
xn

                                                                     0.0

                                   0              2                          -2            0                    2

                                   t [s]                                                   t [s]

Figure 10.7: Adding all red functions in the left figure yields the reconstructed signal () (red) at right, with
original () in blue. Note that in this figure, and the following, the indices  are not shown to prevent clutter.

             1.0            1                                        1.0            1
                            3                                                       10
x(t), xr(t)           t  =     s          xr (t)        x(t), xr(t)       t =           s

             0.5 x(t)                                                0.5

             0.0                                                     0.0

                      -2           0              2                          -2            0                    2

                                   t [s]                                                   t [s]

Figure 10.8: ZOH reconstruction with sampling frequency  = 3 Hz (left) and  = 10 Hz (right). The reconstructed
signal (), (10.12), is shown in red, the original () = sinc2() in blue.
112                                                                                     10. Signal reconstruction

          3                                                                                             fs

          2 fs = 1 t t                                                      1.0

          1                                                                 0.5 - 2 2 fs fs

          0                                                                 0.0

                                    t/2
hr (t)
                                                                  |Hr(f )|

          -3 -2 -1 0                     1  2          3                    -5 -4 -3 -2 -1 0 1 2 3 4 5
                                                                                                     f [Hz]
                        t [s]

Figure 10.9: Left: impulse response of ZOH reconstruction filter () (sampling frequency  = 3 Hz). Right:
magnitude of Fourier transform of ZOH filter, |()| (red) the ideal filter is also shown (green, dashed).

integer multiples of 3 Hz where the energy of the copies ends up in |()|. Apparently,
the staircase shape of () in the ZOH reconstruction in Figure 10.8 requires its amplitude
spectrum |()| to have energy at the higher frequencies to build the `steps' in time.

    Sampling faster makes these steps smaller and the amplitude spectrum of () more
closely resembles the amplitude spectrum of (), Figure 10.10 at right. Sampling with 10 Hz

creates copies at integer multiples of 10 Hz, to be multiplied by () to obtain (). The
spectrum of the reconstructed signal is very similar to the spectrum of the original signal for

a larger range of frequencies around  = 0, compared to the graph at left. Smaller scallops

occur around integer multiples of 10 Hz where the energy of the copies ends up in |()|.

Xs(f )    1.0                                             Xs(f )            1.0

          0.5                                                               0.5

          0.0                                                               0.0         -5 0          5 10 15
             -15-12 -9 -6 -3 0 3 6 9 12 15                                     -15 -10        f [Hz]
                                      f [Hz]                                                          5 10 15
                                                                                                        fs = 10 Hz
|Hr(f )|  1.0                                             |Hr(f )|          1.0
                                                                                                      5 10 15
          0.5                                                               0.5

          0.0                                                               0.0         -5 0
             -15-12 -9 -6 -3 0 3 6 9 12 15                                     -15 -10        f [Hz]
                                      f [Hz]

|Xr(f )|  1.0                               fs = 3 Hz     |Xr(f )|          1.0

          0.5  scallop                                                      0.5  scallop

          0.0                                                               0.0         -5 0
             -15-12 -9 -6 -3 0 3 6 9 12 15                                     -15 -10        f [Hz]
                                      f [Hz]

Figure 10.10: Left and right columns show results for sampling with, respectively,  = 3 Hz and 10 Hz. Top:
Fourier transform () of the sampled signal. Middle: magnitude of Fourier transform of ZOH filter, |()|
(red), and ideal filter (green, dashed). Bottom: magnitude of Fourier transforms of the reconstructed signal,

|()|, using ZOH (red) and ideal filter (green, dashed) the latter is identical to |()|.
10.2 Zero-order hold reconstruction                                                                                                             113

A real and even cosine signal () = 2 cos(2) has been sampled with sampling                                                                           EX 10.2
frequency  = 3 Hz. Use a ZOH filter to reconstruct a continuous-time signal from the
sampled signal, (), with Fourier transform (). Show the magnitude and phase
spectra of this Fourier transform and explain the result. Repeat the example for a
sampling frequency of 10 Hz and explain the differences.

Solution The cosine has a frequency 0 = 1 Hz its Nyquist rate is 2 Hz and no
aliasing occurs for both sampling frequencies. Its Fourier transform equals () =
( + 1) + ( - 1). The Fourier transform of the impulse train model of the samples,
(), will have copies of () at all integer multiples of the sampling frequency. When
reconstructing using a ZOH filter, the Dirac pulses in () (all with a weight of 1)
are multiplied by the ZOH filter (), (10.13), yielding (), see (10.1). This latter
function is zero at all frequencies, except at the frequencies of the Dirac pulses that
occur in (). At these frequencies, it equals ()(), the Dirac pulses get a
complex-valued weight, with magnitude |()| = |()()| = |()| and phase
() = (()()) = (). Hence, at all frequencies in () that contain
Dirac pulses, () equals ().

xn, xr(t)  2                                                            xn, xr(t)  2

           0                                                                       0

           -2 x(t)                                                                 -2

              -1                       0                         1                     -1                      0                         1

                                       t [s]                                                                   t [s]

|Xr(f )|   1                                   |X(f )|                  |Xr(f )|   1                                   |X(f )|

                             |Hr(f )|                                                                |Hr(f )|

           0                                                                       0

              -20 -10 0                        10                   20                 -20 -10 0                       10                   20

                                       f [Hz]                                                                  f [Hz]

Xr(f )      X(f )                                                       Xr(f )      X(f )

                                                         Hr(f )                                                                  Hr(f )

           -                                                                       -

              -20 -10 0                        10                   20                 -20 -10 0                       10                   20

                                       f [Hz]                                                                  f [Hz]

Figure 10.11: Left and right columns show results for sampling with, respectively,  = 3 Hz and 10 Hz.
Top: time-domain signal () (gray), the samples  (blue stems) and reconstructed signal () (red).
Middle: amplitude spectra of () (gray), () (purple, dashed) and () (red). Bottom: phase spectra
of () (gray just at  = ±1 Hz), () (purple, dashed) and () (red).

     Figure 10.11 illustrates the results for sampling frequencies  = 3 (at left) and
10 Hz (at right). Starting with the left column, the top figure shows the original cosine
() in gray, its samples  using blue stems and the reconstructed signal () in red.
Clearly, although meeting the sampling theorem, the signal has been poorly sampled
and reconstructed: () differs greatly from (), it resembles a block wave. This
is reflected in its Fourier transform (), shown in the middle (amplitude spectrum)
and bottom (phase spectrum) plots. We obtain a large array of Dirac pulses at all
frequencies to which the original Fourier transform () is copied: ±1, ±2, ±4, ±5, ... Hz.
114  10. Signal reconstruction

    These Dirac pulses, at each frequency are multiplied by the magnitude of () at that
    frequency. The phase at each frequency equals the phase of () at that frequency,
    as () = () = 0   of the Dirac pulses. Apparently, we need a large number
    of shifted cosines, with these frequencies, to `model' ().

         Sampling faster yields a better reconstruction, shown in Figure 10.11 at right. Al-
    though still far from perfect, () is clearly a cosine wave, and its Fourier transform
    represents () (shown in gray) more closely. However, also in this case, the ZOH-
    reconstructed signal is not even, and its Fourier transform is complex-valued.

    Several other (more advanced) reconstruction techniques exist. Examples are the First-
Order Hold (FOH), which applies a piecewise linear interpolation between consecutive sam-
ples using the triangular -function for (), and higher-order hold techniques. Sampled,
discrete-time signals can also be converted to analog, continuous-time signals using (low-
pass) electronic filters, discussed in Chapter 18.

    The Exercises book contains extensions to the theory explained in Chapters 9 and 10. For
instance, what about sampling signals which have a band-limited, rectangular spectrum, such
as the sinc function. And, perhaps more importantly, sampling signals that are non-band-
limited these are mostly periodic signals such as the square wave function.

    In this and the previous chapter we discussed the Fourier transform () of the impulse
train model of the samples () for infinite-duration signals. The consequences of sampling
and signal reconstruction could all be introduced while still working in continuous time. In the
next chapter, we move our discussion to discrete time, and introduce the Fourier transform
of an infinite-duration sequence of samples , the Discrete-Time Fourier Transform (DTFT).
In Chapter 12 we abandon the infinite signal duration assumption and discuss the practical
case: working with a finite number of samples, using the Discrete Fourier Transform (DFT). A
summary of all steps is presented in Appendix I.
        11

Discrete-Time Fourier Transform

In Chapter 9 the process and consequences of sampling an infinite-duration, continuous-time

signal () have been discussed. The sequence of samples  was modeled by introducing
the impulse train-sampled signal (), (9.2). This signal is defined solely by the samples and
the sample interval  it loses any information of the values of () in between the sampling

times. It was shown that () can be Fourier-transformed, yielding:

                             (9.10)

() =  ( - )

               =-

This Fourier transform is periodic in frequency with sampling frequency : ( + ) =
()    . () consists of an infinite number of copies of the Fourier transform ()
of the original signal (), at integer multiples of . To avoid the copies overlapping, the
sampling theorem, (9.12), states that, for band-limited signals, the sampling frequency 
should be larger than twice the largest frequency  occurring in ().

    But clearly, something is missing. After all, () is still a continuous-time signal, while on
the computer we have a discrete-time sequence of sample values , with the index    as
the `running variable'. Time and frequency disappear altogether.

    In this chapter we introduce the Fourier transform of an infinite-duration discrete-time se-

quence : the Discrete-Time Fourier Transform (DTFT). It will be shown that our formulation
of this transform is identical to (). In addition, it will be explained why () is periodic
with , as a logical, inevitable consequence of moving from continuous time to discrete time.

11.1 Derivation of the DTFT

Let us return to the definition of the Fourier transform of a continuous-time signal ():

    

() = {()} =  ()-2 d                                                                       (5.1)

    =-              ()

the integral of a complex function () = ()-2. Suppose we approximate this Fourier
integral through its Riemann sum, considering this function () only at discrete time instants
 = . The integration differential d becomes , the sampling interval:

                        

()   ()-2  =   ()-2          (11.1)

=-  ()=                 =-

                        115
116                            11. Discrete-Time Fourier Transform

Figure 11.1: Discretization of the Fourier transform integral, (5.1), through a Riemann sum approximation, (11.1).
The blue curve illustrates (the real part of) function (), discretized at time instants  = .

In doing so, the Fourier transform integral has been `discretized', resembling, but not being
equal to, the process of sampling. In this approximation, we assume () to be constant
between time instants  and ( + 1), as if we would approximate it using a ZOH (Sec-
tion 10.2). Figure 11.1 illustrates (the real part of) the product of signal () and the complex
exponential -2 (for some value of frequency ) this product () is generally complex.
In summation (11.1), the integrand () is considered only at discrete time instants  = ,
with sampling interval .

    The approximation of () would be perfect when  becomes infinitesimally small, or
when the corresponding sampling frequency  = 1 becomes infinitely large. But that is
because we attempted to approximate (), the Fourier transform of the continuous-time
signal (). What we need, however, is the Fourier transform of the sequence of samples ,
where the value of  for any  not an integer is not constant, but undefined.

    A formulation for the Discrete-Time Fourier Transform (DTFT) can be obtained by examin-

ing the summation at the right-hand side of (11.1) a little closer. For the time being, we refer
to this summation as ():

                           

     () =   ()-2 =   -2        (11.2)

     =-                    =-

This is a continuous function of frequency , which only uses the sample sequence values ,
in a very similar way as the Fourier transform: the summation represents a linear combination
of the multiplications of samples  (all real-valued) with `building block' -2. But what
would this `Fourier transform of the sample sequence ' look like? Since () is identical
to (), (9.10), the Fourier transform of the impulse train sampling model of the samples is:

                               (11.3)

     () =  ( - ) = ()

                      =-

     Proof Define continuous-time signal () = ()-21 for some frequency 1 which we
     specify later. Then () = ( + 1), (6.6). Next, define the following function, using ()
     and sample time :

                                  

             () =  ( + )

                               =-

     This function is periodic, with period : ( + ) = (),     and   . Hence, ()
11.1 Derivation of the DTFT                                                       117

can be written as a complex exponential Fourier series, with   :

           

() =            2                                                                 (11.4)

                  

           =-

with fundamental frequency  = 1 . The complex exponential Fourier series coefficients 
equal  (), (G.1), with () the Fourier transform of (), see Section G.1. We obtain:

                                 

() =  ( + ) =   ()2

           =-                =-

Set  = 0:

                             

(0) =  () =   ()

           =-                =-

Substitute the expressions for () and () in terms of, respectively, () and ():

                               

 ()-21 =   ( + 1)

=-                           =-

Shifting over any frequency 1 = , we obtain:

                                                    

 ()-2 =   ( + ) =   ( + )

=-                           =-                     =-

Substituting this equation in (11.2), and with  = -, we obtain our final result:

                                   

() =   ()-2 =  ( - )

             =-                    =-

a relationship known as the Poisson summation [9].

    The Fourier transform of the discrete-time sample sequence  is thus obtained, yielding
the expression for the Discrete-Time Fourier Transform (DTFT):

                                                                                  (11.5)

() = DTFT{} =   -2

                                             =-

We obtain a continuous function of frequency  when DTFT-ing the infinite-duration sequence

of samples . The DTFT is identical to () and has the same properties. Its unit equals
[unit seconds] or, equivalently, [unit/Hz], with [unit] the unit of sequence .

    The approximation of () in (11.1) is perfect only when the sampling frequency  be-
comes infinitely large (hence, ultimately staying in continuous time). All copies of () oc-

curring at non-zero integer multiples of  will be positioned at infinitely-high frequencies, and
() will indeed be identical to (). With a limited sampling frequency , the Fourier
transforms of () and  are different, as the result of the latter transform consists of copies
of the former transform centered at integer multiples of that sampling frequency  (11.3).
118                                            11. Discrete-Time Fourier Transform

11.2 Alternative derivation and formulation

An alternative way to arrive at the definition of the DTFT (11.5) is to Fourier-transform the
continuous-time impulse train model of our samples (), (9.2):

                         with                                                  (11.6)

     () =  ()-2 d                              () =  ()  ( - )

                   -                                                      =-

We obtain:           
       ()
            =  ( ()  ( - )) -2 d

            -        =-

                                               

            =    ()( - )-2 d =   ()-2

               =-=-                            =-

               

            =   -2 = () = DTFT{}

               =-

Note that in this alternative derivation we continue to work with the continuous-time model of
the samples and corresponding Dirac pulses, while in the derivation of the DTFT in Section 11.1
we abandoned this model and worked directly, and only, with the sequence of samples itself.

    The definition of the DTFT in (11.5) differs from the definition used in other textbooks on
signal analysis, e.g., [8]. First, as explained in Section 9.2, in these books the impulse train
sampling function (9.3) is not scaled by , while we do this, for multiple reasons. Second,
the term 2 is replaced by  (from 2 = , hence 2 =  = ). As a result, these
books state the following definition of the DTFT:

                                                                               (11.7)

     DTFT{} = () =  -

                                          =-

Since  = 2, its unit is [rad] and we can verify that () is periodic with 2: (+2) =

(), in a similar way as () is periodic with , (11.3). The only difference between (11.7)
and our definition (11.5) is the scaling with  in the latter: () = ().

11.3 Inverse DTFT

For the sake of completeness, we state the expression for the inverse DTFT, returning the
sequence of samples  from the DTFT ():

      = iDTFT{()} =  ()2 d                                                     (11.8)

                                             

The proof, as well as some common DTFT pairs, is included in Appendix G. Equation (11.8)

shows that the infinite-duration sequence of samples  is constructed by integrating the
product of () and complex exponential 2 over all frequencies  within a band with
a width of  () is periodic. A consequence of sampling is that only a band of frequencies,
with a width , is available to construct the sequence  from its DTFT.

    Similar to the complex exponential Fourier series and the Fourier transform, (11.5) is known

as the analysis equation and (11.8) as the synthesis equation of the DTFT.
11.4 DTFT properties                                                    119

11.4 DTFT properties

The DTFT plays a small role in this book. Only the properties which are important in relation to
the Discrete Fourier transform (DFT), see Chapter 12, are described. For more DTFT properties
(e.g., linearity), theorems and pairs the reader is referred to other sources, e.g., [8].

    The DTFT (of sample sequence ) and the Fourier transform (of continuous-time signal
()) have much in common. Consider the following DTFT properties:

    · () is a complex-valued, continuous function of frequency , and can be expressed
       in polar form, see (5.5).

    · () has a magnitude |()| and phase (), and plotting these as a function
       of frequency yields the amplitude and phase spectra, see (5.6) and (5.7).

    · (0) equals the sum of all samples multiplied by , a crude integration of the discrete-
       time sequence , see (5.8).

    · () has a real part and an imaginary part which, respectively, correspond to the DTFT
       of the even and odd parts of sequence , see (5.9).

Three other properties are stated here. First, as holds identically for (), see (5.10):

(-) =  ()                                                               (11.9)

for real-valued signals, which directly follows from (11.5).
    Second, clearly in contrast to (), () is periodic with :

( + ) = () ,                                                            (11.10)

This latter property was already shown in Figure 9.6 for (), follows from (11.3) and can
be easily proven using (11.5).

Proof

                                                                    =1

 ( +  ) =    -2(+) =    -2-2  
                                           

                      =-               =-

                                                              

                      =   -2 -2 =   -2 = ()

                      =-      =1  ,        =-

Third, applying (11.10), for  = - and with  = 1, to (11.9) yields:

( - ) =  ()                                                             (11.11)

    Figure 11.2 illustrates the three properties for the DTFT () of example signal () =
sinc2(), sampled with  = 3 Hz (no aliasing), see Figure 9.6. Because () is real and even,
the imaginary part of the DTFT of its samples is zero, and  () = (). This signal is
therefore less suited as an example to show the effects of a complex conjugate, but we added

graphical elements to clarify. The periodicity of the DTFT, (11.10), is shown at the top.

    Because () is periodic with , (11.10), any frequency range with a width of  (i.e.,
one period) contains all information. Considering () within the frequency range [- 2 , 2 ]
120                                                         11. Discrete-Time Fourier Transform

        1.0                             - 2fs                   fs

Xt(f )  0.5                                                      2

                                                                                     f [Hz]

        0.0 . . .      -1fs                    -f1 0fs      f1      fs - f1  1fs           ...

             -5    -4  -3           -2         -1   0       1       2        3    4               5

                                    1.0

             Xt(-f1) = X t(f1)      0.5                                           [- 2fs , 2fs ]

                                    0.0        -f1          f1

                                               -1   0       1

             Xt(fs - f1) = X t(f1)                  1.0         fs                [0, fs]
                                                    0.5
                                                    0.0         2

                                                         0  f1      fs - f1

                                                            1       2        3

Figure 11.2: DTFT () of , the discrete-time sequence obtained when sampling () = sinc2() sampling
frequency  = 3 Hz (no aliasing), 1 = 0.8 Hz. The three parts of this figure illustrate the full frequency range
(top), the range [- 2 , 2 ] (middle) and the range [0, ] (bottom).

illustrates property (11.9), identical to the continuous-time Fourier transform property (5.10).
At frequency  = 1 > 0 the DTFT equals (1), marked by the black dot. At frequency
 = -1 the DTFT equals (-1) = ((1)) =  (1), marked by the black star. We
highlight the DTFT values for the `positive' frequencies ( = 1 > 0) using blue continuous
lines, and those for the `negative' frequencies ( = -1) using green dashed lines.

    Considering () within frequency range [0, ] illustrates property (11.11). The DTFT
coefficient at a frequency  =  - 1 (black star), which lies beyond the Nyquist frequency 2
equals the complex conjugate of the DTFT coefficient at frequency  = 1 (black dot). Within
this interval, at frequencies beyond the Nyquist frequency, the DTFT evaluates the `negative'
frequencies of the copy of () positioned at 1.

11.5 Discrete-time complex exponential

Properties (11.9)-(11.11) are studied again in this section, to show that these are all the
consequence of the behavior of the DTFT building block -2 appearing in (11.2).

    In continuous time, the Fourier transform uses the continuous-time complex exponential
building block (, ) = -2, a vector with magnitude 1 and phase -2 [rad], (5.1).
Fixing the frequency to  = 1, 1  , results in a harmonic, complex-valued function of time,
(, 1): the unit vector rotates in the complex plane, describing a unit circle centered at zero,
see Figure 2.9. This function (, ) has three properties:

   1. It is periodic in time for any frequency 1,

   2. For any two different frequencies 1 and 2, we get two different functions, and

   3. When frequency 1 increases, the vector rotates faster in the complex plane.
11.5 Discrete-time complex exponential                                                                         121

In discrete time, the DTFT uses the discrete-time complex exponential building block () =
-2, (11.5), where continuous time    is replaced with discrete time ,   . For

any frequency  = 1, the time discretization causes (1) to step along the unit circle as a
function of index , with step size 21 = 2 1  . This `stepping' along the unit circle has
major implications: none of the three properties stated above for (, 1) are true for (1).

    First, (1) is defined to be periodic in time index  with period  ( > 0), when:

+(1) = (1)                                                                                                     (11.12)

The smallest value of   + for which (11.12) holds is the fundamental period in terms of
index. Hence, for (1) to be periodic, +(1) = (1) must hold, which is only true when:

1 =  ,                                                                                                         (11.13)
 

Proof   -21(+) = -21  -21-21 = -21
            -21 = 1  21 = 2  1 =   1  =   

    In discrete time, when frequency 1 is not a rational number multiplied by the sampling

                                    

frequency , 1   , the building block (1) is aperiodic. In this case, when stepping
along the unit circle in the complex plane, (1) never assumes the same position.

Considering Figure 11.2, describe the building block () using (11.13), for three                                        EX 11.1
frequencies:  = 0.8,  = 2 and  = 1.12 Hz. The sampling frequency  = 8 Hz.

Solution At frequency , () is periodic: 0.88 = 110 ,  = 1 and  = 10. At this
frequency, with this sampling frequency, () repeats itself 1 time every 10 samples.

At frequency , () is aperiodic, because 28 is an irrational number.
At frequency , () is periodic: 1.12 = 112 = 7 ,  = 7 and  = 50. At this
                   8                    800  50
frequency, with this sampling frequency, () repeats itself 7 times every 50 samples.

The sketch below (the sine part of ()) shows that after 50 samples () will attain

exactly the same position in the complex plane, after completing 7 full cycles.

cn(fc)             7 cycles

        n = 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50  n

Figure 11.3: Sine part of DTFT building block (), with  = 8 Hz and  = 1.12 Hz.

    Second, the rotation in the complex plane described by (1), as a function of time index ,
is identical for frequencies that are separated by an integer multiple of the sampling frequency:

(1 + ) = (1),                                                                                                  (11.14)

Proof                                                                                                      =1

        -2(1+) = -21-2   = -211 = -21

We used this property several times, e.g., in proving (11.10).
122                                                         11. Discrete-Time Fourier Transform

Im{e-j 2f tn}                                            n                                               n

               0123456789                                      0123456789

     f [Hz] f1 fs0                                                             fs - f1

                                                            2                           fs

                                   f1                                                     f1

            -f1

                                               n

     0123456789

Figure 11.4: The imaginary part of the DTFT building block, the complex exponential function (), is shown for
three frequencies:  = 1 ( = 1 in (11.13)), -1 ( = -1) and  - 1 ( = 9), with 1 = 0.8 Hz and  = 8 Hz,
as a function of discrete time index  = 0, ... ,  - 1 (() is periodic, with  = 10, see Example 11.1). The
gray continuous curves show the continuous-time building block (, ) (sine part) for the same frequencies. The

central horizontal line shows a part of the (infinite, continuous) frequency axis , emphasizing   [0, ], using
the same line and color coding as in Figure 11.2 the red marker shows the Nyquist frequency.

    Third, the apparent rate of rotation in the complex plane described by (1) will increase
when 1 progresses from 0 to 2 , and will then decrease when 1 progresses from 2 to :

     ( - 1) =  (1)                                                             (11.15)

     Proof                                                                 =1

            -2(-1) = -2  21 = 121 = (-21)

    The latter two properties are illustrated in Figure 11.4 for frequency 1 = 0.8 Hz and
sampling frequency  = 8 Hz. Function (-1) (bottom left) is the complex conjugate of
(1) (top left). Function ( - 1) (top right) equals (-1), (11.14), and is the complex
conjugate of (1), (11.15). Although ( - 1) rotates faster in the clockwise direction than
(1) (the gray continuous curves), its apparent rotation (the blue stems) is a slower rotation
in the counterclockwise direction, identical to (-1). Hence, ( - 1) = (-1) =  (1),
(11.15). The DTFT building block at any frequency  - 1 is the complex conjugate of the
DTFT building block at 1 Hz: the highest rate of apparent rotation occurs at  = 2 .

    DTFT properties (11.10) and (11.11) are the consequence of the properties of the DTFT
building block () = -2, (11.14) and (11.15). This function () cannot distinguish
between frequencies  Hz apart: at these frequencies it will have identical values. When
evaluating the sequence  in (11.5) at these frequencies, the DTFT performs the same cal-
culation, so that () will have identical values, explaining the DTFT periodicity, (11.10).

    In short, the consequences of sampling in time on the Fourier transform can be explained
by considering the properties of the DTFT building block (). In time, we have no infor-
mation in between the samples. In frequency, the building block can only uniquely describe
harmonic oscillations within a frequency band with a width of  Hz.

    In this and the previous two chapters, signals were assumed to last forever. Clearly, this is
not very practical. In the next chapter we discuss the effects of limited observation time and
introduce the Discrete Fourier Transform (DFT). The DFT is our main practical signal analysis
tool and is used in all subsequent chapters on spectral analysis and estimation.
      12

Discrete Fourier Transform

In Chapter 5 we considered aperiodic continuous-time signals (), and transformed them to

the frequency domain () by means of the Fourier transform. In Chapter 9 we studied the

Fourier transform () of a sampled signal () (though still employing a continuous-time
model of the signal). In Chapter 11 we presented the Discrete-Time Fourier Transform (DTFT)

() of a discrete-time sequence , defined in (9.1). We are two steps away from applying
the Fourier transform in practice, using the Discrete Fourier Transform (DFT).

12.1 Finite length - discrete frequency

First, continuous-time signals () and their discrete-time sample sequences  last forever.
This is not practical. The phenomenon of interest may only exist or be available for a limited

amount of time, and we can definitely observe or measure it only for a finite amount of time.

Hence, in the first step, in the time domain, we apply a finite-length measurement window,
with duration  or , see (8.1). In this chapter we work with samples in discrete time,
and the resulting windowed or time-truncated discrete-time sequence is indicated by ,. It
follows from the infinite-length sequence  as:

, = { 0 otherwise  for  = 0, ... ,  - 1       (12.1)

This operation can be interpreted as an element-wise multiplication of sequence  by a
dimensionless window sequence  (with  = 1 for  = 0, ... ,  - 1, and 0 otherwise):
, = . The samples , for   {0, ... ,  -1}, being outside the window, equal zero, and
the result , effectively is a length  discrete-time sequence (of finite length), corresponding
to a duration of  seconds. The samples are spaced by  seconds, such that  =  (in line

with the sample-and-hold convention discussed in Chapter 10). In Appendix I it is discussed

that it does not matter whether we sample first and then window, or the other way around.

    Second, the Discrete-Time Fourier Transform (DTFT) () (11.5) of sequence  is a
continuous function of frequency , with   . This does not lend itself well to digital signal

processing on the computer. Therefore, in the second step we choose to evaluate the DTFT

() (in fact, ,(), as we apply the time window) only at a selection of discrete frequen-
cies, with a frequency step size of  Hz. Recall that through the action of sampling, the DTFT

() becomes periodic with period , see (11.3) and (11.10). Therefore, the selection of
discrete frequencies has to cover only the interval [0, ), as beyond this interval () sim-
ply repeats itself, see Figure 11.2. Although we may, equivalently, select different intervals,

                                         123
         124                                                                                        12. Discrete Fourier Transform

         such as [- 2 , 2 ), the interval [0, ) is the default in computer algorithms. When Fourier-
         transforming data sequence ,, effectively a sequence of length  with  = 0, ... ,  - 1, we
         prefer to neither lose information nor introduce redundant information. Hence, the selection

         of frequencies will contain  discrete frequencies, exactly as many as we have samples. With
         an equidistant spacing on interval [0, ) this leads to frequency step:

               =  = 1 = 1                                                                                          (12.2)
                       

         in the frequency domain. Within interval [0, ) the selection of  discrete frequencies then
         becomes 0, , 2, ... , ( - 1) these are referred to as the `analysis frequencies' of the

         Discrete Fourier Transform (DFT). The continuous-frequency function ,() is evaluated at
          =  with  = 0, ... ,  - 1, and consequently denoted as  = ,().

             Starting from the DTFT () of sampled sequence , implementing these two steps
         yields the Discrete Fourier Transform. The DFT transforms a finite-length discrete-time se-

         quence , into a finite-length discrete-frequency sequence , of same length by default.

EX 12.1  We continue the example of Figure 9.3 with even signal (), sampled in Figure 9.5 with

         sampling frequency  = 3 Hz. Apply a time window from  = -2 to  = 2 s ( = 4 s)
         to obtain , and evaluate its DTFT ,() at the  =  = 12 analysis frequencies.

         Solution Figure 12.1 at left shows , with the blue stems. According to the sample-
         and-hold convention in Chapter 10 on signal reconstruction, there are  = 12 samples
         at an interval of  = 1 s (each sample represents a duration of  s), such that

                                                     3

          = , and the sample at  = 2 s is not included.

            Figure 12.1 at right shows the (modulus of the) Fourier transform ,() evaluated
         at the  = 12 frequencies, at an interval of  = 1 = 14 Hz, such that  = , and
         the frequency  =  = 3 Hz is not included.

              1.0      T                                                                       1.0     [0, fs)
              xn,T
                                                                                  |Xt,T (f )|                                                           Xt(f )
              0.5 x(t) t                                                                       0.5 f

              0.0                                                                              0.0

                   -2      0      2                                                                 0  2        4

                           t [s]                                                                       f [Hz]

         Figure 12.1: Left: sampled and time-windowed sequence ,, with  = 13 s,  = 12 and window length
          =  = 4 s, shown by the stems () is shown by the dotted curve as a function of time , as
         in Figure 9.3. Right: the modulus of Fourier transform ,(), evaluated at discrete frequencies (the
         stems) with  = 1 = 14 Hz the DTFT () = () is shown by the dotted curve, as in Figure 9.6, but
         here for a limited frequency range.

            Apart from the zero frequency, the lowest frequency for which we evaluate ,()
         is  = , the frequency of a harmonic which completes one full `revolution' or cycle
         within the observation window of  seconds. We can imagine that trying to evaluate the
         spectrum at (much) lower frequencies is challenging at best, as our window contains
         only a (very) small part of the full cycle of the corresponding harmonic, and the rest
         of its cycle is unknown to us. The graph at right also shows the DTFT () = (),
12.2 Definition of the DFT and inverse DFT                                     125

dotted, and the stems correspond well spectral leakage due to the finite time window
length is very limited here see Figure 8.3 for a shorter window of  = 3 s.

12.2 Definition of the DFT and inverse DFT

12.2.1 DFT derivation and definition

With the goal of obtaining the Fourier transform of a finite-length discrete-time sequence ,,
we begin with the Discrete-Time Fourier Transform (DTFT), (11.5), of the sampled sequence
. The time window of duration  turns the infinite sum in (11.5) into a finite sum from  = 0
to  =  - 1. And in this window, , is identical to , (12.1):

                                                  -1

,() = DTFT{,} =   ,-2 =   -2                                                   (12.3)

          =-                                      =0

Next, the resulting Fourier transform ,() is evaluated at  discrete frequencies, which
cover the range [0, ):  =  with  =  = 1  , and  = 0, ... ,  - 1:

-1                                          -1
                                                                            1
,() =                                                                          (12.4)
          -2 =    -2  

=0                                          =0

This results in the definition of the Discrete Fourier Transform:

                                        -1                                     (12.5)

 = DFT{} =   - 2  

                                        =0

We obtain  complex-valued coefficients, , with index  = 0, ... ,  - 1, which together
represent the evaluation of ,() at  discrete frequencies. Their unit equals [unit seconds]
or, equivalently, [unit/Hz], with [unit] the unit of sequence .

    In the remainder of this book, unless stated otherwise, we omit the  index with the (finite-

length) discrete-time sequence: we use (again)  instead of ,. In practice it is evident
that the sequence can never be of infinite length: it is of finite length,  samples. Our main

tool, the DFT, only considers the time interval for  = 0, ... ,  - 1, where , =  (12.1).
    The DFT is a computer algorithm that has as its input  (in this book real-valued) samples

of (), denoted by  with  = 0, ... ,  - 1, and has as its output  complex-valued numbers
 with  = 0, ... ,  - 1 this operation is also referred to as the `N-point DFT'.

We re-formulate Example 12.1 in the context of the DFT, with DFT input sequence                   EX 12.2
( = 0, ... ,  - 1) and DFT output sequence  ( = 0, ... ,  - 1).

Solution Figure 12.1 from Example 12.1 becomes Figure 12.2 the twelve numerical
values in the two figures at left are identical, and so are the twelve values at right (the
latter are generally complex-valued therefore the modulus is shown).

   In Figure 12.1 the time dimension has been restored in the graph at left, and the
frequency dimension in the graph at right. A computer does not know time and fre-
quency it merely has the indices  and , representing the th sample and the th
DFT frequency, respectively, as illustrated in Figure 12.2.
126                                                                                   12. Discrete Fourier Transform

      1.0                                                                        1.0

      0.5xn                                                                      0.5
                                                                          |Xk |

      0.0                                                                        0.0
             0 1 2 3 4 5 6 7 8 9 10 11                                                  0 1 2 3 4 5 6 7 8 9 10 11
                                      n                                                                          k

Figure 12.2: The sampled and time-windowed signal is a discrete-time sequence  with  = 0, ... ,  - 1,
consisting of  data points (at left), and the DFT turns these into  numerical evaluations of the Fourier

transform ,() at  =  with  = 0, ... ,  - 1, denoted by || (at right), for the example in
Figure 12.1, with  = 12.

   Figure 12.2 shows the DFT input sequence  and output sequence . Since the
DFT by default considers  with  = 0, ... ,  - 1, according to (9.1), corresponding to
time window   [0, ], we apply a time shift of 0 = - 2 (see (6.2)) to the samples
shown in Figure 12.1 at left. This does not affect the magnitude of ,(), shown at
right, but it would impact its phase, (6.2). Therefore, || is shown in Figure 12.2.

12.2.2 Inverse DFT

The inverse DFT is given by:

                                                 -1                                                                 (12.6)

       1 1  = iDFT{} =   2  

                                          
                                                 =0

for  = 0, ... ,  - 1. The proof, as well as some common DFT pairs, is included in Appendix H.
Similar to the complex exponential Fourier series, the Fourier transform and the DTFT, (12.5)
is known as the analysis equation and (12.6) as the synthesis equation of the DFT.

    With (12.5) and (12.6) we can go from  to  and back, without losing any information.
In practice, we typically apply the DFT to a finite-length discrete-time signal, turning the
 = 12 blue dots of Figure 12.2 at left into the  = 12 blue dots in the graph at right.

    Starting from an infinite-length continuous-time signal, Appendix I summarizes the full pro-
cedure of first applying a window in the time domain, then sampling the signal, and eventually
evaluating the Fourier transform in the frequency domain at discrete frequencies.

12.2.3 Alternative definition

Note that in many textbooks and programming environments, such as Python, the DFT (12.5)
and its inverse (12.6) are defined without the factor  and 1 , respectively:

                                                                                                              

                -1                                                                                                  (12.7)

       =  -  2 

      =0

and:       1 -1

                  2 
       =                                                                                                            (12.8)

           =0

meaning that it is up to the user to restore the time and frequency dimension.
12.3 DFT frequencies, properties, and diagram                                        127

    In this book, by default, we refer to the DFT (12.5) and its inverse (12.6), respectively,
with  and 1 included, as we did in the previous chapter on the DTFT, see (11.5). In this

                       

way, the time interval or step size  and frequency step size  = 1 become immediately

                                                                                                                            

apparent, and the unit ([unit/Hz]) of a Fourier transform is maintained, see Section 5.2.

12.2.4 Fast Fourier transform (FFT)

An extremely efficient algorithmic implementation of the DFT is the Fast Fourier Transform
(FFT) [10, 11], available in Python through the numpy.fft.fft for (12.7) and numpy.fft.ifft
for (12.8). The FFT was published by Cooley and Tukey in 1965 [10]1 and, especially in times
of limited computer power, revolutionized spectral analysis, as it substantially reduces the
number of operations needed to obtain the DFT.

    When the number of samples  is a power of 2 ( = 2) the DFT needs 22 operations,
while the FFT needs `only' 2 operations. While computing the same  coefficients 
from  samples, the FFT algorithm is 2 times faster. For instance, for  = 4,096 samples

                                                                        

 = 12, the DFT requires 16,777,216 operations, while the FFT requires 49,152 operations,
which makes it 341 times faster than a plain DFT algorithm for this number of samples.

    The FFT implementation is not discussed further in this book.

12.3 DFT frequencies, properties, and diagram

The properties of the DFT are a logical consequence of its close relation with the DTFT of a
time-windowed discrete-time sequence, as outlined in the previous section.

12.3.1 DFT frequencies
The complex exponential building block -  of the DFT is always 2  periodic in time, as the

analysis frequencies are all a rational number multiplied by the sampling frequency :

 =  =   =                                                                            (12.9)
                 

see (11.13). The building block assumes that the period of the th analysis frequency harmonic

fits exactly  times in the (duration of the) sequence of  samples. This means that the DFT

by its coefficients  always yields a periodic `model' of the finite data sequence , as if

+ =  (see Figure I.7) which is usually not the case with the original signal.

The DFT building block does not know about time and frequency. Recall the DTFT building

block () = -2, introduced in Section 11.5. Since in the DFT frequency  is restricted
to  =   =  1 , the  disappears altogether, (12.4): the DFT building block becomes
2   

-  . When time index  progresses from 0 to  - 1, this complex exponential steps along
the unit circle, in steps that are an integer multiple  of `fundamental frequency' 2  the latter
                                                                                   
can be considered to divide the unit circle into  equal pieces. For  = 1, the first harmonic,

it takes the building block  steps to reach the same position again in the complex plane.

This means that for the DFT building block, only the number of samples  matters: it does

not matter whether the DFT (12.5) is evaluating a sequence of 100 numbers that results from

sampling a signal for 1 s at 100 Hz, or from sampling a signal for 10 s at 10 Hz.

Figure 12.3 illustrates the imaginary part (the sine component) of the building block for a

sequence of  = 8 samples. The same properties occur as were shown in Figure 11.4. All
DFT frequencies are shown: all are an integer multiple  of the fundamental frequency 2 .

                                                                                                                                                                     8

1It was later discovered that Cooley and Tukey together had independently re-invented an algorithm already known
 to Carl Friedrich Gauss around 1805 [4].
128                                                     12. Discrete Fourier Transform

Im{e-jk 2N n}

   N =8

                                           n  k=3       k=5

                   01234567

                      k=1                                           k=7
                          1
       k = -1    0                                 fs                    fs

           -1      0                               2
     k
               k=0               2            3    4    5      6    7    8
                             k=2
                                                             

                                                               k=6

                                                   k=4

Figure 12.3: Imaginary part of DFT building block - 2 , for  = 8, with frequency indices  = {-1, 0, 1, 2, ... , 7},
when discrete-time index  progresses from  = 0 to  =  - 1 = 7. Similar to Figure 11.4, the gray continuous
curves show the continuous-time sine for the same frequencies. Note the resemblance with Figure 11.4 and the
resulting symmetry properties, for instance, 5 = 3, and 6 = 2.

Multiplying, for each frequency index , the sequence  by the building block, then adding,
times , yields the DFT coefficient . The real part of coefficient  equals the `integrated'
product of sequence  and the real part of the th building block, the cosine component.
The imaginary part of coefficient  equals the `integrated' product of sequence  and the
imaginary part of the th building block, the sine component. In doing so, the DFT evaluates

or measures the amount of similarity between input sequence  and the th building block,
in a very similar way as the complex exponential Fourier series, discussed in Chapter 4.

12.3.2 DFT properties

The DFT has much in common with the complex exponential Fourier series, see Chapter 4.
The DFT fundamental frequency equals  = 1 Hz, with the `fundamental period'  seconds.

                                                                          

The DFT results in  complex-valued coefficients  at integer multiples  = 0, 1, ...  - 1 of
this fundamental frequency. The coefficients  can be expressed in polar form, see (4.6),
with a magnitude ||, (4.7), and phase , (4.8). When shown as a function of discrete
frequency , || and  result in, respectively, the amplitude and phase spectra.

    For real-valued signals, and because of the equivalence of the DTFT and DFT at the analysis

frequencies, DTFT properties (11.9)-(11.11) become for the DFT, respectively:

     -  =                                                                (12.10)

                 

     + =  ,                                                              (12.11)

     -  =                                                                (12.12)

                 
12.4 DFT example ( = 8)                                            129

N even                              XN -k  =       X

                                                      k

Xk k = 0 1 . . . N - 2 N - 1 N N + 1 N + 2 N - 1 N . . .
                             2   2         2       2       2

AVG · T              k

   N odd                            XN -k  =       X
Xk 0
                                                      k
         AVG · T
                        ...                                   ...
        0         1 N-1 2 - 1 N-1 N-1 2 2 + 1 N-1 2 + 2 N - 1 N

                     k

                                           2fs fs

Figure 12.4: DFT coefficients  and their relationships for an even (top) and odd (bottom) number of samples
  represents the discrete frequency index. The blue box shows the positive frequencies, the green dashed box
the negative frequencies, similar to Figure 11.2. The Nyquist frequency is shown by the red dot, for even . The
gray horizontal line shows the continuous frequency interval   [0, ) corresponding to the discrete frequencies.

12.3.3 DFT diagram of coefficients

Figure 12.4 shows the diagram for the DFT coefficients . At  = 0, the DFT coefficient 0
with (12.5) yields the sum of all samples multiplied by  0 equals the average (AVG) of
                                                           
the sequence. When the number of samples  is even, the Nyquist frequency is positioned
                                                         
at  = 2 , and the DFT yields an evaluation of ,() at 2 - 1 positive frequencies. When
 is odd, the analysis frequencies do not include the Nyquist frequency, and the DFT yields
                             -1
an evaluation of ,() at 2 positive frequencies. The number of negative frequencies
always equals the number of positive frequencies, and at these negative frequencies the DFT

coefficients are the complex conjugate of the coefficients at positive frequencies, see (12.10).

    When investigating a real-valued signal, the negative frequencies are often omitted, and
when  is even also the Nyquist frequency: the frequencies from  = 0 to  =  - 1 (for even

                                                                                                                                               2

) contain all information. The DFT turns  real-valued numbers  into  complex-valued

numbers , the latter all consisting of a real and an imaginary part, but there is no additional

information created or contained, due to (12.12).

12.4 DFT example ( = 8)

In this section it is demonstrated how the DFT of a discrete-time sequence , with length
 = 8 samples, is constructed. The DFT fundamental frequency 2 becomes 2 , and the

                                            2  8

DFT building block - 8  can assume 8 positions on the unit circle, as a function of fre-
quency index  and time index , see Figure 12.5. We compute and discuss some of the DFT
coefficients, (12.5). Note that computing them in Python relies on (12.7) (without factor ).

  7               2
0 =   -0 8  = (0 + 1 + 2 + 3 + 4 + 5 + 6 + 7)
  =0                                                               zero frequency

  7               2
1 =   -1 8  = ...
  =0                                                               one cycle in  = 8

7 2 =   -2 28  = (0 - 1 - 2 + 3 + 4 - 5 - 6 + 7) two cycles in  = 8

              =0

  7               2
3 =   -3 8  = ...
  =0                                                               three cycles
130                                                                          12. Discrete Fourier Transform

k=0                     Im              k=1                  Im              k=7                  Im
                    j                                    j                                    j n=2
            4                                       4                                    4
                          3                                    3                              3             n=1
                                     2                                    2
                                                                                                         2

         5                     1 Re              5                     1 Re           5                     1 Re
                                 1                                        1                                   1
-1                                      -1                                   -1
                            8                                         8 n=1                              8
              6                                       6                                    6
                                                                 7
                         7                                                                            7
                                                         -j n = 2
                 -j                                                                           -j

Figure 12.5: Positions of the DFT building block - 2  in the complex plane, for  = 8, indicated by the small
black dots, representing integer multiples of 2 . Three values of frequency index  are shown, from left to right

                                                                                           8

 = 0,  = 1 and  = 7. For each frequency, the black, blue and purple dots and lines show the positions of the
DFT building block, for  = 0,  = 1 and  = 2, respectively.

                 7          2
4 =   -4 8  = (0 - 1 + 2 - 3 + 4 - 5 + 6 - 7)
                 =0                                                                                      four cycles

                                        = ((0 + 2 + 4 + 6) - (1 + 3 + 5 + 7)) Nyquist frequency

                 7          2
5 =   -5 8  = ...
                 =0                                                                                      five cycles

                 7          2
6 =   -6 8  = (0 + 1 - 2 - 3 + 4 + 5 - 6 - 7)
                 =0                                                                                         six cycles

                 7          2
7 =   -7 8  = ...
                 =0                                                                                      seven cycles

                                                                                                            (12.13)

    The zero-frequency coefficient 0 is a real number equal to the sum of all samples mul-
tiplied by . Figure 12.5 at left illustrates that for  = 0 the complex exponential maintains
its position in the complex plane, at 1, when time index  progresses. Hence, the sequence
 is `integrated', as in the continuous-time Fourier transform for the zero frequency, (5.8).
Dividing 0 by  yields the average of the sequence: 1 0 = 1   =0 -1  = 1 =0 -1 .

    At  =  (for even ) we obtain the coefficient at the Nyquist frequency. It is a real

                      2

number equal to the sum of the even-indexed samples minus the sum of the odd-indexed
samples, multiplied by . This number is of no practical use and typically discarded. When
sampled properly, i.e., fast enough, (9.13), there should be `nothing of value' at this frequency.

    The coefficients for  = 1, 2 and 3 are complex numbers representing the `positive' fre-
quencies they correspond to the product of the sequence  with the DFT building block
that makes, respectively, 1, 2 and 3 cycles through the complex plane, rotating in clockwise
direction, when time index  progresses from 0 to 7 (=  - 1). Figure 12.5, middle, illustrates
the first two steps of the complex exponential for  = 1, from  = 0 to  = 2.

    The coefficients for  = 5, 6 and 7 are complex numbers representing the `negative'
frequencies they correspond to the product of the sequence  with the DFT building block
that makes, respectively, 5, 6 and 7 cycles through the complex plane, rotating in clockwise
direction, when  progresses from 0 to 7. The coefficients obtained here are the complex
conjugates of those obtained for, respectively,  = 3, 2 and 1, see (12.12). Figure 12.5 at
right illustrates the first two steps of the complex exponential for  = 7, from  = 0 to  = 2.
12.5 Two more examples: DFT of cosine                                     131

    In (12.13), verify that 6 = 2. This is because the complex exponential that rotates 6
times 2 in the complex plane every time step, in clockwise direction, is equal to the conju-

            8

gate of the complex exponential that rotates 2 times 2 every time step, in counterclockwise

                                                                                                  8

direction. This is illustrated in Figure 12.5 for  = 1 and  = 7 as a consequence, 7 = 1.

12.5 Two more examples: DFT of cosine                                                                    EX 12.3

    Compute the DFT of a unit-amplitude, zero-phase cosine signal () with frequency
    0 = 3 Hz, after sampling this signal with  = 10 Hz, for time duration  = 2 s.

Solution With  = 10 Hz and  = 2 s,  = 20 samples are obtained. The DFT
frequency step  equals 1 = 12 Hz, which means that the signal frequency 0 equals
6 times the frequency step size . The sequence  completes exactly 6 cycles in the

time duration  = 2 s, and so it fits perfectly.

In Appendix H it is shown that the DFT of a unit-amplitude, zero-phase cosine sig-

nal with frequency 0 equal to  times frequency step , i.e.,  = cos[20] =
cos[2( 1 )] = cos[ 2 ] equals:
                                         

                                                                          (H.7)
 = 2 (- + -(-))

All  coefficients are zero, except for  =  and  =  - . Here, with  = 20 and
 = 6, we expect that the DFT of  results in two non-zero coefficients:

        
6 = 2 = 2 = 1 and 14 = 1

   Figure 12.6 shows the result of Python's numpy.fft.fft on sequence , multiplied
by : DFT coefficients , (12.5). The  = 20 real-valued time samples  are
transformed to  = 20 complex-valued numbers  in discrete frequency. Since 
is real and even, the imaginary parts of  are very close to 0, in the order of 10-16,
caused by the finite computer precision.

         1.0

Re(Xk )  0.5                                        Nyquist

                                  AVG·T

         0.0

              0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19
                                                                     k

         10 ×10-15

Im(Xk )  5

         0

         -5         positive frequencies            negative frequencies

         -10
                     0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19

                                                 k

Figure 12.6: Result of DFT of unit-amplitude, zero-phase cosine with frequency 0 = 3 Hz, sampled with
 = 10 Hz for a duration of  = 2 s. All  coefficients are shown as blue dots, except for the coefficient
at the Nyquist frequency, which is shown in red.
         132                                                                 12. Discrete Fourier Transform

            Considering the real part of , at  = 0 we obtain 0, the average of the cosine
         sequence , with  = 0, ... ,  - 1, a perfect fit of  = 6 cycles in duration . Since 

                                                                                            

         is even, we get a Nyquist frequency, at  = 2 = 10, at which =10 = 0 also in this
         case. All 's are zero, except at  = 6 (= ) and  = 14 (=  - ), where  = 1, as
         expected.

            For the sake of explanation, and to facilitate a comparison with the DFT diagram of

         Figure 12.4, the coefficient at the Nyquist frequency is shown by means of a red dot.

         The coefficients at the positive frequencies,  = 1, ... , 9, are enclosed by the blue box,
         those at the negative frequencies,  = 11, ... , 19, by the green, dashed box.

            Figure 12.7 shows the result of using Python's numpy.fft.fftshift function on

         sequence , which changes the order of the coefficient sequence as to cover the
         interval [-  ,  ), symmetric about  = 0. The frequency axis is restored by multiplying

                             22

         the index by the frequency step size : the peaks occur exactly at  = ±3 Hz. The
         Nyquist frequency is put at  = -  , but is typically discarded.

                                                                       2

                       1.0

              Re(Xk )  0.5                                                   AVG·T

                                                Nyquist

                       0.0

                            -5    -4                     -3  -2  -1  0       1      2              3  4

                                                                     f [Hz]

                       10 ×10-15

              Im(Xk )  5

                       0

                       -5         negative frequencies                       positive frequencies

                       -10

                            -5    -4                     -3  -2  -1  0       1      2              3  4

                                                                     f [Hz]

         Figure 12.7: Creating a symmetric spectrum for DFT result of Figure 12.6: moving the negative frequencies
         to the left and restoring the frequency axis through multiplying  by frequency step size .

EX 12.4  Repeat Example 12.3 for two cases. First, now use sampling frequency  = 20 Hz
         for the same duration  = 2 s, yielding the finite-duration sequence . Second, use
         the original sampling frequency  = 10 Hz, but increase duration  to 4 s, resulting
         in sequence . Compute  and  and comment on the differences with  from
         Example 12.3.

         Solution Since the cosine is a real, even function, the imaginary part of ,  and 
         is zero (for all cases) and is discarded here. Figure 12.8 shows the three sequences ,
          and  at left and their DFTs ,  and  at right, after performing the frequency
         shift operations explained in Example 12.3. The Nyquist frequency is discarded.

            The middle row shows that doubling the sampling frequency  for constant signal
         duration  results in twice the number of samples . The frequency step, however,
         does not change and  remains 1 = 1 Hz. What does change is that we can study the

                                                                     2
12.6 Window and leakage - revisited                                                        133

DFT  of sequence  from  = - 2 = -10 Hz to 2 = 10 Hz, twice the range of the
analysis frequencies compared to  (top). Increasing the sampling frequency does
not yield smaller DFT frequency steps. Rather, it places the copies of the spectrum at

higher frequencies, allowing a larger range of frequencies for evaluation.

    1                                                2

xn  0 T = 2 s fs = 10 Hz 1                       Xk

    -1                                               0

        0    1  2                                       -5  0                       5

    1                                       2

yn  0 T = 2 s fs = 20 Hz 1              Yk

    -1                                      0

        0    1  2                           -10         -5  0                       5  10

    1                                                2

zn  0 T = 4 s fs = 10 Hz 1                       Zk

    -1                                               0

        0    1  2      3             4                  -5  0                       5

                t [s]                                       f [Hz]

Figure 12.8: Effects on the DFT of a unit-amplitude, zero-phase cosine. Only the real parts are shown,
the imaginary parts are close to zero. Top: sequence  and its DFT  from Example 12.3. Middle:
effect of increasing sampling frequency , sequence . Bottom: effect of enlarging measurement time
, sequence . The coefficient at the Nyquist frequency is discarded.

   The bottom row shows that doubling the signal duration  for constant sampling
frequency  also yields twice the number of samples. The frequency resolution has
improved, step size  becomes 14 Hz, half the step size of  (top row). What does
not change, is the range of analysis frequencies, which remains   (-5, 5) Hz. In-
creasing the signal duration leads to a better DFT frequency resolution, we get `more
DFT analysis frequencies per Hz', the DFT frequency step size becomes smaller.2

    In these examples, the cosine signal completed exactly an integer number of cycles  within
duration  in other words, its frequency was an integer multiple  of the DFT frequency step
size , (12.2). This periodic signal in  could be perfectly modeled using the intrinsically
periodic DFT building blocks, resulting in very clean DFTs with the `spikes' occurring at exactly
the right positions in frequency. In the next section the more general case will be discussed,
involving the - in practice often inevitable - effects of spectral leakage.

12.6 Window and leakage - revisited

Close inspection of the graph in Figure 12.1 at right reveals that the dots of the stems (eval-
uation of |,()|) are close, but not all exactly centered on the dotted lines (representing
()). We can imagine that the Fourier transform of a part of the signal, the time-windowed
signal () may differ from the Fourier transform of the full (infinite-length) signal ().

    As discussed in Chapter 8, in practice, a signal () is only available for a finite time
duration, and consequently the phenomenon of leakage was introduced. In Chapter 8 we

2Therefore, when 1 is the DFT frequency step,  =  is the DFT frequency resolution.
                                     
         134                                                                                           12. Discrete Fourier Transform

         discussed only continuous-time, continuous-frequency signals. In Figure 8.3 at right, we
         observed that the Fourier transform of the time-windowed signal looked a bit `wobbly'.

             In the present chapter, we work with time-windowed, discrete-time (sampled) signals, and
         evaluate the Fourier transform ,() at discrete frequencies. In Figure 12.1 we see that
         |,()| of the sampled and time-windowed signal, evaluated at the analysis frequencies,
         indicated by the stems, is very close to () = () (dotted), the Fourier transform of
         the sampled signal derived in Chapter 9. There are tiny differences due to leakage, as a
         consequence of the (rectangular) time window. In this example, the window is quite long
         compared to the main features of the signal, so the differences are small. As a general
         guideline we can assume that the longer the time window is, the smaller the differences, i.e.,
         the smaller the impact of the window on the spectrum.

EX 12.5  We return to Examples 12.3 and 12.4 to demonstrate the consequences of time-
         windowing a signal in the context of the DFT.

         Solution Recall that the Fourier transform of a unit-amplitude, zero-phase cosine func-

         tion with frequency 0, () = cos(20), consists of two Dirac delta impulses, both
         with a weight of a half, one at frequency  = -0 and one at 0 (5.16). The effects
         of time-windowing were demonstrated in Figure 8.4 and expressed by (8.3). Here we

         consider a unit-amplitude, zero-phase cosine at 0 = 4 Hz and one at 0 = 4.5 Hz, and
         we use a rectangular time window of  = 1 s. The magnitude spectrum |()| of the
         time-windowed signal is shown at left in Figure 12.9, for continuous-time continuous-

         frequency. Only a part of the (double-sided) spectrum is shown. As the window length

          = 1 s, () = sinc() has a main peak width equal to 2 Hz and the side lobes

         have a width equal to 1 Hz, (5.11).

              1.0                                                                                 1.0
                               4 Hz
                               4.5 Hz

              0.5
              |Xw(f )|                                                                                   |Xw(f )|  |Xk |
                                                                                  |Xw(f )|, |Xk|
                                                                                                  0.5 f

              0.0                                                                                 0.0

                   0123456                                                                             0123456

                                       f [Hz]                                                            f [Hz]

         Figure 12.9: Magnitude spectrum |()| of time-windowed unit-amplitude, zero-phase cosine at left, for
         a window length of  = 1 s, in blue for 0 = 4 Hz and in red for 0 = 4.5 Hz ( = 64 Hz). The DFT results
         of the sampled and time-windowed cosine, in magnitude, i.e., ||, are shown as dots at right.

            The graph in Figure 12.9 at right, using dots in blue and red, shows the magnitude
         spectrum || for the two cosines as obtained with the DFT (12.5) (and divided by ,
         see (8.3), see also (H.7) this scaling maintains the analogy with the Fourier transform
         of continuous-time ()). The cosine signal was sampled at  = 64 Hz (intentionally
         a large value was chosen, such that copies of the spectrum, see (9.10), are far away,
         and do not impact our analysis of leakage). As the window duration is  = 1 s, the
         frequency step is  = 1 Hz.

            The spectrum of the 4 Hz cosine, in blue, looks perfect: all values equal zero, ex-
         cept for a clear peak, with a value of 0.5, at  = 4 Hz, exactly as we would expect.
         Leakage will always occur, as soon as a finite window duration is used (as shown by
         the underlying blue curve), but in this case we were `lucky' to `sample' the spectrum
12.7 Zero-padding  135

exactly at the peak and at the zeros of the spectrum of the windowed signal ()
working with the DFT we do not notice leakage at all. The cosine frequency 0 = 4 Hz
is an integer multiple  of the frequency step  = 1 . This cosine wave fits exactly an

                                                                                              

integer number of  = 4 times in the window for the period 0 of this cosine, 40 = 

                                1

and hence 0 = 4  .
   For the cosine with a frequency of 0 = 4.5 Hz (red), the situation is different. With

a frequency step of  = 1 = 1 Hz, the analysis frequencies do not coincide with the

                                                

peak at 4.5 Hz instead, the spectrum is evaluated at  = 4 and 5 Hz. The other ||-
values are non-zero, as we happen to evaluate the spectrum at the peaks of the side

lobes of the windowed signal (). The spectrum, shown by the red dots, looks quite
different from the ideal situation, substantial spectral leakage occurs.

   Obviously, when performing spectral analysis in practice, we typically do not know

in advance whether the harmonics of interest will fit an integer number of times in

window  we need to be prepared to deal with leakage.

12.7 Zero-padding

The DFT uses as input  time samples of the signal,  with  = 0, ... ,  - 1, and evaluates
the DTFT ,() at  discrete frequencies in the frequency domain. The DTFT of the time-
windowed and sampled signal, sequence , is a continuous function of frequency .

    We could evaluate this function ,() at many more discrete frequencies than the default
 = 0, ... ,  - 1 times  this would require changes to the code for the numerical imple-
mentation of the DFT, for instance in Python. As a quick work-around we could instead add
samples with zero value to the DFT input sequence, and use again the default DFT algorithm.
This is referred to as zero-padding.

    In zero-padding, we add to the time-windowed and sampled signal, sequence , samples
with a value equal to zero, extending the signal, yielding a larger time-window duration ,
with  > , and thereby reducing the frequency step  = 1 to  = 1 < 1 . No new infor-
mation is being added (only zeros), and we are interpolating the spectrum, i.e., evaluating the
underlying function ,() with a smaller frequency step: as a result, resolution improves.

    Another application of zero-padding is to make the size of the input sequence  equal to a
power of two:  = 2. In that case, the much faster FFT algorithm can be used to compute
the DFT coefficients (see Section 12.2.4).

We continue with Example 12.1 and zero-pad sequence  from  = 4 s to  = 10 s.                     EX 12.6

Solution The result of zero-padding  is shown as a function of (restored, continu-
ous) time  on the horizontal axis in Figure 12.10.

   The corresponding DFT, by its magnitude ||, as a function of frequency  on the
horizontal axis, is shown in Figure 12.11 as stems. The frequency range covers [0, )
in both cases, but the frequency step size is clearly smaller for the zero-padded signal
at right: the graph is more dense. The dotted curves show |,()| as a continuous
function of frequency for  = 3 Hz and  = 4 s.
136                                                                                        12. Discrete Fourier Transform

                        1.0                                                           1.0

     xn                 0.5        T                               xn                 0.5                                 Tx
                                                                                                                 zeros
                                                                                      0.0
                        0.0                                                                   -2 0

                             -2 0     2         4  6  8                                               2          4  6         8

                                         t [s]                                                            t [s]

Figure 12.10: Time-windowed and sampled signal, sequence  for  = 0, ... ,  - 1, with  = 3 Hz,
 = 12 and time window length  =  = 4 s, at left, shown by the stems (identical to Figure 12.1 at

left), and zero-padded signal with a total window duration of  = 10 s,  = 30, at right. Samples are
shown as a function of the `restored' continuous time on the horizontal axis, i.e.,  = .

     |Xt,T (f )|, |Xk|  1.0                           |Xt,T (f )|  |Xt,T (f )|, |Xk|  1.0                              |Xt,T (f )|

                        0.5 f                                                         0.5 fx

                        0.0                                                           0.0

                             0                  2     4                                    0                     2            4

                                      f [Hz]                                                             f [Hz]

Figure 12.11: DTFT |,()| (dotted), evaluated at discrete frequencies shown by the stems ||, at
left, based on DFT of  with  = 4 s and  = 1 = 14 Hz (identical to Figure 12.1 at right), and, at
right, based on zero-padded input sequence with  = 10 s and  = 1 = 1 Hz sampling frequency
                                                                                                      10

 = 3 Hz. DFT coefficients are shown as a function of `restored' continuous frequency on the horizontal

axis, i.e.,  = .

    With the DFT defined, the entire procedure of Fourier-transforming finite-length, discrete-
time signals, typically sampled measurements, has been covered. In Part IV, we discuss
how the DFT is used to determine power spectral density functions of deterministic signals in
Chapter 13, of random signals in Chapter 14, and of non-stationary signals in Chapter 15.
    IV

Spectral estimation

137
          13

   Energy and power spectral density

In this chapter we introduce the Energy Spectral Density (ESD) () and the Power Spectral
Density (PSD) () both are a continuous function of frequency . They show, respectively,
how energy and power of a signal are distributed over frequency. We start in Chapter 13 with
deterministic signals, both in continuous time and discrete time. Chapter 14 covers spectral
estimates of finite-duration discrete-time sample sequences of random signals.

13.1 Introduction

The concepts of energy and power, in the context of signal processing and analysis, originate
from electrical engineering. Energy refers to the total amount of work a signal can do. The
energy of a continuous-time signal () equals the integral of the square of the signal over
time, see (2.17). Power is the rate at which energy is transferred, delivered or dissipated. As
(2.18) shows, power equals energy per unit time. Obviously, signal amplitude drives energy
and power. For a cosine signal, doubling the amplitude increases the power by a factor of 4.

    Figure 13.1 shows a basic electrical circuit with a power source (e.g., a battery) and a
resistor (e.g., a light bulb). When a voltage () is applied over a resistor (resistance ),
current () starts to flow and the resistor will dissipate heat. According to Ohm's law () =
(), the electric current () is directly proportional to the voltage (), with resistance 
being the proportionality constant. For convenience we refer, in spectral analysis, to a  = 1 
resistor, such that () = () = (). This is referred to as `unit resistance'.

    With signal () being a voltage in [V] (Volt) or a current in [A] (Ampere), energy  in
(2.17) is expressed in Joule [J], and power  in (2.18) in Watt [W] Watt is equal to Joule per
second (all in relation to unit resistance). Signal power is the product of voltage and current
(2.18) expresses the time-averaged value of the instantaneous power.

Figure 13.1: Electrical circuit with a power source at left and a resistor at right. According to Ohm's law, voltage
(), current () and resistance  are related through () = ().

                                                           139
140                                   13. Energy and power spectral density

13.2 Continuous-time signals

We start with signals in continuous time and discuss periodic signals, which are typically power
signals, and aperiodic signals, which are generally energy signals. These signals have, respec-
tively, a power spectral density and energy spectral density, functions which describe how
signal power and energy are distributed over frequencies. We then consider signals which
have a limited duration in time, which generally have limited energy and zero power, and
show how a power spectral density can still be defined for these signals.

13.2.1 Power spectrum and power spectral density
The average power of periodic signal () with period 0 (see also (2.19)) is:

     1 =
      =  |()| d =  || 2         2                                            (4.16)

     0 0                 =-

     Proof     For a real, periodic signal () we have:
             
               = 1  2() d = 1  ()() d
                  0 0           0 0

                  1      =                              =        1
                                                            ( 
               =  0 0    () (   0) d  =                    0 0      ()0 d)   (13.1)

                         =-              =-

     where we substituted the complex exponential Fourier series, (4.2), and then changed the

     order of summation and integration. Substitute the complex conjugate of  for the expres-
     sion between parentheses on the right-hand side of (13.1) and we obtain:

                  =      =

      =   =  ||2

                  =-     =-

    For periodic signals, plotting ||2 as a function of th harmonic 0,   , yields the
(discrete) power spectrum, a line spectrum (Section 3.6). The average power is obtained by
adding up the contributions at the discrete frequencies of the line spectrum, see (4.16).

    The power spectral density function () of periodic signal () is a continuous function
of frequency, defined as:

                                                                             (13.2)

     () =  ||2( - 0)

                  =-

The Dirac delta function is used to put the contribution to the (average) power of the th
harmonic ||2 at frequency 0, while for all other frequencies () is equal to zero. Compare
this result with (6.12). The proof is given in [12].

    The (average) power  follows from integrating the power spectral density function ():

     

      =  () d                                                                (13.3)

     -

When signal () has unit [V], its average power  (on a 1 Ohm basis) has unit Watt [W] and
the unit of the power spectral density function () is [W/Hz].
13.2 Continuous-time signals                                                                              141

Show that signal () =  cos(20), with period 0 = 10 , has a line spectrum, and                                     EX 13.1
determine the PSD () of this signal.

Solution The complex Fourier series coefficients are -1 = 1 = 2  all other coeffi-
cients are zero. Signal () has a double-sided amplitude line spectrum, with amplitude
2 at frequency -0 and amplitude 2 at frequency 0.

    Showing ||2 as a function of , which refers to frequency 0, yields the (discrete)
power spectrum - in this example, |-1|2 = |1|2 = 2 4 , see Figure 13.2 at left. In
Section 4.4 it has been shown that, for a real signal (), the amplitude spectrum is

even, || = |-|, and so is the power spectrum.
The (average) power  follows from (4.16):  = 2 + 2 = 2 . If signal () is a
                                                                                    4  4      2
voltage [V], power  (normalized to 1 Ohm) has unit Watt [W]. Introducing a non-zero

phase  into the signal () =  cos(20 + ) does not change the amplitude and

power spectra.

    The power spectral density function of cosine signal () can be written as () =

2

 4 (( + 0) + ( - 0)) - see [12] for a derivation - and it is shown in Figure 13.2
at right. The Dirac delta function appears in the frequency domain: (). It has an

infinitesimally short `duration', and yet a unit area (see Appendix B). With signal () a

voltage, power spectral density function () has unit [W/Hz].
The (average) power  follows from (13.3):  = 2 + 2 = 2 W.
                                                                                4      4   2

0.3                                                                        0.3

0.2                                                                        0.2
|Xk |2
                                                                    S(f )
0.1                                                                        0.1

0.0                                                                        0.0

     -4 -3 -2 -1 0 1 2 3 4                                                      -4     -2        0  2  4
                              k
                                                                                           f [Hz]

Figure 13.2: Power (line) spectrum ||2 of cosine signal () =  cos(20) at left, and its power
spectral density () at right, for  = 1 and 0 = 1 Hz.

13.2.2 Energy spectral density
Aperiodic signals () have a Fourier transform (). Their energy can be calculated as:

                

 =  |()|2 d =  |()|2 d                                                                                    (5.19)

     -          -

where the energy spectral density function () is defined as:

() = |()|2                                                                                                (13.4)

The total energy  is obtained by integrating () over all frequencies:                                     (13.5)

                    

        =  () d

                 -
         142                                                                             13. Energy and power spectral density

         When signal () has unit [V], () in (5.1) has unit [V/Hz] its energy  (on a 1 Ohm basis)
         has unit Joule [J] and the unit of the energy spectral density function () is [J/Hz].

              Proof For a real, aperiodic signal () the energy can be expressed as:

                                                                                                

               =  ()2 d =  ()() d =  () (  ()2 d) d

                   =-                  =-                =-                                     =-

              where we substituted the inverse Fourier transform, (5.2). We can move () within the
              integral (in ) between parentheses and then change the order of integration:

                                                                                              

               =   ()()2 d d =  (  ()()2 d) d

                   =- =-                                 =- =-

              We can now move () out of the integral (in ) between parentheses and obtain:

                                                                                                          

               =  ()  ()2 d d =  ()() d =  |()|2 d

                   =-     =-                             =-                                         =-

              using the Fourier transform's complex conjugate, (5.1), with () = () for a real signal.

EX 13.2  Determine the ESD () and the total energy  of continuous-time, deterministic, ape-
         riodic voltage signal () = sinc2().

         Solution The Fourier transform is () = (), see Figure 8.2. The energy spectral
         density () = |()|2 is quickly found as:

                           (1 + )2     for      -1   < 0
              () = { (1 - )2           for       01
                                       other
                           0                          

         Signal () and its energy spectral density () are shown in Figure 13.3.

              1.0                                                                        1.0

              0.5x(t)                                                                    0.5
                                                                                  G(f )

              0.0                                                                        0.0

                   -3 -2 -1 0                1  2     3                                       -6 -4 -2 0    2  4  6

                                    t [s]                                                           f [Hz]

         Figure 13.3: Aperiodic signal () = sinc2() at left, and its energy spectral density () at right.

         The total energy  of () is found with (13.5):

                                    0              1
               =  () d = (1 + )2 d + (1 - )2 d = 1 + 1 = 2 J
                                                                                                33 3
              -           -1                       0

         This aperiodic signal () = sinc2() is clearly a finite-energy signal, and has zero
         (average) power:  = 0.
13.2 Continuous-time signals                                                                                             143

    In Section 5.6 it has been shown that for a real signal () the amplitude spectrum is an
even function of frequency, |()| = |(-)|, and so is the energy spectral density ().
The default in this book is that we work with two-sided, or double-sided spectra, although we
may show only one side, typically for positive frequencies   0, to save space. We must not
forget that in calculating the power (or energy) of a signal to include the positive and negative
frequencies: we must integrate from  = 0 to , then multiply by 2.

13.2.3 Finite-duration signals

For an infinite signal duration or record,   , which is only possible in theory, as the signal's
total energy is finite the average power is zero, (2.18), and so would be its PSD (). For
a finite signal duration, however, with power defined as energy per unit time, we often still
consider the PSD (), obtained by dividing the ESD () by the signal duration :

() = lim |()|2                                                                                                           (13.6)
          

(see for a derivation [5] and [12]). Here, () indicates the Fourier transform of signal ()
time-truncated to duration , i.e., () = (  )(), and therefore it equals zero outside the

                                                        
interval [- , ], as in Chapter 8, and ()  ().
       22
With (13.6) the average power  of signal () over interval  can be computed using

(13.3). Note that only power signals truly possess a power spectral density, but since they have

infinite energy this challenges the existence of the Fourier transform integral (5.3). Definition

(13.6) uses time truncation and can be applied to physically-realizable signals.

We continue with Example 13.2, consider the (finite) time duration of the signal () =                                            EX 13.3
sinc2() and compute the energy and power spectral densities.

Solution Figure 13.3 at left shows that signal () = sinc2(), though in principle
running from  = - to  = , decays to zero on both sides therefore it can be
Fourier-transformed and integrated. In practice we work with a finite signal duration
 the top image of Figure 13.4 shows the ESD () for two time-windowed versions
of ():  = 2 s (left) and  = 10 s (right) these graphs were created in discrete time.
Considering the signal over a longer time window yields more energy.

G(f )  1.0                                                 G(f )  1.0
                                                 T =2s                                                      T = 10 s

       0.5                                                        0.5

       0.0                                                        0.0

            -6 -4 -2 0        2  4                      6              -6 -4 -2 0  2  4                               6

                f [Hz]                                                 f [Hz]

S(f )  1.0                                                 S(f )  1.0
                                                 T =2s                                                      T = 10 s

       0.5                                                        0.5

       0.0                                                        0.0

            -6 -4 -2 0        2  4                      6              -6 -4 -2 0  2  4                               6

                f [Hz]                                                 f [Hz]

Figure 13.4: ESD () (top) and PSD (bottom) of signal () = sinc2(), time-windowed to  = 2 s at
left and to  = 10 s at right.

Dividing () by  yields the PSD (), the bottom image of Figure 13.4.
144                                13. Energy and power spectral density

13.3 Discrete-time signals

We limit the discussion to finite-duration discrete-time signals, which have the most practical
relevance. For infinite-duration discrete-time signals and applications of the DTFT in computing
their energy and power spectral densities, we refer to other books, e.g., [3].

    In practice, we deal with finite signal duration  = :  samples , with  = 0, ... , -1,
of signal (), with fixed sampling interval . Because these signals have finite duration and
are, in most practical situations, limited in amplitude, these signals are by definition energy
signals (Section 2.6). Therefore, we start with defining energy and the energy spectral density,
and then use the insights gained in Section 13.2.3 to move on to defining power and the power
spectral density for limited-duration, discrete-time sequences .

13.3.1 Energy spectral density

In discrete time, energy can be defined through discretizing integral (2.17):

                        

             2          2

      = lim  ||2 =  lim  2
                      
       =-               =- 

               2                2

with  =  and  =  (and assuming even  for convenience). For a finite-duration, real
discrete-time sequence, , running from  = 0 to  =  - 1, energy can be computed as:

                  -1                                                           (13.7)

      =   2

                  =0

    Parseval's theorem, stating that energy can also be computed in the frequency domain
(see Section 5.9), also holds in discrete time. We can discretize integral (5.19):

             -1                                                                (13.8)

      =  ||2

             =0

with  the DFT of sequence , (12.5), and  = 1 . We obtain:

                -1                                                             (13.9)

      = 1  ||2

                =0

This leads to the following definition of the energy spectral density in discrete frequency:

      = ||2                                                                    (13.10)

    The energy  can be obtained in the frequency domain by adding up  for  = 0, ... ,  -1,
then multiplying by frequency spacing  = 1 , (13.9).

                                                                                 
13.3 Discrete-time signals                                             145

Proof Consider (13.7) for a real sequence we have  =  . Substitute for  the
inverse DFT of , (12.6), and for  also its inverse DFT but using index  rather than :

                      11          -1      2  1 1  -1

2 =  =  =                      (    )                  - 2 
                                                  (     )
                                           
                                  =0              =0

Multiplying the two summations, and expanding them (and for the moment leaving out factor
()2 1 in front), leads to two sums. The first is a sum of  terms with equal indices ( = ):
 for  = 0, ... ,  - 1, where the associated complex exponentials cancel each other out
(  2 -  2  = 0 = 1). The second sum is a sum of 2 -  = ( - 1) cross terms with

                             (-) 2    for ,  = 0, ... ,  - 1 with   .
unequal indices (  ): 
Including summation =0 -1 of (13.7), we get for the first sum:

-1               1   -1 -1                -1             -1
                                         2 1   ||2 = 1 1  ||2
 2 =                2    =        

=0           ()      =0 =0            ()  =0             =0

proving expression (13.9) for energy , provided that the summation over  of all the cross

terms, i.e., the second sum, equals zero.
     Including summation =0 -1 of (13.7), all cross-terms together yield (leaving out constant

factors):

-1 -1        -1          (-) 2  

                  

=0 =0        =0
             

Focusing on the first summation and the complex exponential, we find that =0 -1 (-) 2  =
0 for   , with ,   {0, ... ,  -1}, similarly as in Appendix H, by using the geometric series
identity (A.22) with  =  and using 2 (-)  as the summation index (rather than ).

13.3.2 Power spectral density

Finite-duration signals are, by definition, energy signals: their average power is zero. The

average power of a discrete-time sequence  during the signal duration can be computed
through dividing the signal energy , (13.7), by signal duration  = :

1                -1         1     -1

= =          (  ) =  2                2                                (13.11)

                 =0               =0

The power of a discrete-time sequence ( samples) can also be computed in the frequency
domain, the equivalent of (13.9):

      1 -1 ||2                                                         (13.12)
= 

           =0

This leads to the following definition of the power spectral density at discrete frequencies, for
a discrete-time sequence :

       | |2                                                            (13.13)
 =

         
         146                                                                             13. Energy and power spectral density

         with  from the DFT, (12.5). This expression is also referred to as the periodogram.
             The average power  can be obtained in the frequency domain by adding up  for  =

         0, ... ,  - 1, then multiplying by the frequency spacing  = 1 , see (13.12).

                                                                                                                       

EX 13.4  We continue with Example 13.2 and consider  = 12 discrete-time samples of signal
         () = sinc2(), with  = 1 s and window length  = 4 s, as shown in Figure 13.5 at

                                                          3

         left, and determine the periodogram.

         Solution The periodogram for  = 12 discrete-time samples of signal () = sinc2()

         is shown in Figure 13.5 at right. The dotted curves show the infinite-duration,

         continuous-time signal (at left) and the corresponding theoretical, infinite-duration sig-
         nal energy spectral density divided by signal duration, () (at right).

                                                                                                               

              1.0                                                                        0.3

                                                                                  0.2
              0.5

                                                                                  0.1
              xn
                                                                                  S(f )

              0.0 0.0

                   -3 -2 -1 0  1  2  3                                                        -3 -2 -1 0  1  2  3

                   t [s]                                                                      f [Hz]

         Figure 13.5: Discrete-time signal  (left), as a result of sampling signal () = sinc2() - the same signal
         as in Figure 12.1 ( = 12,  = 3 Hz,  = 4 s) - and its periodogram (right), evaluating the PSD () at
          discrete frequencies, with  = 1 , from [- 2 , 2 ), rather than [0, ) in Figure 12.1 at right.

             This chapter has defined the energy spectral density and power spectral density functions
         for deterministic signals, in continuous time and discrete time. The following chapter discusses
         how to obtain estimates of these spectra, especially the PSD, from a finite-duration, discrete-
         time sequence  of a random signal.
          14

                         Spectral estimation

So far, we worked with deterministic signals. However, signals measured in practice are subject
to (uncontrolled) variability, whether small or large, both in the phenomenon itself and in the
measurement of it. We refer to this variability as noise. Signals which include noise are called
random signals. Appendix K presents a few statistical concepts for random signals, but by no
means embodies a full account of the subject the reader is referred to, e.g., [5] or [13].

    In this chapter we work with random signals and estimate the Power Spectral Density
(PSD). The resulting PSD estimate is denoted with a `hat' symbol, () or , to indicate that
it results from a measured signal which exhibits variability due to noise. Repeating the experi-
ment and determining the PSD again will result in a different estimate due to the uncontrolled
variability. The most practically relevant statistical properties of the spectral estimator are
covered in this chapter.

14.1 Assumptions

The random signal () is assumed to be ergodic. This means that a time average of a real-
ization of the signal equals the corresponding ensemble average (the statistical expectation).
Ergodicity implies stationarity, see Appendix K. The random signal is assumed to be stochas-
tically continuous and to have a mean of zero [5]. The latter means in practice that a trend in
the measured time series of the signal (e.g., an offset and/or slope) should be removed from
the signal prior to spectral analysis.

14.2 Power spectral density

As [5] points out, the very nature of `stationarity' suggests that a realization of a random signal
will almost certainly not `decay' at infinity, and therefore violate (5.3). Therefore, as in the
previous chapter, a time-truncated version of signal () is considered: () =  (  ) (),
which equals to zero outside the interval [-  ,  ] the signal has been time-truncated to a

                                                                                   22

duration of  seconds. () denotes the Fourier transform of random signal () and is
itself a random function as well () considered at a single, discrete frequency is a random
variable, and its variability is described by a statistical distribution.

    Every time we observe a new realization of the random signal, and subsequently compute
the Fourier transform of the time-truncated realization, it looks different. This aspect needs
to be dealt with in the definition of the PSD.

                                                           147
148                               14. Spectral estimation

                          [W/Hz]

                                  [Hz]

Figure 14.1: Example of a periodogram of a discrete-time signal, consisting of  = 8 samples. The estimated peri-

odogram concerns a density, and is evaluated at  = 8 discrete frequencies , with  = -4, -3, -2, -1, 0, 1, 2, 3,
covering the frequency range [-  ,  ).

                                                                  22

For a continuous-time random, time-truncated signal (), the PSD1 is defined as:

     () = lim  ( |()|2 )                (14.1)
       

see [5], [12]. The above expression is similar to (13.6) for a continuous-time determinis-
tic signal in the previous chapter, though in the above expression the statistical expectation
operation  is introduced, as we deal with a random signal here. This statistical definition
would practically imply taking the mean over all realizations of the random signal (rather than
depending on just a single realization).

    In practice, we have to be satisfied with an estimate for the PSD, indicated by the hat
symbol, computed on the basis of a single realization, of finite duration, of the random signal:

       () = |()|2
                      

with () the Fourier transform of the measured finite-duration signal.

14.3 Periodogram

We focus on discrete-time, random signals (sequences), as this is applicable in practice. Based
on  samples 0, ... , -1 of a single realization (of finite duration) of the random signal, we
compute the DFT 0, ... , -1 (12.5), and the estimate for the PSD reads:

     () =  = 1 ||2                      (14.2)
                       

with  = 0, ... ,  - 1 for frequencies  - the analysis frequencies - with  = 1 (see

                                                                                                                                                             

Chapter 12). This expression is identical to (13.13) in the previous chapter. Estimate (14.2)
is referred to as the periodogram (in Python: scipy.signal.periodogram).

    Figure 14.1 shows an example of a periodogram, for  = 8. By default, the DFT through
fft yields  for  = 0, ... ,  - 1, as shown, for a different example, in Figure 12.2 at right.

1In practice it is common to denote, by means of indices, to which signal the PSD refers, hence (), in this case
 with signal () or actually the time-truncated signal () a double index is used, because the cross-spectral
 density of two different signals, e.g., () and (), might also be considered (this is beyond the scope of this

 book). (14.1) is sometimes referred to as the auto-PSD. In this book, as we work solely with signal () or its

 time truncation, we omit these indices.
14.3 Periodogram                                                                          149

In Figure 14.1, the estimates  for  = 4, 5, 6, 7 have been moved to the left (e.g., using
fftshift), now representing respectively  = -4, -3, -2, -1, to yield a symmetric graph,
centered at  = 0, as explained in Figure 11.2. As (13.3) states, the (average) power  follows
as the integral over the PSD (). Equation (13.12) states that, with a discrete frequency
estimate, as with periodogram (14.2), the power is obtained through discretization of the
integral as the sum of the estimates  multiplied by frequency step  = 1 .

    The term periodogram was introduced in [14] with the purpose of searching for periodicities
in a measured signal, hence concerning discrete spectra based on Fourier series.

The application of the periodogram is demonstrated for a periodic discrete-time voltage                      EX 14.1

signal. We use a plain cosine signal () = cos(20), with 0 = 1 Hz, without and with
noise, shown in Figure 14.2. The sampling frequency is  = 7 Hz and the measurement
duration  = 10 s. Note that the discrete-time samples (black dots) are connected here

by line segments for clearer visualization (it is not a continuous-time signal).

     1                                            1

xn   0                                       xn   0

     -1                                           -1

         0      2     4         6  8     10            0     2     4         6  8     10

                         t [s]                                        t [s]

Figure 14.2: Cosine signal in discrete time: at left, deterministic, without noise, and at right with noise
(additive white Gaussian noise with standard deviation  = 0.1) 0 = 1 Hz, unit amplitude.

Solution Figure 14.3 shows the estimated periodogram, on a linear scale (top row)
and logarithmic scale (bottom row, expressed in dB (Section 2.5)). At left for the de-
terministic cosine signal, and at right for the random cosine signal (i.e., the cosine with
noise added). The periodogram estimates (the black dots at the discrete frequencies
) are connected by blue line segments for clearer visualization.

          2                                            2

     S^k                                          S^k

          0                                            0

             0     1            2     3                   0     1            2     3

                         f [Hz]                                       f [Hz]

          0                                            0

S^k  -150                                    S^k  -150

     -300                                         -300

             0     1            2     3                   0     1            2     3

                         f [Hz]                                       f [Hz]

Figure 14.3: Periodograms of a cosine signal: at left for a deterministic signal, at right with noise (this
corresponds to Figure 14.2). The PSD magnitude is shown on a linear scale (top row, in [W/Hz]) and
logarithmic scale (bottom row, in [dBW/Hz]).

     The theoretical PSD of a continuous-time 1 Hz cosine signal was shown in Fig-
         150                                                                                      14. Spectral estimation

         ure 13.2 at right, with a single `peak' at a discrete frequency, here 1 Hz (and equally
         at the negative frequency). In all graphs in Figure 14.3 we see a peak at 1 Hz, with a
         magnitude of 2.5 W/Hz (exactly at left, approximately at right). The effect of the noise
         can only clearly be seen when showing the periodogram on a logarithmic axis.

             Recall that the periodogram shows a power spectral density. Here, the frequency
         step equals  = 1 = 0.1 Hz. The corresponding power at a frequency of 1 Hz equals

                                         

         2.5 multiplied by  = 0.1, and hence is 0.25 W. By default we work with double-sided
         spectra, therefore the total power of this signal equals 0.5 W.

             Figure 14.3 at left shows that the PSD at other frequencies is around -300 dBW/Hz
         (which is very, very small mind the logarithmic scale). The numerical precision of digital
         computing is typically up to 15 significant digits: the 's for the other frequencies are
         close to zero and numerically at the 10-15 level, while the peak is at the 100 level. As
         a result, ||2 is approximately 10-30, which on a logarithmic dB-scale becomes -300
         (and you would still need to take into account the effect of the measurement duration 
         for a density). In the graph at right, the spectral density is at the -30 dBW/Hz level for
         other frequencies, due to the addition of white noise (with standard deviation  = 0.1).

EX 14.2  This example shows the periodogram applied to a zero-mean Gaussian white noise
         discrete-time voltage signal, see Figure 14.4 at left (later we will use a longer time
         window ). White noise is discussed in more detail in Appendix L.

         Solution The theoretical autocorrelation function, see Appendix K, in continuous time,
         for white noise, is () = 2(), with a noise standard deviation of . The correspond-
         ing theoretical PSD is () = 2, leading to a flat spectrum, such as the dashed line
         in Figure 14.4 at right. All frequencies contribute equally to the signal, in theory from
          = - to  (see Section L.1).

             The dashed line actually refers to a discrete-time white noise sequence, hence band-
         limited white noise, see Section L.2. The power equals the noise variance 2 = 0,
         and the theoretical PSD is given by (L.4), with one-sided bandwidth  =  (the Nyquist

                                                                                                                                            2

         frequency to either side), and the total two-sided bandwidth 2 = .
             The PSD estimate using the periodogram (14.2), shown in blue, is very noisy. On

         average it is close to the dashed line, but the estimated values at the individual discrete
         frequencies [0.0, 0.1, ... , 1.5) Hz with  = 1 = 0.1 Hz show a large degree of variability.

                                                                                     

                 0.5
                                                                                   0.004

              xn
                                                                                 S^k
              0.0                                                                     0.002

              -0.5                                                                    0.000

                    0  20 40 60 80 100                                                       0.0  0.5          1.0  1.5

                       t [s]                                                                           f [Hz]

         Figure 14.4: White noise signal in discrete time,  = 3 Hz,  = 10 s, simulated based on a Gaussian
         distribution with  = 0.1 at left as a function of time (sequence), at right the estimated periodogram of

         this sequence, with the discrete-time theoretical PSD, (L.4), as a dashed horizontal line (in [W/Hz]).
14.4 Statistical properties of the periodogram                                                             151

We continue Example 14.2 and apply a low-pass filter to the white noise discrete-time                           EX 14.3
signal, and again consider the periodogram.

Solution A first-order Butterworth low-pass filter with frequency response function
() given by (18.1), is applied to the white noise sequence. The result is shown in
Figure 14.5: at left the time series, where only a slow behavior remains (the high-
frequency variability of Figure 14.4 at left was removed), and at right the estimated
periodogram with the theoretical PSD (dashed curve, obtained using (16.15)).

0.5
                                                                  0.004

yn                                                                      0.002
                                                                   S^k
0.0

-0.5                                                                    0.000

      0  20 40 60 80 100                                                       0.0  0.5          1.0  1.5

         t [s]                                                                           f [Hz]

Figure 14.5: Filtered white noise signal in discrete time,  = 3 Hz,  = 10 s, and standard deviation
 = 0.1, with first-order Butterworth low-pass filter: at left as a function of time (sequence), at right the
estimated periodogram (in blue) and theoretical (dashed black) PSD, both in [W/Hz]. The bounds of the

95% confidence interval about the estimate are shown in green (lower limit) and red (upper limit).

     The (filtered) signal is subject to variability, and so is the periodogram estimate
based on samples of this signal. The periodogram estimate in blue generally follows
the dashed black curve of the theoretical PSD, though with a large degree of variability.
A confidence interval is used to show the uncertainty in the PSD estimate (J.8). In
Figure 14.5 at right the lower limit of the confidence interval is shown by the green
dotted line, and the upper limit by the red dotted line, for  = 0.05, i.e., a 95%
confidence interval.

14.4 Statistical properties of the periodogram

In Appendix J, especially (J.8), we assume that the periodogram is an unbiased estimator of
the spectral density (practically meaning that the estimates on average yield the true PSD).
Section 6.2 in [12] states, however, that the periodogram is a biased estimator due to the
truncation to , but that it becomes unbiased as   , i.e., the periodogram is asymptotically
unbiased (see also Section 6.2 in [5]).

    In Appendix J, (J.9) states that the standard deviation of the periodogram estimator, as
a measure for the uncertainty in the estimate, equals the target value of the estimate it-
self, meaning that the uncertainty is as large as the quantity of interest itself. Therefore the
periodogram is practically not of much use.

    In Figure 14.5 at right we see that for frequencies  > 1 Hz, the variability of the blue line
is very small, but in this case the dashed curve is already close to zero. For frequencies up
to 0.5 Hz, the variability (and deviation from the dashed curve) is much larger. The variability
scales directly with the magnitude of the target quantity.

    An expression for the confidence interval for the periodogram is derived in Appendix J,
using the Chi-squared distribution. The expression for the confidence interval and the stan-
dard deviation of the periodogram estimator in Appendix J, derived from Gaussian-distributed
samples of the signal, are independent of the measurement duration . This means that a
         152                                                                                      14. Spectral estimation

         longer measurement time  does not improve the periodogram estimator, i.e., the extent of
         variability remains unchanged.

EX 14.4  We continue Example 14.3 with filtered white noise and demonstrate that a longer
         measurement time  does not help in reducing the periodogram variability.

         Solution The first-order Butterworth-filtered white noise used in Example 14.3 is par-
         ticularly suited to demonstrate variability in the periodogram estimate, as we know that
         the theoretical PSD, the dashed curve in Figure 14.5, is a continuous, smooth curve (as
         a function of frequency). Hence, the variability in the periodogram estimate (in blue)
         with respect to the true curve (in black) is clearly shown.

              Initially, we may be inclined to think that using more data (i.e., a longer record)
         will improve the matter, i.e., reduce the variability. This is, however, not the case, as
         stated above. A longer measurement time will merely lead to an increased frequency
         resolution: `more points' of the PSD to be estimated. The variance (per `point') is not
         reduced by this it remains the same.

              The example of Figure 14.5 is repeated, but now for  = 100 s (instead of  =
         10 s as before we actually add 90 s of signal samples to the previous signal record).
         The extended signal record is shown in Figure 14.6 at left, and the corresponding
         periodogram at right. It is clear that a longer measurement record does not improve
         the variance of the periodogram estimator (the same level of variability is present as in
         Figure 14.5 at right in Example 14.3). However, we do see that the frequency resolution
         has improved (smaller frequency step size), by a factor of 10, to  = 1 = 0.01 Hz,

                                                                                                                                              

         and so the graph has become more dense.

              0.5
                                                                                0.004

              yn                                                                      0.002
                                                                                 S^k
              0.0

              -0.5                                                                    0.000

                    0  20 40 60 80 100                                                       0.0  0.5          1.0  1.5

                       t [s]                                                                           f [Hz]

         Figure 14.6: Filtered white noise signal in discrete time,  = 3 Hz,  = 100 s, with standard deviation
          = 0.1 and first-order Butterworth low-pass filter: at left as a function of time (sequence), at right the
         estimated periodogram (in blue) and theoretical (dashed, black) PSD, both in [W/Hz]. The limits of the

         95% confidence interval about the estimate are shown in green (lower limit) and red (upper limit).

         14.5 Welch periodogram

         Several approaches exist to improve the periodogram estimator. In this book we only present
         the Welch periodogram [15], as it is often used in practice.2

             The cornerstone of improving the periodogram is the use of segments. The total data
         record of length  is divided into  segments, each of length  (assuming here for conve-

                                                                                                                             

         nience that  can be divided by  using integer arithmetic), and a corresponding duration of
          =  . Segment averaging is suggested by [16].

         

         2After the American scientist and researcher P.D. Welch (1929-2023).
14.5 Welch periodogram                                                  153

                                                                     +

                        average

Figure 14.7: Diagram of the computation of the periodogram through averaging over  segments, as featured
in the Welch periodogram. The time (top row) and frequency (other rows) indices are absent in these plots to
prevent clutter.

    For each segment, a periodogram is computed, with the frequency step increased to  =
1 =  . Then, the  periodograms are averaged (per frequency). The variance reduces by

  

a factor of , but the price to pay is a poorer frequency resolution.

    Figure 14.7 illustrates the process of first dividing the measured discrete-time signal of

length  into  segments, shown on top for 1, 2, ... , , and then determining a periodogram
for each of them, with estimates (1), (2), through (). The final periodogram  is then
the average, per discrete frequency , with  = 0, ... ,  - 1, of the  segment-wise peri-

                                                                                                        

odograms, with  =  :

                                        

1  

 =     ()                                                               (14.3)

   =1

    In addition to segment averaging, the Welch periodogram [15] features the use of a win-
dow in each segment, e.g., a Hann window (see Chapter 8) and overlapping segments. The
Welch periodogram is available in Python through scipy.signal.welch. The confidence
interval for the Welch periodogram, with non-overlapping segments, is given by (J.10).

We continue Example 14.4 and demonstrate the Welch periodogram, for which the                                 EX 14.5
measurement record is divided into segments.

Solution The measurement record of  = 100 s at  = 3 Hz (Figure 14.6 at left), is
divided into  = 10 segments of  = 30 samples each. Therefore, the measurement

                                                              

duration of each segment is 10 s, and hence the frequency step size in the graph at
right, Figure 14.8, is  = 0.1 Hz.

     The graph of the Welch periodogram shows that, compared to the default peri-
odogram at left (for  = 100 s), the frequency resolution has become worse (larger
step size), but the variance of the estimator has become smaller by a factor of ,
thanks to the averaging.

    The graph at right shows a much reduced variability in the periodogram estimates
and also the confidence interval: the limits, shown in green and red, are much tighter.
154                                                                                       14. Spectral estimation

In other words, the Welch periodogram is closer to the true PSD, represented by the
dashed black curve.

     0.004                                                                    0.004

     S^k
                                                                         S^k
     0.002                                                                    0.002

     0.000                                                                    0.000

            0.0  0.5          1.0  1.5                                               0.0  0.5          1.0  1.5

                      f [Hz]                                                                   f [Hz]

Figure 14.8: Filtered white noise signal in discrete time,  = 3 Hz,  = 100 s, with standard deviation
 = 0.1 and first-order Butterworth low-pass filter. At left, the standard periodogram with  = 0.01 Hz

at right, the Welch periodogram (measurement record divided into  = 10 segments, each of length
 = 30 samples, with no overlap) with  = 0.1 Hz (in blue). The theoretical PSD is shown with the



black, dashed curve. All PSDs are in [W/Hz]. The 95% confidence interval limits about the estimate are

shown in green (lower limit) and red (upper limit).

    We covered spectral density computations for random and deterministic signals in this
and the previous chapter, assuming that the signal properties do not change over time. In
Chapter 15 we abandon this assumption and discuss the spectrogram, allowing for analysis of
non-stationary signals.
frequency 15

                                   Spectrogram

The basic periodogram estimate was given as (14.2) in the previous chapter. A non-stationary
signal is not suited as input to compute a periodogram straightaway, as the properties of the
signal change over time. In this chapter, we present the spectrogram as a simple means to
deal with non-stationary signals in practice.

    Figure 15.1 shows how a spectrogram is computed. The diagram looks very similar to
Figure 14.7, but the key difference is that here each segment is treated separately, whereas
in Figure 14.7 the  segments were averaged to obtain the eventual spectral estimate. When
using a spectrogram we assume that a non-stationary signal may still, by approximation, be
assumed to be `stationary over a short time span'. Per segment a periodogram is estimated,
the results are stacked together and a time axis is added to the figure. Along the time axis
we can see how the signal periodogram evolves as a function of time.

                                                                                                                                                       time

Figure 15.1: Diagram of computation of a spectrogram created by dividing the total signal measurement record
into  segments, and computing a periodogram for each segment separately. The time and frequency indices are
absent in these plots to prevent clutter.

    The DFT frequency step size  depends on the measurement duration :  = 1 using

                                                                                                                                                            

the entire measurement duration. With  segments, for each segment the step size  equals
 . This immediately demonstrates that the price to pay for improved frequency resolution



(smaller ) is reduced temporal resolution (longer segments), and vice versa. For improved
frequency resolution (a smaller step size ), each segment, and therefore the periodogram,
represents a longer time span, resulting in less frequent `updating' along the time axis.

    In Figure 15.2 the spectrogram at left, with  = 4, shows a better frequency resolution
(smaller frequency steps) than the one at right. The spectrogram at right, with a larger ,

                                                           155
         156                                                                                             15. Spectrogram

              frequency                                            frequency

                                                             time                                        time

         Figure 15.2: Trade-off between temporal and spectral resolution with the spectrogram. The total signal mea-

         surement record of duration  is divided into  segments, and a periodogram is computed for each segment
         separately. The segment duration (defining the temporal resolution) is  and the frequency step size (defining

                                                                                                                                                          

         the spectral resolution) is  =  . At left  = 4 at right  = 12.

                                                                         

         here  = 12, has a better temporal resolution (shorter segments). Once the total signal
         measurement duration  is fixed, we cannot have a high resolution in both the time and
         frequency domains, and the segment length needs to satisfy the assumption or approximation
         of the signal being (quasi-) stationary.

EX 15.1  We consider the spectrogram for a signal measured by a vibration sensor. The sensor
         was fixed (tightly bolted) onto a four-cylinder combustion engine. A signal was mea-
         sured for a duration of  = 7 seconds with the engine running, and the throttle was
         used to `speed up' or `rev' the engine.

         Solution The measurement record is divided into  = 7 one-second segments the
         frequency step  = 7 = 1 Hz. Figure 15.3 shows the resulting periodograms, com-

                                                 7

         puted for each segment, as `traces'. No overlap was used, and a rectangular window.

                                                                                        1.0

                                                                                        0.8              magnitude [mW/Hz]

                                                                                        0.6

                                                                                        0.4

              0              [s]                                                    50              0.2
                   1     time
                        2                                                                          0.0
                              3                                                                100
                                   4                                                    75
                                         5
                                               6                              25
                                                             -50 -25 0 quency [Hz]
                                                        -75        fre

                                                  -100

         Figure 15.3: Spectrogram of a signal measured with a vibration sensor on a running combustion engine.
         The  = 7 periodograms are shown by means of traces, next to one other the sampling frequency 
         was 200 Hz, the measurement time  = 7 s, and the segment duration  = 1 s.

                                                                                                                                                         
                                                                                                                                                                                 157

A bird's eye view of Figure 15.3 is shown in Figure 15.4 the (double-sided) peri-
odograms are shown in vertical direction, stacked next to each other along the time
axis. The value of the PSD is shown by the shade of blue, see the color bar at right. This
type of graph is commonly referred to as a `heat map'. The PSD values are expressed
in [dBW/Hz] values smaller than -60 dBW/Hz have been removed to allow for a clear
visualization of the peaks.

100

75                                      -35

50
                                                                                                                                              -40

25
frequency [Hz]
                                                                                                                                                             magnitude [dBW/Hz]0-45

-25
                                                                                                                                                 -50

-50
                                                                                                                                                 -55

-75

-100                                    -60

      0  1  2  3            4  5  6  7

                  time [s]

Figure 15.4: Spectrogram of a signal measured with a vibration sensor on a running combustion engine.
A bird's eye view of the time-frequency area in Figure 15.3 is shown using a color bar for the values of
the PSD (in [dBW/Hz]).

     During the time span from 1 to 4 seconds, the engine was revved, so that the
frequency of the peak in the spectrum increased, from around 50 Hz to over 75 Hz,
and also the signal amplitude became larger (the larger power density is shown as
darker blue). After 5 seconds the throttle was released again.

    The complete double-sided spectrum is shown in Figures 15.3 and 15.4. In practice,
we show only the part for positive frequencies, as the spectrum is symmetric.

    Python offers the short-time FFT scipy.signal.ShortTimeFFT, which computes se-
quential FFTs (DFTs) by sliding a time window over the input signal. In this way, it computes
the coefficients  for each segment, and the spectrogram (spectrogram), part of the above
ShortTimeFFT class, is then readily obtained through (14.2). Segments may partially over-
lap and a window may be used per segment, other than just a rectangular one.

    Now that the spectrogram has been discussed, the practical details of performing spectral
estimation have been covered. In Part V, we briefly discuss systems, focusing in particular on
the process of filtering signals.
     V

      Linear systems

159
          16

          Linear time-invariant systems

This chapter presents a brief introduction into the subject of systems in the context of signal
processing and analysis. The basic concept of a system is illustrated in Figure 16.1.

Figure 16.1: A system turns input signal () into output signal () through operation : () = {()}.

Some device, structure, or algorithm turns input signal () into output signal (). A system
can be considered as performing some `operation' on (), to produce ():

() = {()}       (16.1)

    First, some general system properties are introduced. The scope is then restricted to
the class of linear time-invariant (LTI) systems, which have one input and one output, in
continuous time. For these single-input single-output (SISO) LTI systems, the relation between
input and output can be fully captured by the system impulse response function () which,
when Fourier-transformed, is known as the system frequency response function (). Plotting
the magnitude and phase of () as a function of frequency yields the system's Bode plot, a
common, useful graphical illustration of the LTI system's dynamic response.

16.1 System properties

System models mathematically describe the interaction of signals (the arrows in Figure 16.1)
and systems (the box in Figure 16.1) and the relationships between causes (the system inputs)
and effects (the system outputs) for a system. Systems can have many inputs and outputs,
but we focus on SISO systems.

    Modeling the operation or behavior of a system often starts by developing a differential
equation (DE), e.g., based on physical laws that govern the system behavior. The resulting
relationship can then be used to characterize the system, in a variety of ways:

   1. Instantaneous or dynamic,

   2. Time-invariant or time-varying,

   3. Linear or nonlinear,

           161
         162                                       16. Linear time-invariant systems

            4. Causal or non-causal, and

            5. Continuous-time or discrete-time.

         We briefly discuss these properties, following [7]. Depending on the system characteristics,
         different mathematical techniques are available to analyze the system behavior.

         16.1.1 Instantaneous and dynamic systems

         A system for which the output is a function of the input at the present time  only is said to
         be instantaneous or static. When this is not the case, the system is called dynamic.

             Instantaneous systems are modeled by relations such as:

              () = () +  or () = () + 3()

         for constants  and   . We refer to instantaneous systems as `memory-less' or `zero-
         memory': their output does not depend on past (or future) input.

             For dynamic systems, the output depends on past and/or future values of the input, gen-
         erally in addition to present time. These systems `have memory' and can in some cases be
         modeled by relationships such as:

              d()  d-1()                           d ()  d-1()
          d + -1 d-1 +  + 0() =  d + -1 d-1 +  + 0() (16.2)

         where , ... , 0, , ... , 0 are constants   and non-zero initial conditions may apply. Whereas
         (16.2) describes a system governed by a linear, constant-coefficient, ordinary differential equa-
         tion (ODE) of order , this is more exception than rule. For real-life systems, the mathematical
         relationship between input and output is often more complicated.

             The DE may also contain integral terms on input and output. The order of a system is
         defined as the highest derivative of output () when all integrals in the DE are removed
         through differentiation of that DE, see Example 16.2.

EX 16.1  Consider the system described by () = 2(2). Is it instantaneous or dynamic? What
         is the order of this system?

         Solution Testing the input-output relationship for various values of time  quickly
         shows that this system is dynamic. E.g., for  = 2 s we find (2) = 2(4): the output
         at  = 2 s equals 2 times the input at  = 4 s. The order of the system is zero.

EX 16.2  Consider the system described by the DE:

                                               

              d()
                d + 2() =  () d

                                            -

         Is this system instantaneous or dynamic? What is the order of this system?

         Solution This system is clearly dynamic, as the integral on the right-hand side shows
         that the output depends on all past values of the input. To find the order, we first need
         to get rid of all integrals, which is done by computing the time derivative of the DE:

              d2() d()          The order of the system is 2.
                d2 + 2 d = ()
16.1 System properties                                          163

16.1.2 Time-invariant and time-varying systems
A system is time-invariant, or fixed, if its input-output relationship does not change with time.

Otherwise it is time-varying. In terms of (16.1), a system is time-invariant if and only if:

{( - )} = ( - )                                                 (16.3)

for any () and any   , and with the system considered to be `at rest' (all derivatives of
() are zero) before the input is applied. In other words, a system is time-invariant when, no
matter at what time  the input is given, its output stays the same (though equally time-shifted
as the input).

Consider system I described by d() + 2() = 5() and system II described by                                      EX 16.3

                                                               d

d() + ()() = 5(). Which of these two systems is time-invariant?

  d

Solution System I is time-varying. When applying an input () at  = 0 s, the system
dynamics at that time equal d() = 5(). When applying the same input at  = 4 s,

                                                       d

the system dynamics at that time equal d() + 16() = 5(). Clearly, the system

                                                                              d

dynamics change over time and the system will respond differently to the same input
signal. System II is time-invariant, since its DE coefficients are all constants.

16.1.3 Linear and nonlinear systems
For a linear system, the superposition principle holds:

{11() + 22()} = 1{1()} + 2{2()}                                 (16.4)

for any inputs 1() and 2() and any constants 1 and 2  .
    When the response of the system to 1() equals 1() and the response of the system to

2() equals 2(), the superposition principle is stated as:

{11() + 22()} = 11() + 22()                                     (16.5)

When (16.5) does not hold, the system is said to be nonlinear.

Prove that the (time-varying) system described by the DE d() + () = () is linear.                              EX 16.4

                                                                                                            d

Solution The response 1() to 1() satisfies d1() d +1() = 1() and the response
2() to 2() satisfies d2() + 2() = 2(). Multiplying by arbitrary constants 1 and

                                            d

2   and then adding them together (omit () for brevity) yields:

1 d1 + 2 d2 + 11 + 22 = 11 + 22
d  d

            d
        (11 + 22) + (11 + 22) = (11 + 22)

           d

Thus, the response to the input 11() + 22() equals 11() + 22(). The su-
perposition principle (16.5) holds, and therefore the system is linear.
164                                               16. Linear time-invariant systems

16.1.4 Causal and non-causal systems
A system is causal if its response to an input does not depend on future values of that input.

    An easy way to determine whether a system is causal is to note that () cannot do any-
thing in anticipation of (). It is non-anticipatory: the system output reacts to the input. All
systems that represent real-world phenomena, such as (combinations of) mechanical, biologi-
cal, and electrical systems, are causal. Chapter 18 on filters discusses examples of causal and
non-causal filters. A system must be causal to be applicable in real time.

    As an example, the input-output relation () = ( + 3) describes a non-causal system,
since the output depends on future input.

16.1.5 Continuous-time and discrete-time systems
A continuous-time system operates with continuous-time signals, and a discrete-time sys-

tem with discrete-time signals. Continuous-time systems and discrete-time systems are often
governed by, respectively, differential equations (such as (16.2)) and difference equations.
Chapter 18 discusses examples of both types of systems.

16.1.6 Linear time-invariant systems
Combinations of system properties yield particular system classes, with the linear time-invariant
(LTI) system as a common example. As the definition says, an LTI system is linear (the su-
perposition principle holds) and time-invariant (its input-output relation does not change over

time). LTI systems can be continuous-time or discrete-time, and may be instantaneous or
dynamic. In the remainder of this chapter, the discussion is restricted to LTI systems in con-
tinuous time, often characterized by (16.2). LTI systems are used extensively in signal and
system analysis for two reasons: mathematical techniques to study these systems are well
established, e.g., the Fourier and Laplace transforms, and in real life many systems are by
approximation linear and time-invariant.

16.2 System response

Once the differential equation is given, e.g., in the form of (16.2), how do we find the response
() of the LTI system, given an input ()? Several ways exist to answer this question.

    The first approach is to solve the DE through calculus. For a simple example, this approach
is demonstrated in Appendix M for the system discussed in Example 16.5.

    The second approach is through superposition. Without proof we state that, for LTI sys-
tems, output () is found as the convolution of input () with the so-called impulse response
function () of the system:

                                                  (16.6)

     () = ()  () =  ()( - ) d

                                               -

This convolution integral is also known as a superposition integral. For the example DE in
Appendix M, we can verify that the convolution expression above results as (M.8), with the
impulse response function () = -().

    The third approach is using the Fourier transform. Recall that the Fourier transform of a
convolution of two signals in the time domain equals the multiplication of the Fourier trans-
forms of these two signals, (6.8). Fourier-transforming (16.6) yields:

     () = ()()                                    (16.7)
16.2 System response                                                165

with () the Fourier transform of the impulse response function ():

() = {()}                                                           (16.8)

() is referred to as the frequency response function (FRF) of the LTI system. The response

of an LTI system is completely characterized by either () or () these functions play an

essential role in the analysis of systems.

    Regarding the units of () and (), the following can be stated. When input signal ()

has unit [in] and output signal () unit [out], then their Fourier transforms () and ()
have units [in/Hz] and [out/Hz], respectively. From (16.7) it follows that () = () , which

                                                                                                                                                     ()

means that () has [out/in] as its unit. () is the Fourier transform of (), so the unit of

the latter is [out/in/second]. In Chapter 17 some practical examples are given.

    In forward modeling, with a given input () and system impulse response function (),

after applying the Fourier transform to both, output () can be computed through (16.7),
then transformed back to the time domain to obtain (): () = -1{()}.

    In case we first need to find impulse response function (), e.g., from the differential

equation, this DE is first Fourier-transformed to the frequency domain. Through the relation
() = () , the frequency response function () is found, and can be inverse Fourier-

                ()

transformed to the time domain to obtain (). This is demonstrated in the next example.

For a system, described by the first-order ODE (from Appendix M):                                                                                        EX 16.5

1 d()                                                               (16.9)
 d + () = ()

with  > 0, find the impulse response function () by transforming the equation into
the frequency domain.

Solution With the Fourier transforms of, respectively, input () and output () de-
noted as () and (), Fourier-transforming the above differential equation is easily
achieved by using the differentiation Fourier transform theorem, (6.10):

        1 (2) () + () = ()
       

which can be rewritten as:

          + 2
       (  ) () = ()

The frequency response function equals:

()                    
() = () =  + 2                                                      (16.10)

With Example 5.3 the impulse response function () is found as:

() = -() for  > 0                                                   (16.11)

This function is identical to the expression for () derived in Appendix M. It was shown
in Figure 7.7 at right, for  = 1 and  = 3.
         166                                            16. Linear time-invariant systems

             In an experimental approach, when no differential equation of the system is available, we

         can attempt to recover the system impulse response function () by measuring both output

         and input signals (or by measuring the output to a known input), Fourier-transforming both,
         performing a so-called de-convolution, () = () , and inverse Fourier-transforming ().

                                                                                                 ()

         This procedure is easily defined in theory, but not straightforward to apply in practice.

         16.3 Impulse response function

         Function () is the impulse response function of the system, and for an LTI system it fully
         describes the input-output relation of that system.

             To explain this name of (), consider the system response () to the Dirac delta function,
         an impulse, at  = 0, i.e., () = (). The convolution (16.6) becomes:

                            

              () =  ()( - ) d = ()

                          -

         where we used the sifting property of the Dirac delta function, (B.12). The result implies that
         function () is the response of the system to a Dirac delta impulse as input, applied at  = 0
         hence the name impulse response function (of the system).

             Using the impulse response function (), we can easily determine a system's causality:
         () should be zero for  < 0, because () equals the system response to an impulse input
         at  = 0. For instance, the ideal reconstruction filter introduced in Chapter 10 is a non-causal
         system see Figure 10.2 (left). The ZOH reconstruction filter, Figure 10.9 (left), is causal.

EX 16.6  With the impulse response function of a system given as () = -(), compute the
         output () to the unit step function as input, i.e., to () = ().

         Solution This problem has been solved in Example 7.4 for  = 1. Figure 7.8 shows
         that, from  = 0 onwards the system gradually and asymptotically adjusts to the new
         state, i.e., the unit step function () = () applied at  = 0. The system is causal.

         16.4 Frequency response function

         Function () is the frequency response function of the system, the Fourier transform of the

                                                                                                                                                                    

         impulse response function (). These functions form a Fourier transform pair: ()  ().
         The frequency response function plays a pivotal role in the description of the LTI system
         response to a harmonic input.

             We start with a cosine with frequency 0, initial phase , and amplitude  as input:

              () =  cos(20 + )                          (16.12)

         and rewrite this using Euler's formula (C.1):

                () =  ((20+) + -(20+))
                          2

         Compute the output through convolution, (16.6), using the commutative property, (7.3):

                            

              () =   ((20(-)+) + -(20(-)+)) () d
                            2

                          -
16.4 Frequency response function                                               167

which yields:

                                                 
() =  20  -20() d +  -20-  20() d
               2                              2                                (16.13)

                  -                              -

                               = (0)               = (-0)

obtaining the Fourier transform (5.1) of (), once evaluated at frequency 0 and once at -0.
    Next, we denote the frequency response function () in polar form, (5.5), () =

|()|() with modulus |()| and phase () = arg () (also written as ()). If

impulse response () is real-valued (as in the case of any physically realizable system), the

magnitude is even |()| = |(-)| and the phase is odd, () = -(-). Rewriting (16.13):

() =  20 |(0)|(0)                     +   -20- |( )|-(0)
               2
                                                                            0
                                         2

and rearranging terms yields:

() =  |( )| ((20++(0)) + -(20++(0)))

                         0

         2

which, with Euler's formula, (C.1), becomes:

() =  |(0)| cos (20 +  + (0))                                                  (16.14)

We arrive at an important conclusion, namely that the (steady-state) response of an LTI system

to a harmonic on the input is a harmonic with the same frequency 0. With input (16.12), the
amplitude changes from  to  |(0)| and the phase from  to  + (0) in output (16.14).

16.4.1 System response: magnitude

In the frequency domain, the LTI system response follows as in (16.7). In Appendix C we
have the product of two complex vectors as  = (+), and hence || = |||| = ,

so that in terms of magnitude or amplitude spectrum we have:

|()| = |()()| = |()| |()|                                                      (16.15)

Using this relationship, it is straightforward to relate the energy spectral density functions of
the input and output signals:

|()|2 = |()|2 |()|2                                                            (16.16)

= ()              = ()

A similar relationship holds for the power spectral density functions of the input and output
signals: () = |()|2(), see [17].

16.4.2 System response: phase

For convenience and clarity we set initial phase  to zero in (16.12). The output (16.14) reads:

       () =  |(0)| cos (20 + (0)) =  |(0)| cos (20 ( + (0) ))
                                                                                                 20

clearly telling us that passing a harmonic through an LTI system will cause a phase shift (0).
A phase shift can also be interpreted as a time shift, (2.3): 0 = (0) 20 . When (0) > 0, 0 > 0,
the output will be ahead of the input, a time advance. Vice versa, when (0) < 0, 0 < 0,
the output will be lagging behind the input, a time delay.

    In system analysis we distinguish three types of phase responses:
168                                         16. Linear time-invariant systems

   1. Zero phase: (0) = 0, for any 0. This is true when () is real and positive. Hence,
       () is required to be an even function, which is only true for non-causal systems.

   2. Linear phase: (0) = 0 with constant   : the phase shift depends linearly on fre-
       quency 0 as a result, time shift (0) 20 = 2 is constant and the same for all frequencies.

   3. Nonlinear phase: any other behavior.

    In a system with a nonlinear phase response, signal components at different frequencies
may get time-delayed by different amounts, and as a result the output signal may be distorted.

16.4.3 Bode plot

When the frequency response function of the LTI system is known, a common and practical
way to illustrate the system dynamic response is using a Bode plot1. In a Bode plot, both the
magnitude response |()| and phase response () are shown as a function of frequency
, with the magnitude at the top and the phase at the bottom. Only positive frequencies
are shown,  > 0. The magnitude response is an even function of frequency. For the phase
response we should remember that this is an odd function of frequency.

    A logarithmic frequency axis (the horizontal axis) is often used. Magnitude is also typically
shown on a logarithmic axis. Phase is shown on a linear axis in [rad] or [deg]. Figure 16.2
shows the Bode plot for the LTI system of Example 16.5, for two values of .

          101                                                                     |H(f )| = 1
          100
|H(f )|  10-1                                                                =3
         10-2
         10-3            10-1       =1                                       101               102
                                      100
              10-2
                                    f [Hz]
H(f )       
             4                  =3
                         =1                                                  101               102
            0             10-1        100
                                    f [Hz]
         - 4
         -  H(f ) = - 2

             2

           10-2

Figure 16.2: Bode plot of the FRF of the LTI system characterized by DE (16.9) from Example 16.5 for two values
of , namely  = 1 (blue curves) and 3 (green dashed curves) phase is shown in [rad].

    The main application of LTI systems in this book is filtering, which is the subject of Chap-
ter 18. But before discussing filters, Chapter 17 will first explain what typical measurement
set-ups look like, including the (dynamic) characteristics of sensors.

1Named after H.W. Bode (1905-1981), an American engineer of Dutch ancestry.
                  17

                Measuring signals

Setting up an experiment and successfully collecting the intended measurement signal is an
art. It can be a serious challenge, requiring knowledge, insight and (practical) skills. As this
subject deserves a book on its own (and a lot of practice), we provide a concise introduction,
in continuous time, just touching upon the most important aspects. Throughout this chapter
we use an accelerometer as an example.

17.1 Sensor

A sensor is a device which delivers an output signal, most commonly an electrical voltage,
proportional to a physical quantity of interest, i.e., the phenomenon to be measured. A sensor
is a transducer, which, more generally, is a device that converts a signal of one type of energy
into another. For instance, the (classic) accelerometer in Figure 17.1 at left turns physical
acceleration (motion) into an electric voltage.

y(t)  0.05
                                                                                mean -0.007 [m/s2]

      0.00

      -0.05  0  2    4         6  8                                                                 10

                        t [s]

Figure 17.1: Left: basic outline of an accelerometer. Right: accelerometer measurements (in [m/s2]) obtained
with a smartphone, lying still on a flat and level concrete floor. The measurements, on one of the horizontal axes,
were obtained with a STMicroelectronics K6DS3TR sensor at  = 100 Hz, for a duration of 10 seconds.

    Within its housing (rectangle), a proof mass (in blue, mass ) is suspended by a spring
(spring constant ) and connected to a dashpot damper (damper constant ). The accelero-
meter is put on a horizontal surface, aligned with its input/output axis of interest. It senses
external motion, (input) acceleration () of the housing in [m/s2], through measuring the
displacement () of the proof mass (along one axis), a so-called single degree-of-freedom
system, and outputs a proportional electric voltage [V]. The proportionality is the sensitivity
of the sensor: the ratio of change in sensor output (voltage) and the change in sensor input
(acceleration). In case of the accelerometer, the sensitivity is expressed in [V/m/s2]. Ideally,
the sensor input-output shows a linear relation, materialized by just a single scale factor.

    The sensitivity is one of the key parameters of a sensor determining the measurement
precision. With a digital signal, resolution also plays a role, as the sensor outputs a discrete

                169
170                                                          17. Measuring signals

signal value (with a certain `step size' ) see Appendix D on quantization. Another key
parameter of a sensor is the dynamic range , indicating to which values of the input the
sensor sensitivity is restricted, i.e., which values of the physical quantity can be `handled
properly' by the sensor. Beyond this measurement range, the sensor is saturated, values may
get `clipped' and the measured signal may become seriously distorted.

    Figure 17.1 at right shows 10 seconds of accelerator measurements at a sampling rate of
 = 100 Hz, collected by a three-axis accelerometer sensor in a smartphone, lying still and
flat on a level and solid concrete floor, for one of the horizontal axes. The mean is -0.007
m/s2 close to zero, as expected, but not exactly zero.

    There is a small systematic offset, a bias. This is common in practice, and the experimenter
needs to be aware of it. In this case it could be caused by a slight misalignment of the
sensor's axes (in its housing or the smartphone casing, such that it is not perfectly horizontal,
and actually senses a tiny component of gravity). In addition, it is not uncommon to see,
over long time spans, some slow trend or drift in the observed signal, for instance due to
temperature effects in the sensor. A calibration may remedy a bias as well as drift effects.

    Finally, random fluctuations in the output signal occur, a phenomenon which we call noise,
for instance caused by random motion of electrons in electrical circuitry (thermal noise). The
measured acceleration signal in Figure 17.1 is noisy the empirical standard deviation is 0.005
m/s2 (actual minor background vibrations in the set-up cannot be entirely ruled out, but likely
the measurement noise is dominant in this experiment). Note that in practice, due to noise,
the precision of the sensor is (considerably) worse than its resolution - in the accelerometer
example the difference is about a factor of 5.

17.2 Frequency response

A sensor may be effectively sensitive only to input signal values in a certain range (dynamic
range , as mentioned above). The sensitivity may also depend on the frequency of the input.
Consequently, the frequency response function of the sensor is an important element of the
sensor specification. In this section we study the FRF of an (ideal) accelerometer.

    We start from the theoretical model of motion for an ideal accelerometer, as shown in
Figure 17.1 at left, on a perfectly horizontal surface. In this section and the next one, as is
common in dynamics, we use angular frequency  = 2, rather than linear frequency .
The motion of the mass-spring-damper system is described by a second-order ODE:

     ()  +  2  0  ()  +  2     ()  =     -()                 (17.1)

                            0

with both the positive  and  axis pointing right. The sign on the right-hand side is negative,
because an acceleration of the housing to the right causes the proof mass to move to the left

relative to its housing. In (17.1),  =  2 is the (dimensionless) damping ratio and 0 =  
the undamped natural angular frequency in [rad/s]. Parameter  is the mass in [kg],  the
damper constant in [kg/s] (= [Ns/m]), and  the spring constant in [N/m].

    As explained in Chapter 16, this linear, constant coefficient differential equation can be
solved by transformation to the frequency domain. We are interested in the relation of dis-
placement output () and acceleration input (). Using the differentiation Fourier transform
theorem, (6.10), differential equation (17.1) can be written as:

     ()2()     +  20     ()()         +  2     ()  =  -()    (17.2)

                                            0

with () the Fourier transform of (), and () of (). This is an algebraic equation in ,

from which the frequency response function can be obtained:

            ()                     1
     () = = 2 2                                              (17.3)
            ()  - 0 - 20()
17.3 Impulse response                                                         171

|H ()|  101
                     |H()| = 1

        100

        10-1                                                     0 = 1 rad/s
        10-2
                                      10-1                100                 101
             10-2

                                                 [rad/s]

              

        H ()         = 0.001

              2  = 0.2

                     = 0.6

              0

              10-2                    10-1                100                 101

                                                 [rad/s]

Figure 17.2: Bode plot of FRF () of the accelerometer of Figure 17.1 (left), with magnitude spectrum |()|

(top) and phase spectrum () (bottom, in [rad]), for three values of damping ratio  the undamped natural
frequency is set to 0 = 100 = 1 rad/s.

    The (ideal) accelerometer is a linear time-invariant system, with order 2. The modulus and
argument of () are shown in Figure 17.2, a Bode plot, for three values of damping ratio 
and 0 = 1 rad/s. We see that, with extremely low damping ( = 0.001, shown in red), and
subject to input acceleration with frequency  = 0 rad/s (see (16.14)) the accelerometer
proof mass gets into resonance at that frequency. In practice, the resonance frequency of an
accelerometer is in the order of tens of kHz (when expressed in linear frequency ).

    For frequencies  less than 0 = 1 rad/s (in Figure 17.2 this is shown to the left of the
vertical dotted line), |()| has an ideal and flat behavior: |()|  1. When   0, then
|()|  2 1 (= 1 in this example). The amplitude attenuation or amplification is the same

                   0

for frequencies from zero up to  0.30, which means that for that range of frequencies the
accelerometer sensitivity is independent of frequency. This is the `useful' frequency application
range of the accelerometer, i.e., its bandwidth, as within this range we get a proportional input-
output relation between acceleration and displacement, with a small phase lag ()  .
This phase angle  is because of the presence of a minus sign in (17.1) otherwise the phase
angle would be zero.

17.3 Impulse response

Frequency response function (17.3) can be split into two parts:

                            1                1

        () = 2 - 2
                   0 +  +  0 -  + 

with the damped natural frequency  = 01 - 2, and with 0 <  < 1. Fourier transform

(5.13), with  > 0, can be shown to hold also for complex-valued constant  working with

angular       frequency  ,  we  have   - ()     1 . Applying this twice (and using the linearity
                                                +
                                             
172                                                 17. Measuring signals

         h(t)    1.0                                                                          = 0.001
                 0.5                                                                          = 0.2
                 0.0                                                                          = 0.6
               -0.5
               -1.0   0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
                                                 t [s]

Figure 17.3: Accelerometer impulse response function (), with 0 = 1 rad/s, for three values of .

property of the Fourier transform (6.1)) leads to:

     () = - { 1 -0 () - 1 -0- ()}
     2                      2

Using Euler's formula, (C.1), we obtain for the impulse response function:

     () = - 1 -0 sin() ()                                                                              (17.4)
                 

a damped sinusoid, see Figure 17.3. The accelerometer is a causal system: () = 0 for
  0. The impulse response shows the (in this case horizontal) motion of the proof mass
after applying an impulse at  = 0. When the accelerometer is accelerated to the right, i.e.,
() > 0, its proof mass will initially move left with respect to the housing: () < 0.

17.4 Example

In this example we compare two accelerometers, with the same damping ratio  = 0.6, but

different undamped natural frequencies, respectively, 0 = 1 rad/s (in blue) and 0 = 10 rad/s
(in green) see the Bode plot of the FRFs in Figure 17.4. To facilitate the comparison, the FRFs
have been multiplied by 02, that is:

     ()               2
     () = = 2 2
                         0

     ()  - 0 - 20()

This multiplication can be considered as applying a scale factor to convert proof mass displace-

ment () into an output voltage, scaling the sensor proportionality. Both accelerometers then

have unit modulus, |()| = 1, at  = 0, allowing for a straight comparison.

    We apply to both sensors either a sinusoidal acceleration input signal 1() = cos(1) with
1 = 0.2 rad/s, or a sinusoidal input signal 2() = cos(2), with 2 = 2 rad/s. Figure 17.5
shows the steady-state responses () of the two accelerometers to the input signals 1()
(red, dashed) at the top, and 2() (orange, dashed) at the bottom. For convenient visualiza-
tion, the negative of the responses is shown because of the minus sign in (17.1).

    Starting with the low-frequency input signal 1(), Figure 17.5 (at top) shows that both
accelerometers, in steady state, provide an output with the same amplitude as the input. While

for accelerometer `green' there is no discernible time delay, the output of accelerometer `blue'

is lagging behind the input by approximately 1.4 s. These findings can be easily verified with

the Bode plot in Figure 17.4. At frequency 1 = 0.2 rad/s (vertical dashed line in red), the FRF
magnitude of both accelerometers is |()|  1. This means the output in terms of amplitude

matches the input exactly. At this frequency, the FRF phase () of accelerometer `green'

is  , the FRF phase of accelerometer `blue' is  0.91. While the output of accelerometer
17.5 Sensor and object                                                                    173

         101                                              0 = 1 rad/s  0 = 10 rad/s
                     |H()| = 1                                 1
|H ()|                                  1
         100                            1                 0.24
        10-1                                                           101                102
        10-2                      1 = 0.2                2 = 2

             10-2               10-1                100
                                                 [rad/s]
        H ()                                                                0 = 1 rad/s
                                    0.91                  0.91              0 = 10 rad/s
              
              2                   1 = 0.2    0.24                      101                102
                                                         2 = 2
              0                 10-1
              10-2                                  100
                                                 [rad/s]

Figure 17.4: Bode plot of two accelerometers, with 0 = 1 rad/s in blue and 0 = 10 rad/s in green. The vertical
dashed lines show the two input signal frequencies, 1 = 0.2 rad/s (red) and 2 = 2 rad/s (orange).

`green' has a negligible delay, the output of accelerometer `blue' will lag behind the input with
a phase lag of  - 0.91 = 0.09, equivalent to a time delay of 0.09 = 1.4 s.

                                                                                                                       0.2

    For the high-frequency input signal, 2(), Figure 17.5 (at bottom) shows that, in steady
state, only accelerometer `green' provides an output with the same amplitude as the input
signal. The output of accelerometer `blue' is much smaller in amplitude. Whereas for ac-
celerometer `green' there is no discernible time delay, the output of accelerometer `blue' is
lagging behind the input considerably. Again, these responses can be directly obtained from
the Bode plot, Figure 17.4, at the dashed line in orange. At frequency 2 = 2 rad/s, the FRF
magnitude of accelerometer `green' |()|  1, the FRF magnitude of accelerometer `blue'
is  0.24, explaining the amplitude of its output. At this frequency, the FRF phase () of
accelerometer `green' is  0.91, meaning a phase lag of 0.09 which corresponds to a time
delay of 0.09 = 0.14 s. The FRF phase of accelerometer `blue' equals  0.24, resulting in a

                   2

phase lag of 0.76, equivalent to a time delay of 0.76 = 1.19 s.

                                                                                             2

    The Bode plot allows the amplitude and phase of the steady-state responses to sinusoidal
input signals to be easily obtained. It immediately shows that accelerometer `green' is superior
to accelerometer `blue': its useful range of frequencies (bandwidth) is much larger.

17.5 Sensor and object

This section serves to create awareness of the fact that in practice a sensor cannot function
in ideal, isolated conditions. On the contrary: the sensor is attached or fixed to an object or
structure of interest. The sensor actually becomes part of the object or structure.

    This object or structure can be considered a system as well, that is, upon an input signal (a
certain load or vibration), it produces a response or output signal. The sensor is also a (small)
system, with a certain characteristic relationship between input and output (including the
effects of its mounting on the structure1). The interplay of structure and sensor is illustrated

1Ideally, mounting the sensor on the structure does not affect the structure's dynamic response in any way.
 Typically, to achieve this, the sensor must be much smaller than the structure.
174                                                                                           17. Measuring signals

¨x1(t), -yss(t)    1                    t0 = -1.4 s                                               0 = 1 rad/s
                                                                                                  0 = 10 rad/s
                   0
                                    20                  40                 60                 80                100
                             x¨(t)
                                                                    t [s]
                 -1
¨x2(t), -yss(t)       0                                                                           0 = 1 rad/s
                                                                                                  0 = 10 rad/s
                   1
                                    20                  40                 60                 80                100
                   0

                 -1
                      0

                                                                    t [s]

Figure 17.5: Steady-state time response () of two accelerometers with 0 = 1 rad/s in blue and 0 = 10 rad/s
in green (shown as negative, i.e., -), to input cosines 1() (red, dashed, 1 = 0.2 rad/s) (top) and 2()
(orange, dashed, 2 = 2 rad/s) (bottom).

in Figure 17.6. The goal of this set-up typically is to determine or to validate, by means of
experiment, the characteristics of the structure.

                                                                           measurement
                                                                           (recorded signal)

                                                            sensor

                                                        structure

                                          excitation                structure response
                                        (input signal)                 (output signal)

Figure 17.6: A structure turns excitation, input signal (), into a response, output signal (). Next, a sensor
measures this output signal (), and, being a system as well, it responds through producing output signal ().

    Assuming that both structure and sensor are LTI systems, the structure is fully character-

ized by its impulse response function () and the sensor by its impulse response (). The
eventual sensor output is:

                 () = ()  () = ()  ()  ()                                                         (17.5)

with () = ()  (), or, in the frequency domain:

                 () = ()() = ()()()

    When the structure is subject to an ideal impulse `hammer' input, () = (), then its

output is, by definition, () = ().
    Next, if we work with an ideal sensor, i.e., () = (), then the sensor output directly

delivers the impulse response of the system of interest () = (), and in this `ideal hammer

input' case () = (). Referring to the Bode plot, the ideal sensor () = () shows
as () = 1, that is, with unit amplitude |()| = 1 and zero phase () = 0 for all

frequencies .

    Now that the LTI system properties have been explained and a typical measurement set-up
discussed, the next chapter presents a major application of systems in signal analysis: filters.
                                                        18

                                                              Filters

This chapter presents an introduction to signal filtering in the frequency domain, as an applica-
tion of linear time-invariant systems. The filter, as a system, was conceptually already shown
in Figure 16.1. An input signal () is fed to a filter, which performs some frequency-selective
operation, resulting in output signal (). A typical application is the use of a so-called low-pass
filter to remove unwanted high-frequency noise components from an observed signal.

    The operation of a filter is most conveniently explained with continuous-time signals. The
last section of this chapter is dedicated to the application of digital filters to discrete-time
sequences, i.e., filtering in the discrete time domain.

    We limit the discussion to the principle of a filter. In practice many different filters exist,
which we do not cover. Furthermore, the design of filters is beyond the scope of this book.

18.1 Ideal filters

The operation of a filter is most directly characterized by its response in the frequency domain.
Figure 18.1 shows the frequency response function () of three so-called ideal filters: the
ideal low-pass filter (LPF), the ideal high-pass filter (HPF) and the ideal bandpass filter (BPF).
The latter is a combination of the first two, and is characterized by 2 > 1.

    The frequency response functions of these three ideal filters are specified as follows:

    · Low-pass filter: () = 1 for ||  2, and zero otherwise,

    · High-pass filter: () = 1 for ||  1, and zero otherwise, and

    · Bandpass filter: () = 1 for 1  ||  2, and zero otherwise.

       1.0                    1.0                  1.0

H(f )  0.5                    0.5                  0.5

       0.0                    0.0                  0.0

                         LPF                  HPF                                                                BPF

            -f2  0       f2        -f1 0 f1             -f2 -f1 0 f1 f2
                                      f [Hz]                      f [Hz]
                 f [Hz]

Figure 18.1: FRF of three ideal filters: left, low-pass filter middle, high-pass filter right, bandpass filter.

                                   175
176                                                                                        18. Filters

     10                   8                                                1.0
      5
h(t)  0                                                                    0.5          8
                                                                    H(f )-1.0

                                                                           0.0

             -0.5  0.0       0.5  1.0                                           -5.0 -2.5 0.0 2.5 5.0

                   t [s]                                                        f [Hz]

Figure 18.2: Ideal low-pass filter with 2 = 4 Hz. At right, frequency response () =  (  22 ) at left, corre-
sponding impulse response () = 22sinc(22).

    These filters are ideal, as in the selected frequency range, the pass band, they let input
signal components pass, i.e., |()| = 1 and () = 0, see (16.14), whereas outside this
range, they completely block all input signal components: the stop band, where |()| = 0.
With ideal filters, the transition between pass band(s) and stop band(s) is abrupt.

    Frequencies 1 and 2 are known as the bandwidth frequencies of, respectively, the ideal
high-pass and low-pass filters. For the ideal bandpass filter, the bandwidth equals 2 - 1,
centered about center frequency 1+2 .

                                                                2

    The magnitude of the ideal filters can be changed to an arbitrary value through multipli-
cation by a constant     {0}, the so-called filter gain (rather than the default value of 1).
Then, in the pass band, |()| = || and () = 0 for  > 0 and () = ± for  < 0.

18.1.1 Causality
Ideal filters are non-causal, i.e., ()  0 for  < 0. This is because their FRFs () are all

real, even functions of frequency, see Figure 18.1. Their impulse response functions () are
then all real, even functions in time.

    For example, consider the ideal low-pass filter, with FRF () =  (  ). Its impulse

                                                                                                                                         22

response function () = 22sinc(22) extends from  = - to  = , see Figure 18.2 at
left. This filter cannot be realized in practice we can only work with an approximation of
this, by means of a finite length impulse response function. Second, because the filter is
non-causal, we would need future signal values or samples for the filter to work in real time,
which is impossible. Only in post-processing, an ideal filter can be applied, with the above
approximation.

18.1.2 Impact of ideal filter

The impact of applying an ideal (brickwall) low-pass filter is demonstrated, using as input
signal a unit pulse function () = (), shown at left in Figure 18.3, and its Fourier transform
() = sinc() is shown at right. The low-pass filter has FRF () =  (  ), shown at right

                                                                                                                                     22

in Figure 18.2, with 2 = 4 Hz. The corresponding impulse response, () = 22sinc(22),
see (6.13), is shown at left.

    The filter output is found in the frequency domain through multiplication () = ()(),
(16.7), shown at right in Figure 18.4, and in the time domain through convolution () =
()  (), (16.6), shown at left. In the graph at right we can see that () is indeed zero
beyond 2 = 4 Hz compare this to () in Figure 18.3 at right. At left, we observe that `sharp'
features of the input signal, representing high-frequency components, are lost in the output,
due to low-pass filtering. Instead, we get `wobbles' or ripples in the signal, similar to the
18.1 Ideal filters                                                                                   177

      1.0                                                   1.0                        1

x(t)  0.5                        1                  X(f )   0.5

      0.0                                                   0.0

    -0.5                                            -0.5
         -1.0
               -0.5  0.0            0.5        1.0               -5.0 -2.5 0.0 2.5 5.0

                     t [s]                                                     f [Hz]

Figure 18.3: Left: input signal () = (), the unit pulse function right: corresponding Fourier transform
() = sinc().

      1.0                                x(t)               1.0

y(t)  0.5                                           Y (f )  0.5                               X(f )
                                                                                          2.5 5.0
      0.0                                                   0.0

-0.5                                                      -0.5
                                                                   -5.0
      -1.0 -0.5      0.0            0.5        1.0                       -2.5    0.0
                                                                               f [Hz]
                     t [s]

Figure 18.4: Left: output signal (), the response of the ideal low-pass filter of Figure 18.2 (2 = 4 Hz) to a
unit pulse input right: corresponding Fourier transform (). To highlight the working of the filter, the thin black

dashed curves show the input signal () (left) and its Fourier transform () (right) from Figure 18.3.

approximation of a square wave by just the first few terms of the Fourier series, Figure 3.1.

    The purpose of a low-pass filter is often to filter out high-frequency noise in an observed
signal. Similarly, we may want to filter out all (non-relevant) high-frequency components of a
signal before sampling that signal, so-called pre-sample filtering.

    As an example, a signal with (simulated) random noise is shown in Figure 18.5. Figure 18.6
shows the output of the filter, at left for 2 = 4 Hz and at right for 2 = 20 Hz. With 2 = 4 Hz,
the ideal LPF clearly is effective in removing the high-frequency noise, but unavoidably, also
in removing the high-frequency components of the desired signal () the `sharp' edges are
lost, just like in Figure 18.4 at left. With 2 = 20 Hz, part of the high-frequency noise (in
this example, band-limited at 50 Hz, see Appendix L) remains in the output, but at the same
time the `sharp' characteristics of the unit pulse input are also maintained. Applying filters in
practice often means finding a good compromise.

                            1.0

                     x(t)   0.5

                            0.0

                           -0.5

                            -1.0         -0.5       0.0          0.5     1.0

                                                    t [s]

Figure 18.5: Input signal () = (), with additive noise, simulated, zero mean, and normally distributed with
standard deviation equal to 0.1.
         178                                                                                                                       18. Filters

                       1.0                                                        1.0

              y(t)     0.5                                                  y(t)  0.5

                       0.0                                                        0.0
                                                       f2 = 4 Hz                                                  f2 = 20 Hz

                    -0.5                                                    -0.5

                       -1.0 -0.5             0.0                  0.5  1.0        -1.0 -0.5    0.0                            0.5  1.0

                                          t [s]                                                t [s]

         Figure 18.6: Left: output signal () as a result of ideal low-pass filtering the unit pulse input contaminated with

         noise (Figure 18.5) with filter bandwidth 2 = 4 Hz right: with filter bandwidth 2 = 20 Hz. The thin black dashed
         lines show the unperturbed input signal () = () from Figure 18.3 (at left).

EX 18.1  Consider as input voltage signal () = 10sinc(10), fed through an ideal high-pass
         filter with bandwidth frequency 1 = 3 Hz. Compute the energy of output signal ().

         Solution The Fourier transform of () equals () =  (  ), (6.13), and is shown

                                                                                                                       10

         in Figure 18.7 at the top. The frequency response of the ideal high-pass filter is shown

         in the middle of this figure. Through () = ()(), (16.7), we find that () =

          ( +4 ) +  ( -4 ), shown at the bottom of Figure 18.7.
                    2       2

                                          1

                                  X(f )   0
                                          -7.5 -5.0 -2.5 0.0 2.5 5.0 7.5

                                                                        f [Hz]

                                          1

                                  H(f )   0 -3

                                                                                       f1 = 3

                                          -7.5 -5.0 -2.5 0.0 2.5 5.0 7.5
                                                                       f [Hz]

                                          1

                                  Y (f )

                                              0 -4 4
                                              -7.5 -5.0 -2.5 0.0 2.5 5.0 7.5

                                                                            f [Hz]

         Figure 18.7: Fourier transform of input signal (), () (top), FRF of the ideal high-pass filter ()
         (middle) and Fourier transform of the output signal (), () (bottom).

         The energy of the output signal can be computed with (5.19):

                               

                 =  |()|2 d = (1)22 + (1)22 = 4 J

                            -
18.2 Practical filters                                                                                 179

1.0 1 2                                                                    1
                                                                           2 2
                                                                     2

|H(f )|0.5                                                                 0
                                                                    H(f )

                         - 2 2 1 1                                            - 21
0.0                                                                        -2

     -1                    0        1                                         -1    0               1

                           f [Hz]                                                 f [Hz]

Figure 18.8: FRF of first-order Butterworth low-pass filter (18.1): magnitude response |()| at left and phase
response () at right. The vertical dotted lines indicate  = ± 1 Hz.

                                                                                                                                  2

18.2 Practical filters

The FRF of an ideal filter abruptly changes, causing a discontinuity in () at some frequency.
For instance, for the ideal low-pass filter, () changes from () = 1 to () = 0 at  = 2.
In order to arrive at causal filters, we must allow for a gradual change, in the frequency

domain, from pass band to stop band (and vice versa).
    A basic, causal low-pass filter is the first-order Butterworth filter.1 The frequency response

function of this filter, a first-order LTI system, reads:

               1                                                                                       (18.1)
() = 1 + 2

or () = 1 in terms of angular frequency . The magnitude and phase response of the

                    1+

filter are shown in Figure 18.8. The magnitude response reads:

                    1                                                                                  (18.2)
|()| =

            1 + (2)2

The phase response equals:

() = - arctan(2)                                                                                       (18.3)

For frequencies  close to zero, |()|  1 and ()  0 for large frequencies |()|  0
and ()  -  . The pass band and stop band of the filter become clearer when considering

                            2

the Bode plot of the filter (see Figure 18.9) with logarithmic frequency and magnitude scales.
In contrast to the (in practice non-realizable) ideal low-pass filter, there is indeed a gradual
transition from pass band to stop band.

18.2.1 Cut-off frequency

The width of the pass band of the first-order Butterworth filter can be tuned by `scaling' the
frequency variable. We rewrite (), (18.1), as:

              1                                                                                        (18.4)
() = 1 +  

                        3

with 3 = 12 . At frequency  = 3, we have |(3)| = 22 or |(3)|2 = 12 . Therefore, as is
shown in (16.16), an input signal component at this frequency is attenuated by the filter, such

1After the British physicist S. Butterworth (1885-1958), who proposed an entire family of filters.
180                                                                                                      18. Filters

     101                                                                  0

     100 21 2                                                      H(f )
|H(f )|
       delay10-1                       1 - 2 1 

     10-2 2 2
              10-2      10-1                   100     101                10-2        10-1          100            101

                                       f [Hz]                                               f [Hz]

Figure 18.9: Bode plot of the same first-order Butterworth low-pass filter as in Figure 18.8. Note that usually the
phase response is shown below the magnitude response.

              1.0                              1.0                  100                                  1.0
                                                                   10-1                                  0.1
              0.5                                           delay  10-2

                                  0.1                                   10-2
              0.0

                    -1                 0            1                           10-1   100          101       102
                                                                                      f [Hz]
                                       f [Hz]

Figure 18.10: Magnitude of time delay (in [s]) of the first-order Butterworth low-pass filter (18.1) as a function of
frequency, on a linear (left) and logarithmic (right) axis.

that half of its power or energy remains at the output. By convention, the gradual transition
from pass band to stop band is characterized by this `half power' frequency, referred to as the
cut-off frequency, or 3dB-frequency. The subscript `3' in 3 is chosen on purpose to mark that

                                                                                                                                               1

a reduction by half of the power corresponds to a loss of 3 dB as 10 log10( 2 )  -3. The
cut-off frequency 3 is indicated by the dotted vertical lines in Figures 18.8 and 18.9.

18.2.2 Frequency response function - phase revisited

The phase response of (18.1) is () = () = - arctan(2), (18.3) and shown in Fig-
ures 18.8 and 18.9 at right. The output of a linear time-invariant system to a cosine input was
given by (16.14) and the phase response was discussed in Section 16.4.2.

    For the first-order Butterworth filter, the phase () clearly depends on frequency  in a
nonlinear way. For an input signal with frequency  = 0, the output lags behind the input
(negative time shift, 0 < 0) with time delay 0 = (0) 20 = - arctan(20) 20  this is shown in
Figure 18.10 on linear (left) and logarithmic (right) axes. For small frequencies, the filter
output lags behind the input by exactly one second.

18.2.3 Low-pass filter analogy: mechanical system

By means of an example, we show a mechanical analogy of the first-order Butterworth filter,
(18.1). That is, we present a mechanical system which, upon a load or input force, responds
exactly like this low-pass filter.

    Figure 18.11 shows a simple mass-damper system. The motion of the proof mass is the
result of the external force () = (), the input to the system, and the force of the damper,
which delivers a linear drag, a force proportional to the velocity () of the proof mass. The
mass is assumed to slide without friction over the supporting surface.
18.2 Practical filters                                                       181

Figure 18.11: Basic outline of a mass-damper system: a proof mass (in blue) is connected to a dashpot damper.
The damper delivers a linear drag with a force proportional to the velocity of the proof mass.

The corresponding differential equation follows from () = () - () as:

                  
 () + () =  ()

with mass  in [kg] and damper coefficient  in [Ns/m]. With constant  =  we obtain:

                                                                                                                                            

1                 1
 () + () =  ()                                                               (18.5)

This is an LTI system of order 2. To obtain the relation between the output velocity () and
input acceleration (), we observe that the DE above is similar to the one in Example 16.5,
(16.9) it differs by just a factor of 1 on the right-hand side. Solving the resulting first-order

                                                                 

differential equation in the frequency domain yields:

   ()                   1
() = () =  + 2                                                               (18.6)

For  = 1 the FRF is indeed identical to the first-order Butterworth low-pass filter FRF, (18.1).
    In Example 5.3 the impulse response function () was found to be:

() = -() for  > 0                                                            (18.7)

shown in Figure 5.4 for  = 1 and  = 3. As is shown, due to the presence of the unit step

function () in (18.7), this system is causal: () = 0 for  < 0.

Example 7.4 demonstrates for  = 1 the response of this system to a unit step input.

Figure 7.8 shows that the output is indeed zero for  < 0 and then features a gradual transition
to the new state. At time  = 1 , the output reaches 63% of its final value.
 The 3dB frequency of the FRF (18.6) equals  with  =  . Through varying the dashpot
                           2  
damper coefficient  and/or the proof mass  we can change the bandwidth of the resulting

low-pass filter.

18.2.4 FIR and IIR

In the context of filters, two expressions are often used to distinguish between two types of
impulse responses of the filter system:

    · Infinite Impulse Response (IIR), where () continues indefinitely, and

    · Finite Impulse Response (FIR), where () = 0 for  > 0, for some finite 0 () has a
       finite duration and settles to zero in finite time.
182                                                                                     18. Filters

              M = 10                                                    0.3

1.0  1                                                                              2M + 1 = 11

hn                                                                      0.2                                   1
                                                                    hn                                       11
0.5
                                                                        0.1

0.0 0.0

     -2 -1 0  1       2         3  4                                         -5  0      5

              t [s]                                                              t [s]

Figure 18.12: Left: impulse response sequence  in discrete time of causal first-order Butterworth low-pass filter
( = 1) (with  = 10 and  = 0.2 s) right: non-causal moving-average filter (with  = 5 and  = 1 s).

    The first-order Butterworth low-pass filter is an IIR, with impulse response () = -()
( = 1), shown in Figure 7.7 at right. This function asymptotically approaches zero only for
  . We may approximate an IIR by a FIR: when () gets close to zero from a certain
point onwards, we simply set these non-zero impulse response values to zero.

    The moving-average filter, presented in the next section, is a FIR. Its impulse response
has a finite duration. The FIR filter or system output () to input () is given by:

                  0                                                                        (18.8)

     () =  ()( - ) d

                  0

where we used the commutative property of convolution, (7.3). Output () at time  relies on

input signal () with   [ - 0, ] (a finite interval). We assumed the filter (i.e., the system)
to be causal, () = 0 for  < 0, such that the lower integral bound in (18.8) is  = 0.

18.3 Digital filters

In continuous time, filters are relatively easy to understand and apply, mostly using analog
electrical circuits. In practice, however, we often apply digital filtering to discrete-time se-
quences. The filter output, sequence , follows as the convolution of input sequence  with
filter impulse response sequence :  =    see also Appendix F.

    Now that we covered continuous-time filter impulse response functions for LTI systems,

in this section we take a pragmatic approach to briefly introduce digital filters. The discrete-
time version of the filter impulse response  follows from sampling the continuous-time
impulse response (). In case of an IIR we take an additional approximation and truncate
the continuous-time impulse response to finite length.

18.3.1 Digital Butterworth filter

We apply this approach for the first-order Butterworth low-pass filter, for which the impulse
response () was given by (18.7) with  = 1 (Figure 7.7 at right). This is a causal filter. The
corresponding discrete-time impulse response is obtained by:

      = { () 0     0 otherwise                                                             (18.9)

with finite , and hence we approximate the IIR first-order Butterworth by a FIR. The impulse
response function  = - goes to zero quite quickly (see Figure 18.12 at left, for  = 10
and  = 0.2 s).
18.3 Digital filters                          183

18.3.2 Moving-average filter

A moving-average (MA) operation can also be considered a low-pass filter, as it tends to smooth
(short-term) fluctuations in the input signal. At each time instant  = , the output signal is
computed as the average over a set of adjacent (past, and possibly also future) input signal
values this means the window is sliding over the input data sequence. For instance:

           1                                  (18.10)
 = 2 + 1 (- + -(-1) + ... +  + +1 + ... + +)

for a centralized moving-average filter with window length 2 + 1. This operation can be
expressed as a discrete convolution  =    with impulse response sequence :

                 1   ||                       (18.11)
                    otherwise
 = { 2+1
            0

This is a length 2 + 1 sequence, shown for  = 5 and  = 1 s in Figure 18.12 at right.
    The moving-average filter has a FIR. The centralized moving-average filter as above is

non-causal, as   0 for  < 0. The filter can be made causal through shifting the impulse
response by  positions such that it only considers present and past values:

                  1
        = 2 + 1 (-2 + -(2-1) + ... + -1 + )

still with a length 2 + 1 impulse response sequence then  = 0 for  < 0. The impulse
response shown in Figure 18.12 at right would be shifted by  = 5 positions to the right, and
would be applicable in real time.
           Appendices

185
          A

Common mathematical formulas

A.1 Trigonometric identities                                                  (A.1)
                                                                              (A.2)
       sin2  + cos2  = 1                                                      (A.3)
                                                                              (A.4)
       cos(2) = cos2  - sin2                                                  (A.5)
                                                                              (A.6)
       sin(2) = 2 sin  cos                                                    (A.7)
       cos2  = 1 (1 + cos(2))                                                 (A.8)
                                                                              (A.9)
                            2                                                (A.10)

       sin2  = 1 (1 - cos(2))

                          2

       sin( ± ) = sin  cos  ± cos  sin 

       cos( ± ) = cos  cos   sin  sin 
       sin  sin  = 1 (cos( - ) - cos( + ))

                                 2

       cos  cos  = 1 (cos( - ) + cos( + ))

                                    2

       sin  cos  = 1 (sin( - ) + sin( + ))

                                   2

A.2 Orthogonality of sines and cosines

Assume  and  to be non-zero integers, ,   +and 0 the period corresponding with the
fundamental angular frequency 0 = 2 0 . The orthogonality properties of integrals involving
products of sines and cosines can then be defined as follows:

                          0                 for   
1 =  sin(0) sin(0) d = { 0                  for  =   0                       (A.11)

0                         2

                             0              for   
2 =  cos(0) cos(0) d = { 0                  for  =   0                       (A.12)

0                            2

3 =  sin(0) cos(0) d = 0                    for all ,                        (A.13)

          0

These integrals can be proven using trigonometric identities such as (A.8).

                             187
188                                          A. Common mathematical formulas

A.3 Definite integrals

     
      sin() d =  ,  > 0
                   2                         (A.14)
                                             (A.15)
     0                                       (A.16)

        sin2 

                 
      2 d = 2
         
     0

A.4 Indefinite integrals

                              1
       sin() d = 2 (sin() -  cos())

                              

                                1            (A.17)
       cos() d = 2 (cos() +  sin())

                               

       sin() d =  21+2 ( sin() -  cos())     (A.18)

       cos() d =  21+2 ( cos() +  sin())     (A.19)
                                             (A.20)
A.5 Series expansions                        (A.21)
                                             (A.22)
Sum of squared reciprocal integers [6]:

       1 1 1 1 2
      2 = 12 + 22 + 32 + ... = 6

     =1

Sum of squared reciprocal odd integers [6]:

       1 1 1 1 2
      (2 - 1)2 = 12 + 32 + 52 + ... = 8

     =1

Geometric series identity:

     -1        1 -          for   1
                            for  = 1
       = { 1 - 

     =0        
                                            B

                                      Elementary functions

In this appendix we introduce a couple of elementary continuous-time functions. They are
frequently used as signals in examples to illustrate and elaborate on the theory.

B.1 Unit pulse function

The unit pulse function, with symbol  resembling the function shape, is defined as:

         1  || < 1                                                                      (B.1)
() = 0.5
                      2
                      1
         0
            || = 2
            || > 1

                      2

The unit pulse function is shown in Figure B.1 at left. The unit pulse is an even function with

amplitude equal to 1 and extending to half a second on both sides of the origin. The unit
pulse () has a width of 1 second. At  = ± 1 s we define the function value to be at 1 (the
                                      2                                              2
blue circles in Figure B.1). This function is also known as the boxcar or rectangular function.

B.2 Unit triangular function

The unit triangular function, with symbol  resembling the function shape, is defined as:

() = { 1 - || for ||  1 0 for || > 1                                                    (B.2)

The unit triangular function is shown in Figure B.1 at right. The triangular function is an even
function with amplitude equal to 1 at maximum for  = 0 s it reaches zero on both sides of
the origin at  = ±1 s its width is 2 seconds.

B.3 Sine cardinal function (sinc)

The sine cardinal function, or sinc function for short, is defined as:

             sin()                                                                      (B.3)
sinc() = 

The sinc function is illustrated in Figure B.2. It has a maximum of 1 at  = 0 s (using L'Hôspital's
rule) and equals zero at  = ±1, ±2, ±3, ..., which is implied by the sine in the numerator. Some

                                      189
190                                                                              B. Elementary functions

     1.0                                                                 1.0

(t)  0.5                                                                 0.5
                                     x(t)
     0.0                                                                 0.0
                                                                    (t)
     -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5                                         -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5

                     t [s]                                                             t [s]

Figure B.1: Unit pulse function () at left, and unit triangular function () at right.

                        1.0

                        0.5

                        0.0

                             -2                                          0    2

                                                          t [s]

Figure B.2: Sine cardinal, or sinc function () = sinc().

zero-crossings are shown in Figure B.2 using blue circles. From (B.3) it follows that:

     lim sinc() = 0                                                                           (B.4)

     ±

    The definition in (B.3) is sometimes referred to as the normalized sinc function (to distin-
guish it from the definition without  in both the numerator and denominator).

B.4 Singularity functions

Three common singularity functions are introduced here: the unit step function, the unit ramp
function and the Dirac1 delta function. The unit step function is the derivative of the unit ramp
function, and the Dirac delta function is the derivative of the unit step function.

B.4.1 Unit step function

The unit step function is defined as:

                 0  <0                                                                        (B.5)
     () = { 0.5     =0
                    >0
                 1

Figure B.3 (left) illustrates the unit step function. At  = 0 s its value is defined to lie half-way,
at 1 (blue circle). The unit step function represents a signal that switches on at  = 0 s and

     2

stays on indefinitely. It is also known as the Heaviside2 step function.

1After the British theoretical physicist P.A.M. Dirac (1902-1984).
2After the British mathematician O. Heaviside (1850-1925).
B.4 Singularity functions                                                                   191

1.0                                                                       1.5

                                                                    1.0
0.5

                                                                    0.5
u(t)
                                                                    r(t)

0.0                                                                       0.0

-1.5 -1.0 -0.5 0.0 0.5 1.0 1.5                                            -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5

                    t [s]                                                            t [s]

Figure B.3: Unit step function () at left, and unit ramp function () at right.

B.4.2 Unit ramp function

The unit ramp function is defined as:

() = { 0  < 0    0                                                                          (B.6)

and shown in Figure B.3 at right. This function grows indefinitely for  > 0.                (B.7)
    The unit ramp function is related to the unit step function as:

                          

       () =  () d

                        -

B.4.3 Dirac delta function

The unit impulse or Dirac delta function is definitely the most frequently used singularity
function in Fourier analysis. It is defined to be zero everywhere, except for  = 0:

() = 0   0                                                                                  (B.8)

with unit area:                                                                             (B.9)

             

          () d = 1

           -

which is obtained in an infinitesimal interval of time. The Dirac delta function resembles an
extremely narrow, extremely high spike or peak at  = 0 s, see Figure B.4. Its value at  = 0 s
is infinity its width is zero. When multiplied by a number , the area of the Dirac delta
function, also defined as its weight, becomes . The Dirac delta function is used to represent
a signal or phenomenon that occurs (almost) instantaneously, e.g., the impact of a hammer
on a clock bell, or the impact of a billiard ball on another one when hitting it.

    The Dirac delta function is often referred to as a Dirac pulse. In many practical cases, the
Dirac delta function can be considered as a limiting case of the pulse function, (B.1):

              1                                                                             (B.10)
() = lim  ( )

          0  

This function attains value 1 over the interval from  = -  to  =  s, and has unit area for all
                                                                               2  2
values of . This approximation, known as a `nascent delta function', demonstrates that the

Dirac delta function is an even function.
192                                          B. Elementary functions

The unit step function is related to the unit impulse function as:

                                                                    (B.11)

     () =  () d

                   -

    Note that the Dirac delta function is not a function in the traditional sense. With the above
definition of taking on a non-zero value only for  = 0 s, a Riemann integral over the Dirac
delta function would yield zero (rather than 1, as defined). The above integration must instead
be understood in the sense of so-called generalized functions, and a proper explanation can
be done using the theory of distributions [13].

     1.0

(t)  0.5

     0.0
       -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5
                                      t [s]

Figure B.4: At left: unit impulse or Dirac delta function (). At right: sifting property of the Dirac delta function,
at  = 0, in an integral over smooth function (). Here, the Delta pulse approximation is shown in red, centered
at time 0 with duration  and amplitude 1 , and smooth function () in blue. The integral of the sifting property
is represented by the area in blue, and equal to (0).

    The Dirac delta function is a so-called generalized function. It is not defined in terms of
values, but rather how it acts in an integral, when multiplied by a smooth function. This leads
to the sifting property, with a Dirac delta function occurring at  = 0:

                                                                    (B.12)

       ()( - 0) d = (0)

     -

The result of this integral is the value of function () at  = 0 the Dirac delta function sifts
out the value of () at  = 0. This is illustrated in Figure B.4.

    The Dirac delta function has the inverse dimension of its argument. Consider () with
time  in seconds, then the unit of () is one-over-seconds [ 1 ]. The area under this `function'

                                                                                                              s

equals 1 (dimensionless), see (B.9). This result also follows from the approximation 1  (  ),

                                                                                                                                                            

where  has the same unit as . Similarly, the unit of (), with  in Hz, is one-over-Hertz, or

seconds, [s].

    When changing the `running variable' in a function which includes a Dirac delta function,

we need to assure that (B.9) holds. Generally, it can be shown that:

     () = ||()                                                      (B.13)

for some non-zero constant    and here with a Dirac delta function in time. For instance,
in the frequency domain, when expressing () as (), with angular frequency  = 2,
we substitute 2() for each occurrence of () in (). When expressing (2), with
discrete frequency 2, as (), we substitute (2) = 1 () for each occurrence

                                                                                                                    2

of (2) in (2).
                              C

                          Complex algebra

In signal analysis we heavily rely on the use of complex algebra. Complex numbers can be
considered an extension of real numbers. As shown in Table C.1, complex numbers are of the
form  + , with both  and  real numbers, and  the imaginary number or unit, for which
holds that 2 = -1. The complex number  =  +  has a real part, , and an imaginary
part, . Hence:  = Re() and  = Im().

Table C.1: Overview of number systems.

symbol  number system     examples                           comment

        natural numbers   0, 1, 2, 3, ...                    ratio of two integers
        integers          ... , -3, -2, -1, 0, 1, 2, 3, ...   + 
        rational numbers  - 3 , 17 , ...
        real numbers
        complex numbers      26

                          2, , ...
                          2 - 3, 3 + 4, ...

C.1 Complex plane

A complex number  can be interpreted as a vector in the complex plane, see Figure C.1.
The length, or magnitude, of the vector can be written as  and equals the modulus of the
complex number:  = ||. The complex absolute value is a Euclidean1 norm:  = 2 + 2.
The angle of this vector with the real axis is  = arg() = , and equals  = arctan(  ). The

                                                                                                                                                           

angle goes counterclockwise, its unit is radians.
    When the imaginary part of a complex number is zero, this number lies somewhere on

the real axis of the complex plane. When the real part of that number is positive, it lies on
the positive real axis, its phase is 0. When the real part is negative, it lies on the negative
real axis, its phase is ±. When the real part is also zero, the number lies at the origin of the
complex plane, its magnitude is zero, its phase is defined zero.

    Complex number  can also be written as  =  cos  +  sin , which later will be turned
into  = , using the complex exponential . Note that formally, a complex exponential
has the form  with  a complex number, i.e.,  =  + , hence  = , though in this
book we restrict to complex numbers on the unit circle in the complex plane, hence  = 0.

1After the Greek mathematician Euclid (300 BC).

                                                           193
194                                                                    C. Complex algebra

Figure C.1: Complex number  =  +  interpreted as a vector in the complex plane. The vector has magnitude
, and angle  with the real axis  =  cos  and  =  sin .

Figure C.2: Complex number  =  +  and its complex conjugate  =  - . Complex number  and its
complex conjugate  have equal magnitude , but opposite angle .

    Complex number  =  +  is referred to as expressed in Cartesian2 coordinates  and .
That same complex number  can be expressed in polar coordinates  and :  = .

C.2 Euler's formula

Using , the base of the natural logarithm, Euler's formula reads:

      = cos  +  sin                                                    (C.1)

With this equation we can easily find that:

     1 (- + ) = cos                                                    (C.2)
     2

and:                                                                   (C.3)
        1 (- - ) = sin 
         2

C.3 Complex conjugate

Complex number  =  +  =  has a conjugate defined as  =  -  = -. The
complex conjugation operation is the reflection with respect to the real axis, see Figure C.2.
It is easy to prove that  = || =  = ,  +  = 2Re(), and || = ||.

2After the French mathematician/philosopher R. Descartes (1596-1650).
C.4 Operations                                                                                                    195

      Im                   Im     Im                                                                          Im
               j               j             j                                                                j

-1                  1 Re -1 0     1 Re -1 0     1 Re -1 0                                                         1 Re
            0

a) -j               b) -j         c) -j         d) -j

Figure C.3: Complex number a)  = 1 + 0, b) multiplied by , hence  = , c) again multiplied by , hence
 = -1, and d) multiplied another time by , hence -.

C.4 Operations

The addition and difference of two complex numbers are easiest obtained using the Cartesian
form. With  =  +  and  =  +  we have:

       +  = ( + ) + ( + )                                                                                         (C.4)

and:

       -  = ( - ) + ( - )                                                                                         (C.5)

Multiplication and division are easiest done in the polar form, with  =  and  = :

       = (+)                                                                                                      (C.6)

and:

       =  (-)                                                                                                     (C.7)
      

    Using these relationships, it is relatively easy to understand what happens to a complex
number  when you multiply it by :

                    
       =  2  = (+ 2 )
                                                                                                                  (C.8)

When being multiplied by , the complex number  is rotated in the complex plane by + 

                                                                                                                                                                       2

radians (counterclockwise), as illustrated in Figure C.3. Similarly, dividing a complex number
 with  means multiplying that number by -:

       =   = - = - 2  = (- 2 )                                                                                    (C.9)
       

The complex number is rotated in the complex plane by -  (clockwise).

                                                                                                           2

C.5 Phasor

To start relating complex numbers to the description of signals, we introduce the rotating
phasor signal as a time-variant vector in the complex plane:

      () = (0+) = 0                                                                                               (C.10)

This is a vector with magnitude  and (time-variant) angle 0 +  through time  in the
exponent. The radial frequency 0 (in radians/second) follows from the linear (or ordinal)
196                                 C. Complex algebra

Figure C.4: Real signal () =  cos(0 + ) is constructed as the sum of complex vector 12 () and its complex
conjugate 1 ().

                     2

frequency 0 (in Hz, i.e., `per second') using 0 = 20. In a time duration of 0 = 10 = 2 0
seconds this vector completes a full circle (2 radians) in the complex plane.

    The initial position of this vector at  = 0 s is referred to as the phasor:

      =                                                                        (C.11)

The phasor describes the amplitude  and initial phase , not the frequency.
    Though a nice and very convenient mathematical description, complex-valued signals do

not exist in reality. Physical systems always interact with real signals. To obtain a real signal
from a complex signal, we can simply take only the real part of the complex signal:

     () = Re(()) =  cos(0 + )                                                  (C.12)

or exploit the symmetry with the complex conjugate, see Figure C.4 and (C.2):

     () = 1 () + 1 () =  cos(0 + )                                             (C.13)
     2    2
                             D

                           Quantization

Figure 2.2 in Chapter 2 shows all four possible appearances of a signal. The independent

variable time  can be either continuous (left column) or discrete (right column), and the
signal value  can be either continuous (top row) or discrete (bottom row). The signal on top-
left is referred to as an analog signal, e.g., a voltage from an electrical device, the signal at
bottom-right as a digital signal. The latter follows from the first by sampling and quantization.

    The process of sampling, going from left to right in Figure 2.2, is covered extensively in
Chapter 9. Continuous time    is converted to discrete time  =  with   , see (9.1).

    The process of quantization, going from top to bottom in Figure 2.2, is discussed here.
Continuous signal value    is converted to discrete signal value  = , with    and
   the step size or increment in signal value, and () denoting the quantized signal (in
continuous time). Step size  determines the resolution of the quantized signal a large step

size yields a poor resolution (much signal `detail' may get lost in the quantization).

    A simple and straightforward example of quantization is rounding to the nearest integer.

The value of signal () is a real number   , and it gets converted, or mapped to   .
In this case    and  = 1. All real values in the interval   [16.50, 17.49) get mapped
to integer  = 17. Clearly, quantization is a nonlinear, irreversible process a range of values
for  gets mapped to a single value for .

    Similar to restricting the time duration of the signal, from infinite duration to  or ,
done in Chapter 8, we also restrict the range of signal values in practice, from an expected

minimum signal value to an expected maximum signal value, the dynamic range :

 = max(()) - min(())       (D.1)

    We also restrict or truncate the countable set from  = - to  to a set with a finite
number of values  = 0, ... ,  - 1, typically with  = 2, such that in total there are  = 2
possible values to represent the signal. This last step is referred to as encoding, with  the

number of bits (binary 0/1-values) to represent the signal value in computer memory. With
 = 3 there are  = 23 = 8 possible signal values, which are stored using 3 bits. Note that
the term quantization is often used to refer to both quantization and encoding.

    When the number of bits  is known, the quantization step size  is given by:

                           (D.2)
 = 

       2

    For the example signal () of Figure 2.2, we assume that its values lie in between -3
and 3: the dynamic range  = 6. Figure D.1 illustrates the quantization process when  = 3

                      197
        198                                                                    D. Quantization

        bits would be available: we allow for  = 23 = 8 possible (discrete) signal values, the step
        size  = 6 = 0.75. Discretization takes place along the vertical axis. Usually the quantization

                          8

        is uniform (equispaced),  is constant. Quantized signal () is still continuous in time,
        changes from one quantized signal level to another may occur at any time.

                             3.0

                                                                  111 7

                             2.25

                                                                  110 6

                             1.5 D                                101 5

                      xq(t)  0.75 x                               100 4

                             0

                                                                  011 3

                             -0.75

                                                                  010 2

                             -1.5

                                                                  001 1

                             -2.25

                                                                  000 0

                             -3.0
                                 -3.0 -2.25 -1.5 -0.75 0 0.75 1.5 2.25 3.0

                                             x(t)

        Figure D.1: The process of quantization: (input) signal value () is along the horizontal axis, covering the dynamic
        range , the (output) quantized signal value () along the vertical axis.

            In the example, we chose to deploy the dynamic range symmetrically about  = 0, from

        -3 to 3. Offering signal values outside this range would lead to saturation: the quantized

        signal is clipped to its minimum or maximum value, here, respectively, -2.625 or 2.625.
            We introduce signal value offset 0, set to 0 = -2.625 in this example, to allow for

         = 0, ... , 7, according to:

              = 0 +   with  = 0, ... ,  - 1

        omitting the time-argument, as this holds for both continuous and discrete time. These -

        values represent the center or mid-values of the quantization intervals, see Figure D.1. The

                                                   0                         0               
        corresponding thresholds or boundaries of the interval are [ +  - ,  +  + ).
                                                                            2                2
        The quantization error is the difference between original signal value  and the quantized

        signal value . The magnitude of this error is half the step size 2 at most, much similar to
        a round-off error. Usually quantization is considered to add a (zero mean) contribution to the

        signal's noise. In this book we assume that a high resolution is used for quantization (large

        value for , giving very small steps ), and hence that the effect is negligible.

EX D.1  Assume a voltage signal () = 2 + 3 cos(2 + 0.5) which needs to be quantized with
        a step size of at most 0.05 V. What is the minimum number of bits needed?

        Solution The dynamic range  of this signal is 6 V, as the signal oscillates between -1

        and +5 V. Dividing  by the required step size  = 0.05 yields 120 signal levels. With

         = 7 bits we get  = 2 = 128 possibilities, which is just sufficient, it leads to a step
        size  of 6 = 0.0469 V. Taking  = 6 bits would yield a step size of  = 6 = 0.094
             128                                                                         64
        V, which is too large. The minimum number of bits  is 7.
      E

Fourier transform pairs

              199
200                                                          E. Fourier transform pairs

Table E.1: Common Fourier transform pairs.

()                         ()               remark

unit pulse and sinc

()                         sinc()           see (5.11)
                           ()               see (6.13)
      
                                            duality (see (6.5)) with next pair
sinc()                                      see (7.7) with both () and () di-
                                            vided by factor 
unit triangular and sinc2

()                         sinc2()
                           ()
     
                                 
sinc2()

exponential                     1           see (5.13)
-() with  > 0               + 2
-() with  > 0                               see Exercise .5-5(a)
-|| with  > 0                     1         see Exercise .6-5(b)
-(  )2                     ( + 2)2          see Exercise .5-5(d)

                                 2
                           2 + (2)2

                            -()2

in-the-limit

()                         1                see (5.14)

1                          ()               see (5.15)

( - 0)                      -20             time-shift and pair {(), 1}
 20 
cos(20)                    ( - 0)           duality with previous pair
sin(20)
                           12 (( + 0) + ( - 0)) see (5.16)
                            12 (( + 0) - ( - 0)) see (5.17)
                        F

                   Discrete convolution

In Chapter 7, the concept of convolution of two continuous-time functions was introduced,

(7.1), resulting in a new continuous-time function. In a similar way, the convolution of two
discrete-time sequences  and  (with index , of infinite length, and pertaining to the same
time interval ) can be defined, resulting in a new discrete-time sequence :

 =                                                                                     (F.1)

again with the asterisk `'-symbol, a short-hand notation for the following summation:

                                                                                       (F.2)

 =  -

         =-

The convolution integral, (7.1), has been replaced by a discrete summation, with  as the
running variable.

    Similar to the convolution in continuous time, Section 7.1, a discrete-time convolution can
be split into five steps:

1. Consider both sequences with index , hence  and ,

2. Mirror or flip sequence  about the origin ( = 0) to get -,

3. Shift sequence  by  to yield -,

4. Compute output sequence  for one specific index value  by summing, over index ,
    product - from  = - to , and

5. Repeat steps 3 and 4 for every value of ,

to obtain the full output sequence , as a function of time index .
    When the two discrete-time sequences have been obtained through sampling (with sam-

pling interval ), and we would like to maintain the analogy with continuous-time convolution
(7.1), the integral is to be discretized. The above convolution, (F.2), becomes:

                                                                                       (F.3)

 =   -

              =-

In this case, when convolving with a Dirac delta function () = () in discrete time, we
should use  = 1 for  = 0 and  = 0 otherwise, rather than the commonly-defined

                                    201
        202                                                          F. Discrete convolution

EX F.1  Kronecker delta function,  = 1 for  = 0 and  = 0 otherwise. This is done to maintain the
        analogy with the continuous-time Dirac delta function having unit area, (B.9).

            Convolve discrete sequence  = [1, 2, 3] with sequence  = [4, 5, 6], according to
             =   , (F.2).

            Solution Because  and  are finite length sequences (as usual in practice), the
            summation bounds for  in (F.2), as well as the shift  in the third step above, have
            to be adjusted to the lengths of  and , here  = 3.

              In Table F.1 sequence  has been mirrored, and in Table F.2 it has been shifted, in
            this case by  = 1 the convolution output is =1 = 5 + 8 = 13 (there is `overlap' only
            for  = 0 and  = 1). The full result is shown in Table F.3.

              Convolution of discrete sequences can be done in Python through numpy.convolve
            it can output either the `full' result, as in Table F.3, or the so-called central or `same'
            part  = [13, 28, 27] for index values  = 0, 1, 2 just like the given input.

             Table F.1: Convolution of discrete sequences  and : sequence  has been mirrored.

              -2 -1 0 1 2

                 123

             -   6 54

        Table F.2: Convolution of sequences  and : mirrored sequence  has been shifted by  = 1. The
        bottom row lists the products -, which - summed over  - yields the convolution output for index
         = 1.

                 -2 -1 0 1 2

                        123
             1-  654

             1-        58

        Table F.3: Result of convolution of sequences  and :  =   .

                                                    01234
                                                   4 13 28 27 18
                             G

                         DTFT addendum

In Chapter 11 the Discrete-Time Fourier Transform (DTFT), (11.5), and its inverse, (11.8),
were introduced and discussed. In this appendix, a missing step in the DTFT proof is provided,
the proof of the inverse DTFT is given, and some DTFT pairs are presented.

G.1 Addition to proof of DTFT

In Section 11.1 the Discrete-Time Fourier Transform was derived, where (11.4) introduced the
complex exponential Fourier series coefficients  of signal (), stating without proof that:

 =  (),                                                                                                           (G.1)

with sampling frequency , and () the Fourier transform of (), evaluated at frequency
 = . The proof is as follows.

Proof Since () is periodic with , its fundamental frequency is . We can compute
the complex exponential Fourier series coefficients of (), (4.3):

                     
 = 1  ()-2 d =    ( + )-2 d
        
          0         0 =-

           

        =    ( + )-2 d

                   =- 0

Substitute  =  + , then  =  -  and d = d when  = 0,  =  when  = ,
 =  +  = ( + 1):

                    (+1)

 =    ()-2(-) d

               =- 

The expression -2(-) in the integrand can be written as:

                                                                                               =1  2        = -2

-2(-) = -22  = -2

                                                                                                   =1  ,  

Hence:

         (+1)             

 =                  ()-2 d =   ()-2 d =  ()

               =-                                              -

                    203
204                                                          G. DTFT addendum

G.2 Proof of inverse DTFT

In Section 11.3 the inverse Discrete-Time Fourier Transform was given:

        = iDTFT{()} =  ()2 d                                                       (11.8)

                                                   

The proof is as follows.

     Proof We copy the expression for the DTFT (11.5) but, for the sake of the proof we use
     time index  instead of  (both  and   ).

                                                                                   (11.5)

       () =   -2

                              =-

     Insert this expression for () in the definition of the inverse DTFT, (11.8):

                                                           

        =  (  -2) 2 d =    -2(-) d

                           =-                            =-

                           

           =     -2(-) d

                           =- 

     When  =  the integral on the right-hand side equals , and when    the integral

     is zero. When, for instance,  -  = 1, we integrate the complex exponential function

       -2  =  -2           from  = 0 to :  the complex exponential function completes one cycle

                         

     in the complex plane and the result of the integration is zero. The same is true for other

     values of ( - ), then the complex exponential makes  -  cycles, the integration of which

     is also zero. Hence, only one term of the summation remains, and we obtain:

        =    = 

G.3 DTFT pairs

Because the DTFT plays a marginal role in this book, only a few DTFT transform pairs are

given. Table G.1 lists some common pairs, stating the expression for () for frequencies
- 2 <   2 . Recall that () is periodic with .

    When consulting other resources, for more DTFT pairs, care must be taken regarding the

scaling of the DTFT, and especially the scaling of the Dirac impulse function in discrete time,
when applicable. Consider for example the DTFT of a (properly sampled, i.e., 0 < 2 ) cosine
function, the mathematical expression of which reads:

      = cos[20]

With  = 2 and 0 = 20, [8] states that its DTFT (within the range - <   ,
which is, because  =  =   , equivalent to -  <    ):
                2  2                                 2  2

     () =  (( + 0) + ( - 0))

Our definition of the DTFT, (11.5), scales this () with , and Dirac impulses need to be
G.3 DTFT pairs                                                                            205

Table G.1: Common DTFT pairs.

 () for - 2 <  <  2                                        remarks

pulse, sinc and window                                     pulse,   0, length 2 + 1
                                                           see proof below
                           sin (2 ( 2+1 ))                 note that (0) = (2 + 1)
 1 -                     2 2
                                                           see proof below 0 < 0 <  2
 0 else                          sin ( 2 )
                        (  )                               shifted pulse, length   1
0sinc[0]                                                   see Exercises book
                              0                            note that (0) = 
 1 0-1
                           sin (2 (  )) -1                 direct substitution in (11.5)
 0 else                  2 2 -2( 2 )                       inverse proof,
                                                           direct substitution in (11.8)
                               sin ( 2 )                   direct substitution in (11.5)
                                                           inverse proof, use (11.8)
in-the-limit                                               -  2 < 0 <  2
                                                           inverse proof, use (11.8)
                                                           0 <  2
                                                           inverse proof, use (11.8)
1                       ()                                 0 <  2

-0                       -20
 20                     ( - 0)
cos[20]                 12 (( + 0) + ( - 0))
sin[20]                  12 (( + 0) - ( - 0))

re-scaled when changing the running variable, (B.13). Hence:

  () =   (( + 0) + ( - 0)) =                            1  (( + 0) + ( - 0))

                                               2

                   1
              = (( + 0) + ( - 0))

                   2

which equals (), (5.16), for this band of frequencies.

Note that in Table G.1 the DTFT in-the-limit of the Kronecker delta function  ( = 1 for
 = 0 and  = 0 for other ) yields . Recall from Appendix F that in situations where we
would like to maintain the analogy with the continuous-time Dirac delta function, having unit
area, we should multiply  by 1 . The DTFT of the scaled Kronecker delta, 1 , equals 1,
see (5.14). The same holds for the DTFT of the shifted Kronecker delta function -0.
206                                                                                                          G. DTFT addendum

    Proof of DTFT of pulse function (width 2, an even function in , with 2+1 (odd number)
samples):

     Proof Direct substitution of the pulse function in the DTFT, (11.5) results in:

                                          

     () =   -2 =   -2

       =-                                 =-

     Substitute  =  + , then  =  - , we get:

       2                                                   2

     () =   -2(-) = 2  -2

       =0                                                  =0

     Define 2 =  - 1, then  = 2 + 1 and  = -1 :

                                                                                                  2

                                                   -1

     () =  2( 2-1 )  (-2)

                                                    =0

     on the right-hand side we obtain the geometric series identity, (A.22), with  = -2 and
      = . Hence:

     () =  2( 2 -1 ) ( 1 - -2 -2 )                                                                                            (G.2)
                                        1-

     The denominator can be written as - ( - -) = -2 sin( 2 ). Sim-

                                                                                                                                                               2

     ilarly, the numerator can be written as -2 sin( 2 ). Substitute in (G.2):

                                                                                                                 2

           -1 -2 sin( 2 )                                                                            sin (2 (  ))
     () =   -2 sin( 2 2( 2 ) 2 ) =                                                                                      2     (G.3)

                                                                                                     sin ( 2 )
                                                        2                                                 2

     which, with  = 2 + 1, completes the proof. () is real, even in .1

Proof of DTFT of discrete-time sinc function:

     Proof Use the inverse DTFT, (11.8):

                                                                                                     0

                                                                                                     2
      =  ()2 d =   (  ) 2 d =  2 d
                                          0
                                                                                                     - 0
                                                                                                     2

     = 1 [2  ] 20 1  = (0 - -0) = 2 sin[0]

                                                                    0
     2     - 2 2                                                                                                           2

     = sin[0] 0 = 0 sin[0] = 0sinc[0]
        0  0

     A real, even function in , because () is real, even in .

1The right-hand term in (G.3), without the , is known as the Dirichlet kernel.
                                                                        H

                                                                     DFT addendum

In Chapter 12 the Discrete Fourier Transform (DFT), (12.5), and its inverse, (12.6), were
introduced and discussed. In this appendix a formal proof of the inverse DFT is given, a
matrix interpretation of the DFT presented and some common DFT pairs discussed.

H.1 Proof of inverse DFT

Proof We copy the expression for the DFT (12.5), but, for the sake of the proof we use
time index  instead of :

               -1                                                              (H.1)
                             - 2 

                                                  

 =   

                =0

Insert this expression for  in the definition of the inverse DFT (12.6):

    1 1 -1   -1 - 2   2  1 -1 -1 - 2 (-)
 =    (    )   =   (    )                                                      (H.2)

    =0       =0                                                      =0 =0

where the exponential   could be put into the second summation (over 2  ), because it
is not a function of . Changing the order of summation yields:

    1 -1 -1   2 (-)                                 1 -1      -1
 =                                                                      2 (-)
                                                    =                          (H.3)

    =0 =0                                                     =0 =0

where  could be taken out of the second summation (over ) because  is not a function
of . To compute the summation over  at the right-hand side of (H.3), we use the geometric

series identity:

-1  1 -                                             for   1
                                                    for  = 1
  = { 1 -                                                                      (A.22)

=0  

With  =   in (H.3), this leads to: 2 (-)

       1 -1 1 - 2(-)
 =    =0 1 -   2 (-)

                                                              207
208                                                             H. DFT addendum

       with ,   {0, ... ,  - 1}. Assume that   , then 2(-) = 1, and  2 (-)  1, as
        -   {-( - 1), ... , -1, 1, ... ,  - 1}: the numerator becomes zero, the denominator does
       not, the expression is zero hence the summation is zero for   .

            This leaves only the term for  = , then  = 1, the summation becomes , we obtain:

              1                                                                      (H.4)
        =  () = 

       The inverse DFT has been proven.

H.2 DFT matrix

Recall that the Discrete Fourier Transform turns  samples of real-valued signal (), that is,

, with  = 0, 1, ... ,  - 1, into  numerical evaluations of DTFT ,(), i.e.,  complex
numbers , with  = 0, 1, ... ,  - 1. The computation of each single element  implies a
summation over  in (12.5),  = 0, 1, ... ,  - 1, and this can be cast as the multiplication of
row  of a matrix  with vector (0, 1, ... , -1):

                                                             0

         2        2                        2       2          1 
      =  ( -  0 -  1 -  2  -  (-1) )  2 
                                                             

                                                              -1 

for  = 0, ... ,  - 1. The whole operation can then be described in a vector-matrix-vector form:

         -0 2  0     -0 2  1                  -0 2  2  -0 2  (-1)                    
     0  -1 2  0 -1 2  1 -1 2  2                              -1  2 (-1)  0
 1      2                                                                              1 
 2  =   -2  0
                               2                        2                         2
                     -2  1                    -2  2  -2  (-1)   2 
                                                                                     
                                                                
 -1                                                          -(-1) 2 (-1)  -1 
         -(-1) 2 0 -(-1) 2 1 -(-1) 2 2
        

                                              =

                                                                                     (H.5)

Matrix , referred to as `the DFT-matrix', is square and has dimensions  by . As both

indices  (frequency) and  (time) run from 0 to  - 1, the above matrix  is symmetric,
hence  = , the matrix equals its transpose.

    In fact, with (12.5) and (12.6) we can verify that -1 = 1 , where the superscript

                                                                                                                    

`' denotes the Hermitian of a matrix, meaning that the elements of matrix  need to be

transposed, denoted by `', and complex conjugated, denoted by `'. The complex exponential
- 2   as element (, ) of matrix  in (12.5) becomes  2   as element (, ) of matrix
-1 according to (12.6), with a factor of 1 in front of the matrix (included in -1).

                                                                            

    Applying the DFT (12.5) and then the inverse DFT (12.6) means that the original  time
domain samples are returned, i.e., -1 = . Multiplying (materialized by the sum over )
row  of -1 with column  of , yields zero if   , and yields one if  = .

    Element (, ) of matrix  reads - 2  , and element (( - ), ) reads -(-) 2  .
The latter can be written as - 2   2   =  2  , because -2 = 1 for all   . This
H.3 DFT pairs                                                                               209

means  that  ,  =          ,  or,  that  the  th  row  of  matrix     equals  the  complex  conjugate
                     (-),

of the ( - )th row.

Consider  = 6 and evaluate the complex exponentials in matrix , (H.5).                                   EX H.1

Solution The resulting -matrix reads:

                1          1             11                   1       1

                     1 -  3 - 1 -  3 -1 - 1 +  3                   12 +  3  2 
               1     2  2          2     2             2      2

                1 - 1 -  3 - 1 +  3 1 - 1 -  3 - 1 +  3 
       =             2  2          2     2             2      2    2  2

               1        -1               1 -1                 1       -1 
                1 - 1 +  3 - 1 -  3 1 - 1 +  3 - 1 -  3 
                     2  2          2     2             2      2    2  2

               1     1 +  3 - 1 +  3 -1 - 1 -  3                   21 -  23 
                     2  2          2     2             2      2

The above-mentioned property of , namely that its th row equals the complex con-
jugate of the  - th row clearly shows for rows  = 1 and  =  - 1 = 5, and for
rows  = 2 and  =  - 2 = 4. Note that for  = 3 we have  -  = 3, so that this row
equals the complex conjugate of itself. Recall that, in discrete-time, counting starts at
zero.

H.3 DFT pairs

Some common DFT pairs are listed in Table H.1. In this table, the Kronecker delta function
is used in the discrete time () and discrete frequency () domains. Further note that in
the DFT of the complex exponential function, the cosine and sine functions, we consider only
the frequency 0 equal to  times the frequency step size . The function then completes
an integer number  cycles in the time duration  = . With 0 =  =  1  there is no
spectral leakage. E.g., the cosine function equals:

                                   1                       2
cos[20] = cos[2(  )] = cos[  ]                                                              (H.6)

Table H.1 shows that the DFT of (H.6) equals:

                                                                                            (H.7)
 = 2 (- + -(-))

meaning that all  are zero, except for  = , specifying the `positive frequency'  2 , and for
 = ( - ), specifying the `negative frequency' ( - ) 2 . This follows from:

                                                                                                       

cos[ 2 ] = 1 ( 2   + - 2  )
                     2

                   = 1 (  2  + -  2   2 )
                        2 =1

                   = 1 (  2  + (-)  2 )
                        2
210                                                    H. DFT addendum

Table H.1: Common DFT pairs.

 for 0     - 1                   for 0     - 1         remarks

window, periodic sinc                                  shifted pulse, length 1    
                                                       see Exercises book
                                sin [2  (  )]      -1  note that 0 = 
 1 0-1                             2 -2  ( 2 )
                                sin [ 2  ]             periodic, phase-rotated sinc
 0 else                                        2       dual of DFT of window
                                                       note that 0 =  

1 sin [2  ( 2 )] -2  -1  ( 2 )                         direct substitution in (12.7)
              2                  1 0-1
                                                       inverse proof,
                                 0 else                direct substitution in (12.8)
      sin [ 2 ]                                        direct substitution in (12.7)
                                                       0  0   - 1
in-the-limit                                           inverse proof, use (12.8)
                                                       1-1
                                  
                                                       inverse proof, use (12.8)
1                                                      1     - 1 (even )
-0
 2                                     - 2  0                          2

                                                       1    -1 (odd )
                                 -
                                                                         2
cos[ 2 ]                        
                                                       see Figure 12.4
                                 2 (- + -(-))          inverse proof, use (12.8)
                                                       1     - 1 (even )
sin[ 2 ]                             
                                                                       2
                                - 2 (- - -(-))
                                                       1    -1 (odd )

                                                                         2

                                                       see Figure 12.4
                I

     Spectral analysis in practice: full
                                       procedure

In this appendix we summarize all steps, starting from a theoretical (deterministic) continuous-
time signal, to spectral analysis in practice. That is, we discuss the full procedure from mea-
suring continuous-time signal () for  seconds, to computing the Discrete Fourier Transform
(DFT) coefficients , which typically form the basis for further spectral analysis, such as
estimating the signal's Power Spectral Density (PSD). We go through the following steps:

   1. Apply time window on continuous-time signal (), yielding (),

   2. Sample () to obtain (), and

   3. Compute and evaluate Fourier transform (), at discrete frequencies, in the fre-
       quency domain.

I.1 Window - time domain

As an example we take (again) signal () = sinc2(), with as its Fourier transform () =
(), (7.7) (here we set  = 1) both are shown in Figure I.1.

1.0              1.0                                                       1.0              1.0

x(t)0.5                                                                    0.5
                                                                    X(f )

0.0                                                                        0.0

     -3 -2 -1 0  1    2  3                                                      -6 -4 -2 0  2    4  6

     t [s]                                                                      f [Hz]

Figure I.1: Aperiodic continuous-time signal () = sinc2() at left, and its Fourier transform () = () at
right. Because signal () is real and even, its Fourier transform () is also real and even.

    In the first step we apply a rectangular window with duration  = 4 s. The window function
() and its Fourier transform () are shown in Figure I.2.

    The time-windowed signal () and its Fourier transform () are shown in Figure I.3.
The consequence of applying the time window is leakage: the peak gets rounded, and small

                         211
212                              I. Spectral analysis in practice: full procedure

        1.0                                 4                                               4

w(t)    0.5 T = 4 s 2               W (f )

                                                                             0
        0.0

             -3 -2 -1 0    1  2  3                                              -6 -4 -2 0     2  4  6

             t [s]                                                              f [Hz]

Figure I.2: Rectangular window () at left, with  = 4 s, and its Fourier transform () at right. Because signal
() is real and even, its Fourier transform () is also real and even.

        1.0                                 1.0                                             1.0

xw (t)  0.5                         Xw(f )  0.5

        0.0                                 0.0

             -3 -2 -1 0    1  2  3                                              -6 -4 -2 0     2  4  6

             t [s]                                                              f [Hz]

Figure I.3: Time-windowed signal () = ()() at left, and its Fourier transform () = ()  () at
right. Both () and () are real and even.

`wobbles' appear in the tails, see Figure I.3 at right, and carefully compare with Figure I.1 at
right. Leakage is covered in Chapter 8 and similarly shown in Figure 8.3 at right, though there
a shorter window was used,  = 3 s, as to emphasize the effects of leakage.

I.2 Sampling - time domain

With (9.7) in Chapter 9 we found the Fourier series expression for the periodic impulse train
function (), (9.4), to be:

                      =                                                                              (I.1)

        () =  2

                    =-

    In Chapter 5 we found the Fourier transform (in-the-limit) of () = 1 to be () = (),
(5.15). Applying the frequency translation theorem of Chapter 6, (6.6), to () = 1 yields:

        ()20 = 20          () = ( - 0)                                                               (I.2)

                         

We apply this result to every term of the above Fourier series expression for the periodic
impulse train function, and, using linearity, obtain its Fourier transform:

                                                                                                     (I.3)

        () =  ( - )

                     =-

We find that the Fourier transform of a periodic impulse train function in the time domain,
(9.3), is also a periodic impulse train function, in the frequency domain. Both are shown in
I.3 Evaluating Fourier transform at discrete frequencies                                        213

                1

           0.4  3                                   1.0
                                                                             3
p(t)       0.2                             P (f )
                                                    0.5

           0.0                                      0.0

                -3 -2 -1 0    1      2  3                -6 -4 -2 0                     2    4  6

                   t [s]                                                        f [Hz]

Figure I.4: Periodic impulse train (), (9.3), as sampling function at left, with sampling frequency  = 3 Hz,
sampling interval  = 13 s, and corresponding Fourier transform () at right, with period .

           1.0                1.0                   1.0                                 1.0

fs xsw(t)  0.5                             Xsw(f )  0.5

           0.0                                      0.0

                -3 -2 -1 0    1      2  3                -6 -4 -2 0                     2    4  6

                   t [s]                                                        f [Hz]

Figure I.5: Continuous-time model of the windowed and sampled signal () at left ( = 3 Hz,  = 13 s,
 = 4 s and  = 12), shown here as () (stems representing () as the weights of the Dirac pulses),
and Fourier transform () at right.

Figure I.4, for a sampling frequency  = 3 Hz, sampling interval  = 13 s, and, consequently,
the period of () at right is  = 3 Hz.

    The continuous-time model of the windowed and sampled signal () is obtained by
multiplication of () by (), (9.2). Its corresponding Fourier transform () can then be
obtained through convolution: () = ()  (), (6.8), see Figure I.5. Similar as found
in Chapter 9 we obtain copies of the spectrum, see (9.10), though there the result was derived

without making use of convolution. A careful inspection of, and comparison with Figure 9.6

reveals small differences. Recall that with Figure 9.6 we considered an infinite length signal

(no window was applied yet). In Figures I.3 and I.5 at right one can notice small effects of

leakage due to time-windowing.

    We first applied a time window () = ()(), and then sampled the signal to obtain
() = ()() = ()()(). The corresponding Fourier transforms are () =
()  () and () = ()  () = ()  ()  (). The order of the operations
can be changed, as both multiplication and convolution are commutative.

I.3 Evaluating Fourier transform at discrete frequencies

Fourier transform () is periodic in frequency, with period . Therefore, considering only
one `period' suffices, for instance the range of frequencies between 0 and ,   [0, ). The
last step of the entire procedure is then to evaluate the Fourier transform only at discrete

frequencies. This can be considered as multiplication of (), a function continuous in

frequency, by an periodic impulse train in the frequency domain. In Chapter 12 the frequency

step size was set to  =  = 1 Hz. The Discrete Fourier Transform (DFT) turns  time
                                   
214                          I. Spectral analysis in practice: full procedure

      4                         1.5             1
      24
                                                4

q(t)                            X Q(f ) k1.0

                                         0.5

      0                                  0.0

           -4  -2  0      2  4                -6 -4 -2 0    2  4  6

                   t [s]                           f [Hz]

Figure I.6: Periodic impulse train () in frequency domain at right, with period  = 1 = 1 Hz, and corresponding

                                                                                                                                                                         4

time domain function () at left, with period  = 4 s.

      1.0                                  1.0

xn    0.5                                  0.5

      0.0                                  0.0

           -4  -2  0      2  4                  -6 -4 -2 0  2  4  6

                   t [s]                           f [Hz]

Figure I.7: The DFT evaluates Fourier transform () only at discrete frequencies, with step size  = 1 = 14 s,
its coefficients  are shown at right. Correspondingly, the discrete time sequence  in the time domain at left,
is considered to be periodic by the DFT, with period  = 4 s.

domain samples into the evaluation of the corresponding Fourier transform at  discrete

                                                                                                                              

frequencies in the interval [0, ). The periodic impulse train () =  ( - ) is shown

                                                                                                                           =-

in Figure I.6 at right, for  = 1 Hz. For completeness, also the corresponding time domain

                                                         4

representation () is shown at left, with a period of  = 4 s. The inverse Fourier transform

of the periodic impulse train function in frequency is a periodic impulse train function in time,

in this case scaled by one over the frequency step size .

    Though typically of lesser interest to the practical user, the consequence of evaluating the

Fourier transform only at discrete frequencies is that the DFT actually considers its input, the

discrete-time sequence  to be periodic, with period , see Figure I.7.
    Multiplication of () by () in the frequency domain at right, is equivalent to convolu-

tion of () and () in the time domain at left. This can also be observed from the expres-
sion for the inverse DFT (12.6). From this equation follows:  = +, as  2   =  2  (+)
with both ,   , see also Section 12.3.
           J

Confidence interval of periodogram

In Chapter 14 the periodogram was presented as an estimate of the spectral density of a
continuous-time signal (), based on  discrete-time samples  of that signal. The peri-
odogram is computed at a set of frequencies  =  =   as (14.2):

() = 1 ||2                                                        (14.2)
           

Here,  are the DFT coefficients of the DFT-ed sequence , (12.5):

              -1                                                  (12.7)

                                          2

 =   -  

               =0

Using Euler's formula, (C.1),  can be written as:

-1 2 -1 2
 =  (   cos(  ) -    sin(  ))                                     (J.1)

=0                                           =0

Substituting this in (14.2) with the squared modulus, and  = :

    -1                                       2 -1 2
() =  {(   cos( 2 )) + (   sin( 2 )) }
 =0  =0                                                           (J.2)

    In practice, we work with random signals: signals with noise. As a consequence the
periodogram estimate will not be exact, but instead be subject to uncertainty and exhibit
variability upon repetition of the experiment. To judge the significance of findings on the
basis of the periodogram, a statistical confidence interval for the periodogram is developed.

J.1 Estimating amplitudes

Our input 0, ... , -1 consists of  random variables, for which we assume here that they
are normally distributed, with a zero mean, and that they are all uncorrelated and have the
same variance, such that the variance matrix is the identity matrix , scaled by variance 2.

                                                           215
216                                        J. Confidence interval of periodogram

Casting the  random variables in a vector, we have:

         0           0           100

      1   ( 0  , 2  0 1  0     )                                            (J.3)

      -1        0  001

                                     

in terms of an -dimensional, multivariate normal distribution ().
    Next, we observe that expression (J.2) for the periodogram is closely related to the (simul-

taneous) least-squares estimation of the amplitude of a cosine and the amplitude of a sine,
both with frequency , based on the input data.

    The functional model, in terms of the expectation  (mean), reads:

            0                cos(20)       sin(20)

       1  =  cos(21)                       sin(21)  (  )
                                                       

     -1 cos(2(-1))sin(2(-1)) 

                                         

with  × 2 matrix , and 2 × 1 vector .
    Evaluating the spectrum, as we do with the DFT, for frequencies  =  =   =  1 

with  = 0, 1, ... ,  - 1 (and in the following we exclude  = 0 (the zero frequency), and  = 

                                                                                                                                                                       2

(the Nyquist frequency) for  even), matrix  becomes:

               cos( 2 0)         sin( 2 0)
     =                           sin( 2 1)  
                          
                          2             
                             sin( 2 ( - 1)) 
               cos(  1)
                     

          cos( 2 ( - 1))

The least-squares estimator for vector  follows as  = ()-1, where superscript `'
indicates matrix transpose. We first note that, surprisingly simple:

      =  ( 1 0 )
               2 01

as with cos2  = 1 + 1 cos(2), (A.4), and similar goniometric identities:

                                22

     -1 2  1 -1 2 
      cos2(  ) = 2 + 2  cos(2  ) = 2
     =0                      =0

     -1 2  1 -1                       2    
      sin2(  ) = 2 - 2  cos(2  ) = 2
     =0                      =0

     -1 2               2 1 -1 2
      sin(  ) cos(  ) = 2  sin(2  ) = 0
     =0                          =0

where, for  > 2, the sum of  equidistant samples (  runs from 0 to -1 ), taken over 2
                                                                          
periods of a cosine or a sine, which both are zero mean functions, equals zero (with integer
 = 1, ... ,  - 1, excluding  = 0 and  =  if  even).
                                      2
J.2 Confidence interval for periodogram                                               217

     The least-squares estimator for  becomes:

           2 =0 -1  cos( 2 )
      = ( ) = ( -1           2 )                                                      (J.4)
                =0  sin(  )

With the assumption of (J.3), observing that both amplitude estimators  and  in  are
linear combinations of the signal samples 0, 1, ... , -1, and applying the mean propagation
law, we find that vector  has zero mean, and through the variance propagation law we find

variance matrix:

      = 2()-1 = 2 2 ( 1 0 0 1 )                                                       (J.5)

Normality as a distribution is maintained, see Section 2.13 in [5], hence the amplitude esti-
mators are distributed as:

     (  )  (( 0 ) , 2 2 ( 1  01 ))                                                    (J.6)
                  0      0

     

J.2 Confidence interval for periodogram

We find, under the assumed (J.3) and consequently (J.6), that the quadratic form -1 
2(2, 0) has a central Chi-squared distribution with 2 degrees-of-freedom.

    We use (J.4) and (J.5):

                     -1      2      2 -1 2
             12                                         2
      -1                                                      2
        = 2 {(   cos( )) + (   sin( )) }   (2, 0)                                     (J.7)
                                                        
                     =0                         =0

so that we can evaluate the probability , as 1 - , of this quadratic form lying in between
the two bounds as:

      (2  (2, 0)  -1  2 (2, 0)) = 1 - 

            1- 2 2

where 2 is the threshold value of the Chi-squared distribution for a right-sided tail probability

of  2 , and 2   for a right-sided tail probability of 1 - (and consequently for a left-sided tail
     2 1- 2 2
probability of  ) they are, respectively, the upper and lower 100(  )% points.
          2                                                2
     The above -1 (J.7) is close to, but not exactly identical to () (J.2). We have:

     -1 = 22 ()

and the above probability interval can be turned into:

      (  2  (2, 0)   2 ()   2 (2, 0)) = 1 -  2
       2 1- 2                22

or:

       2       1         1   2           1
     ( 2 2             2 2                      )=1-
         (2, 0) ()     (2, 0)
             1- 2 2
218                                               J. Confidence interval of periodogram

and with:

        ( 22()  2   (2, 0) 22() ) = 1 -   (2, 0)

                    2 1- 2

The expectation or mean of a central Chi-squared distributed random variable with  degrees-
of-freedom equals , and hence with (J.7) (-1) = 2, so (()) = 2 = (), where
the latter follows from assuming the periodogram to be an unbiased estimator of the spectral
density. This yields a confidence interval for the periodogram:

      ( 22()  (  (2, 0) )  22() ) = 1 -   (2, 0)                         (J.8)

              2 1- 2

see also Section 6.2 in [5] and Section 8.5.4 in [17].
    This confidence interval expresses the uncertainty in the spectral density estimator (),

shown as the variability of the value along the vertical axis in, e.g., Figure 14.4 at right, due to

the stochastic model assumed in (J.3), basically considering the noise in the signal samples,
quantified by variance 2. Confidence interval (J.8) is evaluated per frequency .

J.3 Variance of periodogram

The variance of a central Chi-squared distributed random variable with  degrees-of-freedom
                                                  2 2
                         2      2                               42    2
equals 2, and hence -1 = 4, so ( ) = ( ) 4 =   = ((())) , with the
                                                  2

above expectation. This is an important finding, stating:

     () = (())                                                           (J.9)

the standard deviation of the periodogram estimator, as a measure for the uncertainty in the
estimate, equals the target value of the estimate itself (with  the expectation) the uncertainty
is as large as the quantity of interest.

J.4 Advanced spectral estimation

To improve the precision of spectral estimation the data record is split into  disjoint sub-
records or segments, each with  samples (for convenience assuming  an integer multiple

                                                           

of ) and record/window length  (we assume the segments to be non-overlapping and ad-

                                                            

jacent). The periodogram for each segment still satisfies the above confidence interval (J.8)

as the distribution does not depend on  nor  (see (J.7)), although the frequency step size
of the estimator increases from 1 for segment-length , to  with a length  segment
                                                                      
resolution gets poorer.

We can then average the  per-segment-periodogram estimates to produce one final

estimate, and averaging brings down the uncertainty: by averaging the variance reduces by

a factor of . The confidence interval for a spectral density estimator with averaging over 

segments (see Section 14.5), see Section 8.5.4 in [17], reads:

        ( 22()  (  (2, 0) )  22() ) = 1 -   (2, 0)                       (J.10)

                    2 1- 2

This equation reduces to (J.8) for  = 1.
                                                                     K

                                                                    Correlation

In this appendix we define the cross-correlation function of two real-valued signals () and
(), which reduces to the auto-correlation for () = (). First we cover deterministic
signals, then we cover random signals, both in continuous time and discrete time.

K.1 Deterministic signals

K.1.1 Continuous-time signals

The cross-correlation function of two deterministic, continuous-time, and real-valued signals
() and (), see [12], reads:

                                                                 

12                                                          12
() = lim  ()( + ) d = lim  ( - )() d                                (K.1)
                                                             
   - -
   2                                                             2

and it is a function of time shift , referred to as the time lag between the two signals. In
the first integral, signal () is shifted by  to the left, and, in the second integral signal ()
is shifted by  to the right. Both lead to exactly the same situation of the two signals with
respect to each other. Time-lag   , and thus can also be negative.

    The cross-correlation function is a measure, as a function of time-lag , of the similarity
between the two signals () and (). For each value of , it equals the time average of the
product of () and ( + ).

    When both signals () and () share the same unit, e.g., [V], then the cross-correlation
function has the square of that unit, e.g., [V2].

    The autocorrelation function of () is obtained by replacing ( + ) by ( + ) in the first
expression of (K.1), or replacing () by () in the second expression. Then we measure the
similarity of signal () with a time-shifted version of itself. Considering the autocorrelation
function for zero time-shift  = 0 yields:

                                                                    (K.2)

                              12

                                                     2

 = ( = 0) = lim   () d
                         

                                            -

                                                         2

the average (normalized) power of signal ().

                                                            219
        220                                                           K. Correlation

        Definition (K.1) is common in statistics, though alternatively we may find:

                        

                     1  2

             () = lim  ( + )() d                                                           (K.3)

                        -

                        2

        see in particular [13] and it is often implemented accordingly with discrete-time signals in
        programming environments, such as Python. The two definitions are related through  () =
        (-) = (), hence by swapping  and .

            Finally, we comment that we use the time average in the above expressions for the cross-
        correlation function. Later, with stochastic or random signals we use the ensemble average in
        the definition of (), and these two types of averages are equivalent if the random signals
        are ergodic. In many textbooks you may find, however, the cross-correlation function defined
        without time averaging (i.e., without the factor 1 in front of the integral in (K.1) and (K.3)).

                                                                                               

EX K.1  Compute the autocorrelation of signal () =  sin(20).
        Solution Using (K.1) we get:

                                                     

                                 12
               () = lim   sin(20)  sin(20( + )) d

                            

                                           -

                                                     2

        Using trigonometric identity (A.8), this becomes:

               2           
                           12
               () = lim   (cos(20) - cos(40 + 20)) d
                     2   -
                            2

        The integral is broken into two parts:

                        2                                                            

                            1                    11                                  2

               () =        lim { cos(20) [] 2  - [         sin(40 + 20)] }
                        2                       - 2  40                              -

                                                                                        2

                        2      2                11
               = cos(20) + lim {-                          sin(20) cos(20)}
                        2      2 20

                                                           =0

        where we used (A.10) in the last step. The end-result reads:

                           2                                                               (K.4)
               () = 2 cos(20)

        Autocorrelation measures the similarity of a signal with a time-shifted version of itself.
        For  = 0 we get maximum similarity ( = 0) = 2 2 . The sine is periodic, and hence
        for a time shift or lag of  = 1 (the period of the sine), the autocorrelation is again at

                                                            0

        maximum. The autocorrelation function () is periodic in .
K.1 Deterministic signals                                                        221

K.1.2 Discrete-time signals

Discretizing the integral in (K.1) with step size , the cross-correlation for discrete time,
infinite length sequences  and  becomes:

                                    

           2                     12
() = lim   + = lim  +                                                            (K.5)
                                  
                                      
          =- 2                      =- 2

with  = ,  =  and shift  = , and assuming  to be even for convenience. Index
   denotes the shift between the two sequences.

    Working with finite length  sequences (both sequences of equal length), we arrive at:

              1                                                                  (K.6)
() =   +

                         

where the bounds of summation over  depend on the lengths of  and , and on the shift
 (we need to satisfy both 0     - 1 and 0   +    - 1). And thereby the shift

itself is bound as well (||   - 1). The result () is a length 2 - 1 sequence.

Compute the cross-correlation of discrete sequence  = [1, 2, 3] with sequence  =              EX K.2
[4, 5, 6] according to (K.6).

Solution Both  and  are, as usual in practice, finite length sequences, here with
length  = 3.

For a shift of  = -1 we have the situation as shown in Table K.1. Sequence  is
delayed by one step. The overlap (and hence summation index ) runs from  = 1 to

 = 2. The result is ( = -1) = 2 × 4 + 3 × 5 = 23.

Table K.1: Cross-correlation of discrete-time sequences  and , for shift  = -1.
                                                 01234

                                 123
                             -1       456

Table K.2 shows the situation for a shift of  = 2. Sequence  is advanced by two
steps. The overlap is limited to one position and the summation has only one term,

namely for  = 0. The result is ( = 2) = 1 × 6 = 6.

Table K.2: Cross-correlation of discrete-time sequences  and , for shift  = 2.
                                            -2 -1 0 1 2 3

                                             123
                             +2  4 56

The full cross-correlation result is shown in Table K.3.

Table K.3: Cross-correlation () of discrete-time sequences  and .

                                 -2 -1 0 1 2

                             () 12 23 32 17 6
222                                                                            K. Correlation

Figure K.1: Three sample functions, also referred to as realizations, of a continuous-time random signal.

     Cross-correlation of discrete-time sequences can be done in Python through

numpy.correlate. The `full' output is obtained from correlate(x,y,"full")

and results in  () = [6, 17, 32, 23, 12], which equals the result in Table K.3, except
that the output sequence is just reversed. We get  () = () = (-), as

the implementation in Python is based on cross-correlation defined in (K.3). The indices

 for output   in Python actually run from 0 to 2( - 1), in this example [0, 1, 2, 3, 4].

     As a sidenote, we observe that in the Example in Appendix F on discrete convolution,

we used the same two data sequences  = [1, 2, 3] and  = [4, 5, 6]. Convolution and

correlation are very similar operations. Both involve shifting or sliding one sequence

with respect to the other, and for each shift, taking the sum of the element-wise product

of the two sequences. The key difference is that with convolution, according to (F.2),

sequence  is flipped or mirrored prior to the sliding. The result of convolution   

was given in Table F.3. Exactly the same result is obtained in the present example, with

Python, through first reversing the order of the second sequence , and then obtain
                         ,         
the  cross-correlation      where    refers  to  sequence    in  reversed  or  flipped  order,
                          
hence correlate(x,h[::-1],"full").

     Finally, the attentive reader may have noticed that we `forgot' to divide by  in this

example, see (K.6). In Table K.1, for  = -1, the overlap limited to two positions, and

in Table K.2, for  = 2, to only one position. In general the overlap is limited to  - ||

positions, and hence the factor in front of the summation in (K.6) should be adjusted
to 1 , taking the actual overlap between the two sequences into account.

     -||

K.2 Random signals

Having covered deterministic signals, we now turn our attention to random signals, which
result from observing stochastic processes. A stochastic process is a phenomenon which is
subject to uncontrolled variability and associated uncertainty, and additional variability (noise)
is typically involved in the observation of the process, see for example Figure 17.1.

    A random signal is a random variable which is a function of time . The variability in
outcomes of a random variable is mathematically modeled through a probability density func-
tion. Similarly we do so for the variability in outcomes of the (observable) process, and the
probability density function generally includes time as a parameter.

    Would we draw samples (i.e., repeat the experiment under identical circumstances), for
instance, from a normally-distributed random variable with mean equal to 7.2 and standard
deviation equal to 0.2, we could obtain: 7.1, 7.4, 7.6, 6.9 and 7.0. Similarly, we can draw
samples or realizations of a random signal, and several different functions of time would result,
as shown in Figure K.1. The set of all possible realizations is referred to as the ensemble. The
ensemble forms the random signal.

    Cross-correlation, and power spectral density are defined for random signals, and these
K.2 Random signals                                                              223

definitions involve an expectation (mean), in which the probability density function is involved.

K.2.1 Continuous-time signals

The cross-correlation of two random signals for  at time 1 and  at time 2 is defined as:

                                                                  

(1, 2) = ((1)(2)) =   (, ; 1, 2) dd                                             (K.7)

                                                     - -

with (, ) being the joint probability density function of  and . Expectation () refers
to the ensemble average, in this case of the product (1)(2).

    When random signals are (jointly) wide-sense stationary, as we assume throughout this

book, then the mean and cross-correlation do not depend on time (in brief, stationarity means

that the statistical properties of the signals do not change over time, see [5]). The cross-

correlation does not depend on time instants 1 and 2 in an absolute sense, but only in a
relative sense on time lag  = 2 - 1, and we have:

() = (()( + )) = (( - )())                                                      (K.8)

K.2.2 Discrete-time signals

In discrete time, a random signal can be interpreted as a sequence of random variables (also
referred to as a random sequence). Considering a random signal at a single discrete time
instant yields a random variable, which variability is described by a statistical distribution, e.g.,
the normal or Gaussian distribution.

    The cross-correlation of two (jointly wide-sense stationary) random sequences  and 
is defined as:

() = (+)                                                                        (K.9)

with index  denoting the shift between the two sequences, similar as in (K.6).

K.2.3 Cross-correlation estimation in practice

In practice, all we have is a single sample or realization of the random signal, i.e., one time
series of discrete-time measurements of the stochastic process, and we compute an estimate
for the cross-correlation or the power spectral density. It is key to understand that this esti-
mate is subject to uncertainty: the outcome is not exact, nor perfect. Would we repeat the
measurement under identical circumstances, then the second realization will differ from the
first one, see Figure K.1, and hence also the estimate computed from it.

    For the above approach to work, we also need to assume that the random signal is ergodic,
[13]. Ergodicity means that time averages of a sample function equal the corresponding
ensemble averages of the random signal (in practice we do have access only to the first one,
and we are after the second one). Ergodicity implies stationarity (only stationary random
processes can be ergodic).

    An estimate (indicated by the hat-symbol) for the cross-correlation, based on discrete time
sequences  and , as realizations of their respective random sequences, see [17], reads:

                 1 -1-                for   0                                   (K.10)
() =  -   +

                                  =0

and for  < 0 we have:

                 1 -1                 for  < 0
() =  +   +

                                =-
224  K. Correlation

though in practice it may be more convenient to use again (K.10), but swapping  and ,
and exploit (-) = (), as noted with (K.3).

    The factor 1 in front of the summation in (K.10) yields the unbiased cross-correlation

                           -

estimator, [17]. Using instead a factor of 1 , as in (K.6), yields the biased estimator.

                                                                            
                                                                                    L

                                                                                  White noise

In this appendix we present a theoretical continuous-time white noise random signal, and then
a band-limited white noise signal which can be realized in discrete time.

L.1 Theoretical white noise

A random signal is said to be white noise, when it has a flat power spectral density:

   () = 0                                                                                        (L.1)
              2

the PSD is constant over the entire range of frequencies from  = - to , with 0 a positive
constant, commonly used to denote the noise power spectral density, see [12].

    The autocorrelation of the (continuous-time) signal is then:

   () = 0 ()                                                                                     (L.2)
              2

a Dirac delta function, with weight 0 . The spectral density 0 , e.g., in [W/Hz] for a voltage
                                    2                                         2
signal, appears in the autocorrelation function and the Dirac delta function has the inverse

dimension of its argument, see Appendix B.

The autocorrelation function () and the power spectral density () are a Fourier trans-

                            

form pair ()  (), according to the Wiener-Khintchine theorem for a wide-sense station-

ary random signal, see [12], which is a subject beyond the scope of this book. Both functions

are illustrated in Figure L.1.

                                                                                          N0

                                N0                                                        2
R( )
                                                                    S(f )2

0                                                                          0

   -2            0                  2                                         -2  0           2

                  [s]                                                             f [Hz]

Figure L.1: Autocorrelation function () at left, and PSD () at right, of a continuous-time white noise signal.

                                            225
226                                                                                       L. White noise

                              N0B = 2 2N0

                                     1
                                    2B
R( )
                                                                    S(f )

     0                                                                     0

                        0                                                     -B  0       B

                         [s]                                                      f [Hz]

Figure L.2: Autocorrelation function () at left, and PSD () at right, of band-limited white noise signal.

    Considering a white noise signal at two time instants, no matter how far apart or close, the
above given autocorrelation function () implies that the two corresponding random variables
are uncorrelated. If they are not only uncorrelated but also independent, then the signal is
referred to as strictly white noise [13]. Ref. [5] states that the definition of white noise based
on uncorrelatedness is believed to be the more widely used one.

    If noise is not white, one refers to colored noise. The name white noise is an analogy with
white light being composed of all colors of light (all wavelengths/frequencies).

    A continuous-time white noise signal is a theoretical construct and cannot exist in reality
-- it would have infinite power , see (13.3).

L.2 Band-limited white noise

A band-limited white noise signal has a constant power spectral density over a bandwidth of
2 (with 0 <  < ) as:

                     0  for ||                                                                             (L.3)
        () = { 2        otherwise

                     0

The (double-sided) power spectral density can then also be expressed using the pulse function:

        () = 0  (  )                                                                                       (L.4)
                   2 2

covering a band from  = - to . It can be obtained from (L.1) by means of ideal low-pass
filtering, see also Figure 18.2. The corresponding autocorrelation function reads:

        () = 0sinc(2)                                                                                      (L.5)

which for    leads to (L.2), referred to as infinite bandwidth white noise.
    Figure L.2 shows the autocorrelation function () and the power spectral density () of

a band-limited white noise signal. The noise spectral density at right has a bandwidth of 2,
and correspondingly the autocorrelation function () shows its first zero-crossing at  = 1 .

                                                                                                                                                                    2

    The (average) power equals:

                 0
         =  () d = 2 = 0
        -               2

and this equals the noise variance 2, therefore:

        2 = ( = 0) = 0
L.3 Additive White Gaussian Noise (AWGN)  227

    Of practical interest is the discrete-time white noise signal with samples taken an interval of
 = 12 apart, that is, with sampling frequency  = 2. This results in a sequence of samples
drawn from uncorrelated random variables it is a white noise sequence (as we assumed with
(J.3)). Ref. [5] refers to a sequence of uncorrelated random variables as a purely random
process the process has `no memory'.

    The autocorrelation for samples of two random variables  = 1 seconds apart (and integer

                                                                                                                    2

multiples of it), with (L.5), is ( = 1 ) = 0. The sample instants then occur exactly at the

                                                                   2

so-called nulls of the autocorrelation function (the sinc function) in Figure L.2 at left: these
samples are uncorrelated. For other values of  there is correlation between the samples.

L.3 Additive White Gaussian Noise (AWGN)

Related to the topic of white noise, one often encounters the acronym AWGN for Additive
White Gaussian Noise.

    A measured or observed signal is often considered to consist of a deterministic signal of
interest, and added to that, a random white noise signal.

    White noise refers to (L.2) and (L.1), and does not imply anything else on the statistical
distribution of the random variable or process. Hence, white noise can also be Gaussian, but
it does not necessarily need to be.

    Gaussian noise is often assumed and used though, and the stochastic process considered
at a single moment in time is a random variable, which is then normally distributed.
                M

        Solving first-order differential
                      equation - example

In this appendix we demonstrate, by means of an example, how to analytically solve, in the
time domain, a first-order ordinary differential equation (ODE). The ODE is given as:

1 d()                                                   (16.9)
 d + () = ()

with input () and output (), and  > 0. We aim to express output () explicitly in
terms of (arbitrary) input (), where we assume that () is applied at time  = 0 and that
(0) = 0. This linear, first-order, constant coefficient, ordinary differential equation fully
describes a dynamic first-order system that is linear, time-invariant and causal. The example
is taken from [7].

    The above differential equation can describe how the temperature of an object, as a func-
tion of time, is related to the ambient temperature, or the velocity of a mass due to an applied
force in a mass-damper system in mechanical engineering, or the output voltage as a response
to the input voltage in an elementary resistor-capacitor (RC) circuit in electrical engineering.

M.1 Homogeneous solution

The solution to the homogeneous differential equation:

1 d()                                                   (M.1)
 d + () = 0

is found by assuming a solution of the form () = . Substituting this in (M.1) leads to
 = -. The homogeneous solution reads:

() = -                                                  (M.2)

M.2 Total solution

In order to find the total solution we use the technique of `variation of parameters', which
consists of assuming a solution of the form of the above homogeneous solution, but with
undetermined coefficient  replaced by a function of time (), which is to be found. Thus:

() = ()-                                                (M.3)

              229
230                   M. Solving first-order differential equation - example

Differentiating (and using the chain rule for differentiation), leads to:

     d() = ( d() - ()) -                                                     (M.4)
       d     d

    Next, substituting the assumed solution (M.3) and its derivative (M.4) in the original dif-
ferential equation (16.9), we obtain:

     1 - d() = ()  d() = ()                                                  (M.5)
          d           d

Solving for d() , i.e., integrating the right-hand side of (M.5) yields:

                       d

                                     

     () - (0) =   () d

                                   0

Using (M.3) at time 0: (0) = 0 = (0)-0, or (0) = 0 0, we find the time-varying
parameter () as:

                                                                             (M.6)

     () =   () d + 0 0

                    0

This result can be substituted in the assumed solution (M.3), resulting in:

                                                

     () = 0 -(-0) +   ()-(-) d

                                              0

Assuming that the input () is applied at  = -, hence 0 = -, and that 0 = (0) =
( = -) = 0, we obtain:

                                                                             (M.7)

     () =  ()-(-) d

                 -

M.3 Solution

Output () to input () can be found through solving the above integral (M.7). In order to
align with (16.6) we introduce the unit step function () and slightly rewrite (M.7):

          

     () =  () -(-)(-) d                                                      (M.8)

          -     = ( - )

where ( - ) `turns off' at  =  (and stays off). Output () follows as the convolution of
input () and impulse response (), (16.6). The interpretation of function () = -()

is discussed in Chapter 16.
                                   Bibliography

 [1] Wikimedia Commons, (n.d.), media file repository, for public domain and freely licensed
       educational media content.

 [2] T. Boil, Fourier series 3d - interactive demonstration, (2016), website.
 [3] J. G. Proakis and D. K. Manolakis, Digital signal processing, 4th ed. (Pearson Education

       Limited, 2014).
 [4] M. T. Heideman, D. H. Johnson, and C. S. Burrus, Gauss and the history of the fast

       Fourier transform, Archive for History of Exact Sciences 34, 265 (1985).
 [5] M. B. Priestley, Spectral analysis and time series (Academic Press, 1981).
 [6] M. R. Spiegel, S. Lipschutz, and J. Liu, Mathematical handbook of formulas and tables,

       3rd ed. (McGraw-Hill, 2009).
 [7] R. E. Ziemer, W. H. Tranter, and D. R. Fannin, Signals and systems - continuous and

       discrete, 4th ed. (Prentice Hall, 1998).
 [8] A. V. Oppenheim, A. S. Willsky, and I. T. Young, Signals and systems, 1st ed. (Prentice

       Hall International Editions, 1983).
 [9] A. Papoulis, The Fourier integral and its applications, 2nd ed. (McGraw-Hill, 1962).
[10] J. W. Cooley and J. W. Tukey, An algorithm for the machine calculation of complex Fourier

       series, Mathematics of Computation 19, 297 (1965).
[11] G. D. Bergland, A guided tour of the fast Fourier transform, IEEE Spectrum 6, 41 (1969).
[12] L. W. Couch, Digital and analog communication systems, 7th ed. (Pearson Prentice Hall,

       2007).
[13] A. Papoulis, Probability, random variables, and stochastic processes, 2nd ed. (McGraw-

       Hill, 1984).
[14] A. Schuster, On the investigation of hidden periodicities with application to a supposed

       26 day period of meteorological phenomena, Terrestrial Magnetism 3, 13 (1898).
[15] P. Welch, The use of fast fourier transform for the estimation of power spectra: a method

       based on time averaging over short, modified periodograms, IEEE Transactions on Audio
       and Electroacoustics 15, 70 (1967).
[16] M. Bartlett, Smoothing periodograms from time-series with continuous spectra, Nature
       161, 686 (1948).
[17] J. S. Bendat and A. G. Piersol, Random data analysis and measurement procedures, 4th
       ed. (Wiley, 2010).

                                                           231
232                                                      Bibliography

Attribution of photos in Chapter 1

All photos, except for the Citation II (Figure 1.4), were obtained from Wikimedia Commons:
                https://commons.wikimedia.org/

In the following table the photo rights are summarized, for details we refer to [1].

     (provisional)Title               URL (click)        license

     Figure 1.1 (from top left to bottom right)          public domain
                                                         CC0
     ISS astronaut                    Wikimedia Commons  CC BY SA
                                                         public domain
     X-ray of hand                    Wikimedia Commons  public domain
                                                         CC0
     Music studio                     Wikimedia Commons  public domain
                                                         public domain
     Valkyrie robot                   Wikimedia Commons  CC0
                                                         CC0
     Wind turbines                    Wikimedia Commons  CC BY 2.0
                                                         public domain
     Smartphone use                   Wikimedia Commons  public domain
                                                         public domain
     PSX controller                   Wikimedia Commons
                                                         copyrighted TU Delft & NLR
     MRI head                         Wikimedia Commons
                                                         public domain
     Quadcopter                       Wikimedia Commons
                                                         public domain
     Self-driving car                 Wikimedia Commons

     A380 cockpit                     Wikimedia Commons

     Mount Pleasant radio telescope Wikimedia Commons

     GPS satellite                    Wikimedia Commons

     Mars exploration rover           Wikimedia Commons

     Figure 1.4

     Citation II laboratory aircraft  not applicable

     Figure 1.7

     Opening day of the Tacoma        Wikimedia Commons
     Narrows Bridge, Tacoma,          Wikimedia Commons
     Washington, July 1, 1940

     Tacoma-narrows-bridge-collapse
Engineering Signal Analysis
from Fourier to filtering

Theory

This book, Theory, is an introductory textbook on the analysis of signals in time and frequency.
It takes an engineer's perspective and discusses how to characterize, analyze and operate on signals. The
basic theoretical concepts, Fourier series and transform, are x in continuous time. It then introduces discrete-
time signals, addressing how sampling and finite signal duration affect spectral analysis. It discusses the
discrete Fourier transform and its use in spectral estimation. The book concludes with an introduction to
linear systems and signal filtering.

The companion book, Exercises, contains hundreds of exercises, including answers and worked examples,
for studying and practicing the theory. Python scripts illustrate how to perform spectral signal analysis on a
computer.

Christian Tiberius                            © 2026 TU Delft OPEN Publishing
                                              ISBN 978-9-46384-901-2
TU Delft                                      DOI https://doi.org/10.59490/mt.247
Faculty of Civil Engineering and Geosciences  https://books.open.tudelft.nl
Geoscience and Remote Sensing
                                              Cover design:
Max Mulder                                    CYANETICA
                                              Sebastiaan de Stigter
TU Delft
Faculty of Aerospace Engineering
Control and Operations
