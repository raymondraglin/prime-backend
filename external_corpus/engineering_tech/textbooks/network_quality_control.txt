Network quality control

Peter J.G. Teunissen
NETWORK QUALITY CONTROL

                     P.J.G. Teunissen
Series on Mathematical Geodesy and Positioning
Е PJG Teunissen, first edition 2006
Е TU Delft OPEN Publishing, second edition 2024
ISBN: 978-94-6366-950-4 (Ebook)
ISBN: 978-94-6366-949-8 (Paperback/softback)
DOI: https://10.59490/tb.100

This work is licensed under a Creative Commons Attribution 4.0 International license

Keywords: precision, testing, reliability, free networks, connected networks,
constrained networks
Preface 2nd Edition

To promote open access, this new edition of Network Quality Control is published by TU Delft
Open Publishing instead of Delft Academic Press. The book builds on the foundations of
adjustment theory and testing theory to enable precise and reliable designs of geodetic
networks and their connections. As such, the book is a natural follow-on of the books
Adjustment Theory (2nd Ed. 2024) and Testing Theory (3rd Ed. 2024), both with TU Delft Open
Publishing.

September, 2024

Peter J.G. Teunissen
Foreword

This book is the result of a series of lectures and courses the author has given on the topic
of network analysis. During these courses it became clear that there is a need for refer-
ence material that integrates network analysis with the statistical foundations of parameter
estimation and hypothesis testing. Network quality control deals with the qualitative as-
pects of network design, network adjustment, network validation and network connection,
and as such conveys the necessary knowledge for computing and analysing networks in
an integrated manner.

    In completing the book, the author received valuable assistance from Ir. Hedwig Ver-
hoef, Dr. Ir. Dennis Odijk and Ria Scholtes. Hedwig Verhoef has also been one of the
lecturers and took care of editing a large portion of the book. This assistance is greatly
acknowledged.

P.J.G. Teunissen
December 2006
Contents

1 An overview                   1

2 Estimation and precision      9

2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9

2.2 Consistency and uniqueness . . . . . . . . . . . . . . . . . . . . . . . . 10

2.3 The Linear A-model: observation equations . . . . . . . . . . . . . . . . 13

2.3.1 Least-squares estimates . . . . . . . . . . . . . . . . . . . . . . . 13

2.3.2 A stochastic model for the observations . . . . . . . . . . . . . . 17

2.3.3 Least-squares estimators . . . . . . . . . . . . . . . . . . . . . . 18

2.3.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

2.4 The Nonlinear A-model: observation equations . . . . . . . . . . . . . . 20

2.4.1 Nonlinear observation equations . . . . . . . . . . . . . . . . . . 20

2.4.2 The linearized observation equations . . . . . . . . . . . . . . . . 23

2.4.3 Least-Squares iteration . . . . . . . . . . . . . . . . . . . . . . . 28

2.5 The B-Model: condition equations . . . . . . . . . . . . . . . . . . . . . 29

2.5.1 Linear condition equations . . . . . . . . . . . . . . . . . . . . . 29

2.5.2 Nonlinear condition equations . . . . . . . . . . . . . . . . . . . 32

2.6 Special Least-Squares procedures . . . . . . . . . . . . . . . . . . . . . 32

2.6.1 Recursive least-squares . . . . . . . . . . . . . . . . . . . . . . . 33

2.6.2 Constrained least-squares . . . . . . . . . . . . . . . . . . . . . . 34

2.6.3 Minimally constrained least-squares . . . . . . . . . . . . . . . . 36

2.7 Quality control: precision . . . . . . . . . . . . . . . . . . . . . . . . . . 40

3 Testing and reliability       45

3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45

3.2 Basic concepts of hypothesis testing . . . . . . . . . . . . . . . . . . . . 46

3.2.1 Statistical hypotheses . . . . . . . . . . . . . . . . . . . . . . . . 46

3.2.2 Test of statistical hypotheses . . . . . . . . . . . . . . . . . . . . 49

3.2.3 Two types of errors . . . . . . . . . . . . . . . . . . . . . . . . . 51

3.2.4 General steps in testing hypotheses . . . . . . . . . . . . . . . . 56

3.3 Test statistics for the linear(ized) model . . . . . . . . . . . . . . . . . . 57

3.3.1 The null and alternative hypothesis . . . . . . . . . . . . . . . . 57

                            vi
Contents                                 vii

       3.3.2 Residuals and model errors . . . . . . . . . . . . . . . . . . . . . 58
       3.3.3 Significance of model error . . . . . . . . . . . . . . . . . . . . . 60
3.4 Detection, identification and adaptation . . . . . . . . . . . . . . . . . . 61
       3.4.1 Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
       3.4.2 Identification . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
       3.4.3 Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
3.5 Reliability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
       3.5.1 Power of tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
       3.5.2 Internal reliability . . . . . . . . . . . . . . . . . . . . . . . . . . 66
       3.5.3 External reliability . . . . . . . . . . . . . . . . . . . . . . . . . 67
       3.5.4 Connection of two height systems: an example . . . . . . . . . . 69
3.6 Quality control: precision and reliability . . . . . . . . . . . . . . . . . . 73

4 Adjustment and validation of networks  77

4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77

4.2 The free network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80

          4.2.1 Some typical observation equations . . . . . . . . . . . . . . . . 80

          4.2.2 On the invariance of geodetic observables . . . . . . . . . . . . . 83

          4.2.3 Adjustment of free GPS network: an example . . . . . . . . . . . 88

          4.2.4 Testing of free levelling network: an example . . . . . . . . . . . 92

4.3 The connected network . . . . . . . . . . . . . . . . . . . . . . . . . . . 94

          4.3.1 The observation equations . . . . . . . . . . . . . . . . . . . . . 94

          4.3.2 The unconstrained connection for testing . . . . . . . . . . . . . 97

4.4 The constrained connection for coordinate computation . . . . . . . . . . 101

          4.4.1 A levelling example . . . . . . . . . . . . . . . . . . . . . . . . 104

4.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108

A Appendix                               111

A.1 Mean and variance of scalar random variables . . . . . . . . . . . . . . . 111

A.2 Mean and variance of vector random variables . . . . . . . . . . . . . . . 115

B References                             123
Chapter 1

An overview

This introductory chapter gives an overview of the material presented in the book. The
book consists of three parts. A first part on estimation theory, a second part on testing
theory and a third part on network theory. The first two parts are of a more general
nature. The material presented therein is in principle applicable to any geodetic project
where measurements are involved. Most of the examples given however, are focussed
on the network application. In the third part, the computation and validation of geodetic
networks is treated. In this part, we make a frequent use of the material presented in the
first two parts. In order to give a bird's eye view of the material presented, we start with a
brief overview of the three parts.

ADJUSTMENT: The need for an adjustment arises when one has to solve an inconsis-
tent system of equations. In geodesy this is most often the case, when one has to solve
a redundant system of observation equations. The adjustment principle used is that of
least-squares. A prerequisite for applying this principle in a proper way, is that a number
of basic assumptions need to be made about the input data, the measurements. Since mea-
surements are always uncertain to a certain degree, they are modeled as sample values of a
random vector, the m-vector of observables y (note: the underscore will be used to denote
random variables). In case the vector of observables is normally distributed, its distri-
bution is uniquely characterized by the first two (central) moments: the expectation (or
mean) E{y} and the dispersion (or variance) D{y}. Information on both the expectation
and dispersion needs to be provided, before any adjustment can be carried out.

Functional model: In case of geodetic networks, the observables contain information on
the relative geometry of the network points. Examples are: height differences, angles,
distances, baselines, etc. Knowing the information content of the observables, allows
one to link them to the parameters which are used for describing the geometry of the
network. These parameters, which are often coordinates, are to be determined from the
adjustment. The link between the observables and the n-vector of unknown parameters x,
is established by means of the system of m observation equations

        E{y} = Ax

This system is referred to as the functional model. It is given once the design matrix A of
order m О n is specified.

    The system as it is given here, is linear in x. Quite often however, the observation
equations are nonlinear. In that case a linearization needs to be carried, to make the
system linear again. The parameter vector x usually consists of coordinates and possi-
bly, additional nuisance parameters, such as for instance orientation unknowns in case of
theodolite measurements. The coordinates could be of any type. For instance, they could

                                                     1
2                                 Network Quality Control

be Cartesian coordinates or geographic coordinates. The choice of the type of coordinates
is not essential for the adjustment, but is more a matter of convenience and depends on
what is required for the particular application at hand.

Stochastic model: Measurements are intrinsically uncertain. Remeasurement of the same
phenomenon under similar circumstances, will usually give slightly different results. This
variability in the outcomes of measurements is modelled through the probability density
function of y. In case of the normal distribution, it is completely captured by its dispersion.
In order to properly weigh the observables in the adjustment process, the dispersion needs
to be specified beforehand. It is given as

        D{y} = Qy

This is the stochastic model, with Qy being the m О m variance matrix of the observ-
ables. In these lecture notes, we will assume that Qy is known. Hence, unknown variance
components and their estimation are not treated.

    Since the variance matrix describes the variability one can expect of the measurements
when they are repeated under similar circumstances, it is said to describe the precision of
the observables. In order to be able to specify Qy correctly, a good understanding of the
measurement equipment and the measurement procedures used, is needed. Quite often
the variance matrix Qy can be taken as a diagonal matrix. This is the case, when the
measurements have been obtained independently from one another. The variance matrix
becomes full (nondiagonal) however, when for instance, the measurements themselves are
the result of a previous adjustment. This is the case when connecting geodetic networks.

Least-squares: Once the measurements have been collected and the functional model
and the stochastic model have been specified, the actual adjustment can be carried out.
The least-squares estimator of the unknown parameter vector x, is given as

   x^  =  (AT  Q-1A)-1  AT  Q-1y

                 y            y

It depends on the design matrix A, the variance matrix Qy and the vector of observables
y. With x^, one can compute the adjusted observables as y^ = Ax^ and the least- squares
residuals as e^ = y - y^.

    The above expression for the least-squares estimator is based on a functional model
which is linear. In the nonlinear case, one will first have to apply a linearization before
the above expression can be applied. For the linearization one will need approximate
values for the unknown parameters. In case already an approximate knowledge on the
geometry of the network is available, the approximate coordinates of the network points
can be obtained from a map. If not, a minimum set of the observations themselves will
have to be used for computing approximate coordinates. In case the approximate values
of the unknown parameters are rather poor, one often will have to iterate the least-squares
solution.

Quality: Every function of a random vector, is itself a random variable as well. Thus
x^ is a random vector, just like the vector of observables y is. And when x^ is linearly
related to y, it will have a normal distribution whenever y has one. In that case also the
1. An overview                3

distribution of x^ can be uniquely characterized by means of its expectation and dispersion.
Its expectation reads

E{x^} = x

Thus the expectation of the least-squares estimator equals the unknown, but sought for
parameter vector x. This property is known as unbiasedness. From an empirical point
of view, the equation implies, that if the adjustment would be repeated, each time with
measurements collected under similar circumstances, then the different outcomes of the
adjustment would on the average coincide with x. It will be clear, that this is a desirable
property indeed.

    The dispersion of x^, describing its precision, is given as

D{x^}  =        (AT  Q-1A)-1

                       y

This variance matrix is independent of y. This is a very useful property, since it implies
that one can compute the precision of the least-squares estimator without having the actual
measurements available. Only the two matrices A and Qy need to be known. Thus once
the functional model and stochastic model have been specified, one is already in a position
to know the precision of the adjustment result. It also implies, that if one is not satisfied
with this precision, one can change it by changing A and/or Qy. This is typically done at
the design stage of a geodetic project, prior to the actual measurement stage. Changing
the geometry of the network and/or adding/deleting observables, will change A. Using
different measurement equipment and/or different measurement procedures, changes Qy.

TESTING: Applying only an adjustment to the observed data is not enough. The result
of an adjustment and its quality rely heavily on the validity of the functional and stochastic
model. Errors in one of the two, or in both, will invalidate the adjustment results. One
therefore needs, in addition to the methods of adjustment theory, also methods that allow
one to check the validity of the assumptions underlying the functional and stochastic
model. These methods are provided for by the theory of statistical testing.

Model errors: One can make various errors when formulating the model needed for
the adjustment. The functional model could have been misspecified, E{y} = Ax. The
stochastic model could have been misspecified, D{y} = Qy. Even the distribution of y
need not be normal. In these lecture notes, we restrict our attention to misspecifications
in the functional model. These are by far the most common modelling errors that occur
in practice. Denoting the model error as b, we have E{y} = Ax + b. If it is suspected
that model errors did indeed occur, one usually, on the basis of experience, has a fair idea
what type of model error could have occurred. This implies that one is able to specify the
vector b in the form of equations like

        b = C

where C is a matrix of order m О q and  is a q-vector. The vector  is still unknown,
but the matrix C is then known. This matrix specifies how the vector of observables is
4                                                                          Network Quality Control

assumed to be related to the unknown error vector . A typical example of modelling
errors that can be captured through this description are blunders in the measurements. In
case of a single blunder in one of the measurements, the C-matrix reduces to a unit vector,
having a one at the entry that corresponds with the corrupted measurement.

Test statistic: It will be intuitively clear that the least-squares residual vector e^, must
play an important role in validating the model. It is zero, when the measurements form a
perfect match with the functional model, and it departs from zero, the more the measure-
ments fail to match the model. A test statistic is a random variable that measures on the
basis of the least-squares residuals, the likelihood that a model error has occurred. For a
model error of the type C, it reads

        T q = e^T Q-y 1C(CT Q-y 1Qe^Q-y 1C)-1CT Q-y 1e^

It depends, apart from the least-squares residuals, also on the matrix C, on the design
matrix A (through Qe^) and on the variance matrix Qy. The test statistic has a central Chi-
squared distribution, with q degrees of freedom, 2(q, 0), when the model error would be
absent. When the value of the test statistic falls in the right tail-area of this distribution,
one is inclined to belief that the model error indeed occurred. Thus the presence of the
model error is believed to be likely, when Tq > 2q(q, 0), where q is the chosen level of
significance.

Testing procedure: In practice it is generally not only one model error one is concerned
about, but quite often many more than one. In order to take care of these various potential
modelling errors, one needs a testing procedure. It consists of three steps: detection,
identification and adaptation. The purpose of the detection step is to infer whether one
has any reason to belief that the model is wrong. In this step one still has no particular
model error in mind. The test statistic for detection, reads

     T m-n = e^T Q-y 1e^

One  decides  to  reject  the  model,  when  Tm-n  >    2  (m  -  n,  0).

                                                      m-n

   When the detection step leads to rejection, the next step is the identification of the

most likely model error. The identification step is performed with test statistics like T q.

It implies that one needs to have an idea about the type of model errors that are likely to

occur in the particular application at hand. Each member of this class of potential model

errors is then specified through a matrix C. In case of one dimensional model errors, such

as blunders, the C-matrix becomes a vector, denoted as c. In that case q = 1 and the test

statistic T q simplifies considerably. One can then make use of its square-root, which reads

     w = cT Q-y 1e^
              cT Q-y 1Qe^Q-y 1c

This test statistic has a standard normal distribution N(0, 1) in the absence of the model
error. The particular model error that corresponds with the vector c, is then said to have
2 occurred with a high likelihood, when | w |> N 1 1 (0, 1). In order to have the model error
1. An overview                        5

detected and identified with the same probability, one will have to relate the two levels
of significance, m-n and 1. This is done by equating the power and the noncentrality
parameters of the above two test statistics T m-n and w.

    Once certain model errors have been identified as sufficiently likely, the last step con-

sists of an adaptation of the data and/or model. This implies either a remeasurement of the

data or the inclusion of additional parameters into the model, such that the model errors
are accounted for. In both cases one always should check again of course, whether the

newly created situation is acceptable or not.

Quality: In case a model error of the type C occurs, the least-squares estimator x^ will
become biased. Thus E{x^} = x. The dispersion or precision of the estimator however,
remains unaffected by this model error. The bias in x^, due to a model error C, is given
as

x^  =  (AT        -1  A)-1AT  Q-y 1C

                Qy

The purpose of testing the model, is to minimize the risk of having a biased least-squares
solution. However, one should realize that the outcomes of the statistical tests are not
exact and thus also prone to errors. It depends on the 'strenght' of the model, how much
confidence one will have in the outcomes of these statistical tests. A measure of this
confidence is provided for by the concept of reliability. When the above w-test statistic is
used, the size of the model error that can be found with a probability , is given by the
Minimal Detectable Bias (MDB). It reads

|  |=              (1, 1, )
                cT Q-y 1Qe^Q-y 1c

where  (1, 1, ) is a known function of the level of significance 1 and the detection
probability (power) . The set of MDB's, one for each model error considered, is said to
describe the internal reliability of the model.

    As it was the case with precision, the internal reliability can be computed once the

design matrix A and the variance matrix Qy are available. Changing A and/or changing
Qy, will change the MDB's. In this way one can thus change (e.g. improve) the internal
reliability. Substitution of C |  | for C in the above expression for x^, will show by
how much the least-squares solution becomes biased, when a model error of the size of
the MDB occurs. The bias vectors x^, one for each model error considered, is then said
to describe the external reliability of the model.

NETWORKS: The theory of adjustment and testing, is in principle applicable to any
geodetic project where measurements are involved for the determination of unknown pa-
rameters. But in case of a project like computing a geodetic network, some additional
considerations need to be taken into account as well. The aim of computing a geodetic
network is to determine the geometry of the configuration of a set of points. The set of
points usually consists of: (1) newly established points, of which the coordinates still
need to be determined, and (2) already existing points, the so-called control points, of
which the coordinates are known. By means of a network adjustment the relative geome-
try of the new points is determined and integrated into the geometry of the existing control
6  Network Quality Control

points. The determination and validation of the overall geometry is usually divided in two
phases: (1) the free network phase, and (2) the connected network phase.

Free network phase: In this phase, the known coordinates of the control points do not
take part in the determination of the geometry. It is thus free from the influence of the
existing control points. The idea is that a good geodetic network should be sufficiently
precise and reliable in itself, without the need of external control. It implies, when in the
second phase, the connected network phase, rejection of the model occurs, that one has
good reason to belief that the cause for rejection should be sought in the set of control
points, instead of in the geometry of the free network.

    As with any geodetic project, the three steps involved in the free network phase are:
design (precision and reliability), adjustment (determination of geometry) and testing
(validation of geometry). With free networks however, there is one additional aspect
that should be considered carefully. It is the fundamental non-uniqueness in the relation
between geodetic observables and coordinates. This implies, that when computing co-
ordinates for the free network, additional information in the form of minimal constraints
are needed, to eliminate the non-uniqueness between observables and coordinates. The
minimal constraints however, are not unique. There is a whole set from which they can
be chosen. This implies that the set of adjusted coordinates of the free network, includ-
ing their variance matrix and external reliability, are not unique as well. This on its turn
implies, that one should only use procedures for evaluating the precision and reliability,
that are guaranteed to be invariant for the choise of minimal constraints. If this precau-
tion is not taken, one will end up using an evaluation procedure of which the outcome is
dependent on the arbitrary choice of minimal constraints.

Connected network phase: The purpose of this second phase is to integrate the geom-
etry of the free network into the geometry of the control points. The observables are the
coordinates of the free network and the coordinates of the control points. Since the co-
ordinates of the two sets are often given in different coordinate systems, the connection
model will often be based on a coordinate transformation from the coordinate system of
the free network to that of the control network.

    In contrast to the free network phase, the design, adjustment and testing are now
somewhat nonstandard. First of all there is not much left to design. Once the free network
phase has been passed, the geometry of the free network as well as that of the control
points are given. This implies that already at the design stage of the free network, one
should take into account the distribution of the free network points with respect to the
distribution of the control points.

    Secondly, the adjustment in the connected network phase is not an ordinary least-
squares adjustment. In most applications, it is not very practical to see the coordinates
of the control points change everytime a free network is connected to them. This would
happen however, when an ordinary adjustment would be carried out. Thus instead, a
constrained adjustment is applied, with the explicit constraints that the coordinates of the
control points remain fixed.

    For testing however, a constrained adjustment would not be realistic. After all, the
1. An overview  7

coordinates of the control points are still samples from random variables and therefore
not exact. Thus for the validation of the connected geometry, the testing is based on the
least-squares residuals that follow from an ordinary adjustment and not from a constrained
adjustment.
Chapter 2

Estimation and precision

2.1 Introduction

This chapter is an introduction to least-squares estimation theory and it is written at a
level which should be sufficient for understanding the chapters following. The terms
'estimation' and 'adjustment' are often used interchangingly. Adjustment is the process
of making an inconsistent system of equations consistent, whereas estimation refers to
the process of determining values for the unknown parameters that occur in the system of
equations.

    Loosely speaking, the need for an adjustment arises when there are more observations
than unknowns in a system of equations. The system of equations is then usually incon-
sistent. That is, without an adjustment, the given observations will not be compatible with
the system of equations, they will not fit these equations. Adjustment is thus the process
of making an inconsistent system of equations, consistent.

    In section 2, we start off by discussing the concepts of consistency and uniqueness in
more detail. Inconsistency of a system of equations can only occur when the number of
equations, m, exceeds the rank of the system's matrix A, rank A. The difference m-rank A
is known as the redundancy of the system. The system is always consistent when the
redundancy equals zero. In that case, there is no need for an adjustment and the system
will always have a solution. The solution however, may not be unique. The solution is
only unique when the system's matrix is of full rank, that is, when rank A equals the total
number of unknowns in the system.

    The system of equations will generally be inconsistent when the redundancy is un-
equal to zero. In section 3 we show how an inconsistent system of equations can be made
consistent. The method employed is that of least- squares. Both the deterministic as well
as stochastic formulation of the least-squares method are discussed. For the stochastic for-
mulation we rely on the concept of random variables. The first two (central) moments of a
random variable, the mean and the variance, together with the very important propagation
laws, are discussed in the appendix.

    In section 4, we generalize the least-squares method and show how it can be used when
the observation equations are nonlinear. In applications, one will find that the observation
equations are more often than not, nonlinear. Using Taylor's theorem, it is shown how
the nonlinear observation equations can be linearized and how, by means of an iteration
process, the standard least-squares approach can be employed.

    In section 5, we introduce the concept of condition equations. The formulation in
terms of condition equations is the dual to the formulation in terms of observation equa-
tions. It is much like the equation of a circle, x2 + y2 = R, is the dual to the two parametric
equations x = R cos  and y = R sin  . It is shown how the least- squares formulae need
to be formulated, when one is dealing with condition equations. Both linear as well as

                                                     9
10                                                              Network Quality Control

nonlinear condition equations are considered.
    In section 6, we discuss three least-squares procedures which are of use for the ma-

terial discussed in the later parts of these lecture notes. First we discuss recursive least-
squares estimation, then constrained least-squares estimation and finally, minimally con-
strained least-squares estimation. Minimally constrained least-squares estimation is needed
when the systems's matrix is less than of full rank. In most geodetic applications this is
more the rule than the exception. When discussing minimally constrained least-squares,
particular attention is given to the dependency of the variance matrix on the chosen set of
minimal constraints.

    The chapter is concluded with a section on quality control. In this section, only one
aspect of quality is discussed, namely that of precision. Precision as expressed by the
variance matrix, can be said to describe the variability of sample values of an unbiased
estimator around its mean. In this last section, we also present some ways of testing the
precision against a given criterium.

2.2 Consistency and uniqueness

Assume that we want to determine n unknown parameters x  R,  = 1, . . ., n, from a
given set of m measurements yi  R, i = 1, . . ., m. If the measurements bear a known
linear relationship with the unknown parameters, we may write

              n                                                             (2.1)

    yi =  ai x , i = 1, . . .m

            =1

In this equation the known scalars ai determine how the measurements are related to the

unknown parameters. By introducing the matrix A and the vectors y and x as

           a11 . . . a1n                                
                                    y1                    x1
           ..  ..                   ..                    ..  
    A=     .   .          ,  y=     .    , x=             .   

           am1 . . . amn            ym                    xn

equation (2.1) can be written in matrix-vector form as

    y = Ax                                                                  (2.2)

    mО1        mОnnО1

This is a system of m linear equations in n unkown parameters. In order to be able to
solve this linear system, it is first of interest to know under what conditions a solution to
the linear system exists and if so, whether it is unique or not.

    It will be clear that a solution to the linear system (2.2) exists if and only if the m-
vector y can be written as a linear combination of the column vectors of matrix A. If this
is the case, the vector y is said to be an element of the column space or range space of
matrix A. The range space of A is denoted as R(A). Thus a solution to (2.2) exists if and
only if

    y  R(A)                                                                 (2.3)
2. Estimation and precision                        11

Systems of equations for which this holds true, are called consistent. A system is said
to be inconsistent if it is not consistent. In this case the vector y can not be written as a
linear combination of the column vectors of matrix A and hence no vector x exists such
that (2.2) holds true.

    Since y  Rm, it follows from (2.3) that consistency is guaranteed if R(A) = Rm, that
is, if the range space of matrix A equals the m-dimensional space of real numbers. But
R(A) = Rm only holds true, if the dimension of R(A) equals the dimension of Rm, which
is m. It follows therefore, since the dimension of R(A) equals the rank of matrix A (note:
the rank of a matrix equals the total number of linear independent columns of this matrix),
that consistency is guaranteed if and only if

rank A = m                                         (2.4)

In all other cases, rank A < m, the linear system may or may not be consistent.
    Let us now assume that the system is indeed consistent. The next question one may ask

is whether the solution to (2.2) is unique or not. That is, whether the information content
of the measurements collected in the vector y is sufficient to determine the parameter
vector x uniquely. The solution is only unique if all the column vectors of matrix A are
linear independent. Hence, the solution is unique if the rank of matrix A equals the number
of unknown parameters,

rank A = n                                         (2.5)

To see this, assume x and x = x to be two different solutions of (2.2). Then Ax = Ax
or A(x - x ) = 0. But this can only be the case if some of the columns of matrix A are
lineary dependent, which contradicts the assumption (2.5) of full rank. Thus the solution
is unique when rank A = n and it is nonunique when rank A < n.

    Unless otherwise stated, we will assume from now on that the matrix A of the linear
system (2.2) is of full rank.

    With rank A = n and the fact that the rank of a matrix is always equal or less than the
number of its rows or its columns, follows that we can discriminate between the following
two cases

m = n = rank A or m > n = rank A                   (2.6)

In the first case, both (2.4) and (2.5) are satisfied. This implies that the system is both
consistent and unique. Thus a solution exists and this solution is also unique. The unique
solution, denoted by x^, is given as

x^ = A-1y                                          (2.7)

where A-1 denotes the inverse of matrix A.

Example 1
Consider the linear system

2 = 1 3 x1                                  (2.8)
1           2 -1 x2

y                            A
12                                             Network Quality Control

    In this case we have: m = 2, n = 2 and rank A = 2. Thus, the system is
    consistent since rank A = m = 2, and the system has a unique solution since
    rank A = n = 2. The unique solution of (2.8) reads: x^ = (5/7 , 3/7)T .

    In the second case, only (2.5) is satisfied. This implies that a unique solution exists,
provided that the system is consistent. If the system is consistent, the unique solution can
be obtained by inverting n out of the m > n linear equations. Hence, we first partition
(2.2) as

    y1 = A1 x
    y2             A2

where y1 is an n-vector, y2 is an m - n-vector and A1 and A2 are of order n О n and
(m - n) О n respectively. The unique solution x^ follows then as

    x^ = A1-1y

Note that y2 is not used in computing x^. This is due to the fact that in the present situation,
y2 is consistent with y1 and hence, it does not contain any additional information.

    Example 2

    Consider the linear system

         -2   1 3 
         3  =  2 -1  x1
          -1 1 2 x2                            (2.9)

                y             A

    In this case we have: m = 3, n = 2 and rank A = 2. Since m = 3 >rank A = 2,
    consistency of the system is not automatically guaranteed. A closer look at
    the measurementvector y of (2.9) shows however that

         -2                            
                            1               3

         3  = 1.  2  - 1.  -1 

          -1                  1            2

    This shows that y can be written as a linear combination of te columnvectors
    of A. Therefore y  R(A), showing that the system is consistent. And since
    n =rank A = 2, its solution is also unique.

    If we partition (2.9) as

         -2   1 3 

                3        2       -1    x1
         и и и  =  и и ии и и  x2    

          -1             12
2. Estimation and precision                                      13

the unique solution follows as

       x^1 = 1 3 -1 -2 = - 1 -1 -3 -1 -2 = 1
       x^2    2 -1                        3   7 -2 1      3  -1

The system (2.9) may also be partitioned as

      -2   1 3 

       иии                   ииииии       x1
      3  =  2 -1  x2                 

       -1                    12

The unique solution follows then as

       x^1 = 2 -1 -1 3 = - 1 2 1 -1 3 = 1
       x^2    12                          -1  5 -1 2  -1     -1

    As we have seen, a full rank system of m equations in n unknowns (rank A = n),
can only be inconsistent when m - n > 0. For a less than full rank system (rank A < n),
this only holds true when m - rank A > 0. This number, which is a very important one in
adjustment theory, is referred to as the redundancy of the system of observation equations.
Thus

redundancy = m - rank A                                      (2.10)

    The question that remains is: What to do when the linear system of equations is incon-
sistent? In that case, we first need to make the system consistent before a solution can be
computed. There are however many ways in which an inconsistent system can be made
consistent. In the next section we will give a way, which is intuitively appealing. And
lateron we will show that this approach of finding a solution to an inconsistent system
also has some optimality properties, in particular in a probabilistic context.

2.3 The Linear A-model: observation equations

2.3.1 Least-squares estimates

An inconsistent system, that is, a system for which y / R(A) holds, can be made consistent
by introducing an errorvector e as (see figure 2.1):

y = Ax + e , m > n = rank A                                  (2.11)

mО1         mОnnО1                   mО1

    In (2.11), y and A are given, whereas x and e are unknown. From the geometry of
figure 2.1 it seems intuitively appealing to estimate x as x^ such that Ax^ is as close as
possible to the given measurement- or observationvector y. In other words, the idea is to
14                                                                     Network Quality Control

                                                                 y           Rm

                                                              e        ╩(A)
                                     O

                                                          Ax

Figure 2.1 The geometry of y = Ax + e.

find that value of x that minimizes the length of the vector e = y - Ax. This idea leads to
the following minimization problem:

    min (y - Ax)T (y - Ax)                                                       (2.12)

      x

    From calculus we know that x^ is a solution of (2.12) if x^ satisfies:

         F (x^) = 0 and   2F (x^) positive - definite                            (2.13)
         x                x2
where F(x) is given as

    F(x) = (y - Ax)T (y - Ax) = yT y - 2yT Ax + xT AT Ax                         (2.14)

Taking the first-order and second-order partial derivatives of F(x) gives

    F              T        T       2F                              T
       (x) = -2A y + 2A Ax and 2 (x) = 2A A                                      (2.15)
    x                              x

Equating the first equation of (2.15) to zero shows that x^ satisfies the normal equations

    AT Ax^ = AT y                                                                (2.16)

Since rank AT A =rank A = n, the system is consistent and has a unique solution. Through
an inversion of the normalmatrix AT A the unique solution of (2.16) is found as

    x^ = (AT A)-1AT y                                                            (2.17)

That this solution x^ is indeed the minimizer of (2.14) follows from the fact that the matrix
 2F/ x2 of (2.15) is indeed positive-definite. The vector x^ is known as the least-squares
estimate of x, since it produces the least possible value of the sum-of-squares function
F (x).

    From the normal equations (2.16) follows that AT (y - Ax^) = 0. This shows that the
vector e^ = y-Ax^, which is the least-squares estimate of e, is orthogonal to the range space

of matrix A (see figure 2.2):

    AT e^ = 0 , with e^ = y - Ax^                                                (2.18)
2. Estimation and precision                                                   15

                                           y                  Rm

                                              e^

                                     O     y^ = Ax^
Figure 2.2 The geometry of Least-Squares.
                                                        ╩(A)

Example 3
Let us consider the next problem

y1 = 1 x + e1
y2   1                       e2

With A = (1, 1)T the normal equation AT Ax^ = AT y reads

        2x^ = y1 + y2

Hence the least-squares estimate x^ = (AT A)-1AT y is

        x^ = 12 (y1 + y2)

Thus, to estimate x, one adds the measurements and divides by the number
of measurements. Hence, the least-squares estimate equals in this case the
arithmetic average.
The least-squares estimates of the obserations and observation errors follow
from y^ = Ax^ and e^ = y - y^ as

y^1 = 1 y1 + y2                  , and        e^1 = 1   y1 - y2
y^2  2 y1 + y2                                e^2    2  y2 - y1

Finally the square of the length of e^ follows as

e^T e^ = 12 (y1 - y2)2

    So far we have discussed the unweighted least-squares principle. The least-squares
principle can be generalized however by introducing a positive-definite m О m weight
matrix W . This is done by replacing (2.12) by the following minimization problem

min(y - Ax)T W (y - Ax)                                                       (2.19)

  x
16                                                    Network Quality Control

Table 2.1 Weighted Least-Squares.

        Consistent Linear System
        y = Ax + e, y, e  Rm, x  Rn, m > n = rank A

        Weighted Least-Squares Principle
        min(y - Ax)T W (y - Ax), W = positive definite

          x

        Weighted Least-Squares Estimates

        parametervector : x^ = (AT WA)-1ATWy

        observationvector : y^ = Ax^

        errorvector                    : e^ = y - y^

    The solution of (2.19) can be derived along lines which are similar as the ones used
for solving (2.12). The solution of (2.19) reads

    x^ = (AT WA)-1AT Wy                                        (2.20)

This is the weighted least-squares estimate of x. In case of weighted least-squares the
normal equations read: AT WAx^ = AT Wy. This shows that the vector e^ = y - Ax^, which is

the weighted least-squares estimate of e, satisfies

    ATW e^ = 0 , with e^ = y - Ax^                             (2.21)

If the inner product of the observation space Rm is defined as (a, b) = aTW b, a, b  Rm,
(2.21) can also be written as (Ax, e^) = 0, x  Rm. This shows that also in the case of
weighted least-squares, the vector e^ can be considered to be orthogonal to the rangspace
of A.

    A summary of the least-squares algorithm is given in Table 2.1.

    Example 4
    Consider again the problem

    y1 = 1 x + e1
    y2  1                          e2

    As weight matrix we take

    W=  w11 0
         0 w22

    With A = (1, 1)T the normal equation ATWAx^ = AT Wy reads

    (w11 + w22)x^ = w11y1 + w22y2
2. Estimation and precision                                                   17

Hence the weighted least-squares estimate x^ = (AT WA)-1ATWy is

        x^ = w11y1 + w22y2
                w11 + w22

Thus instead of the arithmetic average of y1 and y2, as we had with W = I,
the above estimate is a weighted average of the data. This average is closer
to y1 than to y2 if w11 > w22.
The weighted least-squares estimate of the observations, y^ = Ax^, is

y^1 = 1                      w11y1 + w22y2

y^2  w11 + w22 w11y1 + w22y2

and the weighted least-squares estimate of e^, e^ = y - y^ is

e^1 = 1                      w22(y1 - y2)

e^2  w11 + w22 w11(y2 - y1)

Note that |e^2| > |e^1| if w11 > w22.
Finally the weighted sum of squares e^TW e^ is

e^T W e^ = w11w22 w (y 11 + w22 1 - y2)2

2.3.2 A stochastic model for the observations

In the previous section the principle of least-squares was introduced. The least-squares
principle enables us in case of inconsistent systems, to obtain an intuitively appealing
estimate x^ of the parametervector x. But although the least-squares estimate x^ is intuitively
appealing, no quality measures as yet can be attached to the estimate. That is, we know
how to compute the estimate x^, but we are not able yet to say how good the estimate really
is. Of course, the numerical value of the least value of the sum of squares, e^TW e^, does
indicate something about the quality of x^. If e^TW e^ is small one is inclined to have more
confidence in the estimate x^, than when e^TW e^ is large. But how small is small? Besides,
e^TW e^ is identically zero if the linear system is consistent. Would this then automatically
imply that the estimate x^ has good quality? Not really, since the observations may still be
subject to measurement errors.

    In order to obtain quality measures for the results of least-squares estimation, we start
by introducing a qualitative description of the input, that is of the observations. This de-
scription will be of a probabilistic nature. The introduction of a probabilistic description
is motivated by the experimental fact that the variability in the outcome of measurements,
when repeated under similar circumstances, can be described to a sufficient degree by
stochastic or random variables. We will therefore assume that the observation vector y,
which contains the numerical values of the measurements, constitutes a sample of the
18                                   Network Quality Control

random vector of observables y (Note: the underscore indicates that we are dealing with
a random variable). It is furthermore assumed that the vector of observables y can be
written as the sum of a deterministic functional part Ax and a random residual part e:

    y = Ax + e                                                                        (2.22)

    Although a random vector is completely described by its probability density function,
we will restrict ourselves to the first two moments of random variables. That is, we will
restrict ourselves to the mean and to the variance matrix. Properties of the mean and of
the variance matrix together with the error propagation laws, are given in the appendix.

    If we assume that e models the probabilistic nature of the variability in the measure-
ments, it seems acceptable to assume that this variability is zero "on the average" and
therefore that the mean of e^ is zero:

    E{e} = 0                                                                          (2.23)

where E{.} stands for the mathematical expectation operator. The measurement variabil-
ity itself is modelled through the dispersion- or variance matrix of e. We will assume that
this matrix is known and denote it by Qy:

    D{e} = Qy                                                                         (2.24)

where D{.} stands for the dispersion operator. It is defined in terms of E{.} as

    D{.} = E{(. - E{.})(. - E{.})T}

With (2.23) and (2.24) we are now in the position to determine the mean and variance
matrix of the vector of observables y. Application of the law of propagation of means and
the law of propagation of variances to (2.22) gives with (2.23) and (2.24):

    E{y} = Ax ; D{y} = Qy                                                             (2.25)

This will be our model for the vector of observables y. As the results of the next sec-
tion show, model (2.25) enables us to describe the quality of the results of least-squares
estimation in terms of the mean and the variance matrix.

2.3.3 Least-squares estimators

Functions of random variables are again random variables. It follows therefore, that if the

vector of observables is assumed to be a random vector y and when it is substituted for y

in the formulae of table 1 in section 2.3.1, the results are again random variables:

         =      (AT WA)-1AT W y
     x^

     y^ = Ax^                                                                         (2.26)
        e^ = y - y^

These random vectors will be called least-squares estimators. And if y is replaced by its
sample or measurement value y, we speak of least-squares estimates. The quality of the
above estimators can now be deduced from the first two moments of y.
2. Estimation and precision                                       19

The first moment; the mean: Together with E{y} = Ax, an application of the propaga-

tion law of means to (2.26) gives                                 (2.27)
        
         E{x^} = x
         E{y^} = E{y}
            E{e^} = E{e} = 0

These results show, that under the assumption that (2.25) holds, the least-squares estima-
tors are unbiased estimators. Note that this property of unbiasedness is independent of
the choice for the weightmatrix W .

The second moment; the variance matrix: Together with D{y} = Qy, an application of

the propagation law of variances and covariances to (2.26) gives

         =  (AT WA)-1AT W QyWA(AT WA)-1
 Qx^
Qy^ = AQx^AT
                                                                  (2.28)
    Qe^
         =  [I - A(AT WA)-1AT W ]Qy[I - A(AT WA)-1AT W ]T

and                                                               (2.29)
        
         Qx^y^ = Qx^AT
         Qx^e^ = (AT WA)-1AT W Qy - Qx^AT
            Qy^e^ = AQx^e^

    The above variance matrices enable us now to give a complete precision description

of any arbitrary linear function of the estimators. Consider for instance the linear function
^ = aT x^. Application of the propagation law of variances gives then for the precision of
^ :  2^ = aT Qx^a.

    The above results enable us to describe the quality of the results of least-squares es-
timation in terms of the mean and the variance matrix. The introduction of a stochastic
model for the vector of observables y enables us however also to judge the merits of the
least-squares principle itself. Recall that the least-squares principle was introduced on the
basis of intuition and not on the basis of probabilistic reasoning. With the mathematical
model (2.24) one could now however try to develop an estimation procedure that pro-
duces estimators with certain well defined probabilistic optimality properties. One such
procedure is based on the principle of "Best Linear Unbiased Estimation (BLUE)".

    Assume that we are interested in estimating a parameter  which is a linear function
of x:

 = aT x                                                           (2.30)

1О1         1ОnnО1

The estimator of  will be denoted as ^ . Then according to the BLUE's criteria, the

estimator ^ of  has to be a linear function of y,

^ = lT y                                                          (2.31)

1О1         1ОmmО1

such that it is unbiased,

E{^ } =                                                           (2.32)
20                                                            Network Quality Control

and such that it is best in the sense of minimum variance,

          2      minimum                                                                       (2.33)

        ^

The objective is thus to find a vector l  Rm such that with (2.31), the conditions (2.32)

and (2.33) are satisfied. It can be shown that the solution to the above problem is given by

        lT = aT (AT Q-1A)-1AT Q-1y       y

If we substitute this into (2.31) we get

        ^ = aT (AT Q-1 y A)-1AT Q-1 y y                                                        (2.34)

This is the best linear unbiased estimator of  . The important result (2.34) shows that the
best linear unbiased estimator of x is given by

        x^ = (AT Qy-1A)-1AT Qy-1y                                                              (2.35)

    A comparison between (2.26) and (2.35) shows that the BLUE of x is identical to the
weighted least-squares estimator of x if the weight matrix W is taken to be equal to the
inverse of the variance matrix of y:

        W = Qy-1                                                                               (2.36)

This is an important result, because it shows that the weighted least-squares estimators are

best in the probabilistic sence of having minimal variance if (2.36) holds. The variances

and covariances of these estimators follow if the weightmatrix W is replaced in (2.28) and

(2.29)  by  Q-1  .

              y

    From now on we will always assume, unless stated otherwise that the weightmatrix

W   is  chosen  to  be  equal  to  Q-1.     Consequently  no  distinction will  be  made  any  more  in

                                     y

these lecture notes between weighted least-squares estimators and best linear unbiased

estimators. Instead we will simply speak of least-squares estimators.

2.3.4 Summary

In Table 2.2 an overview is given of the main results of Least-Squares Estimation.
    The table shows the linear model of observation equations, the linear A-model. Based

on A, Qy and y, the least-squares estimator of the unknown parameter vector x can be
determined. And from it, one can determine the adjusted vector of observables, y^, the
least-squares residual vector, e^, and the least-squares estimator of an arbitrary function
 = aT x, as ^ = aT x^. As shown in the table, each of these random vectors have their own
mean and variance matrix. Some of the random vectors are correlated, such as x^ and y^,
and some of them are not, such as x^ and e^.

2.4 The Nonlinear A-model: observation equations

2.4.1 Nonlinear observation equations
Up to this point the development of the theory was based on the assumption that the
m-vector E{y} is linearly related to the n-vector of unknown parameters x. In geodetic
2. Estimation and precision                                                    21

Table 2.2 Least-Squares Estimation.

THE LINEAR A-MODEL

E{y} = Ax , D{y} = Qy , m  n = rank A

mО1         mОnnО1                          mОm      mОm

LEAST-SQUARES ESTIMATORS

x^  =  (AT  Q  -  1          A)-  1  AT  Q  -  1  y  e^ = y - y^
               y                            y        ^ = aT x^

y^ = Ax^

MEAN                                           E{e^} = E{e} = 0
 E{x^} = x                                     E{^ } =  = aT x
 E{y^} = E{y} = Ax

VARIANCES AND COVARIANCES

Qx^ = (AT Q-y 1A)-1                                  Qe^ = Qy - Qy^

Qy^ = AQx^AT                                                     2^ = aT Qx^a
Qx^y^ = Qx^AT ,                             Qx^e^ = 0 , Qy^e^ = 0

applications there are however only a few cases where this assumption truly holds. A
typical example is levelling. In the majority of applications however the m-vector E{y} is
nonlinearly related to the n-vector of unknown parameters x. This implies that instead of
the linear A-model (2.25), we are generally dealing with a nonlinear model of observation
equations:

E{y} = A(x) ; D{y} = Qy                                                        (2.37)

where A(.) is a nonlinear vectorfunction from Rn into Rm. The following two simple
examples should make this clear.

Example 5
Consider the configuration of figure 2.3. The x,y coordinates of the three
22                                                                  Network Quality Control

         y                          3 a34
                                                                                                                  j

                                       yi j = y j - yi
                                                              ai j

                                    4

              a14                                         i         xi j = x j - xi
            1
                                       a24 2

                                 x
Figure 2.3 Azimuth resection.

points 1, 2 and 3 are known and the coordinates x4 and y4 of point 4 are
unknown. The observables consist of the three azimuth variates a14, a24, and
a34. Since azimuth and coordinates are related as

        tan ai j = x j - xi = xi j
                    y j - yi yi j

The model of observation equations for the configuration of figure 2.3 reads

                                       arctan[x14/y14]  
                a14
            E{ a24 } =  arctan[x24/y24] 

                a34                    arctan[x34/y34]

This model consists of three nonlinear observation equations in the two un-
known parameters x4 and y4.
Example 6

Consider the situation of figure 2.4. It shows two cartesian coordinate sys-
tems: the x,y-system and the u,v-system. The two systems only differ in their
orientation. This means that if the coordinates of a point i are given in the
u,v-system, (ui, vi), a rotation through an angle  is needed to obtain the
coordinates of the same point i in the x,y-system, (xi, yi):

            xi = cos  - sin  ui                                                      (2.38)
            yi       sin  cos  vi

Let us now assume that we have at our disposal the coordinate observables
of two points in both coordinate systems: (xi, yi) and (ui, vi), i = 1, 2. Using
(2.38), our model reads then

            E{ xi } = cos  - sin  E{ ui } , i = 1, 2                                 (2.39)
                yi                  sin  cos                 vi
2. Estimation and precision                                                               23

                               y

v

                                                 yi                 i

                                          vi                         u
                                                                           ui

Figure 2.4 A coordinate transformation.                             xi         x

This model is however still not in the form of observation equations. If we
consider the orientation angle  and the coordinates of the two points in
the u,v-system as the unknown parameters, (2.39) can be written in terms of
observation equations as

                               u1 cos  - v1 sin                   
     x1
   y                                                              
    1                          u1 sin  + v1 cos                   
    x2   u2 cos  - v2 sin  
    y2   u2 sin  + v2 cos  
E{ } =                                                                            (2.40)
    u1   u1                                                       
                                                                  
     v1                        v1                                 
                                                                  
    u2   u2                                                       

     v2                        v2

This model consists of eight observations in five unknown parameters. Note
that the first four observation equations are nonlinear.

2.4.2 The linearized observation equations

We know how to compute least-squares estimators in case of a linear A-model. But how
are we now going to compute least-squares estimators if the model of observation equa-
tions is nonlinear? For the majority of nonlinear problems the solution is to approximate
the originally nonlinear A-model with a linear one. In order to show how this can be done,
we first recall the celebrated theorem of Taylor. This theorem reads:

Taylor's Theorem: Let f (x) be a function from Rn into R which is smooth enough. Let
x0  Rn be an approximation to x  Rn and define x = x - x0, and  = x0 +t(x - x0) with
24                                                                               Network Quality Control

t  R. Then a scalar t  (0, 1) exists such that

    f (x) = f (x0) +   f (x0)x +    n 1 n n 2 f (x0)x x + . . .
                            =1                          2 =1  =1
                                                                                                        (2.41)
    ...+ 1                          n    n                q-1

                                 ...                     ...     f (x0)x1 . . .xq-1 + Rq( , x)
                    (q - 1)!  =1  =1 1 q-1
                                1        q-1

with the remainder

    Rq( , x) = 1  . . .   n n q ...q f ( )x1 . . .xq                                                    (2.42)
                    q! 1=1                        1

                                    q =1

    In (2.41) and (2.42), q1...q f (x) denotes the qth-order partial derivative of f (x) eval-
uated at x. For the case q = 2, it follows from (2.41) and (2.42) that

    f (x) = f (x0) +   f (x0)x +    n 1 n n 2 f ( )x x                                                  (2.43)
                                                        2 =1  =1
                        =1

If we introduce the gradientvector and Hessianmatrix of f (x) respectively as

                                                               2                        2             
                    1 f (x)                                       11 f (x) и и и 1n f (x)
                       ..                                               ..                   ..       
    x f (x) =          .      and xx f (x) =         2                  .                   .         

                    n f (x)                                       2     f   (x)  иии    2    f   (x)
                                                                                        nn
                                                                    n1

then equation (2.43) may be written in the more compact matrix-vector form as

                                    T    1 T2
    f (x) = f (x0) + x f (x0) x + x xx f ( )x                                                           (2.44)
                                         2

    This important result shows that a nonlinear function f (x) can be written as a sum of

three terms. The first term in this sum is the zero-order term f (x0). The zero-order term
depends on x0 but is independent of x. The second term in the sum is the first-order term
x f (x0)T x. It depends on x0 and is linearly dependent on x. Finally, the third term in the
sum is the second-order remainder R2( , x).

    A consequence of Taylor's Theorem is that the remainder R2( , x) can be made
arbitrarily small by choosing the approximation x0 close enough to x. Now assume that the
x0 approximation is chosen such that the second-order remainder can indeed be neglected.
Then, instead of (2.44) we may write to a sufficient approximation:

    f (x) = f (x0) + x f (x0)T x                                                                        (2.45)

Hence, if x0 is sufficiently close to x, the nonlinear function f (x) can be approximated to
a sufficient degree by the function f (x0) + x f (x0)T x which is linear in x. This function
is the linearized version of f (x). A geometric interpretation of this linearization is given

in figure 2.5 for the case n = 1. Let us now apply the above linearization to our nonlinear

observation equations

                           a1(x)       

                                ..                                                                      (2.46)
    E{y} = A(x) =               .      

                           am(x)
2. Estimation and precision                                                                                  25

                             y = f (x)

                y                               y  =  f (x0) +  df  (x0)(x - x0)
              f (x0)                                            dx

                                          x0       x

Figure 2.5  The nonlinear curve y =   f (x) and its linear tangent y =   f  (x0)  +  d   f  (x0)(x  -  x0).
                                                                                     dx

Each nonlinear observation equation ai(x), can now be linearized according to (2.45).

This gives

            a1(x)            a1(x0)                    xa1(x0)T 

            ..                        ..                            ..   
              .    =                 .        +                       .   x
                                                                                                       (2.47)
            am(x)            am(x0)                     x am (x0 )T

            mО1              mО1                                mОn               nО1

If we denote the m О n matrix of (2.47) as xA(x0), and substitute (2.47) into (2.46) we
get

E{y} = A(x0) + xA(x0)x                                                                                 (2.48)

If we bring the constant m-vector A(x0) to the lefthandside of the equation and define
y = y - A(x0), we finally obtain our linearized model of observation equations

E{y} = xA(x0)x ; D{y} = Qy                                                                             (2.49)

This is the linearized A-model. Compare (2.49) with (2.37) and (2.25). Note when com-
paring (2.49) with (2.25) that in the linearized A-model y takes the place of y, xA(x0)
takes the place of A and x takes the place of x. Since the linearized A-model is linear, our

standard formulae of least-squares can be applied again. This gives for the least-squares
estimator x^ = x0 + x^ of x:

x^ = x0 + [xA(x0)T Q-y 1xA(x0)]-1xA(x0)T Q-y 1y                                                        (2.50)

And application of the propagation law of variances to (2.50) gives                                    (2.51)
        Qx^ = [xA(x0)T Q-y 1xA(x0)]-1

It will be clear that the above results, (2.50) and (2.51), are approximate in the sense that
the second-order remainder is neglected. But these approximations are good enough if the
second-order remainder can be neglected to a sufficient degree. In this case also the op-
timality conditions of least-squares (unbiasedness, minimal variance) hold to a sufficient
degree. A summary of the linearized least-squares estimators is given in Table 2.3.
26                                                                                        Network Quality Control
Table 2.3 Linearized Least-Squares estimation.

THE NONLINEAR A-MODEL
E{y} = A(x) ; D{y} = Qy ; A(.) : Rn  Rm

THE LINEARIZED A-MODEL

E{y} = xA(x0)x ; D{y} = Qy , m  n = rank xA(x0)

    mО1               mОnnО1                   mОm                  mОm

LEAST-SQUARES ESTIMATORS

x^  =    x0     +  [    x  A(x0  )T  Q-1    x  A(x0  )]  -1    x  A(x0  )T  Q-1  [  y  -  A(x0  )]

                                       y                                      y

y^ = A(x^)

e^ = y - y^

VARIANCES

 Qx^ = [xA(x0)T Q-y 1xA(x0)]-1
 Qy^ = xA(x0)Qx^xA(x0)T
 Qe^ = Qy - Qy^

Example 7

Consider the configuration of figure 2.6. The x,y coordinates of the three
points 1, 2 and 3 are known and the two coordinates x4 and y4 of point 4 are
unknown. The observables consist of the three distance variates l14, l24, l34.
Since distance and coordinates are related as

         li  j  =  (x2i j  +   2 1/2

                              yi j)

the model of observation equations for the configuration of figure 2.6 reads

                   2 2 1/2 
                   l 14               (x14 + y14)
         E{                } =        (x2 + y2 )1/2               
                   l 24                                                                             (2.52)
                                         24 24
                                      (x2 + y2 )1/2
                   l  34                  34 34
2. Estimation and precision                                                                                           27

   This model consists of three nonlinear observation equations in the two un-

   known parameters x4 and y4.

   In order to linearize (2.52) we need approximate values for the unknown

   coordinates  x4  and      y4.  These  approximate           values     will  be  denoted     as   x0  and

                                                                                                      4

                             3

                                                                                                                   j
                             l34

                                                 yi j = y j - yi
                                                                                         li j

                 l14 4 l24

y                                                                   i               xi j = x j - xi

          1                                             2

                              x
Figure 2.6 Distance resection.

   y04. With these approximate values a linearization of (2.52) gives

                         0                              0        0           0
                l14                      (x4 - x1)/l14 (y4 - y1)/l14
          E{ l24 } =  (x04 - x2)/l204 (y04 - y2)/l204  x4
                                                                                            y4       (2.53)

                l                        (x0 - x )/l0 (y0 - y )/l0
                    34                   4 3 34                  4 3 34

                  y                                     x A(x0)                             x

   where

             li4    =   li4  -   l0   ,  l0   =  [(x04  -  xi)2  +  (y04  -  yi )2 ]1/2  ,  i = 1, 2, 3

                                  i4      i4
                                 x0                        y0
             x4     =  x4  -          ,  y4   =  y4     -
                                  4                         4

   Model (2.53) is the linearized version of the nonlinear A-model (2.52).
28                                                                                           Network Quality Control

    Example 8

    Consider the nonlinear A-model (2.40) of example 6. The unknown parame-
    ters are  and ui and vi for i = 1, 2. The approximate values of these param-
    eters will be denoted as 0, u0i , v0i , for i = 1, 2. Linearization of (2.40) gives
    then

      x1       u0 sin 0 - v0 cos0                    cos  0         - sin 0             0    0       
                                                     sin 0           cos  0
                  1         0       1                                                   0    0  0    
      y1        u0             -   v0          0        0               0            cos  0
                     cos                sin             0               0            sin 0
                  1                 1                   1               0
                -u02         0      v0            0     0               1               0
      x2              sin       -         cos           0               0               0    - sin        
                                     2                  0               0                              u1 
     E{ y  2   } =  u02 cos0 - v02 sin 0                                                1    cos  0  
                                                                                                       v1 (2.54)
     u1                         0                                                       0    0  0   u2 
          
     v1                         0
                                                                                             0  v2
     u2                         0

      v2                        0                                                            1         x

      y                                                     xA(x0 )

    where:    xi  =   xi   -    x0   ,       x0   =  u0   cos 0         -  v0   sin 0
            
                                 i            i        i                    i
                                y0           y0      u0        0          v0         0
              yi  =   yi   -         ,            =       sin          +        cos
                                 i            i        i                   i
            
              ui  =   ui   -    u0     ,     ui      =  ui  -  u0    ,  for  i  =  1,  2

                                  i                              i
                                v0                             v0
              vi  =   vi   -         ,       vi      =  vi  -       ,  for   i  =  1,  2
                                 i                              i
               =  - 0

2.4.3 Least-Squares iteration

Up to this point it was assumed that the second-order remainder was sufficiently small
and that x0 was a good enough approximation. If this is not the case, then x^ as computed
by (2.50) is not the least-squares estimate and hence an unacceptable error is made. In
order to repair this situation, we need to improve upon the approximation x0. It seems
reasonable to expect that the estimate

        x1 = x0 + [xA(x0)T Q-y 1xA(x0)]-1xA(x0)T Q-y 1(y - A(x0))

is a better approximation than x0. That is, it seems reasonable to expect that x1 is closer
to the true least-squares estimate than x0. In fact one can show that this is indeed the
case for most practical applications. But if x1 is a better approximation than x0, a further
improvement can be expected if we replace x0 by x1 in the linearization of the nonlinear
model. The recomputed linearized least-squares estimate reads then

        x2 = x1 + [xA(x1)T Q-y 1xA(x1)]-1xA(x1)T Q-y 1(y - A(x1))

By repeating this process a number of times, one can expect that finally the solution
converges to the actual least-squares estimate x^. This is called the least-squares iteration
process. The iteration is usually terminated if the difference between successive solutions
is negligible. A flowdiagram of the least-squares iteration process is shown in Figure 2.7.
2. Estimation and precision                                                    29

                                     initialization
                                     x0, set i = 0

i := i + 1                   r(xi) = x A(xi)T W (y - A(xi))  right-hand side
                             N(xi ) = x A(xi)T W xA(xi)      normal matrix
                             N(xi ) x^i = r(xi)              normal equations
                             xi+1 = xi + x^i                 update

                             No      ix^i 2N(x )  

                                          Yes

                                     x^ := xi+1

Figure 2.7 Least-squares iteration.

2.5 The B-Model: condition equations

2.5.1 Linear condition equations

In the previous sections we considered the model of observation equations. In this section
and the next we briefly review the model of condition equations. As our starting point we
take the linear A-model:

y = Ax + e , m  n = rank A                                                     (2.55)

mО1                mОnnО1            mО1

This linear model is uniquely solvable if m = n, i.e. if the number of observables equals

the number of unknown parameters. In this case A is a square matrix which is invertible

because of rank A = n. If m = n no conditions can be imposed on the observables. If m >

n =rank A, then more observables are available then strictly needed for the determination

of the n unknown parameters. In this case an (m - n)-number of redundant observables

exist. Each seperate redundant observable gives rise to the possibility of formulating a

condition equation. Thus the total number of independent condition equations that can be

formulated equals

r = m-n                                                                        (2.56)

This number is also referred to as the redundancy of the model. We will now show how

one can construct the condition equations, given the linear A-model (2.55). Each of the
column vectors of matrix A is an element of the observationspace Rm. Together, the n-

number of linearly independent column vectors of A span the range space of A. This range
space has dimension n and it is a linear subspace of Rm : R(A)  Rm. Since dimR(A) = n
30                                                 Network Quality Control

and dimRm = m, exactly (m - n)-number of linearly independent vectors can be found
that are orthogonal to R(A). Let us denote these vectors as: bi  Rm, i = 1, . . ., (m - n).
Then

        bi  R(A) or AT bi = 0 , i = 1, . . ., (m - n)

From this follows, if the (m - n)-number of linearly independent vectors bi are collected
in an m О (m - n) matrix B as

      B            = (b1, b2, . . ., bm-n)

      m О (m - n)

that

      BT A               =       0         ; rank B = m - n                      (2.57)

      (m - n) О m m О n       (m - n) О n

This result may now be used to obtain the model of condition equations from (2.55).
Premultiplication of the linear system of observation equations in (2.55) by BT gives

together with (2.57), the following linear model of condition equations:

      BT E{y} = 0 ; D{y} = Qy ; rank B = r = m - n                               (2.58)

      n О mm О 1         nО1     mОm          mОm

      Example 9

      Consider the following linear A-model:

                                                                (2.59)
      E{ y2 y1 } =  1  x ; D{y} = Qy 1

                   y3         1

      Since m = 3, n = 1 and rank A = 1 = n, the redundancy equals m - n = 2.
      Hence two linearly independent condition equations can be formulated. The
      two vectors

      b1 = (1, -1, 0)T and b2 = (0, 1, -1)T

      are linearly independent and are both orthogonal to the column vector of

      matrix A in (2.59). Hence the with (2.59) corresponding linear model of
      condition equations reads

                                 

         1 -1            0  y1  0                  ; D{y} = Qy  (2.60)
                             E{ y2 } =
         0 1 -1                               0
                                    y3
                   BT
                                    y
2. Estimation and precision                                     31

Table 2.4 Least-Squares estimation.

THE LINEAR B-MODEL

BT E{y} = 0 ; D{y} = Qy ; rank B = r = m - n

n О mm О 1    nО1                    mОm     mОm

LEAST-SQUARES ESTIMATORS
y^ = [I - QyB(BT QyB)-1BT ]y ; e^ = y - y^

VARIANCES AND COVARIANCES
Qy^ = QyB(BT QyB)-1BT Qy ; Qe^ = Qy - Qy^ ; Qy^e^ = 0

    Now that we have the disposal of the linear B-model (2.58), how are we going to
compute the corresponding least-squares estimators? We know how to compute the least-
squares estimators for the linear A-model. The corresponding formulae are however all
expressed in terms of the A-matrix. What is needed is therefore to transform these for-
mulae such that they are expressed in terms of the B-matrix. This is possible with the
following important matrix identity:

A(AT Q-1 y A)-1AT Qy-1 = I - QyB(BT QyB)-1BT                    (2.61)

The proof of this matrix identity goes as follows. We define two matrices C and C» as:

          ..                          (AT Q-1A)-1AT Q-1 y  y
C = A . QyB
              and C» =                    и и ии и ии и и       (2.62)

                                     (BT QyB)-1BT

Since both matrices C and C» are of the order m О m and since both can be shown to be of
full rank, it follows that both matrices are invertible. From (2.62) follows with the help of
(2.57) that C»C = Im. Hence C» = C-1 and therefore CC» = Im. Substitution of (2.62) into
this last expression proofs (2.61).

    With (2.61) and the least-squares results of table 2 of section 2.3.4 we are now in the
position to derive the expressions for the least-squares estimators in terms of the matrix
B. The results are summarized in table 2.4.
32                                       Network Quality Control

    For some applications it may happen, when formulating the condition equations, that
the linear combinations of the observables do not sum up to a zero vector, but instead to a
known nonzero vector b. In that case the model of condition equations reads

        BT E{y} = b ; D{y} = Qy

Due to the fact that b = 0, the solution for y^ now takes a somewhat different form. It reads
        y^ = y - QyB(BT QyB)-1(BT y - b)

Note that this solution reduces to the one given earlier, when b = 0.

2.5.2 Nonlinear condition equations

Just like in case of the A-model, there are very few geodetic applications for which the
model of condition equations is linear. In most cases the model of condition equations is
nonlinear. The nonlinear B-model reads:

    BT (E{y}) = 0 ; D{y} = Qy            (2.63)

where BT (.) is a nonlinear vectorfunction from Rm into Rm-n. The relation between the
nonlinear B-model and the nonlinear A-model is given by

    BT (A(x)) = 0 ,  x  Rn               (2.64)

This is the nonlinear generalization of (2.57). If we take the partial derivative with respect
to x of (2.64) and apply the chainrule, we get

    [yB(y0)]T [xA(x0)] = 0 ; y0 = A(x0)  (2.65)

This is the linearized version of (2.64). Compare (2.65) with (2.57). With (2.65) we

are now in the position to construct the linearized B-model from the linearized A-model
(2.49). Premultiplication of (2.49) with the matrix [yB(y0)]T gives together with (2.65)
the result

    [yB(y0)]T E{y} = 0 ; D{y} = Qy       (2.66)

This is the linearized B-model. With (2.66) we are now in the position again to apply our
standard least-squares estimation formulae.

2.6 Special Least-Squares procedures

In this section three special cases of least-squares estimation will be discussed. They
are: recursive least-squares, constrained least-squares and minimally constrained least-
squares. In particular the last two will be needed when we discuss the adjustment and
testing of geodetic networks. Constrained least-squares, which can be seen as a particular
form of recursive least-squares, deals with the problem of solving a model of observation
equations, when there are explicit constraints on the unknown parameters. As it is shown,
the solution can be obtained in two steps. In the first step the model is solved without the
2. Estimation and precision                                                        33

constraints, while in the second step, the final solution is obtained by using the results of
the first step in a formulation based on condition equations.

    Minimally constrained least-squares is needed when the design matrix fails to be of
full rank. This typically occurs in cases of free network adjustments. Due to the rank de-
fect of the design matrix, a set of necessary and sufficient constraints need to be imposed
on the parameter vector in order to be able to compute a solution. The set of constraints
that can be imposed however, is not unique. This implies that a whole family of solutions
can be computed, of which each member is characterized by a particular set of minimal
constraints.

2.6.1 Recursive least-squares

In this section we will partition the observation equations into two sets and show how the
least-squares solution can be obtained using a step-wise or recursive approach.

    Let us assume that the model of observation equations E{y} = Ax , D{y} = Qy is
partitioned as

E{ y1 } = A1 x , D{ y1 } = Q1 0                                                    (2.67)
y2     A2                    y2       0 Q2

Note that y1 and y2 are assumed to be uncorrelated. This assumption is essential in order
to be able to formulate a recursive solution. We will denote the least-squares solution

which is based on the first set of observation equations as x^(1) and the least- squares
solution which is based on both sets, as x^(2). The least-squares solution which is based on
the first set of observation equations reads

       T -1 -1 T -1                   T -1 -1
x^(1) = (A1 Q1 A1) A1 Q1 y1 , Qx^ = (A1 Q1 A1)                                     (2.68)
                                 (1)

and the least-squares solution which is based on the complete set of observation equations,
reads

x^ = (AT Q-1A + AT Q-1A )-1(AT Q-1y + AT Q-1y )
(2)    11 1 22 2                      11 1 22 2                                    (2.69)
Q = (AT Q-1A + AT Q-1A )-1
x^(2)  11 1 22 2

Comparing the two solutions shows that the second solution can also be written as

x^(2) = (Q-1 x^(1) + AT2 Q-1 2 A2)-1(Q-1 x^(1) x^(1) + AT2 Q-1 2 y2) , Qx^(2) = (Q-1 x^(1) + AT2 Q-1 2 A2)-1 (2.70)

This result shows that the solution of the partitioned model can be obtained in two steps.

First one solves for the first set of observation equations. This gives x^(1) and Qx^(1). Then in
the second step this result is used together with y2 to obtain x^(2) and Qx^(2) . This recursive
procedure has an important implication in practice. It implies that if new observations,

say y2, become available one does not need to save the old observations y1 to compute
x^(1). One can compute x^(2) from y2 and the solution x^(1) of the first step. It will be clear
that one can extent the recursion to more than two steps by using a partitioning of the

model of observation equations into more than two sets.
34                                                                                              Network Quality Control

    Note that the first equation of (2.70) can also be written as

                              -1            T -1 -1 T -1
    x^(2) = x^(1) + (Qx^ + A2 Q2 A2) A2 Q2 (y2 - A2x^(1))                                            (2.71)
                              (1)

The expression now clearly shows how x^(2) is found from updating x^(1). The correction to
x^(1) depends on the difference y2 -A2x^(1). Since A2x^(1) can be interpreted as the prediction
of E{y2} based on y1, the difference y2 - A2x^(1) is called the predicted residual of E{y2}.
Note that the predicted residual is not the same as the least-squares residual. The least-
squares residual of E{y2} reads namely y2 - A2x^(2).

    Note that in the above expressions we need to invert a matrix having an order which

is equal to the number of entries in the parameter vector x. One can however also derive

an expression for x^(2) in which a matrix needs to be inverted which has an order equal to
the dimension of y2. From the matrix identity

    (Q-1 + AT Q-1A )-1AT Q-1 = Q                                    AT   (Q2  +  A2  Qx^     AT )-1
         x^(1)       22 2                   22            x^   (1)                      (1)
                                                                      2                        2

follows that the solution (2.70) may also be written as

                        = x^(1) + Qx^(1)AT2 (Q2 + A2Qx^(1) AT2 )-1(y2 - A2x^(1))
     x^(2)

     Qx^                                                                                             (2.72)

                        =     Qx^        - Qx^       AT   (Q2  +    A2   Qx^     AT )-1A Q2
                (2)                 (1)         (1)                         (1)           2 x^(1)
                                                       2

Both expressions (2.70) and (2.72) give identical results. But the second expression is

more advantageous than the first, when the dimension of y2 is small compared to the
dimension of x. In that case the order of the matrix that needs to be inverted is smaller.

    The above expression shows that the correction to x^(1) is small if the predicted residual
is small. This is also what one would expect. Also note that the correction is small if the
variance of x^(1) is small. This is also understandable, because if the variance of x^(1) is
small one has more confidence in it and one therefore would like to give more weight to

it than to y2.
    The above expression also shows how the variance matrix gets updated. Because of

the minus sign, the precision of the estimator gets better. This is understandable, since by
including y2 more information is available to estimate x.

    The above results show how the least-squares solution of the parameter vector x can

be updated in recursive form. But this is not the only solution that can be updated recur-

sively. Also the weighted sum-of- squares of the least-squares residuals, can be updated

recursively. It can be shown that

    e^T    -1   e^   =  e^T1  Q1-1  e^1  +  vT  Q-1v 2                                               (2.73)

         Qy                                  2    v2

where e^1 = y1 - A1x^(1) is the least-squares residual vector of the first step and v2 = y2 -
A2x^(1) is the predicted residual which comes available in the second step.

2.6.2 Constrained least-squares

It may happen for a particular application that we know, when formulating the model
of observation equations, that some strict relations exist between the entries of the pa-
2. Estimation and precision                                        35

rameter vector. In that case, we are dealing with a model of observation equations, with
constraints on the parameter vector. Our model reads then

E{y} = Ax , D{y} = Qy with BT x = b                                (2.74)

The equations of BT x = b constitute the constraints, where matrix B and vector b are
assumed known. In order to solve the above model in a least-squares sense, we can make
use of the results of the previous section. First note that we may write (2.74) also as

E{ y } = TA x , D{ y } = Qy 0                                      (2.75)
         b  B                b       0 Qb = 0

This is again a model of observation equations which has been partitioned into two sets.

The only difference with the model treated in the previous section is that the variance
matrix corresponding to the second set of observation equations is zero (Qb = 0). This
variance matrix is set to zero, since it is assumed that the relations BT x = b are strictly

valid. Thus b, with sample value b, is interpreted as an observable having a zero variance

matrix.

Based on the results of the previous section, it follows that the solution of the above

model can also be obtained in two steps. The solution of the first step will be denoted as
x^ and the solution of the second step as x^b. The solution of the first step reads then

x^ = (AT Q-y 1A)-1AT Q-y 1y , Qx^ = (AT Q-y 1A)-1                  (2.76)

This is the solution one would get when the constraints are not taken into account. The
solution of the second step reads

         x^b = x^ + Qx^B(BT Qx^B)-1(b - BT x^)                     (2.77)
         Qx^ = Qx^ - Qx^B(BT Qx^B)-1BT Qx^

              b

This solution directly follows from using (2.72). Note that A2 and Q2 of the previous
section, correspond with BT and Qb = 0 of the present section.

    The above result shows that also the solution of a constrained model of observation

equations can be obtained in two steps. The constraints are disregarded in the first step.

In the second step, the results of the first step are used together with the constraints. Note

that the solution of the second step is in fact the solution one would obtain when solving
for the model of condition equations BT E{x^} = b , D{x^} = Qx^.

    As in the previous section, also the weighted sum-of-squares of the least-squares resid-

uals can be written as a sum. If we denote the least- squares residual vector for the con-

strained model as e^b and its counterpart when the constraints are disregared as e^, it follows
that

e^Tb Q-y 1e^b = e^T Q-y 1e^ + (b - BT x^)T (BT Qx^B)-1(b - BT x^)  (2.78)

Remark: In some applications it may happen that we perform the adjustment with con-

straints on the parameters, although we know that these constraints are in fact not deter-
ministic but instead stochastic. Thus although we know that Qb = 0, we then still perform
the adjustment as if Qb = 0. This approach of course, will give a result which is less
36                                     Network Quality Control

optimal. Sometimes however, such an approach is dictated by what is practically feasi-
ble. A good example in this respect is the connection of a geodetic network to existing
control points. Since it is not practical to see the coordinates of the control points change,
everytime a new network is connected to them, the adjustment is carried out in such a way
that the coordinates of the existing control points remain fixed. Hence in the adjustment
process, their variance matrix is set to zero. But this does not mean that we are allowed to
set this variance matrix to zero as well when applying the error propagation law. In this
case, the second equation of (2.77) is thus not valid anymore. This equation was namely
obtained from applying the error propagation law to the first equation, under the assump-
tion that the constraints are nonstochastic. To obtain a proper precision description, we
need to take Qb into account as well. If this is done, we obtain instead of the second
equation of (2.77), the following expression for the variance matrix

    Qx^ = Qx^ - Qx^B(BT Qx^B)-1[BT Qx^B - Qb](BT Qx^B)-1BT Qx^                  (2.79)

         b

This result reduces to that of (2.77) when Qb = 0, showing that Qx^b < Qx^. This inequality
is not guaranteed anymore however, when Qb = 0. It then depends on whether BT Qx^B >
Qb holds true or not. This explains why it is of importance when connecting networks
to existing control, to have an existing control which is of a better precision than the

precision of the network connected to it. If this is not the case, then BT Qx^B < Qb and thus
Qx^b > Qx^, showing that the precision of the network after the connection is poorer than it
was before the connection.

2.6.3 Minimally constrained least-squares

Up to this point, all the matrices that we worked with were assumed to be of full rank.
We will now investigate what can be done when the A-matrix of the model of observation
equations

    E{y} = A x , D{y} = Qy                                                      (2.80)

    mО1  mОn nО1             mО1  mОm

is less than of full rank. Let us assume that the rank of matrix A is given as

    rank A = r < n

Hence, the rank defect equals (n - r). This implies that there exist (n - r) linear indepen-
dent combinations of the column vectors of matrix A that produce the zero vector. These
linear combinations are said to span the null space of matrix A. The null space of A is
defined as

    N(A) = {x  Rn | Ax = 0}

It is a subspace of the parameter space Rn and its dimension equals (n - r). Let us assume
that the columns of the n О (n - r) matrix G span the null space of A. Then

    R(G) = N(A) and AG = O
2. Estimation and precision          37

Thus the range space of G equals the null space of A and the columns of matrix G form
the linear combinations that need to be taken of the columns of matrix A to obtain the zero
vector. Since AG = O, it follows that

        E{y} = Ax = A(x + G) , with   Rn-r

This shows that E{y} remains unchanged, when we add the vector G, with   Rn-r
arbitrary, to the vector x. Hence, E{y} is invariant to these type of changes of the pa-
rameter vector. It will now be clear, since E{y} is insensitive to the above changes of the
parameter vector, that one can not expect the rank defect model of observation equations
to have a unique solution. The information content of the measurements is simply not
sufficient enough to determine x uniquely.

    In practice one will meet such situations for instance, when adjusting so-called free
networks. Imagine the simple example of adjusting a single levelling loop. In this case
the levelled height differences constitute the measurements and the heights of the points
in the loop constitute the unknown parameters. It will be clear that one can not determine
heights from observed height differences only. The height differences will not change
when one adds a constant value to all the heights of the points in the levelling loop. This
shows that we need some additional information, in order to be able to solve for the
heights. For this simple example, the height of one of the points in the levelling loop
would suffice already.

    Also for the general case, the lack of information in E{y} to determine x uniquely,
implies that additional information is needed. We will formulate the additional informa-
tion as constraints on the parameter vector x. Thus instead of (2.80), we will consider the
constrained model

E{y} = Ax , D{y} = Qy with BT x = 0  (2.81)

(note: For reasons of simplicity we have set the value of the constant vector b in the
constraints, equal to zero.) The matrix B is assumed to be of full rank. Thus the rows of
the matrix BT are assumed to be linearly independent.

    When introducing the constraints, it is of importance to understand, that they are
merely used as a tool to allow us to be able to compute a solution for x (note: there are
other, but equivalent, ways to deal with a rank defect model of observation equations, for
instance by using the theory of generalized inverses. This however, is beyond the scope
of the present lecture notes). Since the constraints are only there to allow us to compute
a solution for x, the constraints should not interfere with the intrinsic properties of the
adjustment itself. In other words, the constraints should contain the information which
is minimally needed to eliminate the lack of information in E{y}. This implies that the
contraints should satisfy two conditions. First, the constraints should be such that indeed
a solution for x can be computed. This however, is not the only condition the constraints
should satisfy. If it would be the only condition, then B = I would be an admissible
choice. But this choice can not be allowed of course, since it overconstraints the solution.
In fact when B is chosen equal to the identity matrix, no adjustment would be necessary
anymore and the measurements would not contribute to the solution. This clearly, is not
acceptable. Thus the constraints should also satisfy a second condition, which is that the
38                                          Network Quality Control

least-squares solution of the measurements, y^, is invariant to the particular choice of the
constraints.

    If we translate the above conditions in terms of linear algebra, it follows that the
constraints are admissible, if and only if the matrix B satisfies

    Rn = R(B)  R(AT )                       (2.82)

Thus the range space of matrix B should be complementary to the range space of matrix
AT . Two spaces are said to be complementary, when the two spaces together span the

whole space of real numbers and their intersection only contains the zero vector. Since
the dimension of Rn equals n and the dimension of R(AT ) equals the rank of A, which is
rank A = r, it follows that the dimension of R(B) must equal

    dimension R(B) = n - r

This shows that the number of linearly independent constraints that are admitted, equals
(n - r). This is also understandable if one considers the dimension of the null space of A.

That is, one needs as many constraints as there are dimensions in the null space of A.

    An alternative, but equivalent formulation of the above condition (2.82) can be for-
mulated as follows. Let the linear independent columns of the n О r matrix B span the
null space of matrix BT . The above condition is then equivalent to

    Rn = N(A)  R(B)                         (2.83)

Thus the range space of matrix B should be complementary to the null space of matrix

A. A direct consequence of this condition is that the matrix G, of which the columns span
N(A), and the matrix B, together form a square and full rank matrix. Thus the partitioned
matrix (B, G) is square and invertible. Since it is square and invertible, it may be used

to reparametrize the parameter vector x as

    x = (B, G)                              (2.84)
                      

This equation establishes a one-to-one relation between on the one hand x and on the other
hand  and . When we substitute (2.84) into (2.81), we obtain

    E{y} = AB , D{y} = Qy with BT G = 0     (2.85)

since AG = O and BT B = O. Note that this reparametrization shows which part of the

unknown parameters can be determined from the measurements and which part is deter-
mined by the constraints. Since the matrix BT G is square and invertible, it directly follows
from the constraints that  = 0 (note: the invertibility of BT G is a direct consequence of
(2.82) or (2.83)). Thus  is determined by the constraints and  is the part that still needs
to be determined in a least-squares sense from the measurements. The least-squares solu-
tion for  reads

    ^ = [(B)T AT Q-y 1A(B)]-1(B)T AT Qy-1y
2. Estimation and precision                                                             39

If we substitute this together with  = 0 into (2.84), we obtain the least-squares solution

of the minimally constrained model (2.81) as

                 =  (B  )[  (B  )T  AT  Q  -  1  A(B  )]  -  1  (B  )T  AT  Q  -  1  y
 x^b                                       y                                   y

 Qx^                                                                                    (2.86)

                 =  (B  )[  (B  )T  AT  Q  -  1  A(B  )]  -  1  (B  )T
                                           y
              b

This is the unique least-squares solution of the minimally constrained model (2.81) and it
is one of the infinitely many least-squares solutions of the rank defect model (2.80). The
whole family of least-squares solutions of model (2.80) reads then

x^ = x^b + G                                                                            (2.87)

where  is still undetermined.
    An important property of these least-squares solutions is that they all produce the same

least-squares solution for the measurements. Thus y^ = Ax^ is invariant for the choice of
the constraints BT x = 0, as long as they are minimal, that is, as long as matrix B satisfies
the condition (2.82). In terms of the example of the levelling loop this corresponds to the
fact that the least-squares solution for the height differences will be invariant to the choice
which one of the points in the loop is assumed to be constrained in height.

S-transformations: Above we have shown how an arbitrary least-squares solution x^ of

model (2.80) can be obtained from a particular solution x^b and an undetermined part G.

But also the inverse is possible. That is, one can also obtain a particular solution from any

arbitrary solution. To see this, we premultiply (2.87) with BT . This gives BT x^ = BT G,
since BT x^b = 0 by definition. Hence,  = (BT G)-1BT x^. From substituting this into (2.87)

follows then     = [I - G(BT G)-1BT ]x^
        
         x^b

 Qx^ = [I - G(BT G)-1BT ]Qx^[I - G(BT G)-1BT ]T                                         (2.88)

            b

The second equation has been obtained from applying the error propagation to the first
equation. This result shows that we only need to know the transformation matrix

Sb = [I - G(BT G)-1BT ]                                                                 (2.89)

and thus the matrices B and G, in order to obtain x^b from x^. This transformation matrix
is a very famous one in geodesy and is known as the S-transformation. With the S-
transformation we are thus able to transform any least-squares solution to a minimally
constrained solution, specified by the matrix B. The matrix G, of which the columns span
the null space of A, depends on A and thus on the type of measurements which are included
in the model. For a levelling network for instance, the undetermined part will correspond
with a constant shift in the height of all points. For a triangulation network however, the
undetermined part will correspond to a translation, a rotation and a scale change of the
network. Angles do namely not contain information on the position, orientation and size
of the network.
40                                                                           Network Quality Control

    One may wonder how the above results correspond to the results which were obtained
in the previous section for the case of constrained least- squares. In the previous section,
where matrix A was assumed to be of full rank, we showed that the least-squares solution
of the constrained model could be obtained in two steps. The unconstrained solution of
the first step, was used as input for the second step to finally come up with the constrained
solution. These two steps can also be recognized in case of the minimally constrained so-
lution. The first step corresponds then with x^, which is an arbitrary least-squares solution
of model (2.80), and the second step corresponds then with (2.88).

Unbiasedness: In one of our earlier sections we claimed that the least-squares estimator
of the parameter vector is unbiased and thus that E{x^} = x holds true. At that point
however, we still assumed the matrix A to be of full rank. So, what happens when the
matrix A is not of full rank? We know that in that case the measurements fail to contain
enough information to determine x uniquely. We may therefore suspect that in that case
the least-squares estimators will also fail to be unbiased estimators of the parameter vector
x. And indeed, when we take the expectation of (2.86), we obtain

    E{x^b}  =  (B  )[  (B  )T  AT  Q  -  1  A(B  )]  -  1  (B  )T  AT  Q  -  1  E  {  y  }
                                      y                                   y
            = (B)[(B)T AT Q-y 1A(B)]-1(B)T AT Q-y 1Ax = x

Thus x^b is not an unbiased estimator of x. If the minimally constrained least-squares
estimator is not an unbiased estimator of x, what is it then an unbiased estimator of? To
see this, we substitute (2.84) into the above equation. This gives E{x^b} = B = x - G,
or when  = (BT G)-1BT x is substituted in it,

    E{x^b} = [I - G(BT G)-1BT ]x = Sbx                                                      (2.90)

This shows that x^b is an unbiased estimator of Sbx. In terms of our levelling example
it means that the minimally constrained least-squares estimators of the heights are not
unbiased estimators of absolute heights, but instead are unbiased estimators of the heights
which are defined through the minimal constraint, i.e. the constraint height of one of the
points in the levelling network.

2.7 Quality control: precision

Now that we have developed most of the adjustment theory that will be needed in these
lecture notes, let us pause for a few moments and reflect on the various steps involved
when an adjustment is carried out.

Formulate model: Before any adjustment can be carried out, one first must have a clear
idea of: (1) the observables that are going to be used, (2) their assumed relation to the
unknown parameters and (3) their assumed precision. Thus first one must be able to
formulate the model of observation equations

    E{y} = Ax , D{y} = Qy                                                                   (2.91)

This model consists of the functional model E{y} = Ax, which is specified through the
mОn design matrix A, and of the stochastic model D{y} = Qy, which is specified through
2. Estimation and precision    41

the m О m variance matrix Qy. In the functional model the link is established between the
measurements and the unknown parameters. It captures the geometry of the network. In
the stochastic model, one specifies the precision of the measurements. It depends on the
type of measurement equipment used and on the measurement procedures used.

Adjustment: Based on the above model and on an available sample of y, the mea-
surements, one can commence with the actual adjustment. Using the principle of least-
squares, the estimator for the unknown parameter vector x reads

x^  =  (AT  Q-1A)-1  AT  Q-1y  (2.92)

              y            y

Here it has been assumed that the design matrix is of full rank. If this is not the case, then
the approach of using minimal constraints needs to be used.

    If we assume that y is normally distributed, then its probability density function is
completely specified by its first two moments, being its expectation and its dispersion.
This then also holds true for the probability density function of the estimator x^. The qual-
ity of this estimator can thus be characterized by its expectation E{x^} and its dispersion
D{x^}. For its expectation we have

E{x^} = x                      (2.93)

Loosely speaking, this implies that if the adjustment is repeated a sufficient number of
times, each time with a new sample from y, that the various outcomes for x^ would on
the average coincide with x. This property of unbiasedness is an important one and it is
automatically fulfilled when the least-squares principle is used, provided of course that
the model which forms the basis of the adjustment is correct. In the next chapter we will
see what happens when the model is specified incorrectly.

Precision: For the moment we will assume that the model used is correct and thus that the

estimator x^ is unbiased. In that case, the quality of the estimator is completely captured
by its dispersion, being the variance matrix Qx^. Thus under the provision that the least-
squares estimator is unbiased, we may judge its quality on the basis of its variance matrix

Qx^ = (AT Q-y 1A)-1            (2.94)

The variance matrix is said to describe the precision of the estimator. It describes the
amount of variability in samples of x^ around its mean.

    Since the variance matrix depends on A and on Qy, one can change Qx^ by changing
A and/or Qy. This thus gives us a way of improving the precision of the least-squares
solution. For instance, if for a certain application the precision turns out to be not good
enough, one may decide to use more precise measurement equipment. In that case Qy
changes for the better and also Qx^ will then change for the better. In many applications
however, one will not have too much leeway in choosing from different sets or types of
measurement equipment. In that case one depends on A for improving the precision (note:
for that reason, matrix A is also often referred to as the design matrix). There are two ways
in which A can be changed. One can change the dimension of the matrix and/or one can
change its structure. For instance, when the precision of a geodetic network is not good
42                                          Network Quality Control

enough one can decide to add more observations to the network. In that case, matrix A is
extended row wise. However, it may also be possible that a mere change in the geometry
of the network already suffices to improve its precision. In that case, it is the structure of
A that changes, while its dimension stays the same.

Precision testing: Although we know how to change Qx^ for the better or the worse, we
of course still need a way of deciding when the precision, as expressed by the variance
matrix, is good enough. It will be clear that this depends very much on the particular
application at hand. What is important though, is that one has a precision criterium avail-
able by which the precision of the least-squares solution can be judged. The following are
some approaches that can be used for testing the precision of the solution.

    It may happen in a particular application, that one is only interested in one particular
function of the parameters, say  = f T x. In that case it becomes very easy to judge its
quality. One then simply has to compare its variance with the given criterium

      2  =  f T Qx^ f  <   criterium        (2.95)

    ^

The situation becomes more difficult though, when the application at hand requires that

the precision of more than one function needs to be judged upon. Let us assume that
the functions of interest are given as  = FT x, where F is a matrix. The corresponding
variance matrix is then given as Q^ = FT Qx^F. One way to judge the precision in this
case, is by inspecting the variances and covariances of the matrix Q^ . An alternative way
would be to use the average precision as precision measure. In that case one relies on the

trace of the variance matrix

    1  trace  (F  T  Qx^F  )  <  criterium  (2.96)
    p

where p is the dimension of the vector  . When using the trace one has to be aware of
the fact that one is not taking all the information of the variance matrix Q^ into account.
It depends only on the variances and not on the covariances. Hence, the correlation be-
tween the entries of ^ are then not taken into account. When using the trace one should
also make sure that it makes sense to speak of an average variance. In other words the
entries of  should contain the same type of variables. It would not make sense to use
the trace, when  contains completely different variables, each having their own physical
dimension.

    The trace of Q^ equals the sum of its eigenvalues. Instead of the sum of eigenvalues,
one might decide that it suffices to consider the largest eigenvalue max only,

    max(FT Qx^F) < criterium                (2.97)

In that case one is thus testing whether the function of  which has the poorest precision,
still passes the precision criterium. When this test is passed successfully, one knows that
all other functions of  will also have a precision which is better than the criterium. For
some applications this may be an advantage, but it could be a disadvantage as well. It
could be a disadvantage in the sense that the above test could be overly conservative.
That is, when the function having the poorest precision passes the above test, all other
2. Estimation and precision  43

functions, which by definition have a better precision, may turn out to be unnecessarily
precise.

    So far we assumed that the precision criterium was given in scalar form. But this need
not be the case. The precision criterium could also be given in matrix form. In that case
one is working with a criterium matrix, which we will denote as Cx. The precision test
amounts then to testing whether the precision as expressed by the actual variance matrix
Q^ = FT Qx^F is better than or as good as the precision expressed by the criterium matrix
FTCxF. Also this test can be executed by means of solving an eigenvalue problem, but
now it will be a generalized eigenvalue problem

| FT Qx^F -  FT CxF |= 0     (2.98)

Note that when the matrix FTCxF is taken as the identity matrix, the largest eigenvalue of
(2.98) reduces to that of (2.97). Using the generalized eigenvalue problem is thus indeed
more general than the previous discussed approaches. It is characterized by the fact that
it allows one to compare the actual variance matrix Q^ directly with its criterium. The
two matrices FT Qx^F and FTCxF are identical, when all eigenvalues equal one, and all
functions of ^ have a better precision than the criterium when the largest generalized
eigenvalue is less than one.

    So far we assumed the matrix A to be of full rank. In many geodetic applications
however, this is not the case. We know from our section on minimally constrained least-
squares, that the variance matrix Qx^ will not be unique, when A has a rank defect. In that
case the variance matrix depends on the chosen set of minimal constraints. Since these
constraints should not affect our conclusion when evaluating the precision, we have to
make sure that our procedure of precision testing is invariant for the minimal constraints.
This is possible when we make use of the S-transformations. Let Cx be the criterium
matrix and Qx^b the variance matrix of a minimally constrained solution. The eigenvalues
of the generalized eigenvalue problem

| Qx^ -  SbCxSTb |= 0        (2.99)

       b

where Sb is the S-transformation that corresponds to the minimal constraints of Qx^b , are
then invariant to the chosen set of minimal constraints. Thus when the largest eigenvalue

is less than one, it is guaranteed that all functions of x^b have a precision which is better
than what they would have were Qx^b be replaced by the criterium matrix.
Chapter 3

Testing and reliability

3.1 Introduction

In the previous chapter we considered the model of observation equations

E{y} = Ax , D{y} = Qy                                                     (3.1)

and showed how a solution for the unknown parameter vector x, based on the least-squares
principle, could be obtained. The least-squares solution x^ is optimal in the sense that it is
unbiased and that it is of minimal variance in the class of linear unbiased estimators. These
optimality properties only hold true however, when the model (3.1) is correct. There are
many ways in which the model could have been misspecified. The functional model
could be wrong, in which case E{y} = Ax. As a consequence, the least-squares estimator
becomes biased, E{x^} = x. Or, the stochastic model could be wrong, in which case
D{y} = Qy. As a consequence, the property of minimal variance is lost.

    The topic of the present chapter is to present ways of checking the validity of the
above model. We start off in section 3.2, by discussing the basic concepts of hypothesis
testing. We consider the general form of a null hypothesis and an alternative hypothesis,
show what a test between the two implies and discuss the two type of errors one can
make. Following these basic concepts, we move on in section 3.3 to the testing of the
above model against the alternative model

E{y} = Ax +C , D{y} = Qy                                                  (3.2)

Note that the two models only differ in their functional model. Hence we restrict ourselves
in these lecture notes to misspecifications in the mean of y. In geodetic practice these are
by far the most frequently occurring type of model errors (e.g. measurement blunders,
unaccounted systematic effects). In section 3.3, we present and discuss the test statistic
T q through which the model (3.1) can be tested against the alternative (3.2).

    In most practical applications, it is usually not only one model error one is concerned
about, but quite often many more than one. To each of these model errors there belongs
a vector C, with the matrix C specifying how the model error is related to the vector
of observables. As a result one is not dealing with only one alternative hypothesis, but
with as many alternative hypotheses as there are model errors one is willing to consider.
This implies that one needs a testing procedure for handling the various alternative hy-
potheses. In section 3.4 such a procedure is presented. It consists of a detection step, an
identification step and an adaptation step.

    Just like the results of an adjustment are not exact (that is, not deterministic, but
stochastic), also the results of the statistical tests are not exact. The confidence one will
have in the outcomes of the statistical tests depends in a large part on the 'strength' of the
model. A practical way of diagnosing this confidence is provided for by the concept of

                          45
46                                                          Network Quality Control

reliability. It is introduced in section 3.5. Reliability together with precision, can then be
considered to describe the quality which one can expect of the results of an adjustment
and testing. They are both considered in the last section of this chapter.

3.2 Basic concepts of hypothesis testing

3.2.1 Statistical hypotheses

Many social, technical and scientific problems result in the question whether a particular
theory or hypothesis is true or false. In order to answer this question one can try to design
an experiment such that its outcome can also be predicted by the postulated theory. After
performing the experiment, one can then confront the experimental outcome with the
theoretically predicted value and on the basis of this comparison try to conclude whether
the postulated theory or hypothesis should be rejected. That is, if the outcome of the
experiment disagrees with the theoretically predicted value, one could conclude that the
postulated theory or hypothesis should be rejected. On the other hand, if the experimental
outcome is in agreement with the theoretically predicted value, one could conclude that
as yet no evidence is available to reject the postulated theory or hypothesis.

    Example 10

    According to the postulated theory or hypothesis the three points 1, 2 and 3
    of figure 3.1 lie on one straight line. In order to test or verify this hypothesis

                             2

                     l12                               l23

    1                        l13                            3

Figure 3.1 Three distances between three known points

    we need to design an experiment such that its outcome can be compared with
    the theoretically predicted value.

    If the postulated hypothesis is correct, the three distances l12, l23 and l13
    should satisfy the relation:

    l13 = l12 + l23

    Thus, under the assumption that the hypothesis is correct we have:

    H : l12 + l23 - l13 = 0                                             (3.3)

    To denote a hypothesis, we will use a capital H followed by a colon that in
    turn is followed by the assertion that specifies the hypothesis.

    As an experiment we can now measure the three distances l12, l23 and l13,
    compute l12 + l23 - l13 and verify whether this computed value agrees or dis-
    agrees with the theoretically predicted value of H. If it agrees, we are inclined
3. Testing and reliability                                                        47

to accept the hypothesis that the three points lie on one straight line. In case
of disagreement we are inclined to reject hypothesis H.

    It will be clear that in practice the testing of hypotheses is complicated by the fact that
experiments (in particular experiments where measurements are involved) in general do
not give outcomes that are exact. That is, experimental outcomes are usually affected by
an amount of uncertainty, due for instance to measurement errors. In order to take care
of this uncertainty, we will, in analogy with our derivation of Adjustment Theory in the
previous chapter, model the uncertainty by making use of the results from the theory of
random variables. The verification or testing of postulated hypotheses will therefore be
based on the testing of hypotheses of random variables of which the probability distri-
bution depends on the theory or hypothesis postulated. From now on we will therefore
consider statistical hypotheses.

Statistical hypothesis: A statistical hypothesis is an assertion or conjecture about the
probability distribution of one or more random variables, for which it is assumed that a
random sample (mostly through measurements) is available.

The structure of a statistical hypothesis H is in general the following:

H : y  py(y|x)                                                                    (3.4)

with x fully or partially specified. This statistical hypothesis should be read as follows:
According to H the scalar or vector observable random variable y has a probability density
function given by py(y|x). The scalar, vector or matrix parameter x used in the notation
of py(y|x) indicates that the probability density function of y is known except for the
unknown parameter x. Thus, by specifying (either fully or partially) the parameter x, an
assertion or conjecture about the density function of y is made. In order to see how a
statistical hypothesis for a particular problem can be formulated, let us continue with our
example 10.

Example 10 (continued)

We know from experience that in many cases the uncertainty in geodetic mea-

surements can be adequately modeled by the normal distribution. We there-

fore model the three distances between the three points 1, 2 and 3 as normally

distributed random variables 1 If we also assume that the three distances are
uncorrelated and all have the same known variance 13 2, the simultaneous
probability density function of the three distance observables becomes:

                              E{l13}  
   l 13
 l12   N( E{l12}  , Q) with Q = 3 I3    1
                                                                          (3.5)

l 23                          E{l23}

The notation N(a, B) is a shorthand notation for the normal distribution, hav-
ing a as mean and B as variance matrix. Statement (3.5) could already be

    1Note that strictly speaking distances can never be normally distributed. A distance is always nonnegative,
whereas the normal distribution, due to its infinite tails, admits negative sample values.
48                                 Network Quality Control

    considered a statistical hypothesis, since it has the same structure as (3.4).
    Statement (3.5) asserts that the three distance observables are indeed nor-
    mally distributed with unknown mean, but with known variancematrix Q.
    Statement (3.5) is however not yet the statistical hypothesis we are looking
    for. What we are looking for is a statistical hypothesis of which the probabil-
    ity density function depends on the theory or hypothesis postulated. For our
    case this means that we have to incorporate in some way the hypothesis that
    the three points lie on one straight line. We know mathematically that this
    assertion implies that

    l12 + l23 - l13 = 0            (3.6)

    However, we cannot make this relation hold for the random variables l12, l23
    and l13. This is simply because of the fact that random variables cannot be
    equal to a constant. Thus, a statement like: l12 + l23 - l13 = 0 is nonsensical.
    What we can do is assume that relation (3.6) holds for the expected values of

    the random variables l12, l23 and l13:

    E{l12} + E{l23} - E{l13} = 0   (3.7)

    For the hypothesis considered this relation makes sense. It can namely be
    interpreted as stating that if the measurement experiment were to be repeated
    a great number of times, then on the average the measurements will satisfy
    (3.7). With (3.5) and (3.7) we can now state our statistical hypothesis as:

                          E{l13} 
       l 13
     l12   N( E{l12}  , 1 I3)
    H: 3
    l 23                 E{l23}    (3.8)

              with E{l12} + E{l23} - E{l13} = 0

    This hypothesis has the same structure of (3.4) with the three means playing
    the role of the parameter x.

    In many hypothesis-testing problems two hypotheses are discussed: The first, the

hypothesis being tested, is called the null hypothesis and is denoted by H0. The second
is called the alternative hypothesis and is denoted by HA. The thinking is that if the null
hypothesis H0 is false, then the alternative hypothesis HA is true, and vice versa. We often
say thatH0 is tested against, or versus, HA.

    In studying hypotheses it is also convenient to classify them into one of two types by

means of the following definition: if a hypothesis completely specifies the distribution,

that is, if it specifies its functional form as well as the values of its parameters, it is called

a simple hypothesis; otherwise it is called a composite hypothesis.
3. Testing and reliability                                                       49

Example 10 (continued)

In our example, (3.8) is the hypothesis to be tested. Thus, the null hypothesis

reads in our case:

                              E{l13}  
          l 13
       H0 :  l12   N( E{l12}  , 13 I3)
       l 23                   E{l23}      (3.9)

       with E{l12} + E{l23} - E{l13} = 0

Since we want to find out whether E{l12} + E{l23} - E{l13} = 0 or not,
we could take as alternative the inequality E{l12} + E{l23} - E{l13} = 0.

However, we know from the geometry of our problem that the left hand side

of the inequality can never be negative. The alternative should therefore read:

E{l12} +E{l23} -E{l13} > 0. Our alternative hypothesis takes therefore the

form:

                              E{l13}  
          l13
       HA :  l12   N( E{l12}  , 13 I3)
       l23                    E{l23}      (3.10)

       with E{l12} + E{l23} - E{l13} > 0

When comparing (3.9) and (3.10) we see that the type of the distribution

of the observables and their variance matrix are not in question. They are

assumed to be known and identical under both H0 and HA.

Both of the above hypotheses, H0 and HA, are examples of composite hy-
potheses. The above null hypothesis H0 would become a simple hypothesis
if the individual expectations of the observables were assumed known.

3.2.2 Test of statistical hypotheses

After the statistical hypotheses H0 and HA have been formulated, one would like to test
them in order to find out whether H0 should be rejected or not.

Test of a statistical hypothesis: A test of a statistical hypothesis

        H0 : y  py(y|x)
with x fully or partially specified, is a rule or procedure, in which a random sample of y
is used for deciding whether to reject or not reject H0. A test of a statistical hypothesis is
completely specified by the so-called critical region, which will be denoted by K.

Critical region K: The critical region K of a test is the set of sample values of y for which
H0 is to be rejected. Thus, H0 is rejected if y  K.

    It will be obvious that we would like to choose a critical region so as to obtain a test
with desirable properties, that is, a test that is "best" in a certain sense. But let us first
have a look at a simple testing problem for which, on more or less intuitive grounds, an
acceptable critical region can be found.
50                                                                Network Quality Control

    Example 11

    Let us assume that a geodesist measures a scalar variable, and that this mea-
    surement can be modeled as a random variable y with density function

    1               1  2
    y   exp[- (y - E{y}) ]                                          (3.11)
    2               2

    Thus, it is assumed that y has a normal distribution with unit variance. Al-
    though this assumption constitutes a statistical hypothesis, it will not be

    tested here because the geodesist is quite certain of the validity of this as-

    sumption. The geodesist is however not certain about the value of the expec-
    tation of y. His assumption is that the value of E{y} is x0. This assumption
    is the statistical hypothesis to be tested. Denote this hypothesis by H0. Then,

    H0 : E{y} = x0                                                  (3.12)

    Let HA denote the alternative hypothesis that E{y} = x0. Then:

    HA : E{y} = x0                                                  (3.13)

    Thus the problem is one of testing the simple hypothesis H0 against the com-
    posite hypothesis HA. To test H0, a single observation on the random variable
    y is made. In real-life problems one usually takes several observations, but to

    avoid complicating the discussion at this stage only one observation is taken

    here. On the basis of the value of y obtained, denoted by y, a decision will

    be made either to accept H0 or reject it. The latter decision, of course, is
    equivalent to accepting HA. The problem then is to determine what values
    of y should be selected for accepting H0 and what values for rejecting H0. If
    a choice has been made of the values of y that will correspond to rejection,

    then the remaining values of y will necessarily correspond to acceptance. As

    defined above, the rejection values of y constitute the critical region K of the

    test. Figure 3.2 shows the distribution of y under H0 and under two possible
    alternatives HA1 and HA2. Looking at this figure, it seems reasonable to re-
    ject H0 if the observation y is remote enough from E{y}. If H0 is true, the

       HA2 Ho HA1

                                  E{y} = x0 E{y} = x0  E{y} = x0
Figure 3.2 H0 : E{y} = x0 versus HA : E{y} = x0.
3. Testing and reliability                                                            51

    probability of a sample of y falling in a region remote from E{y} is namely

    small. And if HA is true, this probability may be large. Thus the critical re-
    gion K should contain those sample values of y that are remote enough from
    E{y}. Also, since the alternative hypothesis can be located on either side of
    E{y}, it seems obvious to have one portion of K located in the left tail of H0
    and one portion of K located in the right tail of H0. Finally, one can argue
    that since the distribution is symmetric about its mean value, also the critical
    region K should be symmetric about E{y}. This as a result gives the form of

    the critical region K as shown in figure 3.3.

                                          Ho

                              K     E{y} = x0    K
                            reject     accept  reject

Figure 3.3 Critical region K for testing H0 : E{y} = x0 versus HA : E{y} = x0.

3.2.3 Two types of errors

We have seen that a test of a statistical hypothesis is completely specified once the critical
region K of the test is given. The null hypothesis H0 is rejected if the sample value or
observation falls in the critical region, i.e. if y  K. Otherwise the null hypothesis H0 is
accepted, i.e. if y /K. With this kind of thinking two types of errors can be made:

    Type I error: Rejection of H0 when in fact H0 is true.

    Type II error: Acceptance of H0 when in fact H0 is false.

    Table 3.1 shows the decision table with the type I and II errors.

    The size of a type I error is defined as the probability that a sample value of y falls in
the critical region when in fact H0 is true. This probability is denoted by  and is called
the size of the test or the level of significance of the test. Thus:

     = P(type I error) = P(rejection of H0 when H0 is true)

or

     = P(y  K|H0) = py(y|H0)dy                                                        (3.14)

                                       K

The size of the test, , can be computed once the critical region K and the probability
density function of y is known under H0. The size of a type II error is defined as the
52                                                       Network Quality Control

Table 3.1 Decision table with type I and type II error.

         Reject H0                     H0 true             H0 false
           yK                          Wrong                Correct
                                     Type I error
         Accept H0                     Correct              Wrong
           y /K                                          Type II error

probability that a sample value of y falls outside the critical region when in fact H0 is
false. This probability is denoted by  . Thus:

     = P(type II error) = P(acceptance of H0when H0 is false)

or

     = P(y /K|HA) = 1 - py(y|HA)dy                                                    (3.15)

                                             K

The size of a type II error,  , can be computed once the critical region K and the proba-
bility density function of y is known under HA.

    Example 12
    Assume that y is distributed as

         y  N(E{y}, 2)                                                  (3.16)

    with known variance 2.
    The following two simple hypotheses are considered

         H0 : E{y} = x0                                                 (3.17)

    and

         HA : E{y} = xA > x0                                            (3.18)

    The situation is sketched in figure 3.4.

    Since the alternative hypothesis HA is located on the right of the nullhypoth-
    esis H0, it seems intuitively appealing to choose the critical region K right-
    sided. Figure 3.5a and 3.5b show two possible right-sided critical regions
    K. They also show the size of the test, , which corresponds to the area un-
    der the graph of the distribution of y under H0 for the interval of the critical
    region K.

    The size of the test, , can be computed once the probability density function
    of y under H0 is known and the form and location of the critical region K is
3. Testing and reliability                                                          53

                               Ho                HA

                               x0                xA

Figure 3.4 Two simple hypotheses: H0 : E{y} = x0 and HA : E{y} = xA > x0.

known. In the present example the form of the critical region has been chosen
right-sided. Its location is determined by the value of k , the so-called critical
value of the test. Thus, for the present example the size of the test can be
computed as

           

 = py(y|x0)dy

         k

or, since

        py(y|x0) =  12 exp[- 2 2 1 1 (y - x0)2]                            (3.19)
as

         =   1 exp[- k 2 2  2 1 1 (y - x0)2]dy

When one is dealing with one-dimensional normally distributed random vari-
ables, one can usually compute the size of the test, , from tables given for

the standard normal distribution. In order to compute (3.19) with the help

of such a table, we first have to apply a transformation of variables to (3.19).
Since y is normally distributed under H0 with mean x0 and variance 2, it
follows that the random variable z, defined as

z = y - x0                                                                 (3.20)
       

is standard normally distributed under H0. And since

 = P(y > k |H0) = P(z > k - x0  |H0)                                       (3.21)

we can use the last expression of (3.21) for computing . Application of the
change of variables (3.20) to (3.19) gives

           1                       12
 = k -x  exp[- z ]dz                                                       (3.22)
              0             2      2
           
54                                                                      Network Quality Control

                    Ho                                    HA

                                                              

                    x0      k xA                                K

            accept                                              reject

                          (a)

                    Ho                                    HA

                          

                    k x0                                  xA
                                                                   K
    accept
                                                          reject

                                                     (b)
Figure 3.5 Critical region K and size of test, .

    We can now make use of the table of the standard normal distribution. Table
    2.6 shows some typical values of the  and k for the case that x0 = 1 and
     = 2.

    As we have seen the location of the critical region K is determined by the

    value chosen for k , the critical value of the test. But what value should
    we choose for k ? Here the geodesist should base his judgement on his
    experience. Usually one first makes a choice for the size of the test, , and
    then by using (3.22) or Table 3.2 determines the corresponding critical value
    k . For instance, if one fixes  at  = 0.01, the corresponding critical value
    k (for the present example with x0 = 1 and  = 2) reads k = 5.64. The
    choice of  is based on the probability of a type I error one is willing to
    accept. For instance, if one chooses  as  = 0.01, one is willing to accept
    that 1 out of a 100 experiments leads to rejection of H0 when in fact H0 is
    true.

    Let us now consider the size of a type II error,  . Figure 3.6 shows for the
    present example the size of a type II error,  . It corresponds to the area under
    the graph of the distribution of y under HA for the interval complementary to
3. Testing and reliability                                                          55

Table 3.2 Test size , critical value k for x0 = 1 and  = 2.

                             k - x0 k
                                        

                            0.1 1.28        3.56

                            0.05 1.65       4.30

                            0.01 2.32       5.64

                            0.001 2.98      6.96

the critical region K. The size of a type II error,  , can be computed once the

                              Ho        HA

                                            

                            x0 k xA               K
                                                  reject
           accept

Figure 3.6 The sizes of type I and type II error,  and  , for testing H0 : E{y} = x0 versus
HA : E{y} = xA > x0.

probability density function of y under HA is known and the critical region K
is known. Thus, for the present example the size of the type II error can be

computed as

           k

 = - py(y|xA)dy

or, since

        py(y|xA) =  12 exp[- 2 2 1 1 (y - xA)2]              (3.23)
as

         = k -  1  2 exp[- 2  2 1 1 (y - xA)2]dy

Also this value can be computed with the help of the table of the standard
normal distribution. But first some transformations are needed. It will be
clear that the probability that a sample or observation of y falls in the critical
56                                              Network Quality Control

    region K when HA is true, is identical to 1 minus the probability that the
    sample does not fall in the critical region when HA is true. Thus,

     = P(y /K|HA) = 1 - P(y  K|HA)                            (3.24)

    Since for the present example,
            P(y  K|HA) =   1 exp[- k 2 2  2 1 1 (y - xA)2]dy

    substitution into (3.24) gives

    1 -  =   1 exp[- k 2 2  2 1 1 (y - xA)2]dy                (3.25)

    This formula has the same structure as (3.19). The value 1 -  can therefore
    be computed in exactly the same manner as the size of the test, , was com-
    puted. And from 1 -  it is trivial to compute  , the size of the type II error.

3.2.4 General steps in testing hypotheses

Thus far we have discussed the basic concepts underlying most of the hypothesis-testing
problems. The same concept and guidelines will provide the basis for solving more com-
plicated hypothesis-testing problems as treated in the next sections. Here we summarize
the main steps of testing hypotheses about a general probability model.

    a From the nature of the experimental data and the consideration of the assertions
       that are to be examined, identify the appropriate null hypothesis and alternative
       hypothesis:

                H0 : y  py(y|x0) versus H0 : y  py(y|xA)

       In most geodetic applications, the probability density function can be considered
       to be normal under the null hypothesis as well as under the alternative hypothesis.
       Since the normal distribution is uniquely characterized by its first two (central)
       moments, the expectation (mean) and dispersion (variance), the two hypotheses
       can then only differ with respect to the mean and/or variance of y. In these lecture
       notes, we will only consider differences in the mean of y.

    b Determine the critical region K. In practice this implies that one determines a func-
       tion of y, the test statistic, of which the values should always be larger than a chosen
       critical value. The values of y which make this happen, form the critical region.

    c Specify the size of the type I error, , that one wishes to assign to the testing
       process. Use tables to determine the location of the critical region K from

                = P(y  K|H0) = py(y|x0)dy

                                                         K
3. Testing and reliability                                                                     57

   For many distributions, like the normal distribution or the Chi-squared distribution,
   these tables can be found in standard textbooks on statistics.

d Compute the size of the type II error, 

            = P(y /K|HA) = 1 - py(y|xA)dy

                                                         K

   to ensure that there exists a reasonable protection against type II errors. Its com-
   plement,  = 1 -  , is known as the detection probability or as the power of the
   test.

e After the test has been explicitly formulated, determine whether the sample or ob-
   servation y of y falls in the critical region K or not. Reject H0 if y  K, and accept
   H0 if y /K. Never claim however that the hypotheses have been proved false or
   true by the testing.

3.3 Test statistics for the linear(ized) model

3.3.1 The null and alternative hypothesis

Before any start can be made with the application of the theory of statistical testing, one
needs to have a clear idea of the null hypothesis Ho and of the alternative hypothesis Ha.
In the previous chapter on adjustment theory, we have been working with the model of
observation equations

E{y} = Ax , D{y} = Qy

We have seen, that in order to be able to apply the least-squares principle, only the first two
moments of the random vector of observables ,y, need to be specified. The first moment
(mean) E{y} = Ax and the second moment (variance matrix) D{y} = Qy. In the case of
statistical testing however, this is not sufficient. In addition, one will have to specify the
type of probability function of y as well. Since most observational data in geodesy can
be modelled as samples drawn from a normal distribution, we will assume that y has the
normal probability density function

py(y  |  x)  =  (2  )-      m  | Qy  |- 21  ex  p  [  -  1  (y  -  Ax)T  Q  -  1  (y  -  Ax)]
                            2                                               y

                                                         2

Thus we assume that y is Normally distributed, with mean Ax and with the variance matrix
Qy. In shorthand notation

Ho : y  N(Ax, Qy)                                                                              (3.26)

This will be our null hypothesis.
    In order to test the null hypothesis against an alternative hypothesis, we need to know

what type of misspecifications one can expect in the null hypothesis. Of course, every part
of the null hypothesis could have been specified incorrectly. The mean Ax can be wrong,
the variance matrix Qy can be wrong and/or y could have a distribution other than the
58                                                             Network Quality Control

normal distribution. In these lecture notes we will restrict ourselves to misspecifications
in the mean. Experience has shown, that these are by large the most common errors that
occur when formulating the model. As alternative hypothesis , we will therefore use

      Ha : y  N(Ax +C, Qy)                                     (3.27)

where C is a known matrix of order m О q and  is an unknown vector of order q О 1.
With Ha we thus oppose the null hypothesis Ho to a more relaxed alternative hypothesis,
in which more explanatory parameters, namely , are introduced. These additional pa-
rameters are then supposed to model those effects which were assumed absent in Ho. For
instance, through C one may model the presence of one or more blunders in the data,
or the the presence of refraction, or any other systematic effect which was not taken into
account in Ho. The relation between E{y} and  is specified through the matrix C.

    The purpose of testing Ho against Ha is now, to infer whether the data supports Ho or
whether the data rejects Ho on the basis of Ha.

3.3.2 Residuals and model errors

It will be intuitively clear that the data and thus the observable y must be instrumental for
the testing of Ho against Ha. In this section we will discuss which function of y is a likely
candidate to be used in the testing of Ho versus Ha.

The least-squares residual: If we write

      y = Ax + e                                               (3.28)

then

      Ho : E{e} = 0 , Ha : E{e} = C = 0                        (3.29)

Thus the mean of the residual vector e will be zero when Ho is true and unequal to zero
when Ho is false. This shows that if we would have a sample or measurement of e avail-
able, we could use it to decide on the validity of Ho. Would the sample be close to zero,
we would be inclined to accept Ho and would it differ greatly from zero, we would be
inclined to reject Ho. Unfortunately, no sample of e = y - Ax is available, since x is un-
known. Instead of considering e, let us therefore consider its estimator. The least-squares

solution of x and e under Ho, reads

      x^  =  (AT Q-1A)-1AT Q-1yy           y

      e^  =  [Im  -  A(AT       Q-1  A)-1      AT    Q-1   ]y

                                  y                    y

With (3.28), this can also be written as

      x^  =  x    +  (AT  Q  -  1  A)-  1  AT  Q  -  1  e
                             y                    y
                                Q-1  A)-1            Q-1
      e^  =  [Im  -  A(AT                      AT          ]e
                                  y                    y

This shows, when we take the expectation and use (3.29), that

      Ho : E{x^} = x E{e^} = 0 , Ha : E{x^} = x E{e^} = C = 0  (3.30)
3. Testing and reliability                                                                                         59

Thus apart from the mean of e, also the mean of e^ is zero when Ho is true and nonzero
when Ho is false. Note that in the first case, x^ is an unbiased estimator of x, while it
becomes biased when Ho is false. Contrary to e, we do have a sample available of e^, since
it is a function of y. Hence, instead of using e, we could use e^ to decide on the validity of
Ho. If the sample of e^ is close to zero, we are inclined to accept Ho and if it differs greatly
from zero, we are inclined to reject Ho.

The least-squares model error: Instead of using the least-squares residual vector e^, it
also seems intuitively clear that the model error  itself must be instrumental in deciding
on the validity of Ho. Under Ha, we have

Ha : E{y} = Ax +C                                                                                         (3.31)

The model error  itself is unknown of course. Let us therefore consider its least-squares
estimator ^ . Since it is a function of y, we do have a sample value of it available. On the
basis of this value we could also decide on the validity of Ho. If the sample value of ^ is
small, we are inclined to belief that the model error is absent and thus that Ho is true. On
the other hand, if the sample value of ^ turns out be significant, we will certainly not be
inclined to belief Ho and rather have more faith in Ha.

    From the above discussion, it seems that both e^ and ^ can be used for the testing of

Ho against Ha. One can therefore expect that the two estimators must be related in some
way. And this is indeed the case. To show this, we first solve for . The normal equations

that belong to Ha of (3.31), read

AT Q-1A           AT Q-1C               x^a               AT Q-y 1y
                                        ^ = CT Q-1y                                                       (3.32)
       y                 y
                                                                    y
CT Q-y 1A         CT Q-y 1C

where x^a is given the subindex a to indicate that it is the least-squares estimator of x under

Ha and not the least-squares estimator of x under Ho. From the above normal equations,
the reduced normal equations for ^ follow as

CT    -1  [Im  -  A(AT      Q-1A)-1  AT         -1  ]C  ^  =     CT      Q-y 1[Im  -  A(AT    -1  A)-1AT  Q-y 1]y

    Qy                        y               Qy                                            Qy

In this expression, we recognize

      e^       =  [  Im     -  A(AT  Q  -  1  A)-  1  AT  Q  -  1  ]  y
                                        y                    y
Q Q-1                                   -     A)-             -
               =  [  Im     -  A(AT  Q  y  1        1  AT  Q  y  1  ]
  e^ y

The least-squares estimator of the model error and its variance matrix follows therefore
as

        ^ = [CT Q-y 1Qe^Q-y 1C]-1CT Q-y 1e^ , Q^ = [CT Q-y 1Qe^Q-y 1C]-1                                  (3.33)
Thus ^ is indeed a function of e^. Under Ho and Ha, we have

Ho :      E{^ } = 0, D{^ } = Q ^               , Ha :                  E{^ } =  = 0, D{^ } = Q ^           (3.34)

          E{e^} = 0, D{e^} = Qe^                                         E{e^} = C = 0, D{e^} = Qe^
60                                                                                                       Network Quality Control

3.3.3 Significance of model error

Up to this point we spoke about ^ being small or large, with reference to the acceptance
or rejection of Ho. But when do we consider ^ small or large? Here we need an objective
way to measure the significance of ^ .

The one dimensional case: Let us first consider the one dimensional case. In that case,
q = 1 and ^ becomes a scalar instead of a vector. The significance of ^ can now be

measured by using the precision of the estimator, thus by using its standard deviation  ^ .

                                                                                                                                              

We therefore divide ^ by its standard deviation  ^ and define the random variable

                                                                                

       w = ^                                                                                                                           (3.35)
            ^

This random variable has a standard normal distribution under Ho. Thus under Ho, it has
a zero mean, with a variance of one. Under Ha however, it will have a nonzero mean, but
again with a variance of one. Thus

       Ho : w  N(0, 1) and Ha : w  N(  , 1)                                                                                            (3.36)
                                                   ^

Since the distribution of w is completely known under Ho, we are now in a position to test
the significance of the model error. The model error is said to be significant, if

       2 | w | > N 1  (0, 1)                                                                                                           (3.37)

where N 1 (0, 1) is the critical value of the standard normal distribution, based on the level

             2

of significance  (the test is two-sided). Note that instead of working with the absolute
value of w, one can also work with its square. In that case, the test reads

       w2  >      2  (1,      0)                                                                                                       (3.38)

                

where 2 (1, 0) is the critical value of the central Chi- squared distribution having one
degree of freedom.

The higher dimensional case: One can not use the above test when q > 1. In that case we
need to take the complete variance matrix Q ^ into account, to measure the significance

                                                                         

of ^ . Instead of using the one dimensional test statistic w2, we therefore use the q-
dimensional test statistic

       T q = ^ T Q-1 ^ ^                                                                                                               (3.39)

Note that T q = w2, when q = 1. In the higher dimensional case, the model error is said to
be significant, if

       Tq  >      2  (q,      0)                                                                                                       (3.40)

                

where  2   (q,  0)   i  s  t  he  cri  t  i  cal  val  ue  of  t  he  cent  ral  Chi  -  square  di  st  ri  but  i  on,  havi  ng  q  degrees

         

of freedom.
3. Testing and reliability                       61

Using the least-squares residual: We have seen that the least- squares estimator of the
model error, ^ , can be written as a function of the least-squares residual vector e^. This

implies that the above test statistic T q can also be expressed in terms of e^. If we substitute
(3.33) into (3.39), we get

T q = e^T Q-y 1C[CT Q-y 1Qe^Q-y 1C]-1CT Q-y 1e^  (3.41)

Although the two test statistics (3.39) and (3.41) are identical, the second expression is
often the more practical one, since the least-squares residual vector e^ is often already
available from the adjustment based on Ho.

3.4 Detection, identification and adaptation

In the previous section we gave the test statistic for testing the null hypothesis Ho against
a particular alternative hypothesis Ha. In most practical applications however, it is usually
not only one model error one is concerned about, but quite often many more than one. This
implies that one needs a testing procedure for handling the various alternative hypotheses.
In this subsection we will discuss a way of structuring such a testing procedure. It will
consist of the following three steps: detection, identification and adaptation.

3.4.1 Detection

Since one usually first wants to know whether one can have any confidence in the assumed
null hypothesis without the need to specify any particular alternative hypothesis, the first
step consists of a check on the overall validity of Ho. This implies that one opposes the
null hypothesis to the most relaxed alternative hypothesis possible. The most relaxed
alternative hypothesis is the one that leaves the observables completely free. Hence, un-
der this alternative hypothesis no restrictions at all are imposed on the observables. We
therefore have the situation

Ho : E{y} = Ax versus Ha : E{y}  Rm              (3.42)

Since E{y}  Rm implies that matrix (A,C) is square and invertible, it follows that matrix
C has q = m - n columns and that its range space is complementary to the range space of
A. Thus Rm = R(A)  R(C). It can be shown that in this case, the test statistic of (3.41)

simplifies to

T m-n = e^T Q-y 1e^                              (3.43)

The appropriate test statistic for testing the null hypothesis against the most relaxed alter-
native hypthesis, is thus equal to the weighted sum-of-squares of the least-squares resid-
uals. The null hypothesis will then be rejected when

Tm-n  >  2       (m  -  n,  0)                   (3.44)

           

The ^ 2 test: In the literature one often sees the above overall model test also formulated
in a slightly different way. Let us use the factorization Qy = 2Gy, where 2 is the
62                                        Network Quality Control

variance factor of unit weight and where Gy is the corresponding cofactor matrix. It can
be shown that

    ^ 2 = e^T G-y 1e^
            m-n

is an unbiased estimator of 2. Thus E{^ 2} = 2. The test (3.44) can now also be
formulated as

    ^ 2 > 2 (m - n, 0) = F (m - n, , 0)
    2  m-n

where F(m - n, , 0) is the central F-distribution having m - n and  degrees of freedom.

3.4.2 Identification

In the detection phase, one tests the overall validity of the null hypothesis. If this leads to
a rejection of the null hypothesis, one has to search for possible model misspecifications.
That is, one will then have try to identify the model error which caused the rejection of the
null hypothesis. This implies that one will have to specify through the matrix C, the type
of likely model errors. This specification of possible alternative hypotheses is application
dependent and is one of the more difficult tasks in hypothesis testing. It depends namely
very much on ones experience, which type of model errors one considers to be likely.

The 1-dimensional case: In case the model error can be represented by a scalar, q = 1
and matrix C reduces to a vector which will be denoted by the lowercase character c. This
implies that the alternative hypothesis takes the form

    Ha : E{y} = Ax + c                    (3.45)

The alternative hypothesis is specified, once the vector c is specified. The appropriate test
statistic for testing the null hypothesis against the above alternative hypothesis Ha follows
when the vector c is substituted for the C-matrix in (3.41). It gives

                        [cT Q-y 1e^]2
    T q=1 = w2 = T -1 -1

                     c Qy Qe^Qy c

or when the square-root is taken

    w = cT Q-y 1e^                        (3.46)
             cT Q-y 1Qe^Q-y 1c

This test statistic has a standard normal distribution N(0, 1) under Ho. The evidence on
whether the model error as specified by (3.45) did or did not occur, is based on the test

    2 | w | > N 1 1 (0, 1)                (3.47)

Data snooping: Apart from the possibility of having a one dimensional test as (3.47), it is
standard practice in geodesy to always first check the individual observations for potential
blunders. This implies that the alternative hypotheses take the form

    Hai : E{y} = Ax + ci i = 1, . . ., m  (3.48)
3. Testing and reliability                                              63

with

      ci = (0, . . ., 0, 1, 0, . . ., 0)T

Thus ci is a unit vector having the 1 as its ith entry. The additional term ci models the
presence of a blunder in the ith observation. The appropriate test statistic for testing the

null hypothesis against the above alternative hypothesis Hai is again of the general form
of (3.46), but now with the c-vector chosen as ci,

                        cT  Q  -  1  e^
                               y
      wi =               i                                              (3.49)

                    cT  Q-1  Qe^     Q-1  ci

                     i    y            y

This test statistic has of course also a standard normal distribution N(0, 1) under Ho. By
letting i run from 1 up to and including m, one can screen the whole data set on the
presence of potential blunders in the individual observations. The test statistic wi which
returns the in absolute value largest value, then pinpoints the observation which is most
likely corrupted with a blunder. Its significance is measured by comparing the value of
the test statistic with the critical value. Thus the jth observation is suspected to have a
blunder, when

      2 | w j |  | wi | i and | w j | > N 1 1 (0, 1)                    (3.50)

This procedure of screening each individual observation for the presence of a blunder, is
known as data snooping.

    In many applications in practice, the variance matrix Qy is diagonal. If that is the
case, the expression of the above test statistic simplifies considerably. With a diagonal
Qy-matrix, we have

      wi = e^i
            e^i

The appropriate test statistic is then thus equal to the least-squares residual of the ith
observation divided by the standard deviation of the residual.

The higher dimensional case: It may happen that a particular model error can not be
represented by a single scalar. In that case q > 1 and  becomes a vector. The appropriate
test statistic is then the one we met earlier, namely

      T  q  =  e^T  Q-y 1C[CT        Q-1Q Q-1C]-1CT  Q-y 1e^            (3.51)

                                       y e^ y

It is through the matrix C that one specifies the type of model error.

3.4.3 Adaptation

Once one or more likely model errors have been identified, a corrective action needs to be
undertaken in order to get the null hypothesis accepted. Here, one of the two following
approaches can be used in principle. Either one replaces the data or part of the data with
new data such that the null hypothesis does get accepted, or, one replaces the original
64                                                                         Network Quality Control

null hypothesis with a new hypothesis that does take the identified model errors into ac-
count. The first approach amounts to a remeasurement of (part of) the data. This approach
is feasible for instance, when in case of datasnooping some individual observations are
identified as being potentially corrupted by blunders. These are then the observations
which get remeasured. In the second approach no remeasurement is undertaken. Instead
the model of the null hypothesis is enlarged by adding additional parameters such that all
identified model errors are taken care off. Thus with this approach, the identified alterna-
tive hypothesis becomes the new null hypothesis.

    Once the adaptation step is completed, one of course still has to make sure whether
the newly created situation is acceptable or not. This at least implies a repetition of the
detection step. When adaptation is applied, one also has to be aware of the fact that since
the model may have changed, also the 'strength of the model' may have changed. In
fact, when the model is adapted through the addition of more explanatory parameters, the
model has become weaker in the sense that the test statistics will now have less detection
and identification power. That is, the reliability has become poorer. It depends on the
particular application at hand, whether this is considered acceptable or not.

3.5 Reliability

In the previous section we considered a testing procedure for the detection, identification
and adaptation of model errors. Hence, we now know how to search for potential model
errors and how to test their significance. But what we do not know yet, is how well these
tests will perform. In particular we would like to know how the tests perform in terms of
their power of detecting the model errors.

3.5.1 Power of tests

When applying the various tests, one should be aware of the fact that the outcomes of the
tests may be erroneous as well. This is due to the fact that the test statistics which are used
for testing, are random variables. They are random variables, which have a probability
density function that depends on which of the hypotheses is true. For our test statistic T q
we have

    Ho : T q  2(q, 0) and Ha : T q  2(q,  )                                (3.52)

with the noncentrality parameter

     =  e TCT Q-y 1Q ^Q-y 1C                   Q-1               Q-y 1C    (3.53)
                 T     T  Q-1            A(AT       A)-1  AT
    =               C          [  Qy  -          y            ]          
                            y

Thus T q has a central Chi-square distribution with q degrees of freedom, when Ho is true,

but a noncentral Chi-square distribution with q degrees of freedom and a noncentrality

parameter  , when the alternative hypothesis Ha is true.

    Recall from one of the earlier sections that, when testing, two type of errors can be

made. A type I error is made, when one decides to reject the null hypothesis, while in fact

it is true. The probability of such an error is given by the level of significance  and reads

                                                                           (3.54)

     = P[T q > 2 (q, 0) | Ho] =  p 2 (q,0) 2(x | q, 0)dx
3. Testing and reliability                                                                     65

where p2(x | q, 0) is the probability density function of the central Chi-square distribu-
tion, having q degrees of freedom.

    A type II error is made, when one decides to accept the null hypothesis, while in fact
it is false. The probability of such an error is given as  = P[T q  2 (q, 0) | Ha]. Instead
of working with  , one can also work with its complement  = 1 -  , which is known

as the power of the test. It is the probability of correctly rejecting the null hypothesis.

Hence, it is the probability

                                                                                               (3.55)

    = P[T q > 2 (q, 0) | Ha] =  p 2 (q,0) 2(x | q,  )dx

Note that the power  depends on the three parameters , q and  . Using a shorthand
notation, we write

    = (, q,  )                                                                                 (3.56)

When testing, we of course would like to have a sufficiently high probability of correctly

detecting a model error when it occurs. One can make  larger by increasing , or, by

decreasing q, or, by increasing  . This can be seen as follows. A larger  implies a

smaller  critical  value  2   (q,  0)  and  via the  integral  (3.55) thus a  larger  power .  Thus  if

                            

we want a smaller probability for the error of the first kind ( smaller), this will go at the

cost of a smaller  as well. That is, one can not simultaneously decrease  and increase

.

   The power  also gets larger, when q gets smaller. This is also understandable. When

q gets smaller, the less additional parameters are used in Ha and therefore the more "infor-

mation" is used in formulating Ha. For such an alternative hypothesis, one would expect

that if it is true, the probability of accepting it will be higher than for an alternative hy-

pothesis that contains more additional parameters. Finally, the power  also gets larger,

when  gets larger. This is understandable when one considers (3.53). For instance, one

would expect to have a higher probability of correctly rejecting the null hypothesis, when

the model error gets larger. And when  gets larger, also the noncentrality parameter 

gets larger.

   Using  and/or q as tuning parameters to increase , does not make much sense how-

ever. The parameter q can not be changed at will, since it depends on the type of model

error one is considering. And increasing , also does not make sense, since it would lead

to an increased probability of an error of the first kind. Hence, this leaves us with  .

According to (3.53), the noncentrality parameter depends on

   1. the model error C

   2. the variance matrix Qy

   3. The design matrix A

Since one also can not change the model error at will, it is through changes in the variance
matrix Qy and/or in the design matrix A that one can increase  , thereby improving the
detection power of the test. Recall that it is also through these two matrices that one can
improve the precision of the least-squares solution. Thus the detection power  of the tests
66                                                   Network Quality Control

can, if needed, be improved by using more precise measurement equipment, by adding
more obervations to the model and/or by changing the structure of the design matrix A.

3.5.2 Internal reliability

The power  is the probability with which a model error C can be found with the ap-
propiate test. Thus given the model error C, we can compute the noncentrality parameter
as

     = T CT Qy-1Qe^Qy-1C                             (3.57)

and from it, together with  and q, the power as

     = (, q,  )                                      (3.58)

We can however also follow the inverse route. That is, given the power , the level of
significance  and the dimension q, the noncentrality parameter can be computed as

     =  (, q, )                                      (3.59)

This function is the inverse of (3.58). If we combine (3.57) with (3.59), we get a quadratic
equation in 

      (  ,  q,    )  =   T  CT    Q-1Q Q-1C          (3.60)

                                    y e^ y

This equation is said to describe the internal reliability of the null hypothesis E{y} = Ax
with respect to the alternative hypothesis E{y} = Ax + C. Each model error C that
satisfies the quadratic equation, can be found with a probability . Since it is often more
practical to know the size of the model error that can be found with a certain probability,
than knowing the probability with which a certain model error can be found, we will use
the inverse of (3.60).

Minimal Detectable Biases (MDB's): In the one dimensional case q = 1 and the matrix
C reduces to the vector c. Inverting (3.60) becomes then rather straightforward. As a
result we get for the size of the model error

    ||=                   (1, 1, )                   (3.61)

                     cT  Q  -  1  Q  e^  Q  -  1  c
                            y               y

The variate |  | is known as the Minimal Detectable Bias (MDB). It is the size of the
model error that can just be detected with a probability , using the appropriate w-test
statistic. Larger errors will be detected with a larger probability and smaller errors with a

smaller probability.
    In order to guarantee that the model error c is detected with the same probability

by both the w-test and the overall model test, one will have to relate the critical values of

the two tests. This can be done by equalizing both the powers of the two tests and their
noncentrality parameters. Thus if  (m-n, m - n, m-n) is the noncentrality parameter of
the T m-n-test statistic and  (1, 1, 1) the noncentrality parameter of the T 1-test statistic,
we have

     (m-n, m - n, ) =  (1, 1, )                      (3.62)
3. Testing and reliability                                        67

From this relation, m-n and thus the critical value of the overall model test, can be
computed, once the power  and the level of significance 1 is chosen. Common values
in case of geodetic networks, are 1 = 0.001 and  = 0.80.

Data snooping: Note that the MDB can be computed for each alternative hypothesis,
once the vector c of that particular alternative hypothesis has been specified. In case of
data snooping, the MDB's of the individual observations are computed as

| i | =            (1, 1, )           i = 1, . . ., m             (3.63)
                cTi Q-y 1Qe^Q-y 1ci

As with the wi-test statistic, also this expression simplifies considerably when the variance
matrix Qy is diagonal. In that case we have

| i |= yi           (1, 1, )                                      (3.64)
                   1 - y2^i /y2i

where y2i is the a priori variance of the ith observation and y2^i is the a posteriori variance
of this observation. We thus clearly see that a better precision of the observation as well
as a larger amount in which its precision gets improved by the adjustment, will improve
the internal reliability, that is, will result in a smaller MDB.

The higher dimensional case: When q > 1, the inversion is a bit more involved. In this
case  is a vector, which implies that its direction needs to be taken into account as well.
We use the factorization  =  d, where d is a unit vector (dT d = 1). If we substitute

the factorization into (3.60) and then invert the result, we get

   =                  (q, q, )                 (d = unit vector)  (3.65)
                dT CT Q-y 1Qe^Q-y 1Cd

The size of the model error now depends on the chosen direction vector d. But by letting
d vary over the unit sphere in Rq, one can obtain the whole range of MDB's that can be
detected with a probability .

3.5.3 External reliability

With the MDB's one describes the internal reliability. In practical applications however,
one often not only wants to now the size of the model errors that can be detected with
a certain probability, but also what their impact would be on the estimated parameters.
After all, it are the estimated parameters, or functions thereof, which constitute the final
result of an adjustment. The external reliability describes the influence of model errors of
the size of the MDB's, on the final result of the adjustment.

    For the expectation of the least-squares estimator x^ under the alternative hypothesis
Ha, we have

E  {x^  |  Ha}  =  (AT  Q-1A)-1   AT    -1  E  {y  |  Ha  }

                          y           Qy
68                                                                             Network Quality Control

Substitution of E{y | Ha} = Ax +C, gives

        E{x^ | Ha} = x + (AT Qy-1A)-1AT Q-y 1C

This shows that the bias in x^, due to the presence of the model error C, is                            (3.66)
        x^ = (AT Qy-1A)-1AT Q-y 1C

Of course we do not know the actual model error C. But we do know the size of the
model error that can be detected with a probability of . Hence, if we use the MDB and
replace C by c |  | in case q = 1 or by Cd  in case q > 1, the vector x^ will show
us by how much the least-squares estimator x^ gets biased, when a model error of the size

of the MDB would occur. Note that there are as many MDB's as there are alternative
hypotheses. Hence, there are also as many vectors x^. This whole set is said to describe
the external reliability.

    In certain applications, one may not be interested in the whole parameter vector x,
but only in particular functions of it, say  = FT x. In that case the external reliability is
described by

        ^ = FT (AT Qy-1A)-1AT Q-y 1C                                                                    (3.67)

A particular case occurs, when one is only interested in the x1- part of the parameter vector
x = (xT1 , xT2 )T . In that case FT = (I, 0). The bias-vector x^1 can then be shown to equal

        x^1  =    (A»T1  Q-1A» )-1A»T1  Q-y 1C                                                          (3.68)
                         y1

where,  with  the    partitioning   A   =  (AT1  , AT2 )T ,  the  matrix  A»T  Q-1A»  is  the  reduced  normal

             A»1            A2(AT2  Q-1A )-1AT      Q-y 1]A1.               1    y1

matrix  and       =  [I  -          y2           2

Bias-to-Noise Ratios (BNR's): In order to measure the significance of the external relia-

bility, one can compare the bias-vectors x^ with the precision, or variance matrix Qx^, of
x^. This can be done by using the Bias-to-Noise Ratios (BNR)

        x^ = x^T Q-1 x^ x^                 with  Qx^  =      (AT  Q-1A)-1

         ^ = ^ T Q-1^                                               y

                             ^             with Q^ = FT Qx^F                                            (3.69)

        x^ = x^T1 Q-x^ 1x^11 1             with Qx^ 1  = (A»T Q-1A» )-1

                                                               1y 1

Note that the BNR's are dimensionless and that they measure the squared lengths of the
bias-vectors in the metric defined by the appropriate variance matrix. Also note that the
BNR's are scalars. Thus x^ is a scalar, whereas x^ is a vector. This implies that with the
BNR's, one only needs to evaluate a scalar per alternative hypothesis, whereas otherwise
a complete vector would have to be evaluated for each alternative hypothesis.

    From a computational point of view, there are also some shortcuts that can be used
when computing the BNR's. For instance, when the complete vector x^ is considered, it
can be shown that

        x^ = x^T Q-1 x^ x^ = TCT Q-y 1C -  (q, q, )                                                     (3.70)
3. Testing and reliability                                                                                   69

The second expression on the right hand side may sometimes be easier to compute than
the first expression on the right hand side, in particular when Qy is diagonal. For the BNR
of a subset of the parameter vector, one can show that

     x^       =  x^T1    Q-1 x^ x^1  =  x^  -   TCT  Q-y 1A2(AT2   Q-1A )-1AT     2  Q-y 1C        (3.71)
           1             1                                         y2

     An important feature of the BNR's is that they can be used to formulate upperbounds

on the external reliability of functions of the parameters. For instance, if we consider the

function   ^  =  f T x^  having    the  variance  2      and  the  bias   ^  =  f T x^,  then

                                                    ^

     ^  ^ x^                                                                                       (3.72)

This shows, that the potential bias in ^ due to an undetected model error of the size of the
MDB, will never be larger than the standard deviation times the square root of the BNR.

3.5.4 Connection of two height systems: an example

Let us assume that we need to connect two levelling networks of which the heights are

defined in two different height-systems (height-datums). The height of point i in the first

system is denoted as hi and the height of the same point in the second system as Hi. As

observables we have available the heights hi and Hi of the same n points in both height-
systems, i = 1, . . ., n.

     We assume that all heights are uncorrelated, that all heights of the first system have

the  same  variance       2h  and    that  all  heights  of  the  second  system  have  the  same  variance  2

                                                                                                               H

(note: these assumptions are of course not really realistic, but simply serve to make our

example analytically tractable). If we assume that the two height systems differ in scale

and have a constant offset, then the two different heights of point i are related as

     hi =  Hi + t

with the scale factor  and the translation t.

     Based on the above assumptions, the model of observation equations for connecting

the two networks, becomes

                                      H1 + t    
                 h1
           .  . 
            ..   .. 
                                                
                                                
            hn    Hn + t                                     Qh QhH = h2In 02
     E{ } =                                     ,                                                  (3.73)
            H1   H1                                          QHh QH                  0 H In
                                                
            ..   .. 
           .  . 

                 Hn                     Hn

Note that the observation equations are nonlinear due to the presence of the scale factor
 . We will now consider four different cases. First we will assume that the difference
in scale and the constant offset are absent. Then we will assume that there is only a
constant offset, followed by the case that there is only a difference in scale. Finally, we
will assume that there is both a constant offset and a difference in scale. For all four cases
we will discuss the MDB's that correspond to the use of data snooping.
70                                                                                      Network Quality Control

    Data snooping applied to the above connection model, implies that we are testing for
errors in the individual height coordinates. It will be clear that with the above model, one
will never be able to discriminate whether a blunder occurred in the h-coordinate or in the
H- coordinate of a point i. That is, one will never be able to pinpoint a blunder in a height
coordinate to one of the two height systems. Hence, it suffices to restrict our attention to
the wi-test statistics of one of the two sets of heights. We choose to restrict our attention to
the h-coordinates. Since the complete variance matrix of the observables is diagonal, we
can make use of the simplified expression (3.64) for the MDB's. For the hi coordinates, it
reads

    | i |= h               17.075                                                                            (3.74)
                         1 - 2 /2

                                 h^i hi

Note that we assumed  (1, 1, ) = 17.075. This value is based on the values 1 = 0.001
and  = 0.80.

Scale and translation absent: When both scale and translation are absent, the model of

observation equations becomes linear and the least- squares solution of (3.73) amounts to

taking a simple weighted average of the data. The variance of the least-squares solution
h^ i reads then

       2           22
                   hH
    h^i =  2 +  2                                                                                            (3.75)

                   hH

Substitution into (3.74) gives for the MDB

    | i |=         1  7  .  0  7  5  ( 2h  +  2    )                                                         (3.76)

                                                H

This shows that the MDB will be about 5.8 times the standard deviation of the height

coordinate, if the two networks are equally precise. Hence, a blunder of this size in the hi
coordinate can be found with a probability of 80% with the wi-test.

Only scale is absent: In this case the model of observation equations is again linear. Note

however that the redundancy has decreased by one. In the previous case the redundancy
equalled n, whereas now, due to the additional unknown translation, it equals n - 1. Due
to the decrease in redundancy, the model will have less 'strenght' and one can therefore

expect that the results of the adjustment will also be somewhat less precise. And indeed,
the variance of the least-squares solution h^ i reads now

                   22                2 1
    2     =        h H (1 + h )                                                                              (3.77)
       hi  2 +  2                    2 n
      ^

                   hH                      H

Note that (3.75) and (3.77) will differ less, the larger n is. Hence, the difference between

the two variances will become smaller when the number of points in the two overlapping

networks  increases.        The      difference       will  also  be  small  when  2    >>  h2 ,  that  is,  when  the

                                                                                     H

second network is considerably less precise than the first network. But in that case we also

would have  2        h2,       thus  showing that           no  significant  improvement  in  precision      has  taken

              h^i
3. Testing and reliability                                                                71

place. This is of course understandable, because if the second network is considerably less
precise than the first network, it will also not contribute much to the adjustment.

    Substitution of (3.77) into (3.74) gives for the MDB

| i |=  1  7  .  0  7  5  ( 2h    +  2    )                           (3.78)

                                       H

                         1 - 1n

Note that the MDB goes to infinity when n = 1. This is due to the fact that there is no
redundancy when n = 1. In that case no model errors at all can be found by the statistical
tests and the solution is said to be infinitely unreliable. Also note that the above MDB is
larger than the one of (3.76). This is due to the decrease in redundancy of one. That is,
the outcomes of the statistical tests will now be somewhat less reliable than they were in
the previous case. But again, this difference can be made small by increasing the number
of points n.

Only translation is absent: When only the translation is absent, we again have a model

with a redundancy of n - 1. Note however, that the observation equations are now non-

linear. Hence, first a linearization needs to be carried out. The linearized model reads

                              o                    Ho        
    .h1 
                                                     1
                                                           .       
    ..                               .    ..             ..    H1
                                                             
                                                    o Hno     .  .. 
    hn  
E{           } =                                                      (3.79)
    H 1   1                                              0     Hn  
                                                             
    ..                               ..                  .. 
   .                                         .           .     

     H n                                           10

where  o is the approximate scale factor and Hio, i = 1, . . ., n, are the approximate heights
of the second network. Since geodetic networks often only differ slightly in scale, one
may take as approximate value  o = 1. We will do so here also.

    Note that the design matrix of the above model depends on the approximate heights
Hio. Hence, also the precision of the least- squares estimators will depend on them. The
variance of the least-squares solution h^ i reads now

        22                     2 (Ho)2
2       hH                        h             i
h^ = 2 2 (1 + 2 n                                  o 2)               (3.80)
i h + H                       H  j=1(H j )

Compare this result with that of (3.77). Substitution of (3.80) into (3.74) gives for the
MDB

| i |=    1  7  .  0  7  5  ( 2h  +  2    )                           (3.81)

                                       H

                       (H o )2

             1- n i o 2

                    j=1(H j )

Compare this result with that of (3.78). In the previous case, the MDB was, apart from the
a priori precision, only dependent on the number of points n. In the present case however,
the MDB has also become dependent on the heights of the points themselves. Thus in the
above expression for the MDB, we see four effects at work:
72                                                                                        Network Quality Control

     1.  A  priori   precision:           2h   +   2

                                                     H

     2. Number of points: nj=1(.)2

     3.  Height   of   the   point       tested:       Ho

                                                         i

     4.  Height   distribution           of    network:       n (Ho)2

                                                                 j=1 j

As before the MDB gets smaller when the a priori precision improves and/or when the
number of points increases. But now the MDB also gets smaller when the height of the
point being tested is closer to zero and/or when the heights of the remaining points are
further away from zero.

Both scale and translation present: As in the previous case, the observation equations

are again nonlinear. The linearized model reads

                  h1                     o                        1      Ho   

               .                                                           1      H1    
               .                                                  ..     ..   
               .                                       ..  .        . .   .. 

                                                              o          Ho       .     
                hn                                                1                     
         E{               } =                                              n    Hn                                  (3.82)

                H 1   1                                           0 0                   
                                                                                  t     
                ..                                     ..         .      .    
               .                                           .      .. .. 
                                                                                  
                  H n                                         100

Due to the additional unknown, the translation t, the redundancy now equals n - 2. Thus

again we can expect a somewhat less precise and less reliable result. The variance of the
least-squares solution h^ i reads now

                     22                  2         1          (H» io )2
            2          hH
                                           h
         h^ = 2 2 [1 + 2 ( + n » o 2 )]                                                                             (3.83)
            i h + H H n  j=1(H j )

where    H» o  =  Ho   -  1     nj=1 Hoj ,         i.e.     the difference in height of point i with respect        to the
                          n
           i        i

average height of the network. Compare this result with that of (3.80). Substitution of

(3.83) into (3.74) gives for the MDB

         | i |=        1  7  .  0  7  5  ( 2h  +   2     )                                                          (3.84)

                                                     H

                       1 - n - n1            (H» o)2
                                               i

                                               (H» o)2
                                         j=1 j

Compare this result with that of (3.81). We see almost the same four effects present in the

expression for the MDB. The only difference is that the heights are now referenced to the

average height of the network, instead of to zero, as it was the case when the translation

was absent. Thus the point having the smallest MDB is the one which height is closest to

the  average     of  the  network.             If  it  coincides  with   the  average,  then  H» o  =  0  and  the  above

                                                                                                i

expression reduces to that of (3.78).
3. Testing and reliability                                                                         73

3.6 Quality control: precision and reliability

We are now in a position to summarize the two main diagnostics by which the quality
of estimation and testing can be characterized. They are precision and reliability. In
order to discuss them properly, we follow the general steps involved when performing the
adjustment and the testing.

Formulate Ho and Ha: In order to be able to perform the adjustment, one first must have
a working hypothesis, the null hypothesis Ho, available. It reads

      Ho : E{y} = Ax , D{y} = Qy                                                              (3.85)

This is the model one beliefs to be true and on which one would like to base the ad-
justment. But of course, relying on this model without checking its validity would be
dangerous, since errors in it could completely ruin the results of the adjustment. The
purpose of testing is therefore to check whether the null hypothesis is likely to be true
or not. This is done by opposing the above model to one or more alternative hypotheses
Ha. In these lecture notes we have restricted ourselves to alternative hypotheses, which
differ from the null hypothesis in their functional model only. The alternative hypotheses
considered are therefore of the form

      Ha : E{y} = Ax +C , D{y} = Qy                                                           (3.86)

The two type of hypotheses thus differ in their mean of y only, E{y | Ha} = E{y | Ho} +
C. The vector y = C, with matrix C known and vector  unknown, describes the
assumed model error.

The quality under Ho and Ha: Based on our working hypothesis Ho, the least-squares
estimator of x reads

      x^ = (AT Q-1 y A)-1AT Q-1 y y                                                           (3.87)

It will be clear that the quality of x^, as expressed by its expectation and its dispersion,
depends on whether Ho is true or Ha is true. In the first case we have

      Ho true     mean : E{x^} = x                                                            (3.88)
                  variance : D{x^} = Qx^

with Qx^ = (AT Q-y 1A)-1. In the second case however, we have

      Ha true     mean : E{x^} = x + x^                                                       (3.89)
                  variance : D{x^} = Qx^

with  x^  =  (AT  Q-1 A)-1 AT  Q-y 1C.  This  shows  that  the  least-squares  estimator  x^  has  the

                    y

same variance matrix under Ha as it has under Ho. Thus the precision of the estimator is

not affected by the model error C. Its expectation or its mean however, does get affected.

Under Ho it is an unbiased estimator, but under Ha it becomes biased, due to the presence

of the model error C.
74                                                      Network Quality Control

    Since one can never be completely sure whether the null hypothesis is true or whether
one of the alternative hypotheses is true, the quality of the estimator is made up of the two
components x^ and Qx^. The variance matrix, which describes the precision, is known and
in the last section of the previous chapter we discussed how one could evaluate it. The
bias x^ however, is unknown, since it depends on the unknown model error C.

Testing: In order to minimize the risk that a bias like x^ indeed occurs, one needs to
check the validity of the null hypothesis. This is the whole purpose of testing. Although
there is only one null hypothesis, there are in practice often many more than one alter-
native hypotheses. The class of alternative hypotheses considered depends very much on
the application at hand and on ones experience. Through the alternative hypotheses one
specifies the model errors that one beliefs are likely to occur. The testing procedure to
be applied consists then of detection, identification and adaptation. In the detection step
the overall validity of the null hypothesis is checked. Ones this test leads to a rejection
of Ho, one needs to identify the most likely alternative hypothesis. In the final step, the
adaptation step, one then corrects for the identified misspecification in the null hypothesis.

Reliability: Although the statistical testing of Ho minimizes the risk that a bias like x^
occurs, one should realize that the outcomes of the statistical tests are not exact and thus
still prone to errors (type I and type II error). It depends on the 'strength' of the model,
how much confidence one will have in these outcomes. A measure of this confidence is
provided for by the concept of reliability. When the w- test statistics are used, the internal
reliability is described by the set of MDB's, one for each alternative hypothesis. The
MDB is given as

    ||=              (1, 1, )                           (3.90)

                cT  Q  -  1  Q  e^  Q  -  1  c
                       y               y

It is the size of the model error that can be found with a probability , when using the
w-test. The internal reliability improves when the MDB's get smaller and gets worse
when the MDB's get larger. Note however, that it only makes sense to consider these
MDB's when the statistical tests are actually carried out. Hence, the solution is said to be
infinitely unreliable by definition, if no statistical testing has taken place. Also note, that
the MDB, apart from being dependent on the chosen values for 1 and , is governed by
the c- vector, the design matrix A and the variance matrix Qy. The vector c can not be
changed at will, since it depends on the particular model error one is considering. Hence,
this leaves us, much like in the case of precision, with the two matrices A and Qy for
improving the internal reliability.

    The external reliability describes how model errors of the size of the MDB's propagate
into the results of the adjustment,

    x^  =  (AT    -1  A)-1AT           Q-1c     |    |  (3.91)

                Qy                       y

This vector thus describes the bias in x^, when a model error of the size of the MDB has
occurred.
3. Testing and reliability  75

Precision and reliability: Once one has evaluated the precision of the least-squares so-
lution and found it adequate enough for the application at hand, one can evaluate the
significance of the bias vector x^, through the Bias-to-Noise Ratio (BNR)

x^ = x^T Q-1 x^ x^          (3.92)

The dimensionless BNR measures the bias relative to the precision. For an arbitrary
function ^ = f T x^, the BNR can be used to obtain the upperbound

^  x^                       (3.93)
^

Thus if for the particular application at hand, the BNR is considered to be small enough,
the bias in any function of x^ is guaranteed to be sufficiently insignificant as well.
Chapter 4

Adjustment and validation of networks

4.1 Introduction

In this chapter we will discuss the various computational steps involved for determining
the geometry of a geodetic network. Let us first briefly review the general steps involved.
They are the design, the adjustment and the testing.

Design: Before one can start, one needs to specify the functional model and the stochastic
model. In the functional model, one formulates the assumed relation between the observ-
ables and the unknown parameters. These observation equations depend on the type of
observables used (e.g. angles, distances, baselines, etc.) and on the choice of parameteri-
zation (e.g. Cartesian coordinates or geographic coordinates). The observation equations
may be linear or nonlinear. In the nonlinear case, they first need to be linearized. For the
linearization, approximate values for the parameters are needed. When the approximate
geometry of the network is known, it is often possible to obtain the approximate coordi-
nates from a map. The approximate coordinates can also be computed from a sufficient
set of observations. The design matrix A is known, once the functional model is specified.

    With the stochastic model, one specifies the assumed distributional properties of the
observables. In geodesy it often suffices to assume the observables to be normally dis-
tributed. In addition one has to specify the second moment, the variance matrix, of the
distribution. The variance matrix of the observables describes their precision. The spec-
ification of this variance matrix Qy depends on the measurement equipment and on the
measurement procedures used.

    The two matrices A and Qy are known, once the functional and stochastic model are
known. These two matrices can be used before the actual adjustment and testing is carried
out, to infer the expected quality in terms of precision and reliability, of the network. In
order to evaluate the precision of the least-squares solution x^, its variance matrix Qx^ is
used. This matrix quantifies how random errors in the observables propagate into the
least-squares solution. In order to evaluate the reliability of the least-squares solution, the
minimal detectable bias vector x^ is used. It quantifies how a potential error c in the
functional model propagates into the least-squares solution. The size of the potential error
is coupled to the power of the corresponding test statistic. Both Qx^ and x^ depend on A
and Qy. Hence, one can improve the precision and reliability, by changing A and/or Qy.

Adjustment: Once one is satisfied with the design of the network, the actual adjustment
can be carried out. It needs, apart from the design matrix A and the variance matrix Qy,
of course also the actual observations y. The adjustment is based on the principle of least-
squares and it produces a solution for the unknown parameters. This solution is obtained
by solving the normal equations, for which usually the Cholesky-decomposition is used.
In case of nonlinear observation equations, the least-squares solution is usually iterated

                                                    77
78  Network Quality Control

using the normal equations based on the linearized observation equations. The number of
iterations will be small, when good approximate values are used.

    On the basis of the assumption that the model (functional and stochastic) has been
specified correctly, the linear(ized) least-squares estimators are known to be unbiased and
of minimal variance. These properties will fail to hold however, when a misspecified
model has been used in the computations. Hence, before the least-squares solution x^ can
be accepted, one has to test the validity of the model.

Testing: The validity of the model (the null hypothesis) is tested by opposing it to vari-
ous alternative models (the alternative hypotheses). For each alternative hypothesis, one
has an appropriate test statistic. But all test statistics are functions of the least- squares
residual vector e^. The testing procedure consists of the following three steps: detection,
identification and adaptation. In the detection step, the null hypothesis is opposed to the
most relaxed alternative hypothesis. The purpose of the detection step is to infer whether
one has any reason to belief that the null hypothesis is indeed wrong. When the detection
step leads to a rejection of the null hypothesis, the next step is the identification of the
most likely model error. For identification one needs to specify the alternative hypothesis.
This choice depends on the type of model error one expects to be present. Hence, it very
much depends on the application at hand. It is standard practice however, to have data
snooping included in the identification step. In case of data snooping each of the individ-
ual observations is screened for potential blunders. Once certain model errors have been
identified as sufficiently likely, the last step consists of an adaptation of the data and/or
model. Depending on the situation, two approaches are possible in principle. One can
either decide to remeasure some of the observables, or, one can decide to include addi-
tional parameters in the model, such that the model errors are accounted for. The first
approach is possible in case the model errors are due to clear measurement errors, e.g.
blunders in individual observations. The second approach can be used for more compli-
cated situations. In this case the identified alternative hypothesis will become the new null
hypothesis. One should be aware however, that in this case, due to the change in model,
the precision and reliability of the solution will change as well.

The above considerations for the design, the adjustment and the testing, are valid for
any geodetic project where measurements are used to determine unknown parameters.
When computing geodetic networks however, some additional aspects need to be consid-
ered as well. The construction of a geodetic network implies that the geometry of the
configuration of a set of points is determined. The set of points usually consists of: (1)
newly established points, of which the coordinates still need to be determined, and (2)
already existing points, the so-called control points, of which the coordinates are known.
By means of a network adjustment the relative geometry of the new points is determined
and integrated into the geometry of the existing control points. The determination of the
geometry is usually divided into two parts, the so-called free network adjustment and the
connection adjustment.

The free network: In the free network adjustment, the known coordinates of the control
4. Adjustment and validation of networks  79

points do not take part in the determination of the geometry of the point field. This
adjustment step is thus free from the influence of the existing control points. The idea is
that a good geodetic network should be sufficiently precise and reliable in itself, without
the need of external control.

    Since the coordinates of the control points do not take part in the adjustment, one
is confronted with the fundamental non-uniqueness in the relation between geodetic ob-
servables and coordinates. In a levelling network for instance, absolute heights can not
be determined if only height differences are measured. That is, for computing heights,
additional information is needed on the absolute height of the network. Similarly, one
can not obtain the position, the orientation and the scale of a triangulation network if only
angles are measured. The additional information which is needed to be able to compute
coordinates from the geodetic observables, is provided for in the form of so-called min-
imal constraints. These minimal constraints are not unique. There is a whole set from
which the constraints can be chosen. It is important though, that the constraints are mini-
mal. That is, they should not only be necessary to eliminate the lack of information in the
observables, but they should also be sufficient.

    After the design of the free network, its coordinates are computed by means of a
least-squares adjustment and its validity is checked by means of the statistical testing
of the observations. The coordinates depend on the chosen minimal constraints, but the
statistical tests do not.

The connected network: Once the geometry of the free network has been determined
to ones satisfaction, it needs to be integrated into the existing geometry of the control
points. The data used for this connection, are the results of the free network adjustment
together with the coordinates of the control points. One can descriminate between the so-
called constrained connection and the unconstrained connection. In most applications,
it is not very practical to see the coordinates of the control points change everytime a
free network is connected to them. This would happen however, when an ordinary least-
squares adjustment is carried out. In that case all observations, including the coordinates
of the control points, would get corrections due to the least-squares adjustment. In order to
circumvent this, no ordinary adjustment, but a constrained adjustment is carried out. This
implies that the connection is carried out, with the explicit constraints that the coordinates
of the existing control points remain fixed.

    For the statistical testing of the observations however, a constrained adjustment would
not be realistic. Although practice dictates that the coordinates of the control points re-
main fixed, these coordinates are of course still samples from random variables. Hence,
for the statistical testing of the observations, the variance matrix of the control points
should not be set to zero, but should be included in the adjustment as well. Thus, the
constrained connection is carried out for the final computation of the coordinates, but the
unconstrained connection is used for the statistical testing of the control points.

This chapter is organized as follows. First we will discuss the free network case and then
we will show how they can be connected to existing control. For the free networks, we
present the observation equations, discuss their invariance and show how one can choose
80                      Network Quality Control

appropriate sets of minimal constraints. As an example we discuss the adjustment of a
small free GPS network and the testing of a small free levelling network.

4.2 The free network

In this section we consider networks for which the observational data are insufficient
to determine either the (horizontal and/or vertical) position, orientation or scale of the
network (note: here and in the remaining part of the lecture notes, we will disregard so
called configuration defects. That is, we assume that sufficient observational data are used
to determine the configuration of the network).

    As we have seen in the earlier sections on adjustment theory, a consequence of the
non-uniqueness is that the design matrix A of the model of observation equations E{y} =
Ax, D{y} = Qy, will have a rank defect. Therefore no unbiased linear estimator x^ = Ly
of x exists, since this would require E{x^} = LE{y} = Ax for all x, or AL = I, which is
impossible since the rank of a product of two matrices can not exceed the rank of either
factor. But although x is not unbiased estimable, we have seen that functions of x exist
that are unbiased estimable. In particular we have shown that the minimally constrained
solution x^b is an unbiased estimator of Sbx, with Sb being the S-transformation defined by
the null space of the design matrix, N(A) = R(G), and the minimal constraints BT x = 0.

    In this section we will show how minimally constrained solutions for geodetic net-
works can be constructed. We start off by presenting the nonlinear observation equations
of some geodetic observables and their linearized versions. Then we discuss the invari-
ance properties of these geodetic observables. Their invariance can usually be related to
properties of coordinate transformations. Once these properties of invariance are under-
stood, one will be able to identify the null space of the design matrix A. As a consequence
the matrix G, of which the columns span N(A), can be constructed. Understanding the in-
variance, also allows one to specify the minimal constraints BT x = 0. They are needed to
be able to compute a particular least-squares solution of the rank defect model E{y} = Ax,
D{y} = Qy. Such a solution is one of the many free network solutions that can be com-
puted. By means of the S-transformation, one is able to transform one particular free
network solution into another. This section will be concluded with an adjustment exam-
ple of a free GPS network and a testing example of a free levelling network.

4.2.1 Some typical observation equations

In this section we will present some typical examples of geodetic observation equations.
Also their linearized versions are given. We will parametrize the observation equations in
terms of Cartesian coordinates (in one dimension the height h; in two dimensions: x and
y; and in three dimensions: x, y and z).

Height difference: Probably the simplest of all geodetic observation equations is the one
that corresponds with observed height differences. Let hi j denote the height difference be-
tween two points i and j, and let the heights of these two points be denoted as respectively

hi and h j. The observation equation for an observed height difference reads then

    E{hi j} = h j - hi  (4.1)
4. Adjustment and validation of networks                                               81

Since this equation is already linear, no linearization is needed.

Azimuth: If we consider a two dimensional network and assume that all network points
are located in the two-dimensional Euclidean plane, the nonlinear observation equation
for an azimuth ai j between the two points i and j reads

      E{ai j} = arctan x j - xi                                                        (4.2)
                          y j - yi

Linearization gives

                     yo - yo        yo - yo              xo - xo   xo - xo

                       ji             ji                   ji        ji
      E{ai j} = o 2 x j - o 2 xi - o 2 y j + o 2 yi                                    (4.3)
                     (li j)                      (li j)  (li j)    (li j)

with
                                xo - xo

                                             ji

        ai j = ai j - arctan o o
                                y j - yi

and

      x j = x j - xoj , xi = xi - xoi , y j = y j - yoj , yi = yi - yoi

and where (.)o denotes the approximate value.

Distance: For a planar network, the nonlinear observation equation for a distance reads

      E{li j} = (x j - xi)2 + (y j - yi)2                                              (4.4)

Linearization gives

      xo - xo                       xo - xo              yo - yo   yo - yo

        ji                            ji                   ji        ji
      E{li j} = o x j - o xi + o y j - o yi                                            (4.5)
                     (li j)                      (li j)  (li j)    (li j)

with

      li j = li j -  (xo - xo)2 + (yo - yo)2
                     ji                          ji

Angle: An angle i jk between three points i, j and k, is the difference between their
azimuths a jk and a ji. Thus we have

      E{i jk} = E{a jk - a ji} = arctan xk - x j - arctan xi - x j                     (4.6)
                                                         yk - y j  yi - y j

Its linearized version follows then from the linearized versions of the two azimuths.

Distance ratio: The distance ratio vi jk between three points i, j and k, is the ratio of their
distances l jk and l ji. Thus we have

      E{vi jk} = E{ l jk } =        (xk - x j)2 + (yk - y j)2                          (4.7)
                       l ji         (xi - x j)2 + (yi - y j)2
82                                                      Network Quality Control

Linearization gives

    E{vi jk} =       l  o   l  j  k  -  l  o   l  j  i
                        ji                 jk

                                     o2
                               (l ji)

This can be further expressed in terms of the coordinate increments by making use of
(4.5).

Direction: A direction ri j is an "azimuth with an unknown orientation". Its observation
equation reads thus

    E{ri j} = ai j + o j                                                                    (4.8)

where o j is the orientation unknown. Apart form the additional orientation unknown, the
linearized observation equation for a direction is the same as that for an azimuth. Note

that the difference of two directions having the same orientation, produces an angle.

Pseudo-distance: A pseudo-distance si j (not to be confused with the pseudo-distance of
GPS) is a "distance with an unknown scale". Its observation equation reads therefore

    E{si j} = ili j                                                                         (4.9)

where i is the scale unknown. Linearization gives
        E{si j} = ioli j + lioji

This can be further expressed in terms of the coordinate increments by making use of
(4.5). Note that the ratio of two pseudo-distances having the same scale factor, produces
a distance ratio.

The three dimensional case: So far we assumed all network points to lie in the two di-
mensional Euclidean plane. For a three dimensional network though, we have to take the
third dimension into account as well. For a network of a sufficiently large extent, also the
change in the direction of the plumbline will have to be taken into account. This implies
that for direction measurements like the azimuth ai j and the zenith angle zi j, the astro-
nomical latitude i and astronomical longitude i will enter the observation equations as
well. The nonlinear observation equations for respectively the azimuth, the zenith angle
and the distance between two points i and j, are given as

          E{ai j} = arctan - sin i cosi(x j-xi)-sini sin i(y j-yi)+cosi(z j-zi) - sini(x j-xi)+cosi(y j-yi)

    E{zi j} = arccos l cosi cosi(x j-xi)+cosi sini(y j-yi)+sini(z j-zi)                     (4.10)

                                                                                        ij

          E{li j} = (x j - xi)2 + (y j - yi)2 + (z j - zi)2

The unknowns in these observation equations are now apart from the Cartesian coordi-
nates, also the astronomical latitude and longitude. The linearization of the above obser-
vation equations is left as an exercise to the reader.
4. Adjustment and validation of networks                  83

GPS baseline: As our last example we consider the three dimensional baseline. When
expressed in Cartesian coordinates, the observation equations for its three components
read

E{xi j} = x j - xi                                        (4.11)
E{yi j} = y j - yi
E{zi j} = z j - zi

As with the height differences no linearization is needed, since the equations are already
linear in the parameters.

Instead of using Cartesian coordinates, one may of course use other type of coordinates
as well. In three dimensions, one often also makes use of the geographic coordinates  ,
 and h, where h now refers to the height above the reference ellipsoid. The Cartesian
coordinates and geographic coordinates are related as

x = (N + h) cos cos                                       (4.12)
y = (N + h) cos sin 
z = (N(1 - e2) + h) sin 

where N is the radius of curvature in the prime vertical

        N= a
                 1 - e2 sin2 

and e2 = (a2 - b2)/a2, with a and b being the lenghts of the major and minor axes of the
ellipsoid of revolution.

4.2.2 On the invariance of geodetic observables

It will be clear that most geodetic observables (if not all!) do not contain enough infor-
mation to determine the coordinates of a geodetic network uniquely. That is, the geodetic
observables are invariant against certain coordinate transformations. In this section we
will investigate the invariance of some geodetic observables and show how the G-matrix,
of which the columns span the null space of the design matrix A, can be constructed. We
also give examples of the minimal constraints that can be chosen.

The one dimensional case: As a one dimensional example we consider the case of level-
ling. Let hi denote the height of point i in the first coordinate system and let Hi denote the
height of the same point in the second coordinate system. The coordinate transformation

between the two coordinate systems reads then

hi = Hi + t                                               (4.13)

where t is a translation or a shift in height, which is constant for all points. It will be clear
that this coordinate transformation leaves observed height differences invariant. That is,
for the observation equation of a height difference we have

E{hi j} = h j - hi = H j - Hi                             (4.14)
84                                               Network Quality Control

This shows that the height differences are invariant for the translation t. This immedi-

ately implies that the design matrix A of a levelling network that is built up from height

differences only, will have a null space which is spanned by the column of the matrix
                    
                        1

    G     =   ..                                 (4.15)
             .

                  1

                  nО1

where n is the number of points in the network.

    Since matrix G has only one column, only one constraint is needed to eliminate the

nonuniqueness in the relation between height differences and heights. Possible choices
for the matrix B of the minimal constraints BT x = 0 are

    BT = (1, 0, . . ., 0)                        (4.16)
    BT = (0, 0, . . ., 1)
    BT = (1, 0, . . ., 1)

    BT = (1, 1, . . ., 1)

In the first case, the height of the first point is fixed and in the second case, the height of

the last point is fixed. With the third choice, the sum of the heights of the first and last

point is fixed. The fourth case, corresponds to a fixing of the sum of heights of all points.

When the two matrices G and B are known, one can also construct the S-transformation,

Sb = I - G(BT G)-1BT , that corresponds to the chosen minimal constraints. When the

height of the first point is fixed, it reads

           0 0 ... 0 

             -1   1              
                                 
    Sb =  ..           ..        
             .             .     

             -1               1

The two dimensional case: In two dimensions we are dealing with observables like
angles and distances. Their observation equations are nonlinear. The construction of the
G-matrix is now a bit more involved, since a linearization is involved as well. Let us first
show the general principle of constructing the G-matrix. Let us assume that the nonlinear
observation equations are invariant for the coordinate transformation

    x = T (u, p)                                 (4.17)

where the vector x contains the coordinates of the first coordinate system, the vector u
contains the coordinates of the same points in the second coordinate system and where
vector p contains the transformation parameters. Invariance of the nonlinear observation
equations A(x) for the transformation (4.17), implies that A(x) = A(u) and thus that

    A(T (u, p)) = A(u) for all p                 (4.18)

Since the observation equations are insensitive to changes in p, it follows if we take the
partial derivatives of (4.18) with respect to p, that

    xA(xo) pT (uo, po) = 0                       (4.19)

       A          G
4. Adjustment and validation of networks                                                          85

Thus once we know the transformation which leaves the observation equations invariant,
we only need to take its partial derivative with respect to the parameters in order to obtain
the G-matrix.

    The transformation which plays an important role in case of geodetic observables, is
the Similarity transformation. For the two dimensional case, it reads (see figure 4.1)

              xi =  cos  - sin                            ui + tx                                 (4.20)
              yi            sin  cos                      vi  ty

with

           :            scale
           :            rotation
           tx, ty :     translation

and where xi, yi are the coordinates of the first coordinate system and ui, vi, the coordinates
of the second coordinate system. Linearization of the similarity transformation, assuming

                            y
                                   v

                                                                               i
                            yi

                                                                                               u
                                                                                    ui
                                               vi
                                                         1
                            ty

                                   1/                              xi    x
                                                      tx

Figure 4.1 The two dimensional similarity transformation.

that    o  =  1,  o  =  0,  to  =  0  and  to  =  0,  gives

                             x              y

                                                               tx 

              xi = ui + 1 0 xoi -yo o io                           ty                             (4.21)
                                                                       
              yi            vi                                  
                                           0 1 yi xi

                                                                   

    Angles and distance ratios: It will be clear that angles fail to contain information about
scale, orientation and translation. This also holds true for distance ratios. This can be
seen as follows. Substitution of the coordinate transformation (4.20) into the observation
equation of a distance ratio gives

           E{l jk } =   x2jk + y2jk            u2jk + v2jk
               l ji                =           u2 + v2

                        x2 + y2                   ji ji

                          ji ji
86                                                                Network Quality Control

thus showing that the transformation parameters  , , tx and ty are absent. These param-

eters will also be absent when one considers the observation equation of an angle. As

a consequence, the design matrix A of a geodetic network, that has been built up from

distance ratios and/or angles only, will have a null space which is spanned by the columns

of the matrix  . . .                 .

                 .. .. ..            ..  
               1                         
                    0  xo     i  -yoi
                                 xo 
    G=           0  1  yo         i                               (4.22)

                        i            ..
                 .. .. ..
                 .. .               .

                    2n О 4

where n is the number of points in the network.

    Since matrix G has four independent columns, four constraints are needed to take care

of the nonuniqueness. The simplest way to fix the degrees of freedom of scale, orientation

and translation, would be to fix the coordinates of two points. The corresponding B-matrix

reads then

    BT =            0 0 . . . I2 0 . . . 0 0                      (4.23)

                    0 0 . . . 0 I2 . . . 0 0
                                   4 О 2n

    Azimuth: When we substitute the coordinate transformation (4.20) into the observa-

tion equation of an azimuth, we get

    E{ai j} = arctan xi j = arctan cos  ui j - sin  vi j
                       yi j                sin  ui j + cos  vi j

This shows that scale,  , and the two translations, tx, ty, get eliminated, but that the angle
of rotation  does not get eliminated. Hence, with azimuth observables one cannot de-
termine the scale and position of the network, but only its orientation. As a consequence,

the design matrix A of a geodetic network, that has been built up from azimuths only, will

have a null space which is spanned by the columns of the matrix

               . . . 
                  .. .. ..
                1 0 xo 
                              i
    G=                 yo 
                 0  1   i                                         (4.24)

                 .. .. ..
                 .. .

                    2n О 3

Now three constraints are needed. One could for instance fix the two coordinates of the

first point. This takes care of the two translational degrees of freedom. The scale can be

fixed by constraining the distance between the first and second point. The corresponding

B-matrix reads then              0 0 0 0 ... 
                      
                           1

    BT =  0                      1 0 0 0 ...                      (4.25)

                       -xo12     -yo12     xo    yo   0  ...

                                            12    12

                                         3 О 2n
4. Adjustment and validation of networks                                                                                    87

    Distance: For distance observables one can expect that the rotation angle and the
translations get eliminated. And indeed, we have

      E{li j} =            x2 + y2 =          u2 + v2

                            ij ij               ij ij

Thus in this case the G-matrix is given as

                  . .              .
                     .. ..         ..
                   1 0 -yo 
                                     i
      G=                           xo                                                                                (4.26)
                     0        1     i

                     .. .. ..
                     .. .

                           2n О 3

In this case an admissible set of minimal constraints would be: the fixing of the two
coordinates of one point and the fixing of the orientation between the first and the second
point.

The three dimensional case: In three dimensions, the nonlinear similarity transformation

is given as

                                                
            xi                                     ui                tx
       yi  =  R()R( )R()  vi  +  ty 
                                                                                                                     (4.27)

            zi                                     wi                tz

with

                        :        scale
                                 rotation
          ,, :                   translation

          tx, ty, tz :

and the three rotation matrices

                  1        0                   cos  0 -sin                    cos        sin  0 
                                   0

          R() =  0 cos           sin  , R( ) =  0      1     0  , R( ) =  -sin  cos  0 

                  0 -sin  cos                 sin  0 cos                        0        0     1

Linearization,    using       the  approximate     values      o  =  1,  o   =        o  =  o  =  0  and  to  =  to  =  to  =

                                                                                                           x      y      z

0, gives

                                 ui           1 0 0 xi 0 -zi      o                   o             
            xi                                                                                          tx
       yi  =  vi  +  0                                         yo        zo
                                                   1      0                        0            o   ty               (4.28)
                                                                i         i                   yi  tz 
                                                               zo    -yoi       xo          -xoi    
            zi                   wi           0    0      1                                    0   
                                                                i                i
                                                                                                    
                                                               uT (uo,po)                             

                                                                                                        

As in the two dimensional case, the G-matrix is again built up from some or all of the
columns of matrix uT (uo, po). In case of angles and distance ratios, all the columns are
used. For other observables though, only some of the columns are used.
88                                                                   Network Quality Control

Table 4.1 Entrie(s) of G-column(s) for different observation types.

dim.   dimN(A)=    actual observation               entries of G-column(s)
netw.   dimR(G)    types(s)
                                             translation(s)             rotation(s)      scale
  1          1       height differences                                                   xo
  2          2       distances and               1                         yo
             3       azimuts                   10                                           i
  3          4       distances                 01                            i
                                               10                                         yo
             3       angles and/or             01                        -xo
                     distance ratios           10                                           i
             6       distances, azimuts        01                            i
                     astronomical latitudes  100                                          xo
                     and longitudes          010                           yo
                     distances               001                                            i
                                             100                             i
                                             010                                          yo
                                             001                         -xo
                                             100                                            i
                                             010                             i
                                             001                                          zo
                                                                     0 -zoi          yo
                                                                                            i
                                                                                      i
                                                                     zo  0 -xoi

                                                                      i
                                                                     -yo xo
                                                                     i     i         0

                           angles and/or                             0 -zoi          yo

                                                                                      i
                                                                     zo  0 -xoi
       7                   distance ratios
                                                                      i
                                                                     -yo xo
                                                                     i     i         0

    GPS baseline: Since the baseline observable is only invariant for translations and not

for rotations and scale changes, the G-matrix of a geodetic network built up from baselines

only, is given as

       . . .
          .. .. ..
                             
       1 0 0
                             
        G =  0 1 0 
       0 0 1                                                                             (4.29)

                   .. .. ..
                   ...

                   3n О 3

Note that this is the three dimensional analogue of the levelling-case.

    To conclude this section, we have given in table 4.1 the various entries of the G-matrix,
when different type of geodetic observables are used.

4.2.3 Adjustment of free GPS network: an example

Let us assume that we have three GPS baselines available, between the three points 1, 2
and 3. The baseline between the two points i and j will be denoted as bi j. Thus, when the
baseline is expressed in Cartesian coordinates, we have

        bi j = (xi j, yi j, zi j)T
4. Adjustment and validation of networks                                        89

It is our goal to determine the Cartesian coordinates of the three points 1, 2 and 3. The
position vector of point i will be denoted as pi. Thus

        pi = (xi, yi, zi)T

We will assume that the three baselines have been determined independently. Thus no

correlation is assumed to exist between the baselines. We also assume that the three

baselines have been determined with the same precision. Thus we assume that all three

baselines have the same variance matrix, which will be denoted as Q.

    Based on the above assumptions, we can formulate the model of observation equations

as

                    -I3 I3 0                            Q0 0                  
    b12                         p1                 b12
E{ b23 } =  0 -I3 I3   p2  , D{ b23 } =  0 Q 0  (4.30)

    b31             -I3 0 I3    p3                 b31                0 0Q

    y               A                     x        y                      Qy

This is a system of 9 observation equations in 9 unknowns. Note however, that the design

matrix is not of full rank. It has a rank defect of 3. Thus the redundancy of the model

equals 9 - 9 + 3 = 3. Thus if one would formulate the model in terms of conditions

equations, one would have 3 independent condition equations. These three equations are

given as E{b12 + b23 + b31} = 0.
    Due to the rank defect of the model of observation equations, we need to specify

minimal constraints in order to be able to compute a particular least-squares solution. We

know that the range space of the matrix B of the minimal constraints BT x = 0, needs to
satisfy Rn = R(B)  R(AT ) = R(B)  N(A). Thus R(B) needs to be complementary to
N(A) = R(G). Since the null space of A is spanned by the columns of the matrix

                                                                                (4.31)
             I3

    G =  I3 
             I3

it follows that R(B) needs to be complementary to

                          
                I3   0

    G =  -I3         I3 
                0   -I3

This condition is satisfied by the following three choices for matrix B,

                                                   
         I3                 0                      I3

    B(1) =  0  , B(3) =  0  , B(1+2+3) =  I3                                    (4.32)

         0                  I3                     I3

There are of course many more matrices that satisfy the above condition, but we will
confine our discussion to the above three.
90                                                                        Network Quality Control

Point 1 as fixed (datum) point: The choice of matrix B(1) for the minimal constraints,
corresponds to a fixing of the coordinates of point 1. The corresponding least- squares

solution will be denoted as x^(1). It reads

       x^     = B    [(B   )T AT Q-1AB          ]-1(B    )T AT Q-1y
         (1)      (1) (1)                  (1)      (1)
                                      y                             y

and it has as variance matrix

       Q = B [(B )T AT Q-1AB ]-1(B )T   y
           x^(1)     (1) (1)                  (1)        (1)

Since                                           -Q-1   0                         
                                                                              0
       AT    -1   A  =     2Q-1         -Q-1    -Q-1          and  B =    I3  0
                           -Q-1         2Q-1
           Qy              -Q-1         -Q-1       2Q-1              (1)      I3

                                                                          0

it follows that

       [(B )T AT Q-1AB ]-1 =                  2Q-1       -Q-1      -1    23 Q 13 Q
           (1)          y     (1)             -Q-1       2Q-1            13 Q 23 Q
                                                                      =

Hence, the variance matrix of the minimally constrained least-squares solution, keeping

point 1 fixed, reads

                                       
                     00            0

       Qx^(1) =  0 23 Q 13 Q                                                        (4.33)
                    0 13 Q 23 Q

This matrix describes the precision of the coordinates of the free GPS network, when the
minimal constraints correspond to a fixing of point 1.

Point 3 as fixed (datum) point: Instead of choosing point 1 as fixed point, one may

of course also choose another point, say point 3. In that case the variance matrix of the

minimally constrained least-squares solution, reads                                 (4.34)
                  3  2 Q 31 Q 0 

        Qx^(3) =  13 Q 23 Q 0 
                      0 00

This matrix describes the precision of the coordinates of the free GPS network, when the
minimal constraints correspond to a fixing of point 3. Note that the two variance matrices
of (4.33) and (4.34) differ greatly. It is important to recognize however, that these two
variance matrices contain identical information. One can transform the one into the other
by means of an S-transformation. The transformation, that transforms any arbitrary least-
squares solution to x^(1), reads

           S(1)   =     I - G(BT G)-1BT
                        9          (1)        (1)

                           0 00                                                     (4.35)

                  =  -I3 I3 0 

                           -I3 0 I3
4. Adjustment and validation of networks                   91

Hence, the variance matrix Qx^(1) can be obtained from the variance matrix Qx^(3), by means
of the transformation (verify yourself) Qx^(1) = S(1)Qx^(3)ST(1).

Fixing the sum of the coordinates: Instead of fixing the three coordinates of one of the
points of the network, one may also decide to fix of all points of the network, their sum
of x-coordinates, their sum of y-coordinates and their sum of z-coordinates. Also these
three constraints are admissible. The S-transformation that corresponds with this set of
minimal constraints is given as

     S  (1+2+3)    = I9 - G(BT     (1+2+3)  G)-1BT(1+2+3)

                        3  2 I3 - 31 I3 - 31 I3            (4.36)
                   =  - 13 I 2 3 3 I3 - 13 I3 

                           - 13 I3 - 13 I 2 3 3 I3

With this S-transformation, we can thus obtain the variance matrix of x^(1+2+3) from Qx^(3)
as

     Q             =S              Q ST
        x^(1+2+3)      (1+2+3) x^3 (1+2+3)

                        9  2 Q - 91 Q - 91 Q               (4.37)
                   =  - 19 Q 29 Q - 19 Q 

                           - 19 Q - 19 Q 29 Q

The entries of this variance matrix again differ greatly from the entries of Qx^(1) and Qx^(3) .

The three minimally constrained solutions x^(1), x^(3) and x^(1+2+3) are however completely

equivalent. All three produce the same least-squares solution for the measurements. This

thus also holds for the variance matrix Qy^. Verify yourself that indeed Qy^ = AQx^ AT =

                                                                                                                                                                    (1)

AQx^ AT = AQx^(1+2+3)  AT .
(3)

Evaluation of free network precision: When evaluating the precision of a free network,

one has to make sure that the evaluation is not affected by the choice of minimal con-

straints. These constraints do not contain information which is essential for the precision-
evaluation. They are merely a tool to be able to compute coordinates. Thus when the

precision evaluation is based on, say Qx^(1), one should use a procedure which gives re-
sults that are identical to the results that one would obtain, when the precision evaluation
is based on, say Qx^(3) . Hence, when one wants to compare Qx^(1) with a criterium matrix,
one has to make sure that the criterium matrix is defined with respect to the same set

of minimal constraints. This can be accomplished by transforming the criterium matrix

with the appropriate S-transformation. Thus when Cx denotes the criterium matrix, one
should compare Qx^(1) with S(1)CxST(1), and Qx^(3) with S(3)CxST(3). Only then will the two
evaluations give identical results.

    In case one decides to base the evaluation on the generalized eigenvalue problem, the
appropriate formulation is thus

     | Qx^(1) -  S(1)CxST(1) |= 0                          (4.38)
92                                                                    Network Quality Control

and not | Qx^(1) - Cx |= 0. And if it is Qx^(3), that needs to be evaluated, the appropriate
formulation is

    | Qx^(3)  -                T  |=  0                                                (4.39)

                   S(3)CxS(3)

and not | Qx^(3) - Cx |= 0. Both the eigenvalue problems (4.38) and (4.39), will give
identical results for the eigenvalues.

4.2.4 Testing of free levelling network: an example

The levelling network consists of the four points 1, 2, 3 and 4, and the following five
observables

        h12 , h23 , h31 , h34 , h42

Thus the network consists of two levelling loops, loop 1 - 2 - 3 and loop 2 - 3 - 4. The
observables are assumed to be uncorrelated, all having the same variance 2. We know
that we need to introduce one minimal constraint in order to eliminate the rank deficiency.
As minimal constraint we choose to fix the height of the first point:

    h1 = 0

The model of observation equations, with the minimal constraint included, reads then

                                  10                0
              h12
                                                        1 
              h23          -1            1          0  h2

    E{        h31  }  =           0   -1            0    h1            , Qy =  2I5     (4.40)
                                                       
     h34   0 -1                                            3
                                                    1  h1

                                                                   4

              h42                 1 0 -1

Note that due to the elimination of h1, the design matrix is indeed of full rank. The
remaining heights are given the upperindex 1 to show that they are defined with respect
to the fixing of the height of the first point. There are 5 observations and 3 unknowns.
The redundancy is therefore equal to 2. The two condition equations can be identified as
E{h12 + h23 + h31} = 0 and E{h23 + h34 + h42} = 0.

    Detection: In order to test the above model, we will start with the overall model test.
The general expression for the corresponding test statistic reads

    T = e^T Q-y 1e^

When applied to the above model, it becomes (verify yourself)

           1                                     T      3 -1          h12 + h23 + h31  (4.41)
    T= 2                                               -1 3           h23 + h34 + h42
                     h12 + h23 + h31
          8          h23 + h34 + h42

Since the redundancy equals 2, this test statistic has a central Chi- squared distribution,

with 2 degrees of freedom under the null hypothesis. Note that the overall model test
statistic is a function of the misclosure of the levelling loop 1 - 2 - 3 and of the levelling
loop 2 - 3 - 4.
4. Adjustment and validation of networks                            93

    Identification: In case identification of model errors is needed, we need to decide what
type of model errors are likely to occur. It is standard practice to search for blunders in the
individual observations. This is the data snooping approach. Since the variance matrix Qy
is diagonal, the wi-test statistics for data snooping have the simple form

wi = e^i   i = 1, . . ., 5
      e^i

They have a standard normal distribution under the null hypothesis. When the wi-test
statistic is worked out for the above model, we get (verify yourself)

w1  =         1   (3h12    +  2h23  +  3h31  -   h34  -  h41  )
           2 6
             1
w2  =          8  (h12  +  2h23  +  h31  +  h34  +  h41  )

w3 = w1                                                             (4.42)

w4  =         1   (-h12     +  2h23  -  h31  +  3h34  +  3h41    )
           2 6

w5 = w4

Note that some of the test statistics are identical. This is understandable if one considers

the geometry of the levelling network. A blunder in h12 can not be discriminated from a
blunder in h31 (w3 = w1) . Also, a blunder in h34 can not be discriminated from a blunder
in h42 (w5 = w4).

    If in addition to potential blunders in the data, one also suspects that, say all three
observed height differences of the levelling loop 1 - 2 - 3 are erroneous by a constant

amount, then also an additional identification test needs to be performed. In this case one

has to make use of the general expression for the w-test statistic. It reads

w = cT Q-y 1e^
         cT Q-y 1Qe^Q-y 1c

The appropriate c-vector for testing whether a constant shift in the first three observations
occurred or not, reads

        c = (1, 1, 1, 0, 0)T

When the expression of the w-test statistic is worked out for the above model, we get
(verify yourself)

      h12 + h23 + h31                                               (4.43)
w= 

              3

Hence, the appropriate test statistic equals the misclosure of the levelling loop 1 - 2 - 3,
divided by its standard deviation.

    Adaptation: In case data snooping lead to the conclusion that, say the second obser-
vation was erroneous, one can decide to remeasure this observation and after remeasure-
ment, again apply the whole testing procedure. For the case that all first three observations
94                                                   Network Quality Control

are off by a constant amount, one can of course also opt for remeasurement. But instead,

one may also decide to adapt the model. In that case the new model becomes

     h12   1 0 0 1   1 
     h23    -1 1           0 1  h2

                                                1
                                 h3                  2
                                                   
    E{ h31 } =  0 -1 0 1   1  , Qy =  I5                                    (4.44)
        h   0 -1 1 0  h4
    34                           

    h42           1 0 -1 0

One should be aware however, that this change also results in a change for precision and
reliability. Both will become poorer and it depends on the particular application at hand
whether one is willing to accept this or not.

4.3 The connected network

4.3.1 The observation equations

The purpose of the connection is to integrate the geometry of the free network with that of
the network of existing control points and to express the result in the coordinate system of
the control points. Since the coordinate system used for the free network adjustment, may
differ from the one used for the control points, one could be dealing with two different
coordinate systems. In order to show this difference, we will make use of the following
notation. The Cartesian coordinates of the free network are denoted as (x, y) in case of two
dimensions and as (x, y, z) in case of three dimensions. The coordinates of the network of
control points are denoted as (u, v) or (u, v, w). The coordinates of all free network points
are collected in the vector p and the coordinates of all control network points are collected
in the vector q. For the two dimensional case, we thus have

          p = (. . ., xi, yi, . . .)T
          q = (. . ., ui, vi, . . .)T

The free network and the control network will usually overlap. Hence, three type of points
can be discriminated. The points that are part of the free network, but not of the control
network. The points that are part of both the free network and the control network. And
the points that are part of the control network, but not of the free network. The two sets
of coordinates of the free network will be denoted as

          p1 = (x1, y1, . . ., xn1, yn1 )T
          p2 = (xn1 +1, yn1+1, . . ., xn1+n2 , yn1+n2 )T

where p1 contains the coordinates of the n1 points of the free network that are not part of
the overlap, and where p2 contains the coordinates of the n2 points of the free network
that are part of the overlap. Thus the total number of points of the free network is assumed
to be equal to n1 + n2. As a result of the free network adjustment, we thus have available

    E{ p1 } = p1  , D{ p1 } = Qp1 Qp1 p2                                    (4.45)
    p2     p2              p2    Qp2 p1 Qp2
4. Adjustment and validation of networks                                         95

The two sets of coordinates of the control network will be denoted as

          q2 = (u1, v1, . . ., un2, vn2)T
          q3 = (un2+1, vn2+1, . . ., xn2+n3 , yn2+n3 )T

where q2 contains the coordinates of the n2 points of the control network that are part of
the overlap, and where q3 contains the coordinates of the n3 points of the control network
that are not part of the overlap. Thus the total number of points of the control network is
assumed to be equal to n2 + n3. Of the control network, we assume to have available

       E{ q2 } = q2            , D{ q2 } = Qq2 Qq2q3                             (4.46)
           q3           q3            q3        Qq3q2 Qq3

In order to be able to combine (4.45) with (4.46), we still need to consider the coordi-
nate transformation between the two coordinate systems. Although the type of coordinate
transformation that is needed, depends on the particular application at hand, any coordi-
nate transformation will be of the general form p = F(q, t), where t denotes the vector of
transformation parameters. When applied to the two sets p1 and p2, we thus have

       p1 = F1(q1, t)                                                            (4.47)
       p2 = F2(q2, t)

With (4.45), (4.46) and (4.47), we are now in the position to    formulate the model of

observation equations for the connection adjustment. It reads

            F1(q1, t)                                    Qp1 p2    0       0     
   p1                                 p1            Qp1   Qp2                    
 E{ p2 } =  F  2(q2, t)    , D{ p2   } =  Qp2 p1                   0       0     
 q2                 q2              q2   0                 0                     (4.48)
                                                                  Qq2    Qq2 q3  
                                                                 Qq3 q2   Qq3
q3                  q3             q3 0 0

Note that the observation equations are linear in q3, but possibly nonlinear in q1, q2 and t.
Whether the observation equations are nonlinear or not, depends on the type of coordinate

transformation used. In case of nonlinear observation equations, we still need to apply a

linearization in order to obtain linear(ized) observation equations. Linearization of (4.48)

gives

           p1            T1 0 0 A1   q1 

       E{ p  2    } =       0  T2  0  A2        q2                               (4.49)
                                                    
          q                               0   q 
               2            0  I   0            3

           q3               0 0I 0              t

where

       T1 = q1F1(qo1, to)   A1 = t F1(qo1, to)                                   (4.50)
       T2 = q2F2(qo2, to)   A2 = t F2(qo2, to)

The structure of these four matrices of course also depends on the type of coordinate
transformation used. As an example, we will show how they look like when the three
dimensional similarity transformation is used.
96                                                                                                                                 Network Quality Control

     The three dimensional similarity transformation between two sets of Cartesian coor-

dinates reads

                                                                 
     xi                                                                ui                   tx

      yi  =  R()R( )R()  vi  +  ty                                                                                                                             (4.51)

     zi                                                              wi                     tz

with the scale  , the translation vector (tx, ty, tz)T and the three rotation matrices

               1           0                               cos  0 -sin                                      cos             sin  0 
                                      0

     R() =  0 cos                  sin  , R( ) =  0                        1         0  , R( ) =  -sin  cos  0 

               0 -sin  cos                                     sin  0 cos                                           0           0       1

After linearization, the four matrices of (4.50) read

                                oRo                                                       oRo                               

           T1  =  . .                                                    T 2                               .                  
                                                                                                                              
                                                            ,                        =                        .               
           3n1 О 3n1                         .               3n2 О 3n2                                           .            
                                                                                                                     oRo
                                                     oRo

                                   o RoWo                                            oRoW o                                                                       (4.52)
                                                 1  I3                                             3n1 +1           I3 
                                                                                                                                                                 o
                                                      .                                                                                                         vi
                 A1                      .                          A2                        .                     .                                          -uoi 
               3n1 О 7        =          .            .     ,     3n2 О 7      =                                    .                                           0
                                         .            .                                       .                         
                                                                                              .                     .   
                                    o RoW o
                                            3n1     I3                                o RoWo                        I3
                                                                                             3n1 +3n2

with Ro = R(o)R( o)R(o), t = ( ln  , ,  , , tx, ty, tz)T and

               o                         -woi       cos        o  sin       o  -     vo  sin     o                          -w       o     co  s    o
                  ui                                                                                                                 i
     Wo                                                                               i                                                             o
                  vo                        wo             o             o         uo            o                                      o
        i  =                                        cos        cos            +         sin                                     -w      i  si  n  
                   i                          i                                      i
                  wo               -voi                o            o         uo            o                    o      vo           o         o            o
                           i                cos           cos          +           cos           sin                        si  n       +  u   i  co   s  
                                                                                i                                        i

In case of the two dimensional similarity transformation, the matrices of (4.52) will have
the same structure, be it that now Ro = R(o), t = ( ln  , , tx, ty)T and

     Wo    =      uo -voi             i

        i         vo uo

                        i          i

We conclude this section with some remarks.

Remark 1: In the above linearization, no particular assumptions were made about the

values of the approximate transformation parameters. In some applications it may happen

however, that the two coordinate systems differ only slightly in scale and orientation. In

that case one can choose as approximate values  o = 1, o =  o = o = 0. Note that this

results in a considerable simplification of the above matrices. We then have  oRo = I3

and

               o                                         o o
                  ui 0 -wi vi
     Wo           vo               wo               0 -uoi 
           =
        i          i                 i

                  wo -vo uoi             i          i             0

Remark 2: When the similarity transformation is used in the connection model (4.48)
with all its transformation parameters included, one should recognize that the scale, the
orientation and the location of the free network is abandoned in favour of the scale, the
orientation and the location of the control network. Thus only the shape of the free net-
work will then contribute to the determination of the geometry of the connected network.
4. Adjustment and validation of networks                   97

Remark 3: Above it was assumed that all transformation parameters of the similarity
transformation are unknown. In some applications it may happen however, that all or
some of these transformation parameters are known. For instance, it may happen that
one knowns (from an earlier adjustment) the relative orientation and scale between the
WGS84 coordinate system and the National coordinate system. This would imply that
if a free GPS network is expressed in the WGS84 system and the coordinates of the
control network in the National coordinate system, that the scale and three orientation
parameters are not needed as unknowns in the model of observation equations. If they are
known with a sufficient precision, they could be treated as constants. The only remaining
transformation parameters would then be the three translation parameters.

Remark 4: Above it was assumed that the coordinates of the control network are of the
Cartesian type. This need not be the case of course. The model is easily adapted however,
for the case that one uses other than Cartesian coordinates. One only needs to substitute
in the above nonlinear model of observation equations, the relation that exists between
Cartesian coordinates and the type of coordinates that one needs to use.

Remark 5: In the model (4.48), it was assumed that no correlation exists between the
coordinates of the free network and the coordinates of the control network. For most
practical applications this assumption is realistic, since the measurement processes that
produced the coordinates of the two networks can usually be assumed to be independent.

Remark 6: In the model (4.48) it was also assumed that the variance matrix of the co-
ordinates of the control network, Qq, is available. In practice this may not be the case
however. Fortunately, for the constrained connection adjustment it is not needed. In that
case the adjustment is performed with Qq = 0. For the statistical testing of the model
(4.48), it is needed however. In that case one will have to work with a substitute 'vari-
ance matrix', that then hopefully will give a sufficiently good approximation to the actual
variance matrix Qq.

Remark 7: In the connection model (4.48) we included the coordinates of the nonover-
lapping points of the control network, q3. In the remaining part of these lecture notes,
they will be disregarded however. These coordinates do not contribute to the redundancy
of the model and therefore also not to the results of statistical testing, and they remain
unchanged when a constrained connection adjustment with Qq = 0 is applied.

4.3.2 The unconstrained connection for testing

In this section we will discuss ways of testing the validity of the connection model (4.48).
It will be clear that the coordinates of the nonoverlapping points of the two networks do
not contribute to the redundancy of the model and therefore also not to the results of the
statistical tests. Hence, we can restrict our attention to the linear(ized) model

E{ p2 } = T2 A2  q2 , D{ p2 } = Qp2 0                      (4.53)
q2  I0           t                              q2  0 Qq2

The redundancy of this model equals (2n2 - nt ) in two dimensions and (3n2 - nt ) in
three dimensions, where nt is the dimension of the vector of transformation parameters.
98                                                                       Network Quality Control

Thus with the full similarity transformation in two dimensions, we have a redundancy of

(2n2 - 4). This shows that in the overlap of the two networks, a minimum of two points
is needed.

    We can reduce the number of unknown parameters in this model, if we eliminate q2
by making use of the coordinate differences d = p2 - T2q2. This gives

      Ho :     E{d} = A2t         ,      D{d}     =  Qp2   +  T2Qq2  TT                            (4.54)

                                                                      2

This model will be our null hypothesis Ho. As alternative hypothesis Ha, we will consider
the model

      Ha : E{d} = A2 C                            t  , D{d} = Qp2 + T2Qq2 T2            T          (4.55)

                                                  

where  is an r О 1 vector of assumed model errors and C is a matrix that specifies how
the model errors are related to the observations. From our chapter on statistical testing, we
know that the above null hypothesis can be tested against the above alternative hypothesis,
using the test statistic

      T r = e^Td Q-1 d C(CT Q-1 d Qe^d Q-1 d C)-1CT Q-1 d e^d                                      (4.56)

where e^d is the least-squares residual vector of d and Qe^d its variance matrix. Assuming
normally distributed observables, one will decide to reject Ho on the basis of Ha, when

      Tr  >    2   (r,  0)

                 

with  2   (r,  0)  the  critical  value  of  the  central  Chi-  square  distribution,  having  r  degrees  of

        

freedom.

    Usually one will start with the most relaxed alternative hypothesis. This is the hypoth-

esis for which the matrix (A2,C) is square and regular. The corresponding test statistic
reads

      T = e^Td Q-1 d e^d

Under the null hypothesis Ho, it has a central Chi-square distribution with the degrees of
freedom being equal to the redundancy of the model under Ho. The test corresponding to
the most relaxed alternative hypothesis, is usually referred to as the overall model test and
its purpose is to detect unspecified model errors.

    When the overall model test leads to a rejection of the null hypothesis, there is a high
likelihood that the model under Ho has been specified incorrectly. Potential model errors
are: (1) the assumed relation between the two coordinate systems is wrong; (2) the shape
of the free network differs significantly from the shape of the control network, due to,
for instance, an error in one or more of the coordinates; (3) the assumptions about the
stochastic model are incorrect, due to, for instance, a too optimistically specified variance
matrix of the coordinates.

    In the following we will restrict ourselves to model errors in the functional model.
They can be specified by means of the matrix C. We will now discuss some examples.
4. Adjustment and validation of networks  99

Errors in the coordinates: Since the coordinates form the observations in the connec-
tion model, it seems reasonable to first check the coordinates on possible errors. First
note that one will never be able to find errors in the coordinates of the points in the two
nonoverlapping parts of the two networks. These coordinates do not contribute to the
redundancy of the model and will therefore not be part of the test statistics. Fortunately,
these errors will also have no influence on the adjusted coordinates of the other points
after the connection.

    Secondly note, that one will also not be able to discriminate whether an error has oc-
curred in the coordinates of the free network or in the coordinates of the control network.
This is due to the fact that the observations in the model (4.55) are formed from coordinate
differences.

    In order to check for a potential error in the coordinates, the simplest alternative hy-
pothesis is the one for which it is assumed that only one coordinate of one of the points
is wrong. In this case,  is a scalar (r = 1) and matrix C becomes a vector, which will be
denoted by the lowercase character c. Hence, instead of using (4.56) one can in this case
also use its square-root, being the w-test statistic

        w = cT Q-1 d e^d
                 c d T Q-1Qe^Qd-1c

For the three dimensional case, the c-vector then takes one of the following three forms

          cxi = (0 0 0 . . .1 0 0 . . .0 0 0)T
          cyi = (0 0 0 . . .0 1 0 . . .0 0 0)T
          czi = (0 0 0 . . .0 0 1 . . .0 0 0)T

where i refers to the point being tested. In this way one can systematically check all
coordinates of all overlapping points on potential errors.

Point identification errors: An error in only one coordinate of a point may occur due to
typing errors. But in other cases of course, it is more likely, when an error occurs, that it
will not be confined to a single coordinate only, but instead will effect all coordinates of
a point. Such a type of error occurs when one erroneously beliefs that the free network
coordinates and the control network coordinates indeed refer to the same point. These
type of errors are called point identification errors.

    In order to test for these type of errors, the C-matrix takes the form
               0 0 0 . . . 1 0 0 . . . 0 0 0 T

        Ci =  0 0 0 . . . 0 1 0 . . . 0 0 0 
                  0 0 0 ... 0 0 1 ... 0 0 0

Height or eccentricity error: In case of GPS, it not seldom occurs that an error is made
in the measured GPS antenna height. The c-vector then takes the form

                                                                                         T

        ci = 0 . . . 0 hxi hyi hzi 0 . . . 0
100                                                            Network Quality Control

where the vector (hxi , hyi, hzi)T specifies the direction of the potential antenna height offset
of point i in the approximate coordinate system. In case of an eccentricity error, the C-
matrix has two columns,

                                                                                          T

                 0 . . . 0 exi eyi ezi 0 . . . 0
        Ci =

                 0 . . . 0 fxi fyi fzi 0 . . . 0

where the two vectors (exi , eyi, ezi )T and ( fxi, fyi, fzi)T span the plane in which the eccen-
tricity error of point i is supposed to have taken place.

Misspecified coordinate transformation: Instead of having errors in the data, it may

also happen that one has misspecified the coordinate transformation between the two co-

ordinate systems. As an example consider a free GPS network that needs to be connected

to a control network. Let us assume that the coordinates of the GPS network are expressed

in the system of WGS84. Furthermore it is assumed that the coordinate system of the con-

trol network, has a scale and an orientation which is identical to the WGS84 system. The

model under the null hypothesis reads then

            ..   .. ..                      .
            .  . .                          ..
                                                  
               xi - ui             1    0   0        tx
                                              
     Ho : E{ yi - vi } =  0 1               0        ty  
                                                 
            zi - wi                                  tz
                                   0 0      1    

                         ..          .. ..  ...
                        .            ..

Thus only the translation vector is included in the coordinate transformation. Now assume

that the null hypothesis gets rejected by the overall model test and that one suspects that

the two coordinate systems, apart from differing in their location, differ also slightly in

scale. In that case one will have to oppose the above null hypothesis to the following

alternative hypothesis

            ..   .. .. .. .. 
           .                         ... .                     
                                                           tx
               x        - u        1    0   0    uo  
            i i                                    i       ty  

                                                   o
     Ha :  E{  yi - vi       }  =    0  1   0    vi            
                                                          tz   
            zi - wi                              wo       ln
                                     0  0   1      i

                        ..           .. .. .. ..
                          .          ... .

The model is thus enlarged with one additional column, being the column which models
the scale difference between the two coordinate systems. This column vector is thus the
appropriate choice for the c-vector

        c = (. . ., uoi , voi , woi , . . .)T

Note that  o has been taken equal to one, since the difference in scale between the two
coordinate systems, although potentially significant, was still assumed to be small.
4. Adjustment and validation of networks                              101

4.4 The constrained connection for coordinate computation

In the previous section the validation of the connection model was considered. In the
present section we will consider the estimation problem. For validation, we could restrict
our attention to the coordinates of the overlapping points. For estimation however, we
need to take the coordinates of the nonoverlapping points of the free network into account
as well. After all, the purpose of the connection is to obtain the coordinates of these newly
established points in the coordinate system of the control network. Thus instead of the
model (4.54), we will now work with the enlarged model

E{ p1 } = T1 A1         q1 , D{ p1 } = Qp1 Qp1 p2 (4.57)
d  0 A2                 t                 d                Qp2 p1 Qd

Remember that the coordinate differences of the overlapping points are contained in d =
p2 - T2q2, which has as variance matrix, Qd = Qp2 + T2Qq2 T2T .

    The above model can be solved using the standard least-squares algorithm. As a
result one would obtain the unconstrained least- squares solution. In practice however,
circumstances often dictate that the coordinates of the control network remain unchanged.
This can be accomplished by applying the standard least-squares algorithm to the above
model, with the additional constraints that

Qq2 := 0 and q2 := 0                                                  (4.58)

Setting Qq2 to zero, implies that no least-squares correction is given to q2. This together
with the setting of q2 to zero, implies that the coordinates of the control points in the
overlap remain fixed to their original values (note: the original values are used as approx-

imate values in the linearization). In order to discriminate between the unconstrained and

the constrained least-squares solution, we will use the superscript c for the constrained
case. Thus d and dc have the same variance matrix, namely Qd, but their sample
values differ. They are given respectively as

d = p2 - T2q2 and dc = p2

    Instead of solving the above model in one step, we will solve it in three steps. This
will help us in getting a somewhat better insight in the various aspects of the connection
adjustment.

The first step: We already remarked that the coordinates of the nonoverlapping points
of the free network do not contribute to the redundancy of the above model. Likewise,
these coordinates also do not contribute to the determination of the transformation param-
eters. Only the coordinates of the overlapping points contribute to the determination of
the transformation parameters. We therefore start by first considering

E{d} = A2t , D{d} = Qd                                                (4.59)

From it the unconstrained least-squares estimator of the vector of transformation param-
eters follows as

t^ = (AT2 Q-1 d A2)-1AT2 Q-1 d d , Qt^ = (AT2 Q-1 d A2)-1             (4.60)
102                                                     Network Quality Control

The constrained least-squares estimator however, reads

                    (AT Q-1A )-1AT Q-1dc
      t^c        =
                    2p 22       2p     2

      Q c = (AT Q-1A )-1AT Q-1Q Q-1A (AT Q-1A )-1       (4.61)

       t^           2 p2 2      2 p2 d p2 2 2 p2 2

Note that in the application of the error propagation law, the randomness of q2 has been
taken into account. After all, although we constrain the solution to the coordinates of
the control network, these coordinates are still of a stochastic nature. We know that a

least-squares solution that takes the stochasticity of all variables into account will give

estimators that are of minimal variance. In the above constrained least-squares solution

this is not the case. Thus

     Qt^ < Qt^c

showing that the transformation parameters of the unconstrained solution are of a better
precision than the ones of the constrained solution. This is thus the price one pays, when
one is forced to keep the coordinates of the control network unchanged.

    The unconstrained solution for the coordinate differences and their least-squares resid-
uals are given as

     d^ = A2t^ , Qd^ = A2Qt^AT2                         (4.62)
     e^d = d - d^                   -      Q AT
                    ,  Qe^d  =  Qd     A2
                                             t^ 2

As we have seen in the previous section, these least-squares residuals form the basis for
the statistical testing of the validity of the connection model.

The second step: In this second step we will determine the solution for the coordinates
of the free network in the nonoverlapping part. We know that p1 does not contribute to
the redundancy of the model. If in addition, p1 would not correlate with p2 and thus
not with d, then p1 = p^ 2 would hold true. That is, in that case the coordinates of the
free network in the nonoverlapping part would not change as well. But since Qp1 p2 = 0,
it follows that these coordinates do change. In fact, their residual vector can be computed
from e^d as

        e^ p2 = Qp1 p2 Q-d 1e^d

Hence, the unconstrained least-squares solution reads

     p^ 1 = p1 - Qp1 p2 Q-1 d (d - d^ )                 (4.63)
     Qp^1 = Qp1 - Qp1 p2 Q-d 1(Qd - Qd^)Q-d 1Qp2 p1

Of course if the connection model fails to have any redundancy at all, then e^d = 0 and
p^ 1 = p1. This would for instance happen when in the two dimensional case, the con-
nection model is based on the full similarity transformation and the number of points in

the overlap equals only two.

    The constrained least-squares solution reads

     p^ c1 = p1 - Qp1 p2 Q-p 12 (dc - A2t^c)            (4.64)
4. Adjustment and validation of networks                                                 103

and again we have Qp^c > Qp^ .1
                1
The above two solutions are still expressed in the coordinate system of the free net-

work. Hence, in order to express them in the coordinate system of the control network,

we still need a third step.

The third step: After the unconstrained adjustment of the connection model (4.57), we
have

        p^ 1 = T1q^ 1 + A1t^
Since matrix T1 is invertible, it follows after substitution of (4.60) and (4.63), that

q^ 1 = T1-1 I -Qp1 p2 Q-1 d               p1 - A1 t^                                     (4.65)
                                          d           A2

This is the final unconstrained least-squares solution of the coordinates of the nonoverlap-
ping points of the free network, expressed in the coordinate system of the control network.
This is thus the solution one gets for the coordinates of the nonoverlapping points, when
the model (4.57) is solved using the standard least- squares algorithm.

    In case of a constrained adjustment of the connection model (4.57) we have

        p^ c1 = T1q^ c1 + A1t^c
From substitution of (4.61) and (4.64), follows then

q^ c = T -1  I     -Qp          p  Q-1    p1 - A1 t^c                                    (4.66)
11                                        dc
                                      p
                             12 2                     A2

This is the final constrained least-squares solution of the coordinates of the nonoverlap-
ping points of the free network, expressed in the coordinate system of the control network.
Hence, this is the solution of the newly established points, when one requires that the co-
ordinates of the control network remain unchanged.

    The above expression is the most general expression one can get. Let us now consider
three special cases.

No correlation: In the special case that the coordinates of p1 do not correlate with those
of p2, we have Qp1 p2 = 0. The above expression simplifies then to

        q^ c1 = T1-1(p1 - A1t^c)
In this case the transformation A1t^c is directly applied to p1, without taking the vector
of least-squares residuals e^cd = dc - d^ c into account.

No redundancy: The above simplified expression is also obtained in case there is no
redundancy in the connection model. An absence of redundancy implies that the points
in the overlap have as many coordinates as there are transformation parameters. Hence,
there is no need for an adjustment to determine the transformation parameters. They are
uniquely determined from the coordinates of the points in the overlap and the least-squares
residual vector e^cd is identically zero.
104                                                          Network Quality Control

No coordinate transformation: In case the two coordinate systems of the free network
and the control network coincide, the transformation parameters will be absent from the

model and T1 will be equal to the identity matrix. In that case the above constrained
solution simplifies to

         q^ c1 = p1 - Qp1 p2 Q-p 12 dc                               (4.67)

The correction to the coordinates of p1 is now only based on the difference between the
coordinates of the points in the overlap.

We already remarked that the precision of the constrained solution is less than that of

the unconstrained solution, Qq^c > Qq^ . This need not be dramatic. It is simply a conse-1
                           1
quence of the fact that by constraining the solution, one is in fact using a less than optimal

estimator. But what is important to recognize though, is that the constrained least- squares

solution can be poorer than the solution one had before the connection was carried out.

This is easily shown for the above solution (4.67). Application of the error propagation

law to (4.67) gives

         Qq^c = Qp1 - Qp1 p2 Qp2 -1(Qp2 - Qq2 )Qp2 -1Qp2 p1

              1

This shows that

         Qq^c > Qp1 when Qp2 < Qq2                                   (4.68)

              1

But this means, if the coordinates of the control network are of a poorer precision than
the coordinates of the free network (Qp2 < Qq2 ), that the adjusted coordinates, when
constraining is applied, will have a precision which is poorer than before the adjustment
was carried out (Qq^c > Qp1 ). This is the reason, why in surveying one works from the

                                        1

'large to the small'. That is, in the densification process, the free networks are connected

to 'higher order' control networks, i.e. networks that are of a better precision than the free

networks.

4.4.1 A levelling example

To conclude this chapter, we will give an example of testing a connection model. The

adjustment is therefore of the unconstrained type. The constrained adjustment is rather

straightforward and is left to the reader. The connection model is one where two levelling

networks need to be connected. The model is assumed to read
                               
     h1          1         1
.  .                       .  
 ..                  ..    ..           H1
                               
                           1 1     .  ..  , Qh QhH = h2In 02 (4.69)
    hn                     0  .    Hn  QHh QH                0 H In
E{ } = 
 H1        1
     .     
           .               ..  t
 ..   . .

     Hn                    10
4. Adjustment and validation of networks                                                               105

The two overlapping levelling networks are thus assumed to differ in height only. The
redundancy of the model equals (n - 1). First we will give the least-squares estimators of
Hi and t, and then we will discuss the detection and identification step.

Least-squares estimators: After forming the system of normal equations of the above

model and solving it for the translation parameter, we get

       
        ^t = h» - H»

         2 = 1 ( 2 +  2 )                                                                              (4.70)

          t^            nh H

where

       h» = 1  hi , H» =  Hi n 1 n
          n i=1                 n i=1

are the average heights of the two networks. In this particular case, the least-squares

estimator of the translation parameter thus equals the difference of the height averages of

the two networks.

    When we solve the system of normal equations for the height coordinate Hi of point
i, we get

                    =     Hi -  2      2  [(Hi - H» ) - (hi - h» )]
        H^ i
                                   H
                                2
                                h +H

         2                                                                                             (4.71)

               H^         2           2           1

                                        H
                    = H [1 - 2+2 (1 - n )]
                 i              hH

This shows that the least-squares residual of Hi is given as

                 2
       e^H = 2 H 2 [(Hi - H» ) - (hi - h»)]

         i h + H

It is constructed from a difference of two differences. The two differences are the height

of point i with respect to the average height in the first height system and the height of

point i with respect to the average height in the second height system. Note that the

residual  gets  smaller,  when  2     gets  smaller.  This   is  understandable.       The   more      precise  the

                                  H

Hi coordinate is, the more confidence one has in it and the less one is willing to give it a
                                             2                       H^ i
large  correction.  In  the  limiting  case       =  0,  we  have          =  Hi.  In  that  case  no  correction
                                               H
is applied at all. If we consider the other limiting case 2h = 0, then of course the hi
coordinate would not get a correction, h^i = hi. In this case, we also would have

          H^ i = hi + H» - h»
                = hi - t^

thus showing that H^ i is now obtained from simply applying the translation estimator to
hi.

Detection: In order to verify whether it is justified to rely our results of adjustment on
the above model (4.69), we need to test it. In order to be able to test it, the least-squares
106                                                                                Network Quality Control

residuals should at least not be identical to zero, which would happen when there is no
redundancy. Since the redundancy equals (n - 1), we thus at least need more than one

point in the two overlapping networks.

    To test the above model, we first consider its overall validity. The general form of the
appropriate test statistic for detection reads T redundancy = e^T Q-y 1e^. When this expression
is worked out for the above model we get

     T n-1 =       in=1(Hi - hi)2 - n(H» - h»)2                                                                (4.72)

                                     22
                                  h + H

Note that the term n(H» - h»)2 would be absent in case the translation parameter t would
be absent from the model. In that case the test statistic would simply equal the sum of

the squared height differences divided by the sum of the two variances. The redundancy

would then also have increased by one to n.

     With the above test statistic, the model is invalidated or rejected when

     Tn-1    >       2  (n  -  1  ,  0  )                                                                      (4.73)
                     

When rejection occurs, there are of course many type of misspecifications in the model

that could have caused it. The misspecifications could be in the functional and/or the

stochastic model. For instance, when the stochastic model is too optimistic, the variances

are too small and the value of Tn-1 would be unrealistically large. As a result, one could
have an unjustified rejection of the model. This is therefore the reason, why the variance

2    is not set  to zero,   when testing the connection model.             Thus the unconstrained con-

  H                                                                                      2

nection  is  used  for  testing,     while   the  constrained         connection,  with    H  =  0,  is  used  for  the

actual coordinate computation.

     Instead of having a too optimistic stochastic model, one could of course also have used

a too pessimistic stochastic model. In that case the variances are too large and Tn-1 would
be unrealistically small. If one suspects this to be the case, then the above one-sided test

should be replaced by its two-sided counterpart. Since the expectation of T n-1/(n - 1)
equals one under the null hypothesis, one then tests whether this ratio is significantly

larger or smaller than one. We will however assume that the stochastic model is correct

and restrict our attention to misspecifications in the functional model.

Identification: If we assume that the rejection of the model could have been caused by
an error in the coordinate set of the H-system, the c-vector of the w-test statistic will have
the form

     c = (0T , cHT )T with cH = (cH , . . ., cHn )T                                                            (4.74)
                                                                                                               (4.75)
                                                                1

When worked out, the corresponding w-test statistic reads then

     wH =          in=1 cH (Hi - hi) - (H» - h» ) in=1 cHi            i

                   ( 2h  +  2        )(ni=1  c2i  -  n1 (i2=1 cH   i  )2)

                              H               H

and the test would be

     2 | wH | > N 1 1 (0, 1)                                                                                   (4.76)
4. Adjustment and validation of networks                                                                              107

where the level of significance 1 is coupled to that of  by  (1, 1, ) =  (, n - 1, ),
so as to ensure that both tests (4.73) and (4.76) would have an equal probability  of
correct rejection.

    If we want to test for an error in the coordinate set of the h-system instead of in the
H-system, then one would need to choose instead of (4.74), the c-vector c = (cTh , 0T )T .
But for ch = cH, the two test statistics wh and wH would become identical apart from
a change in sign (verify yourself). This simply implies that one is not able to identify

whether the model error originated from the h-system or from the H-system. This is

much like the situation where an error in a sum or difference can not be unambiguously

assigned to either components of the sum or difference. This is also the reason why it
is of importance to have a well-tested free network before the connection is carried out.

Because if then during the testing of the connection model, rejection occurs, one can

confidently assume that the error is present in the coordinate set of the control network,

in our case the coordinate set of the H-system.

Data snooping: If we want to screen the individual Hi coordinates for the presence of
blunders, the cH-vector takes the form

      cH = (0, . . ., 0, 1, 0, . . ., 0)T

          i

and the above wH test statistic reduces to

      wHi = (Hi - hi) - (H» - h» )                1                                                                (4.77)
                                                  n
                   ( 2h     +     2     )(1   -      )

                                    H

The size of the blunder that can be found with a probability  = 0.80 using this test statistic
with a level of significance 1 = 0.001, is given by the MDB,

      | i |=       1  7  .  0  7  5  ( 2h  +  2      )                                                             (4.78)

                                                H

                                  1 - 1n

This is the MDB which we already met earlier in the section on Reliability.

Testing for scale: If one suspects that the model was rejected due to the erroneous as-
sumption that scale is absent, the appropriate c- vector for testing for the presence of scale,
is

      c = (cTh , 0T )T         with      ch   =   (H1o,    .  .  .,  H o)T

                                                                       n

The size of the scale factor that then can be found with probability  = 0.80, is given by
the MDB

      |  |=        1  7     .  0  7  5  ( 2h  +  2      )                                                          (4.79)

                                                   H

                   in=1           (H o   -    H» 0  )2

                                      i         i

This  shows  that  the   presence             of  scale       become        better  identifiable,  when  ni=1(Hio  -  H» 0)2

                                                                                                                        i

is large, that is, when the network would have large height differences with respect to the

average height.
108                                                    Network Quality Control

4.5 Summary

As a conclusion to this chapter, we have summarized the main steps involved in four
flow diagrams. In figure 4.2, the adjustment and testing steps are shown for both the free
network and the connected network (note: the constrained connection is also referred
to as a pseudo least-squares connection). The model used for the connection is usually

     observations                 model
                                  formulation

                                  free network
                                  adjustment

                        rejected  testing
                                  observations
     coordinates
     known control                           accepted
     points
                                  least-squares
                                  connection
                                  adjustment

                   rejected       testing
                                  connection
                                  points               resulting
                                                       coordinates
                                             accepted

                                  pseudo
                                  least-squares
                                  connection

Figure 4.2 Flow diagram of free and connected network phase.

based on a coordinate transformation. After all, the coordinates of the free network may
not be defined in the same coordinate system as those of the control network. In case
of connecting a GPS-based free network to existing control, one is dealing with WGS84
coordinates and with the coordinates of the National Reference System (NatRS). The
model for the connection can be formulated in two ways. Either one formulates it so that
the results are expressed in the WGS84 coordinate system, or one formulates it so that the
results are expressed in the NatRS coordinate system. The first formulation is shown in
figure 4.3 and the second formulation is shown in figure 4.4. With the first formulation
one of course will need an additional (back) transformation step to get the final results
expressed in the NatRS coordinate system.

    The various steps involved in the transformation between the two systems are shown
in figure 4.5.
4. Adjustment and validation of networks                                                            109

                                                 known control   GPS baselines
                                                 points in       in WGS84
                                                 local system
                                                 transformation  least-squares
                                                 local to WGS84  adjustment
                                                 known control   and testing
                                                 points in
                                                 WGS84           pseudo
                                                                 least-squares
                                                 transformation  adjustment
                                                 WGS84 to local
                                                 coordinates in  coordinates in
                                                 local system    WGS84

Figure 4.3 Connection adjustment and testing in WGS84 system.

                                                                                     GPS baselines
                                                                                     in WGS84

                                                                 transformation
                                                                 WGS84 to local

known control                                                    GPS baselines
points in                                                        in local system
local system
                                                                 least-squares
                                                                 adjustment
                                                                 and testing

                                                                 pseudo
                                                                 least-squares
                                                                 adjustment

                                                                 coordinates in
                                                                 local system

Figure 4.4 Connection adjustment and testing in NatRS or local system.
110                                                                             Network Quality Control

     GPS baseline   3D similarity                                               X-Y-Z
     network        transformation                                              in NatRS

     X-Y-Z                                                                      transformation
     in WGS84                                                                   to ellipsoidal
                                                                                coordinates
     geoid heights  H=h-N                                                       ellipsoidal
                                                                                coordinates
                                                                   orthometric  in NatRS
                                                                   heights
                                                                                map projection
Figure 4.5 Datum transformation WGS84 - NatRS.
                                                                                North - East
                                                                                in NatRS
Appendix A

Appendix

In this appendix, we consider the first two moments of a random variable, the mean and
the variance. We also show how they propagate when functions of the random variable
are taken.

    First we consider the mean of a scalar random variable and give the propagation law
of the mean when the function is either linear or nonlinear. In the latter case, use is made
of a linearization. Then we do the same for the variance of a scalar random variable. After
the scalar case has been treated, we generalize to the vectorial case.

A.1 Mean and variance of scalar random variables

Mean of a scalar random variable: Let x be a continuous scalar random variable with
probability density function px(x). The expectation or mean of x is by definition the
integral

                 +                                                        (A.1)

E{x} = xpx(x)dx

               -

This number will also be denoted by mx.

Mean of a function of a scalar random variable: Given a scalar random variable x and
a function f (x), we form the random variable y = f (x). As we see from (A.1), the mean
of y is given by

                 +                                                        (A.2)

E{y} = ypy(y)dy

               -

It appears therefore, that to determine the mean of y, we must first find its probability
density function py(y). This, however, is not necessary. As the next theorem shows, E{y}
can be expressed directly in terms of the function f (x) and the density px(x) of x.

Theorem:              +                                                   (A.3)
        E{ f (x)} =
                          f (x)px(x)dx

                     -

    Proof: We shall sketch a proof using the curve f (x) of figure A.1. With y = f (x1) =
f (x2) = f (x3) as in the figure, we see that

          P(y  y  y + dy) =
          = P(x1  x  x1 + dx1) + P(x2 - dx2  x  x2) + P(x3  x  x3 + dx3)

or

py(y)dy = px(x1)dx1 + px(x2)dx2 + px(x3)dx3

                                         111
112                                                                Network quality control

                                                            f (x)

      dy
         y

                                     d x1         d x2      d x3

                                 x1           x2        x3         x

Figure A.1 The curve y = f (x).

Multiplying by y, we obtain

        ypy(y)dy = f (x1)px(x1)dx1 + f (x2)px(x2)dx2 + f (x3)px(x3)dx3

Thus, to each differential in (A.2) there corresponds one or more differentials in (A.3). As
dy covers the y-axis, the corresponding dx's are nonoverlapping and they cover the entire
x-axis. Hence, the integrals in (A.2) and (A.3) are equal.

    It appears from (A.3) that to determine the mean of y, we must know the probability
density function px(x). In general this is true. However, in the special case that the
function f (x) is linear not the complete density of x needs to be known but only the mean
mx of x.

Theorem (propagation law of the mean): Given a scalar random variable x and a func-
tion f (x), we form the random variable y = f (x). If the function f (x) is linear,

      f (x) = ax + b                                                  (A.4)

then

      my = amx + b                                                    (A.5)

Proof: Substitution of (A.4) into (A.3) gives

      +
      my = - (ax + b)px(x)dx
                      +                    +
      = a - x px(x)dx + b - 1 px(x)dx

      = amx + b

If the function f (x) is nonlinear we need strictly speaking the density px(x) of x in order
to compute the mean of y = f (x). However, by using Taylor's formula an approximation

to the mean of y can be derived that gets round the difficulty of having to know the density
px(x) of x.

Theorem (linearized propagation law of the mean): Given a scalar random variable
x and a nonlinear function f (x), we form the random variable y = f (x). Let x0 be an
Appendix                                                                                  113

approximation to a sample of x and define y = y - f (x0) and x = x - x0. Then a first-
order approximation to the mean of y is

           .d 0                                                                           (A.6)
     my = f (x )mx

             dx

     Proof: Application of Taylor's formula gives

          0d0                          0 1 d2 0                            02
     y = f (x ) + f (x )(x - x ) + 2 f (x )(x - x ) + . . .
               dx                               2 dx

If we take the expectation we get

               0d0                                  0         1 d2 0           02
     E{y} = f (x ) + f (x )E{(x - x )} + 2 f (x )E{(x - x ) } + . . .
                      dx                                      2 dx

This may be written with

     my = E{y} - f (x0) , mx = E{(x - x0)}

and

     E{(x - x0)2} = E{ (x - mx) - x0 - mx) 2}

                      = E{(x - mx)2} - 2E{(x - mx)(x0 - mx)} + e{(x0 - mx)2}

                      =   E    {   (x  -  m  x  )2  }  +   m  2  x
                                                              
                          2            m2
                      =            +
                            x            x

as

          d0                   1 d2 0 2 2
     my = f (x )mx + 2 f (x ) x + mx + . . .
          dx                   2 dx

If we neglect the second and higher order terms, the result (A.6) follows.

     Examples

     1. With f (x) = cos x, we define the random variable y = f (x).               Since

          d f (x0) = - sin x0, we find to a first order
          dx

               E  {y  -   cos  x0  }  .  -   sin    x0  E  {x  -    x0  }
                                      =

     2. With f (x) = x3, we define the random variable y = f (x). Since d f (x0) = 3(x0)2
                                                                                    dx

        we find to a first order

               E  {y  -   (x0  )3  }  .  3(x0   )2  E   {x  -    x0  }
                                      =

Variance of a scalar random variable: The variance of a scalar random variable is by
definition the integral

                                        +                                                 (A.7)

     E{(x - E{x})2} = (x - mx)2 px(x)dx

                                      -
114                                                                                                 Network quality control

This number will also be denoted by x2. The positive constant x is called the standard
deviation of x. From the definition it follows that

         2x = E{(x - mx)2} = E{x2} - 2E{x}mx + m2x

or

       2x = E{x2} - m2x                                                                                              (A.8)

Theorem (propagation law of the variance): Given a scalar random variable x and a
function f (x), we form the random variable y = f (x). If the function f (x) is linear,

      f (x) = ax + b                                                                                                 (A.9)

then

      2      =  a2 2                                                                                                 (A.10)

        y             x

    Proof: According to definition (A.7) we have

      2      =  E{(y     -  E{y})2}        =    E{(  f  (x)  -  my)2}

        y

With (A.3) this gives

                   +

       2y = ( f (x) - my)2 px(x)dx

                 -

Substitution of (A.5) and (A.9) gives

      2         =        +     [(ax  +  b)      -  (amx  +   b)]2    px  (x)d       x
                         -
        y       = a2 -+(x - mx)2 px(x)dx

                =        a2 2

                               x

The   above  result  shows        that  if  f (x)  is   linear,   knowledge            of  2    is  sufficient  for  computing

                                                                                             x

the variance of y = f (x). For nonlinear functions f (x) this is generally not true. If the

function f (x) is nonlinear one will generally need to know the complete density px(x)

of x. However, by using Taylor's formula an approximation to the variance of y can be

derived that gets round the difficulty of having to know px(x).

Theorem (Linearized propagation law of the variance): Given a scalar random vari-
able x and a nonlinear function f (x), we form the random variable y = f (x). Let x0 be an

approximation to a sample of x. Then a first-order approximation to the variance of y is

        2.          d 0 2 2                                                                                          (A.11)
      y =              f (x ) x

                    dx

    Proof: Substitution of

             f (x)   =      f     (x0)  +   d   f  (x0)(x  -  x0  )  +  .  .  .
                                            dx
                                  (x0)      d      (x0)(mx   -   x0)
           mx        =      f           +   dx  f                     +    .  .  .
Appendix                                                                           115

into

      2           +  ( f (x) - my)2 px(x)dx

        y  =  -

gives after neglecting second and higher order terms

      2              + d 0                     2
              =      - dx f (x )(x - mx) px(x)dx
        y
                     ddx f (x0) 2 -+(x - mx)2 px(x)dx
              =

              =      d 022

                     dx f (x ) x

      Examples

      1. A first order approximation to the variance of y = cos x is

                       2. 20 2
                     y = sin x x

      2. A first order approximation to the variance of y = x3 is

                     2    .  9(x0 )4 x2
                          =
                       y

A.2 Mean and variance of vector random variables

Mean of a vector random variable: Let xi, i = 1, 2, . . ., n be n continuous scalar random

variables with joint probability density function px(x1, x2, . . ., xn). The expectation or
mean of the random n-vector (x1, x2, . . ., xn)T is by definition the integral

              x1                           x1  
                                               
              x2           +        +      x2  
                  } =                           px(x1, x2, . . ., xn)dx1 . . .dxn
      E{ .                     иии       .                                         (A.12)
            ..                      -  ..
                          -

              xn                           xn

If we use the notation

      x = ( x1 x2 и и и xn )T , x = ( x1 x2 и и и xn )T

      px(x) = px(x1, x2, . . ., xn)      , dx = dx1 . . .dxn
                     +       +
      and = и и и
                     -       -

we may write (A.12) in the compact form:

      E{x} = xpx(x)dx                                                              (A.13)

From (A.12) follows that

                     +       +
      E{xi} = - и и и - xi px(x1, x2, . . ., xn)dx1 . . .dxn
                                                                                   (A.14)
116                                        Network quality control

This result seems to contradict definition (A.1). Note however that (A.14) reduces to
(A.1), since

               +

          - px(x1, . . ., x j, . . ., xn)dx j = px(x1, . . ., x j-1, x j+1, . . ., xn)

Hence, (A.12) may also be written as

                     +                   
        x1           - x1 px(x1)dx1
                     -+ x2 px(x . 2)dx2 
        x2         
      E{ . } =                                                                (A.15)
       ..                       ..       

        xn           +

                     - xn px(xn)dxn

Thus in order to compute E{xi} one only needs the marginal density pxi (xi).

Mean of a vectorfunction of a random vector: Given a random n-vector x and a vec-
torfunction F(x), F : Rn  Rm we form the random m-vector y = F(x). As we see from

(A.15) the mean of yi is given by

                        +                                                     (A.16)

      E{yi} = - yi pyi (yi)dyi

It appears, therefore, that to determine the mean of yi, we must first find its marginal

probability density function pyi(yi). This, however, is not necessary. As the next theorem
shows, E{yi} can be expressed directly in terms of the vectorfunction F(x) and the joint
density of x.

Theorem:            +         +                                               (A.17)
        E{F(x)} =                                                             (A.18)
                        иии  - F(x1, . . ., xn)px(x1, . . ., xn)dx1 . . .dxn
or
                   -

      E{F(x)} = F(x)px(x)dx

Proof: The proof is similar to the proof given in the earlier section of this appendix.

    The following theorem is an extremely important one, and it is used frequently in
these lecture notes.

Theorem (propagation law of the mean): Given a random n-vector x and a vectorfunc-
tion F(x), F : Rn  Rm, we form the random m-vector y = F(x). If the vectorfunction

F(x) is linear,

      F(x) = A x + b                                                          (A.19)

      mО1          mОnnО1           mО1

then

      my = A mx + b                                                           (A.20)

      mО1          mОnnО1           mО1
Appendix                                                                                                        117

    Proof: If we denote the n columnvectors of matrix A by ai, i = 1, . . ., n, we may write
(A.19) as

                  n

    F(x) =  aixi + b

                i=1

If we substitute this into (A.17) we get

                         n        +           +

    E{F(x)} =  ai и и и xi px(x1, . . ., xn)dx1 . . .dxn
                         i=1 -                -

                               +              +
                         +b - и и и - px(x1, . . ., xn)dx1 . . .dxn

or

    E {F (x)}   =        n        +
                         i=1 ai - xi pxi (xi)dxi + b

                = in=1 aimxi + b

                        = Amx + b
Without proof we also give the linearized version of the propagation law of the mean.

Theorem (linearized propagation law of the mean): Given a random n-vector x and a

nonlinear vectorfunction F(x), F : Rn  Rm we form the random m-vector y = F(x). Let

x0  Rn be an approximation to a sample of x and define y = y - F(x0) and x = x - x0.

Then we have to a first-order

    my = xF(x )mx.0                                                                                (A.21)

    Examples

    1. Let the two random variables y1 and y2 be defined as

                     y1 = 1 x1 + 3 x2 + 5 x3 + 2
                     y2 = 4 x1 + 2 x2 - 1 x3 + 3

          Then                                                   

                E{ y1 } = 1 3                           5 E{ x } + x1 2

                                                                          2
                         y2 4 2 -1 x3 3

    2. Let the two random variables y1 and y2 be defined as

                     y1 = sin(x1 x2) + x3
                            =     2  +        +
                     y2        x  1     x  2     4

          With  the  approximate     values      x01 ,  x0  and  x0  we  have  to  a  first-order

                                                         2        3
          E{ y1 - sin(x0 0 21 x02) - x0 0 3 } = x02 cos(x001 x02) x01 cos(x01 x02) 1  x1 - x1 E{ x2 - x00  2 }
              y2 - (x1) - x2 - 4                        2x1 1 0 x3 - x30
118                                                                       Network quality control

Variancematrix of a random vector: Let xi, i = 1, 2, . . ., n be n continuous scalar random

variables with joint probability density function px(x1, . . ., xn). The variancematrix of the
random n-vector (x1, x2, . . ., xn)T is by definition the integral
      x1 - E{x1}   x1 - E{x1} T
                 ..                  ..        
     E{            .                   .        }=

          xn - E{xn}            xn - E{xn}

                                                                 T        (A.22)

          + +  .   .  x1 - E{x1} x1 - E{x1}
     = - и и и -  ..   ..  px(x1, . . ., xn)dx1 . . .dxn
                           xn - E{xn}             xn - E{xn}

    This variancematrix will also be denoted by Qx. Using vector notation we may write

(A.22) in the compact form

     E{(x - mx)(x - mx)T } = (x - mx)(x - mx)T px(x)dx                    (A.23)

    From (A.22) it follows that
         E{(xi - mxi )(x j - mx j )} =

              +          +
     = - и и и - (xi - mxi )(x j - mx j )px(x1, . . ., xn)dx1 . . .dxn

Integration gives

                                                     + +

     E{(xi - mxi )(x j - mx j )} = - - (xi - mxi)(x j - mx j )px(xi, x j)dxidx j (A.24)

in which px(xi, x j)dxidx j is the joint density function of the two random variables xi and

x j. The scalar (A.24) is called the covariance of the two random variables xi and x j.
This covariance will also be denoted as xix j. Note that the off-diagonal elements of the
variancematrix Qx consist of the covariances between the elements of the random vector

x.

    If i = j it follows from (A.24) that

     E{(xi - mxi )2} =       + +          (xi - mxi)2 px(xi, x j)dxidx j

                            - -

Integration gives

     E{(xi - mxi )2} =       +       (xi - mxi )2 pxi (xi)dxi

                            -

This is the variance of xi. Note that the variance x2i is the ith-diagonal element of the
variancematrix Qx. Hence, the variancematrix Qx can be written as
                          2                               
                                     x1x2    и и и x1xn
                                 x1
                                          2                    
                          x2x1 x                               
          Qx          =                   2                               (A.25)
                             .                                 
                          ..                   .  ..           

          nОn

                            xnx1                      2

                                                        xn
Appendix                                                                               119

Note that the variancematrix is a symmetric matrix.

Theorem (propagation law of variances): Given a random n-vector x and a vectorfunc-
tion F(x), F : Rn  Rm, we form the random m-vector y = F(x). If the vectorfunction
F(x) is linear,

       F(x) = A x + b                                                                  (A.26)

       mО1          mОn nО1        mО1

then

       Qy = A Qx AT                                                                    (A.27)

       mОm          mОn nОn nОn

    Proof: If we denote the n columnvectors of matrix A by ai, i = 1, . . ., n, we may write
(A.26) as

                     n                                                                 (A.28)

       F(x) =  aixi + b

                   i=1

In a similar way we may write (A.20) as

                  n                                                                    (A.29)

       my =  aimxi + b

                i=1

Substitution of (A.28) and (A.29) into

       Qy = E{(y - my)(y - my)T }

                    +         +
                          и и и (F(x) - my)(F(x) - my)T px(x1, . . ., xn)dx1 . . .dxn
             =      -         -

and using

                +         +
       xix j =      и и и (xi - mxi )(xi - mxi )T px(x1, . . ., xn)dx1 . . .dxn
                -         -

gives

                  nn

       Qy =   aiaTj xix j

                i=1 j=1

This can be written as  
                                   n       x               a  T
                                   j=   1         1  x  j     j  

       Qy =     a1  иии   an               ...                   
                                                                 

                                   n    1  x      n  x  j  a  T
                                   j=                         j

or as                                      x1x1 и и и x1xn               T 
          Qy =                                                          a1
                                                ..                  ..    .. 
                    a  1  и и и an                .                .     .

                                           xnx1 и и и xnxn              aT

                                                                          n

             = AQxAT
120                                                                Network quality control

Without proof we also give the linearized version of the propagation law of variances.

Theorem (linearized propagation law of variances): Given a random n-vector x and a
nonlinear vectorfunction F(x), F : Rn  Rm, we form the random m-vector y = F(x). Let
x0  Rn be an approximation to a sample of x. Then we have to a first order

     Qy   .    xF (x0 )  Qx          xF (x0 )T                              (A.30)
          =

     mОm       mОn nОn nОm

     Examples

     1. Let the two random variables y1 and y2 be defined as

               y1 = 1 x1 + 3 x2 + 5 x3 + 2
               y2 = 4 x1 + 2 x2 - 1 x3 + 3

     The variancematrix of x = (x1, x2, x3)T is given as
                                     
                     110

               Qx =  1 2 0 

                     001

     Then                                                               
               Qy =                                    110 1        4
                            13                 5  1 2 0  3          2

                            4 2 -1 0 0 1 5                         -1

                   = 50 25
                           25 41

     2. Let the two random variables y1 and y2 be defined as

               y1    =   x  1  +  x  2  +   x1  x3                        (A.31)
                                     2                                    (A.32)
                            3
               y2    =   x  1  +  si  n  x  2

     The variancematrix of x = (x1, x2, x3)T is given as
                                     
                     100

               Qx =  0 1 0 

                     002

     The 2 О 3 matrix of partial derivatives xF(x0) follows from (A.31) as

                                      x1F1(x0) x2F1(x0) x3F1(x0) 
               xF (x0 )     =                                             

               2 О 3 x1F2(x0) x2F2(x0) x3F2(x0) (A.33)

                            =            1     +  x0  2x02     x0

                                                   3            1
                                                02          0
                                         3(x1) cos x2 0
Appendix                                                                                           121

          If  we  take  as  approximate  values  x0  =  1,  x0  =  0,  x0  =  0,  (A.33)  becomes

                                                  1          2          3

                  xF(x0) = 1 0 1                                                          (A.34)
                                  310

          The variancematrix Qy follows now from (A.32) and (A.34) to a first-

          order as                                                                
                           .                   100 13
                              1 0 1  0 1 0  0 1 
                    Qy =      310 002 10

                            . 33
                            =

                                    3 10
Appendix B

References

In order to aid a further study in the field of adjustment, testing and computation of geode-
tic networks, the list of references given below gives an introductory overview of the
existing international literature. With a few exceptions, the list concentrates on books,
lectures notes and reports.

Adjustment and testing

 [1] Baarda, W. (1967): Statistical Concepts in Geodesy, Netherlands Geodetic Commis-
       sion, Publications on Geodesy, New Series, Vol. 2, No. 4, Delft.

 [2] Baarda, W. (1968): A Testing Procedure for use in Geodetic Networks, Netherlands
       Geodetic Commission, Publications on Geodesy, New Series, Vol. 2. No. 5, Delft.

 [3] Grafarend, E., B. Schaffrin (1993): Ausgeleichungsrechnung in Linearen Modellen,
       B.I. Wissenschaftsverlag.

 [4] Graybill, F.A. (1976): Theory and Application of the Linear Model, Duxbury Press.

 [5] Koch, K.-R. (1987): Parameterschaеtzung und Hypothesestests in linearen Modellen,
       2nd edition, Duеmmler Verlag, Bonn.

 [6] Meissl, P. (1982): Least Squares Adjustment: A Modern Approach, Mitteilungen der
       Geodaеtischen Institute der Technische Universitaеt Graz. Folge 43.

 [7] Mikhail, E. M. (1976): Observations and Least Squares, University Press of Amer-
       ica.

 [8] Rao, C.R. (1973): Linear Statistical Inference and its Applications, 2nd edition,
       Wiley series in probability and mathematical statistics.

 [9] Teunissen, P.J.G. (2001): Adjustment Theory: an introduction, 2nd edition, Series
       on Mathematical Geodesy and Positioning, Delft University Press, ISBN 90-407-
       1974-8.

 [10] Teunissen, P.J.G. (2000): Testing Theory: an introduction, Series on Mathematical
       Geodesy and Positioning, Delft University Press, ISBN 90-407-1975-6.

    The first two references concentrate on testing theory. Much of the quality control
procedures which have become standard practice nowadays, can be traced back to these
two original works. The references [3], [4], [5] and [8] are reference books, whereas the
references [6], [7], [9] and [10] are more of the lecture notes type. Reliability theory is
only delt with in the first two and in the last reference.

                                                   123
124  Network Quality Control

Surveying and geodetic networks

 [1] Alberda, J.E. (1974): Planning and Optimization of Networks: Some General Con-
       siderations. Boll. Geod. Sc. Aff., 33, pp. 209-240.

 [2] Alberda, J.E. (1981): Inleiding Landmeetkunde, 3rd edition, Delft University Press.

 [3] Baarda, W. (1973): S-transformations and Criterion Matrices, Netherlands Geodetic
       Commission, Publications on Geodesy, New Series, Vol. 5, No.1, Delft.

 [4] Borre, K. (1990): Landmaeling, Aalborg.

 [5] Delft Geodetic Computing Centre (Eds.) (1982): Forty Years of Thought. Anniver-
       sary volume (Vol. 1 and 2) on the occasion of prof. Baarda's 65th birthday, Delft.

 [6] Grafarend, E., Sanso (Eds.) (1985): Optimization and Design of Geodetic Networks,
       Springer Verlag.

 [7] Grafarend, E., B. Schaffrin (1974): Unbiased free net adjustment. Survey review, 22,
       pp. 200-218.

 [8] Heck, B. (1987): Rechenverfahren und Auswertemodelle der Landesvermessung,
       Herbert Wichmann Verlag.

 [9] Kahmen, H., W. Faig (1988): Surveying, Walter de Gruyter.

 [10] Mierlo van, J. (1979): Free Network Adjustment and S- transformations, DGK B,
       No. 252, pp. 41-54.

 [11] Moffitt, F.H., H. Bouchard (1982): Surveying, 7th edition, Harper and Row.

 [12] Richardus, P. (1984): Project Surveying, Balkema.

 [13] Teunissen, P.J.G. (1984): Generalized Inverses, The Datum problem and S-transform-
       ations, Mathematical and Physical Geodesy Report 84.1, Delft, pp. 44.

    The references [2], [4], [8], [9], [11], and [12], are all textbooks on surveying. The
references [2], [4], [9] and [11], are of an introductory nature. Reference [8] concentrates
on functional models, in particular spatial modelling and reference [12] includes aspects
of quality control. Optimization and design aspects of geodetic networks are treated in
their whole range of variety in the references [1], [5] and [6]. Free networks are treated
in particular in [3], [7], [10] and [13]. The concept of S-transformations was introduced
in [3] and its relation to generalized inverses is included in [10] and [13].

GPS Surveying

 [1] Husti, G.J. (2000): Global Positioning System (in Dutch), Series on Mathematical
       Geodesy and Positioning, Delft University Press, ISBN 90-407-1977-2.
References  125

[2] Hofmann-Wellenhof, B., H. Lichtenegger, J. Collins (2001): Global Positioning Sys-
      tem: Theory and Practice, 6th edition, Springer Verlag.

[3] Leick, A. (2005): GPS Satellite Surveying, 3rd edition, John Wiley and Sons.

[4] Seeber, G. (2003): Satellite Geodesy, 2nd edition, Walter de Gruyter.

[5] Teunissen, P.J.G. and A. Kleusberg (1998): GPS for Geodesy, 2nd edition, Springer
      Verlag.
Network quality control

Peter J.G. Teunissen

The aim of computing a geodetic network is to determine the geometry of the configuration of a set of points
from spatial observations (e.g. GPS baselines and/or terrestrial measurements). The configuration of points
usually consists of newly established points, of which the coordinates still need to be determined, and already
existing points, the so-called control points, of which the coordinates are known.

Network quality control deals with the qualitative aspects of network design, network adjustment, network
validation and network connection. By means of a network adjustment the relative geometry of the new points is
determined and integrated into the geometry of the existing control points. Prior to the network adjustment, the
geometry of the network is designed on the basis of precision and reliability criteria.

The adjustment and validation of the overall geometry can be divided in two phases, the free network phase
and the connected network phase. In the free network phase, the known coordinates of the control points do not
take part in the adjustment and validation. The possible use of a free network phase is based on the idea that a
good geodetic network should be sufficiently precise and reliable in itself, without the need of external control.
Moreover, it allows one to validate the quality of the external control.

In the connected network phase, the geometry of the free network is integrated into the geometry of the control
points. Adjustment and validation in this second phase differs from the free network phase. The adjustment in
the second phase is a constrained connection adjustment, since it is often not practical to see the coordinates
of the control points change every time a free network is connected to them. For the validation of the connected
network however, the unconstrained connected adjustment is used as input. This allows one to take the intrinsic
uncertainty of the coordinates of the control points in the connection phase into account.

The goal of this introductory text on network quality control is to convey the necessary knowledge for designing,
adjusting and testing geodetic networks. For the purpose of network design, the precision and reliability theory
is worked out in detail. This includes the minimal detectable biases and the bias-to-noise ratios. For the purpose
of the network adjustment, the principles of unconstrained-, constrained-, and minimally constrained least-
squares estimation, are treated. For the network testing, the principles of hypothesis testing are presented and
worked out for the different network cases. For the free network phase this includes the overall model test,
the w-test, and the data snooping procedure. For the connected network phase, it includes the T-test, with an
emphasis on the detection and identification of errors in the control points.

P.J.G. Teunissen                                          Е 2024 TU Delft Open
Delft University of Technology,                           ISBN 978-94-6366-950-4
Faculty of Civil Engineering and Geosciences              DOI https://doi.org/
                                                          10.59490/tb.100
Dr (Peter) Teunissen is Professor of Geodesy at Delft     textbooks.open.tudelft.nl
University of Technology (DUT) and an elected member
of the Royal Netherlands Academy of Arts and Sciences.    Cover image: A. Smits
He is research-active in various fields of Geodesy, with
current research focused on the development of theory
, models, and algorithms for high-accuracy applications
of satellite navigation and remote sensing systems.
His past DUT positions include Head of the Delft Earth
Observation Institute, Education Director of Geomatics
Engineering and Vice-Dean of Civil Engineering and
Geosciences. His books at TUDelft Open are Adjustment
Theory, Testing Theory, Dynamic Data Processing and
Network Quality Control.
