Adjustment theory: an introduction

Peter J.G. Teunissen
Adjustment theory

     an introduction
         Adjustment theory

                an introduction

                  P.J.G.Teunissen

                Delft University of Technology
     Faculty of Civil Engineering and Geosciences
Department of Mathematical Geodesy and Positioning
Series on Mathematical Geodesy and Positioning
© VSSD, first edition 2000-2006
© TU Delft OPEN Publishing, second edition 2024
ISBN: 978-94-6366-884-2 (Ebook)
ISBN: 978-94-6366-885-9 (Paperback/softback)
Doi: https://doi.org/10.59490/tb.95

This work is licensed under a Creative Commons Attribution 4.0 International license

Keywords: adjustment theory, geodesy
Preface 2nd Edition

To promote open access, this new edition of Adjustment Theory is published by
TU Delft Open Publishing instead of Delft Academic Press.
Appendix G has been added to this edition. It places the origin of Adjustment
Theory in its historical context and provides a brief account of the early methods
of adjusting geodetic and astronomical observations.

May, 2024
Peter J.G. Teunissen
Preface

As in other physical sciences, empirical data are used in Geodesy and Earth Observation to make
inferences so as to describe the physical reality. Many such problems involve the determination
of a number of unknown parameters which bear a linear (or linearized) relationship to the set of
data. In order to be able to check for errors or to reduce for the effect these errors have on the
final result, the collected data often exceed the minimum necessary for a unique determination
(redundant data). As a consequence of (measurement) uncertainty, redundant data are usually
inconsistent in the sense that each sufficient subset yields different results from another subset.
To obtain a unique solution, consistency needs to be restored by applying corrections to the data.
This computational process of making the data consistent such that the unknown parameters can
be determined uniquely, is referred to as adjustment. This book presents the material for an
introductory course on Adjustment theory. The main goal of this course is to convey the
knowledge necessary to perform an optimal adjustment of redundant data. Knowledge of linear
algebra, statistics and calculus at the undergraduate level is required. Some prerequisites,
repeatedly needed in the book, are summarized in the Appendices A-F.
Following the Introduction, the elementary adjustment principles of 'least-squares', unbiased
minimum variance' and 'maximum likelihood' are discussed and compared in Chapter 1. These
results are generalized to the multivariate case in chapters 2 and 3. In Chapter 2 the model of
observation equations is presented, while the model of condition equations is discussed in
Chapter 3. Observation equations and condition equations are dual to each other in the sense that
the first gives a parametric representation of the model, while the second gives an implicit
representation. Observables that are either stochastically or functionally related to another set of
observables are referred to as yR-variates. Their adjustment is discussed in Chapter 4. In Chapter
5 we consider mixed model representations and in Chapter 6 the partitioned models. Models may
be partitioned in different ways: partitioning of the observation equations, partitioning of the
unknown parameters, partitioning of the condition equations, or all of the above. These different
partitionings are discussed and their relevance is shown for various applications. Up tot this point
all the material is presented on the basis of the assumption that the data are linearly related. In
geodetic applications however, there are only a few cases where this assumption holds true. In
the last chapter, Chapter 7, we therefore start extending the theory from linear to nonlinear
situations. In particular an introduction to the linearization and iteration of nonlinear models is
given.

Many colleagues of the Department of Mathematical Geodesy and Positioning whose assistance
made the completion of this book possible are greatly acknowledged. C.C.J.M. Tiberius took care
of the editing, while the typing was done by Mrs. J. van der Bijl and Mrs. M.P.M. Scholtes, and
the drawings by Mr. A. Smit. Various lecturers have taught the book's material over the past
years. In particular the feedback and valuable recommendations of the lecturers C.C.J.M.
Tiberius, F. Kenselaar, G.J. Husti and H.M. de Heus are acknowledged.

P.J.G. Teunissen
April, 2003
Contents

Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1

1 Linear estimation theory: an introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5

   1.1    Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5

   1.2    Least-squares estimation (orthogonal projection) . . . . . . . . . . . . . . . 5

   1.3    Weighted least-squares estimation . . . . . . . . . . . . . . . . . . . . . . . . 11

   1.4    Weighted least-squares (also orthogonal projection) . . . . . . . . . . . . 20

   1.5    The mean and covariance matrix of least-squares estimators . . . . . . 25

   1.6    Best Linear Unbiased Estimators (BLUE's) . . . . . . . . . . . . . . . . . 28

   1.7    The Maximum Likelihood (ML) method . . . . . . . . . . . . . . . . . . . 32

   1.8    Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

2 The model with observation equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

   2.1    Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

   2.2    Least-squares estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

   2.3    The mean and covariance matrix of least-squares estimators . . . . . . 45

   2.4    Best Linear Unbiased Estimation . . . . . . . . . . . . . . . . . . . . . . . . 49

   2.5    The orthogonal projector A(A Qy 1A) 1A Qy 1 . . . . . . . . . . . . . . . 55

   2.6    Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59

3 The model with condition equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61

   3.1    Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61

   3.2    Best Linear Unbiased Estimation . . . . . . . . . . . . . . . . . . . . . . . . 64

   3.3    The case B E y b 0, b 0 { 0 . . . . . . . . . . . . . . . . . . . . . . . . . . . 67

   3.4    Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68

4  yR-Variates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71

   4.1    Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71

   4.2    Free variates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75

   4.3    Derived variates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77

   4.4    Constituent variates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77

5  Mixed model representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81

   5.1    Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81

   5.2    The model representation B E y Ax . . . . . . . . . . . . . . . . . . . . 82

   5.3    The model representation E y Ax, B x 0 . . . . . . . . . . . . . . . . 84
6 Partitioned model representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89

6.1    Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89

6.2    The partitioned model E y (A1x1 A2x2) . . . . . . . . . . . . . . . . . . . . 90

6.3    The partitioned model E (y ,y )                             (A1 A2 ) x . . . . . . . . . . . . . 100

                                                               12

       (recursive estimation)

6.4    The partitioned model E (y ,y )                             (A1 A2 ) x . . . . . . . . . . . . . 109

                                                               12

       (block estimation I)

6.5    The partitioned model E (y ,y )                             (A1 A2 ) x . . . . . . . . . . . . . 113

                                                               12

       (block estimation II)

6.6    The partitioned model (B1 B2) E y (0 0) . . . . . . . . . . . . 123

       (estimation in phases)

7 Nonlinear models, linearization, iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137

7.1    The nonlinear A-model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137

7.1.1  Nonlinear observation equations . . . . . . . . . . . . . . . . . . . . . . . . 137

7.1.2  Linearization: Taylor's theorem . . . . . . . . . . . . . . . . . . . . . . . . 142

7.1.3  The linearized A-model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146

7.1.4  Least-squares iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150

7.2    The nonlinear B-model and its linearization . . . . . . . . . . . . . . . . 154

Appendices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159

A Taylor's theorem with remainder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
B Unconstrained optimization: optimality conditions . . . . . . . . . . . . . . . . . . . . . . 164
C Linearly constrained optimization: optimality conditions . . . . . . . . . . . . . . . . . . 168
D Constrained optimization: optimality conditions . . . . . . . . . . . . . . . . . . . . . . . . 176
E Mean and variance of scalar random variables . . . . . . . . . . . . . . . . . . . . . . . . 178
F Mean and variance of vector random variables . . . . . . . . . . . . . . . . . . . . . . . . 184
G A brief account on the early history of adjusting geodetic and

         astronomical observations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192

Literature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
Introduction

The calculus of observations (in Dutch: waarnemingsrekening) can be regarded as the part of
mathematical statistics that deals with the results of measurement. Mathematical statistics is often
characterized by saying that it deals with the making of decisions in uncertain situations. In first
instance this does not seem to relate to measurements. After all, we perform measurements in
order to obtain sharp, objective and unambiguous quantitative information, information that is
both correct and precise, in other words accurate information. Ideally we would like to have
'mathematical certainty'. But mathematical certainty pertains to statements like:

                                    if a=b and b=c and c=d, then a=d.

If a, b, c, and d are measured terrain distances however, a surveyor to be sure will first compare
d with a before stating that a=d. Mathematics is a product of our mind, whereas with
measurements one is dealing with humans, instruments, and other materialistic things and
circumstances, which not always follow the laws of logic. From experience we know that
measurements produce results that are only sharp to a certain level. There is a certain vagueness
attached to them and sometimes even, the results are simply wrong and not usable.
In order to eliminate uncertainty, it seems obvious that one seeks to improve the instruments, to
educate people better and to try to control the circumstances better. This has been successfully
pursued throughout the ages, which becomes clear when one considers the very successful
development of geodetic instruments since 1600. However, there are still various reasons why
measurements will always remain uncertain to some level:

1. 'Mathematical certainty' relates to abstractions and not to physical, technical or social
         matters. A mathematical model can often give a useful description of things and relations
         known from our world of experience, but still there will always remain an essential
         difference between a model and the real world.

2. The circumstances of measurement can be controlled to some level in the laboratory, but
         in nature they are uncontrollable and only partly descriptive.

3. When using improved methods and instrumentation, one still, because of the things
         mentioned under 1 and 2, will have uncertainty in the results of measurement, be it at
         another level.

4. Measurements are done with a purpose. Dependent on that purpose, some uncertainty is
         quite acceptable. Since better instruments and methods usually cost more, one will have
         to ask oneself the question whether the reduction in uncertainty is worth the extra costs.

Summarizing, it is fundamentally and practically impossible to obtain absolute certainty from
measurements, while at the same time this is also often not needed on practical grounds.
Measurements are executed to obtain quantitative information, information which is accurate
enough for a specific purpose and at the same time cheap enough for that purpose. It is because
of this intrinsic uncertainty in measurements, that we have to consider the calculus of
observations. The calculus of observations deals with:
2 Adjustment theory

1. The description and analysis of measurement processes.
2. Computational methods that take care, in some sense, of the uncertainty in measurements.
3. The description of the quality of measurement and of the results derived therefrom.
4. Guidelines for the design of measurement set-ups, so as to reach optimal working

         procedures.

The calculus of observations is essential for a geodesist, its meaning comparable to what
mechanics means for the civil-engineer or mechanical-engineer. In order to shed some more light
on the type of problems the calculus of observations deals with, we take some surveying
examples as illustration. Some experience in surveying will for instance already lead to questions
such as:

1. Repeating a measurement does usually not give the same answer. How to describe this
         phenomenon?

2. Results of measurement can be corrupted by systematic or gross errors. Can these errors
         be traced and if so, how?

3. Geometric figures (e.g. a triangle) are, because of 2, usually measured with redundant
         (overtallig) observations (e.g. instead of two, all three angles of a triangle are measured).
         These redundant observations will usually not obey the mathematical rules that hold true
         for these geometric figures (e.g. the three angular measurements will usually not sum up
         to p). An adjustment (vereffening) is therefore needed to obtain consistent results again.
         What is the best way to perform such an adjustment?

4. How to describe the effect of 1 on the final result? Is it possible that the final result is
         still corrupted with non-detected gross errors?

Once we are able to answer questions like these, we will be able to determine the required
measurement set-up from the given desired quality of the end product. In this book we mainly
will deal with points 1 and 3, the elementary aspects of adjustment theory.

Functional and stochastic model

In order to analyze scientific or practical problems and/or make them accessible for a quantitative
treatment, one usually tries to rid the problem from its unnecessary frills and describes its
essential aspects by means of notions from an applicable mathematical theory. This is referred
to as the making of a mathematical model. On the one hand the model should give a proper
description of the situation, while, on the other hand, the model should not be needlessly
complicated and difficult to apply. The determination of a proper model can be considered the
'art' of the discipline. Well-tried and well-tested experience, new experience from experiments,
intuition and creativity, they all play a role in formulating the model.
Here we will restrict ourselves to an example from surveying. Assume that we want to determine
the relative position of terrain-'points', whereby their differences in height are of no interest to
us. If the distances between the 'points' are not too large, we are allowed, as experience tells us,
to describe the situation by projecting the 'points' onto a level surface (niveauvlak) and by
treating this surface as if it was a plane. For the description of the relative position of the points,
we are then allowed to make use of the two-dimensional Euclidean geometry (vlakke
                                                                                                                      Introduction 3

meetkunde), the theory of which goes back some two thousand years. This theory came into
being as an abstraction from how our world was seen and experienced. But for us the theory is
readily available, thus making it possible to anticipate on its use, for instance by using sharp and
clear markings for the terrain-'points' such that they can be treated as if they were
mathematically defined points. In this example, the two-dimensional Euclidean geometry acts as
our functional model. That is, notions and rules from this theory (such as the sine- and cosine
rule) are used to describe the relative position of the terrain-'points'.
Although the two-dimensional Euclidean geometry serves its purpose well for many surveying
tasks, it is pointed out that the gratuitously use of a 'standard' model can be full of risks. Just
like the laws of nature (natuurwetten), a model can only be declared 'valid' on the basis of
experiments. We will come back to this important issue in some of our other courses.
When one measures some of the geometric elements of the above mentioned set of points, it
means that one assigns a number (the observation) according to certain rules (the measurement
procedure) to for instance an angle a (a mathematical notion). From experience we know that
a measurement of a certain quantity does not necessarily give the same answer when repeated.
In order to describe this phenomenon, one could repeat the measurements a great many times
(e.g. 100). And on its turn the experiment itself could also be repeated. If it then turns out that
the histograms of each experiment are sufficiently alike, that is, their shapes are sufficiently
similar and their locations do not differ too much, one can describe the measurement variability
by means of a stochastic variable. In this way notions from mathematical statistics are used in
order to describe the inherent variability of measurement processes. This is referred to as the
stochastic model.
In practice not every measurement will be repeated of course. In that case one will have to rely
on the extrapolation of past experience. When measuring, not only the measurement result as a
number is of interest, but also additional information such as the type of instrument used, the
measurement procedure followed, the observer, the weather, etc. Taken together this information
is referred to as the registration (registratie). The purpose of the registration is to establish a link
with past experience or experiments such that a justifiable choice for the stochastic model can
be made.
In surveying and geodesy one often may assume as stochastic model that the results of
measurement (the observations) of a certain measurement method can be described as being an
independent sample drawn (aselecte trekking) from a normal (Gaussian) distribution with a
certain standard deviation. The mathematical expectation (verwachting) of the normally
distributed stochastic variable (the observable, in Dutch: de waarnemingsgrootheid), is then
assumed to equal the unknown angle etc. From this it follows, if the three angles a, b and g of
a triangle are measured, that the expectations of the three angular stochastic variables obey the
following relation of the functional model:
4 Adjustment theory

For the individual observations themselves this is usually not the case. A sketch of the relation
between terrain situation, functional model, measurement, experiment and stochastic model is
given in figure 0.1. The functional and stochastic model together from the mathematical model.

               Figure 0.1: Diagram of the fundamental relations in adjustment theory
1 Linear estimation theory: an introduction

1.1 Introduction

Before stating the general problem, let us define a simple example that contains most of the
elements that will require our consideration.

Suppose that an unknown distance, which we denote as x, has been measured twice. The two
measurements are denoted as y1 and y2, respectively. Due to all sorts of influences (e.g.
instrumental effects, human effects, etc.) the two measurements will differ in general. That is y1
{y2 or y1 - y2 {0. One of the two measurements, but probably both, will therefore also differ
from the true but unknown distance x. Thus, y1 {x and y2 {x. Let us denote the two differences
y1 - x and y2 - x, the so-called measurement errors, as e1 and e2 respectively. We can then write:

or in vector notation:
(1)

with:

( * denotes the operation of transposition; thus y* is the transpose of y).

Our problem is now, how to estimate the unknown x from the given measurement or observation
vector y. It is to this and related questions that we address our attention. In this introductory
chapter we will restrict ourselves to the case of two observations and one unknown. In the next
section we will introduce the least-squares principle. Least-squares can be regarded as a
deterministic approach to the estimation problem. In the sections following we will introduce
additional assumptions regarding the probabilistic nature of the observations and measurement
errors. This leads then to the so-called best linear unbiased estimators and the maximum
likelihood estimators.

1.2 Least-squares estimation (orthogonal projection)

The equation y a x e , in which y and a are given, and x and e are unknown, can be

                             2x1 2x1 1x1 2x1

visualized as in figure 1.1a.
6 Adjustment theory

Figure 1.1a: x and e unknown     Figure 1.1b: x^ as estimate of x
                                                 e^ as estimate of e

From the geometry of the figure it seems intuitively appealing to estimate x as x^ , such that ax^
is as close as possible to the given observation vector y. From figure 1.1b follows that ax^ has
minimum distance to y if e^ y ax^ is orthogonal to a. That is if:

                  fl             "orthogonality"
                                 "normal equation"
                  fl             "LSQ-estimate of x"
                                 "LSQ-estimate of observations"
                  fl             "LSQ-estimate of measurement errors"
(2)                              "Sum of squares"

                  fl

                  fl

The name "Least-Squares" (LSQ) is given to the estimate x^ (a a) 1a y of x, since x^
minimizes the sum of squares (y ax) (y ax) . We will give two different proofs for this:

First proof: (algebra)

                              .  End of proof.
                                                                Linear estimation theory 7

Second proof: (calculus)

From calculus we know that x^ is a solution of the minimization problem  if:

With:                                                                    End of proof.
and:
the proof follows.
Example 1: Let us consider the problem defined in section 1.1:

With a = (1,1)* the normal equation (a*a)x^ = a*y reads:

Hence the least-squares estimate x^ = (a*a)-1a*y is:

Thus, to estimate x, one adds the measurements and divides by the number of measurements.
Hence the least-squares estimate equals in this case the arithmetic average. The least-squares
estimates of the observations and observation errors follow from y^ = ax^ and e^ = y - y^ as:

                          and                                   .

Finally the square of the length of e^ follows as:

                                                                         End of example.
8 Adjustment theory

The matrices a(a*a)-1a* and I - a(a*a)-1a* that occur in (2) are known as orthogonal projectors.
They play a central role in estimation theory. We will denote them as:
(3)
The matrix Pa projects onto a line spanned by a and along a line orthogonal to a. To see this,
note that we can write Pay with the help of the cosine rule as (see figure 1.2):

                                                    Figure 1.2
In a similar way we can show that Pa^ projects onto a line orthogonal to a and along a line
spanned by a (see figure 1.3).

                                                    Figure 1.3
If we denote the vector that is orthogonal to a as b, thus b*a=0, the projectors Pa and Pa^ can
alternatively be represented as:
(4)
To see this, consult figure 1.4 and note that with the help of the cosine rule we can write:
                                                                                                      Linear estimation theory 9

                                                    Figure 1.4
With (3) and (4) we have now two different representations of the orthogonal projectors Pa=Pb^
and Pa^=Pb. The representations of (3) are used in (2), which followed from solving equation (1)
in a least-squares sense. We will now show that the representations of (4) correspond to solving
the equation:
(5)

in a least-squares sense. Equation (5) follows from pre-multiplying equation (1) with b* and
noting that b*a=0. We know that the least-squares solution of (1) follows from solving the
minimization problem:

We have used the notation . 2 here to denote the square of the length of a vector. The
minimization problem y-ax 2 can also be written as:

                                                       subject to
The equation z=ax is the parametric representation of a line through the origin with direction
vector a. From linear algebra we know that this line can also be represented in implicit form with
the help of the normal vector b. Thus we may replace z=ax by b*z=0. This gives:

                                                     subject to
With e=y - z, this may also be written as:

                                                  subject to

The least-squares estimate e^ of the unknown e in (5) thus follows as the vector e^ which satisfies
(5) and has minimal length. Figure 1.5 shows that e^ is obtained as the orthogonal projection of
y onto the vector b. The least-squares estimates of (5) read therefore:
10 Adjustment theory

(6)

                                                    Figure 1.5
Example 2: Let us consider the problem defined in section 1.1:

The form corresponding to (5) is:

This equation expresses the fact that the difference between the two observed distances y1 and
y2 is unequal to zero in general because of the presence of measurement errors e1 and e2.
Since the vector b is given in this case as:
we have:

                                                         and
                                                             Linear estimation theory 11

The least-squares estimates read therefore:

                                                        and
which is identical to the results obtained earlier.

                                                             End of example.

1.3 Weighted least-squares estimation
Up to now, we have regarded the two observations y1 and y2 as equally accurate, and solved:

by looking for the value x^ that minimized:

The least-squares principle can however be generalized by introducing a symmetric, positive-
definite weight matrix W so that:

The elements of the 2×2 matrix W can be chosen to emphasize (or de-emphasize) the influence
of specific observations upon the estimate x^. For instance, if w11>w22, then more importance is
attached to the first observation, and the process of minimizing E(x) tries harder to make (y1-a1x)2
small. This seems reasonable if one believes that the first observation is more trustworthy than
the second; for instance if observation y1 is obtained from a more accurate instrument than
observation y2.

The solution x^ of:

is called the weighted least-squares (WLSQ) estimate of x. As we know the solution can be
obtained by solving:

and checking whether:
12 Adjustment theory

From:
follows:

and:

Since the weight matrix W is assumed to be positive-definite we have a*Wa>0 (recall from linear

algebra that a matrix M is said to be positive-definite if it is symmetric and z*Mz>0" z{0). From
the first equation we therefore get:

                   fl                                          "normal equation"
                   fl                                          "WLSQ-estimate of x"
                   fl                                          "WLSQ-estimate of y"
(7)                                                            "WLSQ-estimate of e"
                   fl                                          "weighted sum of squares"
                   fl

Compare this result with (2).

Example 3: Consider again the problem defined in section 1.1:

As weight matrix we take:

With a=(1 1)* the normal equation (a*Wa)x^=a*Wy reads:
                                                                                                     Linear estimation theory 13

Hence, the weighted least-squares estimate x^=(a*Wa)-1a*Wy is:

Thus instead of the arithmetic average of y1 and y2, as we had with W=I, the above estimate is
a weighted average of the data. This average is closer to y1 than to y2 if w11>w22. The WLSQ-
estimate of the observations, y^=ax^, is:

and the WLSQ-estimate of e, e^=y - y^, is:

Note that e^2 > e^1 if w11>w22.
Finally the weighted sum of squares e^*We^ is:

                                                                                                 End of example.
In section 1.2 we introduced the least-squares principle by using the intuitively appealing
geometric principle of orthogonal projection. In this section we introduced the weighted least-
squares principle as the problem of minimizing the weighted sum of squares (y-ax)*W(y-ax). An
interesting question is now, whether it is also possible to give a geometric interpretation to the
weighted least-squares principle. This turns out to be the case. First note that the equation:
or:

describes an ellipse. If w11=w22 and w12=0, the ellipse is a circle. If w11{w22 and w12=0, the ellipse
has its major and minor axis parallel to the z1- and z2- axes, respectively. And if w12{0, the
ellipse may have an arbitrary orientation (see figure 1.6).
14 Adjustment theory

                                                    Figure 1.6
If we define a function F(z) as F(z)=z*Wz, then we know from calculus that the gradient of F(z)
evaluated at z0 is a vector which is normal to the ellipse F(z)=c at z0. The gradient of F(z)
evaluated at z0 is:

This vector is thus normal or orthogonal to the ellipse at z0 (see figure 1.7).

                                                    Figure 1.7
From this follows that the tangent line of the ellipse z*Wz=c at z0 is given by:
                                                                                                     Linear estimation theory 15

Comparing this evaluation with the first equation of (7) shows that the WLSQ-estimate of x is
given by that x^ for which y-ax^ is orthogonal to the normal Wa at a of the ellipse z*Wz=a*Wa.
Hence, y-ax^ is parallel to the tangent line of the ellipse z*Wz=a*Wa at a (see figure 1.8). Note
that if W=I, the ellipse becomes a circle and the unweighted least-squares estimate is obtained
through orthogonal projection (see figure 1.9).

                                                    Figure 1.8

                                                    Figure 1.9
The matrices a(a*Wa)-1a*W and I-a(a*Wa)-1a*W that occur in (7) are known as oblique projectors.
We will denote them as:
(8)
16 Adjustment theory

The matrix  projects onto a line spanned by a and along a line orthogonal to Wa.

To see this, note that we can write Pa,(Wa)^y with the help of the cosine rule as (see figure 1.10):

In a similar way we can show that  Figure 1.10
spanned by a (see figure 1.11).    projects onto a line orthogonal to Wa and along a line

                                                   Figure 1.11

Above we have denoted the vector orthogonal to Wa as (Wa)^ ; thus ((Wa)^)*(Wa)=0. With the
help of the vector b, which is defined in section 1.2 as being orthogonal to a, thus b*a=0, we can
write instead of (Wa)^ also W -1b, because (W -1b)*(Wa)=0 (the inverse W -1 of W exists, since
we assumed W to be positive-definite). With the help of W -1b we can now represent the two
projectors of (8) alternatively as:
                                                                                                     Linear estimation theory 17

(9)

To see this, consult figure 1.12 and note that with the help of the cosine rule we can write:

or:

                                                   Figure 1.12
Note that if W=I, then W -1=I and equations (8) and (9) reduce to equations (3) and (4)
respectively. In section 1.2 we showed that for W=I the representations of (9) correspond to
solving equation (5) in an unweighted least-squares sense. We will now show that the
representations of (9) correspond to solving (5) in a weighted least-squares sense. From the
equivalence of the following minimization problems:

                                               subject to
                                               subject to
                                               subject to
follows that the weighted least-squares estimate of e is given by that e^ on the line b*y=b*e for
which e*We is minimal. We know that e*We = constant = c is the equation of an ellipse. For
different values of c we get different ellipses and the larger the value of c is, the larger the
ellipse (see figure 1.13).
18 Adjustment theory

                                             Figure 1.13 c3>c2>c1
The solution e^ is therefore given by the point of tangency of the line b*y=b*e with one of the
ellipses e*We=c (see figure 1.14).

                                                   Figure 1.14
At this point of tangency the normal of the ellipse is parallel to the normal of the line b*e=b*y.
But since the normal of the line b*e=b*y is given by b, it follows that the normal of the ellipse
at e^ is also given by b. From this we can conclude that e^ has to be parallel to W -1b. Hence we
may write:
The unknown scalar a may now be determined from the fact that e^ has to lie on the line
b*e=b*y. Pre-multiplying e^=W -1ba with b* gives for a
                                                                                                     Linear estimation theory 19

Substituting this in the equation e^=W -1ba shows that the weighted least-squares estimates of
equation (5) read:

 (10)
Compare this result with equation (6).

Example 4: Consider again the problem defined in section 1.1:

The form corresponding to (5) is:
thus the vector b is given as:
As weight matrix we take:
Its inverse reads therefore:

The two projectors  and            can now be computed as:

and
20 Adjustment theory

The weighted least-squares estimates e^ and y^ read therefore:

and

These results are identical to the results obtained in example 3.  End of example.

1.4 Weighted least-squares is also orthogonal projection

In the previous two sections we have seen that unweighted and weighted least-squares estimation
could be interpreted as orthogonal and oblique projection respectively (see figure 1.15).

                                                   Figure 1.15

It is important to realize however that these geometric interpretations are very much dependent
on the choice of base vectors with the help of which the vectors y, a and e are constructed. In
all the cases considered so far we have implicitly assumed to be dealing with the orthogonal base
vectors (1,0)* and (0,1)*. That is, the vectors y, a and e of the equation:

were constructed from the scalars y1, y2, a1, a2, e1 and e2 as:
                                        Linear estimation theory 21

(11)

Let us now investigate what happens if instead of the orthonormal set (1,0)* and (0,1)*, a
different set, i.e. a non-orthonormal set, of base vectors is chosen. Let us denote these new base
vectors as d1 and d2. With the help of the base vectors d1 and d2, the vectors y, a and e can then
be constructed from the scalars y1, y2, a1, a2, e1 and e2 as:

                                     ,

(12)                                 ,

                                     .

Again the equation y=ax+e holds. This is shown in figure 1.16a. Note that the vectors of (12)
are not the same as those of (11). They do however have the same components.

                       Figure 1.16a     Figure 1.16b

Looking at figure 1.16 it seems again, like in section 1.2, intuitively appealing to estimate x as
x^ such that ax^ is as close as possible to given vector y. This is the case if the vector y-ax^ is
orthogonal to the vector a; that is, if:

(13)                                    "orthogonality"

With (12) this gives:

or

or
22 Adjustment theory

(14)

with

The matrix D*D of (14) is symmetric, since (D*D)*=D*D, and it is positive-definite, since z*D*Dz
= (Dz)*(Dz)>0 for all z{0. Hence, the matrix D*D may be interpreted as a weight matrix. By
interpreting the matrix D*D of (14) as a weight matrix we see that the solution of (13) can in fact
be interpreted as a weighted least-squares solution, i.e. a solution of the minimization problem:

We have thus shown that if in a weighted least-squares problem the weight matrix W can be
written as the product of a matrix D* and its transpose D, then the weighted least-squares
estimate y^=ax^ can be interpreted as being obtained through orthogonal projection of y=y1d1+y2d2
onto the line with direction vector a=a1d1+a2d2, where d1 and d2 are the column vectors of matrix
D (see figure 1.17).

                                                   Figure 1.17
The crucial question is now whether every weight matrix W, i.e. every symmetric and positive-
definite matrix, can be written as a product of a matrix and its transpose, i.e. as W=D*D. If this
is the case, then every weighted least-squares problem can be interpreted as a problem of
orthogonal projection! The answer is: yes, every weight matrix W can be written as W=D*D. For
a proof, see the intermezzo at the end of this section.
                                                                                                     Linear estimation theory 23

The weight matrix W=D*D can be interpreted as defining an inner product (Strang, 1988). In the
unweighted case, W=I, the inner product of two vectors u and v with components u1,u2 and v1,v2
respectively, is given as:

This is the so-called standard inner product, where the inner product is represented by the unit
matrix I. The square of the length of the vector u reads then:

In the weighted case, W=D*D, the inner product of u and v is given as:

(15)

The square of the length of the vector u reads then:

Note that the two ways in which (u,v)W is written in (15) correspond with the two figures 1.17a
and 1.17b respectively. In figure 1.17a orthonormal base vectors are used and the metric is
visualized by the ellipse:

In figure 1.17b however, the ellipse is replaced by a circle:

and the metric is visualized by the non-orthonormal base vectors d1 and d2.
Now that we have established the fact that weighted least-squares estimation can be interpreted
as orthogonal projection, it must also be possible to interpret the oblique projectors of equation
(8) as orthogonal projectors. We will show that the matrix Pa,(Wa)^ of (8) can be interpreted as the
matrix which projects orthogonally onto a line spanned by the vector a, where orthogonality is
measured with respect to the inner product defining weight matrix W. From figure 1.18 follows
that:

or
24 Adjustment theory

or

and in this expression we recognize indeed the matrix  of equation (8). From now on we

will assume that it is clear from the context which inner-product is taken and therefore also

denote the matrix     as Pa.

                                                   Figure 1.18

Intermezzo: Cholesky decomposition:
If W is an n×n symmetric positive definite matrix, there is a nonsingular lower triangular matrix
L with positive diagonal elements such that W=LL*.
Proof: The proof is by induction. Clearly, the result is true for n=1 and the induction hypothesis
is that it is true for any (n-1)×(n-1) symmetric positive definite matrix.
Let

where A is a (n-1)×(n-1) matrix. Since W is symmetric positive definite, it will be clear that A
is also symmetric positive definite. Thus, by the induction hypothesis, there is a nonsingular
lower triangular matrix Ln-1 with positive diagonal elements such that A=Ln-1L n-1 * .
Let

where l and b are to be obtained by equating LL* to W:
                                                                                                     Linear estimation theory 25

This implies that l and b must satisfy:

Since Ln-1 is nonsingular, the first equation determines l as l=L n-1 -1 a. The second equation allows
a positive b provided that bnn - l*l>0. In order to proof that bnn - l*l>0 we make use of the fact
that W is positive definite, i.e.:

Substitution of  and x2=1 shows that bnn - l*l >0.

                                                    End of proof.

A simple example of a Cholesky decomposition is:

1.5 The mean and covariance matrix of least-squares estimators
From now on we shall make no distinction between unweighted and weighted least-squares
estimation, and just speak of least-squares estimation. From the context will be clear whether
W=I or not.
In our discussions so far no assumptions regarding the probabilistic nature of the observations
or measurement errors were made. We started by writing the two equations:

in vector notation as:

and showed that the least-squares estimates of x, y and e follow from solving the minimization
problem:
26 Adjustment theory

as:

(16)

Vector e^ contains the least-squares residuals. Since functions of random variables are again
random variables, it follows that if the observation vector is assumed to be a random vector y
(note: the underscore indicates that we are dealing with random variables) and substituted for y
in (16), the left-hand sides of (16) also become random variables:

(17)

These random variables are called least-squares estimators, whereas if y is replaced by its sample
value y one speaks of least-squares estimates.

From your lecture notes in statistics you know how to derive the probability density functions

of the estimators of (17) from the probability density function   of y. You also know that

if the probability density function  of y is "normal" or "Gaussian", then also x^, y^ and e^ are

normally distributed. This follows, since x^, y^ and e^ are linear functions of y. You know

furthermore how to compute the means and covariance matrices of y. This is particularly straight-

forward in case of linear functions (see also appendix E and F).

Application of the "propagation law of means" to (17) gives:

(18)

where E{.} denotes mathematical expectation. The result (18) holds true irrespective the type of
probability distribution one assumes for y. Let us now, without assuming anything about the type
of the probability distribution of y, be a bit more specific about the mean E{y} of y. We will
assume that the randomness of the observation vector y is a consequence of the random nature
of the measurement errors. Thus we assume that y can be written as the sum of a deterministic,
i.e. non-random, component ax and a random component e:

(19)

Furthermore we assume that the mean E{e} of e is zero. This assumption seems acceptable if
the measurement errors are zero "on the average".
                                               Linear estimation theory 27

With (19) the assumption E{e}=0 implies that:
(20)
Substitution of (20) into (18) gives then:
(21)

This important result shows that if (19) with (20) holds, the least-squares estimators x^, y^ and e^

are unbiased estimators. Note that this unbiasedness property of least-squares estimators is

independent of the choice for the weight matrix W ! Let us now derive the covariance matrices

of the estimators x^, y^ and e^. We will denote the covariance matrix of y as Qy, the variance of x^
as sx^2 and the covariance matrices of y^ and e^ as Qy^ and Qe^ respectively. Application of the
"propagation law of covariances" to (17) gives:

(22)

As you know from your lecture notes in Statistics, the variance of a random variable is a
measure for the spread of the probability density function around the mean value of the random
variable. It is a measure of the variability one can expect to see when independent samples are
drawn from the probability distribution. It is therefore in general desirable to have estimators
with small variances. This is particularly true for our least-squares estimators. Together with the
property of unbiasedness, a small variance would namely imply that there is a high probability
that a sample from the distribution of x^ will be close to the true, but unknown, value x. Since
we left the choice for the weight matrix W open up till now, let us investigate what particular
weight matrix makes sx^2 minimal. Using the decomposition Qy=D*D, it follows from (22) that
sx^2 can be written as:

where a is the angle between the two vectors DWa and (D *)-1a. From this result follows that
the variance sx^2 is minimal when the angle a is zero; that is, when the two vectors DWa and
(D*)-1a are parallel. The variance sx^2 is therefore minimal when DWa=(D*)-1a or D*DWa=a or
QyWa=a. Hence, the variance is minimal when we choose the weight matrix W as:
28 Adjustment theory

(23)

With this choice for the weight matrix, the variance sx^2 becomes:
(24)

1.6 Best Linear Unbiased Estimators (BLUE's)

Up till now we have been dealing with least-squares estimation. The least-squares estimates were
derived from the deterministic principles of "orthogonality" and "minimum distance". That is,
no probabilistic considerations were taken into account in the derivation of least-squares
estimates.
Nevertheless, it was established in the previous section that least-squares estimators do have
some "optimal properties" in a probabilistic sense. They are unbiased estimators independent of
the choice for the weight matrix W, and their variance can be minimized by choosing the weight
matrix as the inverse of the covariance matrix of the observations. Moreover, they are linear
estimators, i.e. linear functions of the observations.

Estimators which are linear, unbiased and have minimum variance are called best linear unbiased
estimators or simply BLUE's. "Best" in this case means minimum variance. Thus least-squares
estimators are BLUE's in case the weight matrix equals the inverse of the covariance matrix of
the observations. In order to find out how large the class of best linear unbiased estimators is (is
there only one? or are there more than one?), we will now derive this class. Thus instead of
starting from the least-squares principle of "orthogonality" we start from the conditions of
linearity, unbiasedness and best. Again we assume that the mean of y satisfies:
(25)

Our problem is to find an estimator x^ of the unknown parameter x, where x^ is a function of y.
Since we want the estimator x^ to be a linear function of y, x^ must be of the form:

(26)                                                                "L-property"

Furthermore, we want x^ to be an unbiased estimator of x for all the values x may take. This

means that:

(27)                                                                "U-property"

And finally, we want to find those estimators x^, which in the class of estimators that satisfy (26)
and (27), have minimum variance:

(28)         minimal in the class of LU-estimators                  "B-property"

It will be clear that the class of linear estimators is infinite. Let us therefore look at the class of
linear and unbiased estimators.
                                                                                                     Linear estimation theory 29

From taking the expectation of (26) follows, with (25) and (27) that:
or
(29)
Thus for LU-estimators the vector l has to satisfy (29) (see figure 1.19).

                                   Figure 1.19:
It will be clear that, although the class of permissable vectors l has been narrowed down to the
line l*a=1, the class of LU-estimators is still infinite. For instance, every vector l of the form:

where c is arbitrary, is permitted.
Let us now include the "B-property". The variance of the estimator x^ follows from applying the
"propagation law of variances" to (26). This gives:
(30)
The "B-property" implies now that we have to find the vectors l, let us denote them by ^l, that
minimize sx^2 of (30) and at the same time satisfy (29). We thus have the minimization problem:
(31)
With the decomposition Qy=D*D, this can also be written as:
or as:
30 Adjustment theory

(32)
where:
(33)
Geometrically, the minimization problem (32) boils down to finding those vectors ^ whose
endpoints lie along the line ¯l*(D*)-1a=1 and who have minimum length (see figure 1.20).

      Figure 1.20: ¯l*(D*)-1a = ¯l (D*)-1a cos a=1

From figure 1.20 it is clear that there is only one such vector  ^ . This vector lies along vector
D*-1a and has length (D*)-1a -1. Thus:

(34) ^

where the positive scalar b follows from:
                                     ^* ^

as:
(35)

Substitution of (35) into (34) gives with (33):

or:
(36)

The best linear unbiased estimator x^ of x follows therefore with (26) as:
(37)
                                                                                                     Linear estimation theory 31

From this result we can draw the important conclusion that the best linear unbiased estimator is
unique and equal to the least-squares estimator if W=Qy-1 !
We will now give an alternative derivation of the solution (37). Equation (29) is the equation
of a line with a direction vector orthogonal to a (see figure 1.20a). Since the vector b

                              Figure 1.20a: l*a=1 is equivalent to l=lo+ba
was defined as being orthogonal to a, b*a=0, the line a*l=1 can be represented parametrically
as:
(37a)

where lo is a particular solution of (29). Hence, the condition a*l=1 in (31) may be replaced by
l=lo+ba:

This may also be written as:

The solution of this minimization problem is:
(37b)
This is easily verified by using an algebraic proof like the one given in section 1.2. Substitution
of (37b) into (37a) gives:
32 Adjustment theory

The best linear unbiased estimator x^ of x follows therefore with (26) as:
(37c)

From equations (8) and (9) in section 1.3 we know that:

Substitution in (37c) gives therefore:

or:

since lo*a=1. This concludes our alternative derivation of (37).

1.7 The Maximum Likelihood (ML) method

In the previous sections we have seen the two principles of least-squares estimation and best
linear unbiased estimation at work. The least-squares principle is a deterministic principle where
no probability assumptions concerning the vector of observations is made. In case y=ax+e, the
least-squares principle is based on minimizing the quadratic form (y-ax)* W(y-ax), with the
weight matrix W.

In contrast with the least-squares principle, the principle of best linear unbiased estimation does
make use of some probability assumptions. In particular it is based on assumptions concerning
the mean and covariance matrix of the random vector of observables y. However, only the mean
and covariance matrix of y need to be specified, not its complete probability distribution. We will
now present a method of estimation that in contrast with the BLUE's method needs the complete
probability distribution of y.

Let p(y;x) denote the probability density function of y. For a fixed value of x the function p(y;x)
is a function of y and for different values of x the function p(y;x) may take different forms (see
figure 1.21).
                                                                                                     Linear estimation theory 33

         Figure 1.21: Two one-dimensional probability density functions p(y;x1), p(y;x2)
Our objective is now to determine or estimate the parameter x on the basis of the observation
vector y. That is, we wish to determine the correct value of x that produced the observed y. This
suggests considering for each possible x how probable the observed y would be if x were the true
value. The higher this probability, the more one is attracted to the explanation that the x in
question produced y, and the more likely the value of x appears. Therefore, the expression p(y;x)
considered for fixed y as a function of x has been called the likelihood of x. Note that the
likelihood of x is a probability density or a differential probability. The estimation principle is
now to maximize the likelihood of x, given the observed value of y:

(38)

Thus in the method of maximum likelihood we choose as an estimate of x the value which
maximizes p(y;x) for the given observed y. Note that in the ML-method one needs to know the
complete mathematical expression for the probability density function p(y;x) of y. Thus it does
not suffice, as is the case with the BLUE's method, to specify only the mean and covariance
matrix of y.
As an important example we consider the case that p(y;x) is the probability density function of
the normal distribution. Let us therefore assume that y is normally distributed with mean
E{y}=ax and covariance matrix Qy. The probability density function of y reads then in case it
is a two-dimensional vector:
(39)

As it is often more convenient to work with ln p(y;x) then with p(y;x) itself, the required value
for x is found by solving:
(40)
34 Adjustment theory

this is permitted since ln p(y;x) and p(y;x) have their maxima at the same values of x. Taking
the logarithm of (39) gives:
(41)
Since the first two terms on the right-hand side of (41) are constant, it follows that maximization
of ln p(y;x) amounts to maximization of -1/2(y-ax)*Qy-1(y-ax). But this is equivalent to
minimization of (y-ax)*Qy-1(y-ax). Hence we have obtained the important conclusion that
maximum likelihood estimators, in case p(y;x) is the normal distribution, are identical to least-
squares estimators for which the weight matrix is W=Qy-1 !
Let us now exemplify the above discussion geometrically. From (39) follows that the contours
of constant probability density are ellipses:

For different values of c we get different ellipses. The smaller the probability density, the larger
the value of c and the larger the ellipses are (see figure 1.22).

                               Figure 1.22: Contours of p(y;x) = constant
By varying the values of x the family of contour lines centred at ax translate along the line with
direction vector a (see figure 1.23).

                                                   Figure 1.23
From the figure it is clear that the likelihood of x obtains its maximum there where the
observation point y is a point of tangency of one of the ellipses (see figure 1.24).
                                                                                                     Linear estimation theory 35

                                      Figure 1.24: The ML-estimate x^
Hence, after translating the family of ellipses along the line with direction vector a such that y
becomes a point of tangency, the corresponding centre ax^ of the ellipses gives the maximum
likelihood estimate x^. At the point of tangency y, the normal to the ellipse is Qy-1(y-ax^) (see
figure 1.25).

                                    Figure 1.25: The normal Qy-1(y-ax^)
The normal Qy-1(y-ax^) is orthogonal to the tangent line at y. But this tangent line is parallel to
the line with direction vector a. Thus we have a*Qy-1(y-ax^)=0, which shows once again that in
case of the normal distribution, ML-estimators and LSQ-estimators with W=Qy-1 are identical!
1.8 Summary
In this chapter we have discussed three different methods of estimation: the LSQ-method, the
BLUE-method and the ML-method. For an overview of the results see table 1. The LSQ-method,
being a deterministic method, was applied to the model:
(42)
36 Adjustment theory

In case the measurement errors are considered to be random, e , the observables also become
random, y , and (42) has to be written as:
(43)

With the assumptions that E{e}=0 and E{ee*}=Qy , we get with (43) the model:
(44)

These two assumption were the starting point of the BLUE-method. If in addition to (44), it is
also assumed that y is normally distributed then the model becomes:
(45)

It was shown that for this model the ML-estimator is identical to the BLU-estimator and the
LSQ-estimator if W=Qy-1.

We have restricted ourselves in this introductory chapter to the case of two observations y1, y2
and one unknown x. In the next chapter we will start to generalize the theory to higher
dimensions. That is, in the next chapter we will start developing the theory for m observations
y1, y2,...,ym and n unknowns x1, x2,...,xn. In that case model (45) generalizes to:
(46)

where A is an  matrix, i.e. it has m rows and n columns.
Least Squares Estimation (LSQ)  Best Linear Unbiased Estimation       Maximum Likelihood Estimation (ML)
                                               (BLUE)             model:
                                                                  estimation principle:
model:                          model:
estimation principle:                                     or      In case
                                                                  the solution is identical to BLUE and to LSQ
solution:                       estimation principle:             if
                                solution:

                                                       Table 1.1
2 The model with observation equations

2.1 Introduction
As an introduction consider the following example: Figure 2.1 shows a levelling network. The
points 0,1,2,3,4 and 5 are connected with levelled height differences. The total number of
levelled height differences is 9. The height of point 0 is assumed to be known in the NAP-
reference system (NAP= Normaal Amsterdams Peil). The heights of the remaining points are
unknown. Our objective is to determine their heights from the measured height differences.

                                        Figure 2.1: Levelling network
One way to determine their heights is to use the configuration of figure 2.2. One starts at the
reference point 0 of which the height is known, and then determines the heights of the points
4,5,3,2 and 1 as shown in figure 2.2. In this case only 5 out of the 9 available levelled height
differences are used, namely y1, y6, y8, y9 and y4.

                                                    Figure 2.2
40 Adjustment theory

An alternative way to determine the heights of the points is to use the configuration of figure 2.3.
One starts again at the reference point 0, and then determines the heights of the points 1,2,5,3
and 4 as shown in figure 2.3. In this case again 5 out of the 9 available levelled height
differences are used. Note however that the height differences used differ from the ones used in
the previous case.

                                                    Figure 2.3

It will be clear that the two solutions of figure 2.2 and figure 2.3 are identical in case the height
differences were perfectly levelled. However, perfect measurements do not exist! Hence, since
one relies in the two cases considered on different observations with different measurement
errors, the computed heights will also differ. Therefore, if one wants to make use of all the 9
levelled height differences, one has to take care in one way or another of the resulting
inconsistencies or discrepancies.

One could also argue that in the case of figure 2.1 too many height differences are measured.
This is true as far as the construction of the levelling network is concerned. That is, strictly
speaking only 5 out of the 9 available height differences are needed to determine the 5 unknown
heights. However, with 5 measured height differences one will never be able to detect blunders
or outliers in the measurements. This is one of the reasons why always more measurements are
taken than strictly necessary for determining the geometric figure of the network. In the course
of your studies in Mathematical Geodesy you will learn to answer questions as: how many
measurements are needed, how should they be distributed over the network etc. That is, in the
course of your studies you will learn, given the purpose of a network, how to design a network
with a certain quality.

In the above example of figure 2.1 we have 9 measured height differences and 5 unknown
heights. As we did in the introductory section of chapter 1, we can write down the 9 equations
that relate the measurements to the unknowns. If we denote the unknown heights as x1, x2,...,x5
and the known height of point 0 as x0, we obtain with the help of figure 2.1:
                                                                                  The model with observation equations 41

(1)

where the ei, i=1,...,9, are the unknown random measurements errors.The above 9 equations can
be written in matrix notation as:

(2)

For simplicity we will assume that the known height x0 of the reference point 0 is equal to zero.
The matrix equation of (2) is then of the form:
(3)
with m=9 and n=5. Matrix A thus has 9 rows and 5 columns. If the known height x0 is unequal
to zero, then (2) can still be brought into the form of (3) by bringing the known vector
containing x0 in (2) to the left-hand side.
Equation (3) is the multi dimensional generalization of equation (19) in section 1.5. In this
chapter we will generalize the theory of chapter 1 for multi dimensional cases.
42 Adjustment theory

2.2 Least-squares estimation
The equations of the linear system:
(4)

are known as observation equations (waarnemingsvergelijkingen). Let us assume that an
observation vector y is given, and that we want to solve for the unknown vectors x and e in the
system:
(5)

If we denote the column vectors of the matrix A by ai, i=1,...,n, then A=(a1, a2,....,an), and (5)
may be written as:
(6)

In chapter 1 we solved equation (6) in a least-squares sense for the case n=1. We will now
consider the general case that n may be arbitrary, provided that n£m. That is, the restriction is
that the number of observations, m, has to be larger than or equal to the number of unknown
parameters, n. For m=3 and n=2 we may visualize equation (6) as in figure 2.4.

              Figure 2.4: x and e unknown in y=Ax+e.

Fnrom the geometry of the figure it seems intuitively appealing to estimate x as x^, such that

aix^ i nor Ax^ is as close as possible to the given observation vector y. This is the case if
i1
          aix^ i is orthogonal to the "plane" spanned by the column vectors ai, i=1,...,n, of A.
e^ y

      i1

That is, if:
        The model with observation equations 43

or if:

or if:

or if:

(7)

This is a system of n linear equations with n unknowns. If the matrix A*A is invertible, the
solution of (7) is unique and reads:
(8)

Compare this result with equation (2) in section 1.2. The n×n matrix A*A is invertible if and only
if rank A*A=n. And since rank A*A = rank A (for a proof see the end of this section), it follows
that A*A is invertible if rank A=n. We will therefore assume that matrix A has full rank n. Verify
that the matrix of equation (2) has full rank; its rank is 5. Verify also that if the height of the
reference point 0 is unknown, the matrix which follows from including xo in the vector of
unknowns has a rankdefect of 1. The reason is of course that "absolute heights" cannot be
determined from height differences only. The derivation of the estimate x^ of (8) was based on
the principle of orthogonality. The estimate x^ however also minimizes the sum of squares (y-
Ax)*(y-Ax). That is, x^ of (8) is the solution of:
(9)

This is easily shown, by using the same algebraic derivation as was used in section 1.2. Along
the same lines one can show that the solution of:

(10)    .

where W is an m×m symmetric and positive definite weight matrix, is:
(11)
44 Adjustment theory

Proof: (algebra)

      (y-Ax)*W(y-Ax)  = [y-Ax^-A(x-x^)]*W[y-Ax-A(x-x^)]
                      = (y-Ax^)*W(y-Ax^) - 2(y-Ax^)*WA(x-x^) + (x-x^)*A*WA(x-x^).

      But A*W(y-Ax^) = A*W[(y-A(A*WA)-1A*Wy)] = 0, hence,

      (y-Ax)*W(y-Ax)  = (y-Ax^)*W(y-Ax^)+(x-x^)*A*WA(x-x^)
                      = minimal for x=x^

                                                                                   End of proof.

The estimate x^ of (11) is the weighted least-squares estimate of x. As a generalization of
equation (7) in section 1.3, the weighted least-squares estimators read therefore for the multi
dimensional case:

(12)

Intermezzo:           rank A = rank A*A
(13)

From theory in Linear Algebra we know that:

where N(A) denotes the nullspace (kern) of the m×n matrix A. For A*A we have:

Hence, rank A = rank A*A is equivalent to:
(14)

The nullspace of A is defined as the set of vectors x that satisfy:

Since A*Ax=0 if Ax=0, it follows that:
(15)
                                                   The model with observation equations 45

But also:
(16)

holds. Because if xOEN(A*A) then A*Ax=0, and this implies that Ax=0 or xOEN(A). A*Ax=0 can
never imply that Ax{0. From (15) and (16) follows that N(A)=N(A*A) and thus that dim. N(A)
= dim N(A*A) or rank A = rank A*A.

                                                                                                     End of proof.

2.3 The mean and covariance matrix of least-squares estimators

In section 5 of chapter 1 we derived the means and covariance matrices of the least-squares
estimators of the model y=ax+e. This was done through application of the propagation laws for
means and variances to (17) in section 1.5. In the present section we want to generalize these
results. That is, we want to derive the mean and covariance matrix of the estimators of (12). This
is again done through application of the propagation laws for means and variances. For
completeness sake we first state and prove three laws for the general multi-dimensional case.

Let v be a p×1 dimensional random vector and let u be a q×1 dimensional random vector.
Furthermore let u and v be functionally related by:

(17)

where F(.) is a vector function.

The mean of v, E{v}, is then defined as (see also appendices E and F):

(18)

where      is the probability density function of u. Equation (18) shows that the mean of v can

be computed once the vector function F(.) and probability density function  are known.

We will now use (18) to derive the two propagation laws of means and variances for the case

of linear functions.

The propagation law of means

Let the two random vectors u and v be related as:

(19)
46 Adjustment theory

where M is a constant p×q matrix and m is a constant p×1 vector. Using (18) we get for the
mean of v:

(20)

According to (18),  equals the mean of u, E{u}. Furthermore, since the "area" or

"volume" of a probability density function equals 1, we have that  . Hence, (20)

reduces to:

(21)

This result shows that in case u and v are linearly related, the mean of v, E{v}, can be computed
once the mean of u, E{u}, is known! This is generally not the case for nonlinear functions.

The propagation law of variances and covariances

The covariance matrix of the random vector v is defined as:

If u and v are related as (19), it follows with (21) that:

Using (18) this gives:
(22)

But E{(u-E{u})(u-E{u})*} is by definition the covariance matrix of u, Qu. Hence, (22) reduces
to:
                                                                                  The model with observation equations 47

(23)

This result shows that in case u and v are linearly related, the covariance matrix of v, Qv, can
be computed once the covariance matrix of u, Qu, is known! Thus in case of linear functions one
does not need to know the complete probability density function of u.
If we denote the covariance between u and v as Quv, then by definition:

With a derivation similar to the one given above folllows then that:
(24)
Prove this yourself!
It will now be clear since the first three estimators of (12) are all linear functions of y, and thus
of the form (19), that their means and covariance matrices can be derived from the results of
(21), (23) and (24).
The model used for deriving the estimators of (12) was:
(25)

We will now assume that E{e}=0 and E{e e*}=Qy. With (25) this gives:
(26)

With the first equation of (26), application of the propagation law for means, (21), to (12) gives:

(27)

Compare this result with equation (21) in section 1.5. This important result shows that if (26)
holds, the least-squares estimators are unbiased estimators. And this property of unbiasedness is
independent of the choice for W.
48 Adjustment theory

With the second equation of (26), application of the propagation law for variances, (23), to (12)
gives:

(28)

In a similar way, application of the propagation law of covariances, (24), to (12) gives:

(29)

In order to derive the mean of the random variable e^*We^ of (12), we cannot make use of (21)
directly. The random variable e^*We^ is namely not a linear function of y. It is possible however
to write e^*We^ as a linear combination of products of the elements of e^, in which case (21) can
be applied again. If we denote the ith-element of e^ by e^i, and the (i,j)th-element of the matrix
W by (W)ij, then:

This shows that e^*We^ is a linear combination of the products e^ie^j. Hence:

where E{e^ie^j} is the covariance between e^i and e^j. But the covariance between e^i and e^j is the
(i,j)th-element of the covariance matrix Qe^. Thus:

But since the covariance matrix Qe^ is symmetric and thus (Qe^)ij=(Qe^)ji, we may write:

For fixed i, the term  equals the product of the ith-row of W with the ith-column
of Qe^:
                                                The model with observation equations 49

Thus E{e^*We^} is the sum of these m products:

This shows that E{e^*We^} equals the trace (spoor) of the matrix product WQe^:
(30)

For a definition of the trace of a square matrix consult your lecture notes on Linear Algebra.
It should be noted that all the results concerning the means, variances and covariances of the
least-squares estimators, were derived without making any assumptions on the probability
distribution of y! This is in principle also possible for the variance of e^*We^. The derivation is
however rather complicated and therefore ommitted. Even for the case that y is normally
distributed, the derivation of the variance of e^*We^ is complex. We therefore state without proof
that if y is normally distributed with mean and covariance matrix as given in (26), the variance
of e^*We^ reads:
(31)

2.4 Best Linear Unbiased Estimation
We consider again the model:
(32)

Let us assume that we are interested in estimating a parameter q, which is a linear function of
x:
(33)

For instance, in case of the levelling network of figure 2.1 we could be interested in estimating
the height difference between the points 1 and 5. Then q of (33) plays the role of the height
difference between the points 1 and 5, and the vector f of (33) takes the form:

Let q^ be the estimator of q. Then according to the criteria of best linear unbiased estimation, we
want the estimator q^ to be a linear function of y:
50 Adjustment theory                "L-property"

(34)

such that                           "U-property"
(35)

and:

(36)                                "B-property"

Our objective is now to find a vector l such that with (34), the conditions (35) and (36) are
fulfilled. Substitution of (34) into (35) gives:

and with (32) and (33) this gives:

Since we want this condition to hold for all x, it follows that:

(37)

Compare this with equation (29) in section 1.6. Equation (37) is a system of n linear equations
with m unknowns. The unknowns are the elements of the vector l. Every vector l that satisfies
(37) gives with (34) a linear unbiased estimator of q. If C is an m×n matrix such that (A*C)-1
exists, then:
(38)

is a solution of (37). This means that:
(39)

is an unbiased estimator of q. That this is indeed the case, follows if we take the expectation of:
(39)

The m×n matrix C of (38) is not unique. One choice for matrix C would be:
Substitution into (39) gives:
                                                                                  The model with observation equations 51

This shows once again that the weighted least-squares estimator is an unbiased estimator.
From your lecture notes in Linear Algebra (Strang, 1988) you know that the general solution of
the inhomogeneous system (37) is given by the sum of a particular solution of (37) and the
solution of the homogeneous system:
(40)

The solution of this homogeneous system is the nullspace, N(A*), of the matrix A*:

The dimension of this space follows from the dimension theorem (Strang, 1988) as:

where R(A*) is the range space or column space (beeldruimte of kolomruimte) of A*. Since
dim. R(A*)= rank A*= rank A and since we assumed that rank A=n, it follows that:

Since the dimension of the solution space of (40) equals m-n, there exist m-n linearly
independent vectors bi, i=1,..., m-n, that form a basis of N(A*). Each of these vectors satisfy:
(41)
If we collect the m-n vectors bi in a matrix B:

then matrix B is of order m×(m-n) with full rank, rank B=m-n, and satisfies because of (41):

(42)

This shows that every solution of (40) can be represented as:

Hence, if lo is a particular solution of (37), the general solution of (37) takes the form:
(43)
52 Adjustment theory

Compare this result with equation (37a) in section 1.6. Equation (43) shows that all vectors l that
satisfy (37) lie on a linear variety or linear manifold (lineaire variëteit) parallel to the range
space of matrix B.

Let us now continue our derivation of the best linear unbiased estimator of (33). With (37) or
(43) we have found the class of linear unbiased estimators. We now want to find within this
class, the estimator that has minimum variance. Application of the propagation law of variances
to (34) gives with (32):

Hence, our minimization problem reads:

(44)

Compare this with equation (31) in section 1.6. Since (43) is the solution of (37), (44) may also
be written as:

                                                     subject to

or as:
(45)

The solution of this minimization problem is:
(46)

This is easily verified by using an algebraic proof like the one given in section 1.2. Substitution
of (46) into (43) gives:
(47)

The best linear unbiased estimator q^ of q follows therefore with (34) as:
(48)

This solution is expressed in terms of the matrix B, which is related to matrix A of (32) through
(42). Since our starting point was the model of (32), it would be nice if we could express the
solution (48) in terms of the matrix A. In chapter 1 (see section 1.6) this was possible by making
use of the identity:
                                                                                  The model with observation equations 53

Compare with equation (8) and (9) in section 1.3. For the multi-dimensional case, this suggests
the identity:
(49)

We will prove this identity, by proving that:
(50)

Since Ix=x, "x, we have proved (50) once we can show that:
(51)

If R is a square and invertible matrix, then (51) is equivalent to:
(52)

If we take for R the m×m partitioned matrix:
(53)
then:
(54)

where we have made use of the fact that A*B=0, see (42). Equation (54) shows that with the
choice (53) for R, the equation (52) holds. What remains to be shown is that matrix R of (53)
is invertible. This square matrix is invertible if it has full rank m, and it has full rank only if the
solution of the system:
(55)

is the trivial solution (a* b*)*=0.
Since B*A=0, premultiplication of (55) with B* gives:
(56)
54 Adjustment theory

But matrix B has full rank m-n and therefore b=0 is the only solution of (56). With b=0 equation
(55) reduces to:
(57)

But matrix A has full rank n and therefore a=0 is the only solution of (57). Hence (a* b*)*=0
is the only solution of (55), which shows that (A QyB) is invertible. This concludes the proof
of the identity (49).
Substitution of (49) into (48) gives:

or since lo is a particular solution of (37):
(58)

This important result tells us two things. First of all it tells us that the best linear unbiased
estimator of x is given by:
(59)

This follows from (58) if we take for the vector f respectively (1,0,..,0)*, (0,1,0,...,0)*....,
(0,0,...,1)*. Compare (59) with equation (37) in section 1.6. Equation (59) shows that the BLUE
of x is identical to the weighted least-squares estimator of x if W=Qy-1. The result (58) tells us
also that the BLUE of q=f*x is:
(60)
where x^ is the BLUE of x.
Now that we know that the best linear unbiased estimators are identical to the least-squares
estimators if W=Qy-1, we can use the results of the previous section 2.3 to obtain the covariance
matrices of the BLUE's. Substitution of W=Qy-1 into equation (28) gives:

(61)

Substitution of W=Qy-1 into equation (29) gives:

(62)
                                                                                  The model with observation equations 55

This shows that in case of best linear unbiased estimation, the estimator e^ is not correlated with
either x^ or y^.

Substitution of W=Qy-1 into (30) and (31) gives:

Since trace (Qy-1Qy^) = trace (Qy^Qy-1)=n if rank A=n (for a proof see end of next section) we get:
(63)

To conclude this section let us investigate what happens with our results if we assume that the

number of observations, m, equals the number of unknowns, n. If m=n, then matrix A is a square

matrix. We assumed that rank A=n. This implies that if m=n, the matrix A is invertible. That is,

A-1  exists.  If  A-1  exists,  the  matrix      * -1 -1  can  be  written  as:

                                             (A Qy A)

(64)

Substitution into (59) gives then:

and from this follows that:
                                                       and

Similarly we find from substituting (64) into (61) that:

These results show that if m=n (there is no redundancy), the estimate x^ is simply the unique
solution of the system y=Ax and the best linear unbiased estimator of E{y}=Ax, y^, is simply y.
Thus if m=n we have just enough observations available to determine the unknown parameter
vector x uniquely, but not enough observations to get a better estimator for E{y}=Ax than y.

2.5 The orthogonal projector

As a generalization of the theory developed in chapter 1 we will prove in this section that the
matrix A(A*Qy-1A)-1A*Qy-1 is an orthogonal projector that projects onto the rangespace of A and
along its orthogonal complement.
56 Adjustment theory

Let the m-dimensional linear vectorspace m have the inner product:
(65)

Matrix Qy-1 is thus the matrix representation of the inner product (65).

The range space of the m×n matrix A is defined as:
(66)

The range space R(A) is a linear subspace of  m, R(A) Ã m, and its dimension equals the rank
of A, which in our case is n:
(67)

The orthogonal complement of R(A) is defined as the set of vectors that are orthogonal to R(A)
(Strang, 1988):
(68)

According to your lecture notes in Linear Algebra, dim. R(A)+dim. R(A)^= dim  m. With dim.
R(A)=n and dim. m=m this gives:
(69)

In section 2.4 a full rank matrix B of order m×(m-n) was introduced that satisfied:
(70)

With this matrix B we can represent the orthogonal complement R(A)^ of (68) as:
(71)

According to theory in Linear Algebra, the space m is the direct sum of R(A) and R(A)^:
(72)

This means that R(A) « R(A)^={0}, i.e. the two subspaces R(A) and R(A)^ only have the null
vector in common, and that a basis of R(A) together with a basis of R(A)^ forms a basis of m.
Since the m×n matrix A has full rank n, its column vectors form a basis of R(A). Similarly, since
the m×(m-n) matrix B has full rank m-n and the m×m matrix Qy has full rank m, the m×(m-n)
matrix QyB has full rank m-n and its column vectors form a basis of R(A)^. From this follows
that the column vectors of the m×m matrix:
                                                                                  The model with observation equations 57

(73)

form a basis of m. This implies that the m×m matrix of (73) is square and invertible (this was
already shown in the previous section). If we write equation (50) of the previous section as:

it follows that the inverse of (73) is given as:
(74)

According to theory in Linear Algebra, a vector z OE m can be decomposed in only one way
along R(A) and R(A)^. If:

(75)                                              and

then u is called the orthogonal projection of z on R(A). The matrix PA, defined as:
(76)

is called the orthogonal projector on R(A). We now want to find the matrix representation of PA.
Since the column vectors of the matrix of (73) form a basis of m, every z OE m can be
represented as:

In this representation u=Aa is the component of z along R(A). With (76) this gives:
or:
(77)

Since (76) holds for all z OE m, (77) holds for all a and b:
58 Adjustment theory

Post-multiplication with (A QyB)-1 gives:

And substitution of (74) gives then finally:
(78)

According to theory in Linear Algebra, PA has the following properties:

(79)

From Linear Algebra we know that I-PA is also an orthogonal projector. I-PA projects onto R(A)^
and along R(A). I-PA has the properties:

(80)

Instead  of  I-PA  we  will  sometimes  use  the  notation     ^  It  follows  from  equation  (49)  that:

                                                            PA .

(81)

To conclude this section we will prove that:
(82)

Substitution of Qy^=A(A*Qy-1A)-1A* (see (61)) gives:

or with (78):
(83)

Since the trace of a matrix is the sum of its eigenvalues (Strang, 1988 and Lay, 1997), we need
the eigenvalues of PA in order to prove (83). The eigenvalue problem for PA reads:
                                                                                  The model with observation equations 59

(84)

with PAPAz=lPAz=l2z and PAPAz=PAz=lz follows that l2z=lz or for z{0:
(85)

This shows that the eigenvalues of PA are 1 or 0. In order to prove (83) we thus need to show
that PA has n-number of eigenvalues equal to 1. This corresponds to showing that for l=1 the
solution space of (84) has dimension n. For l=1 the solution space of (84) is the solution space
of (I-PA) z=0 and thus the nullspace of I-PA. According to (80) the nullspace of I-PA is the
rangespace of A. Hence, dim. N(I-PA)=dim. R(A)= rank A=n. This concludes the proof.

2.6 Summary

In the table on the next page an overview is given of the results of Best Linear Unbiased
Estimation.
60 Adjustment theory

                      Best Linear Unbiased Estimation
                                       Model

                      Estimators

                                   Mean
                      Variances and covariances

                      Orthogonal projectors
3 The model with condition equations

3.1 Introduction
Figure 3.1 shows the levelling network of section 2.1.

                                        Figure 3.1: Levelling network
In section 2.1 it was shown that the observation equations of the levelling network of figure 3.1
take the form:

 (1)

For simplicity it is assumed here that the known height of the reference point 0 is equal to zero.
The matrix equation of (1) is of the form:
(2)
with m=9 and n=5. Thus since rank A=n=5, the number of redundant observations is m-n=4.
62 Adjustment theory

Instead of writing the observed height differences as functions of the unknown heights, as is done
in (1), it is also possible to write down condition equations (voorwaarde vergelijkingen) which
the height differences have to fulfill. In figure 3.1 four loops (kringen) are shown. With perfect
measurements the height differences of each loop would sum up to zero. Due however to the
presence of measurement errors these sums will differ from zero. That is, misclosures will exist.
If we denote these four misclosures (tegenspraken) by respectively t1, t2, t3 and t4, the four
condition equations become (see figure 3.1):

(3)

These four condition equations can be written in matrix notation as:

 (4)

This matrix equation is of the form:
(5)

with b=4 and m=9. Thus matrix B has m=9 rows and b=4 columns.
Note that in the above example of the levelling network the number of redundant observations,
m-n=4, equals the number of condition equations, b=4. This is generally true, because each
additional observation on top of the n-number of observations which are needed for the unique
determination of x in (2), gives rise to an extra condition equation. Thus:
(6)
                                                                                    The model with condition equations 63

Also note that if the matrix of (1) is pre-multiplied with the matrix of (4), the result equals the
zero-matrix. In terms of the matrices A and B this means that:

(7)

Also this relation between the two matrices A and B is generally true. This can be seen as
follows. Let us start from equation (2). If we pre-multiply equation (2) with a b×m matrix B* we
get:
(8)

Our objective is now to investigate whether a full rank matrix B*, with rank B*=b=m-n, exists
such that B*A=0. If such a matrix exists, then the unknown parameter vector x gets eliminated
from (8), and (8) reduces to the form of (5) with:
(9)
If we denote the b-number of columnvectors of matrix B by bi, i=1,...,b, then B*A=0 is equivalent
to:
(10)

The question is thus if and how many linearly independent vectors exist that satisfy (10). From
Linear Algebra we know that for an n×m matrix A*:

(11)

Since rank A=n, it follows from (11) that:

(12)

Thus the dimension of the nullspace of A* is m-n. This shows that indeed m-n linearly
independent vectors bi exist that satisfy (10). Note that if there are no redundant observations
m=n and (10) has only the trivial zero vector as solution. Thus if m=n, no matrix B exists for
which (7) holds and no condition equations exist.

With (2) and (5) we now have two equivalent representations of the same model. Equation (2)
is a parametric representation. The equations of (2) are known as observation equations. Equation
(5) is an implicit representation. The equations of (5) are known as condition equations. In
64 Adjustment theory

chapter 2 we developed the theory of linear estimation for (2). In this chapter we will develop
the theory for the model with condition equations (5).

3.2 Best Linear Unbiased Estimation
From equations (5) and (9) follows that:
(13)

This shows that the misclosure vector t is due to the random measurement error vector e.
We will assume, like we did in chapter 2, that:
(14)

This, together with (13) gives:
(15)

This model will be our starting point for best linear unbiased estimation. Let us assume that we
are interested in estimating a parameter j, which is a linear function of E{y}:
(16)

For instance, in case of the levelling network of figure 3.1 we could be interested in estimating
the height difference between the points 1 and 5. Then j of (16) plays the role of the height
difference between the points 1 and 5, and the vector g of (16) takes the form:

Let j^ be the estimator of j. Then according to the criteria of best linear unbiased estimation, we
want the estimator j^ to be a linear function of y:

(17)       "L-property"

such that  "U-property"
(18)

and

(19)       "B-property"
                                                                                    The model with condition equations 65

Our objective is now to find a vector l such that with (17), the conditions (18) and (19) are
fulfilled.
Substitution of (17) into (18) gives:
(20)

If we subtract (16) from (20) we get:
(21)

This equation states that the vector l-g is orthogonal to the mean E{y}. From the first equation
of (15) we learn that the subspace orthogonal to E{y} is spanned by the b linearly independent
column vectors of matrix B. This, together with (21) shows that the vector l-g is an element of
the range space of the matrix B:

This means that a b×1 vector a exists such that:

or:
(22)
The meaning of this result is that every vector l that can be represented as (22), gives with (17)
an unbiased estimator of (16). To verify this, we substitute (22) into (17):

If we take the expectation we get:

and with (15):

which according to (16) is equal to j.
Let us now introduce condition (19), the condition of minimum variance. Application of the
propagation law of variances to (17) gives with (15):
66 Adjustment theory

Hence, our minimization problem reads:
(23)

This minimization problem may also be written as:

The solution of this minimization problem is:
(24)

This is easily verified by using an algebraic proof like the one given in section 1.2. Substitution
of (24) into (22) gives:
(25)

The best linear unbiased estimator j^ of j follows therefore with (17) as:
(26)

If we take for the vector g respectively (1,0,...,0)*, (0,1,0,...,0)*, ...., (0,...,0,1)*, the BLUE of E{y}
follows as:
(27)

This, together with (26) shows that:
(28)
If we pre-multiply (27) with B*, we get:
(29)

which shows that the estimator y^ satisfies the condition equations. Since t=B*y (see (13)),
application of the propagation law of variances gives Qt=B*QyB. This shows that (27) can also
be written as:
(30)

This equation shows perhaps more clearly than (27), that the estimator y^ is obtained from y by
subtracting a part which depends on the misclosure vector t such that y^ satisfies (29). Since:
                                                                                    The model with condition equations 67

(31)

(see equation (49) in section 2.2), the estimator of (27) is identical to the estimator of E{y}
which was derived in the previous chapter (see for instance the summary in section 2.6). This
is of course understandable, because we used the same principle of estimation, namely BLUE,
and the same model. Model (15) is namely equivalent to the model in section 2.6. The two
models only differ in their representation.
The results of estimation for model (15) are therefore identical to the results of the previous
chapter. The results only differ in their appearance. In chapter 2 everything was expressed in
terms of the matrix A, whereas in this chapter everything is expressed in terms of the matrix B.

3.3 The case
The model of condition equations (4) was based on the fact that the observed height difference
of each levelling loop in figure 3.1 should sum up to zero. Now consider the triangulation
network of figure 3.2.

                                  Figure 3.2: Small triangulation network
From the figure it is clear that the six angle observables should fulfill two conditions: in each
triangle the expectation of the three observed angles sums up to p. Due however to the presence
of measurement errors these condition equations will result in small misclosures. The two
condition equations become in matrix notation:

(32)
68 Adjustment theory

Compared to (5) this matrix equation is of the form:
(33)
with b=2, m=6 and
For a non-zero vector b0 straightforward application of the estimation formulae of the previous
section is no longer possible. At first sight we can rewrite (32) e.g. as:

so that a model of the form of (5) is obtained. However, in general it is difficult to redefine the
vector of observables so that it is of the form of (5). Therefore we will adapt the estimation
formulae so that the non-zero vector b0 can be taken into account. From (33) the vector of
misclosures and its variance matrix are obtained as:
(34)
Then with (30) the estimators y^ and e^ become:

(35)

Comparing this result with (27) shows that the non-zero vector b0 results in an additional term
for the least-squares estimators. Since b0 is a vector of constants the extension of the model of
condition equations with a non-zero b0-vector does not affect the variance matrices Qt, Qy^ and
Qe^.

3.4 Summary
Since the results of estimation for model (15) are identical to the results of the previous chapter,
we merely need to represent the results of section 2.6 in terms of the matrix B to obtain a
summary of the results for model (15). This is done on the next page, there upon the main results
of this and the previous chapter are compared.
                                       The model with condition equations 69

Best Linear Unbiased Estimation
                 Model

              Estimators

                  Mean
Variances and covariances

       Orthogonal projectors
70 Adjustment theory

                                     Best Linear Unbiased Estimation

observation equations  condition equations
4 yR -Variates

4.1 Introduction
For the case that m=2 and n=1 the model with observation equations reads:
(1)
The corresponding model with condition equations reads:
(2)
In chapter 1 (see figure 1.8 or figure 1.14 in section 1.3) it was shown that the estimator y^
of E{y} is the oblique projection of y onto the line with direction vector a. The direction of
projection is parallel to the tangent line of the ellipse z*Qy-1z = a*Qy-1a at a (see figure 4.1).

                                   Figure 4.1: Oblique projection of y
If we rotate the direction vector a so that it becomes parallel to the 1-axis, figure 4.1
transforms into figure 4.2.

                       Figure 4.2: Oblique projection of y in case a= (a1 0)*
72 Adjustment theory

In this case the second component a2 of the vector a is zero, and the first component b1 of
the vector b is zero. The corresponding two models (1) and (2) then take the form:

(3)
and:
(4)

Equation (4) shows that the coefficient of E{y1} equals zero. This implies that the observable
y1 does not appear in the condition equation. Observables that do not appear in the condition
equations are called free variates. Thus E{y1} is a free variate. If one writes (4) as:
(5)
one might be inclined to conclude, since E{y1} does not appear in (5), that the estimator y^1
of E{y1} equals y1 and thus that e^1=y1-y^1 is zero. This however is not true. Figure 4.2 shows
namely clearly that both e^1 and e^2 are unequal to zero. The reason for this is that the major
and minor axes of the ellipse of figure 4.2 make a non-zero angle with the 1-axis and 2-axis.
If the major and minor axes of the ellipse are parallel to the 1-axis and the 2-axis, then figure
4.2 transforms into figure 4.3.

                          Figure 4.3: The covariance matrix Qy is diagonal
In this case s12=0, Qy is diagonal and e^1=0. It thus seems that the covariance, s12, between
y1 and y2 determines to what extent e^1=y1 - y^1 differs from zero. If s12=0 then e^1=0, and if
                                                             yR - Variates 73

s12{0 then also e^1{0. In order to find out how e^1 depends on s12 consult figure 4.4. Figure
4.4 shows that:

(6)                                         ,

and that b is the angle between the vector  and the 1-axis.

                             Figure 4.4: tan b e^1
                                                    e^2

Since the inverse of Qy is:

it follows with a=(a1 0)* that:

From this it follows that:
(7)
which with (6) gives that:
(8)

This result shows how e^1=y1 - y^1, and thus the estimator y^1=y1 - e^1 of free variates, can be
determined from e^2=y2 - y^2. In the next section we will generalize (8) to the multi-dimensional
case.
74 Adjustment theory

Free variates are examples of so-called yR-variates. The definition of yR-variates reads as
follows:

                    yR-variates are observables that are either stochastically or
                    functionally Related to another set of observables y.

There are three types of yR-variates:

1. yR-variates that correlate with y-variates. These are the free variates (vrije grootheden);
2. yR-variates that are functions of y-variates. These are the derived variates (afgeleide

     grootheden);
3. yR-variates of which the y-variates are functions. These are the constituent variates

     (samenstellende grootheden).

ad 1.  These variates occur for instance when new measurements need to be included in
       existing geodetic networks. Consider for instance the levelling network of figure 3.1
       of Chapter 3. Assume that the five heights have been estimated with the available nine
       levelled height differences. This gives the five estimators of the heights, x^1, x^2, x^3, x^4,
       x^5. Now assume that a new height difference has been measured, say between the
       points 1 and 5. If we denote this height difference by y10, we can write the condition
       equation as:

       (9)

       On the basis of this condition equation we can compute y^10 and ^ ^ , the improved
       estimators for the heights of the points 1 and 5. However, since the estimators x^2, x^3
       and x^4 are correlated with x^1 and x^5, also the estimators x^2, x^3 and x^4 can be improved.
       The variates x^2, x^3 and x^4 are in this case the free variates since they do not appear in
       condition equation (9).

ad 2.  Examples of derived variates in the case of the levelling network are height
       differences which are not directly measured, but which can be computed as functions
       of the estimated heights.

ad 3.  Consider the levelling network of figure 4.5. Assume that the height difference
       between the points 1 and 2 has not been measured directly, but that instead it equals
       the sum of a number of measured height differences.

       Figure 4.5: Levelling network
                                                                                                                yR - Variates 75

         One can now include either the individual measured height differences in the condition
         equation or their sum y1. In the last case the individual measured height differences
         are the constituent variates.
In the following sections it will be shown that formula (8) holds for all three types of yR-
variates.

4.2 Free variates
In this section we will generalize formula (8) to the multi-dimensional case. We will give two
derivations. One based on the model with condition equations, and one based on the model
with observation equations.
We know that the solution of the model with condition equations
(10)
reads:
(11)
Now let us assume that instead of (10) we have the model:

(12)

In this case the coefficients of yR are zero; thus yR is a vector of free variates. When we
compare (12) with (10) we see that B* of (10) is replaced by (B* 0) and Qy of (10) by:

Using the second equation of (11), we therefore get for model (12):

or:
(13)
From the first equation of (13) it follows that:

If we substitute this into the second equation of (13) we finally get:
76 Adjustment theory

(14)
This is the multi-dimensional generalization of equation (8) of section 4.1.
We will now derive formula (14) using the model with observation equations. The equivalent
of model (12) in terms of observation equations reads:

(15)

Note that:

holds.
If we denote the inverse of the covariance matrix of the observables by:

(16)

the normal equations for model (15) take the form:

(17)

From the last equation of (17) it follows that:

or:

or:
(18)
This results looks already very similar to (14). However, equation (14) is expressed in terms
of covariance matrices, whereas (18) is expressed in terms of weight matrices.
Since:
                                                                yR - Variates 77

(see (16)), it follows that GRy Qy + GR QRy = 0 and thus that:
(19)
This, together with (18) proves (14).

4.3 Derived variates
In this section we will prove that formula (14) also holds for derived variates. yR-variates are
derived variates if they are functions of the observables y. Let us assume that this functional
relationship takes the form:

(20)

Then also:

(21)

Subtraction of (21) from (20) gives:

(22)
What remains to be shown is now that L equals QRy Qy-1. Application of the propagation law
of covariances to (20) gives:

or:

This, together with (22) shows that formula (14) also holds for derived variates.

4.4 Constituent variates

In this section we will prove that formula (14) holds for constituent variates. Constituent
variates are yR-variates of which the y-variates are functions. Let us assume that this
functional relationship takes the form:

(23)

Application of the propagation law of variances gives:

(24)

and application of the propagation law of covariances gives:
78 Adjustment theory

(25)
As we know e^= y - y^ follows from solving the model:
(26)
as:
(27)
With (23) we may formulate the model with condition equations also as:
(28)
When compared with (26) this means that B of (26) is replaced by L*B, that y is replaced by
yR and that Qy is replaced by QR. Instead of (27) we therefore get for model (28):
(29)
With (23), (24) and (25) this can also be written as:

or with (27) as:
which is identical to (14).
Let us, as an example of the above, "rederive" the solution of the model with condition
equations. The model with condition equations reads:
(30)
If we define the misclosure vector as:
(31)
then (30) can also be written as:
(32)
This model is also in the form of condition equations. The coefficient matrix of the condition
equations is in this case the unit matrix I. If we solve for model (32) we get for e^t = t - ^t:
(33)
                                                                                                                yR - Variates 79

We can now use our formula (14) in order to derive e^ = y - y^. This is done by interpreting
y of (31) as an yR-variate and t of (31) as an y-variate. Application of formula (14) gives then:
(34)
Application of the propagation law of variances to (31) gives:
(35)
and application of the propagation law of covariances to (31) gives:
(36)
Substitution of (33), (35) and (36) into (34) gives with (31) then finally:

Hence, we have obtained our well-known and familiar result again.
5 Mixed model representations

5.1 Introduction
In chapter 2 we developed the theory of linear estimation for the model with observation
equations:

(1)

It was shown in chapter 2 that the estimator x^ of x follows from solving the system of normal
equations:
(2)
as:
(3)

From this result the estimators y^ = Ax^ and e^ = y - y^ follow as:
(4)
and:
(5)

In chapter 3 we developed the theory of linear estimation for the model with condition equations:
(6)

In this case the estimators y^ and e^= y - y^ can be represented in terms of the matrix B as:
(7)
and:
(8)

In the present chapter we will introduce two additional model representations. The first new
model representation we will consider takes the form:

(9)

In this model representation x is an n×1 vector of unknown parameters and y is an m×1 vector
of observables. Note that (9) is a mixture of (1) and (6). If matrix B of (9) equals the identity
matrix, then (9) reduces to the model with observation equations (1). If on the other hand matrix
82 Adjustment theory

A of (9) equals the zero matrix, then (9) reduces to the model with condition equations (6). In
the next section the solution of (9) will be derived.
The second new model representation that we will consider takes the form:

(10)

This representation is in the form of observation equations with conditions on the parameter
vector x. If B=0 then (10) reduces simply to (1). The solution of (10) will be derived in section
5.3.

5.2 The model representation B*E{y}=Ax
Our starting point is the representation:

(11)

We will assume that both matrices A and B have full column rank. Thus:
(12)
Note that matrix A can only have full column rank if n £ b: the number of unknown parameters
should not exceed the number of conditions on the observations.
There are different derivations possible for deriving the estimators x^, y^ and e^ for model (11). One
possibility would be to transform (11) into a form with observation equations only. This can be
done as follows. Consider the system:
(13)

as an inhomogeneous system of linear equations with the unknown vector E{y}. Vector E{y} can
then be written as the sum of a particular solution of (13) and the homogeneous solution of:
(14)

If we denote the m ×(m b) matrix of which the column vectors are orthogonal to the column
vectors of matrix B by B^, then:
                                                                                              Mixed model representations 83

From this it follows that the solution of (14) takes the form:
(15)

This is the homogeneous solution. A particular solution of (13) is:

(16)                                              (verify !)

Hence, the solution for E{y} of (13) takes the form of the sum of the particular solution (16) and
the homogeneous solution (15):

(17)                                                                  .

Note that this representation is in terms of observation equations only. Since (17) is completely
equivalent to the first equation of (11), we can now apply our solution method for observation
equations to (17) in order to get the solution of (11).

Another possibility to solve for (11) would be to transform (11) into a form with condition
equations only. This can be done as follows. Consider again the system (13). If we denote the b × (b n)
matrix of which the column vectors are orthogonal to the column vectors of matrix A by A^, then

From this it follows that pre-multiplication of (13) with A^* gives:

(18)                                              .

Note that this representation is in terms of condition equations only. Since (18) is completely
equivalent to the first equation of (11), we can now apply our solution method for condition
equations to (18) in order to get the solution of (11).

Instead of following the above two mentioned approaches for solving (11), we will make use of
a third approach. This approach makes use of the concept of yR-variates. Define a b×1 vector t
as:
(19)

Substitution into (11) gives the representation:
(20)
84 Adjustment theory

This representation is in the form of observation equations only. Hence, we know how to
compute the estimators x^, ^t and e^t. The estimator x^ follows as:
(21)

Application of the propagation law for variances to (19) gives for Qt:
(22)

Substitution of (19) and (22) into (21) gives then for the estimator x^ of x:

(23)

The estimators ^t and e^t follow simply as:

(24)                                         .

Now, in order to obtain e^y = y - y^, note that y of (19) can be interpreted as a constituent yR-
variate.

Application of formula (14) of chapter 4 gives then:
(25)

The matrix Qyt follows from application of the propagation law of covariances to (19) as:
(26)

The estimators e^y and y^ = y - e^y follow then finally with (22), (23), (24), (25), and (26) as:

(27)

5.3 The model representation E{y}=Ax , B*x=0

Our starting point is the representation:
(29)

This representation is in the form of observation equations with conditions on the parameter
vector x. In order to derive the estimators x^, y^ and e^ we will first transform (29) into a form
with observation equations only. This is done by finding the parametric representation for:
                                                                                              Mixed model representations 85

(30)

Let us denote the n×(n-b) matrix of which the column vectors are orthogonal to the column
vectors of matrix B by B^. Then:
(31)

With (31) the parametric representation of the homogeneous system, (30) becomes:
(32)

Thus (32) is completely equivalent to (30). It is remarked that we assume that both the matrices
A and B are of full column rank; thus rank A = n and rank B = b. Note that matrix B can only
have full column rank if b £ n: one cannot impose more than n independent conditions on the
n unknown parameters.
If we substitute (32) into the first equation of (29) we get:
(33)

This representation is completely equivalent to (29) and it is in the form of observation equations
only. Thus we know how to compute the estimator l^ of l . This estimator reads:
(34)

Now let us consider the solution of (29) without the conditions on the parameter vector. This is
our standard model representation with observation equations. As you know the solution of (29)
without the conditions on the parameter vector satisfies the normal equations:
(35)

We have given x^A of (35) the subscript A to emphasize that x^A is not the solution of (29) but of:
(36)

The covariance matrix of x^A reads:
86 Adjustment theory

(37)

With (35) and (37) we can now write (34) as:
(38)

This result together with (32) gives the estimator of x for model (29):
(39)

If we look at the two formulae (38) and (39) we note that they take a very familiar form. And
indeed, (38) and (39) are the solution of the model:

(40)

This model representation is in terms of observation equations. The equivalent of (40) in terms
of condition equations follows with (31) as:

(41)

As you know the solution of (41) is:

(42)                  ^

But since (41) is equivalent to (40) and (39) is the solution to (40), (42) must be identical to
(39). This implies that the estimator x^ of x for model (29) can be written as:

(43)

We thus have now found an expression for the estimator x^ of x for model (29), which is in terms
of the matrices A and B of (29). It follows then with (35), (37) and (43) that we can compute
the estimators for model (29) as:

(44)
                                                                                              Mixed model representations 87

This result shows that the solution of (29) can be obtained in two steps. In the first step one
considers (29) without the conditions on the parameters. That is, one solves for the model with
observation equations (36). This gives the estimator x^A. Then in a second step one includes the
conditions on the parameters. That is, one solves for the model with condition equations (41).
This then finally gives the estimator x^.

It follows from (44) that:

or that:

Since                this may also be written as:

This gives finally:

(45)                                               .

Note that the first term on the right hand side of (45) corresponds with the squared norm of the
least-squares residual vector of the model E{y}=Ax, whereas the second term on the right hand
side of (45) corresponds with the squared norm of the least-squares residual vector of the model
B E x^ 0.

             A
6 Partitioned model representations

6.1 Introduction

In chapters 2 and 3 the theory of linear estimation was developed for the model with observation

equations and for the model with condition equations. Recall from chapter 2 that the estimators

of the model:

(1)               1)

are given as:

(2)

And recall from chapter 3 that the estimators of the model:
(3)
are given as:

(4)

In this chapter we will partition the matrices A and B of (1) and (3) respectively, columnwise and
rowwise. The following four cases will therefore be considered in this chapter:

                  1o

     1) The notation "D{y}" will be used instead of the notation "E{(y-Ax)(y-Ax)*}". The
         kernel "D" stands for "Dispersion".
90 Adjustment theory

                  2o

                  3o

                  4o 2)

In this chapter it will be shown how the above partitioning of A and B can be carried through
to the estimators of (2) and (4) respectively.

6.2 The partitioned model
Let us consider the partitioned model:
(5)
The corresponding partitioned system of normal equations reads:
(6)
Now let us assume that for a particular application we are only interested in the estimator x^1.
There are two ways for finding x^1. Either we solve the complete system of normal equations (6)
and then look at the first n1-elements of the estimator x^. Or, we eliminate x^2 from the first n1
equations of (6) and solve for x^1. This second approach is more efficient since it gets round the
necessity of solving the complete system of normal equations. The estimator x^2 gets eliminated
from the first n1 equations of (6), if we premultiply (6) with the square and full rank matrix:
(7)

Premultiplication of (6) with (7) gives:

     2) This case will not be dealt with explicitely in this chapter.
                                         Partitioned model representations 91

(8)

In the first n1 equations of (8) we recognize the expression for the orthogonal projector:
(9)

Using the properties:
(10)

and the abbreviation:
(11)

we may therefore write (8) also as:

(12)                                     3)

The solution for x^1 follows therefore as:
(13)

Application of the propagation law of variances gives:
(14)

Once x^1 is known, x^2 can be found from the last n2 equations of (12). This gives:
(15)

Application of the propagation law of variances gives with (13) and (14):

Since                  this reduces to:
(16)

Summarizing, we thus have shown that:

      3) The normal matrix of this system is called a "reduced normal matrix". It is said to be
          reduced for x2.
92 Adjustment theory

(17)

Instead of eliminating the estimator x^2 from the first n1 equations of (6), one may also
eliminate the estimator x^1 from the last n2 equations of (6). This is achieved by premultiplying
(6) with the square and full rank matrix:
(18)

This gives analogous to (12) the equations:
(19)
with
(20)
From (19) the estimators x^1 and x^2 and their variance matrices follow as:

(21)

Compare (21) with (17) and note that (21) follows from (17) by interchanging the role of x1
and x2. The above results can now be used to investigate what happens to the precision of the
estimators when a model with observation equations is enlarged with additional parameters.
Let us assume that our original model is given as:
(22)

The corresponding estimator and its variance matrix will be denoted by and
respectively. Then:
(23)
                                              Partitioned model representations 93

Suppose now that model (22) is enlarged to :
(24)

Then according to (17):
(25)

Note the resemblance between (23) and (25). The two estimators and are identical if

and only if A1  A1 , that is if:
(26)

This is the case if the range space of A1 is orthogonal to the range space of A2, that is if
R(A1)^R(A2) or A1*Qy-1A2=0. Note that in this case the partitioned normal matrix of (6)
becomes blockdiagonal. We may thus conclude that the estimator of x1 is not affected by an

enlargement of the model if and only if R(A1) is orthogonal to R(A2). In order to find out what

happens when R(A1) and R(A2) are not orthogonal to each other, we take instead of the
expression Qx^ (A1Qy 1A1) 1 , the following expression from (21):

                                                1

With (23) this can be written as:
(27)

Now suppose we are interested in a particular linear function,  . With model

(22) the estimator of reads        and with model (24) we get as estimator of ,

Their variances read:

With (27) this gives:

Since the second term on the right hand side is always non-negative, it follows that:
(28)

This important result shows that the precision of the estimators generally gets poorer if more
parameters are added to the linear model.
94 Adjustment theory

Let us now investigate the variance matrix of the estimator of the added parameter vector x2.
According to (21), Qx^ is the inverse of the matrix A2Qy 1A2 ; see also (19). With A2 PA1 ^A2 ,

                                                                            2

we may write:
(29)

Since PA A2 A2 if and only if R(A2)ÃR(A1), it follows that A2 Qy 1 A2 0 if the column
                               1

vectors of A2 are linear dependent of the column vectors of A1. Thus, Qx^ does not exist and
                                                                                                                                                                                                                                                          2

x2 is not estimable if R(A2)ÃR(A1). In this case matrix (A1 A2) also fails to be of full rank.
Now assume that x2 is a scalar. Then n2=1 and A2 is a vector. With the cosine rule it follows
then that:
(30)

see figure 6.1. But we also have that:
(31)

From (30) and (31) it follows that:
(32)

This together with (29) shows that:

(33)

                          Figure 6.1: A2 Qy 1 PA A2                                                                                                    A2  PA A2 cos a
                                                                                                                                                    1              1

From this it follows that:
(34)

The angle a measures the degree of dependence between A2 and A1. If a= 1 p, then A2 is                                                                                     2
      2  1                                                                                                                                                              2
orthogonal to R(A1) and sx^2 (A2 Qy A2) . If a=0, then A2OER(A1) and sx^2 · . If a is small,1

then sx^2 2 is large and x2 is poorly estimable. Since (see(33)):

(35)
                                                                                   Partitioned model representations 95

the degree of dependence between A2 and A1 can be computed in a rather straight forward
manner from the ratio of the diagonal elements of the normal matrix after and before
reduction. Compare (19) and (6). The computation of the scalar sin2a is usually included in
the software in order to diagnose and detect possible singularities or rankdefects in the normal
matrix.
So far we have been concerned with the estimators x^1 and x^2. Let us now consider the
estimator y^. Since y^ A1x^1 A2x^2 , it follows from (17) that:

or:
(36)

Since y^=PAy, with A=(A1 A2), we have with (36) the following orthogonal decomposition of
the orthogonal projector PA:
(37)

In a completely analogous way it follows from (21) that:
(38)
and:
(39)

The above orthogonal decomposition is shown in figure 6.2.
96 Adjustment theory

                                    Figure 6.2:
Since e^ (I PA) y PA^y , it follows with (39) that:
(40)
where e^1 is the least-squares residual vector of model (22). From (40) it follows that:
(41)

But:
Hence, (41) can be written as:

or as:
(42)

This important results shows that the length of the residual vector generally gets smaller if

more paremeters are added to the linear model. In the extreme case that the number of

parameters equals the number of observations, that is m=n1+n2, matrix A=(A1 A2) becomes

square and regular, and PA reduces to the identity matrix I. In this case (see(39))
I PA PA or PA PA1 ^ , and therefore e^1 PA1 ^y PA y . With (42) this gives e^ 2 0 . Thus
1  2  2                                               2

the least-squares residual vector is identically zero if the redundancy is zero.

The results of this section will play an important role and are used repeatedly in testing
theory.

Example 1:
The model that will be considered first is given as:
                                                                                   Partitioned model representations 97

(43)

                                                                1
Since  the  observation  equations  are  of  the  form  E  y    ai x1 , they describe the equation of

                                                             i
a straight line through the origin with slope x11 . This is shown in figure 6.3.

                      Figure 6.3: The line E y a x11 with slope x11 tan f
The least-squares estimate of x11 follows from the minimization problem:

Since yi ai x11 is the vertical distance from the point (ai, yi) to the straight line
E y a x11 , the least-squares estimate follows from a minimization of the sum of the
squares of these vertical distances. The least-squares estimator reads:
(44)
Its variance is given as:
(45)
Let us now consider the enlarged linear model:
98 Adjustment theory

(46)
In this case the observation equations E{yi}=aix1+x2 describe a straight line with intercept x2
and slope x1. This is shown in figure 6.4.

         Figure 6.4: The line E y a x1 x2 with slope x1 tan f and intercept x2
With the definitions A1 (a1, a2, , am ) and A2 (1 1) it follows that:

(47)

In order to simplify the expression for A somewhat, we define:
(48)
We now may write A as:
(49)
Using (17) and (49) the least-squares estimator x^1 becomes:
                                                                                   Partitioned model representations 99

(50)

Compare (50) with (44). The variance of x^1 is given as:
(51)

According to the theory of this section  or:

(52)
should hold. This is easily verified. From (52) it follows that:

or:

or:

or:

or:
or:

Hence, inequality (52) holds indeed. Since A1 (a1, a2, , am ) and A2=(1...1)*, it follows with
(50) from (17) that the least-squares estimator x^2 is given as:

or as:
(53)

The variance of x^2 follows from (17) as:
100 Adjustment theory

With (48) and (51) this gives:
(54)

6.3 The partitioned model
Recursive estimation
Consider the partitioned model:
(55)

Note that it is assumed that y1 and y2 are uncorrelated. This is usually the case in practical

applications.    The   least-squares  estimator    of  x                of  model       (55)  will  be  denoted  as  x^ .   It  reads:

                                                                                                                       (2)

(56)

Let us now consider the partial model:
(57)

Its solution will be denoted as x^ . It reads:

                                                                   (1)

(58)

Using this result, we may write (56) as:
(59)
But this is also the solution of:
(60)

Hence we have proven that the solution of the partitioned model (55) can be found in two

steps: First one solves for the partial model (57). This gives x^(1) (1) and Qx^ . Then in the second

step  one  uses  this  result  together  with  y2  to  find                 x^     via  model  (60).    This  two  step  procedure

                                                                              (2)
                                                                                         Partitioned model representations 101

has an important practical implication. It implies that if new observables, say y2, become

available  one             does   not  need     to  save  the  old  observables     y1   to  compute  x^ .   One  can  compute x^

                                                                                                        (2)                                (2)

from  the  solution               x^        of  the  first  step   and   the  new   observables       y2.  In  this  way  one  can

                                    (1)

recursively determine x^(k) from x^(k 1) and yk. This is shown in table 6.1. Expression (59) for x^(2)

shows that a matrix of the order n needs to be inverted. One can however also derive an

expression  for            x^     in     which  a    matrix    of   the  order  m2  needs    to  be   inverted.   This  expression

                             (2)

is found if we solve (60) via the model of condition equations. Model (60) in terms of

condition equations reads:

(61)

Its solution reads:

Hence for x^ we get:

                        (2)

(62)

Application of the propagation law of variances gives:
(63)

Both the expressions (59) and (62) give identical results. But expression (62) is more

advantageous if m2 is small compared to n. In particular if m2=1, only a scalar needs to be

                                                                                                      1           1
inverted in (62), whereas in (59) all three matrices Qx^ , Q2 and (Qx^(1) A2 Q2 A2) need to be
                                                                                    (1)

inverted.

The equations (59) and (62) are called measurement update equations. Expression (62) clearly

shows how x^(2) is found from updating x^(1). The correction to x^(1) depends on the difference

y    A2x^(1) . Since              A2x^ (1)  can be interpreted as the prediction of E{y2} based on y1, the

  2

difference y               A2x^(1) is called the predicted residual of E{y2}. Note that the predicted residual

                        2

is not equal to the least-squares residual of E{y2}. This least-squares residual reads namely

y2 - y^2 = y2 - A2x^(2).

Expression (62) shows that the correction to x^(1) is small if the predicted residual is small, and
that the correction to x^(1) is large if also the predicted residual is large. This is also what one
would expect. Also note that the correction to x^(1) is small if the variance of x^(1) is small. This
is also understandable, because if the variance of x^(1) is small one has more confidence in x^(1)
and therefore would like to give more weight to x^(1) than to y2.
102 Adjustment theory

                       Partitioned model

                       Recursive estimation

                       Table 6.1: Recursive estimation (A-form)
                                                                                 Partitioned model representations 103

Expression (63) shows how the variance of the estimator gets updated. Because of the minus
sign in (63) the precision of the estimator gets better. This is understandable, because by
including the additional observable y2 more information is available to estimate x. Table 6.2
shows the recursive estimation scheme based on (62) and (63).

Apart from the recursive scheme for the estimator of x, it is also possible to derive a recursive
scheme for e^*Qy-1e^. Consider again the partitioned model:

Then:
(64)

If we use the abbreviation:
(65)

we may write the measurements update equation (62) as:
(66)

Substitution of (66) into the first term on the right-hand side of (64) gives:
(67)

            1         1                1
Since A1 Q1 (y1 A1x^(1)) 0 and Qx^(1) A1 Q1 A1 , equation (67) can be written as:

(68)

Since  y    A1x^ (1)  is the least-squares residual vector of the partial model:

         1

(69)

we have:
(70)

Substitution of (70) into (68) gives:
(71)
104 Adjustment theory

                       Partitioned model

                       Recursive estimation

                       Table 6.2: Recursive estimation (B-form)
                                                                                 Partitioned model representations 105

Substitution of (66) into the second term on the right-hand side of (64) gives together with
(65):
(72)
Substitution of (71) and (72) into (64) shows finally that:
(73)
This equation shows how the norm of the least-squares residual vector should be updated
when the observable y2 becomes available. The update depends on the predicted residual.
Expression (73) can be generalized to more steps. This is shown in table 6.3.

                                             Partitioned model

                                             Recursive update

                            Table 6.3: Recursive update of
So far it was assumed that the observables y1 and y2 are uncorrelated. Let us now assume that
they are correlated. Hence, we consider the partitioned model:
106 Adjustment theory

(74)

With Q12{0, Q21{0. In this case it is not possible to apply the above derived recursive
schemes directly. In order to be able to apply the above derived recursive schemes, the
observables y1 and y2 first need to be de-correlated. This is possible if we transform model
(74) such that the transformed observables are uncorrelated. For instance, if we define new
observables as:

(75)

then it follows with (74) that:
(76)

with:
(77)
Similarly, if we define new observables as:

(78)

we get with (74) that:
(79)

with:
(80)

With (76) or (79) it is now possible again to apply the recursive schemes. In this case

however the recursive schemes lose their practical implication. The transformed observable

y    of  (76)  reads  namely:

  2

This   shows   that   one  still  needs  to  save  the  old  observable  y1  for  computing  x^     from  x^     and

                                                                                               (2)          (1)

y2. The conclusion reads therefore that the recursive schemes of this section are only of
                                                                                 Partitioned model representations 107

practical value if the observables are uncorrelated. Fortunately this is the case in most
practical applications.
The theory as developed in this section plays an important role in the estimation theory of
dynamic systems and in the theory of Kalman filtering.
Example 2
Consider the linear model:

(81)

Its solution reads:
(82)
The solution of the partial model:

(83)

reads:
(84)

According to table 6.2, the solution of (81) can also be written as:

(85)
108 Adjustment theory

We will now show that (85) is indeed identical to (82). We will first consider the first
equation of (85). This equation can be written as:
or as:
or with (84) as:

or as:

which indeed is identical to the first equation of (82). Now consider the second equation of
(85). This can be written as:
or as:
or with (84) as:

or as:
which is indeed identical to the second equation of (82).
                                            Partitioned model representations 109

6.4 The partitioned model
Block estimation I
Consider again the partitioned model:
(86)

Its system of normal equations reads:
(87)
Now consider the two partial models:
(88)

Their systems of normal equations read: 4)
(89)

Comparison of (89) with (87) shows that the normal matrix of model (86) can be found from
the sum of the two normalmatrices of the two partial models in (88), and that the righthand
side of (87) can be found from the sum of the two right hand sides of the normal equations
in (89).

Let us now generalize model (86) to:

(90)

The corresponding partitioned system of normal equations reads:

      4) Do not confuse x^ of (89) with x^ of the previous section.
      (2)                              (2)
110 Adjustment theory

(91)

The estimator x^1 gets eliminated from the second set of normal equations if we premultiply
(91) with the square and full rank matrix:
(92)
This gives:

(93)

where we have used the abbreviation:  (see also section 6.2)
(94)

The estimator x^3 gets eliminated from the second set of normal equations if we premultiply
(93) with the square and full rank matrix:

(95)

This gives:

(96)

where we have used the abbreviation:
(97)
                                                                                 Partitioned model representations 111

The partially reduced system of normal equations (96) can now be solved in the following
way. First one uses the second set of normal equations in (96) to solve for x^2. Once x^2 is
known, it can be substituted into the first and third set of normal equations in (96) to solve
for x^1 and x^3 respectively. Thus the solution of (96) is found as:

(98)

What is the practial implication of the above solution method? Let us assume that two
countries, say the Netherlands and Germany, are covered with a large geodetic network. (See
figure 6.5.)

             Figure 6.5: Geodetic network covering the Netherlands and Germany
The Dutch part of the network (full lines in figure 6.5) has been measured by the Dutch
Triangulation Department (Rijksdriehoeksmeting), and the German part of the network
(dashed lines in figure 6.5) has been measured by the German Triangulation Department. We
will assume that the vector x1 contains the unknown coordinates of the network points in the
Netherlands, that the vector x2 contains the unknown coordinates of the network points on the
border, and that the vector x3 contains the unknown coordinates of the network points in
Germany.
The partial model of the Dutch part of the network reads then:
(99)

Similarly, the partial model of the German part of the network reads:
(100)
112 Adjustment theory

There are now two ways in which a Computing Centre can perform the adjustment of the
geodetic network. The first method consists of the following: Both the Dutch and the
Germans send their partial models, (99) and (100), together with the measured data to the
Computing Centre. The Computing Centre merges the two partial models to get (90), the
complete model. This model is then solved by the Computing Centre for x^1, x^2 and x^3. The
solution x^1, x^2 is then sent by the Computing Centre back to the Dutch, and similarly the
solution x^2, x^3 is sent back to the Germans. The second method consists of the following:
based on their partial model, the Dutch form their partial system of normal equations and
reduce it for x1. This gives:

(101)

Similarly, the Germans form, on the basis of their partial model, their partial system of
normal equations and reduce it for x3. This gives:

(102)

The Dutch then sent their reduced normal block A12Q1 1A12 and the reduced right-hand side

       to the Computing Centre. And the Germans do the same for their reduced normal

block  A22Q2  1A22  and their reduced right-hand side  A22Q2  1y    .  The  reduced  normal  blocks  and

                                                                 2

the reduced right-hand sides are then added by the Computing Centre to form the system:

(103)

This system is solved by the Computing Centre for x^2 (see(98a)). The solution x^2 is then sent
back to both the Dutch and the Germans. With x^2 and (101) the Dutch form the system:
(104)

This system is solved by the Dutch for x^1 (see (98b)). In a similar way the Germans use x^2
and (102) to form the system:
(105)

This system is solved by the Germans for x^3 (see (98c)).

When we compare the above two methods we note that:

1o. The computational load for the Computing Centre is less with the second method than
       with the first method. With the second method some of the computational load stays
       namely at the two national Triangulation Departments. The order of the system to be
       solved by the Computing Centre is then equal to the dimension of x2 (see (103)).
                                                    Partitioned model representations 113

2o. With the second method the Computing Centre never gets to know the original

measured  data,  since  Ai2Qi  1y ,i  1,2,      is  provided  and  not  yi,  i=1,2.  Hence,  the

                                  i

Computing Centre will never be able to compute the complete network. This may be

an advantage in case secrecy of data and coordinates is asked for.

The above developed second method was orginally introduced by the famous German
geodesist F.R. Helmert (1843-1917). The method is known as Helmert's Block method. The
method has been used with great success for the readjustment of the European triangulation
network (RETrig: Réseau Européen Trigonometrique).

6.5 The partitioned model

Block estimation II
Consider again the partioned model:
(106)

Its solution reads:
(107)
Now consider the two partial models:
(108)

Their solutions read:
(109)

Using this results we may write (107) also as:
(110)
But this is also the solution of:
(111)
114 Adjustment theory

This model is in terms of observation equations. Its equivalent form in terms of condition
equations reads:
(112)

And its solution reads:
(113)

Hence we have the following two equivalent expressions for the solution of (112):
(114)

and:
(115)

Note that (115) follows from (114) by interchanging the role of x^(1) and x^(2) . The estimators
of (114) and (115) are identical to (110) and therefore also identical to (107). Hence, we have
shown how the solution of the partitioned model (106) can be obtained in three separate steps.
This is shown in table 6.4.
                                                                      Partitioned model representations 115

                                   Partitioned model

                                    Block estimation
1o.
2o.
3o.
 or

                             Table 6.4: Block estimation
116 Adjustment theory

We will now generalize the above results. Consider therefore the partitioned model:
(116)
Its solution reads:
(117)
Now consider the two partial models:
(118)
Their solutions read:
(119)

If we use the notation:

and a similar notation for Qx^(2) , it follows with (119) that (117) may also be written as:
       Partitioned model representations 117

(120)

But this is also the solution of:
(121)
This model is in terms of observation equations. Its equivalent form in terms of condition
equations reads:

(122)

Note that in this model, and are free yR-variates. The solution of (122) reads:

(123)

Note that the solution (123) generalizes solution (113). The above results are summarized in
table 6.5.
118 Adjustment theory

 (1)
                                         Solving the partial model

 (2)
                                                     gives:

 (3)
                                         Solving the partial model

                                                     gives:
 (4)

                With (3) and (4) we formulate the model of condition equations:

 (5)
                                             Its solution reads:

 (6)

                                     This is also the solution of (1).
                                      Table 6.5: Block estimation
                                                                                 Partitioned model representations 119

Example 3
Consider the levelling network of figure 6.6. Point 0 is taken as reference point.

                                     Figure 6.6: Levelling network
Its height is assumed to be known and equal to zero, that is x0=0. The corresponding model
of observation equations reads then:

(124)

The normal equations read:
(125)
In order to solve (125), we first eliminate x^1 from the second normal equation. This is
achieved if we premultiply (125) with the square and regular matrix:
120 Adjustment theory

This gives:
(126)

The estimators x^2 and x^3 follow then from:

as
(127)
Substitution of x^2 into the first equation of (126) gives for x^1:
(128)
With (127) and (128) we have found the solution of (124). Let us now try to solve (124)
using the block estimation scheme of table 6.6. The two partial models read:
(129)

and:
(130)
The system of normal equations for partial model (129) reads:
(131)
From this it follows that:
(132)
                                                                                 Partitioned model representations 121

The system of normal equations for partial model (130) reads:

(133)

From this it follows that:
(134)

It will be clear that (132) is the solution of the partial levelling network shown in figure 6.7a,
and that (134) is the solution of the partial levelling network shown in figure 6.7b.

       (a)  (b)

       Figure 6.7: Two partial levelling networks

Now that the solutions of the two partial networks have been found, we can connect the two
networks by formulating the model of condition equations:

(135)

Note that the number of condition equations in (135) equals one. Thus the redundancy equals
one. The redundancy in partial model (130) equals zero and the redundancy of partial model
(129) equals one. Hence the total redundancy equals 1+0+1=2. And this is of course equal
to the number of linear independent condition equations that can be found in the levelling
network of figure 6.6.

The solution of (135) follows as (see also (123)):
122 Adjustment theory

(136)

We will now verify that solution (136) is indeed identical to (127) and (128). Substitution of

x^ (1) , x^ (1) and x^ (2) from (132) and (134) into the first equation of (136) gives:
1      2  2

(137)

And this is indeed identical to (128). The verification of the remaining equations in (136) is
left to the reader.

The block estimation scheme of table 6.5 that has been developed in this section, can also be
derived from the mixed model representation of section 5.3. To see this, note that (116) is
equivalent to:

(138)

But this representation is of the form:
(139)

Show for yourself that the block estimation scheme of table 6.5 follows when the results of
section 5.3 are applied to model (138). According to (45) of section 5.3 we have

If we "translate" this result to model (138) we get with                                 and

                       :
                                                                                 Partitioned model representations 123

(140)

This shows that the squared norm of the residuals of model (138) and thus of model (116),
can be found from the sum of the squared norm of the residuals of the three models in (118)
and (122).

6.6 The partitioned model
Estimation in phases
In this section we consider the partitioned model of condition equations:

(141)

It will be shown that the solution of this model, which will be denoted by y^B, can be found
by first solving the partial model:
(142)

which gives y^ B1 B1 and Qy^ , and then solving for the model:
(143)

This solution method is known as the method of estimation in phases (fase vereffening). This
method was originally introduced by the famous Dutch geodesist J.M. Tienstra (1895-1951).
Tienstra was appointed professor in Mathematical Geodesy in 1935 at the Delft University
of Technology.

With B=(B1 B2), the solution of (141) reads:
(144)

If we define the vector random variable z^ as:
(145)

then   is the solution of the partitioned system of these "normal equations":

(146)
124 Adjustment theory

The estimator z^1 can be eliminated from the second set of normal equations if we premultiply
(146) with the square and full rank matrix:

This gives:
(147)
In (147) we recognize the orthogonal projector:

Since:
(148)
we may write (147), using the abbreviation:
(149)
also as:
(150)
The solution of this reduced system of normal equations follows then as:
(151)
With B=(B1 B2) it follows from (144) and (145) that y^B can be written as:

Substitution of (151) gives:
or:
or, with (149):

or:
(152)
                                                      Partitioned model representations 125

Clearly:
(153)

is the solution of (142). The variance matrix of   reads:
(154)

Substitution of (153) and (154) into (152) gives:
(155)

And this is clearly the solution of (143). Hence we have shown that the solution of the
partitioned model of condition equations (141) can indeed be found by first solving for the
partial model (142) and then solving for the model (143).

Let us now consider the least-squares residual vector e^B=y-y^B. From (155) it follows that:
(156)

With:

and:

this shows that:

(157)                                              .

Hence, the least-squares residual vector of the partitioned model (141) can be found from the
sum of the least-squares residual vectors of the two separate models (142) and (143). From
(157) it follows that:

(158)                                                      .

Since:

because:
it follows that (158) reduces to:
126 Adjustment theory

(159)

From the result it seems that one needs Qy for computing the scalar       e^B Qy     1e^    .  If  this  would

                                                                              2         B2
be the case, it would disrupt the method of estimation in phases, because it would imply that

Qy is also needed in the second phase, that is, when model (143) is solved for. Fortunately
                                                                       1
one can show that Qy is not required for computing the scalar  e^B Qy   e^    .  In  order     to  show  this

we define the vectors of misclosures:                          2          B2

(160)

With (160) we have:

From this it follows that:                                     (see (154)),
(161)
With (160) we also have:
(162)
Since:

it follows from (162) that:
(163)
Substitution of (161) and (163) into (159) finally gives:
(164)

A summary of the above results is given in table 6.6.
                                                                        Partitioned model representations 127

                                     Partitioned model

1.
2.

                            Table 6.6: Estimation in phases
128 Adjustment theory

Example 4
Consider the levelling network of figure 6.8.

                               Figure 6.8: A one loop levelling network
The corresponding model of condition equations reads:
(165)
Its solution reads:

or:
(166)
The variance matrix of (166) reads:
(167)
Now let us assume that the levelling network of figure 6.8 is enlarged with two additional
height difference observables, such that the two loop levelling network of figure 6.9 results.
                                                                                 Partitioned model representations 129

                               Figure 6.9: A two loop levelling network
The corresponding model of condition equations reads:
(168)
According to table 6.6, this model can be solved in two phases. The first phase would then
correspond to solving the partial model:
(169)
Its solution reads:
130 Adjustment theory

or:

(170)

The variance matrix of (170) reads:

(171)

Comparison of (170) and (171) with (166) and (167) shows that the results of the first phase
for the first three observables is identical to the solution of (165). Also note that the last two
observables do not get a correction in the first phase. Hence, they do not play a role in the
first phase. It should be recognized however that this is a consequence of the assumption that
y4 and y5 are not correlated with y1, y2 and y3. This is fortunately the case in most practical
applications.
In order to obtain the final solution of (168) we have to solve in the second phase the partial
model:

(172)
                                                   Partitioned model representations 131

With (170) and (171) the solution of (172) reads:

or:
(173)

It will be clear that if point 0 in figure 6.9 is taken as reference point with a height equal to

zero, then x^ y^ , x^ y^ and x^ y^ . Compare now (173) with (127), (128) and also (137).
       11  2  3  34

There exists a close correspondence between the results of this section and the results of
section 6.2. In section 6.2 we considered the partitioned model:

(174)

and the partial model:
(175)

In the present section we considered the partitioned model:

(176)

and the partial model:
(177)
132 Adjustment theory

Since model (175) follows from model (174) by setting x2 equal to zero, the redundancy of
model (175) is higher than that of (174). In fact the redundancy of (174) is m-n1-n2, whereas
the redundancy of (175) is m-n1. Since the redundancy of model (175) is higher than that of
(174), more condition equations can be formed for model (175) than for model (174). Hence,
one may consider (176) to be the equivalent of model (175), and model (177) to be the
equivalent of model (174). With A=(A1 A2) and B=(B1 B2) this gives:

(178)

This implies that the solution of (176) is identical to the solution of (175), and that the
solution of (177) is identical to the solution of (174). For the two models (174) and (175) we
have the orthogonal decomposition:

(179)                                      (see (39))

In a similar way we have for the two models (176) and (177) the orthogonal decomposition:
(180)

With (179) and (180) it is now possible to relate the various orthogonal projectors to each
other. Clearly:

(181)

And with (179) and (180) this shows that:
(182)

The above results can now be used to show the correspondence between the solutions of the
four models (174), (175), (176) and (177). An overview of this correspondence is given in
table 6.7.

With the results of table 6.7 we can now also give an alternative proof of the method of
estimation in phases. Since:

it follows that:
                                                            Partitioned model representations 133

Table 6.7: Correspondence between partitioned condition
              equations and partitioned observation equations
134 Adjustment theory

Substitution of PA PA PA in the right-hand side gives:
1                      2

But PA^PA 0 . Hence:
(183)

With:

equation (183) becomes:
(184)

The geometry of this relation is shown in figure 6.10.

                              Figure 6.10: Geometry of PQyB ^ PQ B ^ PQyB1 ^
                                                                                                                                                                                                              y2

Relation (184) already shows the two phases of the method of estimation in phases.
Substitution of:

                                                                                                   (see (180)),
into:

using:

and:

gives:
(185)
                                                                                 Partitioned model representations 135

Since the variance matrix of y^B PQyB1 ^ y is given by:
                                                                                                            1

it follows that (185) may be written as:

And this shows that:
(186)
This concludes the proof of the method of estimation in phases.
7 Nonlinear models, linearization, iteration

7.1. The nonlinear A-model

7.1.1 Nonlinear observation equations

Up to this point the theoretical development was based on the assumption that the m-vector
E{_y} is linearly related to the n-vector of unknown parameters x. In geodetic applications
there are however only a few cases where this assumption truly holds. A typical example is
levelling. In the majority of applications however the m-vector E{_y} is nonlinearly related to
the n-vector of unknown parameters x. This implies that instead of the linear A-model (1) of
section 6.1, we are generally dealing with a nonlinear model of observation equations:

(1)

where A(.) is a nonlinear vector function from n into m. The following definition makes
clear when a vector function A(.) : n Æ m is linear or nonlinear.

Definition: The vector function A(.) : n Æ m is linear if and only if:

(2)                                        .

The vector function A(.) is said to be nonlinear if it does not satisfy (2).

Example 1
Verify yourself that the vector function:

is linear and from 2 into 3.
Example 2
Verify yourself that the vector function:
138 Adjustment theory

is nonlinear and from 2 into 3.
Example 3
Consider the levelling network of figure 7.1.

                                      Figure 7.1: Levelling network
Point 0 is taken as reference point. Its height is assumed to be known and equal to zero, that
is, x0=0. The observed height differences are assumed to be uncorrelated and to have equal
variances s2. The corresponding model of observation equations reads then:

(3)

Verify yourself that the vector function A(x) of (3) is linear and from 3 into 5.
                                                     Nonlinear models 139

Example 4

Consider the configuration of figure 7.2a. The Cartesian x,y coordinates of the three points

1, 2 and 3 are known and the two Cartesian coordinates x4 and y4 of point 4 are unknown.
The observables consist of the three distance variates _l14, _l24 and _l34 . These observables are
assumed to be uncorrelated and to have equal variances s2. Since distance and coordinates
are related as (see figure 7.2b):

the model of observation equations for the configuration of figure 7.2a reads:

(4)

This model consists of 3 nonlinear observation equations in the 2 unknown parameters x4 and
y4. Hence, the vector function A(x) of (4) is nonlinear and from 2 into 3.

           (a)                                  (b)

                Figure 7.2: Distance resection

Example 5
Consider the parabola of figure 7.3a.
140 Adjustment theory

                     (a)                             (b)                 (c)

         Figure 7.3         (a) The parabola y=ax2+b
                            (b) The points (o) on the parabola that need to be digitized.
                            (c) The sample values ( ) of the digitized points

We are asked to determine the values of the parameters a and b of the parabola y=ax2+b.

In order to solve this problem we decide to measure the x,y coordinates of an n-number of

points on the parabola. The coordinates of the points 1,2, .., n (see figure 7.3b) are measured

with  a  digitizer.  These  coordinate  observables  (x ,   y ),  i  1,  ,n, are assumed to be uncorrelated

                                                         i    i
and to have equal variances s2. The observed or sample values of the coordinates are shown
in figure 7.3c. The model of observation equations for this problem reads:

(5)

This model consists of 2n observation equations in (n+2) unknown parameters, namely x1,...,
xn and a,b. Note that the first n observation equations are linear, but that the second set of n
observation equations are nonlinear. Hence, the vector function A(x), with x=(x1, ,xn,a,b)*, of
(5) is nonlinear and from n+2 into 2n.
           Nonlinear models 141

Example 6

Consider figure 7.4. It shows an equilateral triangle. The position, orientation and size of this
triangle are unknown. In order to determine these parameters, the x, y coordinates of the three
points 1, 2 and 3 are observed.

                                   Figure 7.4: An equilateral triangle
The coordinate observables (xi, yi), i = 1, 2, 3, are assumed to be uncorrelated and to have
equal variances s2. The corresponding model of observation equations reads:

(6)

This model consists of 6 observation equations in 4 unknown parameters, namely x1, y1, a and a.
The vector function A(x) of (6), with x=(x1, y1,a,a)*, is nonlinear and from 4 into 6.
142 Adjustment theory

7.1.2 Linearization: Taylor's theorem
In the previous section a number of examples were given of nonlinear A-models. We do
know how to compute least-squares estimators in case of a linear A-model. But what about
a nonlinear A-model? How should we compute the least-squares estimators if the model of
observation equations is nonlinear? For the majority of non-linear problems the solution is to
approximate the originally nonlinear A-model with a linear(ized) one. This approximation is
made possible with the theorem of Taylor (see also appendix A). This section is therefore
devoted to a discussion of Taylor's theorem.
Taylor's theorem for f: Æ
Let f: Æ be a function of which all its qth-order derivatives exist and for which
 q d q f (x)is continuous. Then a scalar q between x and x 0 exists such that:

 dx

(7)

with D x x x o and with the remainder:

(8)

It follows from (7) and (8) that for q = 2 we have:

(9)
Note that the second term within the brackets can be made arbitrarily small with respect to
the first term within the brackets, by letting x0 approach x. If this second term is negligible
with respect to the first term, we may decide to ignore it, and approximate (9) as:

(10)

In this case we speak of a linear approximation of f(x) or a linearization of f(x) at x0. A
geometric interpretation of (10) is given in figure 7.5.
                                                                                Nonlinear models 143

Figure 7.5:  The curve y f(x) and its tangent line y f(x 0) d f(x 0)(x x 0)
                                                                                dx
             at the point (x 0, f(x 0))

Example 7
The linearization of f (x) tan x at x 0 reads:

                                                                          .
Example 8
The linearization of f (x) ax 2 b at x0 reads:

                                                                             .

We will now consider functions of more than one variable. A function (x1, ,xn) of the
variables x1, ,xn will be denoted as f(x) with the n-vector x=(x1, ,xn)*. The first-order partial
derivatives of f(x) , xxx f(x) , a a 1, ,n, will be denoted as x x2 a f(x),2 a 1, ,n. Similarly, we
denote the second-order partial derivatives of f(x), xxaxxb f(x), as xabf(x), a, b 1, ,n. And so
on.

Taylor's Theorem for f: n Æ :

Let f: n Æ be a function of which all its qth-order partial derivatives exist and for which
              is continuous. Let D x x x 0 and q x 0 t (x x 0) with tOE . Then a scalar tOE(0,1)

exists such that:
144 Adjustment theory

(11)

with the remainder:
(12)

See appendix A for a proof of this theorem. Note that (11) and (12) reduce to (7) and (8) for
n = 1.
For the case q = 2, it follows from (11) and (12) that:
(13)

If we introduce the gradient vector and Hessian matrix of f(x) respectively as:

                       and  ,

then equation (13) may be written in the more compact matrix-vector form as:

or as:

(14)

Note that this equation reduces to (9) in case n = 1. Equation (14) shows that a nonlinear
function f: n Æ can be written as the sum of three terms. The first term in this sum is the
zero-order term f(x0). The zero-order term depends on x0 but is independent of x. The second
term in the sum of (14) is the first-order term xx(x0)*Dx. It depends on x0 and is linearly
dependent on x. Finally, the third term in the sum is the second-order remainder R2(q,Dx).
                                                                                                       Nonlinear models 145

An important consequence of Taylor's theorem is that the remainder R2(q,Dx) can be made
arbitrarily small by choosing x0 close enough to x. Now assume that x0 is indeed chosen close
enough to x, such that the second-order remainder can be neglected. Then, instead of (14) we
may write to a sufficient approximation:
(15)
Hence, if x0 is sufficiently close to x, the nonlinear function f: n Æ can be approximated
to a sufficient degree by the function (x0)+xx(x0)*Dx which is linear in x. This function is
called the linearization of f(x) at x0. Note that (15) is the n-dimensional generalization of (10).

Example 9
The linearization of f(x1, x2) (x12 x22)½ at (x10, x20) reads:

Example 10
The linearization of (x1,x2) = x12 + sin x2 at (x10,x20) reads:
146 Adjustment theory

7.1.3 The linearized A-model
In the previous section we have seen how Taylor's theorem can be applied to obtain a linear
approximation of a nonlinear function. In this section we will see how the results of the
previous section can be used to approximate the originally nonlinear A-model:

(16)

with its linearized version. Consider the nonlinear vector function A(.): n Æ m:

(17)

Each function ai(x), i=1, ,m, of (17) is a function from n into . Hence, each function ai(x),
i=1, ,m may be linearized according to (15). This gives:

(18)  .

If we denote the m × n matrix of (18) by xxA(x0) and substitute (18) into (16) we get:

(19)

If we bring the constant m-vector A(x0) of (19) to the left hand side of the equation and define
D_y=_y - A(x0) we finally obtain our linearized model of observation equations:

(20)

This is the linearized A-model. Compare (20) with (16) and with (1) of subsection 7.1.1. Note
when comparing (20) with (1) that in the linearized A-model D_y takes the place of _y, xxA(x0)
takes the place of A and Dx takes the place of x. Since the linearized A-model is linear in
Dx=x-x0, our standard formulae for least-squares estimation can be applied again. This gives
for the least-squares estimator x^ x 0 D x^
(21)
                                                                    Nonlinear models 147

And application of the propagation law of variances to (21) gives:
(22)

It will be clear that the above results, (21) and (22), are approximate in the sense that the
second-order remainder has been neglected in (20). But these approximations are good enough
if the second-order remainder can be neglected to a sufficient degree. In this case also the
optimality conditions of least-squares (e.g. unbiasedness, minimal variance) hold to a
sufficient degree. A summary of the linearized least-squares estimators is given in table 7.1:

The non linear A-model

The linearized A-model

Least-squares estimators

Variances

Table 7.1: Linearized least-squares estimation
148 Adjustment theory

Example 11
Consider the nonlinear A-model (4) of example 4:

(23)

The corresponding linearized A-model reads:

(24)

with
Up to this point nothing has been said about how one can obtain the approximate values of
the parameters, which are needed to perform the linearization. In the majority of geodetic
applications one can compute these approximate values of the parameters directly from the
observations. In the above problem for instance, the approximate coordinates of point 4, x40
and y40, can be computed from the two observed distances l14, l24 and the known coordinates
of the two points 1 and 2, x1, y1, x2 and y2.

              Figure 7.6: Computation of the approximate coordinates x40 and y40
                                                                                                       Nonlinear models 149

The approximate angle ao (see figure 7.6) satisfies the cosine-rule:
Hence:
(25)

With this result the approximate coordinates of point 4 can be computed as:
(26)

These approximate values x40, y40 can now be used to linearize (23) to obtain (24). Of course,
there is still an ambiguity in the above computation. It is namely not clear whether the angle
a0 should be counted clockwise or counter clockwise. In expression (26) it has been assumed
that a0 should be counted counter clockwise. In practical applications this ambiguity usually
does not occur, since one usually knows whether point 4 lies left or right from the line
connecting the two points 1 and 2.
Example 12
Consider the nonlinear A-model (6) of example 6:

(27)

The corresponding linearized A-model reads:
150 Adjustment theory

(28)

The approximate values x10, y10, a0 and a0 of the four parameters can be computed from the
observed coordinates as follows:

(29)                   .

7.1.4 Least-squares iteration

Up to this point it was assumed that x0 was a good enough approximation such that the
second-order remainder could be neglected. If this is not the case however, then _x^ as compu-
ted by (21) is not the least-squares estimator and hence an unacceptable error is made. In
order to repair this situation, we need to improve upon the approximation x0. It seems reaso-
nable to expect that the estimate:

is a better approximation than x0. That is, it seems reasonable to expect that x1 is closer to the
true least-squares estimate than x0. In fact one can show that this is indeed the case for most
practical applications. But if x1 is a better approximation than x0, a further improvement can
be expected if we replace x0 by x1 in the linearization of the nonlinear A-model. The recom-
puted linearized least-squares estimate reads then:

Now x2 can be expected to be a better approximation than x1. But this implies that a further
improvement can be expected if we replace x1 by x2 in the linearization of the nonlinear A-
                                                                                                       Nonlinear models 151

model. The recomputed linearized least-squares estimate, based on the approximation x2 reads
then:

                                                                                            .

By repeating this process a number of times, one can expect that finally the solution
converges to the actual least-squares estimate x^. This process is called the least-squares
iteration process. The iteration is usually terminated if the difference between successive
solutions is negligible. A flow diagram of the least-squares iteration process is shown in the
table on the following page.
152 Adjustment theory

                                  Table 7.2: Least-squares iteration
                                       Nonlinear models 153

Example 13
Consider the nonlinear A-model:

(30)

It consists of two observation equations in one unknown parameter. Hence, A(.):  Æ 2.
The corresponding linearized A-model reads:

(31)

Let us assume that we are asked to solve model (30). The two observations are given as:
(32)

In order to find the least-squares estimate x^ of x, we need the linearized model (31), a first
approximation x0, and the iteration scheme:

(33)                                   for i=0,1,2,... .
In our case, expression (33) becomes:

(34)

As a first approximation x0 we take y2. Thus:
(35)

With (32), (35) and (34) we are now able to compute xi+1 for i = 0,1,2, ... . The results of this
least squares iteration process are given in the following table:
154 Adjustment theory

                       iteration step i  solution xi
                               0             1.9
                               1
                               2         2.0205959
                               3         2.0176464
                               4         2.0176324
                               5         2.0176344
                                         2.0176344

Note that the solution does not change much after iteration step i = 2. Hence, depending on
the required numerical precision, any of these solutions can be taken as the least-squares
estimate x^. For instance, if the solution is required to have 5 significant digits, then the least-
squares solution can be taken as x^= 2.01763.

7.2 The nonlinear B-model and its linearization

Just like in case of the A-model, there are very few geodetic applications for which the model
of condition equations is linear. In most cases the model of condition equations is nonlinear.
The nonlinear B-model reads:

(36)                                             ,

where B*(.) is a nonlinear vector function from m into m-n. The relation between the
nonlinear B-model and the nonlinear A-model is given by:

(37)                                             .

This is the nonlinear generalization of (7) of section 3.1. If we take the partial derivative with
respect to x of (37) and apply the chain rule (kettingregel), we get:

(38)

This is the linearized version of (37). Compare (38) with (7) of section 3.1. With (38) we are
now in the position to construct the linearized B-model from the linearized A-model (20). Pre-
multiplication of (20) with the matrix (xyB(y o)) gives together with (38) the result:
                                                                                                       Nonlinear models 155

(39)
This is the linearized B-model. With (39) we are now in the position again to apply our
standard least-squares estimation formulae.
As with the nonlinear A-model, also the solution of the nonlinear B-model needs an iteration
process. But since the iteration process needed for solving the nonlinear B-model is quite
involved, it will not be discussed here.

Example 14
Consider the nonlinear A-model (5) of example 5:

(40)

This model consists of 2n observation equations in (n+2) unknowns. Hence, the redundancy
equals 2n-(n+2) = n-2. Therefore, (n-2) independent condition equations can be formulated.
These (n-2) independent nonlinear condition equations are:

or:
(41)
In order to obtain the linearized B-model, we need to linearize (41). Linearization of (41)
gives:
156 Adjustment theory

(42)
with yi20 y20 yi0 , yi10 y10 yi0 , i 3, ,n . This is the linearized B-model. The linearized A-model
follows from linearizing (40) as:
(43)
Verify yourself that xy B(y 0) xxA(x 0) 0 indeed holds, if yi0 a 0(xi0)2 b 0
                                                    Nonlinear models 157

Example 15
Consider the nonlinear A-model (27) of example 12:

(44)

This model consists of 6 observation equations in the four unknown parameters x1, y1, a, and a. Hence,
the redundancy equals 6-4 = 2. Therefore two independent condition equations can be formulated.
These two nonlinear condition equations read:
(45)
Note that these condition equations state that the sides of the equilateral triangle should be equal.
Linearization of (45) gives:

(46)

Verify yourself that xyB(y 0) xxA(x 0) 0 indeed holds (see (28) of subsection 7.1.3), if the
approximate values used in (46) satisfy the nonlinear condition equations.
                                            Appendix A 159

Appendix A

Taylor's theorem with remainder

In this appendix we review the basic theorem of Taylor for functions of one or more variables.
We will start with functions of one variable.

Definition: Let f: DÃ Æ be a function whose domain D is an open set of . If all of the qth-
order derivatives of f(x) exist at every xOED and each q d q f(x)is a continuous function on D, then
f(x) is said to be a function of class C(q) dx on D.

Taylor's Theorem                            , x, x0 OED, and Dx = x-x0. Then a scalar q between
Let f(x) be a function of class C(q) on DÃ
x and x0 exists such that:

(1)

with the remainder:
(2)

If we ignore the remainder Rq(q,Dx), the right-hand side of (1) is a polynomial in Dx of degree
q-1. If the remainder is small this polynomial furnishes an approximation to f(x). We speak of
a linear approximation or a linearization of f(x) at x0 if f(x) is approximated as:

(3)

This approximation is acceptable if the second term within the square brackets of:

is negligible with respect to the first term. Note that this second term can be made arbitrarily
small by letting x0 approach x. A geometric interpretation of (3) is given in figure A.1.
160 Adjustment theory

        Figure A.1: The curve y=f(x) and its tangentline y f(x 0) d f(x 0)(x x 0) at f(x0)
                                                                                  dx

Examples
1. The linearization of f(x)=cos x at x0 reads:
2. The linearization of f(x)=(x2+a2)1/2 at xo reads:
3. The linearization of f(x)=x3 at x0 reads:

We will now consider functions of more than one variable. A function f(x1, x2,...,xn) of the
variables x1, x2,....,xn will be denoted as f(x) with the n-vector x=(x1, x2,...,xn)*. The first-order
partial derivatives of f(x), xxx f(x) , a a 1,...,n , will be denoted as xa f(x) , x2 a 1, ,n. Similarly,
2 we denote the second order partial derivatives of f (x), xxaxxb f(x), a, b 1,...,n, as
xab f(x), a ,b 1, ,n. And so on.
Definition: Let f(x): DÃ nÆ be a function whose domain D is an open set of n.
If all of the qth-order partial derivatives of f(x) exist at every x (x1, x2, ,xn) OED and each

             is a continuous function on D, then f(x) is said to be a function of class C(q) on D.
Taylor's Theorem: Let f(x) be a function of class C(q) on DÃ n and x, x0OED such that the line
segment joining x and x0 is contained in D. In particular, if D is convex then x and x0 can be any
pair of points in D. Let D x x x 0 and q x 0 t(x x 0) with tOE . . Then a scalar tOE (0,1) exists
such that:
                                                                                                                    Appendix A 161

(4)

with the remainder:

(5)
Proof: Define the functions q (t) and f (t) by:

The domain of f (t) is t x 0 t (x x 0)OED which is an open set of        containing the closed
interval [0,1]. By repeated application of the chain rule we find that:

(6)

By Taylor's theorem for functions of one variable there exists a tOE(0,1) such that:

(7)

But f (1) f (x), f (0) f (x 0) , and we have, by substitution of (6) into (7), Taylor's formula with
remainder.
If we use the vector and matrix notation:
162 Adjustment theory

(8)

Taylor's formula with third-order remainder becomes:
(9)
The vector xx f(x 0) is known as the gradient of f (x) at x 0 . The matrix xxx2 f (x 0) is known as
the Hessian matrix of f (x) at x 0 .
Examples:
1. The linearization of f (x , y) x cos y at x 0, y 0 reads:

2. The quadratic approximation of f (x , y) x cos y at x 0, y 0 reads:

3. The quadratic approximation of f (x,y)=(x2+y2)1/2 at x0y0 reads:
                                                                                                                    Appendix A 163

It is possible to give an explicit estimate of the remainder in Taylor's formula, in terms of bounds
for the qth-order partial derivatives of f(x).
Theorem: Suppose that K is a convex subset of D, such that x,x0OEK. Then also
q x 0 t (x x 0)OEK for t OE [0,1]. Moreover, suppose that all qth-order partial derivatives of f(x)
satisfy:
(10)
Then the remainder in Taylor's formula satisfies:

(11)

Proof: From (5) and (10) it follows that:

or that:
(12)

For the inner product of two vectors x and y we have:

From this expression the Cauchy-Schwarz inequality follows:

This gives for y=(1,1,...,1)*:
(13)

If we use this inequality in (12) we obtain the estimate of (11).
164 Adjustment theory

Appendix B

Unconstrained optimization: optimality conditions

In this appendix we will formulate necessary and sufficient conditions for finding (local or
global) solutions to the minimization problem:
(1)

In the investigation of the minimization problem (1) we distinguish two kinds of solution points:
local minimizers and global minimizers.

Definition: The vector x^ is said to be a local minimum of F(x) if F(x^) £ F(x) " x OE B(x^ , e).
The e -ball, B(x^,e) , is defined as B(x^,e) xOE x x^ < e . The local minimimum is said to be
isolated if F(x^) < F(x) " x OE B(x^,e).
Definition: The vector x^ is said to be a global minimum of F(x) if F(x^) £ F(x) " x OE n. The
global minimum is unique if F(x^) < F(x) " x OE n .

In the following we will assume that a (local or global) minimum of F(x) exists. The problem
of computing the minimum of F(x) can be facilitated by deriving certain properties that must be
satisfied by the minimizing vector x^. The following theorem states necessary conditions for x^ to
be a minimum of F(x) .

Theorem (necessary conditions):
Let F(x) be a class C(3). If x^ is a (local or global) minimum of F(x), then:

(2)

Proof:
First we shall prove (2.1). Consider the vector x x^ td where t is a scalar and d an n-vector.
Expansion of F(x) in a Taylor series at x^ gives:
(3)

The order term O(t) indicates the remainder in Taylor's formula. It has the property:
(4)

Since x^ is a (local or global) minimum of F(x) by assumption, it follows that for sufficiently
small t:
(5)
                                                    Appendix B 165

This, together with (3) gives after dividing by t:

Taking the limit as tÆ0 and using (4) shows that (2.1) must be true.
Next we shall prove (2.2). Expanding F(x) in a Taylor series at x^, but now retaining the quadratic
terms, gives with (2.1):
(6)

where O(t2) has the property:
(7)

Equations (5) and (6) imply that:

Taking the limit as tÆ0 and using (7) shows that:
(8)

Since the n-vector d is arbitrary, (8) must hold for all dOE n . This means by definition that the
Hessian matrix xxx2F(x^) is positive semi-definite. Positive semi-definite matrices A are denoted as
"A u 0". This proves (2.2).

Examples

1. The function  has a minimum at (x^,y^)=(0,0). According to the theorem,

the gradient of F (x,y) must be identical to the zero-vector at this point, and the Hessian

matrix of F (x,y) must be positive semi-definite at this point. This is easily verified, since:
166 Adjustment theory

2. Let the n×n matrix Q be positive definite. A matrix Q is said to be positive definite if:
     (9)
     Since Q is positive definite it follows that the function F: nÆ :
     (10)
     has a minimum at x^=0. According to the theorem it must hold that xxF(x^)=0 and
      xxx2 F(x^) u 0 . This is easily verified, since it follows from (10) that:

The above theorem gives necessary conditions for x^ to be a minimum of F(x). The stated
conditions are however not sufficient. This is easily illustrated by the following simple example.
Consider the function F1(x)=x4. Clearly it has a (global) minimum x^=0 and xxF1(0)=0 and
xxx2F1(0)=0 hold. Consider now the function F2(x)=-x4. Then once again xxF2(0)=0 and xxx2F2(0)=0
hold. But now, however, x^=0 is a (global) maximum of F2(x). The following theorem gives
sufficient conditions for x^ to be a minimum of f(x).
Theorem (sufficient conditions):
Let F(x) be of class C(3). If:

(11)

then x^ is a (local or global) minimum of F(x).
Proof:
A Taylor expansion of F(x) at x^ gives with x=x^+td and (11.1):

(12)
Since xxx2F(x^) is positive definite by assumption, the first term in the bracket on the right hand
side of (12) is strictly positive. Hence, in view of (7), we can conclude that the bracketed term
                                                                                                                    Appendix B 167

of (12) is strictly positive for sufficiently small t. Thus from (12) follows that F(x)>F(x^) for all
x near x^, that is, for t small.

It should be noted that x^ can be a minimum of F(x) and still violate the sufficient conditions of
the above theorem (for example F(x)=x4). Nevertheless the above two theorems facilitate the
problem of computing the minimum of F(x). The idea is that one first solves the system of
equations xxF(x)=0. The solutions of xxF(x)=0 are called stationary or critical points of F(x).
Once the stationary points of F(x) are found, one can check whether xxx2F(x^) > 0 holds in order
to show that the corresponding stationary point must be a local minimum.

If xxx2F(x^) u 0, one first computes the vectors d that satisfy xxx2F(x^)d=0 and then checks whether
F(x^) £ F(x) for all those x that lie on the ray x=x^+td that emanates from x^ with direction vector
d. After the local minima are found, the global minima follow from comparison of the function
values at the local minima.

Example (least-squares)

The function F: nÆ is given as:

(13)

From this it follows that:
(14)
and:

(15)

If we assume that the m×n matrix A has full rank n, then A*A has full rank and (A*A)-1 exists.
The solution of xxF(x)=0 follows from (14) as:
(16)

Thus the function F(x) has only one stationary point. Since x A Ax > 0 "x OE n\ 0 it
follows that the Hessian matrix of (15) is positive-definite. Hence, x^ of (16) is the unique global
minimum of F(x).
168 Adjustment theory

Appendix C

Linearly constrained optimization: optimality conditions

In the following appendix we consider general minimization problems. However, there are
special classes of constrained minima which are important and can be studied with the help of the
theory of appendix B. These are the linearly constrained minima. In problems of this type we
restrict our points yOE m to lie in an n-plane in m. An n-plane can be considered to be an
n-dimensional Euclidean space embedded in m. It need not pass through the origin of m. If
y0 is a point in the n-plane and a1, a2,...,an are n linearly independent vectors in this plane,
every point y in the n-plane is expressible in the form:
(1)

with suitably chosen constants xiOE , i 1, ,n. Equation (1) is sketched in figure C.1 for the case
m=3 and n=2.

                                  Figure C.1: A 2-plane embedded in 3

An n-plane is uniquely determined by a point y0 and a set of n linearly independent vectors
a1,a2,...,an. If n=1, we have a line; if n=2, we have a two-dimensional plane; and so on. To relate
the ideas here presented to those given for more general constrained minima on arbitrary
surfaces, it will be convenient to introduce the concept of the tangent space and the normal space
to an n-plane. A tangent vector a is a vector parallel to the n-plane. If the n-plane is represented
in the parametric form (1), a tangent vector a is any vector expressible as a linear combination:
(2)

of the vectors a1,...,an. The tangent space of the n-plane is the set of all vectors that can be
written as (2). It is the n-plane through the origin that is parallel to the set {a1,...,an}. Any vector
b that is orthogonal to the n-plane and hence to the tangent space will be called a normal vector
to the n-plane. The set of all normal vectors to the n-plane is the orthogonal complement of the
tangent space and will be called normal space. It is of dimension m-n. If b1,b2,...,bm-n form a basis
for the normal space, every normal vector b is uniquely expressible as a linear combination:
                                                                                                                    Appendix C 169

(3)
We may use the normal vectors b1,...,bm-n to express the n-plane as:
(4)
This representation of the n-plane is in contrast to the parametric form (1), known as the implicit
form. Equation (4) is sketched in figure C.2 for the case m=3 and n=2.

                     Figure C.2: A 2-plane embedded in 3 with normal vector b
If we collect the vectors a1,...,an into an m×n matrix A=(a1,a2,...,an) and define the n-vector x as
x=(x1,...,xn)*, we may write (1) in matrix form as:
(5)
Similarly, if we collect the vectors b1,...,bm-n into an m×(m-n) matrix B=(b1,...,bm-n), we may write
(4) in matrix form as:
(6)
Observe that the two matrices A and B satisfy the relation:
(7)

We now turn to the characterization of the minimum y^ of a function F(y) on an n-plane. The
following theorem gives necessary conditions for y^ to be a solution to the linearly constrained
minimization problem:
170 Adjustment theory

(8)

Theorem (necessary conditions):
Let F(y) be of class C(3). If y^ is a point on the n-plane that affords a (local or global) minimum
of F(y) on the n-plane, then:

(9)

where A is an m×n matrix of which the column vectors form a basis of the tangent space of the
n-plane.
Proof:
The vector x^=0 minimizes:

Hence, it follows from the first theorem of appendix B that:

and:

The following theorem gives sufficient conditions for y^ to be a solution to (8).
Theorem (sufficient conditions):
Let F(y) be of class C(3). If:

(10)

where A is an m×n matrix of which the column vectors form a basis of the tangent space of the
n-plane, then y^ is a (local or global) minimum of F(y) on this n-plane.
Proof:
The proof follows with f(x)=F(y^+Ax), xxf(x)*=xyF(y^+Ax)*A and xxx2 f(x) A xyy2 F(y^ Ax) A from the
second theorem of appendix B.
                                                                                                                    Appendix C 171

The above two theorems can be used if the n-plane is given in the parametric form (5). In case
the n-plane is defined by the implicit form (6), the results of the above two theorems can be
restated as in the following theorem.
Theorem (Lagrange multiplier rule):
Let y^ be a point satisfying the linear constraints:
(11)

If y^ minimizes F(y) subject to these constraints, there exists a multiplier vector lOE m n , such
that, if we set:
(12)
then:

(13)

for all vectors a that satisfy:
(14)

Conversely, if there exists for y^ a function L(y) of form (12) such that:

(15)

for solutions a{0 of (14), then y^ affords a (local or global) minimum to F(y) subject to the
constraints (11).
The result here given is known as the Lagrange multiplier rule. The function L(y) of (12) is
known as the Lagrangian and the vector l as the Lagrange multiplier vector.
Proof:
We first prove (13.1). Condition (9.1) states that the gradient xyF(y^) is orthogonal to the range
space of A. This implies that xyF(y^) can be written as a linear combination of the base vectors
of the normal space of the n-plane. Hence, there exists a vector lOE m n such that:
172 Adjustment theory

But this implies that the gradient of L(y) must vanish at y^. Condition (13.2) follows simply from
(9.2) by noting that xyy2 L(y^) xyy2 F(y^).
The sufficient conditions (15) follow in a similar way from (10).

Example 1 (observation equations)
Let y0 be a fixed given vector in m, and let F: mÆ be defined as:
(16)
Note that the function F(y) measures the square of the distance between y and the fixed vector
y0. Hence, the (m-1)-dimensional surface F(y)=constant is an hypersphere embedded in m (see
figure C.3).

                                     Figure C.3: F(y)= yo-y 2= constant
Our objective is to minimize the function F(y) of (16) subject to the linear restrictions:
(17)

Those vectors that satisfy (17) lie on an n-plane through the origin with tangent vectors given
by the column vectors of the matrix A (see figure C.4). The problem of minimizing F(y) subject
to (17) can now be visualized as the problem of finding the proper radius of the hypersphere of
figure C.3 such that the sphere just touches the n-plane of figure C.4. This is shown in figure
C.5. The point of contact between sphere and n-plane is the solution y^.
                                                                                                                    Appendix C 173

                                Figure C.4: The n-plane: y=Ax, rank A=n

                    Figure C.5: Hypersphere y0-y 2= constant, and n-plane y=Ax
The solution y^ can be found using (10). Since xyF(y)=-2(y0-y), it follows that:

                                                                        .
Substitution of (17) and setting the results equal to zero gives:
This system of equations has the unique solution x^=(A*A)-1A*y0. Hence:
(18)
Since xyy2 F(y) 2 Im and A*A is positive definite it follows that:
Hence, y^ is a minimum of F(y) subject to (17).
Example 2 (condition equations)
Let y0 be a fixed given vector in m, and let F: mÆ be defined as:
(19)
174 Adjustment theory

Our objective is to minimize the function F(y) of (19) subject to the linear restrictions.
(20)

Since these restrictions are in implicit form, we have to make use of the Lagrange multiplier rule.
We construct the Lagrangian:
(21)

The factor 2 in the multiplier vector has merely been introduced for convenience. The stationary
points of the Lagrangian have to satisfy:
(22)

This shows that:
(23)

In order to determine y^, we first have to determine l. Pre-multiplication of (22) with B* gives
together with (20):
(24)

Since matrix B has full rank, the inverse of B*B exists. Hence l follows from (24) as:
(25)

Substitution of this result into (23) gives:
(26)

Thus the Lagrangian (21) has only one stationary point. Since xy2yL(y) = 2 Im, it follows that
condition (15.2) is satisfied. Hence, y^ of (26) is the solution of our linearly constrained
minimization problem.

Example 3 (Best Linear Unbiased Estimation)

In Chapter 2, section 4 the principle of "Best Linear Unbiased Estimation" has been discussed.
This principle leads to the following linearly constrained minimization problem:

(27)  subject to

This problem may be solved by the Lagrange multiplier rule. We consider the following
Lagrangian:
(28)

The stationary points of the Lagrangian have to satisfy:
(29)
                               Appendix C 175

This shows that:
(30)

In order to determine ^l we first have to determine l. Pre-multiplication of (29) with A Qy 1
gives together with the constraints A*l=f :

(31)

Since A is of full rank and Qy is positive definite, also A Qy 1A is of full rank and positive
definite. Hence, the inverse of A Qy 1A exists and l follows from (31) as:

(32)

Substitution into (30) gives:
(33)

Since xll2 L(l )  2Qy and Qy is positive definite, it follows that ^l of (33) is the unique solution
of (27).
176 Adjustment theory

Appendix D

Constrained optimization: optimality conditions
In the previous appendix we were concerned with the problem of minimizing a function F(y)
with linear equality constraints. In this appendix we seek to minimize a function F(y) with
nonlinear equality constraints. Hence, we want to solve the constrained minimization problem:
(1)

The constraints in (1) are in implicit form and they are nonlinear if the vector function B: mÆ
  m-n is nonlinear. Except in abnormal situations, the constraints of (1) describe an n-dimensional

surface (or manifold) embedded in m. Examples of B(y) = 0 are:

The first equation describes the surface of a sphere with radius R embedded in 3. The last two
equations together describe a circle embedded in 3.
By virtue of the implicit function theorem the constraints:
(2)
are expressible locally in parametric form as:
(3)
For instance, example 1 is expressible in parametric form as:

         1.

And the equations of example 2 as:

         2.
                                                                                                                    Appendix D 177

Since (3) puts the same restrictions on y as (2) we may write the minimization problem (1) also
as:

(4)

Consequently, the problem of minimizing F(y) subject to B(y) = 0 is equivalent to the
unconstrained problem of minimizing F(A(x)). In some cases when it is simple to transform (2)
into (3) this is a suitable procedure. However, in many cases it is easier to use the Lagrange
multiplier rule. The following two theorems, which will be given without proof, give necessary
and sufficient conditions for y^ to be a (local or global) solution to (1).
Theorem (necessary conditions):
Suppose that y^ affords a (local or global) minimum to:

subject to the constraints:
                                                                          .

Suppose further that the m×(m-n) matrix of partial derivatives xyB(y^) has full rank m-n. Then
there exists a unique multiplier vector lOE m n such that, if we set:
then:

(5)

for all vectors a that satisfy:

Theorem (sufficient conditions):
Suppose that for a point y^ satisfying B(y) = 0 there is a Lagrangian L(y) F(y) l B(y) such that:

(6)

for all vectors a { 0 satisfying xyB(y^)*a = 0, then y^ is a (local or global) minimum of F(y)
subject to B(y) = 0.
178 Adjustment theory

Appendix E

Mean and variance of scalar random variables

Mean of a scalar random variable

Let x be a continuous scalar random variable with probability density function px(x). The
expectation or mean of x is by definition the integral:

(1)  .

This number will also be denoted by mx.

Mean of a function of a scalar random variable
Given a scalar random variable x and a function f(x), we form the random variable y =f(x).
As we see from (1), the mean of y is given by:
(2)

It appears, therefore, that to determine the mean of y, we must first find its probability density
function py (y). This, however, is not necessary. As the next theorem shows, E{y} can be
expressed directly in terms of the function f(x) and the density px(x) of x.

Theorem:

(3)     .

Proof:
We shall sketch a proof using the curve f(x) of figure E.1. With y = f(x1) = f(x2) = f(x3) as in
the figure, we see that:

or:
                                                                                     .

Multiplying by y, we obtain:
                                                                                                               Appendix E 179

                                                                                                .
Thus, to each differential in (2) there corresponds one or more differentials in (3). As dy
covers the y-axis, the corresponding dx's are nonoverlapping and they cover the entire x-axis.
Hence, the integrals in (2) and (3) are equal.

                                      Figure E.1: The curve y = f(x)
It appears from (3) that to determine the mean of y, we must know the probability density
function px(x). In general this is true. However, in the special case that the function f(x) is
linear not the complete density of x needs to be known but only the mean mx of x.
Theorem (propagation law of the mean):
Given a scalar random variable x and a function f(x), we form the random variable y = f(x).
If the function f(x) is linear:
(4)
then:
(5)
Proof:
Substitution of (4) into (3) gives:

If the function f(x) is nonlinear we need strictly speaking the density px(x) of x in order to
compute the mean of y = f(x). However, by using Taylor's formula an approximation to the
180 Adjustment theory

mean of y can be derived that gets around the difficulty of having to know the density px(x)
of x.

Theorem (linearized propagation law of the mean):
Given a scalar random variable x and a nonlinear function f(x), we form the random variable
y = f(x). Let x0 be an approximation to a sample of x and define Dy = y-f(x0) and Dx = x-x0.
Then a first-order approximation to the mean of Dy is:

(6)                                     .

Proof:
Application of Taylor's formula gives:

If we take the expectation we get:

This may be written with:
and:

as:
If we neglect the second and higher order terms, the result (6) follows.
                                                                         Appendix E 181

Examples                                                                     , we
                                                                         we find
1. With f(x) = cos x, we define the random variable y = f(x). Since
     find to a first order:

2. With f(x) = x3, we define the random variable y = f(x). Since
     to a first order:

Variance of a scalar random variable
The variance of a scalar random variable is by definition the integral:

(7)  .

This number will also be denoted by sx2. The positive constant sx is called the standard
deviation of x. From the definition it follows that:

or
(8)

Variance of a function of a scalar random variable

Theorem (propagation law of the variance):
Given a scalar random variable x and a function f(x), we form the random variable y = f(x).
If the function f(x) is linear:
(9)

then:

(10)
182 Adjustment theory

Proof:
According to definition (7) we have:

With (3) this gives:
                                                                          .

Substitution of (5) and (9) gives:

The above result shows that if f(x) is linear, knowledge of sx2 is sufficient for computing the
variance of y = f(x). For nonlinear functions f(x) this is generally not true. If the function f(x)
is nonlinear one will generally need to know the complete density px(x) of x. However, by
using Taylor's formula an approximation to the variance of y can be derived that gets around
the difficulty of having to know px(x) .
Theorem (linearized propagation law of the variance):
Given a scalar random variable x and a nonlinear function f(x), we form the random variable
y = f(x). Let x0 be an approximation to a sample of x. Then a first-order approximation to the
variance of y is:

(11)

Proof:
Substitution of:

into:
                                                                 Appendix E 183

gives after neglecting second and higher order terms:

Examples
1. A first order approximation to the variance of y = cos x is:

2. A first order approximation to the variance of y = x3 is:
184 Adjustment theory

Appendix F

Mean and variance of vector random variables
Mean of a vector random variable
Let xi, i = 1, 2, ,n be n continuous scalar random variables with joint probability density function
px (x1, x2, ,xn). The expectation or mean of the random n-vector (x1, x2, ,xn)* is by definition the
integral:

(1)

If we use the notation:

we may write (1) in the compact form:

(2)                                    .

From (1) it follows that:

(3)

This result seems to contradict definition (1) of appendix E. Note however that (3) reduces to
(1) of appendix E, since:

Hence, (1) may also be written as:
                                                                                             Appendix F 185

(4)

Thus in order to compute E{xi} one only needs the marginal density px (xi) .
                                                                                                                                                                                                                                                         i

Mean of a vector function of a random vector
Given a random n-vector x and a vector function F(x), F: n Æ m, we form the random m-
vector y F(x) . As we see from (4) the mean of yi is given by:
(5)

It appears, therefore, that to determine the mean of yi, we must first find its marginal probability

density  function  pyi  (yi).  This,  however,  is  not  necessary.  As  the  next  theorem  shows,  Ey        can

                                                                                                            i

be expressed directly in terms of the vector function F(x) and the joint density of x.

Theorem:

(6)

or:
(7)

Proof:
The proof is similar to the proof given in appendix E.

The following theorem is an extremely important one, and it is used frequently.

Theorem (propagation law of the mean):
Given a random n-vector x and a vector function F(x), F: n Æ m, we form the random m-
vector y = F(x). If the vector function F(x) is linear:

(8)
186 Adjustment theory

then:

(9)                    .

Proof:
If we denote the n column vectors of matrix A by ai, i=1, .., n, we may write (8) as:

                                                                       .

If we substitute this into (6) we get:

or:

Without proof we also give the linearized version of the propagation law of the mean.
Theorem (linearized propagation law of the mean):
Given a random n-vector x and a nonlinear vector function F(x), F: n Æ m we form the random
m-vector y = F(x). Let x0 OE n be an approximation to a sample of x and define Dy = y - F(x0)
and Dx = x - x0. Then we have to a first-order:
(10)

Example 1
Let the two random variables y1 and y2 be defined as:

Then:
      Appendix F 187

Example 2
Let the two random variables y1 and y2 be defined as:

With the approximate values x10 x20 and x30 we have to a first-order:
                                                                                                             .

Variance matrix of a random vector

Let _x, i=1,2, ,n be n continuous scalar random variables with joint probability density function
px (x1, ,xn). The variance matrix of the random n-vector (x1,x2, ,xn)* is by definition the integral:

(11)                                                                                                            .

This variance matrix will be denoted by Qx. Using vector notation we may write (11) in the
compact form:

(12)

From (11) it follows that:
188 Adjustment theory

Integration gives:

(13)

in which px(xi,xj) is the joint density function of the two random variables xi and xj. The scalar
(13) is called the covariance of the two random variables xi and xj. This covariance will be
denoted as sx x . Note that the off-diagonal elements of the variance matrix Qx consist of the

                                                 ij

covariances between the elements of the random vector x. If i=j it follows from (13) that:

                                                                                         .

Integration gives:

This is the variance of xi. Note that the variance of is the ith-diagonal element of the
variance matrix Qx. Hence, the variance matrix Qx can be written as:

(14)                                                      .

Note that the variance matrix is a symmetric matrix.

Variance matrix of a vector function of a random vector

Theorem (propagation law of variances):                   n Æ m, we form the random m-vector
Given a random n-vector x and a vector function F(x), F:
y = F(x). If the vector function F(x) is linear:

(15)

then:
                                                                                                                    Appendix F 189

(16)

Proof:
If we denote the n column vectors of matrix A by ai, i=1,...,n, we may write (15) as:

(17)                                   .

In a similar way we may write (9) as:

(18)                                   .

Substitution of (17) and (18) into:

gives:

This can be written as:

or as:

Without proof we also give the linearized version of the propagation law of variances.
190 Adjustment theory

Theorem (linearized propagation law of variances):
Given a random n-vector x and a nonlinear vector function F(x), F: n Æ m, we form the random
m-vector y = F(x). Let x0 OE n be an approximation to a sample of x. Then we have to a first-
order:

(19)                                                   .

Example 1
Let the two random variables y1 and y2 be defined as:

The variance matrix of x = (x1, x2, x3)* is given as:

Then:

Example 2
Let the two random variables y1 and y2 be defined as:
(20)

The variance matrix of x = (x1, x2, x3)* is given as:
                                                                                                                    Appendix F 191

The 2-by-3 matrix of partial derivatives xxF(x0) follows from (20) as:

(22)

If we take as approximate values x10=1, x20=0, x30=0, (22) becomes:
(23)
The variance matrix Qy follows now from (21) and (23) to a first-order as:
192

                                                                        Appendix G
 A BRIEF ACCOUNT ON THE EARLY HISTORY OF ADJUSTING GEODETIC

                   AND ASTRONOMICAL OBSERVATIONS1

                    Prof. dr ir P.J.G. Teunissen, Netherlands Geodetic Commission (KNAW)

I.  Introducon

Adjustment theory can be regarded as the part of mathemacal geodesy2 that deals with the opmal
combinaon of redundant observaons for the purpose of determining values for parameters of
interest. It is essenal for a geodesist, its meaning comparable to what mechanics means to a civil
engineer or a mechanical engineer. The two main reasons for performing redundant measurements
are the wish to increase the accuracy of the results computed and the requirement to be able to check
for errors. Due to the intrinsic uncertainty in observaons, redundancy generally leads to an
inconsistent system of equaons. Without addional criteria, such a system is not uniquely solvable.

The problem of solving an inconsistent system of equaons has atracted the atenon of leading
sciensts in the middle of the 18th century. Historically, the first methods of combining redundant
observaons originate from studies in geodesy and astronomy, namely from the problem of
determining the size and shape of the Earth, and the problem of finding a mathemacal representaon
of the moons of the moon. Since its discovery almost 200 years ago, least-squares has been the most
popular method of adjustment. Although the method of least squares may seem `natural' for a modern
student of adjustment theory, its discovery evolved only slowly from earlier methods of combining
redundant observaons3. In this contribuon we sketch the historical line of development of these
adjustment methods in the second half of the 18th century.

II. The method of selected points

It is convenient to cast the problem of combining redundant observaons in terms of vectors and
matrices. Suppose we are given a set of linear equaon of the form

  =  

×1 × ×1

where  is a vector of observaons,  is a given matrix of full rank and  is the vector of unknown
parameters. This set of linear equaons is said to be overdetermined when there are more
observaons than unknowns,  > . The problem is to combine the  observaons so that one can

1 Based on a presentaon given at the occasion of the official opening of the Geodec-Astronomical Observatory Westerbork,
24 September 1999.
2 Mathemacal geodesy covers the development of theory and its implementaon as is needed in order to process, analyse,
integrate and validate the various geodec data. It concerns itself with the calculus of observaons (adjustment and
esmaon theory), with the validaon of mathemacal models (tesng and reliability theory) and with the analysis for spaal
and temporal phenomena (interpolaon and predicon theory), Founders of the Dutch School of Mathemacal Geodesy,
internaonally known as the `Del School', are the professors J.M. Tienstra (1895-1951) and W. Baarda (1917-2005).
3 Sgler, S.M. (1986): The History of Statistics, Belknap, Harvard. Hald, A. (1998): A History of Mathematical Statistics, Wiley-
Interscience.
                         193

solve for the  unknown parameters. If we restrict ourselves to linear combinaons of the
observaons, we can write the general soluon in the following form

  =      with   = ()-1 

×1 × ×1        ×  × ×

and where matrix  is a suitably chosen matrix defining the linear combinaons. Different choices of
 give different linear combinaons and therefore different soluons. In modern terminology, matrix
 is called a le-inverse of , since  mes  equals the identy matrix. Before 1750 a popular, albeit
subjecve, method of solving an overdetermined set of linear equaons was the method of selected
points. It consists of choosing  out of  observaons (referred to as the selected points) and using
their equaons to solve for . If the choice falls on the first  observaons, the corresponding  matrix
takes the form

 =   0 

         × ×(-)

where  is the identy matrix. For the method of selected points,  residuals (the difference between
the observed and adjusted observaons) are by definion equal to zero. Many sciensts using this
method calculated the remaining  -  residuals and studied their sign and size to get an impression
of the goodness of fit between observaons and the proposed law. The method is subjecve because
no clear rule is given which observaons to select and which to throw out. Selecng another set of 
observaons leads to a different soluon for . Although the disadvantage of not using all observaons
was recognized, no simple method existed to tackle this shortcoming. Somemes all possible
combinaons of  observaons were considered and then averaged to obtain the final result. But since
this approach requires handling  over  combinaons, it was only praccal for problems of low
dimensions.

III. The method of averages

Tobias Mayer (1723-1762), professor of mathemacs and head of the Göngen observatory, made
numerous observaons of the Moon with the purpose of determining the characteriscs of the Moon's
orbit. In 17504 Mayer proposed a new method for adjusng his Moon data, a method which solved
the above-menoned piall of the method of selected points. Apart from his adjustment method,
Mayer is also known for his other contribuons to surveying and navigaon. In 1752 he invented the
Repeang or Reflecng Circle, an instrument for observing the angle between two celesal bodies.
The accuracy of Mayer's instrument was comparable to John Hadley's reflecng octant (1731), but had
the advantage that it could be used to measure angles of over 90 degrees5. Mayer also contributed to
solving the mariner's `longitude problem'. It was the Brish Parliament, which in 1714, offered the
`Longitude Prize' to those who could find a `useful and praccal' method for determining longitude at
sea. To determine longitude of a ship at sea, the mariner needs to know both his local me and the
me at some standard locaon. Local me was readily determined, but the determinaon of standard
me at sea was more complicated. Mayer's detailed lunar tables (1755) made it possible to translate
the instrument readings into longitude posions. The use of Mayer's lunar tables was later superseded

4 Mayer, T. (1750): Abhandlung ueber die Umwalzung des Monds um seine Axe und die scheinbare Bewegung der
Mondsflecten, Kosmogr. Nachr. Samml. Auf das Jahr 1748, 1, 52-183.
5 Forbes, E.G. (1974): The Birth of Scientific Navigation, Marime Monographs and Reports, No. 10, Naonal Marime
Museum, Greenwich, London.
194

by John Harrison's marine chronometer H-4 (1759). In recognion of their contribuons, both men
were awarded part of the `Longitude Prize', with the larger sum going to Harrison6.

Mayer studied the liberaon of the Moon by observing the changing posion of the crater Manilius as
seen from the Earth. Using spherical geometry, he found a linearized relaonship between his
observables and some locaon parameters of Manilius and the Moon's pole. This gave him an
inconsistent system of  = 27 linear equaons in 3 unknown parameters

1    1 12 13 1

   =      
  12 3  3

       

Mayer proposed to divide the 27 equaons into 3 groups of 9 each, to sum the equaons within each
group, and to solve the resulng 3 equaons in 3 unknowns. For a general set of  equaons in 
unknowns, this approach amounts to a separaon of the  equaons into  groups, followed by a
groupwise summaon. For example, in case  = 2, the corresponding  matrix takes the form

 = 1 0 0 2

where the 1 and 2 are row vectors having only 1's as their entries. Since one may use averages
instead of sums, the method became later known as the method of averages. Mayer's method of
averages soon became popular. It used all observaons and it was very simple to apply. However, due
to the lack of an objecve criterion on how to group the observaons, the method was sll a subjecve
one.

IV. The method of least absolute deviaons

To determine the Earth's flatening as predicted by Newton's theory of gravitaon (Principia7, 1687),
the French Academy of Sciences organized arc-measurement expedions to Peru, Lapland and the
Cape of Good Hope in the period 1735-1754. These expedions aroused the interest in other countries
and in 1750 Pope Benedict XIV commissioned the Jesuit and professor of mathemacs Roger Joseph
Boscovich (1711-1787) to perform a similar geodec survey near Rome, the results of which were
published in 1755. In a summary of this report, published in 17578, Boscovich formulated his new
method, now known as the method of least absolute deviations, and applied it to the data of the

                                              French and Italian arc measurements.

                                              In order to understand the equaons used by Boscovich, we first
                                              need to introduce some elements from ellipsoidal geodesy. For
                                              short meridian arcs, the arc length  (see figure 1) can be writen
                                              as  = (), with () the meridian radius of curvature, 
                                              the geodec latude of the midpoint of the arc and  the
                                              latude difference of the two arc endpoints.

Figure 1: Latude arc measurements along a meridian.

6 Sobel, D. (1995): Longitude. Walker, New York, Dutch translaon by E. van Altena (1997): Dava Sobel: Lengtegraad, Ambo.
7 Newton, I. (1687): Philosophiae Naturalis Principia Mathematica.
8 Boscovich, R.J., Maire, C. (1770): Voyage astronomique et géographique dans l'etat de l'église. French translaon of original
1755 publicaon. For Boscovich' contribuons, see also Sheynin, O.B. (1973): Arch. History Exact Sci., 9: 306-324.
                                                                       195

The meridian curvature and its expansion are given as

         = (1 - 2) 3 = (1 - 2) 1 + 3 22 +              2
                    (1 - 22)2

with 2 = (2 - 2)/2 the eccentricity,  and  the half lengths of the major and minor axis and
(1 - 2) the length of an arc at the equator. Using only the first two terms in the expansion, the

lengths of an -arc can be writen as

   = 1 + 2 2 with 1 = (1 - 2) and 2 = 322(1 - 2)

This is one equaon in two unknowns, 1 and 2. The arc length  and geodec latude  are
determined from astronomical and geodec measurements, while 1 and 2 contain the unknown
dimensions of the ellipsoid of revoluon. Although a minimum of two arcs is needed to solve for the

unknowns, it is preferable to use more than two -arcs. As a result, one obtains the following system
of linear equaons

1 1 2 1    =     1
  12  2

      

This is the system of equaons which formed the start of Boscovich' analysis. Note that he -matrix
becomes near rank defect, when all arcs are chosen to the same latude. For an accurate
determinaon of the two unknowns, it is therefore preferable to choose arcs at widely different
latudes. From the data available, Boscovich chose five such arcs ( = 5). In his first analyses,
Boscovich used the method of selected points. He chose the two arcs with the largest difference in
latude. Not sasfied with the result obtained (the 3 residuals were considered too large), he considers
all possible pairs of measured arcs. This gave him 10 selected points to solve, but again he is not
sasfied with the results obtained. Aer having struggled for some me on how to proceed, Boscovich
finally formulates his new method of soluon in 1757. He states that the parameters 1 and 2 should
be chosen in such a way that the residuals sum up to zero and have minimum absolute sum. In formula
form these two condions read

                                                        

  ( - 1 - 22) = 0              and                     | - 1 - 22| = 

  =1                                                   =1

The first condion (although not essenal) was movated by the assumed symmetry in the error
distribuon, while the second was chosen to get the adjusted values `as close as possible' to the
observed ones. Boscovich gave the graphical algorithm for solving his problem, but no analycal one.
The analycal proof of the soluon was first given by Laplace in 1793. Using his principle, Boscovich
first determined the two parameters 1 and 2 and from them the flatening as  = 2/31. Here he
only used the first term of the expansion

                     = ( - ) = 1 2 + 1 4 + 1 6 + 
                               28                      16

The value obtained by Boscovich equals  = 1/246, which was smaller than the flatening predicted
by Newton. Based on the rotaonal ellipsoid as an equilibrium figure for a homogeneous, fluid,
rotang Earth, Newton obtained the value  = 1/230. Boscovich value is however larger than the
value known today (Internaonal Associaon of Geodesy (1980):  = 1/298.257). Boscovich' method
was the first adjustment method that started from the principle of minimizing a funcon of the
196

residuals. However, although the method is objecve and uses all the observaons, it did not reach
the same level of popularity as Mayer's method. The method, being nonlinear, was difficult to apply,
while the at that me available algorithm could only handle a system of equaons with a maximum of
two unknowns. In the second half of the 20th century the method gained in popularity due to its
property of being resistant (robust) against outliers. Nowadays Boscovich adjustment method is
usually referred to as an 1adjustment, since the 1-norm of a vector is the sum of absolute values of
its entries.

V. The method of least squares

Adrien-Marie Legendre (1752-1833), a professor of mathemacs at the École Militaire in Paris, was
appointed by the French Academy of Sciences as a member of various commitees on astronomical
and geodec projects, among them the commitee on the standardizaon of weights and measures.
The commitee proposed to define the meter as 10-7 mes the length of the terrestrial meridian
quadrant through Paris at mean sea-level. The arc-measurements took place in the period 1792-1795
and were analysed, among others9, by both Laplace and Legendre. Legendre's 180510 publicaon on
the determinaon of the orbits of comets, contains a nine-page appendix in which for the first me
the method of least-squares11 is described, together with an applicaon of the method to the arc
measurements. Legendre used a different equaon than Boscovich. Boscovich used the equaon

 = (1 - 2) + 3 2(1 - 2)2 
                      2

while Legendre, having the determinaon of the meter in mind, used a parametrizaon which differed
from the one used by Boscovich. With sin2  = 12(1 - cos 2),  = (1 - 2) + 1 3 2 2 2(1 - 2) ,
sin  , and   12(1 - 2)2, the above equaon can be writen as  =  -
32sin cos 2  . As a result, we may write for the two endpoints of an arc,
12 = 12-1 + 32 12(1 + 2).
This is one equaon in two unknowns,  and . Note that   180 is the length of a 1-degree arc at 45
degree latude.

Legendre understood that the observed latude differences would correlate in case the arcs were
connected. He therefore transformed the above equaon of differences into an equivalent
undifferenced form. This can be achieved by introducing an appropriate addional equaon with an
addional unknown. As a result, we obtain Legendre's linear system of equaons as

9 In 1795 the French government invited other European governments to delegate sciensts to Paris for compleng and
checking the computaons for the standardizaon of weights and measures. The Dutch delegates were professor Jan Hendrik
van Swinden and the navy officer Henricus Aeneae. The European sciensts met in Paris in 1798 and reported on their findings
in 1799. The report on the meter was given by van Swinden, the standard meter, a bar of planum with rectangular secon
of 254 millimetres, was placed in the French State Archives, Mètre et Kilogramme des Archives. In 1983 the standard meter
was defined as the length travelled by light in vacuum in 1/299.792.458 seconds.
10 Legendre, A.M. (1805): Nouvelles méthodes pour la determination des orbits des comètes. (Appendix: Sur la méthode des
moindre carrés).
11 When Carl Friedrich Gauss published his first probabilisc version of the method of least-squares in 1809, he claimed that
he had been using the method (`our principle') already since 1795. This claim resulted in a priority dispute between Legendre
and Gauss, for a discussion see e.g. Placket, R.L. (1972): The discovery of the method of least-squares. Biometrica 59: 239-
251.
                                         197

 1  1 1   3
     
          2  1 1+   
   = 1 0                             -1
            0                         

                                      
                          3
                                    
    1 2 +

          

Since Legendre had four connected arcs ( = 5) at his disposal, he had to solve 5 equaons in 3
unknowns. In order to solve this overdetermined linear system of equaons, Legendre proposed to

determine  such that the sum of the squares of the residuals is minimized. In a vector-matrix form
( - )( - ) = .

By seng the derivaves of this quadrac form equal to zero, he shows that the soluon sasfies the
consistent system of linear equaons (nowadays referred to as `the normal equaons')  = .
Note that this result corresponds to the following choice of the  matrix:  = .

Aer solving his set of equaons for the three unknowns, 3,  and , Legendre obtained for the
flatening the value  = 1/148, which he recognizes as being too large. He therefore recomputed his
least-squares adjustment, but now with  constrained to the at that me adopted value for the Earth's
flatening. As a result, he obtained a value for , which was now close to the value obtained earlier by
Laplace and on which the actual definion of the meter was based. The reason for Legendre having to
constrain  lies in the poor resoluon of his data. The total arc length of his data covered only about
10 degrees.

Although Legendre did not give a clear movaon for his `least-squares' criterion, he did realize its
potenal. His method used all the observaons, had an objecve criterion and most importantly,
resulted - as opposed to Boscovich' method - in a solvable linear system of equaons. The method
met with almost immediate success. Within ten years aer Legendre's publicaon, the method of
least-squares became a standard tool in astronomy and geodesy in various European countries. And
within twenty years, also the probabilisc foundaons of the method were largely completed, the
main contributors being Laplace and Gauss.

De Hollandse Cirkel. 2 (1/2), 2000
198

Literature

1. Calculus

         Almering, J.H.J. Revised by H. Bavinck and R.W. Goldbach; Analyse
         VSSD, 6th ed. (1996)

2. Linear Algebra

         Braber, C.A. den, H. van Iperen and M.A. Vlietgever; Matrixrekening
         VSSD, 2nd ed. (1989)

         Lay, D.C.; Linear algebra and its applications
         Reading, Addison-Wesley, 2nd ed. (1997)

         Ortega, J.M.; Matrix theory, a second course
         New York, Plenum Press (1987)

         Strang, G.; Linear algebra and its applications
         San Diego, Harcourt Brace Jovanovich Publishers, 3rd ed. (1988)

3. Probability Theory

         Grimmett, G. and D. Welsh; Probability; an introduction
         Oxford, Clarendon Press (1986)

         Lopuhaä, H.P.; Lecture notes: Statistiek voor geodeten
         Delft University of Technology, Faculty of Information Technology and Systems
         (1994)

         Papoulis, A.; Probability, random variables and stochastic processes
         New York, McGraw-Hill Book Company (1991)

         Soest, J. van; Elementaire statistiek
         VSSD, 7th ed. (1997)

4. Estimation Theory

         Cooper, M.A.R.; Control surveys in civil engineering
         London, Collins Publishers (1987)

         Koch, K.R.; Parameter estimation and hypothesis testing in linear models
         Berlin Springer Verlag, 2nd (1999)

         Mikhail, E.M., Gracie, G.; Analysis and adjustment of survey measurements
         New York, van Nostrand Reinhold Company (1981)

         Tienstra, J.M.; Theory of the adjustment of normally distributed observations
         Amsterdam, Argus, (1956)
Index                                                                                  199

a                                              minimization 29, 52, 164, 168
                                               minimum variance 28, 52
A-model; see model with observation equations  misclosures 62
39                                             mixed model 81
                                               model with condition equations 61
b                                              model with observation equations 39

Best Linear Unbiased Estimation 28, 49, 64,    n
174
block estimation 109, 113                      nonlinear observation equation 137
B-model; see model with condition equations    nonlinear A-model 137
61                                             nonlinear B-model 154
                                               normal equation 90
c
                                               o
Cholesky 24
condition equation 62                          observation (measurement) 2
constituent variates 77                        observation equation 42
constrained optimization 168, 176              optimization 164
covariance 188                                 optimization constrained 168, 176
covariance matrix; see variance matrix 187     orthogonal projection 15
                                               orthogonal projector 8, 55, 95
d
                                               p
degree of dependence 94
derived variates 77                            partitioned model 89
dispersion 89; see also variance matrix        phases, estimation in 123
                                               predicted residual 101
e                                              propagation law 26, 45, 46, 47, 179, 185

eigenvalues 58                                 r
ellipse 14, 18, 34
estimable 94                                   rank defect 43, 95
estimate 26                                    random variable 178
estimation in phases 123                       random variable, vector 184
estimator 26                                   recursive estimation 100
expectation 26, 178                            reduction 91
                                               redundancy 55, 61
f                                              remainder 163

free variates 72, 75                           s
functional model 3
                                               slope 97, 98
h                                              stochastic model 3

Helmert 113                                    t
Helmert's block method 113
                                               Taylor's theorem 142, 159
i                                              Tienstra 123
                                               trace 58
inner product 23, 56
intercept 98                                   u
iteration 150
                                               unbiased 27, 49
l
                                               v
Lagrange 171
Lagrange multiplier rule 171                   variance 181, 188
least-squares 6                                variance matrix 187
least-squares residuals 26
levelling 39, 61                               w
linearized A-model 146
linearization 142                              weight matrix 11, 43
                                               weighted least-squares 11, 44
m
                                               y
maximum likelihood 33
mean 178, 185                                  yR-variates 74
measurement error 5
measurement update 101
Adjustment theory:

an introduction

Peter J.G. Teunissen

Adjustment theory can be regarded as the part of mathematical geodesy that deals
with the optimal combination of redundant measurements together with the estimation
of unknown parameters. It is essential for a geodesist, its meaning comparable to
what mechanics means to a civil engineer or a mechanical engineer. Historically, the
first methods of combining redundant measurements originate from the study of three
problems in geodesy and astronomy, namely to determine the size and shape of the
Earth, to explain the long-term inequality in the motions of Jupiter and Saturn, and to find
a mathematical representation of the motions of the Moon. Nowadays, the methods of
adjustment are used for a much greater variety of geodetic applications, ranging from, for
instance, surveying and navigation to remote sensing and global positioning.

The two main reasons for performing redundant measurements are the wish to increase
the accuracy of the results computed and the requirement to be able to check for errors.
Due to the intrinsic uncertainty in measurements, measurement redundancy generally
leads to an inconsistent system of equations. Without additional criteria, such a system
of equations is not uniquely solvable. In this introductory course on adjustment theory,
methods are developed and presented for solving inconsistent systems of equations. The
leading principle is that of least-squares adjustment together with its statistical properties.

The inconsistent systems of equations can come in many different guises. They could
be given in parametric form, in implicit form, or as a combination of these two forms. In
each case the same principle of least-squares applies. The algorithmic realizations of the
solution will differ however. Depending on the application at hand, one could also wish to
choose between obtaining the solution in one single step or in a step-wise manner. This
leads to the need of formulating the system of equations in partitioned form. Different
partitions exist, measurement partitioning, parameter partitioning, or a partitioning of both
measurements and parameters. The choice of partitioning also affects the algorithmic
realization of the solution. In this introductory text the methodology of adjustment is
emphasized, although various samples are given to illustrate the theory. The methods
discussed form the basis for solving different adjustment problems in geodesy.

P.J.G. Teunissen                                                                                 © 2024 TU Delft Open
                                                                                                 ISBN 978-94-6366-884-2
Delft University of Technology                                                                   DOI https://doi.org/10.59490/tb.95
Faculty of Civil Engineering and Geosciences                                                     textbooks.open.tudelft.nl

Dr (Peter) Teunissen is Professor of Geodesy at Delft                                            Cover image: J.E. Alberda
University of Technology (DUT) and an elected member
of the Royal Netherlands Academy of Arts and Sciences.                                           Series on Mathematical
He is research-active in various fields of Geodesy, with                                         Geodesy and Positioning
current research focused on the development of theory,
models, and algorithms for high-accuracy applications
of satellite navigation and remote sensing systems.
His past DUT positions include Head of the Delft Earth
Observation Institute, Education Director of Geomatics
Engineering and Vice-Dean of Civil Engineering and
Geosciences. His books at TUDelft Open are Adjustment
Theory, Testing Theory, Dynamic Data Processing and
Network Quality Control.l
