1. Filter Structures
     1. Filter Structures
     2. FIR Filter Structures
     3. IIR Filter Structures
     4. State-Variable Representation of Discrete-Time Systems

2. Fixed-Point Numbers
     1. Fixed-Point Number Representation
     2. Fixed-Point Quantization

3. Quantization Error Analysis
     1. Finite-Precision Error Analysis
     2. Input Quantization Noise Analysis
     3. Quantization Error in FIR Filters
     4. Data Quantization in IIR Filters
     5. IIR Coefficient Quantization Analysis

4. Overflow Problems and Solutions
     1. Limit Cycles
     2. Scaling
Filter Structures

A realizable filter must require only a finite number of computations per
output sample. For linear, causal, time-Invariant filters, this restricts one to
rational transfer functions of the form

                       -1              -m
                   b0 + b1z + ... + bmz

H (z) =

                   -1      -2               -n
                   1 + a1z + a2z + ... + anz

Assuming no pole-zero cancellations, H(z) is FIR if i, i > 0 : (ai = 0),
and IIR otherwise. Filter structures usually implement rational transfer
functions as difference equations.

Whether FIR or IIR, a given transfer function can be implemented with
many different filter structures. With infinite-precision data, coefficients,
and arithmetic, all filter structures implementing the same transfer function
produce the same output. However, different filter strucures may produce
very different errors with quantized data and finite-precision or fixed-point
arithmetic. The computational expense and memory usage may also differ
greatly. Knowledge of different filter structures allows DSP engineers to
trade off these factors to create the best implementation.
FIR Filter Structures

Consider  causal  FIR  filters:           M -1          h(k)x(n - k); this can be realized using

                                 y(n)  =  

                                                   k=0

the following structure

or in a different notation

This is called the direct-form FIR filter structure.

There are no closed loops (no feedback) in this structure, so it is called a non-
recursive structure. Since any FIR filter can be implemented using the direct-form,
non-recursive structure, it is always possible to implement an FIR filter non-
recursively. However, it is also possible to implement an FIR filter recursively, and
for some special sets of FIR filter coefficients this is much more efficient.

Example:
                                                       M -1

                            y(n) =  x(n - k)

                                                         k=0

where

               h(k) =       0, 0, 1 , 1, ..., 1,  1 , 0, 0, 0, ...

                                                  

                            k=0                   k=M -1

But note that

               y(n) = y(n - 1) + x(n) - x(n - M )

This can be implemented as

Instead of costing M - 1 adds/output point, this comb filter costs only two
adds/output.

Exercise:

   Problem: Is this stable, and if not, how can it be made so?

IIR filters must be implemented with a recursive structure, since that's the only way a
finite number of elements can generate an infinite-length impulse response in a linear,
time-invariant (LTI) system. Recursive structures have the advantages of being able
to implement IIR systems, and sometimes greater computational efficiency, but the
disadvantages of possible instability, limit cycles, and other deletorious effects that
we will study shortly.

Transpose-form FIR filter structures
The flow-graph-reversal theorem says that if one changes the directions of all the
arrows, and inputs at the output and takes the output from the input of a reversed
flow-graph, the new system has an identical input-output relationship to the original
flow-graph.
Direct-form FIR structure

reverse = transpose-form FIR filter structure

or redrawn

Cascade structures

The z-transform of an FIR filter can be factored into a cascade of short-length filters

b0 + b1z  -1         -3  + ... + bmz       -m                    -1                      -1                  -1
              + b2z                                   = b0 (1 - z1 z ) (1 - z2 z )... (1 - zm z )

where the zi are the zeros of this polynomial. Since the coefficients of the polynomial

are usually real, the roots are usually complex-conjugate pairs, so we generally

combine   (1  -             -1  (1  -             -1  into  one  quadratic  (length-2)  section  with  real

                 ziz )                 ziz )

coefficients

                 -1                    -1                        -1         2 -2
          (1 - zi z ) (1 - zi z ) = 1 - 2R(zi )z
                                                                            + (|zi |) z  = Hi (z)

The overall filter can then be implemented in a cascade structure.
This is occasionally done in FIR filter implementation when one or more of the short-
length filters can be implemented efficiently.

Lattice Structure
It is also possible to implement FIR filters in a lattice structure: this is sometimes
used in adaptive filtering
IIR Filter Structures

IIR (Infinite Impulse Response) filter structures must be recursive (use feedback); an infinite number of
coefficients could not otherwise be realized with a finite number of computations per sample.

         N (z)                          b0 + b1z      -1        + b2z   -2  + ... + bM z  -M

H (z) =                              =

         D(z)                           1 + a1z       -1     + a2z      -2  + ... + aN z  -N

The corresponding time-domain difference equation is

y(n) = (- (a1 y(n - 1))) - a2 y(n - 2) + ... - aN y(n - N ) + b0 x(0) + b1 x(n - 1) + ... + bM x(n - M )

Direct-form I IIR Filter Structure

The difference equation above is implemented directly as written by the Direct-Form I IIR Filter Structure.

Note that this is a cascade of two systems, N(z) and      1  .  If  we  reverse  the  order  of  the  filters,  the  overall
                                                      D(z)

system is unchanged: The memory elements appear in the middle and store identical values, so they can be

combined, to form the Direct-Form II IIR Filter Structure.

Direct-Form II IIR Filter Structure
This structure is canonic: (i.e., it requires the minimum number of memory elements).
Flowgraph reversal gives the

Transpose-Form IIR Filter Structure
Usually we design IIR filters with N = M, but not always.

Obviously, since all these structures have identical frequency response, filter structures are not unique. We
consider many different structures because

   1. Depending on the technology or application, one might be more convenient than another
   2. The response in a practical realization, in which the data and coefficients must be quantized, may differ

      substantially, and some structures behave much better than others with quantization.

The Cascade-Form IIR filter structure is one of the least sensitive to quantization, which is why it is the most
commonly used IIR filter structure.

IIR Cascade Form

The numerator and denominator polynomials can be factored

                           b0 + b1z  -1  + ... + bM z  -m                      M

                                                              b0 k=1 z - zk

                  H (z) =                                  =

                           1 + a1z  -1   + ... + aN z  -N     zM -N N  z - pk

                                                                                            i=1

and implemented as a cascade of short IIR filters.

Since the filter coefficients are usually real yet the roots are mostly complex, we actually implement these as
second-order sections, where comple-conjugate pole and zero pairs are combined into second-order sections
with real coefficients. The second-order sections are usually implemented with either the Direct-Form II or
Transpose-Form structure.

Parallel form

A rational transfer function can also be written as

b0 + b1z       -1  + ... + bM z  -m                                  A1             A2             AN
                                                                 z - p1  +                     z - pN
                                                      -1
                                     = c0 + c1 z          +...+                 z - p2  +...+

1 + a1z        -1  + ... + aN z  -N

which by linearity can be implemented as

As before, we combine complex-conjugate pole pairs into second-order sections with real coefficients.

The cascade and parallel forms are of interest because they are much less sensitive to coefficient quantization
than higher-order structures, as analyzed in later modules in this course.

Other forms

There are many other structures for IIR filters, such as wave digital filter structures, lattice-ladder, all-pass-
based forms, and so forth. These are the result of extensive research to find structures which are
computationally efficient and insensitive to quantization error. They all represent various tradeoffs; the best
choice in a given context is not yet fully understood, and may never be.
State-Variable Representation of Discrete-Time Systems

State and the State-Variable Representation

State
      the minimum additional information at time n, which, along with all current and future input values, is
      necessary to compute all future outputs.

Essentially, the state of a system is the information held in the delay registers in a filter structure or signal
flow graph.

Note: Any LTI (linear, time-invariant) system of finite order M can be represented by a state-variable
description

                                                                                            x(n + 1) = Ax(n) + Bu(n)
                                                                                                 y(n) = Cx(n) + Du(n)

where x is an M x 1 "state vector," u(n) is the input at time n, y(n) is the output at time n; A is an M x M
matrix, B is an M x 1 vector, C is a 1 x M vector, and D is a 1 x 1 scalar.

One can always obtain a state-variable description of a signal flow graph.

Example:
3rd-Order IIR

y(n) = (- (a1y(n - 1))) - a2y(n - 2) - a3y(n - 3) + b0x(n) + b1x(n - 1) + b2x(n - 2) + b3x(n - 3)

x1(n + 1)     0  1  0      x1(n)  0

x2(n + 1)  =  0  0  1      x2(n) + 0 u(n)

x3(n + 1)     -a3 -a2 -a1  x3(n)  1
y(n) = ( - (a3b0)                  - (a2b0)  - (a1b0) )  x1(n)                                                                  + (b0 )u(n)
                                                         x2(n)
                                                         x3(n)

Exercise:
   Problem: Is the state-variable description of a filter H(z) unique?

Exercise:
   Problem: Does the state-variable description fully describe the signal flow graph?

State-Variable Transformation

Suppose we wish to define a new set of state variables, related to the old set by a linear transformation:
q(n) = T x(n), where T is a nonsingular M x M matrix, and q(n) is the new state vector. We wish the
overall system to remain the same. Note that x(n) = T -1q(n), and thus

                               -1            -1                                                                                                -1

x(n + 1) = Ax(n) + Bu(n)  T q(n) = AT q(n) + Bu(n)  q(n) = T AT q(n) + T Bu(n)

                                                                                                                                           -1

y(n) = Cx(n) + Du(n)  y(n) = CT q(n) + Du(n)

This defines a new state system with an input-output behavior identical to the old system, but with different
internal memory contents (states) and state matrices.

                                   ^             ^

                                   q(n) = Aq(n) + Bu(n)

                                   ^             ^

                                   y(n) = Cq(n) + Du(n)

A = T AT ^ -1, B = T B ^ , C = CT ^ -1, D = D ^

These transformations can be used to generate a wide variety of alternative stuctures or implementations of a
filter.

Transfer Function and the State-Variable Description

Taking the z transform of the state equations

                                                                                    Z[x(n + 1)] = Z[Ax(n) + Bu(n)]
                                                                                          Z[y(n)] = Z[Cx(n) + Du(n)]
                                                                                                                              
                                                                                               zX(z) = AX(z) + BU (z)

Note: X(z) is a vector of scalar z-transforms X(z) = (X1(z) X2(z) ... ) T
so                                            Y (z) = CX(n) + DU (n)
Equation:
                                                                                                                                                                            -1

           (zI - A)X(z) = BU (z)  X(z) = (zI - A) BU (z)

                                                                                                                -1

                            Y (z) = C(zI - A) BU (z) + DU (z)

                                                                                                                   -1

                                             = (C(- (zI )) B + D)U (z)

and thus

                                                                               -1

           H (z) = C(zI - A) B + D

                                                                                                                                                                                                                    T

Note that since (zI - A) = -1 ±(det (zI-A)red) , this transfer function is an M th-order rational fraction in z.
                                                                                                                det(z(I )-A)

The denominator polynomial is D(z) = det (zI - A). A discrete-time state system is thus stable if the M

roots of det (zI - A) (i.e., the poles of the digital filter) are all inside the unit circle.

Consider the transformed state system with A = T AT ^ -1, B = T B ^ , C = CT ^ -1, D = D ^ :
Equation:

                     -1

           ^      ^      ^  ^

           H (z) = C(zI - A) B + D

              -1            -1 -1
           = CT (zI - T AT ) T B + D

              -1                                                  -1 -1
           = CT (T (zI - A)T ) T B + D

              -1  -1 -1                                           -1 -1
           = CT (T ) (zI - A) T T B + D

                                                              -1

           = C(zI - A) B + D

This proves that state-variable transformation doesn't change the transfer function of the underlying system.
However, it can provide alternate forms that are less sensitive to coefficient quantization or easier to analyze,
understand, or implement.

State-variable descriptions of systems are useful because they provide a fairly general tool for analyzing all
systems; they provide a more detailed description of a signal flow graph than does the transfer function
(although not a full description); and they suggest a large class of alternative implementations. They are even
more useful in control theory, which is largely based on state descriptions of systems.
Fixed-Point Number Representation

Fixed-point arithmetic is generally used when hardware cost, speed, or
complexity is important. Finite-precision quantization issues usually arise in
fixed-point systems, so we concentrate on fixed-point quantization and
error analysis in the remainder of this course. For basic signal processing
computations such as digital filters and FFTs, the magnitude of the data, the
internal states, and the output can usually be scaled to obtain good
performance with a fixed-point implementation.

Two's-Complement Integer Representation

As far as the hardware is concerned, fixed-point number systems represent
data as B-bit integers. The two's-complement number system is usually
used:

               binary integer representation  if 0  k  2B-1 - 1
k={                                           - 2B-1  k  0

               bit-by-bit inverse(-k) + 1 if
The most significant bit is known at the sign bit; it is 0 when the number is
non-negative; 1 when the number is negative.

Fractional Fixed-Point Number Representation

For the purposes of signal processing, we often regard the fixed-point
numbers as binary fractions between [-1, 1), by implicitly placing a
decimal point after the sign bit.

or

                             B-1

                  x = -b0 +                  -i

                                  bi2

                             i1

This interpretation makes it clearer how to implement digital filters in
fixed-point, at least when the coefficients have a magnitude less than 1.

Truncation Error
Consider the multiplication of two binary fractions

Note that full-precision multiplication almost doubles the number of bits; if
we wish to return the product to a B-bit representation, we must truncate
the B - 1 least significant bits. However, this introduces truncation error
(also known as quantization error, or roundoff error if the number is
rounded to the nearest B-bit fractional value rather than truncated). Note
that this occurs after multiplication.

Overflow Error

Consider the addition of two binary fractions;
Note the occurence of wraparound overflow; this only happens with
addition. Obviously, it can be a bad problem.

There are thus two types of fixed-point error: roundoff error, associated
with data quantization and multiplication, and overflow error, associated
with data quantization and additions. In fixed-point systems, one must
strike a balance between these two error sources; by scaling down the data,
the occurence of overflow errors is reduced, but the relative size of the
roundoff error is increased.

Note: Since multiplies require a number of additions, they are especially
expensive in terms of hardware (with a complexity proportional to BxBh,
where Bx is the number of bits in the data, and Bh is the number of bits in
the filter coefficients). Designers try to minimize both Bx and Bh, and
often choose Bx  Bh!
Fixed-Point Quantization

The fractional B-bit two's complement number representation evenly

distributes      B  quantization  levels  between  -1  and  1  -   . -(B-1)  The

             2                                                    2

spacing between quantization levels is then

                                   2  =2  -(B-1)    B
                                  2B

Any signal value falling between two levels is assigned to one of the two
levels.

XQ = Q[x] is our notation for quantization. e = Q[x] - x is then the
quantization error.

One method of quantization is rounding, which assigns the signal value to

the nearest level. The maximum error is thus           B    = 2-B.
                                                         2

Another common scheme, which is often easier to implement in hardware,
is truncation. Q[x] assigns x to the next lowest level.
The  worst-case  error  with  truncation  is    =   , -(B-1)  which  is  twice  as

                                                   2

large as with rounding. Also, the error is always negative, so on average it

may have a non-zero mean (i.e., a bias component).

Overflow is the other problem. There are two common types: two's
complement (or wraparound) overflow, or saturation overflow.

                wraparound

     saturation

Obviously, overflow errors are bad because they are typically large; two's
complement (or wraparound) overflow introduces more error than
saturation, but is easier to implement in hardware. It also has the advantage
that if the sum of several numbers is between [-1, 1), the final answer will
be correct even if intermediate sums overflow! However, wraparound
overflow leaves IIR systems susceptible to zero-input large-scale limit
cycles, as discussed in another module. As usual, there are many tradeoffs
to evaluate, and no one right answer for all applications.
Finite-Precision Error Analysis

Fundamental Assumptions in finite-precision error analysis

Quantization is a highly nonlinear process and is very difficult to analyze
precisely. Approximations and assumptions are made to make analysis
tractable.

Assumption #1

The roundoff or truncation errors at any point in a system at each time are
random, stationary, and statistically independent (white and independent
of all other quantizers in a system).

That is, the error autocorrelation function is re[k] = E[enen+k] = q 2[k].

Intuitively, and confirmed experimentally in some (but not all!) cases, one

expects the quantization error to have a uniform distribution over the

interval                         for  rounding,  or  (-, 0]          for  truncation.

          [- ,                )

                       2  2

In this case, rounding has zero mean and variance

                                      E[Q[xn ] - xn ] = 0

                                      2               2                          2

                                      Q = E[en ] =                   B

                                                                       12

and truncation has the statistics

                                                                                        

                                      E[Q[xn ] - xn ] = -
                                                                                         2

                                                   2              2

                                         Q =          B

                                                        12

Please note that the independence assumption may be very bad (for
example, when quantizing a sinusoid with an integer period N). There is
another quantizing scheme called dithering, in which the values are
randomly assigned to nearby quantization levels. This can be (and often is)
implemented by adding a small (one- or two-bit) random input to the signal
before a truncation or rounding quantizer.

This is used extensively in practice. Altough the overall error is somewhat
higher, it is spread evenly over all frequencies, rather than being
concentrated in spectral lines. This is very important when quantizing
sinusoidal or other periodic signals, for example.

Assumption #2
Pretend that the quantization error is really additive Gaussian noise with
the same mean and variance as the uniform quantizer. That is, model

                                             as

This model is a linear system, which our standard theory can handle easily.
We model the noise as Gaussian because it remains Gaussian after passing
through filters, so analysis in a system context is tractable.
Summary of Useful Statistical Facts

correlation function rx[k]  E[xnxn+k]

power spectral density Sx(w)  DTFT [rx[n]]

Note rx[0] = x 2 =  Sx(w) d w 1 
              2  -

                                              *

rxy [k]  E[x [n]y[n + k]]

cross-spectral density Sxy(w) = DTFT [rxy[n]]

For y = h*x:

              Syx (w) = H (w)Sx (w)

                                                                                  2

              Syy (w) = (|H (w)|) Sx (w)

Note that the output noise level after filtering a noise sequence is

                                                 1  

        2                                                2

y = ryy [0] =                                        (|H (w)|) Sx(w) d w

                                                  -

so postfiltering quantization noise alters the noise power spectrum and
may change its variance!
For x1, x2 statistically independent

              rx1+x2 [k] = rx1 [k] + rx2 [k]

                                             Sx1+x2 (w) = Sx1 (w) + Sx2 (w)

For independent random variables

                                                 2    2  2
              x1+x2 = x1 + x2
Input Quantization Noise Analysis
All practical analog-to-digital converters (A/D) must quantize the input
data. This can be modeled as an ideal sampler followed by a B-bit
quantizer.

The signal-to-noise ratio (SNR) of an A/D is
Equation:

                                                                                                                            Px

                                           SNR = 10 log

                                                                                                                            Pn

                                                                                                                                                                                                                                                      2

                                                                                                                                                                      B

                                                             = 10 log Px - 10 log

                                                                                                                                                                        12

                                                             = 10 log Px + 4.77 + 6.02B

where Px is the power in the signal and Pn is the power of the quantization
noise, which equals its variance if it has a zero mean. The SNR increases by
6dB with each additional bit.
Quantization Error in FIR Filters
In digital filters, both the data at various places in the filter, which are
continually varying, and the coefficients, which are fixed, must be
quantized. The effects of quantization on data and coefficients are quite
different, so they are analyzed separately.

Data Quantization

Typically, the input and output in a digital filter are quantized by the analog-
to-digital and digital-to-analog converters, respectively. Quantization also
occurs at various points in a filter structure, usually after a multiply, since
multiplies increase the number of bits.

Direct-form Structures
There are two common possibilities for quantization in a direct-form FIR
filter structure: after each multiply, or only once at the end.

    Single-precision accumulate; total variance M 12 2
                    Double-precision accumulate; variance 12 2

In the latter structure, a double-length accumulator adds all 2B - 1 bits of
each product into the accumulating sum, and truncates only at the end.
Obviously, this is much preferred, and should always be used wherever
possible. All DSP microprocessors and most general-pupose computers
support double-precision accumulation.

Transpose-form
Similarly, the transpose-form FIR filter structure presents two common
options for quantization: after each multiply, or once at the end.

      Quantize at each stage before storing intermediate sum. Output
                                   variance M 12 2
                                             or

   Store double-precision partial sums. Costs more memory, but variance

                                                                                                                         2
                                                                                                                          12

The transpose form is not as convenient in terms of supporting double-
precision accumulation, which is a significant disadvantage of this
structure.

Coefficient Quantization

Since a quantized coefficient is fixed for all time, we treat it differently than
data quantization. The fundamental question is: how much does the
quantization affect the frequency response of the filter?
The quantized filter frequency response is

            DTFT [hQ ] = DTFT [hinf . prec. + e] = Hinf . prec. (w) + He (w)

Assuming the quantization model is correct, He(w) should be fairly
random and white, with the error spread fairly equally over all frequencies
w  [-, ); however, the randomness of this error destroys any equiripple
property or any infinite-precision optimality of a filter.
Exercise:

   Problem:
   What quantization scheme minimizes the L2 quantization error in
   frequency (minimizes - (|H(w) - HQ(w)|) d w  2 )? On average,
   how big is this error?
Ideally, if one knows the coefficients are to be quantized to B bits, one
should incorporate this directly into the filter design problem, and find the
M B-bit binary fractional coefficients minimizing the maximum deviation (
L error). This can be done, but it is an integer program, which is known
to be np-hard (i.e., requires almost a brute-force search). This is so
expensive computationally that it's rarely done. There are some sub-optimal
methods that are much more efficient and usually produce pretty good
results.
Data Quantization in IIR Filters

Finite-precision effects are much more of a concern with IIR filters than
with FIR filters, since the effects are more difficult to analyze and
minimize, coefficient quantization errors can cause the filters to become
unstable, and disastrous things like large-scale limit cycles can occur.

Roundoff noise analysis in IIR filters

Suppose there are several quantization points in an IIR filter structure. By

our simplifying assumptions about quantization error and Parseval's

theorem,  the  quantization  noise      variance                       2  at  the  output              of  the  filter

                                                            y,i

from the ith quantizer is

Equation:

                     2         1                                 2

                y,i     =    2 - (|Hi (w)|) SSni (w) d w

                             n       2

                                  i                                       2

                        = 2 - (|Hi (w)|) d w

                                     2                        2

                        = ni n=- hi (n)

where ni 2 is the variance of the quantization error at the ith quantizer,
SSni(w) is the power spectral density of that quantization error, and
HHi(w) is the transfer function from the ith quantizer to the output point.
Thus for P independent quantizers in the structure, the total quantization
noise variance is

                                     P                     

                       2    1                           2                                       2  dw
                           2
               y =              ni                            (|Hi (w)|)

                                  i=1                      -

Note that in general, each Hi(w), and thus the variance at the output due to
each quantizer, is different; for example, the system as seen by a quantizer
at the input to the first delay state in the Direct-Form II IIR filter structure
to the output, call it n4, is
with a transfer function

                                                 z -2

                H4 (z) =                         -1             -2

                                     1 + a1z             + a2z

which  can  be  evaluated  at  z  =      iw  to  obtain  the  frequency  response.

                                     e

A general approach to find Hi(w) is to write state equations for the
equivalent structure as seen by ni, and to determine the transfer function
according to H(z) = C(zI - A) B + d -1 .
Exercise:
   Problem:

   The above figure illustrates the quantization points in a typical
   implementation of a Direct-Form II IIR second-order section. What is
   the total variance of the output error due to all of the quantizers in the
   system?

By making the assumption that each Qi represents a noise source that is
white, independent of the other sources, and additive,
the variance at the output is the sum of the variances at the output due to
each noise source:

                4

             2         2

             y =  y,i

                i=1

The variance due to each noise source at the output can be determined from

 1 (|Hi(w)|) Sni (w) d w  2 ; note that Sni (w) = ni 2 by our
2  -

assumptions, and Hi(w) is the transfer function from the noise source to

the output.
IIR Coefficient Quantization Analysis

Coefficient quantization is an important concern with IIR filters, since
straigthforward quantization often yields poor results, and because
quantization can produce unstable filters.

Sensitivity analysis

The performance and stability of an IIR filter depends on the pole locations,
so it is important to know how quantization of the filter coefficients ak
affects the pole locations pj. The denominator polynomial is

                        N                     N

                                     -k                   -1

                 D(z) = 1 +  ak z        =  1 - piz

                        k=1                   i=1

We wish to know   pi  , which, for small deviations, will tell us that a 
                 a
                 k

change in ak yields an  =  a  pi change in the pole location. a  pi is the
                        k                                     k

sensitivity of the pole location to quantization of ak. We can find   pi   using
                                                                     a
                                                                     k

the chain rule.

                  A(z)                A(z)  z
                    ak
                        z=pi =         z      ak    z=pi

                                     

                       pi             A(zi )  z=pi
                                  =      ak

                      ak              A(zi )
                                          z
                                              z=pi

which is
Equation:
 pi               z -k
ak
     =  -(z-1 N                         1-pj z-1 )  z=pi
     =
        j=ji,1

                                  N -k

                -p i

                N

        j=ji,1 pj -pi

Note that as the poles get closer together, the sensitivity increases greatly.
So as the filter order increases and more poles get stuffed closer together
inside the unit circle, the error introduced by coefficient quantization in the
pole locations grows rapidly.

How can we reduce this high sensitivity to IIR filter coefficient
quantization?

Solution

Cascade or parallel form implementations! The numerator and denominator
polynomials can be factored off-line at very high precision and grouped into
second-order sections, which are then quantized section by section. The
sensitivity of the quantization is thus that of second-order, rather than N-th
order, polynomials. This yields major improvements in the frequency
response of the overall filter, and is almost always done in practice.

Note that the numerator polynomial faces the same sensitivity issues; the
cascade form also improves the sensitivity of the zeros, because they are
also factored into second-order terms. However, in the parallel form, the
zeros are globally distributed across the sections, so they suffer from
quantization of all the blocks. Thus the cascade form preserves zero
locations much better than the parallel form, which typically means that the
stopband behavior is better in the cascade form, so it is most often used in
practice.

Note: On the basis of the preceding analysis, it would seem important to
use cascade structures in FIR filter implementations. However, most FIR
filters are linear-phase and thus symmetric or anti-symmetric. As long as
the quantization is implemented such that the filter coefficients retain
symmetry, the filter retains linear phase. Furthermore, since all zeros off
the unit circle must appear in groups of four for symmetric linear-phase
filters, zero pairs can leave the unit circle only by joining with another pair.
This requires relatively severe quantizations (enough to completely remove
or change the sign of a ripple in the amplitude response). This "reluctance"
of pole pairs to leave the unit circle tends to keep quantization from
damaging the frequency response as much as might be expected, enough so
that cascade structures are rarely used for FIR filters.

Exercise:

   Problem: What is the worst-case pole pair in an IIR digital filter?

   Solution:

   The pole pair closest to the real axis in the z-plane, since the complex-
   conjugate poles will be closest together and thus have the highest
   sensitivity to quantization.

Quantized Pole Locations

In a direct-form or transpose-form implementation of a second-order
section, the filter coefficients are quantized versions of the polynomial
coefficients.

                                                                                        2

                                      D(z) = z + a1 z + a2 = (z - p) (z - p )

p=  -a1 ± a1 2 - 4a2
                          2

                           i

    p = re

    2                         2
D(z) = z - 2r cos() + r
So

                                                                 a1 = - (2r cos())

                                                                                                                                        2

                                                                               a2 = r

Thus the quantization of a1 and a2 to B bits restricts the radius r to
r = kB, and a1 = - (2R(p)) = kB The following figure shows all
stable pole locations after four-bit two's-complement quantization.

Note the nonuniform distribution of possible pole locations. This might be

good for poles near r = 1,  =     ,  but  not  so  good  for  poles  near  the  origin
                               2

or the Nyquist frequency.
In the "normal-form" structures, a state-variable based realization, the poles
are uniformly spaced.

This can only be accomplished if the coefficients to be quantized equal the
real and imaginary parts of the pole location; that is,

              1 = r cos() = R(r)

              2 = r sin() = I(p)

This is the case for a 2nd-order system with the state matrix

     1     2  ): The denominator polynomial is

A=(

     -1 1

Equation:
                                  2          2

det (zI - A) = (z - 1 ) + 2

                  2                          2           2
                  = z - 21 z + 1 + 2

                  2                                2         2            2
                  = z - 2r cos()z + r (cos () + sin ())

                  = z2 - 2r cos()z + r2

Given any second-order filter coefficient set, we can write it as a state-space

system,  find  a  transformation  matrix  T  such  that   ^  =  T  -1 AT  is  in  normal

                                                         A

form, and then implement the second-order section using a structure

corresponding to the state equations.

The normal form has a number of other advantages; both eigenvalues are
equal, so it minimizes the norm of Ax, which makes overflow less likely,
and it minimizes the output variance due to quantization of the state values.
It is sometimes used when minimization of finite-precision effects is
critical.
Exercise:

Problem: What is the disadvantage of the normal form?

Solution:

It requires more computation. The general state-variable equation
requires nine multiplies, rather than the five used by the Direct-Form II
or Transpose-Form structures.
Limit Cycles

Large-scale limit cycles

When overflow occurs, even otherwise stable filters may get stuck in a
large-scale limit cycle, which is a short-period, almost full-scale persistent
filter output caused by overflow.

Example:
Consider the second-order system

                   H (z) =                    1
                            1 - z-1 +
                                                 1 z-2

                                                 2

with zero input and initial state values z0[0] = 0.8, z1[0] = -0.8. Note

y[n] = z0[n + 1].

The filter is obviously stable, since the magnitude of the poles is

1 = 0.707, which is well inside the unit circle. However, with

2

wraparound overflow, note that y[0] = z0[1] = 4 - ( 1 - ) 4 = 6 = - 4
                                                    5            2  5  5  5

, and that z0[2] = y[1] = (- ) 4 - 1 4 = - 6 = 4 , so
                   5              25             5            5

y[n] = - 5 , 5 , - 5 , 5 , ... 4 4 4 4 even with zero input.
Clearly, such behavior is intolerable and must be prevented. Saturation
arithmetic has been proved to prevent zero-input limit cycles, which is one
reason why all DSP microprocessors support this feature. In many
applications, this is considered sufficient protection. Scaling to prevent
overflow is another solution, if as well the inital state values are never
initialized to limit-cycle-producing values. The normal-form structure also
reduces the chance of overflow.

Small-scale limit cycles

Small-scale limit cycles are caused by quantization. Consider the system

Note that when z0  >  z0 -  B    ,  rounding  will  quantize  the  output  to  the
                              2

current level (with zero input), so the output will remain at this level

forever. Note that the maximum amplitude of this "small-scale limit cycle"

is achieved when

z0 = z0 -             B          == zmax =                    B
                        2                           2 × (1 - )

In a higher-order system, the small-scale limit cycles are oscillatory in
nature. Any quantization scheme that never increases the magnitude of any
quantized value prevents small-scale limit cycles.
Note:Two's-complement truncation does not do this; it increases the
magnitude of negative numbers.

However, this introduces greater error and bias. Since the level of the limit
cycles is proportional to B, they can be reduced by increasing the number
of bits. Poles close to the unit circle increase the magnitude and likelihood
of small-scale limit cycles.
Scaling

Overflow is clearly a serious problem, since the errors it introduces are very
large. As we shall see, it is also responsible for large-scale limit cycles, which
cannot be tolerated. One way to prevent overflow, or to render it acceptably
unlikely, is to scale the input to a filter such that overflow cannot (or is
sufficiently unlikely to) occur.

In a fixed-point system, the range of the input signal is limited by the
fractional fixed-point number representation to |x[n]|  1. If we scale the
input by multiplying it by a value , 0 <  < 1, then |x[n]|  .

Another option is to incorporate the scaling directly into the filter
coefficients.

FIR Filter Scaling

What value of  is required so that the output of an FIR filter cannot overflow
(n : (|y(n)|  1), n : (|x(n)|  1))?

M -1  M -1  M -1

|y(n)| =  h(k)x(n - k)   |h(k)| || |x(n - k)|    |h(k)|1

k=0   k=0   k=0

      

                                                                                                                       M -1

                                                                         <  |h(k)|

                                                                                                                        k=0

Alternatively, we can incorporate the scaling directly into the filter, and
require that
                                        M -1

                                         |h(k)| < 1

                                         k=0

to prevent overflow.

IIR Filter Scaling

To prevent the output from overflowing in an IIR filter, the condition above
still holds: (M = )

                                                                                                                                  

                                                                   |y(n)| <  |h(k)|

                                                                                                                                k=0

so an initial scaling factor  <                  1          can be used, or the filter itself can

                                           

                                        k=0 |h(k)|

be scaled.

However, it is also necessary to prevent the states from overflowing, and to

prevent overflow at any point in the signal flow graph where the arithmetic

hardware would thereby produce errors. To prevent the states from

overflowing, we determine the transfer function from the input to all states i,

and  scale  the  filter  such  that                 

                                     i  :  (                |hi (k)|    1)

                                                       k=0

Although this method of scaling guarantees no overflows, it is often too
conservative. Note that a worst-case signal is x(n) = sign (h(-n)); this
input may be extremely unlikely. In the relatively common situation in which
the input is expected to be mainly a single-frequency sinusoid of unknown
frequency and amplitude less than 1, a scaling condition of

                                     w : (|H (w)|  1)

is sufficient to guarantee no overflow. This scaling condition is often used. If
there are several potential overflow locations i in the digital filter structure,
the scaling conditions are

                               i, w : (|Hi (w)|  1)
where Hi(w) is the frequency response from the input to location i in the
filter.

Even this condition may be excessively conservative, for example if the input
is more-or-less random, or if occasional overflow can be tolerated. In
practice, experimentation and simulation are often the best ways to optimize
the scaling factors in a given application.

For filters implemented in the cascade form, rather than scaling for the entire
filter at the beginning, (which introduces lots of quantization of the input) the
filter is usually scaled so that each stage is just prevented from overflowing.
This is best in terms of reducing the quantization noise. The scaling factors
are incorporated either into the previous or the next stage, whichever is most
convenient.

Some heurisitc rules for grouping poles and zeros in a cascade
implementation are:

   1. Order the poles in terms of decreasing radius. Take the pole pair closest
      to the unit circle and group it with the zero pair closest to that pole pair
      (to minimize the gain in that section). Keep doing this with all remaining
      poles and zeros.

   2. Order the section with those with highest gain (argmax |Hi(w)|) in the
      middle, and those with lower gain on the ends.

Leland B. Jackson has an excellent intuitive discussion of finite-precision
problems in digital filters. The book by Roberts and Mullis is one of the most
thorough in terms of detail.
