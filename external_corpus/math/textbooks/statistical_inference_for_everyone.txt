BRIAN BLAIS

S TAT I S T I C A L I N F E R E N C E
FOR EVERYONE
L I F E ' S M O S T I M P O RTA N T Q U E S T I O N S A R E , F O R T H E M O S T PA RT, N OT H -
ING BUT PROBABILITY PROBLEMS.
PIERRE-SIMON LAPLACE

S TAT I S T I C A L T H I N K I N G W I L L O N E DAY B E A S N E C E S S A RY F O R E F F I C I E N T
CITIZENSHIP AS THE ABILITY TO READ AND WRITE.
H.G.WELLS

S TAT I S T I C S A R E T H E H E A R T O F D E M O C R A C Y.
SIMEON STRUNSKY
BRIAN BLAIS

S TAT I S T I C A L I N F E R E N C E
FOR EVERYONE

SAVE THE BROCCOLI PUBLISHING
Copyright © 2020 Brian Blais

published by save the broccoli publishing

typeset with tufte-latex

This book is licensed under the Creative Commons Attribution-ShareAlike license, version 4.0, http://
creativecommons.org/licenses/by-sa/4.0/, except for those photographs and drawings of which I am
not the author, as listed in the photo credits. If you agree to the license, it grants you certain privileges
that you would not otherwise have, such as the right to copy the book, or download the digital version
free of charge from http://web.bryant.edu/~bblais. At your option, you may also copy this book
under the GNU Free Documentation License version 1.2, http://www.gnu.org/licenses/fdl.txt, with no
invariant sections, no front-cover texts, and no back-cover texts.

First printing, September 2014. Last Compiled April 17, 2020.
Dedicated to all of the wonderful people at
Bryant University in the Library, Writing
Center, and the Center for Teaching and
Learning who have been exceedingly supportive
of me through the entire process of writing.
Acknowledgements

I would like to acknowledge the following people who have added
to this book, in big ways and in small. The book is much better as a
result.

Billie Anderson    Jim Bishop                                         Jenifer Bond
Paul Campbell      Stephanie Carter                                   Allen Downey
Robert Fairhead    Rodrigo Fernandez-Vizarra                          Thomas Hartl
Harold Hausmann    Steve Heller                                       Jeremy Hussell
Laura Kohl         David Louton                                       Patrick Marchand
Alan Olinksy       John Quinn                                         Matt Renfro
Steven Rush        Hakan Saraoglu                                     Phillis Schumacher
James Scott-Brown  Robert Shea                                        Shergunov Vasiliy
    Contents

      Proposal 29

1 Introduction to Probability 33

       1.1 Models and Data 34
       1.2 What is Probability? 35

                    Card Game 36
                    Other Observations 38

       1.3 Conditional Probability 39

                    Probability Notation 39

       1.4 Rules of Probability 40

                    Negation Rule 41
                    Product Rule 42
                    Independence 43
                    Conjunction 43
                    Sum Rule 45
                    Marginalization 46
                    Bayes' Rule 47

       1.5 Venn Mnemonic for the Rules of Probability 49
       1.6 Lessons from Bayes' Rule - A First Look 50

2 Applications of Probability 53

       2.1 Cancer and Probability 53
       2.2 Weather 55

                    First Solution - Independence 55
                    Second Solution - Correlation 55
        10

       2.3 Adding Dice 56
       2.4 The Birthday Problem 58

                    Two People on April 3 58
                    Two People 58
                    Three People 59
                    Two People...Out of Three 60
                    Two People...Out of Thirty 62

       2.5 The Lottery Problem or Rare Things Are Common 62
       2.6 Monty Hall Problem 64

                    Two Doors with Information 65
                    Three Doors with Information 65
                    Three Doors Down To Two 66

       2.7 Exercises 66
       2.8 Some Philosophical Applications 67

                    Doctors' Claims - English Language and Probability 67
                    Diverging Opinions 68
                    A problem of independence 69
                    Another problem with independence 71
                    Prosecutor's Fallacy 72

       2.9 Computer Examples 72

                    Coin Flips 72

3 Random Sequences and Visualization 75

       3.1 Coin Flipping 75

                    Counting the Rearrangements 78
                    Sequences of Heads and Tails 79

       3.2 Binomial Distribution 82
       3.3 Some Philosophical Applications 82

                    Streaks 82
                    Gambler's Fallacy 83
                    The Hot Hand - Correlations in Random Sequences 84
                    Regression Toward the Mean 85
                                                                                                                                                  11

       3.4 Visualization of Data 87

                    Histograms 87
                    Scatter Plots 90

       3.5 Computer Examples 92

                    Histograms 92
                    Scatter Plot 92

4 Introduction to Model Comparison 95

       4.1 The High/Low Deck Game 95

                    What does our intuition say? 95
                    Before the data - the prior 97
                    The "easy" question - the likelihood 97
                    Applying the Bayes' recipe 98
                    Drawing the next card 99
                    Prior information or not? 100

       4.2 Multiple Hypotheses 102

5 Applications of Model Comparison 109

       5.1 Disease Testing 109

                    Consequences 111

       5.2 M&M's 112

                    Updating with other data 113

       5.3 Psychic Octopi 114

                    Making a Well Posed Problem 114
                    The First Model Comparison 116
                    Furthering the Comparison 117

       5.4 Monty Hall Problem 117

6 Introduction to Parameter Estimation 121

       6.1 Bent Coins 121
       6.2 Priors versus Data 124
        12

       6.3 Moving Toward the Continuous 125
       6.4 MAP and Areas 127
       6.5 Quartiles 129
       6.6 Best Estimates 131
       6.7 Uncertainty in the Best Estimates 133
       6.8 Marginalization 134
       6.9 Exercises 134
       6.10 Computer Examples 135

                    Beta Distribution Example 135

7 Priors, Likelihoods, and Posteriors 139

       7.1 Binomial and Beta Distributions 139
       7.2 The Normal Distribution - Properties 140

                    The Shape 140
                    The location parameter, µ 141
                    The deviation parameter,  141
                    Summarizing the Distribution 142
                    Moving from a General Normal to the Standard Normal and Back 142
                    Sum and Differences 144

       7.3 The Normal Distribution - Estimating From Data 146

                    Estimating the mean, µ, knowing the deviation,  146
                    Estimating the mean, µ, not knowing the deviation,  148

       7.4 Normal Approximation 149

                    The Beta Distribution 149
                    The Binomial Distribution 151
                    The Student's t Distribution 152

       7.5 Summary 154
       7.6 Computer Examples 155

                    Estimating Lengths 155
                                                                                                                                                  13

8 Common Statistical Significance Tests 159

       8.1 z-test 159
       8.2 What it means and doesn't mean 161

                    Significance 161

       8.3 Student-t-test 162
       8.4 Computer Examples 163

9 Applications of Parameter Estimation and Inference 165

       9.1 Normal Model - Inference about Means 165
       9.2 Normal Model Again - Inference about Means and Deviations 166
       9.3 Beta Model - Inference About Proportions 170
       9.4 Model Construction 173
       9.5 Computer Examples 180

                    Iris Example 181
                    Sunrise 183
                    Cancer Example 184
                    Pennies 185
                    Ball Bearing Sizes 190

10 Multi-parameter Models 193

       10.1 Simple Linear Regression 193

                    Mean Squared Error 195
                    An Educational Example 197

       10.2 Multiple regression 199
       10.3 Polynomial Regression 202
       10.4 Computer Examples 202

11 Introduction to MCMC 211

       11.1 One-Dimensional Models 211

                    Reading the Output 212
14

11.2 Multi-Dimensional Models 213
11.3 Hierarchical Model Example - Kruschke BEST Test 215

12 Concluding Thoughts            219

       12.1 Where have we come?   219
       12.2 Where are we going?  220

Bibliography 221

Appendix A: Computational Analysis 223

Appendix B: Notation and Standards              225

B.1 Useful Greek Letters 225                  227
B.2 Some Math Notation 225

            Variables 225
            Sums 226
            Products 226
            Sample Mean 226
            Sample Standard Deviation 227
            Estimates 227
            Factorials 227

B.3 Qualitative labels to probability values

Appendix C: Common Distributions and Their Properties 229

C.1 Discrete and Continuous 229
C.2 Uniform 229

            Discrete 229
            Continuous 229

C.3 Binomial 232
C.4 Beta 232
C.5 Normal (Gaussian) 233
                                                                                                                                          15

Appendix D: Tables 235

D.1 Credible Intervals for Standard Normal Distribution 235
D.2 Credible Intervals for Student's t Distribution 236
D.3 Cumulative Standard Normal Distribution 238
List of Figures

1.1 Standard 52-card deck. 13 cards of each suit, labeled Spades, Clubs,
      Diamonds, Hearts. 36

1.2 Venn diagram of a statement, A, in a Universe of all possible state-
      ments. It is customary to think of the area of the Universe to be equal
      to 1 so that we can treat the actual areas as fractional areas represent-
     ing the probability of statements like P(A). In this image, A takes
     up 1/4 of the Universe, so that P(A) = 1/4. Also shown is the nega-
     tion rule. P(A) + P(not A) = 1 or "inside" of A + "outside" of A
      adds up to everything. 49

1.3 Venn diagram of the sum and product. The rectangle B takes up 1/8
      of the Universe, and the rectangle A takes up 1/4 of the Universe. Their
     overlap here is 1/16 of the Universe, and represents P(A and B). Their
     total area of 5/16 of the Universe represents P(A or B). 49

1.4 Venn diagram of conditional probabilities, P(A|B) and P(B|A). (Right)
     P(A|B) is represented by the fraction of the darker area (which was
      originally part of A) compared not to the Universe but to the area of
     B, and thus represents P(A|B) = 1/2. In a way, it is as if the con-
     ditional symbol, "|," defines the Universe with which to make the com-
      parisons. (Left) Likewise, the same darker area that was originally
     part of B represents P(B|A) which makes up 1/4 of the area of A.
     Thus P(B|A) = 1/4. 50

1.5 Venn diagram of mutually exclusive statements. One can see that P(A and B) =
     0 (the overlap is zero) and P(A or B) = P(A) + P(B) (the total area
      is just the sum of the two areas) 50

2.1 Probability for rolling various sums of two dice. Shown are the re-
      sults for two 6-sided dice (left) and two 20-sided dice (right). The dashed
      line is for clarity, but represents the fact that you can't roll a fractional
      sum, such as 2.5. 57

2.2 Probability of having at least two people in a group with the same
      birthday depending on the number of people in the group. The 50%
      mark is exceeded once the group size exceeds 23 people. 63
18

3.1 Probability of getting h heads in 30 flips. Clearly the most likely value
      is 15, but all of the numbers from 12 up to 18 have significant prob-
      ability. 81

3.2 Probability of getting h heads in 30 flips given a possible unfair coin.
     One coin has p = 0.1, where the maximum is for 3 heads (or 1/10
     of the 30 flips), but 2 heads is nearly as likely. Another has p = 0.5,
      and is the fair coin considered earlier with a maximum at 15 heads
     (or 1/2 of the 30 flips). Finally, another coin shown as p = 0.8 where
      24 heads (or 8/10 of the 30 flips) is maximum. 83

4.1 High Deck - 55 Cards with ten 10's, nine 9's, etc... down to one Ace.
      Aces are equivalent to the value 1. 96

4.2 Low Deck - 55 Cards with ten Aces, nine 2's, etc... up to one 10. Aces
      are equivalent to the value 1. 96

4.3 Drawing a number of 9's in a row, possibly from a High, Low, and
      Nines deck. 105

5.1 Rare disease and testing. Shown is a population of 3000 where 1 in
      every 200 people have the disease (large circles). A test which is 99%
      effective is applied to everyone in the population, and the positive
      test results (i.e. the test says that you have the disease) are shown ask
      small black dots. Notice that although nearly all of those that have
      the disease test positive (a small black dot inside a large circle), there
      are many false positives (black dot in an empty square) - healthy peo-
      ple that test positive for the disease. Even though the test is quite good,
      there are many more healthy people and 1 out of 100 of them will
      erroneously test positive. 111

5.2 The full results of the predictions of Paul the Octopus, reproduced
      from en.wikipedia.org/wiki/Psychic_octopus. 115

6.1 Bent Coins 121
6.2 Probability for different bent-coin models, given the data=9 tails, 3

      heads. 123
6.3 Probability for different bent-coin models, given no data (left), the

      first tails (middle), and the second tails (right). The curve for no data
      is the same as the prior probability, and in this case all models are
      equally likely. When the first tails is observed, the model which states
      that heads are certain (coin 10) goes to zero probability. As more tails
      are observed, the probability for the lower models is increased. 125
6.4 Probability for different bent-coin models, given three tails (left), the
      first heads (middle), and another tails (right). When the first heads
      is observed, the model which states that heads are impossible (coin
      0) goes to zero probability. 125
                                                                                                                                          19

6.5 Probability for different bent-coin models, given no data (left), the
      first half of the data set (middle), and the entire data set of 9 tails and
      3 heads (right). 126

6.6 Posterior probability distribution for the  values of the bent coin -
      the probability that the coin will land heads. The distribution is shown
     for data 3 heads and 9 tails, with a maximum at  = 0.25. 128

6.7 Posterior probability distribution for the  values of the bent coin -
      the probability that the coin will land heads. The distribution is shown
     for data 3 heads and 9 tails. The area under the curve from  = 0
     (the "all heads" coin) to  = 0.5 (the "fair" coin) is 0.954. 129

6.8 Posterior probability distribution for the  values of the bent coin -
      the probability that the coin will land heads. The distribution is shown
     for data 3 heads and 9 tails. The area under the curve from  = 0
     (the "all heads" coin) to  = 0.28 is 0.5 - half the area. This repre-
      sents the median of the distribution. 130

6.9 Posterior probability distribution for the  values of the bent coin -
      the probability that the coin will land heads. The distribution is shown
      for data 3 heads and 9 tails. The various quartiles are shown in the
      plot, and summarized in the accompanying table. 131

6.10 Posterior probability distribution for the  values of the bent coin -
      the probability that the coin will land heads. The distribution is shown
      for data 10 heads and 20 tails. The various quartiles are shown in the
      plot, and summarized in the accompanying table. 135

7.1 The Normal Distribution. 140
7.2 The Normal distribution with different location parameters, µ. 141
7.3 The Normal distribution with different deviation parameters, . 142
7.4 The Standard Normal Distribution (the Normal distribution in the

     special case where µ = 0 and  = 1). The percentiles shown are
      for positions 1- away from the center, 2- away, and 3- away. The
      area within 1- is 0.68, within 2- is 0.95, and 3- is 0.99. These lo-
      cations are the most prevalently used in any kind of statistical test-
      ing, and thus we will see them many times. 143

9.1 Probability distributions for the subset of iris petal lengths. Each dis-
      tribution follows a Student-t form. 168

9.2 Probability distributions for the difference between iris petal lengths
      for the closest two iris types, Virginica and Versicolor. The distribu-
      tion follows a Student-t form, and clearly shows significant proba-
      bility (greater than 99%) for being greater than zero. 169

9.3 Mass of Pennies from 1960 to 1974. 174
9.4 Mass of Pennies from 1960 to 1974, with best estimates and 99% CI

      (i.e. 3) uncertainty. 177
20

9.5 Mass of Pennies from 1960 to 2003, with best estimates and 99% CI
      (i.e. 3) uncertainty. 179

9.6 Mass of Pennies from 1960 to 2003, with best estimates for the two
      true values and their 99% CI (i.e. 3) uncertainty plotted. There is
      clearly no overlap in their credible intervals, thus there is a statisti-
      cally significant difference between them. 180

9.7 Difference in the estimated values of the pre- and post 1975 pennies,
     µ1 - µ2. The value zero is clearly outside of the 99% interval of the
      difference, thus there is a statistically significant difference between
      the two values µ1 and µ2. 181

10.1 Heights (in inches) and shoe sizes from a subset of McLaren (2012)
      data. 194

10.2 Posterior distribution for the slope for the linear model on the shoe
      size data subset. 195

10.3 Posterior distribution for the intercept for the linear model on the
      shoe size data subset. 196

10.4 Best linear fit for the shoe size data subset. 196
10.5 Minimizing the Mean Squared Error (MSE) results in the best lin-

      ear fit for the shoe size data subset. 197
10.6 Total SAT score vs expenditure (top) and the distributions for the slope

      (bottom left) and intercept (bottom right). 198
10.7 Percent of students taking the SAT vs per pupil expenditure (top)

      and the distributions for the slope (bottom left) and intercept (bot-
      tom right). 200
10.8 The posterior distributions for coefficients on the expenditure term,
      the percent taking term, and the intercept. 201

11.1 So-called MCMC "chains" for parameter  versus time. Observe that
      the values of  start spread evenly from 0 to 1 at the beginning and
      then thin down to a range of about 0.5-0.8 with the middle around
      0.7 (17/25 = 0.68). 212

11.2 Distribution of , and the 95% credible interval. 213
11.3 Chains for parameters a, b, and the noise . 213
11.4 Data (blue) and predictions (green) for the model - the width of the

      predictions demonstrates the uncertainty. 214
11.5 Distributions for parameters a and b (slope and intercept). 215
11.6 Chains for parameter mu1, the mean of the drug group. 217
11.7 Distribution for parameter mu1, the mean of the drug group. 217
11.8 png 218
11.9 Distribution for parameter , the mean of the difference between the

      drug group and the placebo group. 218

C.1 Discrete uniform distribution for values 1 to 6. The value for each
     is p(xi) = 1/6. 230
                                                                                                                                          21

C.2 Continuous uniform distribution between values 0 and 1 231
C.3 Continuous uniform distribution for the plumber example (Exam-

      ple C.1). 231
C.4 Probability of getting h heads in 30 flips given a possible unfair coin.

     One coin has p = 0.1, where the maximum is for 3 heads (or 1/10
     of the 30 flips), but 2 heads is nearly as likely. Another has p = 0.5,
      and is the fair coin considered earlier with a maximum at 15 heads
     (or 1/2 of the 30 flips). Finally, another coin shown as p = 0.8 where
      24 heads (or 8/10 of the 30 flips) is maximum. 232
C.5 Posterior probability distribution for the  values of the bent coin -
      the probability that the coin will land heads. The distribution is shown
      for data 3 heads and 9 tails. The various quartiles are shown in the
      plot. 233
C.6 The normal distribution. 234
List of Examples

1.1 What is the fraction of the first card as a jack given that we know
      that the first card is a face card? . . . . . . . . . . . . . . . . . 41

1.2 What is the fraction of cards that are Jacks and a heart? . . . 42
1.3 What is the probability of drawing two Kings in a row? . . 42
1.4 What is the probability of flipping two heads in a row? . . . 43
1.5 Marginalization and Card Suit . . . . . . . . . . . . . . . . . 46
1.6 What is the probability of drawing a jack, knowing that you've

      drawn a face card? . . . . . . . . . . . . . . . . . . . . . . . . 47
2.1 What is the probability of both having cancer and getting a

      positive test for it? . . . . . . . . . . . . . . . . . . . . . . . . 53
2.2 What is the probability of both not having cancer and getting

      a positive test for it? . . . . . . . . . . . . . . . . . . . . . . . 54
2.3 What is the probability of having cancer given a positive test

      for it? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
2.4 If the probability that it will rain next Saturday is 0.25 and the

      probability that it will rain next Sunday is 0.25, what is the prob-
      ability that it will rain during the weekend? . . . . . . . . . 55
2.5 What is the probability of the sum of two dice getting a par-
      ticular value, say, 7? . . . . . . . . . . . . . . . . . . . . . . . 56
2.6 What is the probability of rolling a sum more than 7 with two
      dice? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
2.7 What is the probability of rolling various sums with two dice
      each with 20 sides? . . . . . . . . . . . . . . . . . . . . . . . . . 57
2.8 Let's imagine we have the case where two people meet on the
      street. What is the probability that they both have April 3 as
      their birthday? . . . . . . . . . . . . . . . . . . . . . . . . . . 58
2.9 Two people meet on the street, and we ask what is the prob-
      ability that they both have the same birthday? . . . . . . . . 58
2.10 What is the probability that three random people have the same
      birthday? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
2.11 What is the probability that at least two have the same birth-
      day? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
2.12 What is the probability that at least two have the same birth-
      day? A clever shortcut. . . . . . . . . . . . . . . . . . . . . . . 61
24

2.13 When you have a group of 30 people, like students in a class-
      room, and you ask what the probability of finding two in the
      room with the same birthday, would your intuition say it is
      greater or less than 50%? . . . . . . . . . . . . . . . . . . . . 62

2.14 Suppose you're on a game show, and you're given the choice
      of three doors: behind one door is a car; behind the others, goats.
      You pick a door, say No. 1 (but the door is not opened), and
      the host, who knows what's behind the doors, opens another
      door, say No. 3, which has a goat. He then says to you, "Do
      you want to change your choice to door No. 2?" Is it to your
      advantage or disadvantage to switch your choice, or does it
      matter whether you switch your choice or not? . . . . . . . 64

2.15 Imagine we have a game with two doors: Behind one door
      is a car; behind the other is a goat. You pick a door, say No.
      1 (but the door is not opened), and the host, who knows what's
      behind the doors, says that there is a 90% chance that the car
      is behind door No. 2. Is it to your advantage to switch your
      choice? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65

2.16 The host, who knows what's behind the doors, points to a door,
      choosing the correct door 90% of the time and the incorrect
      one 10%. You pick a door, say No. 1, and the host points to
      door No. 2. Is it to your advantage to switch your choice? . 65

2.17 Suppose you're on a game show, and you're given the choice
      of three doors: Behind one door is a car; behind the others, goats.
      You pick a door, say No. 1 (but the door is not opened), and
      the host, who knows what's behind the doors, says that an-
      other door, say No. 3, has a 0% chance of having a car, and that
      the remaining door (that you haven't chosen - i.e door No. 2)
      has a 66% of having the car. He then says to you, "Do you want
      to pick door No. 2?" Is it to your advantage to switch your choice?
         . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65

2.18 Suppose you're on a game show, and you're given the choice
      of three doors: behind one door is a car; behind the others, goats.
      You pick a door, say No. 1 (but the door is not opened), and
      the host, who knows what's behind the doors, opens another
      door, say No. 3, which has a goat. He then says to you, "Do
      you want to change your choice to door No. 2?" Is it to your
      advantage or disadvantage to switch your choice, or does it
      matter whether you switch your choice or not? . . . . . . . . 66

2.19 Beard and Mustache - An Examination of Independence . . 70
3.1 What is the probability of flipping three heads in a row, with

      a fair coin? . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
3.2 What is the probability of flipping thirty heads in a row, with

      a fair coin? . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
                                                                                                                                          25

3.3 What is the probability of flipping two heads in three flips,
      with a fair coin? . . . . . . . . . . . . . . . . . . . . . . . . . 76

3.4 What is the probability of flipping ten heads in thirty flips,
      with a fair coin? . . . . . . . . . . . . . . . . . . . . . . . . . 77

3.5 How many ways can we rearrange the unique symbols A, B,
      C, and D? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78

3.6 How many ways can we rearrange the symbols A, A, A, and
      D? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78

3.7 How many ways are there of rearranging the symbols "A A
      A D D"? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79

3.8 What is the probability of flipping ten heads in thirty flips,
      with a fair coin? . . . . . . . . . . . . . . . . . . . . . . . . . 80

3.9 What is the probability of getting 17 or more heads in 30 flips? 81
4.1 What is the probability of drawing a 9, given that we know

      that we're holding the High Deck? . . . . . . . . . . . . . . 97
4.2 What is the probability that you are holding one of either the

      High or the Low Deck having drawn five 9's in a row from that
      deck? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
4.3 What is the probability that you are holding one of either the
      High or the Low Deck having drawn m 9's in a row from that
      deck, where m stands for a number (m = 1, 2, 3, · · ·)? . . . . 103
4.4 What is the probability that you are holding one of either the
      High, Low, or Nines Deck having drawn m 9's in a row from
      that deck? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
5.1 Is it better to switch doors? - Monty Hall Problem revisited 117
6.1 What is the best estimate of the probability of a bent coin flip-
      ping heads, given the observation of 9 tails and 3 heads? . . 132
7.1 Given a Normal distribution with a mean of µ = 150 and a
       = 20, what is the most likely value? . . . . . . . . . . . . . 143
7.2 Given a Normal distribution with a mean of µ = 150 and  =
      30, what is the probability P(x > 170) . . . . . . . . . . . . . 143
7.3 We have two Normal distributions P(x) = Normal(µ = 8,  =
      2) and P(y) = Normal(µ = 20,  = 7). What is the distri-
      bution for z = y - x? . . . . . . . . . . . . . . . . . . . . . . 145
7.4 Estimating the True Length of an Object . . . . . . . . . . . . 147
7.5 Estimating the True Length of an Object...Again . . . . . . . 148
7.6 Estimating the True Length of an Object...Yet Again . . . . . 153
9.1 Iris petal lengths - Best estimate . . . . . . . . . . . . . . . . 165
9.2 Iris petal lengths - A different species? . . . . . . . . . . . . . 166
9.3 Iris petal lengths - Significantly different? . . . . . . . . . . . 166
9.4 Ball Bearing Sizes . . . . . . . . . . . . . . . . . . . . . . . . . 168
9.5 What is the best estimate (and uncertainty) for each of the two
      production lines of ball bearings? . . . . . . . . . . . . . . . . 169
26

9.6 Is it reasonable to believe that there is a difference in the size
      produced between the two lines? . . . . . . . . . . . . . . . . 170

9.7 The Sunrise Problem . . . . . . . . . . . . . . . . . . . . . . . 170
9.8 Cancer Rates . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
9.9 Cancer Rates - Normal Approximation . . . . . . . . . . . . 171
9.10 Will it rain on the 4th of July? . . . . . . . . . . . . . . . . . . 172
9.11 Hot Hand Reexamined . . . . . . . . . . . . . . . . . . . . . . 172
9.12 Mass of the Penny, Model 1 - One True Value . . . . . . . . . 173
9.13 Mass of the Penny, Model 1 - One True Value with More Data176
9.14 Mass of the Penny, Model 2 - Two True Values . . . . . . . . 179
C.1 You call a plumber, and they say that they can come anytime

      in the next 4 hours. The probability of them arriving at any
      particular time can be represented with a uniform distribu-
      tion. What is the probability that they arrive in the first 20 min-
      utes of the second hour? . . . . . . . . . . . . . . . . . . . . 230
D.1 Usage of the Credible Interval Table for the Normal Distribu-
      tion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
D.2 Usage of the Credible Interval Table for the Student's t Dis-
      tribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
List of Tables

1.1 Rough guide for the conversion of qualitative labels to probability
      values. 51

3.1 Total Correct Guesses from Students "Predicting" the Results of 50
      Coin Flips. Shown are the results of a first round and a second round
      of guessing. 86

3.2 Performance in the Second Round of Students "Predicting" the Re-
      sults of 50 Coin Flips. Shown are the results for those students who
      performed best in the first round (left), and those that performed worst
      in the first round (right). 86

3.3 106 Male Student Heights (in cm) from a Survey. 88

4.1 Drawing m 9's in a row, from either a High Deck or Low Deck. 104

6.1 Probabilities for flipping heads given a collection of bent coins 121
6.2 Probability for different bent-coin models, given the data=9 tails, 3

      heads. The middle column is the non-normalized value from Bayes'
      Rule, needing to be divided by K (the sum of the middle column)
      to get the final column which is the actual probability. 123

8.1 Rough guide for the conversion of deviations away from zero and
      the qualitative labels for probability values for being a significant de-
      viation. 161

9.1 Iris petal lengths, in centimeters, for Iris type Setosa. 165
9.2 Subset of iris petal lengths, in centimeters, for iris types Virginica, Se-

      tosa, and Versicolor. 166
9.3 Production lines are produce a ball bearing with a diameter of ap-

      proximately 1 micron. Ten ball bearings were randomly picked from
      the production line (i.e. the First line) at one time, and then again for
      a different production line (i.e. the Second line). Romano, A. (1977)
      Applied Statistics for Science and Industry. 169
9.4 Mass of Pennies from 1960 to 1974. 174
9.5 Mass of Pennies from 1989 to 2003. 176
28

10.1 Heights (in inches) and shoe sizes from a subset of McLaren (2012)
      data. 193
Proposal

I would like to propose a new introductory statistical inference text-
book, which I believe takes a fresh look at a course that fits into
nearly every quantitative major at universities.

Initial Motivation

My motivation for this project stems from my dissatisfaction with tra-
ditional approaches to the topic, and my belief that there is a better
way. A first semester statistics course is generally divided into the
following four parts:

I Basic Statistical Concepts
   · Basic statistical concepts including population, parameter, sam-
       ple, and statistic
   · Types of data (ordinal, time-series, etc...), and sampling method-
       ology
   · Organizing the data visually or graphically - including his-
       tograms, pie graphs, box plots, and stem-and-leaf plots
   · Statistical computations including mean, median, mode, stan-
       dard deviation, and percentiles

II Probability

   · Properties of unions, intersections, conditional probability,
       independence and mutual exclusivity

   · Permutations and combinations
   · Discrete distributions
   · Continuous distributions
   · Normal distribution

III One-sample Statistics

   · Confidence intervals
30

   · Sampling distributions
   · Computations involving the normal distribution, t-distribution,

       and binomial distribution (for proportions)
   · Hypothesis testing

IV Two-sample Statistics

   · Two sample problems - expanding topics from Part III to two
       variables

   Obviously, there is some variability to these topics, but as one can
see from most introductory statistics textbooks, there is a consistent
approach. My main concerns about the traditional approach can be
summarized as follows:

1 Part II (probability) generally covers at least one quarter of the ma-
   terial in an introductory statistics course. There is a shift from data
   collection and analysis (Part I) to probability theory. Subsequently,
   Part III shifts back to a data centered approach and only a small
   portion of Part II generally applies in Part III. This disconnect be-
   tween Parts I, II, and III, impedes the learning process. It seems to
   the students as if the parts are related somehow, but the connec-
   tion is rarely made. The students are then left with a feeling that
   the course concerns two completely unrelated topics: probability
   and statistics.

2 The normal distribution is covered repetitively throughout many
   chapters of most introductory statistics books. The coverage is
   included in sections such as: empirical bell-shaped curve (Part
   I), normal distribution as a type of continuous distribution (Part
   II), sampling distributions (Part III), interval estimation (Part III),
   hypothesis testing (Part III), and two population testing (Part
   IV). There is redundant focus on the normal and t-distributions.
   These topics are closely related, but not handled cohesively. More
   importantly, there is little or no discussion of the assumptions of
   the normal model or how to tell what constitutes "close enough"
   to normal. In addition, there is generally equal consideration given
   to the rare practical situation in which the standard deviation is
   known (and knowing this does not generally alter the result much
   at all).

3 After the concept of a "statistic" is covered, there are many chap-
   ters which repeat essentially the same problem multiple times,
   from only slightly different perspectives. This gives the student a
   feeling that these are all very different problems, despite the ap-
   pearances, and leads the student to approach solving problems
                                                                           31

   like a "cookbook": just find the right recipe for the right problem.    1 E. T. Jaynes. Probability Theory:
   The fundamental understanding of statistical inference is under-        The Logic of Science. Cambridge
   mined by this approach.                                                 University Press, Cambridge, 2003.
                                                                           Edited by G. Larry Bretthorst
   It is my view that the traditional approach detracts from student
understanding, with its "cookbook" perspective, disjointed cover-          2 One reason why "Probability
age of probability, and the almost exclusionary focus on the normal        theory as Logic" concepts are
distribution.                                                              covered only in advanced courses
                                                                           is the misperception that they are
A New Approach                                                             applicable only to more advanced
                                                                           problems, and not applicable to
In the field of statistical inference, there are two primary schools of    problems normally found in an
thought. Each has its proponents, but it is generally accepted that on     introductory class. The fact that
all problems covered in an introductory course, that both approaches       this misperception exists is a strong
are valid and lead to the same numerical values when applied to            argument for a book like this one,
actual problems. Only one of these approaches is covered in a tra-         to dispel this misperception and to
ditional course, which denies the students access to an entire field       communicate both to students and
of statistical inference. The traditional approach, also called the fre-   instructors alike the value of a this
quentist or orthodox perspective, leads almost directly to problem (1)     approach to basic problems.
above. The other approach, also called Probability Theory as Logic1,
derives all statistical inference from probability theory directly. It is
this approach that I hope to expose students to in an introductory
course.

   The probability theory approach to statistical inference has several
benefits:

1 All of the same problems as handled traditionally can be handled
   with this perspective, yielding exactly the same answers2.

2 Statistical inference is theoretically grounded in probability theory,
   which, although admittedly beyond an introductory course, avoids
   the "cookbook" approach, where different problems need different
   methods, that students take away from the traditional textbooks.
   Here all problems use the same method, derived from probability
   theory.

3 The reasoning process using the probability theory perspective
   is more intuitive than the orthodox perspective, especially when
   dealing with hypothesis testing.
       For example, every statistics instructor faces the challenge of
   getting students to interpret p-values properly, and the logic be-
   hind setting up null-hypotheses. They have to combat the stu-
   dents' initial intuition that the p-value represents the "probability
   that the null is true," and many students never really obtain the
   proper understanding. I have even heard instructors use it this
   way.
32

       In the Probability Theory as Logic perspective, this same calcu-      3 One of the reasons why this ap-
   lated value is interpreted exactly like the students' initial intuition!  proach is usually covered only in
   Thus, testing hypotheses, estimating parameters, and determining          more advanced courses is the diffi-
   uncertainties are far more direct and intuitive using this approach       culty of the mathematics generally
   than the traditional approach.                                            associated with it. Orthodox statis-
                                                                             tics makes heavy use of sampling,
What I Am Proposing                                                          which is deemed more intuitive
                                                                             than probability distributions. It
This text can help solve the challenges described above, and more.           is my intention to start with low-
By focusing on models and data, as opposed to populations and                dimensional cases, building to
samples, this text can more cohesively bridge the topics described in        distributions, and to augment all
Parts I, II, and III above. Probability will be introduced as a natural      concepts with numerical exercises.
part of solving problems, as opposed to its standalone treatment
traditionally done in today's texts.

   In this text, I will use the Probability Theory as Logic approach
applied to the same problems that are traditionally covered. This
viewpoint can greatly enhance our understanding of statistics and
can handle topics such as confidence intervals and hypothesis testing
in a very intuitive manner. Statistical inference covered in this way
also addresses real-life questions that are not addressed by traditional
statistical methods.3

   Finally, this will be a problem oriented textbook. It is imperative
that the problems are cohesive with the pedagogy. I will also plan to
use technology, where appropriate, to further student learning and
make the textbook more interactive.

   At the level targeted for this book, there is only one textbook that
I know of that covers inference from the perspective proposed here,
and that is Donald Berry's book Statistics: A Bayesian Perspective,
1996. It is my intention to modernize the approach, and include some
topics that are not covered, specifically from the physical sciences
and business.
1 Introduction to Probability

    Life's most important questions are, for the most part, nothing but probability  1 J. Sullivan. People v. Collins ,
    problems. - Laplace                                                              68 cal.2d 319, 1968. URL http:

   In 1968 a jury found defendant Malcolm Ricardo Collins and his                    //scocal.stanford.edu/opinion/
wife defendant Janet Louise Collins guilty of second degree robbery1.                people- v- collins- 22583
The decision hinged on the testimony of bystanders, which stated
that the perpetrators had been "black male, with a beard and mous-                   2 Lord Justice Kay. R vs Sally
tache, and a caucasian female with blonde hair tied in a ponytail,"                  Clark, April 2003. URL http:
and that they escaped in a "yellow motor car." A mathematician
testified that the odds against this couple being innocent were one                  //www.bailii.org/ew/cases/EWCA/
in twelve million, and this was enough for the jury to convict. Later,               Crim/2003/1020.html
in an appeal, the California Supreme Court reversed the decision
primarily because of lack of evidence, and faulty inference.

   In another case, Sally Clark was convicted in 1999 of the murder
of her two young sons2. Again, the testimony hinged on a statistical
argument - the chances of one baby dying in their bed 1 in 8500, so
therefore the chances of two of them dying in the same way is the
square of this, or 1 in 73 million. Several years later, and a public
statement from the Royal Statistical Society highlighting the erro-
neous logic, Sally Clark was released - although she never overcame
the resulting damage to her life that the conviction had caused.

   We will cover these cases in more detail later, and why the in-
ference was faulty, but I introduce the stories here for two reasons.
First, is to point out that there are cases in which proper statistical
inference can be a life and death matter. Second, it is to highlight the
fact that such inference can run counter to one's intuition. Part of the
purpose of this book is to retrain your intuitions and your habits of
intuition to avoid such failures.

   We have to make decisions nearly every second of our lives, and
those decisions are based on our state of knowledge. Unfortunately,
we are never 100% sure of any information in our lives, so we are
constantly forced to make decisions in the face of uncertainty. In
many cases our common sense is enough to make sophisticated deci-
sions, taking into account the uncertain nature of the situation. How-
ever, there are many times where our common sense is not enough to
34 statistical inference for everyone

quantitatively resolve the level of uncertainty, and make valid infer-     3 L Heaps. Operation morning
ences. It is in these cases that statistical inference is most useful.     light. Paddington, S.l, 1978. ISBN
                                                                           0709203233
   Statistical inference refers to a field of study where we try to infer
unknown properties of the world, given our observed data, in the           4 James Oberg. U.S. satellite shoot-
face of uncertainty. It is a mathematical framework to quantify what       down: The inside story. IEEE
our common sense says in many situations, but allows us to exceed          Spectrum, 2008
our common sense in cases where common sense is not enough. Ig-
norance of proper statistical inference leads to poor decisions and
wasted money. As with ignorance in any other field, ignorance of sta-
tistical inference can also allow others to manipulate you, convincing
you of the truth of something that is false.

   For example, in 1978 a Russian satellite deviated from its orbit and
became increasingly erratic, and was going to crash into the Earth.3
This sort of event occurs from time to time, even including a recent
crash of a US spy satellite in 2008.4 There was a local news broadcast
about the impending Russian satellite crash which said something
like, "the scientists had studied the trajectory of the satellite, and
determined that there was only a 25% chance of it striking land, and
even a much smaller chance striking a populated area." The report
was clearly designed to calm the public, and convince them that
the scientists had a good handle on the situation. Unfortunately,
given a little thought, one realizes that the Earth's surface consists
of about 25% land and 75% water, so if you knew nothing about the
trajectory of the satellite, you would simply state that it had a 25%
chance of striking land. Instead of communicating knowledge of the
situation, the news broadcast communicated (to those who knew
basic statistical inference) that either the scientists were in complete
ignorance of the trajectory or the reporter had misinterpreted a casual
statement about probabilities and didn't realize what was implied.
Either way, the intent of the message and the content of the message
(to those who understood basic probability) were in direct conflict.

1.1 Models and Data

There are two main aspects of statistical inference: description of
data and model analysis. In the description of data, one attempts to
summarize a set of data with a smaller set of numbers. Grades in
the classroom are summarized by the average, votes in a state are
summarized by a percentage, etc... This smaller description of the
data is useful for both practical and theoretical reasons. It is more
expedient to communicate a small set of numbers than the entire data
set, and it is almost always the case that the detailed properties of a
set of data are not relevant to the questions that you are asking.

   A model refers to a mathematical structure which is used to ap-
                                                                                          introduction to probability 35

proximate the underlying causes of the data, and unify seemingly
unrelated problems. One may have a (mathematical) model for a
coin flip which ignores all of the details of the flip, the bounce, and
the catch and summarizes the possible results by a single number:
the chance that the coin will come up heads. You may then use that
same model to describe the voting behavior of citizens during a pres-
idential election, or to describe the radioactive decay of particles in a
physics experiment. The mathematics is identical, but the interpreta-
tion of the components of the model will be different depending on
the problem. Models simplify, by summarizing data with a small set
of causes, and they are used for inference, allowing one to predict the
outcome of subsequent events.

   The goal of statistical inference is then to take data, and update
our knowledge about various possible models that can describe the
data. This often means deciding which of several models is the most
likely. It can also entail the refinement of a single model, given the
new data. All of these activities are closely related to (and perhaps
identical to) the methods in science. What we are trying to do is
make the best inferences from the data, improve our inferences as
new data come in, and plan what data would be the most useful to
improve our inferences. In a nutshell, the approach is:

           Initial Inference + New Data  Improved Inference

   In order to deal with a wide variety of problems, we require a
minimal amount of mathematical structure and notation, which we
introduce in this chapter.

1.2 What is Probability?

    Probability theory is nothing but common sense reduced to calculation. -
    Laplace

   When you think about probability, the first things that might come
to mind are coin flips ("there's a 50-50 chance of landing heads"),
weather reports ("there's a 20% chance of rain today"), and politi-
cal polls ("the incumbent candidate is leading the challenger 53% to
47%"). When we speak about probability, we speak about a percent-
age chance (0%-100%) for something to happen, although we often
write the percentage as a decimal number, between 0 and 1. If the
probability of an event is 0 then it is the same as saying that you are
certain that the event will never happen. If the probability is 1 then you
are certain that it will happen. Life is full of uncertainty, so we assign a
number somewhere between 0 and 1 to describe our state of knowl-
edge of the certainty of an event. The probability that you will get
36 statistical inference for everyone

struck by lightning sometime in your life is p = 0.0002, or 1 out of    In this book, our approach is to
5000. Statistical inference is simply the inference in the presence of  determine, for each problem, what
uncertainty. We try to make the best decisions we can, given incom-     degree of confidence we have in
plete information.                                                      all of the possible outcomes. The
                                                                        approach of statistical inference
   One can think of probability as a mathematical short-hand for the    covered in this book is about the
common sense statements we make in the presence of uncertainty.         procedure of most rationally assign-
This short-hand, however, becomes a very powerful tool when our         ing various degrees of confidence
common sense is not up to the task of handling the complexity of a      (which we call probability) to the
problem. Thus, we will start with examples that will perhaps seem       possible outcomes of some process
simple and obvious, and move to examples where it would be a            using all the objectively available
challenge for you to determine the answer without the power of          data.
statistical inference.
                                                                        Figure 1.1: Standard 52-card deck.
   Let's walk through a simple set of examples to establish the nota-   13 cards of each suit, labeled
tion, and some of the basic mathematical properties of probabilities.   Spades, Clubs, Diamonds, Hearts.

Card Game

A simple game can be used to explore all of the facets of probability.
We use a standard set of cards (Figure 1.1) as the starting point, and
use this system to set up the intuition, as well as the mathematical
notation and structure for approaching probability problems.

We start with what I simply call the simple card game5, which goes      5 In this description of the game,
                                                                        we do not reshuffle after each draw.
                                                                        The differences between this non-
                                                                        reshuffled version and the one with
                                                                        reshuffling will be explored later,
                                                                        but will only change some small
                                                                        details in the outcomes.
                     introduction to probability 37

like:                                                                       Principle of Knowledge and
                                                                            Probability Equivalent states of
                                   From a standard initially shuffled       knowledge must yield equivalent
                                   deck, we draw one card, note what        probability assignments.
                                   card it is and set it aside. We then

   simple card game  draw another card, note what card (1.1)
                                   it is and set it aside. Continue until 
                                   there are no more cards, noting each
                                   
                                      one along the way.

   There are certain principles that guide us in developing the math-
ematical structure of probability. We start with some common sense
notions, written in English, and then write them as general princi-
ples. These principles, then, constrain our mathematics so that we
can apply the ideas quantitatively.

   When asked "what is the probability of drawing a red on the
first draw?" you would generally say 50-50, or 50%, or equivalently
written as a probability, P(R1) = 0.5. The reason for this is that
we are completely ignorant of the initial conditions of the deck (i.e.
where each card is located in the deck after the initial shuffling).
Given this level of (or lack of) knowledge, we could swap the colors
of the two suits and we would have an equivalent state of knowledge
- the problem would be identical. We will keep coming back to this
concept, but in general:

   Principle of Knowledge and Probability Equivalent states of
knowledge must yield equivalent probability assignments.

   Because of this principle, we are led to the conclusion that

                                P(R1) = P(B1)

where R1 represents the statement "a red on the first draw" and B1
represents "a black on the first draw." Because these are the only two
options, and they are mutually exclusive, then they must add up to 1.
Thus we have

                              P(R1) = 1 - P(B1)

   which leads directly to our original assignment

P(R1) = P(B1) = 0.5

   Mutually Exclusive If I have a list of mutually exclusive events, then   Mutually Exclusive If I have a
that means that only one of them could possibly be true. Example            list of mutually exclusive events,
events include flipping heads or tails with a coins, rolling a 1, 2,        then that means that only one
3, 4, 5 or 6 on dice, or drawing a red or black card from a deck of         of them could possibly be true.
                                                                            Examples includes the heads
                                                                            and tails outcomes of coins, or
                                                                            the values of standard 6-sided
                                                                            dice. In terms of probability, this
                                                                            means that, for events A and B,
                                                                            P(A and B) = 0.
38 statistical inference for everyone

cards. In terms of probability, this means that, for events A and B,      Non Mutually Exclusive If I have
P(A and B) = 0.                                                           a list of events that are not mutually
                                                                          exclusive, then it is possible for
   Non Mutually Exclusive If I have a list of events that are not mutu-   two or more to be true. Examples
ally exclusive, then it is possible for two or more to be true. Examples  include weather with rain and
include weather with rain and clouds or holding the high and the          clouds or holding the high and the
low card in a poker game.                                                 low card in a poker game.

   Now, this was a long-winded way to get to the answer we knew
from the start, but that is how it must begin. We start working things
out where our common sense is strong, so that we know we are
proceeding correctly. We can then, confidently, apply the tools in
places where our common sense is not strong.

   In summary, with no more information than that there are two
mutually exclusive possibilities, we assign equal probability to both.
If there are only two colors of cards in equal amounts, red and black,
then the probability of drawing a red is P(R1) = 0.5 and the probabil-
ity for a black is the same, P(B1) = 0.5.

Other Observations

If instead of just the color, we were interested in the suit (hearts,
diamonds, spades, and clubs), then there would be four equal and
mutually exclusive possibilities. We have a certain number of possi-
bilities, and our state of knowledge is exactly the same if we simply
swap around the labels on the cards. If we're interested in the specific
card, not just the suit, the logic is the same. Thus, we have

                      P() = P() = P() = P()
and for drawing one specific card from the deck,

               P(A) = P(2) = P(3) = · · · = P(K)
   Further, they all must add up to 1, so we get for suits

                    P() + P() + P() + P() = 1
and for the specific card from the deck,

             P(A) + P(2) + P(3) + · · · + P(K) = 1
                                   52 cards

   Putting it together, we get for the suits
                   P() = P() = P() = P() = 1
                                                                  4
                                               introduction to probability 39

and for the specific card                                                    Probabilities for Mutually Exclu-
                                                                             sive Events
           P(A) = P(2) = P(3) = · · · = P(K) = 1
                                                                         52  P(A) = (number of cases favorable to A)
                                                                                        (total number of equally possible cases)
   Probabilities for Mutually Exclusive Events In general, for mutu-
ally exclusive events, we have

           P(A) = (number of cases favorable to A) (1.2)
                       (total number of equally possible cases)

1.3 Conditional Probability

It is important to understand that probability reflects our state of
knowledge about the system. As our knowledge changes, so do our
probability assignments. As we gain more information, we change
our probability assignments. Two people observing the same system,
but with different information about the system, will give different
probability assignments. All we need to make sure probability theory
matches our common sense is for two people with the same state of
knowledge, or the same information, to yield identical probability
assignments.

   Because our information about a system is so important in assign-
ing probabilities, we introduce a way of writing it mathematically
that we will use for the rest of the book. It will be good for the reader
to get used to reading the mathematical short-hand in English in
order to gain an understanding for what it means.

Probability Notation

In math, we choose to abbreviate long sentences in English, in order
to use the economy of symbols. In this book we choose a middle-
ground between mathematical succinctness and the ease of under-
standing English. We start with the simple card game (Equation 1.1)

   We then define a new symbol, |, which should be read as "given."
When there is information given we call this probability conditional
on that information. When we write the following:

    P(red on first draw|simple card game)      (1.3)

or

                       P(R1|simple card game)  (1.4)

    this is short for
40 statistical inference for everyone

    "The probability of drawing a red on the first draw, given that we have a       Conditional Probability When
    standard initially shuffled deck and we follow the procedure where we draw      information is given, and expressed
    one card, note what color it is and set it aside and continue drawing, noting,  on the right-hand side of the |
    and setting aside until there are no more cards."                               sign, we say that the probability
                                                                                    is conditional. P(I'm going to get
   One can easily see that the mathematical notation is far more                    wet today|raining outside) is an
efficient. It is important to be able to read the notation, because it              assessment of how likely it is that
describes what we know and what we want to know.                                    I will get wet given, or conditional
                                                                                    on, the fact that it is raining outside.
   Conditional Probability When information is given, and ex-                       Clearly this number will be differ-
pressed on the right-hand side of the | sign, we say that the proba-                ent if it was conditional on the fact
bility is conditional. P (I'm going to get wet today|raining outside) is            that it is sunny outside.
an assessment of how likely it is that I will get wet given, or condi-
tional on, the fact that it is raining outside. Clearly this number will            Causation. Imagine we have a 2-card
be different if it was conditional on the fact that it is sunny outside -           game: a small deck with one red
different states of knowledge yield different probability assignments.              card and one black card, and I draw
                                                                                    a red card first. Clearly this makes
   When we put a comma (",") on the right side then we read this as                 the probability of drawing red as
"and we know that." For example, when we write the following:                       the second card equal to zero - it
                                                                                    can't happen. We're tempted to
    P(red on second draw|simple card game,red on first draw) (1.5)                  interpret

or                                                                                    P(R2|R1, 2-card game) = 0

                         P(R2|simple card game, R1)  (1.6)                          to mean that because we drew a red
                                                                                    on the first draw, this causes the
    this is short for                                                               impossibility of drawing the red on the
                                                                                    second - there is only 1 red card after
    "The probability of drawing a red on the second draw, given that we have a      all, and drawing it seems to cause
    standard initially shuffled deck and we follow the procedure where we draw      the impossibility of drawing red in
    one card, note what color it is and set it aside and continue drawing, noting,  the future. However, consider the
    and setting aside until there are no more cards and we know that we drew a      following:
    red on the first draw."
                                                                                      P(R1|R2, 2-card game) = 0
1.4 Rules of Probability
                                                                                    which is, if we knew that the second
From the rule for mutually exclusive events (Equation 1.2), we assign               card we drew was red, then it
the following probabilities for the first draw from this deck6:                     makes it impossible to have drawn
                                                                                    a red card as the first card. This is
·   P(10)  =  4                                                                     just as true as the previous case,
              52                                                                    however, you can't interpret this as
                                                                                    causation - the second draw didn't
· P() = 1352 = 14                                                                   cause the first draw.

·   P(10)     =   1                                                                   Instead, probability statements are
                  52                                                                statements of logic, not causation. One
                                                                                    can use probabilities to describe
·   P(face  card)     =  12                                                         causation (i.e. P (rain|clouds)), but
                         52                                                         the statements of probability have
                                                                                    no time component - later draws
·   P(number      card)  =   40                                                     from the deck of cards act exactly
                             52                                                     the same as earlier ones.

                                                                                    6 A face card is defined to be a Jack,
                                                                                    Queen, or King. A number card is
                                                                                    defined to be Ace (i.e. 1) through
                                                                                    10.
                                                                 introduction to probability 41

   It turns out that mathematically, the rules for fractions of things
and of probabilities are the same. Thus, to gain an understanding
for the rules of probability, we will calculate fractions (which are
more immediately intuitive), and then summarize the same rule for
probabilities.

Negation Rule                                                                                Either-or fallacy. The negation
                                                                                             rule, should not be taken to imply
In this section I'll use the letter F for fraction, and we can determine                     that everything is "black and
the values simply by counting. The fraction of cards which are hearts                        white," or "there are only two
() is                                                                                        sides to every story." It really is
                                                                                             just a statement of logic, should
                                F() = 13 = 1                                                 be carefully considered and has
                                             52 4                                            some limitations. For example, the
                                                                                             following is true,
The fraction of cards which are not hearts (i.e. the 3 other suits) is:                      P (object is black) + P (object is not black) = 1
                           F(not ) = 13 × 3 = 3                                              However, this does not mean the
                                               52 4                                          same thing as
                                                                                             P (object is black) + P (object is white) = 1
These numbers add up to one: F() + F(not ) = 1. We can do this                               "Not black" is not the equivalent of
with more complex statements.                                                                "white." It could be red, or gray, or
                                                                                             some other color. A common logical
                                           F (first card is a face card) = 12                fallacy sometimes referred to as the
                                                                                         52  "either-or fallacy" or the "fallacy
                                                                                             of the excluded middle," turns on
                                      F (first card is not a face card) = 40                 this point. Some examples of these
                                                                                         52  fallacies are:
                                                                                             · If we don't reduce public spend-
 F (first card is a face card) + F (first card is not a face card) = 1
                                                                                                 ing, our economy will collapse.
Example 1.1 What is the fraction of the first card as a jack given that we                   · You're either with us or you're a
know that the first card is a face card?
                                                                                                 terrorist.
   We can also apply the negation rule to conditional statements, like                       · Either modern medicine can
"the first card is a jack given that we know that the first card is a face
card." Notice that there are 12 cards that are face cards, so we restrict                        explain how Ms. X was cured, or
our counts to those.                                                                             it is a miracle.

                                        F (jack|face card) = 4 = 1/3                         Negation Rule
                                                                          12                    P(A|B) + P(not A|B) = 1

                                 F (not a jack|face card) = 8 = 2/3
                                                                          12

       F (jack|face card) + F (not a jack|face card) = 1

and they add up to one.

    Negation Rule Given any information, we have

    P(statement|information) + P(not statement|information) = 1

or

    P(A|B) + P(not A|B) = 1                                      (1.7)
42 statistical inference for everyone

Product Rule

The product rule comes from looking at the combination of events:
event A and event B. As before, we'll work on the numbers from the
fractions of the card game.

Example 1.2 What is the fraction of cards that are Jacks and a heart?

   This is clearly F (J) = 1/52, but we can look at it a different way
that is equivalent. We note that the Jacks constitute 4/52 of the cards,
and that of those 4, only one quarter of them are hearts (one card out
of the four cards). So, we can arrive at the fraction of J by taking
one quarter of the fraction of jacks. So what we have is

       F (jack and ) = F (|jack) × F (jack) = 1 × 4 = 1
                                                               4 52 52

One can equivalently reason from the suit first: the hearts constitute
13/52 of the cards, and that of those 13, the Jacks constitute 1/13
of the cards. So, we can arrive at the fraction of J by taking one
thirteenth of the fraction of . Again, we have

       F (jack and ) = F (jack|) × F () = 1 × 13 = 1                          Product Rule
                                                             13 52 52
                                                                                 P(A and B) = P(A|B)P(B)
In general we have                                                                                 = P(B|A)P(A)
   Product Rule

P(A and B) = P(A|B)P(B) = P(B|A)P(A)                                   (1.8)

Example 1.3 What is the probability of drawing two Kings in a row?

This is the same as

                     P(K2 and K1)

From the product rule (Equation 1.8) we have

P(K2 and K1) = P(K2|K1)P(K1)

The second part is straight forward: P(K1) = 4/52. The first part is
asking the probability of drawing a second king, knowing that we
have drawn a king on the first draw. Now, there are only 51 cards
remaining when we do the second draw, and only 3 kings. Thus, we
have P(K2|K1) = 3/51 and finally

                      P(K2 and K1) = P(K2|K1)P(K1)
                                           = 3×4= 1
                                                  51 52 221
                                                                     introduction to probability 43

Independence

As a specific case of the product rule, we can change the rule of the
card games such that we reshuffle the deck after each draw. In this
way, the result of one draw gives you no information about other
draws. In this case, the events are considered independent.

   Independent Events Two events, A and B, are said to be inde-             Independent Events Two events,
pendent if knowledge of one gives you no information on the other.          A and B, are said to be indepen-
Mathematically, this means                                                  dent if knowledge of one gives
                                                                            you no information on the other.
                                P(A|B) = P(A)                               Mathematically, this means

and                                                                                  P(A|B) = P(A)
                                                                            and
                                P(B|A) = P(B)
                                                                                      P(B|A) = P(B)
   In this case, the product rule reduces to the simplified rule for
independent events: the product of the individual event probabilities.      Joint Probabilities for Independent
                                                                            Events
Joint Probabilities for Independent Events
                                                                             P (A and B) = P(A) × P(B)
P (A and B) = P(A) × P(B)                                            (1.9)

   We have already seen an example of this, when we looked at
drawing the Jack of Hearts: drawing a heart gives you no informa-
tion about whether it is a jack, and vice versa. Thus,

P (|jack) = P ()

Example 1.4 What is the probability of flipping two heads in a row?

   The probability of getting "heads" on any given coin flip is P(H) =
0.5. The probability of flipping two heads in a row is then simply
P(H1) × P(H2) = 0.5 × 0.5 = 0.25, because the second flip is in-
dependent of the first. If it wasn't, then you'd have to determine
how the knowledge of the first flip influences our knowledge of the
second flip, which is written as P(H2|H1) and the full product rule
(Equation 1.8) would need to be used.

Conjunction

One of the consequences of combinations of events is that the prob-
ability of two events happening, A and B, has to be less than (or
possibly equal to) the probability of just one of them, say A, happen-
ing. The mathematical fact is seen by looking at the magnitude of the
44 statistical inference for everyone

terms in the product rule                                                     Combinations of Events and the
                                                                              English language I believe that the
                 P (A and B) = P(B|A) ×P(A)  P(A)                             issue of the conjunction fallacy is
                                                                              more subtle than this. In English,
                                      less                                    if I were to say "Do you want steak
                                      than or                                 for dinner, or steak and potatoes?"
                                      equal to                                one would immediately parse this
                                      1                                       as choice between

   In other words, coincidences are less likely than either event hap-        1 steak with no potatoes
pening individually. We intuitively know this, when we make com-
ments like "Wow! What are the chances of that?" referring to, say,            2 steak with potatoes
someone winning the lottery and then getting struck by a car the
next day. Sometimes, however, it seems as if one's intuition does not         Although strict logic would parse
match the conclusions of the rules of probability. One such case is           this choice as
called the conjunction fallacy.
                                                                              1 steak, possibly with potatoes
   In an interesting experiment, Tversky and Kahneman[Tversky and                 and possibly without potatoes
Kahneman, 1983] gave the following survey:
                                                                              2 steak, definitely with potatoes,
    Linda is 31 years old, single, outspoken, and very bright. She majored
    in philosophy. As a student, she was deeply concerned with issues of      it is common in English to have
    discrimination and social justice, and also participated in anti-nuclear  the implied negative (i.e. steak
    demonstrations.                                                           with no potatoes) when given
    Which is more probable?                                                   a choice where the alternative
                                                                              is a conjunction (i.e. steak with
    1 Linda is a bank teller.                                                 potatoes).
    2 Linda is a bank teller and is active in the feminist movement.
                                                                              Combinations of Events and the
85% chose option 2.[Tversky and Kahneman, 1974] This, they at-                English language If we interpret the
tributed, to the conjunction fallacy - mistaking the conjunction of two       doctor's choice with this implied
events as more probable than a single event. They went further and            negative, we have:
did a survey of medical internists with the following
                                                                              1 clot with paralysis and no
    Which is more likely: the victim of an embolism (clot in the lung) will       shortness of breath
    experience partial paralysis or that the victim will experience both
    partial paralysis and shortness of breath?                                2 clot with paralysis and shortness
                                                                                  of breath
   and again, 91 percent of the doctors chose that the clot was less
likely to cause the rare paralysis rather than to cause the combination       and the first one is much less likely,
of the rare paralysis and the common shortness of breath.                     because it would be odd to have a
                                                                              clot and not have a very common
   Even when correct, the consequence for conjunctions can be mis-            symptom associated with it. The
used, or at least misidentified. Returning to our example of someone          doctor's probability assessment is
winning the lottery and then getting struck by a car the next day, rare       absolutely correct: both symptoms
events occur frequently, as long as you have enough events. There are         together are more likely than just
millions of people each day playing the lottery, and millions getting         one. The "fallacy" arises because
struck by cars each day. We will explore this problem later in Sec-           the English language is sloppier
tion 2.5, but one immediate consequence is that winning the lottery           than mathematical language.
and getting struck by a car the next day probably happens somewhere
fairly regularly.
                                                                               introduction to probability 45

Sum Rule

Now we consider the statements of the form A or B. For example,

in the card game, what is the fraction of cards that are jacks or are

hearts. By counting we get the 13 hearts and 3 more jacks that are not

contained  in  the  13  hearts,  or  F (jack  or  )  =  13+3  =  16/52.  Now,  if
                                                          52

we tried to separate the terms, and do:

                     F (jack) + F () = 4 + 13 = 17
                                               52 52 52

then we get a number that is too big! It is too big because we've
double-counted the jack of hearts. Adjusting for this, by subtracting
one copy of this fraction, we get

F (jack) + F () - F (jack and ) = 4 + 13 - 1 = 16 = F (jack or )                   Sum Rule
                                                 52 52 52 52                       P(A or B) = P(A) + P(B) - P(A and B)

In general
   Sum Rule

               P(A or B) = P(A) + P(B) - P(A and B)                      (1.10)

   Sum Rule for Exclusive Events If two events are mutually exclusive              Sum Rule for Exclusive Events If
the sum rule reduces to                                                            two events are mutually exclusive the
                                                                                   sum rule reduces to
                        P(A or B) = P(A) + P(B)                          (1.11)
                                                                                      P(A or B) = P(A) + P(B)
because P(A and B) = 0 for such events.                                            because P(A and B) = 0 for such
   So the probability of rolling a 1 or a 2 on one die is 2/6.                     events.
   One more variant on the Sum Rule is where we have 3 propo-
                                                                                   Sum Rule for Three Events
sitions. It can be a bit tedious to write it all out, but the end result           P(A or B or C) = P(A) + P(B) + P(C) -
looks a lot like the original Sum Rule. All we do is break up the
terms in pieces, and then apply the Sum Rule to each piece.                                                     P(A and B) -
                                                                                                                P(B and C) -
P(A or B or C) = P(A or [B or C])                                                                               P(A and C) +
                      = P(A) + P(B or C) - P(A and [B or C])                                                    P(A and B and C)
                      = P(A) + P(B) + P(C) - P(B and C) -
                           P(A and B or A and C)
                      = P(A) + P(B) + P(C) - P(B and C) -
                           [P(A and B) + P(A and C)-
                            P(A and B and A and C)]

which leads finally to
   Sum Rule for Three Events

P(A or B or C) = P(A) + P(B) + P(C) -

                        P(A and B) - P(B and C) - P(A and C) +

                        P(A and B and C)                                 (1.12)
46 statistical inference for everyone

   In words, when you're looking for the sum of several events, we
add the probabilities (i.e. P(A) + P(B) + P(C)), then subtract the
double counting (i.e. P(A and B)) as before. Finally, we need to
add back in the triple count (i.e. P(A and B and C)) because it was
taken out too many times with the double count. The accounting
here can be somewhat prone to error, but the concepts are always the
same: when you add probabilities of events, say A and B, together
the term P(A) includes the probability of both P(A and B) and the
term P(B) includes the probability of both P(A and B), so you've
included that probability twice and need to subtract one of them to
balance the books. Likewise (although it is harder to show), the first
six terms in Equation 1.12 end up subtracting one too many copies of
P(A and B and C), and we need to add one in at the end.

Marginalization

Another consequence of the sum rule and the product rule is a pro-
cess called marginalization.

Example 1.5 Marginalization and Card Suit

Imagine we have a number of conditional statements, like:
                                P (jack|) = 1
                                                        13
                                P (jack|) = 1
                                                        13
                                P (jack|) = 1
                                                        13
                                P (jack|) = 1
                                                        13

but we are interested in just the probability of drawing a jack, regard-
less of the suit, or in our notation

                                     P (jack)

The marginalization procedure for this problem looks like:

                                   all possibilities
            P (jack) = P (jack|) × P () +

                              P (jack|) × P () +
                              P (jack|) × P () +
                              P (jack|) × P ()
                         = 1 ×1+ 1 ×1+ 1 ×1+ 1 ×1
                                13 4 13 4 13 4 13 4
                         =4
                                52
                                                                        introduction to probability 47

   Marginalization If we have a complete set of conditional state-           Marginalization If we have a com-
ments, like                                                                  plete set of conditional statements,
                                                                             like
                                        P( A| B1 )                           P(A|B1), P(A|B2), P(A|B3), P(A|B4), · · ·
                                        P( A| B2 )                           then the unconditional probability
                                        P( A| B3 )                           is found by marginalizing over all
                                        P( A| B4 )                           possible values of the conditional
                                                                             events, like
                                              ..
                                              .                                                        all possible Bs
                                                                             P(A) = P(A|B1)P(B1) + P(A|B2)P(B2) + · · ·
then the unconditional probability is found by marginalizing over all
possible values of the conditional events, like                              In the 1700's Reverend Bayes
                                                                             proved a special case of this rule,
                                        all possible Bs                      and rediscovered in the general
  P(A) = P(A|B1)P(B1) + P(A|B2)P(B2) + P(A|B3)P(B3) + · · · (1.13)           form by Pierre-Simon Laplace.
                                                                             Laplace then applied the rule in
Bayes' Rule                                                                  a large range of problems from
                                                                             geology, astronomy, medicine, and
One of the most consequential rules of probability is what is known          jurisprudence.
as Bayes' Rule, sometimes called Bayes' Theorem. We will use this
rule throughout this book, and see its many applications. It comes as        Bayes' Rule
a direct result of the product rule (Equation 1.8)                               P(A|B) = P(B|A)P(A)
                                                                                                    P(B)
P(A and B) = P(A|B)P(B) = P(B|A)P(A)

Rearranging, we get
   Bayes' Rule

                     P(A|B) = P(B|A)P(A)                             (1.14)
                                       P(B)

   We can verify this again with the intuitions we have in the simple
card game.

Example 1.6 What is the probability of drawing a jack, knowing that
you've drawn a face card?

   In terms of fractions, this should be F (jack|face card) = 4/12 =
1/3. Applying Bayes' Rule to the fractions we get:

                  F (jack|face) = F (face|jack) × F (jack)
                                                    F (face)

                                     4 = 4 × 452 = 4 = 1
                                               1252 12 3

Although this calculation is true, it isn't particularly enlightening.
It is nicer to cast the problem back into probability terms, rather
48 statistical inference for everyone

than fractions, and compare the probability of drawing a jack to the           All of learning is simply updating
probability of the same thing (i.e. drawing a jack) given that we know         ones beliefs given the data. The
that we've drawn a face card. This is                                          data may be words in a book, the
                                                                               results of an experiment, a conver-
                                        P (jack) = 1                           sation with another person, etc...
                                                             13                The strength of our beliefs are not
                                                                               often thought of in mathematical
                            P (jack|face card) = 1                             terms, but you are doing the math
                                                             3                 of probabilities whenever you are
                                                                               weighing the strength of your be-
   This comparison highlights what Bayes' Rule represents: learning.           liefs. Thus, the probabilistic rule -
When you are asked what the probability of drawing a jack, from                Bayes' rule - for updating beliefs
the knowledge of the simple card game, you calculate the value of              given data is really the quantitative
1/13. Once you learn that you drew a face card, you update your                specification of learning. One can
knowledge to include that information, and modify your probability             use it qualitatively as well, which is
assignments to reflect this. This leads to an increased chance of the          often useful in fields such as history
card being a jack.                                                             where the data do not tend to be
                                                                               quantitative.
   In a nutshell, Bayes' Rule represents learning:

             Initial Belief + New Data  Improved Belief

   It is used in science to infer causes from effects, and can thus be
written

             P (cause|effect) = P (effect|cause) × P (cause)
                                                     P (effect)

To infer the probability of a particular cause, given the events you
observe in the world, you first have to know the probability of the
cause itself (i.e. rarer causes will reduce the prior probability), and
how likely that the cause you're looking at could have produced
the effects you've observed. These two items are the P (cause) and
P (effect|cause) terms, respectively. The entire calculation is scaled
by P (effect) which is all of the other ways that the effects could have been
produced by other causes.. Thus, it is not enough to show that giving
a particular medicine is followed by the symptoms disappearing to
establish that the medicine was the likely cause of the symptoms
disappearing. You have to calculate what other possible causes could
have had those effects, such as the normal functioning of the immune
system or the placebo effect. This is why carefully controlled studies
are necessary, to eliminate all of the other possible causes and to
determine the true cause of the effects observed.

   We will spend large portions of several chapters on Bayes' Rule, to
explore its long-ranging consequences.
                                                                       introduction to probability 49

1.5 Venn Mnemonic for the Rules of Probability                                               UNIVERSE not A UNIVERSE A and B

that it is easier to remember the equations and to understand their 1/ It is often useful to have a picture to represent the mathematics, so 16

meaning. It is common to use what is called a Venn Diagram to rep-                                                                      A              A     B
resent probabilities in an intuitive, graphical way. The idea is that
probabilities are represented as the fractional area of simple geomet-                                                              1/4            1/4          1/8
ric shapes. We can then find a picture representation of each of the
                                                                                                                                    {
rules of probability. We start by looking at a sample Venn Diagram,                                                                    {                  A or B
in Figure 1.2.
                                                                                               Figure 1.2: Venn diagram of a
   The fractional area of the rectangle A represents the probability                           statement, A, in a Universe of all
P(A), and can be thought of as a probability of one of the statements                          possible statements. It is customary
we've explored, such as P(). This diagram is strictly a mnemonic,                              to think of the area of the Universe
because the individual points on the diagram are not properly de-                              to be equal to 1 so that we can treat
fined. The diagram in Figure 1.2 also represents the Negation Rule                             the actual areas as fractional areas
(Equation 1.7),                                                                                representing the probability of
                                                                                               statements like P(A). In this image,
                            P(A) + P(not A) = 1                                                A takes up 1/4 of the Universe, so
                                                                                               that P(A) = 1/4. Also shown is the
In the diagram it is easy to see that the sum of the areas inside of                           negation rule. P(A) + P(not A) = 1

                                                                                             Wednesday, May 28, 14

                                                                                               or "inside" of A + "outside" of A
                                                                                               adds up to everything.

A (i.e. 1/4) and outside of A (i.e. 3/4) cover the entire area of the                        UNIVERSE A and B                                      UNIVERSE

                                                           UNIVERSE                                             1/16
Universe of statements, and thus add up to 1.                          not A

Figure 1.3 shows the diagram which can help us remember the

sum and product rules. The Sum Rule (Equation 1.10)

P(A or B) = P(A) + P(B) - P(A and B)                               A                                                                    A  B           A          B

is represented in the total area occupied by the rectangles A and B,                                                                1/4       1/8  1/4               1/8

                                                                                        1/4

and makes up all of A (i.e. 1/4) and the half of B sticking out (i.e.

1/8-1/16=1/16) yielding P(A or B) = 5/16. This is also the area                                      A or B
of each added up (1/4+1/8), but subtracting the intersection (1/16)
because otherwise it is counted twice. Adding the areas this way                             Figure 1.3: Venn diagram of the
directly parallels the Sum Rule.                                                             sum and product. The rectangle B
                                                                                             takes up 1/8 of the Universe, and
   Conditional probabilities, like those that come into the Product                          the rectangle A takes up 1/4 of
Rule (Equation 1.8) and Bayes Rule (Equation 1.14) are a little more                         the Universe. Their overlap here is
challenging to visualize. In Figure 1.4, P(A|B) is represented by the                        1/16 of the Universe, and represents
fraction of the darker area (which was originally part of A) com-                            P(A and B). Their total area of
pared not to the Universe but to the area of B, and thus represents                          5/16 of the Universe represents
P(A|B) = 1/2. In a way, it is as if the conditional symbol, Wednesday,May28,14 "|," defines  P(A or B).

the Universe with which to make the comparisons. On the left of Fig-

ure 1.4, the same darker area that was originally part of B represents
P(B|A) making up 1/4 of the area of A. Thus P(B|A) = 1/4. The

Product Rule (Equation 1.8) then follows,

P(A and B) = P(A|B) P(B) = P(B|A) P(A) = 1
                                                               16

1/2 1/8  1/4 1/4
50 statistical inference for everyone

   We can further see the special case of mutually exclusive state-                                   B|A       A|B
ments shown in Figure 1.5. The Sum Rule for Exclusive Events
(Equation 1.11) is simply the sum of the two areas because there is                                       A     B
no overlap
                                                                                                      1/4          1/8

                         P(A or B) = P(A) + P(B)                                                      Figure 1.4: Venn diagram of con-
                                                                                                      ditional probabilities, P(A|B) and
Further, it is straightforward to see from this diagram the following                                 P(B|A). (Right) P(A|B) is repre-
properties for mutually exclusive events
                                                                                                      sented by the fraction of the darker
                                P(A and B) = 0
                                      P(A|B) = 0                                                      area (which was originally part of
                                      P(B|A) = 0                                                      A) compared not to the Universe but
                                                                                                      to the area of B, and thus represents
1.6 Lessons from Bayes' Rule - A First Look                            Wednesday, May 28, 14          P(A|B) = 1/2. In a way, it is as if
                                                                                                      the conditional symbol, "|," defines
                                                                                                      the Universe with which to make

                                                                                                      the comparisons. (Left) Likewise,
                                                                                                      the same darker area that was orig-
                                                                                                      inally part of B represents P(B|A)
                                                                                                      which makes up 1/4 of the area of
                                                                                                      A. Thus P(B|A) = 1/4.

Bayes' Rule is the gold standUaNrIdVEfoRrSaEll statistical infUerNeInVcEeR. SItEisAa and B            UNIVERSE  B
mathematical theorem, proven from fundanmoetntAal principles. It struc-
                                                                                                           A       1/8
tures all inference in a systematic fashion. However, it can be used16
                                                                                                      1/4
                                                                                                  1/

without doing any calculations, as a guide to qualitative inference.

Some of the lessons which are conseqAuences of Bayes' Rule aAre listBed

here, and will be noted throughout this text in various examples. 1/8

                                                  1/4 1/4

· Confidence in a claim should scale with the evidence for that claim
                                             {
· Ockham's razor, which is the philosophical idea that simApleorrthBe-
                                                                                                      Figure 1.5: Venn diagram of mu-
   ories are preferred, is a consequence of Bayes' Rule when compar-                                  tually exclusive statements. One
   ing models of differing complexity.                                                                can see that P(A and B) = 0
                                                                                                      (the overlap is zero) and
· Simpler means fewer adjustable parameters                                                           P(A or B) = P(A) + P(B) (the
                                                                                                      total area is just the sum of the two
· Simpler also means that the predictions are both specific and not                                   areas)
   overly plastic. For example, a hypothesis which is consistent with
   the observed data, and also be consistent if the data were the op-
   posite would be overlyWedpnesdayl,Maay28,s14 tic.

· Your inference is only as good as the hypotheses (i.e. models) that                                 7 Carl Sagan. Demon-Haunted
   you consider.                                                                                      World: Science as a Candle in the Dark.
                                                                                                      Random House LLC, 1996
· Extraordinary claims require extraordinary evidence.7
· It is better to explicitly display your assumptions rather than im-

   plicitly hold them.
· It is a good thing to update your beliefs when you receive new

   information.
· Not all uncertainties are the same.
                                          introduction to probability 51

   There is not a universal agreement for the translation of numerical
probability values to qualitative terms in English (i.e. highly unlikely,
somewhat unlikely, etc...). One rough guide is shown in Table 1.1. I
will be following this convention throughout the book, but realize
that the specific probability distinctions are a bit arbitrary.

          term            probability                                      Table 1.1: Rough guide for the
                                                                           conversion of qualitative labels to
virtually impossible      1/1,000,000                                      probability values.
extremely unlikely     0.01 (i.e. 1/100)
                        0.05 (i.e. 1/20)
    very unlikely
       unlikely          0.2 (i.e. 1/5)
                         0.4 (i.e. 2/5)
  slightly unlikely     0.5 (i.e. 50-50)
      even odds          0.6 (i.e. 3/5)
                         0.8 (i.e. 4/5)
    slightly likely    0.95 (i.e. 19/20)
         likely       0.99 (i.e. 99/100)
                      999,999/1,000,000
      very likely
  extremely likely
  virtually certain
2 Applications of Probability

In this chapter we go through a number of examples of the uses of
probability, and present several useful mathematical tools along the
way.

2.1 Cancer and Probability

This is perhaps the most important probability question to learn, so
we will spend some time covering it here and then cover it again, in a
slightly different way, in Section 5.1 on page 109. Imagine we have a
population of 10000 people who have been tested for cancer, and we
get the following hypothetical data:

Number of     Negative Test Positive Test Total
Individuals
              9200          700  9900
Doesn't Have
Cancer         20           80   100
Has Cancer    9220

                            780  10000

   We may be interested in a number of related probabilities.

Example 2.1 What is the probability of both having cancer and getting a
positive test for it?

   We can determine this by simply dividing the person counts from
the table

P (cancer and positive test) = # of people with both cancer and positive test
                                                                 total # of people

                                      = 80 = 0.008
                                             10000

   Doing this process for every part of the table yields a posterior
probability table, giving the probability for every combination of
variables (i.e. with cancer and positive test, without cancer and posi-
tive test, etc...)
54 statistical inference for everyone

Posterior Prob-  Negative Test Positive Test Total
ability
                 0.92                  0.07   0.99
Doesn't Have
Cancer           0.002                 0.008  0.01
Has Cancer       0.922

                                       0.078  1.0

Example 2.2 What is the probability of both not having cancer and getting
a positive test for it?

Reading off of the table, we have

                   P (no cancer and positive test) = 0.07

   This question, it turns out, is a not very interesting question. The
type of question that actually arises in life is the following,
Example 2.3 What is the probability of having cancer given a positive test
for it?

   Here we can perform the calculation in a couple of different ways,
to give the (unintuitive) result.

1 Counting the individuals.

   P (cancer|positive test) = # of people with both cancer and positive test
                                                     # of people with a positive test

                                   = 80 = 0.103
                                           780

   Although those with cancer nearly always test positive, out of the
   pool of all people who test positive - including a large number of
   false-positives - those actually having cancer are a small minority.
   It is because there are many more people without cancer, so even
   if a small fraction of those mistakenly test positive it will outweigh
   the small fraction of those people with the disease. This is why
   we insist on second opinions and why the rarity of a disease often
   matters even more than the accuracy of the test.

2 Applying Product Rule

   Using the Product Rule (Section 1.4 on page 42), we have

P (positive test) = P (no cancer and positive test) + P (cancer and positive test)
                       = = 0.07 + 0.008 = 0.078
                                                                  applications of probability 55

   P (cancer|positive test) = P (cancer and positive test)
                                                 P (positive test)

                                   = 0.008 = 0.103
                                           0.078

   where we have used the sum of the Positive Test column for P (positive test).
   This is simply a shortcut to the marginalization process (Section 1.4
   on page 46) - determine the probability of an event by adding up
   all of the possible conditional situations.

2.2 Weather

Example 2.4 If the probability that it will rain next Saturday is 0.25 and
the probability that it will rain next Sunday is 0.25, what is the probability
that it will rain during the weekend?

First Solution - Independence

If we assume that Sunday and Saturday weather are independent then
the sum-rule (Section 1.4) applies:

P(rain Saturday or rain Sunday) =

P(rain Saturday) + P(rain Sunday) - P(rain Saturday and rain Sunday)

= P(rain Saturday) + P(rain Sunday) - P(rain Saturday) × P(rain Sunday)

= 0.25 + 0.25 - 0.25 × 0.25 = 0.4375                                           (2.1)

The diagrams in Figure 1.3 are useful in making this calculation

more intuitive, especially the term where we subtract P(rain Saturday) ×

P(rain Sunday) because otherwise we over count the double-rain

weekends.                                                                               Another way to think of this term
                                                                                        can be seen in answering a different
Second Solution - Correlation                                                           question - what is the total number
                                                                                        of weekends with rain?. Imagine we
Is it really reasonable that rain on Saturday and Sunday are indepen-                   have, in a year, 40 Saturdays with
dent events? Probably not! It's probably the case that knowing that                     rain (by simply going through
it rained on Saturday, then rain on Sunday is more likely. It may also                  all of the Saturdays and counting
be that if it didn't rain on Saturday then it will be less likely for rain on           them if it rains on that day) and we
Sunday. So we'd have information possibly like:                                         also have 40 Sundays with rain. If
                                                                                        we want to know the number of
                     P (rain Sunday|rain Saturday) = 0.35                               weekends with rain we can add
                P (rain Sunday|not rain Saturday) = 0.15                                the Saturdays with rain and the
                                                                                        Sundays with rain (coming to 80!)
Knowing this changes the equation as                                                    and it becomes clear that we've over
                                                                                        counted those weekends where it
P(rain Saturday or rain Sunday) =                                                       rained both days - a year can only
  = P(rain Saturday) + P(rain Sunday) - P(rain Saturday and rain                        have 52 (or possibly 53) weekends.
                                                                                        We need to subtract those double-
                                                                                        counts to get a reasonable answer.
                                                                                        The same logic applies to the
                                                                                        calculation of probabilities.

                                                                               Sunday)
56 statistical inference for everyone

Notice, however, that we don't have a direct expression for P(rain Sunday)
anymore. We only have the conditional or dependent forms, like P (rain Sunday|rain Saturday).
We can use the marginalization procedure (Equation 1.13 on page 47),
and sum over all of the conditional expressions

P (rain Sunday) = P (rain Sunday|rain Saturday) P (rain Saturday) +
                            P (rain Sunday|not rain Saturday) P (not rain Saturday)

                       = 0.35 × 0.25 + 0.15 × (1 - 0.25) = 0.2

and then we have

= P(rain Saturday) + P(rain Sunday) - P(rain Saturday) × P (rain Sunday|rain Saturday)

= 0.25 + 0.2 - 0.25 × 0.35 = 0.3625                                                  (2.2)

which makes it less likely to rain on the weekend if the Sunday rain
is correlated with the Saturday rain (Equation 2.2) than if they are
independent (Equation 2.1). Why is that?

   One way to think of it is that, although the probability of rain on
Sunday is increased due to rain on Saturday, it is more likely that Sat-
urday is not rainy. In those cases, which are more frequent, Sunday
is less likely to be rainy as well. When the two days are indepen-
dent, Sunday's rain is the same probability regardless of Saturday's
weather. When they are dependent, then the more often clear Satur-
day weather makes it a little less likely for the Sunday rain, and thus
lowers the chance of weekend rain by a little bit.

2.3 Adding Dice                                                           All possible results from rolling
                                                                          two dice:
Example 2.5 What is the probability of the sum of two dice getting a
particular value, say, 7?                                                     sum (die 1,die 2)
                                                                                2 (1,1)
   In this case, we simply outline every single possibility, and count          3 (1,2),(2,1)
the fractions. In a more complex case we may need to find a better              4 (3,1),(1,3),(2,2)
method of counting, but the idea will be the same.                              5 (1,4),(4,1),(3,2),(2,3)
                                                                                6 (1,5),(5,1),(4,2),(2,4),(3,3)
   We find immediately that the probability of getting a sum of 7               7 (1,6),(6,1),(5,2),(2,5),(4,3),(3,4)
is the largest, because there are more arrangements of the two dice             8 (3,5),(5,3),(6,2),(2,6),(4,4)
which yield a sum of 7 than for any other sum. Each probability                 9 (5,4),(4,5),(3,6),(6,3)
of a particular sum is just the number of arrangements to get that             10 (4,6),(6,4),(5,5)
particular sum divided by the total number of arrangements of a two            11 (6,5),(5,6)
dice (i.e. 36).                                                                12 (6,6)
                                                                                        (36 arrangments total)
                                                                                  applications of probability 57

P(2) = 1 = 0.028          P(8) = 5 = 0.139                                        P(Sum of Two Dice)  0.18
              36                         36                                                           0.16
                                                                                                      0.14    4 6 8 10 12
P(3) = 2 = 0.055          P(9) = 4 = 0.111                                                            0.12     Sum of Two Dice
              36                         36                                                           0.10
                                                                                                      0.08
P(4) = 3 = 0.083         P(10) = 3 = 0.083                                                            0.06
              36                         36                                                           0.04
                                                                                                      0.02
P(5) = 4 = 0.111         P(11) = 2 = 0.055                                                            0.00 2
              36                         36

P(6) = 5 = 0.139         P(12) = 1 = 0.028
              36                         36

Example 2.6 What is the probability of rolling a sum more than 7 with
two dice?

In our notation this is

        P(8 or 9 or 10 or 11 or 12)

which are all exclusive events, so we use the Sum Rule for exclusive
events (Equation 1.11) and obtain

P(8 or 9 or 10 or 11 or 12) = P(8) + P(9) + P(10) + P(11) + P(12)
                                    = 0.139 + 0.111 + 0.083 + 0.055 + 0.028
                                    = 0.416

Example 2.7 What is the probability of rolling various sums with two dice
each with 20 sides?

   20-sided dice are common in some kinds of games, and provide a
nice alternative to the standard 6-sided variety. The figure comparing
the 6-sided and 20-sided dice can be see in in Figure 2.1 on page 57.

0.18P(Sum of Two Dice)                                                      0.05
0.16                                           P(Sum of Two 20-Sided-Dice)  0.04
0.14    4 6 8 10 12                                                         0.03  5 10 15 20 25 30 35 40
0.12     Sum of Two Dice                                                    0.02   Sum of Two 20-Sided-Dice
0.10                                                                        0.01
0.08                                                                        0.00                     Figure 2.1: Probability for rolling
0.06                                                                                                 various sums of two dice. Shown
0.04                                                                                                 are the results for two 6-sided dice
0.02                                                                                                 (left) and two 20-sided dice (right).
0.00 2                                                                                               The dashed line is for clarity, but
                                                                                                     represents the fact that you can't
                                                                                                     roll a fractional sum, such as 2.5.
58 statistical inference for everyone

2.4 The Birthday Problem                                                        1

This is a famous problem in probability1, which we address here in              This is the simplest assumption -
stages. We introduce a simple version, and make it more complex in              that each day is equally likely to be
steps until we can tackle the general problem.                                  born on. However, this is probably
                                                                                not true - there are some days that
Two People on April 3                                                           are more likely than others. In
                                                                                addition, once you start including
Example 2.8 Let's imagine we have the case where two people meet on the         February 29, then things obviously
street. What is the probability that they both have April 3 as their birthday?  change.

   This can be solved with a straightforward application of the prod-           In all of these examples we are
uct rule, Equation 1.8 on page 42.                                              not considering leap days, which
                                                                                occur approximately once every
                     A  Person 1 has a birthday on, say, April 3                four years. These extra days do
                     B  Person 2 has a birthday on, say, April 3                not change any of the qualitative
        P(A and B) = P(A|B)P(B)                                                 results, and really only serve as
                                                                                a small extra correction to any
Each of these terms can be calculated. Firstly, P(A|B) is the proba-            analysis. However, it does add a fair
bility that person 1 has a certain birthday given that person 2 has the         amount of bookkeeping with very
same birthday. However, knowing the birthday of the second person               little increase in enlightenment, so
doesn't tell us anything about the birthday of the first person, thus           we choose to avoid this problem in
they are independent and P(A|B) = P(A).                                         our examples.

   Secondly, the probability of having any particular birthday is sim-
ply P(A) = 1/365. Finally, we have

                     A  Person 1 has a birthday on, say, April 3
                     B  Person 2 has a birthday on, say, April 3
        P(A and B) = 1 × 1 = 1 = 0.0000075

                                 365 365 133, 225
which is extremely unlikely (see Table 1.1 on page 51)!

Two People

Example 2.9 Two people meet on the street, and we ask what is the proba-
bility that they both have the same birthday?

   How is this different than the previous question, where we spec-
ified which birthday they had? Our intuition immediately suggests
that this probability must be higher than the previous one, because
there are more possibilities - rather than April 3, they could be born
on January 1 or May 3 or any other day. Using our notation we have
the following definitions:

  C1  Person 1 and Person 2 both have a birthday on January 1
                                                                        applications of probability 59

  C2  Person 1 and Person 2 both have a birthday on January 2
         ..
         .

C365  Person 1 and Person 2 both have a birthday on December 31

and the probability we are looking for is

P(C1 or C2 or · · · or C365)

   In this situation we can note that these are exclusive statements.
For example, it can't be true that both C1 and C2 are true - you can't
have more than one birthday. Thus, the Sum Rule (Equation 1.10 on
page 1.10) reduces to the Limited Sum Rule (Equation 1.11). Further,
each term in that rule is the same

             P(C1) = P(C2) = · · · = P(C365) = 1 × 1
                                                            365 365

so we have

P(C1 or C2 or · · · or C365) =

1×1 + 1×1                                  +···+   1×1
365 365              365 365                      365 365

                                365 terms, one for each day             Here we find another example
                                                                        of the general requirement that
         = 1 = 0.0027                                                   equivalent states of knowledge
               365                                                      give rise to equivalent probability
                                                                        assignments. In this case it means
   Another way to think of this is to imagine that person 1 randomly    that if there is more than one way
"chooses" their birthday, D1, and person 2 randomly "chooses" their     to arrive at a conclusion, they each
birthday, D2, and then they compare to see if the days are the same,    must give the same answer. We
or D1 = D2. In general, we can think of the problem broken up in        can then choose the way that is
this way:                                                               easiest to calculate, simply out of
                                                                        convenience.
P(D1 = D2) =                      × number of possible
      P D1 is a specific day and       specific days
          D2 is the same day

In this way, we get

P(D1 = D2) =                     1 × 1 × (365)
                                365 365

                     = 1 = 0.0027
                          365

which is extremely unlikely (see Table 1.1 on page 51), but not nearly

as unlikely as them both having the same April 3 birthday.

Three People

Example 2.10 What is the probability that three random people have the
same birthday?
60 statistical inference for everyone

Going through the same logic, we have

P(D1 = D2 = D3) =                  1 × 1 × 1 × 365
                                  365 365 365

                           = 1 = 0.0000075
                                133, 225

which is even more extremely unlikely (see Table 1.1 on page 51)
than the previous two-person example. It is interesting to note that
this is the same answer we received when we asked for the probabil-
ity of two people with a specific birthday. One can think of the three
people having the same, unspecified, birthday in the following way if
it helps. The first person's birthday specifies the necessary birthday
for the other two, so it is the same as the case where we specify a
single birthday for two people.

Two People...Out of Three

Usually, we don't have a situation where we have random people
meeting and all agreeing on birthdays. What we have is a group of
people talking, and two people in the group end up saying "Hey,
my birthday is April 3 too!" This is quite a bit different, and leads to
some unintuitive consequences. Let's go through the situation with
three people, and we ask the question

Example 2.11 What is the probability that at least two have the same
birthday?

Writing this out we get (somewhat messily)                                Writing the possibilities out like
                                                                          this is quite tedious, and can
P(at least two out of three have the same birthday) =                     lead to errors. Directly after this
                                                                          calculation we find an equivalent,
= P(exactly 2 the same or exactly 3 the same)                             and much easier, way of writing
                                                                          the same calculation. However, it is
= P(exactly 2 the same) +                                                 important to note that all ways of
     P(exactly 3 the same) - P(exactly 2 and exactly 3 the same)          writing the same information must
                                                                          lead to the same answer.

13                                          0
( 365 ) ×365

The term P(exactly 2 the same) can be broken up like

                                                         number of        

P(exactly 2 the same) = P(a specific 2 are the same) × possibilities of 

                                                         2the same          
                                                         number of

              = P(D1 = D2 and not D1 = D3) × possibilities of 

                                                         2 the same

Applying the product rule we get                                          I'm sure you're wishing for the
                                                                          easier way about now...it's coming
                                                                          in Example 2.12.
                                                                      applications of probability 61

P(exactly 2 the same) =                          

                                     number of

= P(D1 = D2 and not D1 = D3) × possibilities of 

                                     2 the same  number of            

= P(D1 = D2|not D1 = D3)P(not D1 = D3) × possibilities of 

                                   number of     2 the same

= P(D1 = D2) P(not D1 = D3) × possibilities of 

1                        364       2 the same
365                      365

Noting that there are 3 ways of getting a specific 2 the same, we        These 3 ways are "person 1 and 2
obtain for this single term                                              match", "person 1 and 3 match",
                                                                         "person 2 and 3 match."

                 P(exactly 2 the same) = 1 × 364 × 3
                                                        365 365

Putting it all together we have

P(at least two out of three have the same birthday) =

= P(exactly 2 the same or exactly 3 the same)

= 1 × 364 × 3 + 1 3 × 365
     365 365                  365

= 0.0082

Example 2.12 What is the probability that at least two have the same
birthday? A clever shortcut.

   A clever way of rethinking this problem, which significantly re-
duces the calculations, is found by asking the following question: in a
group of people, what is the probability that none of the people have
the same birthday? This can be approached in a step-wise fashion.
Person 1 "chooses" a birthday, out of 365 they have all 365 possibili-
ties. Person 2 "chooses" their birthday, with probability P = 364/365
of not being the same as Person 1. Person 3 now has 363 "choices"
out of 365 to avoid both other birthdays, etc... So the probability of
using this process and getting to Person 3 and not have any overlap-
ping birthdays is simply

          P(none the same in 3 people) = 365 × 364 × 363
                                                            365 365 365

Now, if we're interested in the probability that at least two are the
same, then this is the exact opposite of the probability that none are
the same. Using the Negation Rule (Equation 1.7 on page 41) we have

                              not "none 
P none the same + P the same in 3  = 1
   in 3 people
                              people"
62 statistical inference for everyone

                 at least 2 the 
P none the same + P same in 3  = 1
in 3 people
                 people

which leads to

at least 2 the 
P same in 3  = 1 - P none the same
                         in 3 people
people

                 = 1 - 364 × 363
                            365 365

                 = 0.0082

Two People...Out of Thirty                                                   We've often used our intuition to
                                                                             verify the result, but now we've
Example 2.13 When you have a group of 30 people, like students in a          reached a state where the problems
classroom, and you ask what the probability of finding two in the room with  get subtle enough that our intuition
the same birthday, would your intuition say it is greater or less than 50%?  fails. It is good to use ones' intu-
                                                                             ition on the "easy" problems, but
Many people find that their intuition suggests reasonably strongly           now that we've established the pro-
that it would be less than 50%. We can now do this problem quite             cess we can tackle problems where
easily, and we find that our intuition does not match. Following the         our intuition is not good enough to
same procedure as with 3 people, we imagine each person "choos-              confirm a result.
ing" their birthday with a dwindling selection as we go on to avoid
"choosing" one that has already been taken. The probability that no
one in the room has the same birthday as any other is

P(none the same in 30 people) = 365 × 364 × 363 × · × 336
                 365 365 365                     365

                                       30 terms

                 = 0.29

   So the probability of having at least 2 people in the room having

the same birthday is
                          at least 2 the 

                       P same in 30  = 1 - 0.29
                             people
                                                    = 0.71

which is 71%! Compare this likely outcome to the extremely rare out-
come of having two random people having matched birthdays, from
page 58. See Figure 2.2 to see a plot of this unintuitive observation.

2.5 The Lottery Problem or Rare Things Are Common

This problem is identical to the birthday problem mathematically,with
the only difference that the probability numbers are much smaller
                                                                      applications of probability 63

P(at least 2 with same birthday)1.0                                    Figure 2.2: Probability of having at
                                0.8 50% mark                           least two people in a group with
                                                                       the same birthday depending on
                                     reached at                        the number of people in the group.
                                     23 people                         The 50% mark is exceeded once the
                                0.6                                    group size exceeds 23 people.

                                0.4

                                0.2

                                0.00 10 20 30 40 50 60 70 80 90
                                               Number of People

and the number of participants is much larger. We start with a story

about someone winning the lottery twice in the same day!2              2

Can you imagine winning the lottery twice in one day?

Angelo and Maria Gallina don't have to imagine - they hit twice on
Nov. 20.

The married couple from Belmont, Calif., had separately bought tick-
ets in two different California state lottery games, and both could
hardly believe their eyes as all 11 winning numbers over two games
came up....Before taxes, their winnings amounted to $126,000 for the
Fantasy 5 and $17 million for the SuperLotto Plus, according to The
Associated Press....Orkin arrived at the number by multiplying the
roughly 41-million-to-one odds of winning the SuperLotto game and
the 575,000-to-one odds of winning the Fantasy 5 game to arrive at
odds of 23,575,000,000,000-to-one.

   Pretty amazing! That's something like

             P (winning two tickets) = 1 2 · 1013  5 · 10-14 (2.3)

which truly is quite improbable as a single event, but is it truly an
improbable event to happen somewhere? The assumption stated in
the quote is that only two tickets were purchased. We all know that
many lottery tickets are purchased daily, which should increase the
chance that somewhere this will occur. Even this winning couple pur-
chased tickets every day for 20 years before winning this.
64 statistical inference for everyone

Like the birthday problem, you have to set up the problem in

the negative, and as what the probability of no one winning two

lotteries. If we assume 5 million people playing daily for 20 years,

this probability is:

                      5 million 

  no one              plays daily  =  1 - 2 · 1013 1  5·106 ×365×20
P winning
  two tickets for 20                                                (2.4)

                      years

                                   (1 - 5 · 10-14)5·106×365×20

                                  = 0.998                        (2.5)

yielding a 0.2% chance of this happening sometime in those 20 years
- still pretty rare, but not outrageously so. If we further imagine that
this is occurring across the 50 states, this increases to 10% chance of
this happening sometime in those 20 years. If we further imagine that
there are as many as 5 different lotteries (there are usually more) that
could be played per state, this jumps up to 40%.

   What we see as an initially highly unlikely event starts to become
likely and in fact common when considering all of the possible ways
that event could be produced.

2.6 Monty Hall Problem                                                          3
                                                                                4
One of the most popular probability problems is called the Monty
Hall problem, and is based on the television game show "Let's Make              Most people will state that, because
a Deal."3 It can take on many forms, but a common form is as fol-               we are left with 2 choices, it must
lows4                                                                           be 50-50. However there is added
                                                                                information in the system which
Example 2.14 Suppose you're on a game show, and you're given the choice         moves us from knowing nothing
of three doors: behind one door is a car; behind the others, goats. You pick a  about the two choices (i.e. 50-50
door, say No. 1 (but the door is not opened), and the host, who knows what's    chance) to knowing a little bit more
behind the doors, opens another door, say No. 3, which has a goat. He then      about the two choices (i.e. not 50-50
says to you, "Do you want to change your choice to door No. 2?" Is it to        chance).
your advantage or disadvantage to switch your choice, or does it matter
whether you switch your choice or not?

   The result is that it is always better to switch, where the probability
of getting the car moves up from 1/3 to 2/3 by switching! Because
this problem is particularly unintuitive, we will break it up into
smaller pieces. The critical aspect of this is that a change in our as-
signment of probability to an event must be somehow tied to a change in our
information about that event. In order to understand the problem, we
must then understand where the extra information is coming from.

   We will step up to the full problem listed, but for now we explore
some simpler versions of the problem.
                                                                                           applications of probability 65

Two Doors with Information

Example 2.15 Imagine we have a game with two doors: Behind one door
is a car; behind the other is a goat. You pick a door, say No. 1 (but the door
is not opened), and the host, who knows what's behind the doors, says that
there is a 90% chance that the car is behind door No. 2. Is it to your advan-
tage to switch your choice?

   Initially there is a two-door choice, with no information about ei-
ther choice, so we assign equal probabilities to the choices: P (car behind No. 1) =
P (car behind No. 2) = 0.5 (i.e. a 50-50 chance). After the host gives
information, this changes. Although this is still a two-door choice, it
is no longer a 50-50 chance. By having a knowledgable person give
you information suddenly changes the situation to a 10-90 chance,
and it is much better for you to switch.

   What if the host were a little less direct? Perhaps something like

Example 2.16 The host, who knows what's behind the doors, points to a
door, choosing the correct door 90% of the time and the incorrect one 10%.
You pick a door, say No. 1, and the host points to door No. 2. Is it to your
advantage to switch your choice?

This amounts to an identical situation as the previous one - the host
is giving you correct information 90% of the time, and we are in a
much better position switching.

Three Doors with Information

We return to the three-door case with a slight variation

Example 2.17 Suppose you're on a game show, and you're given the choice
of three doors: Behind one door is a car; behind the others, goats. You pick a
door, say No. 1 (but the door is not opened), and the host, who knows what's
behind the doors, says that another door, say No. 3, has a 0% chance of
having a car, and that the remaining door (that you haven't chosen - i.e door
No. 2) has a 66% of having the car. He then says to you, "Do you want to
pick door No. 2?" Is it to your advantage to switch your choice?

   In this case, switching to door No. 3 would be ridiculous - we
know the car isn't there, because the (honest) host knows that it is not
there. The host also has told us that there is a 66% chance of the car
behind door No. 2, and thus we have P (car behind No. 1) = 0.34
and P (car behind No. 2) = 0.66 and it is better to switch to door No.
2.

   It isn't the number of choices that is important, it is the informa-
tion we have about those choices. When you have no information, we
assign equal probabilities. When we have information, we can assign
non-equal probabilities.
66 statistical inference for everyone

Three Doors Down To Two                                                         Another way to look at this is to
                                                                                imagine a game with 1000 doors,
Back to our original problem, we have                                           car behind only one, and the host
                                                                                has to open up 998 doors (not yours
Example 2.18 Suppose you're on a game show, and you're given the choice         and not the prize - if the prize is
of three doors: behind one door is a car; behind the others, goats. You pick a  different than yours). Once you
door, say No. 1 (but the door is not opened), and the host, who knows what's    pick, say door number 1, and the
behind the doors, opens another door, say No. 3, which has a goat. He then      host opens every door except door
says to you, "Do you want to change your choice to door No. 2?" Is it to        576, and gives you the opportunity
your advantage or disadvantage to switch your choice, or does it matter         to switch is it a good choice? Of
whether you switch your choice or not?                                          course! Ones intuition realizes that
                                                                                my initial 1/1000 chance of getting
   The key part is that, no matter what happens,                                it right (and thus have the other
                                                                                door have a goat) is swamped by
1 the host never opens your door                                                the 999/1000 chance of getting it
                                                                                wrong, and the host being forced to
2 the host always opens a door with a goat                                      open every door without the prize.

   Given that your first choice, with three equal probability choices
(i.e. you have no information about any of the choices), we expect to
be correct only about 33% of the time. If we happened to get lucky
with our first choice, then the host has a pick of two doors with goats
and has some freedom. If we happened to get unlucky with our first
choice (and there is a goat behind it), then the host has no freedom at
all, because there is only one remaining door with a goat. So, about
66% of the time the host is forced to reveal some of his information,
because the door he leaves closed (other than your door) must have
the car. Thus, 66% of the time the host is telling you where the car is,
just a little indirectly.

   Formally, we need to involve model comparison, so we postpone
this particular analysis until Section 5.4.

2.7 Exercises

Exercise 2.1 What is the probability that at least 3 people have the same
birthday in a group of 50?

Exercise 2.2 Examine the case of Monty Hall with 4 doors, the host open-
ing one door with a goat, and leaving you with a choice of 3. Should you
switch? Does it matter which of the other two you choose?

Exercise 2.3 What is the probability of rolling various sums from two
9-sided dice?

Exercise 2.4 What is the probability of rolling an odd sum with two dice?

Exercise 2.5 What is the probability of rolling more than 7 from two 20-
sided dice?
                                                                                           applications of probability 67

Exercise 2.6 Given the table above, determine the following quantities, and
describe what they mean:

1 P (cancer and negative test)

2 P (cancer|negative test)

3 P (not cancer)

4 P (not cancer) + P (cancer)

2.8 Some Philosophical Applications

Doctors' Claims - English Language and Probability

In Section 1.4 we introduced work by Tversky and Kahneman doc-
umenting supposed failures in proper reasoning. In the example
survey of medical internists, the internists were asked

    Which is more likely: the victim of an embolism (clot in the lung) will
    experience partial paralysis or that the victim will experience both
    partial paralysis and shortness of breath?

and 91 percent of the doctors chose that the clot was less likely to
cause the rare paralysis rather than to cause the combination of the
rare paralysis and the common shortness of breath.

   This may not be a failure of reasoning, but a (correct!) failure of
the doctors to translate the English language literally into logical
language. It is likely that when doctors are asked: "Which is more
likely: that the victim of an embolism will experience partial paral-
ysis or that the victim will experience both partial paralysis and
shortness of breath?" they interpret it as:

1 someone is claiming that the patient has an embolism

2 the patient is claiming, or someone has measured, that she has
   partial paralysis

3 the patient is claiming, or someone has measured, that she has
   shortness of breath

   The doctors are separating the analysis of the claim of the clot,
which is given information, from the other claims. Another way of
looking at it is to include the knowledge of the method of reporting.
Someone who is reporting information about an ailment will tend to
report all of the information accessible to them. By reporting only the
paralysis, there are two possibilities concerning the person measuring
the symptoms of the patient:
68 statistical inference for everyone

1 they had the means to measure shortness breath in the patient, but
   there was none

2 they did not have the means to measure shortness of breath

In the first case, the doctor's probability assessment is absolutely
correct: both symptoms together are more likely than just one. In the
second case, the doctors are also correct: one of the sets of diagnostic
results (i.e. just paralysis) is less dependable than the other set (i.e.
both symptoms), thus the second one is more likely to indicate a clot
or is consistent with the known clot.

   It isn't that the doctors are reasoning incorrectly. They are includ-
ing more information, and doing a more sophisticated inference than
the strict, formal, minimalistic interpretation of the statements would
lead one to do. This analysis works well for other examples stated in
the book A Drunkard's Walk by Mlodinow[?], like "Is it more probable
that the president will increase federal aid to education or that he
or she will increase federal aid to education with funding freed by
cutting other aid to states?"

   All of this underscores the need to be careful translating state-
ments of probability into plain English and vice versa.

Diverging Opinions                                                         5 E. T. Jaynes. Probability Theory:
                                                                           The Logic of Science. Cambridge
Is it possible to have people informed by the same information, and        University Press, Cambridge, 2003.
reasoning properly, to have diverging opinions? It might seem in-          Edited by G. Larry Bretthorst
tuitive that people given the same information, reasoning properly,
would tend to come to agreement, however this is not always the
case. What is interesting is that it turns on the prior probabilities for
claims. This example comes from Jaynes, 20035. We have the follow-
ing piece of information:

D :=  "Mr N. has gone on TV with a sensational claim
      that a commonly used drug is unsafe"

and we have observers A, B, and C with different prior assignments
to the reliability of Mr N and of the safety of the drug. These prior
assignments may have been the result of previous inference by these
observers, in a different context, or possibly due to expert knowledge.
Observers A and C believe, before the announcement, that the drug
is reasonably safe. Observer B does not. We have the probability
assignments then:

      PA(Safe) = 0.9
      PB(Safe) = 0.1
      PC(Safe) = 0.9
                                                                                           applications of probability 69

   They all agree that if the drug is not safe, then Mr N would an-
nounce it, so we have

                              PA(D|not Safe) = 1
                               PB(D|not Safe) = 1
                              PC(D|not Safe) = 1

   Finally, we have the perceptions from the observers about the reli-
ability of Mr N if the drug is actually safe. In this case, observer A is
trusting of Mr N, observer C is strongly distrustful, and observer B is
mildly distrustful. By "distrustful" we are referring to the probabili-
ties that Mr N would make the announcement that the drug is unsafe
even if the drug were actually safe. So we have

                               PA(D|Safe) = 0.01
                               PB(D|Safe) = 0.3
                               PC(D|Safe) = 0.99

We want to know how each observer then determines whether the
drug is safe, given the announcement, or P(Safe|D) for each observer.

   Applying Bayes' Rule we have

PA(Safe|D) = PA(D|Safe)PA(Safe) PA(D|Safe)PA(Safe) + PA(D|not Safe)PA(not Safe)
                  = 0.01 · 0.9 = 0.083
                       0.01 · 0.9 + 1 · 0.1

Following the same calculation for the others, we get the observers
updating their probability assignments after the announcement, D, as

                  PA(Safe) = 0.9  PA(Safe|D) = 0.083
                   PB(Safe) = 0.1  PB(Safe|D) = 0.032
                   PC(Safe) = 0.9  PC(Safe|D) = 0.899

Observer A changed their mind, Observer B had their assessment
confirmed a bit, and Observer C barely budged.

   Although you'd think that hearing the announcement of the un-
safe nature of the drug would have moved all of the probabilities by
the same amount, but the information isn't that the drug is unsafe,
but the someone is claiming that the drug is unsafe. Thus, ones prior
information about both the drug and who is making the claim comes
into play.

A problem of independence

As said in the beginning of Chapter 1 (Introduction to Probability),
in 1968 a jury found defendant Malcolm Ricardo Collins and his wife
70 statistical inference for everyone

defendant Janet Louise Collins guilty of second degree robbery. The            6 J. Sullivan. People v. Collins ,
prosecutor focussed on the the distinctive features of the dependence,         68 cal.2d 319, 1968. URL http:
and assigned a probability to each as follows6:
                                                                               //scocal.stanford.edu/opinion/
1 Partly yellow automobile 1/10                                                people- v- collins- 22583

2 Man with mustache 1/4

3 Girl with ponytail 1/10

4 Girl with blond hair 1/3

5 Negro man with beard 1/10

6 Interracial couple in car 1/1000

He then followed with the calculation applying the product rule for
independent events (Section 1.4 on page 43), to find the probability
that all these things could have been observed:

              1 ×1× 1 ×1× 1 × 1 = 1
              10 4 10 3 10 1000 12, 000, 000
   The initial conviction was overturned for two primary reasons,
one legal and one mathematical. The legal argument was that the
prosecution had not established that these initial probabilities were
supported by the evidence. However, the really devastating part of
the argument was mathematical. As you may recall, the product rule
used in this way assumes the independence of the terms (Section 1.4 on
pageSection 43).

Example 2.19 Beard and Mustache - An Examination of Independence

   For an example, the proper product rule for two of the terms
above would look like:

   P (Man with beard and Man with mustache) =
         P (Man with mustache|Man with beard) P (Man with beard)

   What the prosecutor was assuming is that these two items were
independent, from which it would follow that

    P (Man with beard and Man with mustache) =
          P (Man with mustache) P (Man with beard) = 1 = 0.025
                                                                           40

   However, with a very brief thought, we notice that this is equiva-
lent to saying

    Knowing the man has a beard tells us nothing about the probability of
    him having a mustache!
                                                                     applications of probability 71

   Clearly, it is not nearly as common to have a beard with no mus-
tache than with one, so knowing that the man had a beard would
nearly certainly imply that he had a mustache or,

P (Man with beard and Man with mustache) =
     P (Man with mustache|Man with beard) P (Man with beard)  1
                                                                                            10

                                        1

   and the probability calculated, just from these two terms, is much
higher than the prosecutor was communicating.

   Similar sorts of absurdities occur with other terms, like "blond
hair" and "pony tail", as well as others. Finally, even if it was the
case that this is a somewhat rare combination, given the number of
people in Los Angeles, one might be able to calculate the probability
that there is at least one more couple satisfying these characteristics.
Just like the lottery problem (Section 2.5 on page 62), it becomes
likely that there are more couples in the area like this, and thus the
ruling was overturned.

Another problem with independence

Another problem brought up in the opening of Chapter 1 (Intro-                                  7 Lord Justice Kay. R vs Sally
duction to Probability) is the case of Sally Clark. Sally Clark was                             Clark, April 2003. URL http:
convicted in 1999 of the murder of her two young sons7. In the case,
the statistical argument was                                                                    //www.bailii.org/ew/cases/EWCA/
                                                                                                Crim/2003/1020.html
    Professor Meadow was asked if a figure of 1 in 8,543 reflected the risk
    of there being a single SIDS within such a family. He agreed that it
    was. A table from the CESDI report was placed before the jury. He was
    then asked if the report calculated the risk of two infants dying of SIDS
    in that family by chance. His reply was: ``Yes, you have to multiply 1 in
    8,543 times 1 in 8,543 and I think it gives that in the penultimate paragraph.
    It points out that it's approximately a chance of 1 in 73 million."

   What he was doing was equating the following in the product rule
(Section 1.4 on page 1.4):

P (second child dying of SIDS|first child dying of SIDS) = P (second child dying of SIDS)

which is equivalent to saying

    Knowing that the child dies of a [not well understood] disease tells us
    nothing about the probability of the second child dying of the same [not
    well understood] disease.

Clearly this is ridiculous, because if there is a common source to
the disease, the one death certainly increases the probability of the
second. Such a common source could be something shared in the
environmental or perhaps a genetic disposition in the family for the
disease.
72 statistical inference for everyone

Prosecutor's Fallacy

Both of the cases above are examples of what is called the prosecu-

tor's fallacy. It occurs when someone assumes that the prior prob-

ability of an event is equal to the probability that the defendant is

innocent. A simple example is that "if a perpetrator is known to have

the same blood type as a defendant and 10% of the population share

that blood type; then to argue on that basis alone that the probability

of the defendant being guilty is 90% makes the prosecutors's fallacy,

in a very simple form."8                                                 8

Essentially the prosecutor is ignoring the number of people who

match the rare event. Also, although double-deaths by SIDS are rare,

they are much more common than double-murders! One really has to

look at

                      P (innocence|evidence)

which is not the same as

                          P (evidence)

2.9 Computer Examples

Coin Flips

from s i e import *
   Generate a small list of data...

data=randint (2 , size =10)
print data

[1 0 0 1 0 0 0 1 0 0]

   Generate a slightly larger list of data...
data=randint (2 , size =30)
print data

[1 1 1 0 0 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0 0 1 1 0 0]

data=randint (2 , size =(2000 ,10))
data

array([[1, 0, 1, ..., 1, 0, 0],
            [1, 1, 1, ..., 0, 1, 0],
            [0, 0, 1, ..., 0, 0, 0],
            ...,
            [0, 0, 0, ..., 1, 1, 0],
                                                            applications of probability 73

            [0, 1, 0, ..., 0, 1, 1],
            [0, 1, 1, ..., 1, 0, 1]])

   We have here a large collection of numbers (20000 of them!), organized in 2000 rows of 10 columns.
We can sum all of the 20000 values, or we can sum across columns or across rows, depending on what
we want.

sum ( data ) # add up a l l o f t h e 1 ' s

9988                      # sum up a l l o f t h e columns

sum ( data , a x i s =0)

array([1011, 1010, 1001, 1051, 1001, 1008, 962, 990, 976, 978])

sum ( data , a x i s =1) # sum up a l l o f t h e rows

array([3, 7, 3, ..., 5, 4, 6])

   Typically the hist command makes its own bins, which may not center on the actual count values.
That's why we call countbins(N), to make bins centered on the counts.

N=sum ( data , a x i s =1) # number o f heads i n each o f many f l i p s
h i s t (N, countbins ( 1 0 ) )
x l a b e l ( 'Number o f Heads ' )
y l a b e l ( 'Number o f F l i p s ' )

<matplotlib.text.Text at 0x10856e990>

To get a probability distribution, we divide the histogram result by N.
This distribution is Bernoulli's equation, or in other words, the binomial distribution.

                                             p(h, 10) = 10 0.5h · 0.510-h
                                                                 h
74 statistical inference for everyone

h= a r r a y ( [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 1 0 ] )
# or . . .
h=arange ( 0 , 1 1 )

   (recall that ** is exponentiation in Python, because the caret (^) was already used for a computer-
sciency role.) The spaces in the equation below are not needed, but highlight the three parts of the
binomial distribution.
p=nchoosek ( 1 0 , h ) * 0 . 5 * * h * 0 . 5 * * ( 1 0 - h )
h i s t (N, countbins ( 1 0 ) , normed=True )
p l o t ( h , p , '--o ' )
x l a b e l ( 'Number o f Heads , $h$ ' )
y l a b e l ( ' $p ( h|N=10) $ ' )

<matplotlib.text.Text at 0x108560290>

Exercise 2.7 You flip a coin five times...
1 What is the probability of flipping 0, 1, 2, 3, 4, and 5 heads each in these

   5 flips?
2 Show in a simulation that this matches these probabilities you just found.
3 Random Sequences and Visualization

Now that we understand the rules of probability, and how they are           All possible results from three coin
applied in a number of practical examples, we explore the use of            flips:
these rules to sequences of random events. This will produce several
interesting and unintuitive observations, failures of inference, and            1 TTT
the proper ways to handle them. Finally, we examine how visual-                 2 TTH
ize both data in general and what we can communicate with such                  3 THT
visualization.                                                                  4 THH
                                                                                5 HTT
3.1 Coin Flipping                                                               6 HTH
                                                                                7 HHT
We'll start with some simple examples of coin flipping, asking some             8 HHH
simple questions, and move to more complex observations and unin-
tuitive conclusions.

Example 3.1 What is the probability of flipping three heads in a row, with
a fair coin?

   We can approach this problem in two different ways. The first way,
is a brute-force counting method with the definition of probability for
exclusive events (using Equation 1.2) and the second way makes use
of the other rules of probability. In the first way, we simply outline
every possible combination of three flips, see how many are "three
heads in a row", as we show in the margin.

   Because there is only one case of "H H H" in all eight, the proba-
bility of three heads in a row is

                      P (three heads in a row) = 1/8

which is an unlikely outcome, but not extremely so (see Table 1.1 on
page 51).

   In terms of the rule of probability, we have

          P (three heads in a row) = P(H1 and H2 and H3)

where H1 is heads on the first flip, H2 is heads on the second flip,
etc... Because these are independent events (Section 1.4), the probabil-
ity is just the product of the probabilities of the individual events
76 statistical inference for everyone

(Equation 1.9)

          P (three heads in a row) = P(H1 and H2 and H3)                     Yet again, we see that if there
                                            = P(H1) × P(H2) × P(H3)          are multiple ways of arriving at
                                            = 1×1×1                          an answer, that it must yield the
                                                    222                      same answer - equivalent states
                                            =1                               of knowledge yield equivalent
                                                    8                        probability assignments.

the same answer as before.

Example 3.2 What is the probability of flipping thirty heads in a row, with
a fair coin?

   Our intuition will clearly insist that this will be a very small num-
ber, but how small? Our first method, of listing all of the possibilities
gets quite a bit cumbersome with this question. The second method
is quite straightforward

P (thirty heads in a row) = P(H1 and H2 and · · · and H30)
                                  = P(H1) × P(H2) × · · · × P(H30)
                                               30 times

                = 1 × 1 ×···× 1
                22                     2

                        1 30
                =

                        2

                = 0.000000001 (one in a billion!)

This is virtually impossible (Table 1.1).

Example 3.3 What is the probability of flipping two heads in three flips,
with a fair coin?

   Our intuition suggests that this should be a reasonably common
occurrence. We address this problem in exactly the same two ways:
first, by counting, the second with the rules of probability. In the
first method, we observe from the table that there are three ways of
getting two heads: "T H H," "H T H," and "H H T." Thus,

                     P (two heads in three flips) = 3
                                                                8

In the second method we write

P (two heads in three flips) =
     P ((T1 and H2 and H3) or (H1 and T2 and H3) or (H1 and H2 and T3))

from which we can apply the sum rule for exclusive events (Equa-
tion 1.11) and, like before, the product rule for independent events
(Equation 1.9),
                                     random sequences and visualization 77

P (two heads in three flips) =

P(T1 and H2 and H3) + P(H1 and T2 and H3) + P(H1 and H2 and T3)

= 1×1×1 + 1×1×1 + 1×1×1
222                222          222

= 1+1+1=3
     888 8

which is about a 38% chance, slightly unlikely (Table 1.1).

Example 3.4 What is the probability of flipping ten heads in thirty flips,
with a fair coin?

   Once the numbers start getting large, our intuition fails, and we
can't list all the possibilities. In order to proceed, we need to develop
a systematic way of approaching these sorts of problems. Essentially
it comes down to two parts:

1 What is the probability of one particular sequence being considered?

2 How many ways can this type of sequence appear in the process
   described in the question?

   Point 1 is asking, what is the probability of this particular se-
quence:

HHHHHHHHHHTTTTTTTTTTTTTTTTTTTT

or this sequence:

TTHTTTTHHHHTTTTTHTTHTTTTTTHHTH

Although it is unintuitive, mathematically both of these specific se-
quences have exactly the same probability: each head or tail has equal
probability, is not related to the others, and there are the same num-
ber of them. So we have

P (HHHHHHHHHHTTTTTTTTTTTTTTTTTTTT) =

        P (TTHTTTTHHHHTTTTTHTTHTTTTTTHHTH)
           1 30

  =
           2

  = 0.000000001 (one in a billion!)

Every single specific length-thirty sequence of heads and tails has the
same probability, one in a billion.

   Point 2 is asking, how many sequences are there of thirty heads
and tails where ten of them are heads? Another way of phrasing it is,
given a sequence like:

HHHHHHHHHHTTTTTTTTTTTTTTTTTTTT

how many different ways can I rearrange this sequence and get a
unique sequence?
78 statistical inference for everyone

Counting the Rearrangements                                                Symbols: A B C D
                                                                           Boxes:
We are going to determine the answer to our question in small steps.
   First, we ask,                                                           Choices  Remaining Symbols

Example 3.5 How many ways can we rearrange the unique symbols A, B,        A                  BCD
C, and D?                                                                  B                  ACD
                                                                           C                  ABD
To make this intuitive, we set up four empty boxes and we imagine          D                  ABC

placing our symbols in the boxes, one at a time. How many choices

do we have? For the first box, we have four choices. For each of these

choices, we've removed one of the symbols, and one of the boxes.            Choices  Remaining Symbols
                                                                           AB                  CD
Thus, we are left with three remaining symbols for each choice, and        AC                  BD
                                                                           AD                   BC
three remaining boxes. For each of the original four choices, we now
                                                                           BA                  CD
have three choices for the second box. This immediately leads to           BC                  AD
                                                                           BD                  AC
4 × 3 = 12 possibilities by the time we've filled two boxes. For each of
                                                                           CA                  BD
these twelve possibilities, there are two symbols remaining and two        CB                  AD
                                                                           CD                  AB
boxes. Continuing this logic, we have two choices for the third box,
                                                                           DA                   BC
and then only one choice for the final box. In summary, for each of        DB                  AC
                                                                           DC                  AB
the four choices for the first box we have three choices for the second,

two choices for the third, and one for the final box. Thus we have

number of rearrange-     
ments of four different   = 4 × 3 × 2 × 1 = 24

symbols

In general we have                                                         Number of Rearrangements of N
   Number of Rearrangements of N Unique Symbols                            Unique Symbols

C(N) = N × (N - 1) × · · · × 2 × 1                                         C(N) = N × (N - 1) × · · · × 2 × 1
                                                                                     = N!
         = N!                                                       (3.1)

where we've introduced the notation for the factorial of N as N!.

Example 3.6 How many ways can we rearrange the symbols A, A, A, and
D?

   By eye we can see that there are only four rearrangements of these      Symbols: A A A D
symbols. How is this different from the previous question with four          Rearrangements
symbols? We can imagine going from the first question, with four
unique symbols "A B C D," and replace both "B" and "C" with "A"                   DAAA
to get it. "BC" and "CB" are different sequences of unique symbols.               ADAA
However, if we replace "B" with an "A" and "C" with an "A", both                  AADA
sequences become the same sequence, namely "AA". If we try to                     AAAD
blindly apply Equation 3.1, the one for the number of rearrangements
of unique symbols, to the case where there are duplicates, we will
overestimate the number of rearrangements - we are over counting
duplicate subsequences. Further, we can be specific about how much
                                                      random sequences and visualization 79

we are over counting and thus find a new equation which includes

the possibility of duplicates.

For example, if we have three duplicates in a sequence, the num-

ber of over countings will be the number of possible rearrangements

of three unique symbols, because all of these rearrangements result in

the same sequence of duplicate symbols. Thus, our procedure should

be,

                                  number of rearrange- 

       number of rear-            ments of four unique 

                                    symbols
     rangements of "A  = number of rearrange- 
       A A D"
                                  ments of the over-          
                                                              
                                  counted duplicate three 

                                    symbols

                                = 4!
                                     3!

                                = 4×3×2×1
                                        3×2×1

                                =4

Example 3.7 How many ways are there of rearranging the symbols "A A
A D D"?

Following the same logic, we have

                        5! ways of
                        rearranging
                        5 unique
                        symbols

               AAA                  DD

               3! ways of 2! ways of
               rearranging rearranging
               3 duplicates 2 duplicates

                                                                        All possible results of rearranging
         number of rear- 5!                                             the symbols "A A A D D":
       rangements of  = 3!2!
         "A A A D D"                                                         1 AADDA
                                                                             2 DAADA
                                  = 5×4×3×2×1                                3 ADADA
                                       (3 × 2 × 1) × (2 × 1)                 4 DAAAD
                                                                             5 DADAA
                                  = 120                                      6 AADAD
                                       6×2                                   7 DDAAA
                                                                             8 ADDAA
                                  = 10                                       9 AAADD
                                                                            10 A D A A D

Sequences of Heads and Tails
Now we can return to our original question,
80 statistical inference for everyone

Example 3.8 What is the probability of flipping ten heads in thirty flips,
with a fair coin?

We broke it down into two parts:

                                            number of re-          

           one sequence of  arrangements 
                                      of a length-30 
P(h = 10, N = 30) = P 10 heads and 20 ×                            
             tails sequence with   
                                            10 "H" and 20

                                            "T"

1 What is the probability of one particular sequence being considered?

  one sequence of  

P 10 heads and 20 =               1 10 ×  1 20
                                  2       2
  tails
                                  1 30
                     =            2

                     = 0.00000000093 (one in a billion!)

2 How many ways can this type of sequence appear in the process
   described in the question?

Because we have a length-thirty sequence of "H" and "T" with

10 duplicate "H" symbols and 20 duplicate "T," we have the fol-

lowing number of ways that this could occur (i.e. the number of

rearrangements of these sequences):

           number of re-          

         arrangements             
                                  
         of a length-30  = 30!
         sequence with                    10!20!
                                  
         10 "H" and 20 

           "T"
                                     = 30045015

   So the probability of flipping 10 heads in 30 flips is

                                                30! 1 30
            P(h = 10, N = 30) = 10!20! 2

                                       = 30045015 × 0.00000000093
                                       = 0.028

which is extremely unlikely (Table 1.1).
   In general we have
                                                                 random sequences and visualization 81

   Probability of flipping h heads and t tails Given the probability      Probability of flipping h heads
of flipping a single heads as 1/2, and the total number of flips is       and t tails Given the probability
N = h + t, we have the following equivalent forms:                        of flipping a single heads as 1/2,
                                                                          and the total number of flips is
                   P(h, t) = (h + t)! × 1 h × 1 t (3.2)                   N = h + t, we have the following
                                                                          probability for h heads and t tails:

                                  h!t!      2          2                  P(h, t) = (h + t)! × 1 h × 1 t

                   P(h, N) =      N! × 1 h × 1 N-h                        h!t!  2  2

                              h!(N - h)! 2                2

                   P(h, N) =      N × 1 h × 1 N-h
                                  h         2          2

where we have introduced the notation that is sometimes used, called
choose, read as "N choose h,"

                              N  N!
                              h         h!(N - h)!

   Shown in Figure 3.1 is the probability of flipping h heads in 30
flips, for each value of h from h = 0 (no heads or, in other words, 30
tails) up to h = 30 (all 30 heads). Clearly the most likely value is 15,
but all of the numbers from 12 up to 18 have significant probability.

            0.16                                                          Figure 3.1: Probability of getting
                                                                          h heads in 30 flips. Clearly the
            0.14                                                          most likely value is 15, but all of
                                                                          the numbers from 12 up to 18 have
                                                                          significant probability.

            0.12

            0.10

P(h,N =30)  0.08

            0.06

            0.04

            0.02

            0.000  5          10        15         20        25  30
                                  Number of heads

Example 3.9 What is the probability of getting 17 or more heads in 30
flips?

   Because these are independent events, we can simply sum up the
terms for P(h = 17, N = 30), P(h = 18, N = 30), etc... yielding
82 statistical inference for everyone

the following, either through direct calculation, or by reading the
Figure 3.1.

P(h  17, N = 30) = 0.11 + 0.08 + 0.05 + 0.028 + 0.013 + 0.005 + 0.002 + (tiny numbers)

                        h=17 h=18 h=19 h=20 h=21 h=22 h=23 h=23,24,25,26,27,28,29,30

= 0.29

which is quite likely!

3.2 Binomial Distribution

The distribution of the possible number of heads, given N flips with
a coin with probability p of flipping heads, is referred to as the Bino-
mial Distribution. It has the form of Equation 3.3, with the "fair coin"
probability, 1/2, replaced with p:

               P(h|N, p) = N! × ph × (1 - p)N-h (3.3)
                                    h!(N - h)!

   Probability of flipping h heads and t tails with an unfair coin         Probability of flipping h heads and
Given the probability of flipping a single heads is, say, p and the total  t tails with an unfair coin Given
number of flips is N = h + t, we have the following equivalent forms:      the probability of flipping a single
                                                                           heads as p, and the total number
P(h, t) = (h + t)! × ph × (1 - p)t (3.4)                                   of flips is N = h + t, we have the
                    h!t!                                                   following probability for h heads
                                                                           and t tails:
P(h, N) =               N! × ph × (1 - p)N-h
                        h!(N - h)!                                         P(h, t) = (h + t)! × ph × (1 - p)t
                                                                                          h!t!

P(h, N) =               N × ph × (1 - p)N-h
                        h

where the probability of tails is 1 - p.

3.3 Some Philosophical Applications

Streaks
In the previous section we looked at the probability of getting a cer-
tain number of heads in a number of flips. Look at the following two
sequences:

1 HTTHTHHTTHTHTTHHHTHHTTHHTHHTTHTHHTHHTTHTTHHHTHTHTT

2 HHTHHHTTTTTTTHTHTTHTTTHTHTHHTHTTHTTTHHTTTHHHHTHHHH

One of these sequences was generated from actually flipping a coin
50 times. The other one is from a person pretending to flip a coin, and
                                                   random sequences and visualization 83

            0.25                                                                           Figure 3.2: Probability of getting h
                                                                                           heads in 30 flips given a possible
                                                                               p =0.1      unfair coin. One coin has p = 0.1,
                                                                               p =0.5      where the maximum is for 3 heads
                                                                                           (or 1/10 of the 30 flips), but 2
            0.20 p =0.8                                                                    heads is nearly as likely. Another
                                                                                           has p = 0.5, and is the fair coin
P(h,N =30)  0.15                                                                           considered earlier with a maximum
                                                                                           at 15 heads (or 1/2 of the 30 flips).
                                                                                           Finally, another coin shown as
                                                                                           p = 0.8 where 24 heads (or 8/10 of
                                                                                           the 30 flips) is maximum.

            0.10

            0.05

            0.000  5  10  15               20  25                                      30
                          Number of heads

writing down a sequence that they thought would look like a random
flipping of a coin. Which one is which? While many people think
that sequence 1 looks more "random" (i.e. it seems to flip around a
lot), sequence 2 is actually the random sequence.

   One of the truly unintuitive things about real random sequences,
as opposed to designed sequences, is that there are long runs or
streaks. Why is this? The general solution is beyond this book but
we can think about it this way. Although a sequence of, say, 5 heads
in a row is very unlikely (P (5 heads in a row) = (1/2)5 = 0.03),
there are many opportunities for such a sequence somewhere within a
sequence of 50. Because of these many opportunities, this raises the
probability from 3% (the probability of 5 heads in a row in 5 flips), to
over 55%, the probability of finding 5 heads in a row somewhere in 50
flips. Streaks of 6 heads in a row occur nearly one third of the time in
50 flips, or over half the time if you consider a run to be either heads
or tails. Even streaks of 9 heads or tails in a row, in 50 flips, are not
extremely unlikely!

Gambler's Fallacy

When we look at a sequence of real coin flips, like:

· HHTHHHTTTTTTT

and we ask about the probability of flipping heads in the next flip, it
is common to (mistakenly!) reason that, because we've seen 7 tails in
84 statistical inference for everyone

a row, then the next flip is more likely to be heads. However, this is      1 One can imagine a flipping pro-
not the case for two reasons:                                               cedure where the flips are not
                                                                            independent. Say, you always place
1 long streaks are common in completely fair and random sequences           the resulting face (heads or tails)
   - so observing a streak of 7 tails does not contribute much to one's     initially up in a flip, and the you
   confidence that we are looking at a rigged coin or one that has          do not flip particularly vigorously.
   changed its probability properties.                                      Thus, the result of one flip would
                                                                            be related to the result of the next
2 the process of flipping a coin is independent each time, nearly by        flip. However, in nearly all real
   definition, and thus the result of one flip cannot influence the         cases, people go to great lengths to
   result of the next flip.1                                                avoid this sort of procedure.

   The faulty, but intuitive, reasoning goes by the name of the Gam-        2 A. Tversky and T. Gilovich. The
bler's Fallacy and appears in many places. We can ask a question:           cold facts about the" hot hand" in
                                                                            basketball. Anthology of statistics in
    How could we tell the difference between a random, independent          sports, 16:169, 2005
    sequence and one where the events are not independent, where the
    next flip depended on a previous flip?

We'll have to return to this question later, when we consider model
comparison, but roughly, one would have to look at all pairs of events
to see if one pair (say heads-tails) occurs more frequently (even if
only by a little) than another pair (say heads-heads).

   In a total fit of irony, casino slot machines do not produce indepen-
dent winnings - they are programmed so that if you've lost many
times, then that machine is a little less likely to lose the next time. In
effect, at gambling houses they train the gamblers in the Gambler's
Fallacy!

The Hot Hand - Correlations in Random Sequences

Some work by Tversky and Gilovich2 looks at the following issue in
the sport of basketball: there are times when it seems as if basketball
players have a "hot hand" - they are on a shooting streak. Tversky
and Gilovich looked at how basketball fans perceived streaks, by hav-
ing them rate sequences of shots as random shooting or streak shooting.
Most (65%) of the respondents classified artificially generated, purely
random sequences as streak shooting. In real data, they discovered that
the actual probability of "making a given shot (i.e. a player's shoot-
ing percentage) is unaffected by the player's prior performance." We
examine this effect in a later section (see Example 9.11 on page 172)
where we explore the quantitative procedure for assessing this con-
clusion. It is enough here to note the large difference between the
perception of the sequence and the likely cause of the sequence, and
thus the need to always be vigilant against faulty perceptions. Tver-
sky and Gilovich insist that "their observations do not tell us any-
thing general about sports, but it does suggest a generalization about
                            random sequences and visualization 85

people, namely that they tend to 'detect' patterns even where none
exist."

   What we have here, again, is the general perception that long se-
quences are somehow not "random," when in fact the opposite is the
case. People have a natural tendency to see patterns in random data,
to infer order where there is none, and to ascribe importance to the
appearance of pattern. It is the role of statistical inference in gen-
eral to provide the tools to properly handle the distinction between
random effects and patterns, and to retune our intuitions.

Regression Toward the Mean

There is a peculiar phenomenon referred to as regression toward the
mean, which often is misinterpreted and leads to failures of proper
statistical inference. It can be seen in a simple example. Imagine that
we "test" a number of students by having them guess the results of a
coin flip. Clearly this will be entirely luck, because the coin flip has
no pattern. If a student guesses the results of 50 flips, there will be an
expectation of getting 25 correct. Here we simulate 20 students each
"predicting" the result of 50 flips, the results shown in Table 3.1. The
test is done twice, and we will look at a particular subset presently.
One can, by eye, see that most of the students get around 25 correct -
exactly as expected from random performance.

   Now, imagine that we look at the top five coin flip predictors on
the first round. Will they do better or worse in the the second round?
What about the bottom five coin flip predictors? The results of these
two cases are summarized in Table 3.2. The pattern, even in this
small sample, is quite clear:

1 Those that did the best the first time did worse the second (on
   average)

2 Those that did the worst the first time did better the second (on
   average)

One might be tempted (had you not known that this is artificial data,

and completely random) to interpret this as a causal pattern, e.g.

"the students that did better the first time, grew over-confident the

second time," "the students that did worse the first time, worked

harder to improve the second time," etc... This interpretation of the

results by students has been observed in the classroom.3 However, it        3

runs into serious trouble when the data is something like the heights

of children compared to their parents - the tallest parents tend to

have children shorter than they are, the shortest parents tend to have

children taller than they are, a pattern first quantified by Galton in

18694. He noted that clearly the children are not trying to be tall, so     4
86 statistical inference for everyone

         Student  Total Correct  Total Cor-                  Table 3.1: Total Correct Guesses
                  First Round    rect Second                 from Students "Predicting" the
             1                   Round                       Results of 50 Coin Flips. Shown are
             2    23                                         the results of a first round and a
             3    23             24                          second round of guessing.
             4    19             29
             5    26             23
             6    28             27
             7    26             29
             8    23             22
             9    30             26
            10    24             28
            11    27             21
            12    25             23
            13    30             31
            14    20             21
            15    28             22
            16    24             29
            17    25             25
            18    23             22
            19    20             24
            20    20             28
                  28             29
                                 25

Top Five the First Time          Bottom Five the First Time  Table 3.2: Performance in the Sec-
                                                             ond Round of Students "Predicting"
Student  Performance the  Student      Performance the       the Results of 50 Coin Flips. Shown
         Second Time                   Second Time           are the results for those students
    8    Worse                3        Better                who performed best in the first
   12    Worse               13        Better                round (left), and those that per-
   14    Better              18        Better                formed worst in the first round
    5    Better              19        Better                (right).
   20    Worse                1        Better
                                                                            random sequences and visualization 87

effort is not a good explanation for the pattern.
   What is happening here is that, if the process is dominated by luck

or simple random variation, then outliers occur, but are rare. Thus
a particularly high value will likely be followed by a lower value -
closer to the mean. The tendency is to regress toward the mean in pro-
cesses dominated by luck. This can be confused with the Gambler's
Fallacy discussed earlier, where flipping 3 heads in a row doesn't
give you any information about flipping another heads - it is not
more likely to be tails. Part of the difference is that we are dealing
with a process that has many possible values, not just two, and thus we
can have a mean value, and outliers.

   When each of these ideas is applied to sports, the weather, or
business there are some interesting conclusions.

1 even when the process is entirely random, long streaks occur - and
   are often misinterpreted as an increase in the probability of the
   event.

2 when a person performs very well at their job (a number of suc-
   cessful business decisions, a high batting average, etc...) they will
   often do worse the next year - and again many are surprised, and
   interpret the result as the person "losing their touch" - when in
   fact, they may just have been lucky for a bit, and are now perform-
   ing closer to their typical average level.

3 when one has a particularly bad winter, it may be more likely that
   the next winter won't be quite do bad - due entirely to regression
   to the mean. It may, however, be part of a larger pattern (e.g. a
   large-scale climate oscillation, such as El Niño) and the probability
   of another bad winter might be higher. In order to tell the differ-
   ence, we need to construct reasonable models of the phenomena,
   test those models with predictions, and apply those models into
   the future. At each step, we need to be careful not to jump to the
   conclusion of the existence of a pattern too quickly.

3.4 Visualization of Data

There are two main methods of visualizing data, and several others
that are related to these methods. In this section we introduce just
two, histograms and scatter plots, and we will use these throughout
the text.

Histograms

Histograms are a way of summarizing data, when presenting the
entire data set is impractical, or where some understanding of the
88 statistical inference for everyone                                   Another advantage to learning
data is made clearer by summarizing. The histogram plot is done         to understand how to generate
with the following steps:                                               histograms is that it alerts you
1 Choose a number of bins to divide the data.                           to the possible abuses of these
                                                                        plots. These abuses can be simple
2 Count up the data that fall into each bin                             mistakes, which end up giving a
                                                                        misleading message, or a deliberate
                                                                        deception. Either way, a proper
                                                                        understanding of the process helps.

3 Make a bar plot, or a scatter plot to present the data.

The following is an example with a small data set. The process of

binning and counting is often done by computer, but it is instructive

to perform the process by hand a few times in order to understand

what the results are.

Table 3.3 shows a collection of 106 heights (in centimeters) of the

male students in a class5. As a collection of numbers it is relatively  5

opaque, but as a histogram it is clearer.

177.8   160.0          165.0   182.88      175.0           167.0        Table 3.3: 106 Male Student Heights
182.88  190.5          177.0   190.5       180.34          180.34       (in cm) from a Survey.
184.0   172.72         175.26  167.0       180.0           180.0
190.0   182.5          185.0   171.0       172.0           180.34
180.0   170.0          200.0   190.0       170.18          179.0
182.0   171.0          177.8   175.26      187.0           183.0
180.0   176.0          185.42  176.5       167.64          179.0
183.0   179.0          190.0   165.0       187.0           170.0
180.0   180.34         190.5   185.0       193.04          184.0
177.0   180.0          175.26  180.34      178.5           187.96
178.0   175.26         189.0   182.88      170.0           180.0
185.0   187.96         185.42  195.0       172.72          180.34
173.0   187.96         187.0   168.0       191.8           177.0
189.0   180.34         182.88  172.72      172.0           170.0
175.0   168.0          165.0   173.0       196.0           179.1
180.0   176.0          154.94  174.0       179.1           160.0
165.0   165.0          170.0   185.0       188.0           171.0
185.0   185.0          180.34  183.0
Number of People                                                            random sequences and visualization 89

          30
          25
          20
          15
          10
           5
           1050 160 170 180 190 200 210

                                      Height [cm]

From this histogram, we can immediately observe several quantities
which summarize their data:

1 The average value (around the middle) should be around 175 cm.
   The actual value can be calculated from the data, as

             x¯ = 177.8 + 160.0 + · · · + 180.34 + 183.0 = 178.83
                                             106

2 The range of the data is around 155 cm up to about 205 cm. Again
   we can be more precise, and find the minimum of the data (154.94
   cm) and the maximum (200 cm) but the histogram picture yields
   an approximate value instantly.

3 The values are roughly symmetric about the mean (i.e. average)
   value. This can give us a clue concerning how to model the data.

What is quite clear is that it is far easier to deal with a histogram, as
above, than find the same information from the table of numbers.

Too Few Bins Plotting the same histogram with too few bins might
look like:
Number of People90 statistical inference for everyone

Number of People90
          80
          70
          60
          50
          40
          30
          20
          10
           1040 150 160 170 180 190 200 210

                                      Height [cm]

   Clearly all the information is washed out.

Too Many Bins Plotting the same histogram with too many bins
might look like:

          16
          14
          12
          10
           8
           6
           4
           2
           1050 160 170 180 190 200 210

                                      Height [cm]

   We lose any of the summary information here, where we essen-
tially have one bar for each data-point.

Scatter Plots
A scatter plot is used to explore the relationship between two values.
For example, in the survey of male students, in addition to height the
students also measured the width of their writing hand viewed as a
histogram, here
Number of People                                                            random sequences and visualization 91

Writing Hand Span [cm]14
          12
          10
           8
           6
           4
           2
           016 17 18 19 20 21 22 23 24

                                 Writing Hand Span [cm]

   However, due to the possibility that these two variables could be
related, it makes more sense to make a scatter plot. In such a plot, one
designates one variable as "x" and another as "y," and places a single
dot for each pair of values in the data set. Thus, each dot on the plot
corresponds to height and hand-width for a single student.

          24
          23
          22
          21
          20
          19
          18
          17
          16
          11550 160 170 180 190 200 210

                                      Height [cm]

   What we can see here, which was obscured with a histogram, is
the relationship between these values - for the taller students, their
hands are wider. We will explore quantifying this relationship later,
but much can be done by eye using a scatter plot.
92 statistical inference for everyone

3.5 Computer Examples

This section summarizes how to make histograms and scatter plots
with the computer software.
Histograms
from s i e import *

   Load a sample data set, and select only the Male data...
data=load_data ( ' data/survey . csv ' )
male_data=data [ data [ ' Sex ' ]== ' Male ' ]

   select only the height data, and drop the missing data (na)...
male_height=male_data [ ' Height ' ] . dropna ( )

   make the histogram
hist ( male_height , bins =20)
x l a b e l ( ' Height [cm] ' )
y l a b e l ( 'Number o f People ' )

<matplotlib.text.Text at 0x1085728d0>

Scatter Plot
from s i e import *

   Load a sample data set, and select only the Male data...
                                                                            random sequences and visualization 93

data=load_data ( ' data/survey . csv ' )
male_data=data [ data [ ' Sex ' ]== ' Male ' ]

   select only the height and the width of writing hand data, and drop the missing data (na)...
subdata=male_data [ [ ' Height ' , 'Wr . Hnd ' ] ] . dropna ( )
height=subdata [ ' Height ' ]
wr_hand=subdata [ 'Wr . Hnd ' ]

   plot the data
p l o t ( height , wr_hand , ' o ' )
y l a b e l ( ' Writing Hand Span [cm] ' )
x l a b e l ( ' Height [cm] ' )

<matplotlib.text.Text at 0x1085774d0>
4 Introduction to Model Comparison

A model1 as we use the term in this book is a specific description of a      1 A similar term is hypothesis, and
possible state of nature. This is in contrast to an actual state of nature,  model comparison would then be
which we practically never have access to. We can never know any-            hypothesis testing. We don't choose
thing with 100% certainty, and must therefore be open to alternate           to use that term, partly because of
possible explanations, or models, describing our observations. For           the colloquial use of hypothesis as
example, in medicine such models could include "I have lung can-             a kind of "guess," but also because
cer," "I have pneumonia," and "I have a cold." In physics, models            hypothesis testing in some treat-
could include "the Earth moves around the Sun" and "the Sun moves            ments focus on true/false tests of
around the Earth." We can imagine many possible models that are              hypotheses which can lead to some
consistent with the observed data, and our job in doing statistical in-      significant misunderstandings. The
ference is to determine the probabilities of our models given the data       use of models implies the possibil-
we observe. In our notation, what we are always looking for is               ity of multiple (i.e. more than two)
                                                                             models.
P(model|data)  (4.1)
                                                                             2 Although we could make a game
We will explore model comparison through a series of examples.               without replacement, which may be
                                                                             simpler to implement, the version
4.1 The High/Low Deck Game                                                   of the game with reshuffling will
                                                                             help with an example later.
In this example we use a simple card game as a platform for dis-
cussing model comparison in general. We start with two atypical
decks of cards called the High Deck and the Low Deck (Figures 4.1
on page 96 and 4.2 on page 96 respectively). The game goes as fol-
lows.

    You're handed one of the two decks, but you don't know which. First,
    you draw the top card and note the value. Second, you replace the
    card and reshuffle the deck2. You repeat this procedure of drawing,
    noting, and reshuffling for as many turns as you need. The goal is to
    determine which of the the two decks (High or Low) you are in fact
    holding in your hand.

What does our intuition say?

We start by exploring our intuitions, before we do anything math-
ematically. Thus, we are in a position to check to see if the math is
96 statistical inference for everyone

                                                                                                                               Figure 4.1: High Deck - 55 Cards
                                                                                                                               with ten 10's, nine 9's, etc... down
                                                                                                                               to one Ace. Aces are equivalent to
                                                                                                                               the value 1.

                                                                                                                               Figure 4.2: Low Deck - 55 Cards
                                                                                                                               with ten Aces, nine 2's, etc... up to
                                                                                                                               one 10. Aces are equivalent to the
                                                                                                                               value 1.
introduction to model comparison 97

reasonable before we use the same math in areas where our intuition         3 The prior is sometimes mischar-
is not as strong. Imagine we draw only one card, and it is a 9. Intu-       acterized as simply our guess, or
ition suggests that this constitutes reasonably strong evidence toward      some other completely subjective
the belief that we're holding the High Deck. If we then (as the pro-        assessment of our knowledge. In
cedure states) place the 9 back in the deck, reshuffle and then draw        fact in this example, and many
a 7 we can be more strongly convinced that we are holding the High          others, we can make the positive
Deck. Repeating the reshuffle, and then drawing a 3 would make              case for equal probabilities given
us a little less confident in this conclusion, but still quite certain. In  the state of our knowledge. This
this way we can sense how drawing different cards pushes our belief         can be quantified with the concept
around, depending on how often that card comes up in the different          of entropy, which is beyond this
decks.                                                                      chapter.

Before the data - the prior

Before we take any data, we need to quantify our state of knowledge
concerning all of the models that we are considering. In this case it
is quite simple, because there are two models (High Deck and Low
Deck), and we have been given no information about whether either
is more common. With no such information, it is equivalent to a coin
flip - we assign equal probabilities to both models before we see data,
also known as the prior probabilities3.

                                    P(H) = 0.5
                                    P(L) = 0.5

Surely this assessment will change after we see data, but that is the
rest of the problem.

The "easy" question - the likelihood

Although our ultimate goal is to infer the type of deck from the cards
that we draw from it, we can start looking at an easier part of this
question which serves as a first step toward the more challenging,
and interesting goal. That question is the following,

Example 4.1 What is the probability of drawing a 9, given that we know
that we're holding the High Deck?

This related question is written

                                 P(data = 9|H)

where data = 9 means that we have observed (i.e. drawn) one 9.
This question is "easy" in the sense that it is simply related to the
properties of the High Deck: the number of 9's and total number
of cards. If you know that you have the high deck, then you know
there are nine 9's in that deck out of 55 cards, and thus we have the
98 statistical inference for everyone

probability of drawing one 9, given that we are holding the High          4 The term likelihood is a poorly
Deck, is                                                                  chosen word. In English, this word
                                                                          is nearly synonymous with the
                             P(data = 9|H) = 9                            word probability and thus easily
                                                      55                  leads to confusion. We could try to
                                                                          use a different term, like consequent
We give this the name likelihood4, and is simply the probability that     probability or generative likelihood
the data could be the result of a known model. It is also the first part  to stress the idea that the likelihood
of the top of Bayes' Rule, Equation 1.14 on page 47.                      is the probability that the data we
                                                                          observe could be generated or could
Applying the Bayes' recipe                                                be a consequence of the particular
                                                                          model. However, we'd be going up
Here we introduce for the first time a recipe we will follow for all      against two centuries of continued
model comparison examples.                                                use of the term likelihood and thus
                                                                          would probably increase confusion
   Now that we have our intuition, and we have the likelihoods, we        rather than decrease it.
can address the math. The two models are:

                   H  "We're holding the High Deck"
                    L  "We're holding the Low Deck"

and the initial data is

              data  "We've drawn one card, and it is a 9"

According to Equation 4.1 on page 95 we are looking for the two
probabilities:

                                 P(H|data = 9)
                                 P(L|data = 9)

which are related to the prior and the likelihood via Bayes' Rule (Equa-
tion 1.14):

                  P(H|data = 9) = P(data = 9|H)P(H)
                                                   P(data = 9)

                   P(L|data = 9) = P(data = 9|L)P(L)
                                                  P(data = 9)

   To calculate actual numbers, we apply the Bayes' Recipe to this
problem,

1 Specify the prior probabilities for the models being considered

                                     P(H) = 0.5
                                      P(L) = 0.5

2 Write the top of Bayes' Rule for all models being considered

                    P(H|data = 9)  P(data = 9|H)P(H)
                     P(L|data = 9)  P(data = 9|L)P(L)
                                               introduction to model comparison 99

   where we are using the symbol  to denote proportionality or re-
   lated to. Essentially, by calculating the top of Bayes' Rule first, the
   numbers are not equal to the final (i.e. posterior) probabilities but
   must be rescaled to make sure that they add up to 1. This is done
   in the final step. Up until that rescaling, we use the symbol  and
   think of it as related to.

3 Put in the likelihood and prior values

                      P(H|data = 9)  9 × 0.5 = 0.082
                                                    55

                       P(L|data = 9)  2 × 0.5 = 0.018
                                                    55

4 Add these values for all models

                             K = 0.082 + 0.018 = 0.1

5 Divide each of the values by this sum, K, to get the final probabili-
   ties

                        P(H|data) = 0.082/0.1 = 0.82
                         P(L|data) = 0.018/0.1 = 0.18

   From which we can conclude that drawing a 9 does indeed consti-
tute reasonably strong evidence toward the belief that we're holding
the High Deck - the probability of us holding the High Deck, given
the data, is 0.82.

Drawing the next card

So, when we draw a 7 next (after reshuffling), our intuition suggests
that we'd be more confident that we're holding the High Deck. Re-
peating our recipe we have

   The two models are:

             H  "We're holding the High Deck"
             L  "We're holding the Low Deck"

and data is  "We've drawn one card, and it is a 9, replaced
     data    and reshuffled, and then drawn a 7"

According to Equation 4.1 we are looking for the two probabilities:

                           P(H|data = 9 then a 7)
                            P(L|data = 9 then a 7)
100 statistical inference for everyone

which are related to the prior and the likelihood via Bayes' Rule (Equa-
tion 1.14):

       P(H|data = 9 then a 7) = P(data = 9 then a 7|H)P(H)
                                                   P(data = 9 then a 7)

        P(L|data = 9 then a 7) = P(data = 9 then a 7|L)P(L)
                                                  P(data = 9 then a 7)

   To calculate actual numbers, we apply the Bayes' recipe to this
problem,

1 Specify the prior probabilities for the models being considered

                                     P(H) = 0.5
                                      P(L) = 0.5

2 Write the top of Bayes' Rule for all models being considered

         P(H|data = 9 then a 7)  P(data = 9 then a 7|H)P(H)
         P(L|data = 9 then a 7)  P(data = 9 then a 7|L)P(L)

3 Put in the likelihood and prior values                                  As a reminder, we are performing

                                                                          this next draw having shuffled

            P(H|data = 9 then a 7)    9 × 7 × 0.5 = 0.0104                the first draw back into the deck.
             P(L|data = 9 then a 7)   55 55
4 Add these values for all models     2 × 4 × 0.5 = 0.0013                Although somewhat artificial, it is
                                      55 55
                                                                          useful for a later example. If we

                                                                          had simply set the first card asside,

                                                                          the value of the likelihood would

                                                                          account for the removal of one more

                                                                          card,  and  would  thus   be  9   ×   7   for
                                                                                                        55      54
                                                                                                2      4
K = 0.0104 + 0.0013 = 0.0117                                              the  high  deck  and  55  ×  54  for  the

                                                                          low deck. Note the denominators.

5 Divide each of the values by this sum, K, to get the final probabili-
   ties

P(H|data = 9 then a 7) = 0.0104/0.0117 = 0.889
P(L|data = 9 then a 7) = 0.0013/0.0117 = 0.111

   which again matches our intuition - we're more confident that
we're holding the High Deck, now with probability 0.889 increased
from 0.82 when we just observed the 9.

Prior information or not?

In the above example, we started with a prior probability of holding
the High Deck at P(H) = 0.5, because we had no information other
than that there were two possibilities. We then observed a 9, and
updated the probability to 0.82, and then observed a 7, and further
updated the probability to 0.889 - making it more likely that we were
                                                  introduction to model comparison 101

holding the High Deck. One of the basic tenets of probability the-     5 E. T. Jaynes uses the principle that
ory is that if there is more than one way to arrive at an answer, one  "if there is more than one way to
should arrive at the same answer.5 In the above, we calculated the     arrive at an answer, one should
probability of holding the High Deck given the observed data           arrive at the same answer" to help
                                                                       derive the rules of probability
data   "We've drawn one card, and it is a 9, replaced                  from first principles. Failures of
       and reshuffled, and then drawn a 7"                             this principle result in paradoxes.
                                                                       This principle is also applied
and prior information                                                  in Section 2.4 for the birthday
      prior  "We know there are only two decks."                       problem.

   An equivalent situation is found after our first draw, after we've
observed the 9, and we're about to draw our second card. In this case
we have the prior information:

                    
                    "We know there are only two decks, and then
    prior  we draw one card and it is a 9, replace it and

                       reshuffle."

and observed data:

      data  "We've drawn one card and it is a 7"

   Mathematically, we apply the Bayes' recipe, but with the different
prior information

1 Specify the prior probabilities for the models being considered

                                   P(H, 9) = 0.82
                                    P(L, 9) = 0.18

2 Write the top of Bayes' Rule for all models being considered

             P(H|data = 9 then a 7)  P(data = 7|H)P(H, 9)
              P(L|data = 9 then a 7)  P(data = 7|L)P(L, 9)

3 Put in the likelihood and prior values  7 × 0.82 = 0.104
                P(H|data = 9 then a 7)    55
                 P(L|data = 9 then a 7)   4 × 0.18 = 0.013
                                          55

4 Add these values for all models

       K = 0.104 + 0.013 = 0.117
102 statistical inference for everyone

5 Divide each of the values by this sum, K, to get the final probabili-
   ties

              P(H|data = 9 then a 7) = 0.104/0.117 = 0.889
               P(L|data = 9 then a 7) = 0.013/0.117 = 0.111

yielding the same result.
   In other words our updated probabilities from the first draw can be

seen as our prior probabilities for the subsequent draws. Thus, Bayes'
Rule describes how we update our knowledge with new evidence, or
in other words, learning.

4.2 Multiple Hypotheses

We start this section with an example.

Example 4.2 What is the probability that you are holding one of either the
High or the Low Deck having drawn five 9's in a row from that deck?

   We have observed the following data:
                    
                    "We've drawn one card, and it is a 9, replaced
                    

     data  and reshuffled, redrawn and observed another
                    9, repeated this procedure and observed three
                    
                      more 9's, for a total of five 9's in a row."

Technically, drawing 5 9's in a row should give us really strong con-
fidence that you are drawing from the High Deck, because we would
have

1 Specify the prior probabilities for the models being considered

                                     P(H) = 0.5
                                      P(L) = 0.5

2 Write the top of Bayes' Rule for all models being considered
    P(H|data = 5 9's in a row)  P(data = 5 9's in a row|H)P(H)
     P(L|data = 5 9's in a row)  P(data = 5 9's in a row|L)P(L)

3 Put in the likelihood and prior values

P(H|data = 5 9's in a row)  9 × 9 · · · 9 ×P(H)
                                             55 55 55

                                          5 times

                          9 5 × 0.5
                                 55

                         = 0.0000587

P(L|data = 5 9's in a row)                2 5 × 0.5
                                          55

                         = 0.0000000318
                                                           introduction to model comparison 103

4 Add these values for all models
              K = 0.0000587 + 0.0000000318 = 0.0000587318

5 Divide each of the values by this sum, K, to get the final probabili-
   ties
          P(H|data = 5 9's in a row) = 0.0000587 = 0.99946
                                                       0.0000587318
          P(L|data = 5 9's in a row) = 0.0000000318 = 0.00054
                                                       0.0000587318

which is fantastically on the side of the high deck, even though we
might start getting suspicious in this situation.

Example 4.3 What is the probability that you are holding one of either the
High or the Low Deck having drawn m 9's in a row from that deck, where m
stands for a number (m = 1, 2, 3, · · ·)?

   In general, if we look at m 9's in a row, where m could be 1, 2, 3,
etc..., we can see this following the Bayes' Recipe

1 Specify the prior probabilities for the models being considered

                                     P(H) = 0.5
                                      P(L) = 0.5

2 Write the top of Bayes' Rule for all models being considered

   P(H|data = m 9's in a row)  P(data = m 9's in a row|H)P(H)
    P(L|data = m 9's in a row)  P(data = m 9's in a row|L)P(L)

3 Put in the likelihood and prior values

P(H|data = m 9's in a row)  9 × 9 · · · 9 ×P(H)
                                              55 55 55

                                            m times
P(L|data = m 9's in a row)                9 m × 0.5
                                          55

                                          2 m × 0.5
                                          55

4 Add these values for all models

K = 9 m × 0.5 + 2 m × 0.5
55                                 55

5 Divide each of the values by this sum, K, to get the final proba-
   bilities This step is easiest done in a table (Table 4.1), because the
   resulting expression is pretty messy.
104 statistical inference for everyone

m P(H|data) P(L|data)                                                       Table 4.1: Drawing m 9's in a row,
                                                                            from either a High Deck or Low
1 0.81818  0.18182                                                          Deck.

2 0.95294  0.047059

3 0.98915  0.010855

4 0.99757  0.0024327

5 0.99946 0.00054163

6 0.99988 0.00012041

7 0.99997 0.000026761

8 0.99999 0.0000059470

   It is clear from Table 4.1 that after drawing five 9's using our pro-
cedure, it should be extraordinarily likely that we are holding the
High Deck. However, after a certain number of 9's observed, some-
thing starts to bother us. Perhaps not after five 9's, but what if the
procedure were repeated and we drew ten 9's in a row? Or perhaps
twenty 9's. At some point, we'd refuse to believe this is the High
Deck because, although it was true that there are more 9's in the
High Deck, there are many more other cards in the High Deck that we
should see. What do we do in this case?

Example 4.4 What is the probability that you are holding one of either the
High, Low, or Nines Deck having drawn m 9's in a row from that deck?

   The proper thing to do is to introduce a new model, say, a Nines         What is interesting here is that
deck. Clearly this model should have a very low prior probability,          once we admit that there are many
because we didn't even consider it before we saw the streak of 9's.         possible models we could consider,
Let's say that we assign the prior probability for the Nines deck to        we realize that we have these
be a one in a million. To make all of the prior probabilities add up to     models in our head all the time,
1, then the prior probabilities for the High and Low Deck must be a         or we construct them as we need
little less than 0.5. After that, we simply apply the Bayes' Recipe as      them. Every model comparison
before                                                                      is a multiple model comparison,
                                                                            with most of the models with very
1 Specify the prior probabilities for the models being considered           low prior probabilities that our
                                                                            brain naturally suppresses until
                                                                            needed. Mathematically, we need to
                                                                            unsuppress them as needed.

P(N) =     1 = 0.000001
        1, 000, 000

P(H) = 0.4999995

P(L) = 0.4999995

2 Write the top of Bayes' Rule for all models being considered

   P(N|data = m 9's in a row)  P(data = m 9's in a row|N)P(N)
   P(H|data = m 9's in a row)  P(data = m 9's in a row|H)P(H)
    P(L|data = m 9's in a row)  P(data = m 9's in a row|L)P(L)
                                                                             introduction to model comparison 105

3 Put in the likelihood and prior values

                             P(N|data = m 9's in a row)  1 × P(N) = 0.000001
                             P(H|data = m 9's in a row)  9 × 9 · · · 9 ×P(H)

                                                                           55 55 55

                                                                           m times
                                  P(L|data = m 9's in a row)             9 m × 0.4999995
                                                                         55

                                                                         2 m × 0.0.4999995
                                                                         55

4 Add these values for all models

K = 0.000001 + 9 m × 0.4999995 + 2 m × 0.4999995
                                  55                                     55

5 Divide each of the values by this sum, K, to get the final proba-
   bilities Again, this step is easiest done in a table or, even better, a
   picture (Figure 4.3).

                             1.0                                                            Figure 4.3: Drawing a number of
                                                                                            9's in a row, possibly from a High,
                                                                                            Low, and Nines deck.

Model Posterior Probability  0.8

                           0.6                                           Low Deck

                                                                         High Deck
                                                                         Nines Deck
                           0.4

                             0.2

                             0.00 2 4 6 8 10 12 14
                                     Number of 9's in Drawn in a Row

   We have a clear picture here in Figure 4.3. As we initially draw
9's, our confidence that we're holding the High Deck goes up, at
the expense of our confidence that we're holding the Low Deck. At
a certain point (around six 9's in our example), our confidence in
the High Deck starts to drop and we become more confident that
something odd is happening, and our previously ignored model of
106 statistical inference for everyone

the Nines deck becomes more likely. Eventually, this new model is               The creative part of science is not
the one in which we are the most confident.                                     in the calculations performed, but
                                                                                in the generation of new and useful
   Imagine further that if, after drawing ten 9's in a row we draw a 1.         models. Until we come up with a
What do we do then? The likelihood for the Nines deck goes to zero              better model for our data we make
instantly - the probability of drawing a 1 from a Nines deck is zero,           do with the ones that we have, all
P(1|N) = 0. Are we left again with the original two models, High                the while being aware that a better
and Low Deck? No! We would then introduce other models, perhaps                 model may come into play later.
something like a Mostly Nines Deck, or perhaps a High Deck with a               Newton's Theory of Gravity was
weird shuffling procedure, or perhaps others. No matter how many                used for over 200 years, even when
models one has, the recipe is still the same. It is important to realize        there was known data that made
that in any model comparison case, there are always other models                it less likely, until it was replaced
that could be brought to bear on the problem, perhaps with low prior            by Einstein's Theory of Gravity.
probability. Simply showing that a model is consistent with a set of            Newton's Laws, however, are still
data does not insure against the possibility that another model could           used in nearly all gravitational
be better, if we could only think of it.                                        calculations because it is "good
                                                                                enough" and is a lot easier to work
Exercise 4.1 Complete the example demonstrating the updated probabil-           with practically.
ities for the High and Low Deck, having drawn a 9, 7, and a 3. Compare
with the case of drawing just the 9 and the 7, and discuss how it matches
your intuition.

Exercise 4.2 Repeat the analysis of the sequence of 9's drawn in a row
with an added hypothesis of a deck with one hundred 9's and one 8. Discuss
the results. Demonstrate what happens to the probabilities for all of the
hypotheses after drawing one 8, after ten 9's in a row. Discuss.

Exercise 4.3 I tell you that I have a coin that could have both sides heads,
both sides tails, or a normal single-heads single-tails coin.

1 Before seeing the data, what would be a reasonable prior probability for
   the three hypotheses H0 (no-heads), H1 (one head), and H2 (two heads)?

2 Would this have been different if you had simply been given a coin by a
   friend to flip to see who has to do the dishes? Why or why not?

3 Now I flip the coin once, and get a heads. Write down the likelihood of
   this data given each of the models. In other words, what are the values of:

   · P (data=1 heads|H0)
   · P (data=1 heads|H1)
   · P (data=1 heads|H2)

4 Apply Bayes' Recipe, and determine the probability of each of these three
   models given this data. In other words, what are the values of:

   · P (H0|data=1 heads)
   · P (H1|data=1 heads)
                                                                             introduction to model comparison 107

   · P (H2|data=1 heads)
5 Apply this recipe for the case of observing 3 heads in a row.
5 Applications of Model Comparison

This chapter presents several applications of the model comparison
concepts introduced in Chapter 4 (Introduction to Model Comparison).

5.1 Disease Testing

Let's imagine there is a rare, one in a million, disease that is lethal
but does not have many outward symptoms at first. A new test
boasts 99.9% accuracy, so you go to get tested, and receive the bad
news that you test positive for the disease. Should you be devastated
by the news? What is the probability that you actually have the dis-
ease? We are looking at two, quite different, probabilities here. In the
first case, we have the claims of the test which state that if you have the
disease, the probability that the test will be positive is 0.999, or, if you have
the disease, test will discover that fact 99.9% of the time. In the second
case we have your concern which is, if you test positive for the test, what
is the probability that you have the disease. In our notation this is:

           P(positive test|disease) = 0.999 (claim from test)
           P(disease|positive test) = ? (your concern)

   These two are related by Bayes' Rule (Equation 1.14).
   The Bayes' Recipe proceeds as follows

1 Specify the prior probabilities for the models being considered
   The models we have are simply "have the disease" and "don't
   have the disease". The prior probabilities for these two come from
   the prevalence of the disease in the population, before you get
   tested. Since this is a "one in a million" disease, we have

                               P (disease) = 1
                                                        1, 000, 000

                           P (no disease) = 999, 999
                                                        1, 000, 000

2 Write the top of Bayes' Rule for all models being considered
110 statistical inference for everyone

The top of Bayes' Rule comes down to, given the truth of the             In many medical applica-
model (i.e. either with or without the disease), what is the proba-      tions, the false positive rate
bility of getting the data (i.e. the positive or negative test result).  (P (positive test|no disease)) is not
This is measured by how good the test is.                                always equal to the false negative
                                                                         rate (P (negative test|disease)), so to
                     P (positive test|disease) = 0.999                   say that a test is 99.9% accurate is
and                                                                      actually incomplete - one needs to
                                                                         specify both rates of effectiveness.
                  P (positive test|no disease) = 0.001                   In this case, we are assuming that
   So the top of Bayes' Rule looks for both models looks like:           they are the same.

       P (disease|positive test)  P (positive test|disease) × P (disease)
                                          0.999 × 1 = 9.99 · 10-7
                                                           1, 000, 000

   P (no disease|positive test)  P (positive test|no disease) × P (no disease)
                                          0.001 × 999, 999 = 9.99 · 10-4
                                                           1, 000, 000

3 Add these values for all models

                K = 9.99 · 10-7 + 9.99 · 10-4 = 0.000999999

4 Divide each of the values by this sum, K, to get the final probabili-
   ties

                P (disease|positive test) = 9.99 · 10-7 = 0.1%
                                                          0.000999999

            P (no disease|positive test) = 99.9%

   Which means that, overwhelmingly, if you have a rare one-in-a-
million disease, you are very unlikely to have it even given a 99.9%
accurate positive test for it! This is a seriously unintuitive result, so it is
helpful to visualize it in another way to build your intuition.

   One way to see this result is to visualize it, as in Figure 5.1. Here,
the numbers are a bit smaller - the disease is 1 out of 200 in a popu-
lation of 3000, and the test is 99% accurate. This means about 15 sick
people and about 2985 healthy people. If all of the sick people test
positive, and 1% of the healthy people test positive due to the 99%
accuracy, we would have 15 sick and 29 healthy people who all test
positive. Even in this case, with much smaller numbers, we see that
getting a positive test alone does not imply that it is likely you have
the disease. It depends on the rarity of the disease (the more rare, the
applications of model comparison 111

less likely) and the false positive rate (the number of healthy people
who test positive anyway). This will vary depending on the disease
and the test, but can lead to this unintuitive result, and thus can lead
one to make poor medical decisions.

                                                                               Has the Disease
                                                                               Test Positive

Consequences                                                                   Figure 5.1: Rare disease and testing.
                                                                               Shown is a population of 3000
This sort of disease testing has serious consequences, especially for          where 1 in every 200 people have
rare diseases with tests that aren't precise. In the book "The Theory          the disease (large circles). A test
That Would Not Die: How Bayes' Rule Cracked the Enigma Code,                   which is 99% effective is applied
Hunted Down Russian Submarines, and Emerged Triumphant from                    to everyone in the population, and
Two Centuries of Controversy" by Sharon McGrayne there is a dis-               the positive test results (i.e. the
cussion concerning the 2009 advice from the U.S. government task               test says that you have the disease)
force that "most women in their forties not to have annual mammo-              are shown ask small black dots.
grams." (emphasis mine) According to McGrayne,                                 Notice that although nearly all of
                                                                               those that have the disease test
    Thus the probability that a woman who tests positive has breast cancer     positive (a small black dot inside
    is only 3%. She has 97 chances out of 100 to be disease free. None of      a large circle), there are many
    this is static. Each time more research data become available, Bayes'      false positives (black dot in an
    rule should be recalculated. As far as Bayes is concerned, universal       empty square) - healthy people
    screening for a disease that affects only 4/10 of 1% of the population     that test positive for the disease.
    may subject many healthy women to needless worry and to additional treat-  Even though the test is quite good,
    ment which in turn can cause its own medical problems. In addition, the    there are many more healthy people
    money spent on universal screening could potentially be used for other     and 1 out of 100 of them will
    worthwhile projects. Thus Bayes highlights the importance of improv-       erroneously test positive.
112 statistical inference for everyone

    ing breast cancer screening techniques and reducing the number of    1 Sharon McGrayne. The Theory
    false positives.1 (emphasis mine)                                    That Would Not Die: How Bayes'
                                                                         Rule Cracked the Enigma Code,
   Thus the proper application of probability theory allows us to        Hunted Down Russian Submarines,
separate true but unintuitive things from this which only seem true      and Emerged Triumphant from Two
and intuitive but are in fact false.                                     Centuries of Controversy. Yale
                                                                         University Press, 2011. ISBN
                                                                         0300169698

5.2 M&M's

From various sources I have found the fraction of chocolate M&Ms
candies are red. The sources found are the following:
· Source A: 28% of M&Ms are red, 20% of M&Ms are orange.
· Source B: 20% of M&Ms are red, 10% of M&Ms are orange
· Source C: 13% of M&Ms are red, 21% of M&Ms are orange.

   From actually counting of a bag of M&Ms I found the following
data:
· 3 red M&Ms in 17 total (R = 3, N = 17)

   The question is, which source can we trust the most? Here we follow
Bayes' recipe,
· Specify the prior probabilities for the models being considered

                          P(A) = P(B) = P(C) = 1/3

· Write the top of Bayes' Rule (i.e. likelihood × prior) for all models
   being considered

P(A|R = 3, N = 17)   17 0.283(1 - 0.28)17-3 × 1
                     3                                 3

P(B|R = 3, N = 17)   17 0.203(1 - 0.20)17-3 × 1
                     3                                 3

P(C|R = 3, N = 17)   17 0.133(1 - 0.13)17-3 × 1
                     3                                 3

· Add these values for all models, to get K   0.05006
                         P(A|R = 3, N = 17)   +
                         P(B|R = 3, N = 17)   0.07975
                         P(C|R = 3, N = 17)   +
                                              0.07087

                     K = 0.20068
                                                        applications of model comparison 113

· Divide each of the values by this sum, K, to get the final probabili-
   ties

              P(A|R = 3, N = 17) = 0.05006/0.20068 = 0.250
              P(B|R = 3, N = 17) = 0.07975/0.20068 = 0.397
              P(C|R = 3, N = 17) = 0.07087/0.20068 = 0.353

   So we are most confident in Source B, although none of them
really changed by a lot - there is no clear winner.

Updating with other data

· 5 orange M&Ms in 16 total (G = 5, N = 16)

   Again, we follow the same recipe, starting with out posterior prob-
abilities from above as our starting prior probabilities - they are prior
to the new data.

· Specify the prior probabilities for the models being considered

                  P(A|old data) = 0.250
                  P(B|old data) = 0.07975/0.20068 = 0.397
                  P(C|old data) = 0.07087/0.20068 = 0.353

· Write the top of Bayes' Rule (i.e. likelihood × prior) for all models
   being considered

P(A|G = 5, N = 16 and old data)   16 0.205(1 - 0.20)16-5 × 0.250
P(B|G = 5, N = 16 and old data)   5
P(C|G = 5, N = 16 and old data)   16 0.105(1 - 0.10)16-5 × 0.397
                                  5
                                  16 0.215(1 - 0.21)16-5 × 0.353
                                  5

· Add these values for all models, to get K

                               P(A|data)  0.0300
                                                     +

                                P(B|data)  0.00544
                                                     +

                                P(C|data)  0.0471

K = 0.08254
114 statistical inference for everyone

· Divide each of the values by this sum, K, to get the final probabili-
   ties

                    P(A|data) = 0.0300/0.08254 = 0.363                   2 Paul the octopus, July 2012. URL
                    P(B|data)  0.00544/0.08254 = 0.0659
                    P(C|data)  0.0471/0.08254 = 0.5706                   http://en.wikipedia.org/wiki/
                                                                         Psychic_octopus
   Given this new data, we update our state of knowledge, and we're
much more confident that Source C is the best one. It is clear that      3 The basic procedure for Paul to
Source B is unlikely, with a probability of only about 6.5%. We could    make a "prediction" was for his
extend this example with more data, and more models if we'd like.        trainers to present two food dishes,
                                                                         labeled with a flag representing
5.3 Psychic Octopi                                                       the two countries, respectively,
                                                                         competing. Whichever food dish
There was a German octopus named Paul2 who was claimed to be             Paul chose first was his prediction
psychic during his lifetime. He was given this designation because       for the winner of the game.
he was supposedly able to pick the result of World Cup matches
before they occurred3. His impressive results, across 2 years, shown
in Figure 5.2 can be summarized as follows:

                  data  12 out of 14 correctly predicted

   The question we have to ask is, is this data strong evidence for a
psychic octopus? In order to have a well-posed problem we need the
following three components:

1 a set of hypotheses, or models, to compare - we need at least two,
   otherwise the question is meaningless

2 for each model, an equation denoting the likelihood, or in other
   words, how probable is the data given the particular model

3 a specification of the prior probability, or in other words, how
   likely was our model before we saw the data

Making a Well Posed Problem
We are interested in the probability of this octopus being psychic,
given this data, or

                                P(psychic|data)

which really is an example of a model comparison, or hypothesis
testing. In any kind of model comparison, we need to have multiple
applications of model comparison 115

                            Figure 5.2: The full results
                            of the predictions of Paul
                            the Octopus, reproduced from
                            en.wikipedia.org/wiki/Psychic_octopus.

models to compare to in order to proceed. The models we consider         4 J. Randi. Flim-flam!: psychics, ESP,
constrain the problem, and define which ideas we are willing to          unicorns, and other delusions, volume
consider. To be specific, as a first step, let's consider the following  342. Prometheus Books Amherst,
two models                                                               NY, 1982

          H := {Paul is psychic}
           R := {Paul is completely random, like a coin flip}

   The next step is to be able to assign probabilities from these mod-
els. It is easy for the random hypothesis

                         P(correct prediction|R) = 0.5
                       P(incorrect prediction|R) = 0.5

   What does it mean to be psychic? What is the probability of get-
ting a correct result if you are psychic? According to James Randi4
many of the psychics and dowsers claim 100% accuracy in their pre-
dictions before they are tested. However this would mean a single
wrong answer would drive the probability of that model to zero: a
perfect predictor cannot, logically, make any mistakes. For our case
here, we choose to be generous to the psychic and allow for a reason-
able failure rate, using 90% as our accuracy, thus

                         P(correct prediction|H) = 0.9
                      P(incorrect prediction|H) = 0.1
116 statistical inference for everyone

   Specifying the prior probability of these two models is a bit more    It is possible that we could be
challenging. It seems reasonable to assign a small prior probability to  accused of an anti-psychic bias
a psychic octopus - how many psychic octopi have you ever encoun-        here, especially from someone who
tered? A small, but still quite conservative value, would be 1/100, so   is a true believer. Why shouldn't
we have for the two models:                                              the prior be P(H) = 1/2? If you
                                                                         had no world experience, that is
                                 P(H) = 1/100                            what you'd start with, but then the
                                 P(R) = 99/100                           behavior of the first octopi that you
                                                                         encounter would generally lower
The First Model Comparison                                               your assignment of the probability
                                                                         of the next octopi being psychic.
Now that we've set up the problem, we can apply the Bayes' Recipe        After enough world experience,
1 Specify the prior probabilities for the models being considered        updating your probability with
                                                                         Bayes' Rule, you'd arrive at a very
                                   P(H) = 1/100                          small prior for Paul, the current
                                   P(R) = 99/100                         octopus we are examining.

2 Write the top of Bayes' Rule for all models being considered

P(H|data = 12 out of 14)  P(data = 12 out of 14|H)P(H)
P(R|data = 12 out of 14)  P(data = 12 out of 14|R)P(R)

where we are using the symbol  to denote proportionality or re-
lated to. Essentially, by calculating the top of Bayes' Rule first, the
numbers are not equal to the final (i.e. posterior) probabilities but
must be rescaled to make sure that they add up to 1. This is done
in the final step. Up until that rescaling, we use the symbol  and
think of it as related to.

3 Put in the likelihood and prior values

P(H|data = 12 out of 14)                  14 0.9120.114-12 × 1
                                          12  100

                            = 0.00257

P(R|data = 12 out of 14)                  14 0.5120.514-12 × 99
                                          12  100

                            = 0.00549

4 Add these values for all models

                      K = 0.00257 + 0.00549 = 0.00806

5 Divide each of the values by this sum, K, to get the final probabili-
   ties
                         P(H|data) = 0.00257 = 0.32
                                             0.00806
                          P(R|data) = 0.00549 = 0.68
                                             0.00806
                                                                              applications of model comparison 117

and the psychic loses! We continue this problem discussing the po-
tential anti-psychic bias in the presentation of the problem.

Furthering the Comparison

Typically, a person who is supportive of psychic phenomena would
choose a prior for our psychic hypothesis (H) that would be at least
as large as the prior for the random hypothesis (R). In this case, the
(posterior) probability of the octopus being psychic given the data
of 12 correct out of 14 would be much higher. After "ruling out" the
random octopus hypothesis, we'd be left with psychic. But is that all
that is really left? No, and the analysis is easy to do.

   Once presented with the success of Paul, most people instantly
are suspicious of random octopus, but don't adopt psychic octopus
as the answer. Perhaps the keepers, being German, biased the data
taking a little bit. Perhaps the octopus chose flags with bright yellow
stripes. Notice that each of these cases still results in similar data -
the octopus would have gotten 11 or 12 out of 14, but the prior prob-
ability of these cases should be much higher than psychic, even if
lower than random. We leave it as an exercise to perform the calcula-
tion in this case, but it is directly parallel to the Nines deck example
of Section 4.2 on page 102.

5.4 Monty Hall Problem

This problem was introduced in Section 2.6.

Example 5.1 Is it better to switch doors? - Monty Hall Problem revisited

   You may recall that we were presented with a choice of 3 doors
where a car is behind one and goats behind the others. Having
picked one, the host opens up a door with a goat, and offers you
the opportunity to change your answer. In order to assess the proba-
bilities, we must remember that

1 the host never opens your door

2 the host always opens a door with a goat

   We'll go through a specific example, that of you choosing door 1
and the host opening door 2. The analysis proceeds in identical ways
for the other possibilities. We apply the Bayes' Recipe, where the
models under consideration are

· "car behind door 1"

· "car behind door 2"
118 statistical inference for everyone

· "car behind door 3"

   The Bayes' Recipe proceeds as follows

1 Specify the prior probabilities for the models being considered

                              P (car 1|you 1) = 0.333
                              P (car 2|you 1) = 0.333
                              P (car 3|you 1) = 0.333

   where, for example, P (car 1|you 1) represents the probability that
   the door contains the car given that you chose door 1. Since your
   choice of door doesn't add any information about the location of
   the car, all of the probabilities are equal.

2 Write the top of Bayes' Rule for all models being considered

    P (car 1|you 1, host 2)  P (host 2|you 1, car 1) P (car 1|you 1)
    P (car 2|you 1, host 2)  P (host 2|you 1, car 2) P (car 2|you 1)
    P (car 3|you 1, host 2)  P (host 2|you 1, car 3) P (car 3|you 1)

3 Put in the likelihood and prior values
   Due the restrictions on the host above, the host cannot open a
   door with a car, so P (host 2|you 1, car 2) = 0. In the case where
   you choose door 1 and the car is also behind door, the host has the
   freedom to choose either door 2 or door 3, so P (host 2|you 1, car 1) =
   0.5. Where the information comes in is when the car is behind
   door 3 and you've chosen door 1. In that case, the host cannot
   open your door (door 1) or the door with the car (door 3) and must
   open door 2. Thus, P (host 2|you 1, car 3) = 1.
   The final result of this step is

                      P (car 1|you 1, host 2)  0.5 · 0.333
                      P (car 2|you 1, host 2)  0 · 0.333
                      P (car 3|you 1, host 2)  1 · 0.333

4 Add these values for all models

                        K = 0.5 · 0.333 + 1 · 0.333 = 0.5

5 Divide each of the values by this sum, K, to get the final probabili-
   ties
                 P (car 1|you 1, host 2) = 0.5 · 0.333 = 0.333
                                                            0.5
                 P (car 2|you 1, host 2) = 0 · 0.333 = 0
                                                           0.5
                 P (car 3|you 1, host 2) = 1 · 0.333 = 0.666
                                                           0.5
                                        applications of model comparison 119

   Thus, in the case, given that you choose door 1 and the host
chooses 2, the probability that the car is behind door 1 (your door)
is 0.333 and the other door (door 3) is 0.666. Following the same steps
through the other cases, we get in summary

                                 Probability of...

Your Choice  Host Choice  Car Behind 1 Car Behind 2 Car Behind 3
       1            1
       1            2            (host can't open your door)
       1            3
       2            1     0.333  0                                   0.666
       2            2
       2            3     0.333  0.666                               0
       3            1
       3            2     0      0.333                               0.666
       3            3
                                 (host can't open your door)

                          0.666  0.333                               0

                          0      0.666                               0.333

                          0.666  0                                   0.333

                                 (host can't open your door)

   In summary, it is always better to switch to the remaining door,
given these rules.
6 Introduction to Parameter Estimation

We will introduce the idea of what is called parameter estimation using
a simple system of bent coins. This will generalize to more complex
models, and form the basis for much of statistical inference.

6.1 Bent Coins                                                             Figure 6.1: Bent Coins
                                                                           Why do we number them from zero
Imagine we have a series of coins bent by various amounts (Fig-            here? It's so that the number of the
ure 6.1). If the coin is bent completely in half, then we could have the   coin, say number 7, corresponds
coin always flip heads (i.e. P (heads) = 1) or tails (i.e. P (tails) = 1)  the probability that that coin flips
depending on how it is bent. If you don't bend the coin at all then        heads, P (heads) = 0.7
we'd have a fair coin (P (heads) = P (tails) = 0.5). So, let's say         Table 6.1: Probabilities for flipping
that we have a collection of bent coins which are bent by different        heads given a collection of bent
amounts. For convenience we will number them from 0 to 10. The             coins
Table 6.1 summarizes the probability of each coin flipping heads.
                                                                           1 D. V. Lindley and L. D. Phillips.
Coin Number  Probability for Flipping Heads (P (heads))                    Inference for a bernoulli process
        0                                                                  (a bayesian view). The American
        1                              0.0                                 Statistician, 30(3):112-119, 1976
        2                              0.1
        3                              0.2
        4                              0.3
        5                              0.4
        6                              0.5
        7                              0.6
        8                              0.7
        9                              0.8
       10                              0.9
                                       1.0

Now I have the following scenario1, with a few questions.

Imagine I have taken a random coin from my collection, flipped it and
observed the following data:

T T T H T H T T T T T H (i.e. 9 tails and 3 heads)

1 From this data, which coin do I most likely have?
122 statistical inference for everyone

2 Can we be significantly confident that this particular coin will result
   in more tails than heads in the future?

   The way we've set up this problem is exactly like the model com-
parison example with the High and Low Deck (Section 4.1), except in
this case we have 11 models (one for each coin). Applying the Bayes'
Recipe we have

1 Specify the prior probabilities for the models being considered.
   Given no further information, we select a uniform distribution for
   the prior (i.e. all models are initially equally probable):

                                   P(M0) = 1/11
                                   P(M1) = 1/11

                                                 ..
                                                 .
                                  P(M10) = 1/11 .

   where M0 is the model defined by "we're flipping coin 0," M1 is
   the model defined by "we're flipping coin 1," etc...

2 Write the top of Bayes' Rule for all models being considered:

         P(M0|data = 9T, 3H)  P(data = 9T, 3H|M0)P(M0)
         P(M1|data = 9T, 3H)  P(data = 9T, 3H|M1)P(M1)

                                           ..
                                           .
        P(M10|data = 9T, 3H)  P(data = 9T, 3H|M10)P(M10) .

3 Put in the likelihood and prior values. Here we are drawing from
   a binomial distribution for the likelihood:

P(M0|data = 9T, 3H)                  12 0.03 × (1 - 0.0)9 × 1/11
                                     3

 P(M1|data = 9T, 3H)                 12 0.13 × (1 - 0.1)9 × 1/11
                                 ..  3
                                 .   12 1.03 × (1 - 1.0)9 × 1/11 .
                                     3
P(M10|data = 9T, 3H) 

4 Add these values for all models: see Table 6.2.

5 Divide each of the values by this sum, K, to get the final probabili-
   ties: see Table 6.2.

   When we are dealing with this many models, it is easier to plot the
results, shown in Figure 6.2. We are now in a position to address the
questions posed at the beginning of the section.
                                             introduction to parameter estimation 123

Model                   P(Mi|data = 9T, 3H)   P(Mi|data = 9T, 3H)/K  Table 6.2: Probability for different
                                                                     bent-coin models, given the data=9
  M0                               0.000                   0.000     tails, 3 heads. The middle column
  M1                             0.00774                   0.110     is the non-normalized value from
  M2                              0.0214                   0.306     Bayes' Rule, needing to be divided
  M3                              0.0217                   0.310     by K (the sum of the middle col-
  M4                              0.0128                   0.184     umn) to get the final column which
  M5                             0.00488                  0.0696     is the actual probability.
  M6                             0.00113                  0.0161
  M7                             0.000135                0.00192
  M8                           0.00000524               0.0000748
  M9                          0.0000000145            0.000000208
 M10                               0.000                   0.000
                                K=0.0700

                       0.35                                          Figure 6.2: Probability for different
                       0.30                                          bent-coin models, given the data=9
                       0.25                                          tails, 3 heads.
                       0.20
P(model|data={9T,3H})  0.15
                       0.10
                       0.05
                       0.00 0 1 2 3 4 5 6 7 8 9 10

                                         Model Number
124 statistical inference for everyone

1 From this data, which coin do I most likely have?                        2 Notice that the only models with
                                                                           probability equal to zero are ones
   The maximum probability is for coin 3, but coin 2 is a close sec-       that are logically impossible. It's not
   ond. Thus we can be reasonably confident that we have been flip-        the colloquial usage of impossible,
   ping one of those two coins, but can't narrow our confidence any        as in "it is impossible for the Red
   more than that.                                                         Sox to win this year," but in the
                                                                           strict usage, as in "it is impossible
2 Can we be significantly confident that this particular coin will result  to flip both heads and tails at the
   in more tails than heads in the future?                                 same time." The reason this is
                                                                           the case is that a statement with
   This is another way of asking for the total probability for coins less  zero probability cannot be made
   than coin 5 (the fair coin), or                                         possible with any about of data - it is
                                                                           an utterly dogmatic statement. Thus,
               P (coin 0 or coin 1 or coin 2 or coin 3 or coin 4) =        we reserve it only for things that are
                     0.000 + 0.110 + 0.306 + 0.310 + 0.184 = 0.912         logically impossible.

   which says that this coin is "likely" to "very likely" (Table 1.1 on
   page 51) to have a probability of yielding heads less than a fair
   coin, and thus yield more tails in the future.

6.2 Priors versus Data

It is instructive to pause and look at this example one flip at a time,
to see how the probability and thus our state of knowledge adjusts
as we collect more data. In Figure 6.3 we see the result of our proce-
dure when there is no data (i.e. our initial, prior probabilities) and
when we've flipped once and then again, both times tails. The curve
for "no data" is the same as the prior probability, and in this case all
models are equally likely. When the first tails is observed, the model
which states that heads are certain (i.e. coin 10) goes to zero proba-
bility because coin 10 cannot flip tails.2. At this point we know that it
is impossible for us to be flipping coin 10. We see also that the high-
numbered coins (i.e. the ones with high probability of flipping heads)
have greatly reduced probability while we've seen only tails.

   As more tails are observed, the probability for the lower models
is increased. As we flip more tails we become more confident in the
lower-number models. Because at this point we haven't flipped any
heads, the model 0 still has non-zero probability - it is still possible
that we are holding a coin that cannot flip heads.

   When we continue with the next few flips (Figure 6.4) we en-
counter our first heads on the fourth flip. At this point the model
which states that heads are impossible (i.e coin 0) goes to zero proba-
bility. Finally, across our entire data set (Figure 6.5) we see that the
curve gets narrower, where more of the probability falls on only a
few of the models and the other models become less and less likely.
With only 12 data points, there is still a lot of uncertainty in which
                                         introduction to parameter estimation 125

               0.35  data={}       0.35  data={T}      0.35 data={TT}

               0.30                0.30                0.30

P(model|data)  0.25                0.25                0.25

               0.20                0.20                0.20

               0.15                0.15                0.15

               0.10                0.10                0.10

               0.05                0.05                0.05

               0.00 0 1 2 3 4 5 6 7 8 9 10 0.00 0 1 2 3 4 5 6 7 8 9 10 0.00 0 1 2 3 4 5 6 7 8 9 10
                     Model Number        Model Number                       Model Number

model - several models have reasonably high probability values. We          Figure 6.3: Probability for different
still can rule out a few models confidently (like coins 0, 6, 7, 8, 9, and  bent-coin models, given no data
10). We are most confident in coins 2 and 3, with the most probabil-        (left), the first tails (middle), and
ity.                                                                        the second tails (right). The curve
                                                                            for no data is the same as the prior
                                                                            probability, and in this case all
                                                                            models are equally likely. When
                                                                            the first tails is observed, the model
                                                                            which states that heads are certain
                                                                            (coin 10) goes to zero probability.
                                                                            As more tails are observed, the
                                                                            probability for the lower models is
                                                                            increased.

               0.35 data={TTT} 0.35 data={TTTH} 0.35 data={TTTHT}

               0.30                0.30                0.30

P(model|data)  0.25                0.25                0.25

               0.20                0.20                0.20

               0.15                0.15                0.15

               0.10                0.10                0.10

               0.05                0.05                0.05

               0.00 0 1 2 3 4 5 6 7 8 9 10 0.00 0 1 2 3 4 5 6 7 8 9 10 0.00 0 1 2 3 4 5 6 7 8 9 10
                     Model Number        Model Number                       Model Number

6.3 Moving Toward the Continuous                                            Figure 6.4: Probability for different
                                                                            bent-coin models, given three tails
There is a practical problem that we face at this point, when we            (left), the first heads (middle), and
consider a generic bent coin. Perhaps it doesn't fit in one of the 11       another tails (right). When the first
models considered, falling somewhere in between, for example with           heads is observed, the model which
                                                                            states that heads are impossible (coin
                                                                            0) goes to zero probability.
126 statistical inference for everyone

               0.35  data={}       0.35 data={TTTHTH} 0.35 data={TTTHTHTTTTTH}

               0.30                0.30                0.30

P(model|data)  0.25                0.25                0.25

               0.20                0.20                0.20

               0.15                0.15                0.15

               0.10                0.10                0.10

               0.05                0.05                0.05

               0.00 0 1 2 3 4 5 6 7 8 9 10 0.00 0 1 2 3 4 5 6 7 8 9 10 0.00 0 1 2 3 4 5 6 7 8 9 10
                     Model Number        Model Number                    Model Number

P (heads) = 0.132464. Ones' first thought might be to include one        Figure 6.5: Probability for different
thousand coins or one million coins instead of the 11 we've consid-      bent-coin models, given no data
ered so far, so we could have coin 132464, coin 132465, coin 132466,     (left), the first half of the data set
etc... Although this can be done, we run into two problems               (middle), and the entire data set of
                                                                         9 tails and 3 heads (right).
1 Because we are dealing with so many models, the probability
   associated with any single model gets very small - and gets smaller
   with the more models you consider

2 We can't practically distinguish between models such as P (heads) =
   0.132464 and P (heads) = 0.132465 (the last digit is different here)

   In order to solve both of these problems mathematically, we in-
troduce the concept of a continuous distribution. We start by labeling
the model with a continuous number rather than an integer. In our
present case it makes sense to label the model with the probability
that the coin flips heads. We'll call this label , and it will have a
value between 0 (heads are impossible) and 1 (heads are certain) and
can take on any value in between. Because we now have an infinite
number of labels, we have two consequences:

1 We can't simply add up all the probabilities to get our value of K
   to make everything add up to 1. Instead, we look at areas under the
   curve and make sure the entire area equals 1.

2 Because, with distributions, areas under the curve (and not the
   values of the distribution itself) are the probabilities, we can
   only speak about ranges of values. For example, we can speak
   meaningfully about the probability of  between 0.3 and 0.4 (i.e.
   P(0.3 <  < 0.4)). When we write down something like P() = 1
   we're not talking about a probability of a single label but rather
   the magnitude of the distribution at that label, .
                          introduction to parameter estimation 127

   We revisit Bayes' Recipe again, using the distributions. This time                         1.0
we also will look at pictures of the distributions as we progress.
                                                                                              e 0.8 = 1
1 Specify the prior probabilities for the models being considered:                            0.4 Unde 0.6 r Curv
                                      P() = 1 .                            P()

2 Write the top of Bayes' Rule for all models being considered:                               0.2  Area
   We can write one equation for all of the models labeled by  at
   once as                                                                                    0.0
                                                                                               0.0 0.2 0.4 0.6 0.8 1.0
                                                                                                                    

P(|data = 9T, 3H)  P(data = 9T, 3H|)P() .

3 Put in the likelihood and prior values.

   We use the binomial model, one equation for all models, remem-
   bering that for a model labeled by  the probability for that coin
   flipping heads is P (heads) = . Thus we get the likelihood and
   prior values as

P(|data)  P(data|) · P()

P(|data9T, 3H)   12 3 × (1 - )9 · 1                                        P(|data = 9T,3H )  0.25
                 3
                                                                                              0.20
4 Find the area under this curve, and call it K.                           P(|data = 9T,3H )  0.15 rve NOT 1
                                                                                              n 0.10 der Cu
5 Divide each of the values of the curve by this are, K, to get the                           0.05 Area U
   final probabilities where the area under the curve is 1.                                   0.00

   Usually these steps are done for you, for a specific data set, and                           0.0 0.2 0.4 0.6 0.8 1.0
you are given the final posterior distribution to use in answering                                                       
any questions. However, for any particular case it is important to
know what assumptions have been made in the choice of models and                               3.5
model parameters.                                                                              3.0
                                                                                               2.5
                                                                                               2.0
                                                                                               1.51.0 er Curve=1
                                                                                               0.5 rea Und
                                                                                               0.0 A

                                                                                                0.0 0.2 0.4  0.6 0.8 1.0

6.4 MAP and Areas

Now we revisit the questions posed in Section 6.1 on page 121 about
the bent coin, this time using the distribution found above, repro-
duced here in Figure 6.6.

    Imagine I have taken a random coin from my collection, flipped it and
    got the following data:

                   T T T H T H T T T T T H (i.e. 9 tails and 3 heads)
128 statistical inference for everyone                                         Figure 6.6: Posterior probability
                                                                               distribution for the  values of
    1 From this data, which "coin" do I most likely have? (or in this          the bent coin - the probability
        interpretation, what is my best estimate for the probability of this   that the coin will land heads. The
        coin flipping heads, denoted by )                                      distribution is shown for data 3
                                                                               heads and 9 tails, with a maximum
    2 Can we be significantly confident that this particular coin will result  at  = 0.25.
        in more tails than heads in the future?
P()
                         3 heads and 9 tails
       4

                  maximum probability
       3

       2

       1

       0
           0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

                                     

   One answer to the first question can be accomplished by looking             3 The maximum of the posterior dis-
at the maximum of the posterior distribution, shown in Figure 6.6.3            tribution, which represents the most
By eye, it seems to have a maximum at  = 0.25. In fact one can                 likely value of a quantity, is often
demonstrate that this distribution has a maximum at                            referred to as the MAP estimate. It
                                                                               is also commonly referred to as the
                    max = number of successes ,                                mode of the distribution.
                               total number of attempts
                                                                               4 This distribution, given how
where in our example, a success is head, and an attempt is a flip.4            common it is, is given the name Beta
We take up this question of the best estimate of , given the posterior         distribution. There are a handful
probability for , in more detail in Section 6.6.                               of common distributions that are
                                                                               given names for convenience. We've
   The answer to the second question can be done by looking at                 already seen the uniform distribution,
the area under the curve from  = 0 , the "all heads" coin, to  =               and there will be others.
0.5, the "fair" coin, as shown in Figure 6.7. This area represents the
probability, given the data, that the coin is skewed towards heads
or, in other words, how confident are we that this is an unfair coin.
Given the value of P( < 0.5) = 0.954 we can say that this is "very
likely" an unfair coin (see Table 1.1 on page 51).
                                            introduction to parameter estimation 129

                       3 heads and 9 tails                                Figure 6.7: Posterior probability
     4                                                                    distribution for the  values of
                                                                          the bent coin - the probability
     3                                                                    that the coin will land heads. The
                                                                          distribution is shown for data 3
                                                                          heads and 9 tails. The area under
                                                                          the curve from  = 0 (the "all
                                                                          heads" coin) to  = 0.5 (the "fair"
                                                                          coin) is 0.954.

P()  2  area=0.954

     1

     0
         0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

                                  

6.5 Quartiles                                                             Quartiles The term quartiles refers
                                                                          to the values of the parameter
Given that we are dealing most often with continuous distributions,       which result in an area of 25%,
and thus need to look at areas under the curve from one point to          50%, or 75%, or one, two, or three
another, it is useful to make a table for a distribution of these areas.  quarters of the area.
Typically we look at the values of the parameter at which we have a
given area under the curve from the minimum possible value of the
parameter up to to that value. For example, we might be interested
in the value of  (i.e. how skewed the coin is) such that we have an
area of 50% from 0 up to , shown in Figure 6.8. This point (called
the median) represents the point where we would be just as confident
(given our data) that the coin is more skewed than this as less skewed.

   A table of these values for a distribution can be very useful. For
example, consider the table and plot shown in Figure 6.9. Shown are
the various points where the area under the curve up to those points
is specified. For example, the area under the curve from  = 0 up
to  = 0.11 is 5%. This means, given the data of 3 heads and 9 tails,
there is a probability P = 5% of the coin having less than  = 0.11, or
an extreme skew towards tails.

   Quartiles The term quartiles refers to the values of the parameter
which result in an area of 25%, 50%, or 75%, or one, two, or three
quarters of the area.

   When we wish to refer to a non-quarter percentage, then we'll call
130 statistical inference for everyone

                       3 heads and 9 tails                               Figure 6.8: Posterior probability
     4                                                                   distribution for the  values of
                                                                         the bent coin - the probability
     3  area=0.5                                                         that the coin will land heads. The
                                                                         distribution is shown for data 3
                                                                         heads and 9 tails. The area under
                                                                         the curve from  = 0 (the "all
                                                                         heads" coin) to  = 0.28 is 0.5 - half
                                                                         the area. This represents the median
                                                                         of the distribution.

P()  2

     1

                           0.28
     0

         0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

                                  

it a percentile.                                                         Percentiles The term percentile
   Percentiles The term percentile refers to the value of the parameter  refers to the value of the parameter
                                                                         which result in a particulare area
which result in a particulare area under the curve.                      under the curve.
   For example, we can say from Figure 6.9 that the 99% percentile is

0.59. Thus, it is extremely unlikely to have the coin skewed towards
heads more than  = 0.59 given the observation that we flipped 3
heads and 9 tails with this coin.
                                         introduction to parameter estimation 131

     4              3 heads and 9 tails

            25%50%                                                        Beta(heads=3,tails=9)

                                                                          Value  Area

     3              75%                                                   0.07   0.01

                                                                          0.11   0.05

                                                                          0.14   0.10

P()  2  5%                                                                0.20   0.25

                                                                          0.28   0.50

                         95%                                              0.36   0.75
                             99%
     1 1%                                                                 0.44   0.90

                                                                          0.49   0.95

                                                                          0.59   0.99

     0 0.11 0.28 0.49
           0.07 0.20 0.36 0.59

         0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

                                  

6.6 Best Estimates                                                        Figure 6.9: Posterior probability
                                                                          distribution for the  values of
Perhaps surprisingly, there is not a single answer to the best esti-      the bent coin - the probability
mate for  given the posterier distribution, like the one shown in         that the coin will land heads. The
Figure 6.9. There are several plausible measures, each with their own     distribution is shown for data 3
advantages. Any specific estimate of a parameter (e.g. ) is denoted       heads and 9 tails. The various
with a hat (e.g. ^) in the descriptions that follow.                      quartiles are shown in the plot, and
                                                                          summarized in the accompanying
   The Mode Also known as the maximum a-posteriori probability            table.
(MAP) estimate, the mode is the maximum of the posterior probabil-
ity. In the case of a Beta distribution with h successes in N trials, we  The Mode Also known as the max-
have                                                                      imum a-posteriori probability (MAP)
                                                                          estimate, the mode is the maximum
                                                                          of the posterior probability.

               mode =^   h

                         N

   The Mean Also known as the expected value or average value, the        The Mean Also known as the
mean of a distribution of a parameter  is defined to be the sum of all    expected value, the mean of a distri-
of the possible values of  times the posterior probability of ,           bution of a parameter  is defined
                                                                          to be the sum of all of the possi-
        ^mean =   × P(|data)                                              ble values of  times the posterior
                                                                          probability of , as in
                        
                                                                             ^mean =   × P(|data)
It is one measure of the middle of the distribution. In the special case
of a Beta distribution with h successes in N trials, we have                                    

                                                                          It is one measure of the middle of
                                                                          the distribution.

            mean =^ h+1

                    N+2
132 statistical inference for everyone

Intuitively this is the same as the MAP of the Beta distribution, with      The Median Also known as the
one more success and one more failure than actually observed. Fur-          50%-percentile, the median repre-
ther, for the Beta distribution, the mean value ^mean represents the        sents the middle of the distribution
predictive probability of a successful event on the next observation.       such that the probability of the
                                                                            parameter below the median equal
   The Median Also known as the 50%-percentile, the median rep-             to the probability of the parameter
resents the middle of the distribution such that the probability of the     above the median.
parameter below the median equal to the probability of the parame-
ter above the median.                                                           P(  ^median|data) = 0.5
                                                                                P(  ^median|data) = 0.5
           P(  ^median|data) = P(  ^median|data) = 0.5
                                                                            "Assume 2 successes and 2 fail-
   "Assume 2 successes and 2 failures" median approximation For             ures" median approximation For
the Beta distribution there is no simple form for the median, but a         the Beta distribution there is no
decent approximation which we will use is given by5                         simple form for the median, but
                                                                            a decent approximation which we
                                ^median  h + 2                              will use is given by
                                                  N+4
                                                                                      ^median  h + 2
Intuitively this is the same as the MAP of the Beta distribution, with                                   N+4
two more successes and two more failures than actually observed,
and is thus referred to as the "Assume 2 successes and 2 failures"          Intuitively this is the same as the
median approximation.                                                       MAP of the Beta distribution, with
                                                                            two more successes and two more
   Although each of these has their advantages, most notably ease of        failures than actually observed, and
computation (especially for the mode and the mean), we will typi-           is thus referred to as the "Assume
cally use the median of the distribution as the best estimate for the       2 successes and 2 failures" median
following reasons:                                                          approximation.
                                                                            5 Alan Agresti and Brian Caffo.
1 the median is intuitive as literally the middle of the distribution       Simple and effective confidence
                                                                            intervals for proportions and
2 the median is not as sensitive to distributions that are highly           differences of proportions result
   asymmetric                                                               from adding two successes and two
                                                                            failures. The American Statistician, 54
In most practical examples it may not make much difference, and             (4):280-288, 2000
for some distributions (such as the Normal distribution described in
Chapter 7 (Priors, Likelihoods, and Posteriors)) there is not difference -
the mean is the median which is also the mode.

Example 6.1 What is the best estimate of the probability of a bent coin
flipping heads, given the observation of 9 tails and 3 heads?

   If we take the best estimate to be the median, then we have from
the "assuming 2 successes and 2 failures" method,

                             ^median  h + 2
                                               N+4

                                         = 5 = 0.313
                                                 16
                                 introduction to parameter estimation 133

Notice that the maximum probability was at the somewhat lower

value

       mode =^ h  =  3   = 0.25

               N     12

One reason why the median is a better estimate in this case is

because, as shown in Figure 6.9, there is more probability (i.e. area

under the curve) to the right of the maximum than to the left, so the

best estimate should be greater than the one given by the mode.

6.7 Uncertainty in the Best Estimates

To quantify the uncertainty in the best estimates, we need a value
which represents the width of the distribution. Looking at Figure 6.10
we'd like to provide a quick way of saying that the range of probable
values lies somewhere between  = 0.2 and  = 0.5 - anything
outside of this contributes only a small amount to the probability,
or in other words, we are most confident that our best estimate of
 lies between those 0.2 and 0.5. Depending on the application, the
symmetry of the distribution, and other practical factors one may see
a few potential measures of the width of the distribution.

   Inter-Quantile Range The Inter-Quantile Range (ICR) is the range     Inter-Quantile Range The Inter-
between the 25% and 75% quartiles, and represents 50% of the proba-     Quantile Range (ICR) is the range
bility.                                                                 between the 25% and 75% quar-
                                                                        tiles, and represents 50% of the
   In Figure 6.10, the Inter-Quantile Range range is [0.29,0.40].       probability.

   95% Credible Interval (CI) The 95% Credible Interval (CI) is the     95% Credible Interval (CI) The
range between the 2.5% and 97.5% quantiles, and thus represents         95% Credible Interval (CI) is the
95% of the probability. According to Table 1.1 on page 51, it is "very  range between the 2.5% and 97.5%
likely" that our best estimate lies in this range.                      quantiles, and thus represents 95%
                                                                        of the probability. According to
   In Figure 6.10, the 95% Credible Interval is nearly [0.2,0.5].       Table 1.1 on page 51, it is "very
                                                                        likely" that our best estimate lies in
   Standard Deviation The standard deviation is a measure of the        this range.
half-width of a distribution, most commonly used specifically with
reference to the particular Normal distribution. This will be defined   Standard Deviation The standard
more precisely in Section 7.2 on page 140), and will thus not be de-    deviation is a measure of the
fined in general here.                                                  half-width of a distribution, most
                                                                        commonly used specifically with
   An approximate value for the standard deviation for the Beta         reference to the particular Normal
distribution is                                                         distribution. This will be defined
                                                                        more precisely in Section 7.2 on
                                ^(1 - ^)/N                              page 140), and will thus not be
                                                                        defined in general here.
From Figure 6.10, and using the median as the best estimate, ^, we
get

                        0.34(1 - 0.34)/30 = 0.09
134 statistical inference for everyone

   Standard Deviation to Uncertainty To convert this number to an            Standard Deviation to Uncertainty
uncertainty, it is a mathematical consequence that about 65% of the          To convert this number to an
area is within 1 value of , 95% of the area is within 2 values of ,          uncertainty, it is a mathematical
and 99% of the area within 3 values.                                         consequence that about 65% of the
                                                                             area is within 1 value of , 95% of
   So, of for the approximate 95% CI for the case shown in Fig-              the area is within 2 values of , and
ure 6.10 is                                                                  99% of the area within 3 values.

               [0.34 - 2 · 0.09, 0.34 + 2 · 0.09] = [0.16, 0.52]

a bit more conservative range (larger uncertainty) than is given by the
direct method of quantiles, but it much easier to calculate.

6.8 Marginalization

In Section 1.4 we introduced the concept of marginalization, and in
Section 2.1 we performed a discrete example of this. In that section
it was seen as simply a consequence of the sum and product rules. It
was a way of taking a probability that depended on several factors,
and eliminating all but the single factor we're interested in. If we
have a continuous distribution, this process involves calculus and we
will not cover it in detail, but it is the same process. In the case of the
distribution above, we have a distribution over a single variable, like
Beta(|h, t). Imagine that we have a distribution that depends on two
parameters,

                                  MyDist(, )

which specifies the probability of an event given each combination of
the parameters,  and . We'd have to do a three-dimensional plot to
visualize this. Many times, however, we want just the probability of
one of the single parameters. In those cases we will write

                    P()  [MyDist(, )]marginalize over 
where we are "summing" over all the values of the other parame-
ters, leaving the details to the mathematicians, and simply using the
result.

   Likewise we can marginalize the parameter  to get the distribution
of the other variable.

                    P()  [MyDist(, )]marginalize over 
   This becomes important in Chapter 7 and Chapter 9.

6.9 Exercises

Exercise 6.1 Given the posterior shown in Figure 6.10 for 10 heads and 20
tails, answer the following:
                                                                       introduction to parameter estimation 135

1 The most likely estimate for the parameter . What does this mean?
2 Is it likely that this is a fair coin?
3 What is P(0    0.3) approximately?
4 What is P(0.2    0.35) approximately?
5 What is the median value? What are the quartiles?

                      10 heads and 20 tails

                        50%

     5                25%                                                 Beta(heads=10,tails=20)

                           75%                                            Value  Area

     4                                                                    0.17   0.01

                                                                          0.21   0.05

                                                                          0.24   0.10

P()  3                                                                    0.29   0.25
                  5%
                                                                          0.34   0.50
     2
     1 1%                       95%                                       0.40   0.75
                                   99%
                                                                          0.45   0.90

                                                                          0.49   0.95

                                                                          0.55   0.99

     0  0.21 0.34 0.49
        0.17 0.29 0.40 0.55
     0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
                             

6.10 Computer Examples                                                    Figure 6.10: Posterior probability
                                                                          distribution for the  values of
from s i e import *                                                       the bent coin - the probability
                                                                          that the coin will land heads. The
                                                                          distribution is shown for data 10
                                                                          heads and 20 tails. The various
                                                                          quartiles are shown in the plot, and
                                                                          summarized in the accompanying
                                                                          table.

Beta Distribution Example

3 heads and 9 tails Plot a beta distribution with 3 heads and 9 tails...

d i s t = b e t a ( h=1 ,N=3)
distplot ( dist , xlim =[0 ,1] , show_quartiles=False )
136 statistical inference for everyone

   The median of this distribution...
d i s t . median ( )

0.27527583248615201

   the 95
credible_interval ( dist )

(0.067585986488542985, 0.38572756813238962, 0.80587955031675662)

1 heads and 3 tails This should be about the same fraction as the previous example, but broader
d i s t = b e t a ( h=1 ,N=4)
distplot ( dist , xlim =[0 ,1])

<matplotlib.figure.Figure at 0x108768cd0>
                                                                       introduction to parameter estimation 137

credible_interval ( dist )

(0.052744950526316919, 0.31381017045569742, 0.71641793611808946)
7 Priors, Likelihoods, and Posteriors

7.1 Binomial and Beta Distributions

In Chapter 6 (Introduction to Parameter Estimation on page 121) we
estimated the chance, , that a bent coin would come up heads by
combining a uniform prior for  (i.e. all possible values are a-priori
equally likely) and a binomial likelihood (i.e. given , what is the
probability of the data). This resulted in a Beta distribution for the
posterior probability for .

   Notice what the procedure of Bayes' Recipe is and how the Bayesian
inference works here.

1 Specify the prior probabilities for the models being considered

        We want to estimate a quantity (which we label as ), but begin
        with absolutely no knowledge of its value - we have a uniform prior
        probability.

2 Write the top of Bayes' Rule for all models being considered

        We construct a model for how different possible values of  influ-
        ence the outcome - a model we call the likelihood. In the case of the
        bent coin, the likelihood model is a binomial model, and describes
        the probability of flipping heads or tails given how bent the coin is
        (i.e. given ).

3 Put in the likelihood and prior values

4 Add these values for all models

5 Divide each of the values by this sum, K, to get the final probabili-
   ties

        Once we observe data, we can combine the prior and the model or
        likelihood using the Bayes' recipe, and obtain the posterior distribu-
        tion for the unknown value, , giving us the probability for each
        value, now updated with our new observations.

   The last couple of steps of the recipe, for simple cases, is done by
the mathematicians so we don't have to manually add and divide as
we did in the previous chapters. In the case of the coin flips we get:
140 statistical inference for everyone

                                                         likelihood

                   Beta(|data)  Binomial(data|) × Uniform()

                   posterior probability  prior probability

   From this Beta distribution, we can get the most likely values (i.e.
maximum probability value) for the unknown quantity of interest,
, our uncertainty in this quantity (i.e. the width of the Beta distribu-
tion) consistent with the known data. In other words, the posterior
probability summarizes all of our knowledge about the parameter of
interest given the data.

7.2 The Normal Distribution - Properties                                   1 The distribution is named after
                                                                           Carl Friedrich Gauss who intro-
The Normal distribution, also referred to as the Gaussian distribution,1   duced it in 1809. However, it has
is by far the most commonly occurring distribution in all of statistical   been called in the past the Gauss-
inference, so it requires some special attention.                          Laplacian distribution, due the the
                                                                           fact that Pierre Simone de Laplace
The Shape                                                                  was the first to apply it to real prob-
                                                                           lems, and proved a number of very
The shape of the Normal distribution is sometimes described as             useful properties of it.
bell-shaped, as shown in Figure 7.1, and is thus referred to as the bell-
curve (although there are several other mathematical functions which       Figure 7.1: The Normal Distribu-
are bell-shaped). The function is referred to as Normal(µ, ) where         tion.
µ and  are parameters of the model. (see Appendix B.1 on page 225
for a review of greek letters)

p(x) =Normal(0,1)  0.4
                   0.3
                   0.2
                   0.1
                   0.0

                      4 3 2 10 1 2 3 4

                                                        x
                                                      priors, likelihoods, and posteriors 141

The location parameter, µ

The location parameter (see Figure 7.2) is the value of x for which
the Normal distribution has a maximum probability. In a real sense,
it is the middle of the distribution, and the best estimate of x. For the
Normal distribution the location parameter, µ, is at once the mean,
median and mode of the distribution.

                                                                             Figure 7.2: The Normal distribution
                                                                             with different location parameters,
                                                                             µ.

                   0.4        µ =-2 µ =0     µ =3

p(x) =Normal(µ,1)  0.3

                   0.2

                   0.1

                   0.0

                        6  4  2  0        2        4  6
                                 x

The deviation parameter, 

As shown in Figure 7.3 the deviation parameter, , is a measure of
how spread out the distribution is. As the width increases, the height
goes down to keep the area under the curve constant (at 1). As a
result, more of the probability sits at larger values of x as  gets larger.

   Three useful properties of  for the Normal distribution are the
following:

1 the Normal distribution value at the maximum (i.e. at x = µ)
   is around 2.7 times larger than the value one- away from the
   maximum (at x = µ -  and x = µ + )

2 the total probability between these two points is 65%. This is typi-
   cally written, µ ± .

3 95% of the distribution lies between µ - 2 and µ + 2 (see Fig-
   ure 7.3)

   For example, writing 5 ± 2 typically implies a Normal distribution
with mean µ = 5 and deviation  = 2. One is 65% certain that the
142 statistical inference for everyone

range of the estimated value is between 3 and 7, and 95% certain that
the range is between 1 and 9 (i.e. mean minus two deviations and
mean plus two deviations).

                                                                         Figure 7.3: The Normal distribution
                                                                         with different deviation parameters,
                                                                         .

                  0.4   =1

p(x) =Normal(0,)  0.3

                  0.2   =2

                  0.1   =4

                  0.0
                     8 6 4 20 2 4 6 8

                                                       x

Summarizing the Distribution                                             The Standard Normal Distribution
                                                                         The Normal distribution in the
We can specify the Normal distribution with just the two parameters,     special case where µ = 0 (the
µ and  - the location and deviation parameters, respectively. How-       distribution is centered at x = 0)
ever, due to its symmetry, we can summarize this distribution for        and  = 1 (the distribution has a
all cases by looking a a single special case called the standard Normal  spread of 1).
distribution.

   The Standard Normal Distribution is the Normal distribution in
the special case where µ = 0 (the distribution is centered at x = 0)
and  = 1 (the distribution has a spread of 1).

   For any Normal distribution, the area within 1- is 0.68, within 2-
is 0.95, and 3- is 0.99. These locations are the most prevalently used
in any kind of statistical testing, and thus we will see them many
times.

Moving from a General Normal to the Standard Normal and Back

In order to use the table of percentiles for the standard Normal dis-
tribution, we need to be able to translate from the Normal to the
standard Normal and back again. Luckily, it is a simple process, and
is one of the main reasons for using the Normal distribution - other
distributions are not so easily manipulated.
                                                          priors, likelihoods, and posteriors 143

                                                   50.0%                        Figure 7.4: The Standard Normal
                   0.4                                                          Distribution (the Normal distribu-
                                                                                tion in the special case where µ = 0
p(x) =Normal(0,1)                          15.9%          84.1%                 and  = 1). The percentiles shown
                   0.3                                        area=0.68         are for positions 1- away from the
                                                                                center, 2- away, and 3- away. The
                   0.2                                           97.7%          area within 1- is 0.68, within 2- is
                                    2.3%                                 99.9%  0.95, and 3- is 0.99. These locations
                                                                                are the most prevalently used in
                   0.1 0.1%                                                     any kind of statistical testing, and
                                                                                thus we will see them many times.

                   0.0 -3.00 -2.00 -1.00 0.00 1.00 2.00 3.00
                      4 3 2 10 1 2 3 4

                                                        x

   To facilitate this translation, we will use the variable x for the
Normal distribution and z for the standard Normal. So now, we need
to have a recipe for translating x to z (or vice versa), given µ and .
These recipes are:

1 x to z: subtract x by µ, and divide by 

2 z to x: multiply z by  and add µ

Example 7.1 Given a Normal distribution with a mean of µ = 150 and a
 = 20, what is the most likely value?

   The most likely value is the peak of the probability distribution,
x^ = µ = 150.
Example 7.2 Given a Normal distribution with a mean of µ = 150 and
 = 30, what is the probability P(x > 170)

   To use the tables in Section D.3 on page 238, we first need to trans-
late everything to the standard Normal values.

                       x = 170  z = x - 150 = 0.67
                                                     30

   From the table in Section D.3 on page 238, the area to the left of
z = 0.67 is 0.7486. Because we are asked the probability greater than
x = 170 we need to have the area to the right of the curve, or

                    P(x > 170) = 1 - 0.7486 = 0.2514
144 statistical inference for everyone

or about 1/4. In other words, with a mean µ = 150 and deviation
 = 20, we'd expect about a quarter of the time that the value of the
variable would be greater than 170. Or, given our uncertainty of a
specific value, we'd assign a probability of around 25% to it being
larger than 170.
Exercise 7.1 Given a Normal distribution, with parameters µ = 10 and
 = 2, determine the following probabilities:

1 P(x < 12)
2 P(6 < x < 14)
3 P(2 < x < 12)

Exercise 7.2 Given a Normal distribution, with parameters µ = 2 and
 = 10, answer the following questions (see Table 1.1 on page 51 for refer-
ence):

1 Make a qualitative plot of the distribution to help you with the other parts
   of the question

2 Is likely that x > 0?

3 Above which value of x is it very unlikely to observe?

4 Below which value of x is it extremely unlikely to observe?

Exercise 7.3 Given a Normal distribution, with parameters µ = 2 and
 = 0.5, answer the following questions (see Table 1.1 on page 51 for
reference):

1 Make a qualitative plot of the distribution to help you with the other parts
   of the question

2 Is likely that x > 0?

3 Above which value of x is it very unlikely to observe?

4 Below which value of x is it extremely unlikely to observe?

Sum and Differences
One more convenient property of the Normal distribution is that
sums and differences of variables that individually have Normal
distributions also have Normal distributions, although each with
a different mean and deviation parameter. The relationships are
summarized as follows.
                      priors, likelihoods, and posteriors 145

   Sum of two Normally distributed variables If we have two vari-     Sum of two Normally distributed
ables, x and y, which have Normal distributions                       variables If we have two Normally
                                                                      distributed variables, x and y, we
P(x) = Normal(µx, x)                                                  have
P(y) = Normal(µy, y)
                                                                             P(x) = Normal(µx, x)
                                                                             P(y) = Normal(µy, y)
                                                                        P(x + y) = Normal(µx + µy,

                                                                                               x2 + y2)

then their sum, x + y, has a mean the sum of the two, µx + µy and a
deviation x2 + y2.

   One way to remember this is that the new squared deviation pa-
rameter is the sum of the two old ones,

  2  =  2    +  2

x+y       x       y

   Differences between two Normally distributed variables For         Differences between two Normally
differences, x - y, we have a new mean of µx - µy and deviation       distributed variables
parameter again x2 + y2. Note the "+" sign in the new , which
keeps the new  positive which is must be by definition.                 P(x - y) = Normal(µx - µy,
                                                                                               x2 + y2)
   If we are asked for the distribution of a quantity with an added
constant, like                                                        (Note the "+" sign in the new .)

                               z = x + constant

then the probability of z is just the same as that of x (i.e. Normal
distribution with the same deviation), with the location parameter
moved by the constant

                    P(z) = Normal(µx + constant, x)

Example 7.3 We have two Normal distributions P(x) = Normal(µ =
8,  = 2) and P(y) = Normal(µ = 20,  = 7). What is the distribution
for z = y - x?

   The distribution P(z) is also a Normal distribution, with mean
                                                

µz = 20 - 8 = 12 and deviation z = 72 + 22 = 7.3.
146 statistical inference for everyone

0.20 p(x) =Normal(8,2)

0.15

0.10

                            p(y) =Normal(20,7)

0.05

0.00

             p(z) =p(y-x) =Normal(12.0,7.3)

10    0  10             20                      30  40

7.3 The Normal Distribution - Estimating From Data                        In scientific applications, this
                                                                          notation is often shortened to
Estimating the mean, µ, knowing the deviation,                            µ = x¯ ± / N, so it is clear what
                                                                          is the best estimate of µ (i.e. x¯)
Typically one is provided with a series of measurements of a quan-        and what is the uncertainty in that
tity, and we want to estimate the value of that quantity, and have a      estimate (i.e. / N).
description of our uncertainty in the estimate. In Chapter 9 (Applica-
tions of Parameter Estimation and Inference on page 165) we go through    Sample Mean The sample mean of
a number of detailed examples of this process. Here, we simply sum-       a set of N samples, x1, x2, · · · , xN is
marize the result. We are given:                                          given by

1 A series of N measurements, data={x1, x2, x3, . . . , xN}                x¯  x1 + x2 + x3 + · · · + xN
                                                                                                N
2 The real deviation, 

3 We are modeling the data as a true value, µ, with uncertainty with
   a likelihood from the Normal distribution with known deviation,
   , as in Normal(0, ). Further, we assume independence between
   the measurements.

Since in this case we are given , we wish then to estimate the pa-
rameter µ. The result will be a probability distribution over µ, with a
best (i.e. most probable) value and an uncertainty in that value. The
result is that the distribution of µ is also a Normal distribution,

                                                          
                    P(µ|data, ) = Normal(x¯, / N)

where the center value (and thus the most probable value of µ) is
given by the sample mean of the data.

   Sample Mean The sample mean of a set of N samples, x1, x2, · · · , xN
priors, likelihoods, and posteriors 147

is given by                                                           Estimate of location parameter µ
                        x¯  x1 + x2 + x3 + · · · + xN                 given N samples and , the known
                                              N                       deviation The best estimate for
                                                                      the location parameter µ in the
                                                                      Normal distribution given a set of
   The uncertainty in µ is given by / N. As a consequence, larger     N samples, x1, x2, · · · , xN is given
N (i.e. more data points), makes us more confident in the particular  by
estimate for µ.                                                       µ^ = x1 + x2 + · · · + xN ± /N

   Estimate of location parameter µ given N samples and , the                          N
known deviation In summary, the best estimate for the location
parameter µ in the Normal distribution given a set of N samples,      In real measurements, there is
x1, x2, · · · , xN is given by                                        always the problem of bias or
                                                                      systematic uncertainties, where
                  µ^ = x1 + x2 + x3 + · · · + xN ± /N                 the uncertainty does not follow a
                                       N                              Normal distribution. We will not
                                                                      consider this issue here.
Example 7.4 Estimating the True Length of an Object
                                                                      The 95% credible interval (CI) is
Say we have an object, and 5 measurements of its length from the      really at the 1.96 level, yielding
same ruler but from different people,                                 [4.481[cm], 5.358[cm]]. We will
                                                                      almost always approximate it as
                   5.1[cm], 4.9[cm], 4.7[cm], 4.9[cm], 5.0[cm]        2 by hand, but the computer will
                                                                      generate the true 95% credible
Say that we further know that the uncertainty (given this ruler) of   interval when requested.
one measurement has  = 0.5[cm]. What is the best estimate of the
length? The best estimate should be given by the sample mean of
these 5 samples,

  µ^ = x1 + x2 + · · · + xN
                        N

      = 5.1[cm] + 4.9[cm] + 4.7[cm] + 4.9[cm] + 5.0[cm] = 4.92[cm]
                                            5

with uncertainty related to the known uncertainty of a single mea-
surement,

                                        
                           ^ = 

                                         N
                                     0.5[cm]
                               =  = 0.223[cm]

                                            5

yielding a final best estimate of

                           µ^ = 4.92[cm] ± 0.223[cm]

or (with 2 range),

              µ^ = 4.92[cm], 95% CI = [4.474[cm], 5.366[cm]]
148 statistical inference for everyone

Estimating the mean, µ, not knowing the deviation,                        Sample Deviation The sample
                                                                          deviation of a set of N samples,
If we are not so fortunate to be given the deviation, as in the previous  x1, x2, · · · , xN is given by
case, then this parameter too must be estimated from the data. As a
first step we can estimate the deviation with the sample deviation.       S  1 N - 1 ((x1 - x¯)2 + · · · + (xN - x¯)2)

   Sample Deviation The sample deviation of a set of N samples,           Approximate estimate of location
x1, x2, · · · , xN is given by                                            parameter µ and deviation 
                                                                          given N samples The posterior
       S  1 N - 1 ((x1 - x¯)2 + (x2 - x¯)2 + · · · + (xN - x¯)2)          probability for µ and  given a set
                                                                          of N samples, x1, x2, · · · , xN can be
   Approximate estimate of location parameter µ and deviation             approximated with
given N samples The posterior probability for µ and  given a set of
N samples, x1, x2, · · · , xN can be approximated with                                                            
                                                                          P(µ|data)  Normal(x¯, S/ N)
                                                                          P(|data)  Normal (S,
               P(µ|data)  Normal(x¯, S/ N)
                                                                                                 S2/ (N - 1)/3
               P(|data)  Normal S, S2/ (N - 1)/3
                                                                          which works well if we have many
which works well if we have many (N > 30) data points.                    (N > 30) data points.
   With a smaller data set, the value of S as an estimate for the devi-   Because the uncertainty in the mean
                                                                          depends explicitly on the number of
ation becomes too small. When the estimate for  is too small, then        data points, it goes beyond the level
the result is claiming more confidence in the estimate of the mean, µ,    of this chapter to give a form for the
than is warranted. This discrepancy depends on the number of data         posterior probability distribution
points, and thus it makes sense that the proper distribution should       for the deviation, .
depend on the number of data points, in addition to the sample
mean and deviation. The proper, although less convenient, result is       Estimate of location parameter µ
that the posterior probability for µ takes the form of the Student's t    given N samples and unknown
distribution,                                                              The posterior probability for µ
                                                                          takes the form of the Student's t
   Estimate of location parameter µ given N samples and unknown           distribution,
 The posterior probability for µ takes the form of the Student's t
distribution,                                                                                                         
                                                                          P(µ|data) = Studentdof=N-1(x¯, S/ N)
                                                                          This distribution requires three
                 P(µ|data) = Studentdof=N-1(x¯, S/ N)                     numbers to specify, referred to as
                                                                          the mean (µ), deviation () and the
This distribution requires three numbers to specify, referred to as the   degrees of freedom (dof). The degrees
mean (µ), deviation () and the degrees of freedom (dof). The degrees      of freedom is defined in this case to
of freedom is defined in this case to be the number of data points less   be the number of data points less
one, N - 1.                                                               one, N - 1.

Example 7.5 Estimating the True Length of an Object...Again

Say we have an object, and 5 measurements of its length from the
same ruler but from different people,

                   5.1[cm], 4.9[cm], 4.7[cm], 4.9[cm], 5.0[cm]
                               priors, likelihoods, and posteriors 149

Unlike earlier, let's say that we don't know the uncertainty (given this
ruler) of one measurement What is the best estimate of the length?
Again, the best estimate should be given by the sample mean of these
5 samples,

  µ^ = x1 + x2 + · · · + xN
                        N

      = 5.1[cm] + 4.9[cm] + 4.7[cm] + 4.9[cm] + 5.0[cm] = 4.92[cm]
                                            5

with uncertainty related to the sample deviation

S2 = 1 N - 1 (x1 - x¯)2 + · · · + (xN - x¯)2
     = 1 (5.1[cm] - 4.92[cm])2 + (4.9[cm] - 4.92[cm])2 + (4.7[cm] - 4.92[cm])2+
           5-1
           (4.9[cm] - 4.92[cm])2 + (5.0[cm] - 4.92[cm])2

     = 0.024[cm]2

   S = 0.024[cm]2 = 0.155[cm]

S  0.155[cm]
=   = 0.069[cm]
N  5

   Looking at Table D.2on page 236 with "Degrees of Freedom" equal
to 4, we find that the 95% credible interval for µ (between areas 0.025

                                  
and 0.975) falls ±2.776 · S/ N, thus we have

µ^ = 4.92[cm], 95% CI = [4.92[cm] - 2.776 · 0.069[cm], 4.92[cm] + 2.776 · 0.069[cm]]
    = 4.92[cm], 95% CI = [4.73[cm], 5.11[cm]]

   Although much of this is easier with the computer, it is instructive
to go through simple examples by hand.

7.4 Normal Approximation

The Normal distribution is useful for many reasons: its simple shape,
the fact that there are only two parameters which describe it, and the
ease with which one can compare the general Normal distribution to
the single standard Normal. Further, it can be used as an approxima-
tion for several other distributions, under certain limits.

The Beta Distribution

We first saw the beta distribution as the posterior description in a
bent-coin parameter estimation problem (see Section 6.3 on page 125
in Chapter 6 (Introduction to Parameter Estimation)). The Normal ap-
proximation occurs when the number of flips gets large, compared
150 statistical inference for everyone
to how likely the coin flips heads. For notation, we will write the
frequency of heads as

                                      f h
                                              N

   Normal Approximation to the Beta Distribution The Normal             Normal Approximation to the Beta
Approximation to the Beta Distribution , for large number of flips (N)  Distribution The Normal Approxi-
of which a fraction f  h/N are successful is given by                   mation to the Beta Distribution , for
                                                                        large number of flips (N) of which a
         Beta(h, N)  Normal µ = f ,  = f (1 - f )/N                     fraction f  h/N are successful is
                                                                        given by

                                                                        Beta(h, N)  Normal (µ = f ,

                                                                                                = f (1 - f )/N

   To see how close this approximation can be, observe the following
two cases:

          3 heads and 9 tails

     3.5                               Beta
                                       Normal

     3.0                               µ =0.25

     2.5                                =0.12

P()  2.0

     1.5

     1.0

     0.5

     0.0

          0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

                                   

With ten times as many flips, we have
                                      priors, likelihoods, and posteriors 151

     12        30 heads and 90 tails

                                      Beta
                                      Normal
     10

     8                                µ =0.25
                                       =0.04

P()  6

     4

     2

     0
         0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

                                  

   and the curves are so close as to be nearly identical! There still is  This is an approximation, and as
a (small) probability for getting a negative , which is problematic       such will certainly give seriously
in theory but not typically in practice. To use the properties of the     incorrect answers under certain
Normal distribution here to quantify our uncertainty about the bent       circumstances. For example, in this
coin. Given 30 heads and 90 tails, the best estimate for  (i.e. the       case, the Normal approximation
top of the curve) is 0.25. Our uncertainty is quantified by the width     predicts that there is around a 1.8%
of the distribution, given by . Thus, we can be confident to a 95%        chance that the bent coin might
degree for  within 2, or between 0.17 and 0.33 (0.25 - 2 · 0.04 and       have a negative , or probability of
0.25 + 2 · 0.04, respectively).                                           flipping heads (look a the Normal
                                                                          curve to the left of  = 0)! The
                                                                          beta distribution is zero for any
                                                                          value below zero or over one, and
                                                                          thus will never lead to such absurd
                                                                          answers.

The Binomial Distribution                                                 Normal Approximation to the
Similarly, with the (discrete) binomial distribution (see Equation 3.3)   Discrete Binomial Distribution
we have the Normal approximation.
                                                                          Binomial(N, p) = Normal(µ = N · p,
   Normal Approximation to the Discrete Binomial Distribution
                                                                                                        = N · p(1 - p))
     Binomial(N, p) = Normal(µ = N · p,  = N · p(1 - p))

with examples
152 statistical inference for everyone

Binomial(N =10,p =0.25)   0.30                     Binomial
                          0.25                     Normal
                          0.20
                          0.15                  µ =2.50
                          0.10                   =1.37
                          0.05
                          0.00 0 2   4 6 8 10 12

and                                        k

    0.10                                                    Binomial
                                                            Normal
                          0.08
                                                   µ =25.00
Binomial(N =100,p =0.25)  0.06                      =4.33

                          0.04

                          0.02

                          0.000  20  40  k  60  80  100

The Student's t Distribution

For smallish data sets, 5 < N < 30, we can replace the estimate of the
mean from the Student's t distribution to a Normal distribution with
an increased estimate for the deviation. It then becomes practical to
use the more convenient z-score to estimate credible intervals rather
                                                  priors, likelihoods, and posteriors 153

than the full t tables. The approximation in this domain looks like2   2 D. Berry. Statistics: A Bayesian
                                                                       Perspective. Duxbury, 1996

Normal Approximation to the Student's t Distribution For               Normal Approximation to the
                                                                       Student's t Distribution For
smallish data sets, 5 < N < 30,                                        smallish data sets, 5 < N < 30,
                       
   Studentdof=N-1(x¯, S/ N)  Normal(x¯, Sk/ N)                                                          
                                                                            Studentdof=N-1(x¯, S/ N) 
                                   k  1 + N2 20
                                                                                                           
Example 7.6 Estimating the True Length of an Object...Yet Again                       Normal(x¯, k · S/ N)

                                                                           k  1 + N2 20

Say we have an object, and 5 measurements of its length from the
same ruler but from different people,

   5.1[cm], 4.9[cm], 4.7[cm], 4.9[cm], 5.0[cm]

Unlike earlier, let's say that we don't know the uncertainty (given this
ruler) of one measurement. What is the best estimate of the length?
Again, the best estimate should be given by the sample mean of these
5 samples,

  µ^ = x1 + x2 + · · · + xN
                        N

      = 5.1[cm] + 4.9[cm] + 4.7[cm] + 4.9[cm] + 5.0[cm] = 4.92[cm]
                                            5

with uncertainty related to the adjusted sample deviation,

      S2 = 1 N - 1 (x1 - x¯)2 + · · · + (xN - x¯)2
            = 1 (5.1[cm] - 4.92[cm])2 + (4.9[cm] - 4.92[cm])2 + (4.7[cm] - 4.92[cm])2+
                 5-1
                 (4.9[cm] - 4.92[cm])2 + (5.0[cm] - 4.92[cm])2

            = 0.024[cm]2

   S = 0.024[cm]2 = 0.155[cm]

S  0.155[cm]
=   = 0.069[cm]
N                   5

                  20
   k = 1 + 2 = 1.8

                  5

     S
k ·  = 1.8 · 0.069[cm] = 0.124[cm]

      N

yielding a final best estimate of

                       µ^ = 4.92[cm] ± 0.124[cm]

or (with 2 range),

   4.92[cm], 95% CI = [4.672[cm], 5.168[cm]]

Compare this range to the one shown in Example 7.5 on page 148.
The one here has a slightly larger range, which is a bit more conser-
vative than is needed, but the calculation is quite a bit easier.
154 statistical inference for everyone

7.5 Summary

It is useful to see all of these results stemming from the same Bayes'
Recipe, applied to different models of the data and (possibly) differ-
ent prior probabilities. As we have stated, many of the simple cases
have been worked out by the mathematicians, so we don't need to
do the work of deriving them. It will be our task to understand their
properties, to be able to apply them to real problems, and to under-
stand their consequences. One of the immediate observations that
we make is the prevalence of the Normal distribution, justifying our
detailed exploration of it in this chapter.

1 Proportions

Parameter of Interest: , the chances of a single event
Applications: coin flips, voting percentages, success in sports,

   performance on tests
Form of the data: h successes in N total events
Model of the data:

data =         success , with probability 
               failure , otherwise (i.e. with probability 1 - )

Posterior Probability:

                                      likelihood

Beta(|data)  Binomial(data|) × Uniform()

posterior probability                   prior probability

2 Magnitude with Known Deviation

Parameter of Interest: µ, the true magnitude of a quantity, given
   the deviation, labeled by , from the central value

Applications: percentages with large samples, scientific measure-
   ments such as weight and size of objects, time scales of events

Form of the data: N total data points, labeled x1, x2, · · · , xN, and
   given known 

Model of the data:

data = µ + uncertainty with probability Normal(µ = 0,known )

Posterior Probability:

                        likelihood

Normal(µ2|data, )  Normal(data|µ, ) × Uniform(µ)

posterior probability                   prior probability
                                           priors, likelihoods, and posteriors 155

3 Magnitude with Unknown Deviation

   Parameter of Interest: µ, the true magnitude of a quantity, and
      the unknown deviation, labeled by , from the central value

   Applications: scientific measurements with small samples (less
      than around 30), such as weight and size of objects, time scales
      of a small number of events

   Form of the data: N total data points, labeled x1, x2, · · · , xN
   Model of the data:

        data = µ + uncertainty with probability Normal(µ = 0, )

Posterior Probability:

P(µ, |data)                         likelihood

                        Normal(data|µ, ) × Uniform(µ) · Uniform(log )

posterior probability                      prior probability

Student - T(µ|data)  [P(µ, |data)]marginalized over 

   posterior probability

    F(|data)  [P(µ, |data)]marginalized over µ

posterior probability

7.6 Computer Examples

from s i e import *

Estimating Lengths

Known deviation,   4.7 ,  4.9 ,  5.0]
   x=[5.1 , 4.9 ,

   sigma =0.5

mu=sample_mean ( x )
N= l e n ( x )

d i s t =normal (mu, sigma/ s q r t (N) )
distplot ( dist )

<matplotlib.figure.Figure at 0x10713c710>
   156 statistical inference for everyone

    credible_interval ( dist )

    (4.4817387297117088, 4.9199999999999999, 5.358261270288291)

Unknown 
   mu=sample_mean ( x )
   s=sample_deviation (x)
    p r i n t mu, s

    4.92 0.148323969742

   d i s t = t d i s t (N-1,mu, s/ s q r t (N) )
    distplot ( dist , xlim =[4.6 ,5.4])

    <matplotlib.figure.Figure at 0x1085b5c50>
                                                                             priors, likelihoods, and posteriors 157

credible_interval ( dist )

(4.7358314667008017, 4.9199999999999999, 5.1041685332991982)
8 Common Statistical Significance Tests

The basic idea of common statistical tests in the approach we have           In some cases we are not comparing
taken has been the following:                                                the parameter to zero but to some
                                                                             theoretical expectation. Even there,
1 Observe some data                                                          we are comparing the parameter
                                                                             minus the theoretical expectation
2 Construct a model of the data, with a parameter that needs to be           to zero and thus we don't lose
   estimated, such as the "true" single value (µ, in Section 7.3), or the    any generality in the procedure by
   proportion of the event (, in Section 7.4).                               always comparing to zero.

3 Calculate the final, posterior probability of that parameter

4 "Test" to see if there is a significant (usually 95%) probability that
   the parameter is not zero.

5 If the test passes, then one can be reasonably confident that the
   parameter is non-zero - that the effect is real. If the test fails, then
   under the model, the possibility of a zero-effect cannot be reason-
   ably excluded.

   These tests are a subset of the parameter estimation techniques
covered in both Chapter 6 (Introduction to Parameter Estimation on
page 121) and Chapter 7 (Priors, Likelihoods, and Posteriors on page 139),
in the special case where we are interested in determining if there is
an effect at all. For example, we might be interested to see if a med-
ical treatment works, so we compare the before- and after-treatment
values to see if the difference is non-zero.

   The tests that one typically employs in simple cases go by various
names, depending on the model. This chapter summarizes several of
the common ones, and applies them to some typical cases.

8.1 z-test

The z-test is the simplest test to use, and is perhaps the most com-
mon. It is used when we have the following assumptions:
1 We are modeling the data as a true value, µ, with uncertainty
160 statistical inference for everyone

2 We are modeling the as a Normal distribution with known devia-
   tion, , as in Normal(0, ).

3 We are assuming independence between the measurements.

   The model of the data is

data = µ + uncertainty with probability Normal(µ = 0,known )

where µ represents the "true" value. The posterior distribution for µ
                                                                                     

also follows a Normal distribution, with a smaller uncertainty, / N
where N is the number of data points.

   To use the z-test, we perform the following steps:

1 Calculate our best estimate for µ, denoted as µ^.

2 Given the known uncertainty,  of a single measurement, determine
   the range of credible values for µ within the uncertainty of the
                                                 
   estimate for the N observations, / N.

3 Test to see if the credible range includes zero.

4 If so, then the test passes, and we can be reasonably confident that
   the parameter is non-zero - that the effect is real.

5 If the test fails, i.e. the credible range does not include zero, then
   under the model the possibility of a zero-effect cannot be reason-
   ably excluded.

   There are several scenarios where we use the z-test, each with
the same procedure, differing only in the method of estimating the
"true" value µ.

1 For N independent observations, x1, x2, . . . , xN we have the best
   estimate given by the sample mean, and uncertainty related to the
   single-measurement deviation, , as
                       µ^ = x1 + x2 + · · · + xN ± /N
                                         N

2 When estimating a proportion, for a large number of events N of
   which a fraction f  h/N are successful, we have

                                    µ^  f
                                 
                             / N  f (1 - f )/N

3 For smallish data sets, 5 < N < 30, where the uncertainty is not

known,

          µ^  x1 + x2 + · · · + xN
          N
        
        / N  kS/ N
                                       common statistical significance tests 161

                                             
   where we replace the known / N from the previous case with
   an estimate using the sample standard deviation and an adjust-
   ment for small data set parameter k,

                 S2 = 1 N - 1 (x1 - x¯)2 + · · · + (xN - x¯)2
                   k  1 + N2 20

8.2 What it means and doesn't mean

For all of these tests, we use the vocabulary of "statistical signifi-
cance", which needs to be further clarified.

Significance                                                                   1 Although the word "significant"
There is a term used in the literature called statistical significance.1       occurs in the term "statistically
Roughly it means a value that is very unlikely to be zero (see Table 1.1       significant," it does not imply that
on page 51), or in other words, the value of zero is not within the            the result itself is important - it may
95% percentile. This is within 2 standard deviations of the value, so          be a small, uninteresting effect, but
the following estimated values are not statistically significant:              credibly non-zero. Perhaps a term
                                                                               like "statistically detectable" would
· 5 ± 3 - the two-deviation range is [-1,11] contains the value 0              be better, but we are unfortunately
                                                                               bound to the historical use of the
· 7±4                                                                          term.

· -3 ± 2                                                                       Table 8.1: Rough guide for the
                                                                               conversion of deviations away
but the following are statistically significant:                               from zero and the qualitative labels
                                                                               for probability values for being a
· 5 ± 2 - the two-deviation range is [1,9] does not contain the value 0        significant deviation.

· 7±3

· -3 ± 1

   Statistical significance, at the very unlikely level (i.e. 95% percentile)
is often used as a rough guideline to publish a positive effect.

number of de-  term                    probability
viations away
from zero

 1             slightly likely/likely       0.7 (i.e. 7/10)
 2             very likely                0.95 (i.e. 19/20)
 3             extremely likely           0.01 (i.e. 1/100)
> 4            virtually certain       > 999, 999/1, 000, 000
162 statistical inference for everyone

An unintuitive consequence One consequence of this is that two stud-     2 A. Gelman, J. Hill, and Ebooks
ies that display different magnitudes for a quantity may not be statis-  Corporation. Data analysis using
tically significant in their difference. The following example is based  regression and multilevel/hierarchical
on an example from Gelman and Hill's book on Data Analysis.2 Say         models, volume 625. Cambridge
we have two measurements with means and standard deviations:             University Press Cambridge, UK:,
                                                                         2007
· 25±10
                                                                         The odd name "Student-t test"
· 10±3                                                                   comes from the fact the test was
                                                                         originally published by William
   Further, let us suppose that we are interested in whether the mea-    Gosset who worked at the Guinness
surements are zero or not. The first measurement shows a significant     brewery in Dublin and his pen
effect (the two-deviation range is [5,45] does not contain zero), and    name was "Student".
the second one does as well (the two-deviation range is [4,16] does
not contain zero). The difference between them is

                    (25 - 10) ± 102 + 32 = 15 ± 10.4

which is not significant. One should be careful comparing the magni-
tudes and uncertainties of measurements!

8.3 Student-t-test

When we are not given the uncertainty of the measurements, , and
the data are insufficient to estimate the uncertainty, then we need to
estimate both the "true" value, µ, and the uncertainty. This leads to
a wider credible range for the "true" value. We can apply the same
testing procedure, by looking at the 95% credible region to see if it
includes zero, but this time the posterior distribution we use comes
from the so-called Student-t distribution.

1 For N independent observations, we still have the best estimate of
   the "true" value given by the sample mean
                             µ^ = x1 + x2 + · · · + xN
                                               N

2 The best estimate of the uncertainty of a single measurement is
   given by the sample standard deviation

                                        ^ = S

   where S is the sample standard deviation

                 S2 = 1 N - 1 (x1 - x¯)2 + · · · + (xN - x¯)2

3 The credible region is determined by the 95% interval of the poste-
   rior, Student-t distribution, of the following form
                                                      
                           Studentdof=N-1(x¯, S/ N)
                                                              common statistical significance tests 163

4 Test to see if the credible range includes zero.

5 If so, then the test passes, and we can be reasonably confident that
   the parameter is non-zero - that the effect is real.

6 If the test fails, i.e. the credible range does not include zero, then
   under the model the possibility of a zero-effect cannot be reason-
   ably excluded.

8.4 Computer Examples

from s i e import *
data=load_data ( ' data/ i r i s . csv ' )
x _ s e r t o s a =data [ data [ ' c l a s s ' ]== ' I r i s -s e t o s a ' ] [ ' p e t a l l e n g t h [cm] ' ]

x= x _ s e r t o s a
mu=sample_mean ( x )
N= l e n ( x )
sigma=sample_deviation ( x )/ s q r t (N)
t _ s e r t o s a = t d i s t (N, mu, sigma )

print  " t o t a l number of data         p o i n t s : " ,N
print  " b e s t e s t i m a t e : " ,mu
print  " uncertainty : " , sigma

total number of data points: 50
best estimate: 1.464
uncertainty: 0.0245381834898

new_length =1.7

distplot ( t_sertosa , label= ' petal length ' , xlim =[1.37 ,1.8] ,
                               quartiles =[.01 ,0.05 ,.5 ,.95 ,.99] ,

)
ax=gca ( )
ax . axvline ( 1 . 7 , color= ' r ' )
savefig ( ' . . / . . / figs/ z _ t e s t _ i r i s . pdf ' )

<matplotlib.figure.Figure at 0x10f9d2710>
164 statistical inference for everyone
9 Applications of Parameter Estimation and Inference

9.1 Normal Model - Inference about Means

Example 9.1 Iris petal lengths - Best estimate

                              1.4 1.4 1.3 1.5 1.4                         Table 9.1: Iris petal lengths, in
                              1.7 1.4 1.5 1.4 1.5                         centimeters, for Iris type Setosa.
                              1.5 1.6 1.4 1.1 1.2
                              1.5 1.3 1.4 1.7 1.5                         1 K. Bache and M. Lichman. UCI
                              1.7 1.5 1.0 1.7 1.9                         machine learning repository, 2013.
                              1.6 1.6 1.5 1.4 1.6                         URL http://archive.ics.uci.edu/
                              1.6 1.5 1.5 1.4 1.5
                              1.2 1.3 1.5 1.3 1.5                         ml
                              1.3 1.3 1.3 1.6 1.9
                              1.4 1.6 1.4 1.5 1.4                           2.0

   Table 9.1 shows data for the lengths (in centimeters) of the petals    1.5
of one species of Iris flower1. If we want to estimate the "true" length                        
of the the petal for this species, given all of these examples, we would
apply the following model of the data:                                                (known deviation)
                                                                          1.0
            data = true value + Normal(mean=0,known )
                                                                          0.5
or equivalently
                                                                          0.0  0                         µ
               data = Normal(mean=true value,known )
                                                                                  (unknown true value)
The resulting distribution for the "true value", µ, is also a Normal
distribution (Section 7.3),

                                                          
                    P(µ|data, ) = Normal(x¯, / N)

where the best estimate of the true value, µ is the sample mean, x¯,
and the uncertainty is related to the sample deviation (which we're
166 statistical inference for everyone

going to take as the "known" deviation,   0.174) in this case. Thus,

      µ^ = x¯ = 1.4 + 1.4 + 1.3 + 1.5 + · · · + 1.6 + 1.4 + 1.5 + 1.4
                                                     50

           = 1.464

and the full answer, with uncertainty, is

                                                      0.174
                          µ^ = 1.464[cm] ±  [cm]

                                                         50
                              = 1.464[cm] ± 0.025[cm]

Example 9.2 Iris petal lengths - A different species?

   Here we apply the z-test (Section z-test on page 159) to a new
observation to see if there is reason to believe it to be a different
species. Imagine we have a single observation of another iris with
petal length 2.5 [cm]. Is this likely to be the same type as the Setosa
type above? As outlined in Section 7.2, we get the best estimate for
the difference as:

          µdiff = 2.5 - 1.464 = 1.036

with uncertainty the same as the uncertainty of the Setosa type, so the
final estimate with uncertainty is:

          1.036[cm] ± 0.025[cm]

which is

          1.036[cm] = 41 deviations away from zero!
          0.025[cm]

which makes it virtually certain to be a different type (see Table 8.1).

9.2 Normal Model Again - Inference about Means and Deviations

                        Setosa 1.4 1.4 1.3 1.5 1.4                          Table 9.2: Subset of iris petal
                       Virginica 6.0 5.1 5.9 5.6 5.8                        lengths, in centimeters, for iris
                      Versicolor 4.7 4.5 4.9 4.0 4.6                        types Virginica, Setosa, and Versi-
                                                                            color.
Example 9.3 Iris petal lengths - Significantly different?

   In this example we apply the t-test (Section Student-t-test on
page 162) to a subset of the iris samples, to see if the different species
can be reasonably separated using their petal lengths. Shown in Ta-
ble 9.2 is a very small subset of the full iris petal-length data. Are the
               applications of parameter estimation and inference 167

types Virginica and Versicolor longer than the type Setosa? Is the Vir-
ginica longer than Versicolor? For each of these, we need to specify the
model, determine the best estimate for the parameters of the model,
and then compare the distributions.

   The model we will use is the simple Normal model,

             data = Normal(mean=true value,unknown )

which is the same as the previous example, except that the deviation,
, is unknown. In addition to being unknown, there are so few data
points that the deviation can't be well approximated with the sample
deviation.

   The resulting distribution for the "true value", µ, is a Student-t
distribution (Section 7.3),

                                                             
                 P(µ|data) = Studentdof=N-1(x¯, S/ N)

The best estimates for the true length-values of each type is given by
their sample means,

                 µ^setosa = 1.4 + 1.4 + 1.3 + 1.5 + 1.4 = 1.40
                                              5

              µ^virginica = 6.0 + 5.1 + 5.9 + 5.6 + 5.8 = 5.68
                                              5

             µ^versicolor = 4.7 + 4.5 + 4.9 + 4.0 + 4.6 = 4.54
                                              5

and the sample deviations for each is given by

Ssetosa =        1 · ((1.4 - 1.40)2 + (1.4 - 1.40)2 + (1.3 - 1.40)2 + (1.5 - 1.40)2 + (1.4 - 1.40)2)
               5-1

= = 0.07

Svirginica =     1 · ((6.0 - 5.68)2 + (5.1 - 5.68)2 + (5.9 - 5.68)2 + (5.6 - 5.68)2 + (5.8 - 5.68)2)
               5-1

= = 0.36

Sversicolor =    1 · ((4.7 - 4.54)2 + (4.5 - 4.54)2 + (4.9 - 4.54)2 + (4.0 - 4.54)2 + (4.6 - 4.54)2)
               5-1

= 0.34

   The posterior probability distributions, shown in Figure 9.1, have

the following form:
                                                                         

               P(µsetosa|data) = Studentdof=4(1.40, 0.07/5)
            P(µvirginica|data) = Studentdof=4(5.68, 0.36/5)
            P(µversicolor|data) = Studentdof=4(4.64, 0.34/ 5)

It is clear from the picture that they are very well separated, but we
can quantify this by looking at the probability that the difference
between their means is greater than zero.
168 statistical inference for everyone

                      12                                Setosa           Figure 9.1: Probability distributions
                                                                         for the subset of iris petal lengths.
                      10                                Virginica        Each distribution follows a Student-
                                                                         t form.

                                                        Versicolor

P(Petal Length|data)  8

                      6

                      4

                      2

                      00 1 2 3 4 5 6 7 8
                                              Petal Length [cm]

   The probability of their difference approximately takes the form of   This approximation is called
a Student's t distribution, with the same center and deviation shown     Welch's method. The exact anal-
for the Normal in Section 7.2. Here we do the calculation between the    ysis is beyond this book, but
closest two iris types, Virginica and Versicolor:                        numerically one can calculate
                                                                         it and it doesn't differ from
                          µdiff = 5.68 - 4.64 = 1.04                     this approximate analysis in
                                                                         any significant way. Essentially
                          diff =  0.362 + 0.342 = 0.22                   you calculate P(µversicolor >
                                  5  5                                   µvirginica|data) by adding up the
                                                                         P(µversicolor|data) × P(µvirginica|data)
The degrees of freedom used for this Student's t distribution is ap-     for all possible lengths where
proximately the smallest one from the two samples, or in this case       versicolor is longer than virginica.
(since both samples have the same number of data points), dof=4.
The resulting posterior probability distribution for the difference of   2 David J Hand, Fergus Daly, K Mc-
means is shown in Figure 9.2.                                            Conway, D Lunn, and E Ostrowski.
                                                                         A handbook of small data sets, vol-
   We observe that the difference of the means is over 4 times the       ume 1. CRC Press, 2011
deviation away from zero, so even with 4 degrees of freedom, this is
significant at the 99% level. We can be highly certain that these two
species have different petal lengths, and that the difference observed
is not just a product of the random sample.

Example 9.4 Ball Bearing Sizes

   Here's a data data set, measuring the size of ball bearings2 from
two different production lines.

   We can ask questions such as:

· What is our best estimate of the size of a ball bearing, given one of
   the production lines?
                                                    applications of parameter estimation and inference 169

                                2.0                 50%                                       Figure 9.2: Probability distributions
                                                                                              for the difference between iris petal
P(Lvirginica-Lversicolor|data)  1.5                 25% 75%                                   lengths for the closest two iris
                                                                                              types, Virginica and Versicolor. The
                                                                                              distribution follows a Student-t
                                                                                              form, and clearly shows significant
                                                                                              probability (greater than 99%) for
                                                                                              being greater than zero.

                                1.0            10%           90%
                                            5%                   95%
                                0.5
                                        1%  0.570.70 0.88 1.04 1.20 1.381.51        99%
                                            0.5     1.0                        1.5  1.86 2.0
                                0.0              Petal Length Difference [cm]
                                 0.0 0.22

                              First line [microns]                                            Table 9.3: Production lines are pro-
    1.18 1.42 0.69 0.88 1.62 1.09 1.53 1.02 1.19 1.32                                         duce a ball bearing with a diameter
                                                                                              of approximately 1 micron. Ten ball
                             Second line [microns]                                            bearings were randomly picked
    1.72 1.62 1.69 0.79 1.79 0.77 1.44 1.29 1.96 0.99                                         from the production line (i.e. the
                                                                                              First line) at one time, and then
· Is it reasonable to believe that there is a difference in the size pro-                     again for a different production line
   duced between the two lines?                                                               (i.e. the Second line). Romano, A.
                                                                                              (1977) Applied Statistics for Science
                                                                                              and Industry.

Example 9.5 What is the best estimate (and uncertainty) for each of the
two production lines of ball bearings?

   Using the normal approximation to the Student-T distribution
(Section 7.4), we have the best estimates of the two lines as

 µ1 = 1.180000 + 1.420000 + 0.690000 + · · · + 1.190000 + 1.320000
                                                   10

       = 1.194

µ2 = 1.720000 + 1.620000 + 1.690000 + · · · + 1.960000 + 0.990000
                                                  10

     = 1.406

and their uncertainties calculated by first calculating the sample
deviations

S1 =                                    1 · ((1.18 - 1.194)2 + (1.42 - 1.194)2 + · · · + (1.19 - 1.194)2 + (1.32 - 1.194)2)
                                     10 - 1

= 0.289

S2 =                                    1 · ((1.72 - 1.406)2 + (1.62 - 1.406)2 + · · · + (1.96 - 1.406)2 + (0.99 - 1.406)2)
                                     10 - 1

= 0.428
170 statistical inference for everyone

and then scaling the deviations by the number of data points

1 = S1/sqrt10 = 0.092
2 = S2/sqrt10 = 0.135

yielding the best estimates and uncertainties for the two production            This is just the ±2 ·  range
lines
· Production line 1: 1.194 [microns]± 0.092[microns]
· Production line 2: 1.406 [microns]± 0.135[microns]

or looking at the 95% CI for each line

· Production line 1: 1.01[microns] - 1.378[microns]

· Production line 2: 1.136[microns] - 1.676[microns]

Roughly, given that these intervals overlap, there is not strong evi-
dence that there is a difference between the two lines.

Example 9.6 Is it reasonable to believe that there is a difference in the size
produced between the two lines?

Using the best estimate of the difference, we get

12 = µ2 - µ1 = 0.212

with the uncertainty in the difference from the individual uncertain-
ties,

12 =  2   +  2   =    0.163

       1      2

So the 2 ·  uncertainty range for the difference,

[0.212 - 2 · 0.163, 0.212 + 2 · 0.163] = [-0.114, 0.538]

includes the value zero, which we interpret as a statement that the
difference is not statistically significant. In other words, it is not reason-
able to believe that there is a difference in the size produced between
the two lines.

9.3 Beta Model - Inference About Proportions

Example 9.7 The Sunrise Problem

   The sunrise problem, as first stated by Laplace, is "What is the
probability that the sun will rise tomorrow?" We'll start with the as-
sumption that initially one has never seen a sunrise, and then observe
a year of sunrises each morning with no morning without one. Thus
                                applications of parameter estimation and inference 171

we have the form of the data as h successes (days with a sunrise) in
N total days. Our model of the data is specified as before with a bi-
nomial distribution, resulting in the posterior Beta, as described in
Section 6.6.

   After a only 10 years of watching sunrises, and no failures of a
sunrise, the best estimate for the probability of a sunrise is

                         ^median  h + 2
                                          N+4

                                    = 3650 + 2 = 0.9995
                                          3650 + 4

making it virtually certain for a sunrise.

Example 9.8 Cancer Rates                                                     3 D. Berry. Statistics: A Bayesian
                                                                             Perspective. Duxbury, 1996
   This example is from Donald Berry's Statistics textbook3:

    pp 192: A study (Murphy and Abbey, Cancer in Families, 1959) ad-
    dressed the question of whether cancer runs in families. The investiga-
    tor identified 200 women with breast cancer and another 200 women
    without breast cancer and asked them whether their mothers had had
    breast cancer. Of the 400 women in the two groups combined, 10 of the
    mothers had had breast cancer. If there is no genetic connection, then
    about half of these 10 would come from each group.

The data is that 7 of the daughters had cancer and 3 did not. Is there
strong evidence of a connection?

   The proper way, assuming total initial ignorance, is to use the Beta
distribution:

                  P (cancer|data) = Beta(h = 7, N = 10)

which has a median of ^cancer = 0.68, but a 95% credible interval of
^cancer = 0.39 up to ^cancer = 0.89. This means there is not strong
evidence of an effect.

Example 9.9 Cancer Rates - Normal Approximation

We can estimate the the Beta distribution median and credible

intervals with a Normal distribution, by using the "assuming 2 suc-

cesses and 2 failures" method.

     ^cancer =                  h+2

                                N+4

        = 7 + 2 = 0.643
             10 + 4

and

      = ^cancer(1 - ^cancer)/(N + 4)

     = 0.643(1 - 0.643)/(10 + 4)
     = 0.128
172 statistical inference for everyone

So the approximate 95% credible interval is
                                   ^cancer ± 2

which is between 0.387 and 0.899, again with the same conclusion of
no strong evidence of an effect.
Example 9.10 Will it rain on the 4th of July?

   In the United States, the 4th of July is Independence Day, and is
known for parades. The oldest continuously running parade is in
Bristol, RI, and it runs rain or shine. Is it likely to rain on the pa-
rade? Climate data from nearby Providence is here from wunder-
ground.com:

   We can estimate the the Beta distribution median and credible
intervals with a Normal distribution, by using the "assuming 2 suc-
cesses and 2 failures" method.

^rain = h+2

        N+4

   = 19 + 2 = 0.404
        48 + 4

around 40%, less than an even chance (50%) of rain, but

 = ^rain(1 - ^rain)/(N + 4)
    = 0.404(1 - 0.404)/(48 + 4)
    = 0.068

So the approximate 95% credible interval is
                                    ^rain ± 2

which is between 0.268 and 0.540. This is not strong evidence against
a purely fair and random "coin flip" for rain on the 4th of July.

Example 9.11 Hot Hand Reexamined
                   applications of parameter estimation and inference 173

   In Tversky and Gilovich4 we have the following data for Larry         4 A. Tversky and T. Gilovich. The
Bird free throws in basketball:                                          cold facts about the" hot hand" in
                                                                         basketball. Anthology of statistics in
· Given each of 53 missed shots, Larry Bird successfully shot 48 of      sports, 16:169, 2005
   the next attempt.
                                                                         5 this data was extracted from
· Given each of 285 successful shots, Larry Bird successfully shot       student measurements during a
   251 of the next attempt.                                              physics lab

   This data alone almost suggests an anti-hot-hand (where you're
less likely to make a successful attempt following a successful shot).
However, we can demonstrate that these numbers are not in fact
statistically different. Given the relatively large number of attempts
(greater than 30) we can use the Normal approximation to estimate
the two probabilities of success:

                                                 48 + 2
                        after a miss = 53 + 4 = 0.877

                                                 251 + 2
                    after a success = 285 + 4 = 0.875

and the uncertainty,

   after a miss =  0.877(1 - 0.877)/(53 + 4) = 0.044
after a success =  0.875(1 - 0.875)/(285 + 4) = 0.019

making the 95% credible intervals for probability of a Larry Bird
successful attempt

           95%CIafter a miss = 0.877 ± 2 · 0.044 = [0.789, 0.965]
       95%CIafter a success = 0.875 ± 2 · 0.019 = [0.837, 0.913]

Notice that the intervals overlap, so there is no significant evidence
for a difference in Larry Bird's success following another success
or following a miss. Thus, there is no significant evidence for a hot
hand, or an anti-hot hand.

9.4 Model Construction

In practice, we either don't know what the optimum model we need
is, or the needs of the model change as we obtain more data.

   We start with the data in Table 9.4 for the mass of pennies of vari-
ous years (shown graphically in Figure 9.3)5:

   We are going to ignore the measurement uncertainties in these
individual measurements, because they are quite small.

Example 9.12 Mass of the Penny, Model 1 - One True Value
174 statistical inference for everyone

                    Year  Mass [g]      Table 9.4: Mass of Pennies from
                    1960    3.133       1960 to 1974.
                    1961    3.083
                    1962    3.175
                    1963    3.120
                    1964    3.100
                    1965    3.060
                    1966    3.100
                    1967    3.100
                    1968    3.073
                    1969    3.076
                    1970    3.100
                    1971    3.110
                    1972    3.080
                    1973    3.100
                    1974    3.093

Mass per Penny [g]                                                                                                                   Figure 9.3: Mass of Pennies from

                    3.20 1960 to 1974.
                    3.15
                    3.10
                    3.05
                    3.00

                      1960 1962 1964 1966 1968 1970 1972 1974
                                                                year
    applications of parameter estimation and inference 175

   If we assume a model that states that there is a "true" value and
the variation from this "true" value caused by some unknown pro-
cess, but with known magnitude, ,

            data = true value + Normal(mean=0,known )

or equivalently

               data = Normal(mean=true value,known )

we can get the best estimate and uncertainty in that estimate from
the following procedure, Using the normal approximation to the
Student-T distribution (Section 7.4):

                                                     
                               µ^ = x¯ ± k · S/ N

where the symbols in this equation are

1 the number of data points, N.

2 the best estimate for the true value, µ^, is given by the sample mean,
   x¯:
                      x¯ = = x1 + x2 + · · · + xN
                                                N
                          = 3.133g + 3.083g + · · · + 3.093g
                                                    15
                          = 3.100g

3 The uncertainty is directly related to the sample standard deviation,
   S:

S=  (x1 - x¯)2 + (x2 - x¯)2 + · · · + (xN - x¯)2
                         N-1

= (3.133g - 3.100g)2 + (3.083 - 3.100g)2 + · · · + (3.093 - 3.100g)2
                                                  14

= 0.0278g

4 The scale factor, k, adjusts for the small number of data points -
   there is more uncertainty in our estimate when there are fewer
   data points:

                                                      20
                                     k = 1+ 2

                                                      N
                                                      20
                                         = 1+ 2
                                                      15
                                         = 1.0889

   Finally, we have the best estimate and uncertainty for the pennies
in this dataset:

                                           
                     µ^ = x¯ ± k · S/ N 

                         = 3.100g ± 1.0889 · 0.0278g/ 15
                         = 3.100g ± 0.0078g
176 statistical inference for everyone

or, as a 99% credible range (3 times the uncertainty written above),
we have, (see also Figure 9.4)

99% CI for µ = 3.100g ± 3 × 0.0078g
                   = [3.077g, 3.124g]

Example 9.13 Mass of the Penny, Model 1 - One True Value with More
Data

   Now we collect the additional data with more recent pennies
shown in Table 9.5. We can follow the same procedure, assuming
our original model of one "true" value, to get the best estimate and
uncertainty for this model, combining the two data sets.

Year  Mass [g]                                                            Table 9.5: Mass of Pennies from
1989    2.516                                                             1989 to 2003.
1990    2.500
1991    2.500
1992    2.500
1993    2.503
1994    2.500
1995    2.497
1996    2.500
1997    2.494
1998    2.512
1999    2.521
2000    2.499
2001    2.523
2002    2.518
2003    2.520

                                                     
                               µ^ = x¯ ± k · S/ N

where the symbols in this equation are

1 the number of data points, N = 30.

2 the best estimate for the true value, µ^, is given by the sample mean,
   x¯:
                      x¯ = 3.133g + 3.083g + · · · + 2.520g
                                                    30
                          = 2.804g
                                     applications of parameter estimation and inference 177

p(µ) Mass per Penny [g]3.18                                                                                               Figure 9.4: Mass of Pennies from
                       3.16                                                                                               1960 to 1974, with best estimates
                       3.14                                                                                               and 99% CI (i.e. 3) uncertainty.
                       3.12
                       3.10        Best estimate of "true" value: µ^ =3.100 ±0.0078
                       3.08                                 99% CI: [3.077,3.124]
                       3.06
                               1962 1964 1966 1968 1970 1972 1974
                         1960                               year

                         60 Best Estimate for µ =3.100 ±0.0078 grams
                             99% CI for µ :[3.077,3.124]

                         50

                         40

                         30

                         20

                         10

                         0

                         3.00  3.05  3.10                             3.15  3.20
                                     µ [g]
178 statistical inference for everyone

3 The sample standard deviation, S:

S=  (3.133g - 2.804g)2 + (3.083 - 2.804g)2 + · · · + (2.520 - 2.804g)2
                                             29

= 0.3024g

4 The scale factor, k, adjusting for the small number of data points:

                                                      20
                                     k = 1+ 2

                                                      30
                                         = 1.0222

Finally, we have the best estimate and uncertainty for the pennies

in this full dataset:                   
                     µ^ =   x¯ ± k · S/ N 
                         =  2.804g ± 1.0222 · 0.3024g/ 30

           = 2.804g ± 0.0564g

or, as a 99% credible range (3 times the uncertainty written above),
we have,

                   99% CI for µ = 2.804g ± 3 × 0.0564g
                                      = [2.634g, 2.973g]

   There are several things one should notice:

1 The scale factor, k, is less for 30 data points than it is for 15 data
   points. This is because the adjustment for small number of data
   points gets less relevant as we obtain more data. This is what we
   expect.

2 Despite there being twice as much data, our uncertainty increased.
   This is unusual, if our model is correct - more data should sharpen
   the estimates. Although it is possible that adding more data in-
   creased the system variability somehow, it is more likely that some
   assumption of our model is incorrect. This becomes obvious when
   we look at the result graphically, shown in Figure 9.5.

   This should highlight a few things:

1 Always look at your data graphically. What you might miss look-
   ing at a table of numbers, you'll catch with a picture.

2 Assume your model is wrong, and outline other possible models
   ahead of time and explore them. The most obvious improvement
   in this problem is to notice that we are dealing with two separate
   "true" values, possibly caused by a change in the manufacturing
   materials.
                           applications of parameter estimation and inference 179

Mass per Penny [g]  3.2                                                                                                   Figure 9.5: Mass of Pennies from
                    3.1                                                                                                   1960 to 2003, with best estimates
                    3.0                                                                                                   and 99% CI (i.e. 3) uncertainty.
                    2.9
                    2.8      Best estimate of "true" value??: µ^ =2.804 ±0.0564
                    2.7                                 99% CI: [2.634,2.973]
                    2.6
                    2.5    1965 1970 1975 1980 1985 1990 1995 2000 2005
                    2.4                                    year
                     1960

Example 9.14 Mass of the Penny, Model 2 - Two True Values

   In this model, we assume there are two true values:
· µ1 - before 1975
· µ2 - after 1988

   There are two roughly equivalent ways of telling whether there is
a significant difference.

Overlapping Intervals The first is the easiest to do mathematically,
and yields a nice picture: obtain the best estimates for µ1 and µ2,
and see if their 99% credible intervals overlap. From this analysis
(identical to the previous examples, however we leave the details of
the calculation to the student), we get (see Figure 9.6):

· Best estimate for µ1
                                µ^1 = 3.100 ± 0.0078

   with 99% CI: [3.077,3.124].
· Best estimate for µ2

                               µ^2 = 2.507 ± 0.0029
   with 99% CI: [2.498,2.516]
180 statistical inference for everyone

where the 99% credible intervals (CI) clearly do not overlap, thus
there is a statistically significant difference between them.

                    3.2                                                                Figure 9.6: Mass of Pennies from
                                                                                       1960 to 2003, with best estimates
                    3.1                                                                for the two true values and their
                                                                                       99% CI (i.e. 3) uncertainty plotted.
                    3.0 Best estimate µ^1 =3.100 ±0.0078                               There is clearly no overlap in their
Mass per Penny [g]             99% CI: [3.077,3.124]                                   credible intervals, thus there is a
                    2.9                                                                statistically significant difference
                                                                                       between them.
                    2.8
                                                                                 2005
                    2.7                        Best estimate µ^2 =2.507 ±0.0029

                    2.6                                   99% CI: [2.498,2.516]

                    2.5

                    2.4
                     1960 1965 1970 1975 1980 1985 1990 1995 2000

                                                               year

Is the Difference Zero? The proper way is to estimate the quantity
µ1 - µ2 and test to see if it is greater than zero, as shown in Sec-
tion 7.2 on page 144. The estimate of this quantity, which we'll call
12 = µ1 - µ2 is related to the means and uncertainties of two data
sets

                         ^12 = x¯1 - x¯2 ± 12

                         12 =  2 + 2

                                12

                         1 = k1S1/ N1 (uncertainty from data set 1)

                         2 = k2S2/ N2 (uncertainty from data set 2)

where the sample standard deviations, S1 and S2, and the scale fac-
tors, k1 and k2 were calculated earlier. This leads to, for this data set,

                             ^12 = 0.593g ± 0.008g

with the 99% credible interval [0.568g, 0.618g], the distribution shown
in Figure 9.7. Again, the estimated quantities are clearly different sta-
tistically: the value of zero is well outside of the 99% credible interval
for 12.

9.5 Computer Examples
                           applications of parameter estimation and inference 181

                                                                  Figure 9.7: Difference in the esti-

                                                                  mated values of the pre- and post

                                                                  1975 pennies, µ1 - µ2. The value

                                                                  zero is clearly outside of the 99% in-

            50 Best Estimate for µ1 -µ2 =0.593 ±0.008 terval of the difference, thus there is a statistically significant difference
                      99% CI for µ1 -µ2 :[0.568,0.618]
                                                                  between the two values µ1 and µ2.

            40

p(µ1 -µ2 )  30

            20

            10

            0

            0.0  0.1  0.2  0.3  µ1 -µ2  0.4  0.5        0.6  0.7

from s i e import *

Iris Example

data=load_data ( ' data/ i r i s . csv ' )

x _ s e r t o s a =data [ data [ ' c l a s s ' ]== ' I r i s -s e t o s a ' ] [ ' p e t a l l e n g t h [cm] ' ]
x _ v i r g i n i c a =data [ data [ ' c l a s s ' ]== ' I r i s -v i r g i n i c a ' ] [ ' p e t a l l e n g t h [cm] ' ]
x _ v e r s i c o l o r =data [ data [ ' c l a s s ' ]== ' I r i s -v e r s i c o l o r ' ] [ ' p e t a l l e n g t h [cm] ' ]

print x_sertosa [ : 1 0 ] # print the f i r s t 10

0 1.4
1 1.4
2 1.3
3 1.5
4 1.4
5 1.7
6 1.4
7 1.5
8 1.4
9 1.5
Name: petal length [cm], dtype: float64
182 statistical inference for everyone

x= x _ s e r t o s a
mu=sample_mean ( x )
N= l e n ( x )
sigma=sample_deviation ( x )/ s q r t (N)
t _ s e r t o s a = t d i s t (N, mu, sigma )

print  " t o t a l number of data         p o i n t s : " ,N
print  " b e s t e s t i m a t e : " ,mu
print  " uncertainty : " , sigma

total number of data points: 50
best estimate: 1.464
uncertainty: 0.0245381834898

x=x _ v e r s i color
mu=sample_mean ( x )
N= l e n ( x )
sigma=sample_deviation ( x )/ s q r t (N)
t _ v e r s i c o l o r = t d i s t (N, mu, sigma )

print  " t o t a l number of data         p o i n t s : " ,N
print  " b e s t e s t i m a t e : " ,mu
print  " uncertainty : " , sigma

total number of data points: 50
best estimate: 4.26
uncertainty: 0.0664554477121

x=x _ v i r g i nica
mu=sample_mean ( x )
N= l e n ( x )
sigma=sample_deviation ( x )/ s q r t (N)
t _ v i r g i n i c a = t d i s t (N, mu, sigma )

print  " t o t a l number of data         p o i n t s : " ,N
print  " b e s t e s t i m a t e : " ,mu
print  " uncertainty : " , sigma

total number of data points: 50
best estimate: 5.552
uncertainty: 0.078049696361

distplot2 ([ t_sertosa , t_versicolor , t_virginica ] , show_quartiles=False )

<matplotlib.figure.Figure at 0x1058d9690>
                                                 applications of parameter estimation and inference 183

distplot ( t_virginica )

credible_interval ( t_versicolor )

(4.1265203051077082, 4.2599999999999998, 4.3934796948922914)

credible_interval ( t_virginica )

(5.3952325713636533, 5.5519999999999996, 5.7087674286363459)

Sunrise
184 statistical inference for everyone
d i s t = b e t a ( h =365 ,N=365)
distplot ( dist )

credible_interval ( dist )

(0.98997171634278669, 0.99810794743679487, 0.99993082805373457)

Cancer Example
d i s t = b e t a ( h=7 ,N=10)
distplot ( dist , figsize =(8 ,5))
                                                 applications of parameter estimation and inference 185

credible_interval ( dist )

(0.39025744042757882, 0.67619553741481253, 0.89073655618090186)

   Essentially no evidence of any effect over 50 percent.

Pennies

data1=load_data ( ' data/pennies1 . csv ' )
print data1
year , mass=data1 [ ' Year ' ] , data1 [ ' Mass [ g ] ' ]

Year Mass [g]

0 1960   3.133

1 1961   3.083

2 1962   3.175

3 1963   3.120

4 1964   3.100

5 1965   3.060

6 1966   3.100

7 1967   3.100

8 1968   3.073

9 1969   3.076

10 1970  3.100

11 1971  3.110

12 1972  3.080

13 1973  3.100

14 1974  3.093

plot ( year , mass , ' o ' )
xlabel ( ' year ' )
y l a b e l ( ' Mass per Penny [ g ] ' )

<matplotlib.text.Text at 0x1087c2d90>
186 statistical inference for everyone

x=mass
mu=sample_mean ( x )
N= l e n ( x )
sigma=sample_deviation ( x )/ s q r t (N)
t_penny1= t d i s t (N, mu, sigma )
d i s t p l o t ( t_penny1 , l a b e l = ' mass [ g ] ' )

CI=c r e d i b l e _ i n t e r v a l ( t_penny1 , percentage =99)
print CI
                                                 applications of parameter estimation and inference 187

(3.0790129206702002, 3.1002000000000001, 3.1213870793298)

plot ( year , mass , ' o ' )
credible_interval_plot ( t_penny1 , percentage =99)
xlabel ( ' year ' )
y l a b e l ( ' Mass per Penny [ g ] ' )

<matplotlib.text.Text at 0x1087fcf10>

Do the 2 datasets                                     [g] ']
   data2=load_data ( ' data/pennies2 . csv ' )

    print data2

   year1 , mass1=year , mass

   year2 , mass2=data2 [ ' Year ' ] , data2 [ ' Mass

Year Mass [g]

0 1989   2.516

1 1990   2.500

2 1991   2.500

3 1992   2.500

4 1993   2.503

5 1994   2.500

6 1995   2.497

7 1996   2.500

8 1997   2.494

9 1998   2.512

10 1999  2.521

11 2000  2.499

12 2001  2.523

13 2002  2.518
188 statistical inference for everyone

14 2003  2.520

x=mass1
mu=sample_mean ( x )
N= l e n ( x )
sigma=sample_deviation ( x )/ s q r t (N)
t_penny1= t d i s t (N, mu, sigma )

x=mass2
mu=sample_mean ( x )
N= l e n ( x )
sigma=sample_deviation ( x )/ s q r t (N)
t_penny2= t d i s t (N, mu, sigma )

d i s t p l o t 2 ( [ t_penny1 , t_penny2 ] , show_quartiles=False , l a b e l = ' mass [ g ] ' )
legend ( [ r ' $\mu_1$ ' , r ' $\mu_2$ ' ] )

<matplotlib.figure.Figure at 0x1087d3f10>
<matplotlib.legend.Legend at 0x1088198d0>

plot ( year1 , mass1 , 'o ' )
credible_interval_plot ( t_penny1 , percentage =99)
plot ( year2 , mass2 , ' ro ' )
credible_interval_plot ( t_penny2 , percentage =99 , xlim =[1989 ,2005])
xlabel ( ' year ' )
y l a b e l ( ' Mass per Penny [ g ] ' )

<matplotlib.text.Text at 0x10907e310>
                                                    applications of parameter estimation and inference 189

Distribution of the difference, normal approximation
   N1= l e n ( mass1 )
   N2= l e n ( mass2 )
   mu1=sample_mean ( mass1 )
   mu2=sample_mean ( mass2 )
   sigma1 = (1+2 0.0/N1 * * 2 ) * s a m p l e _ d e v i a t i o n ( mass1 )/ s q r t (N1)
   sigma2 = (1+2 0.0/N2 * * 2 ) * s a m p l e _ d e v i a t i o n ( mass2 )/ s q r t (N1)
   d e l t a _ 1 2 =mu1-mu2
   sigma_delta12=sqrt ( sigma1 **2+ sigma2 **2)
    dist_delta=normal ( delta_12 , sigma_delta12 )
    distplot ( dist_delta )
190 statistical inference for everyone

clearly larger than zero at well over the 99

Ball Bearing Sizes

data1 =[1.18 ,1.42 ,0.69 ,0.88 ,1.62 ,1.09 ,1.53 ,1.02 ,1.19 ,1.32]
data2 =[1.72 ,1.62 ,1.69 ,0.79 ,1.79 ,0.77 ,1.44 ,1.29 ,1.96 ,0.99]
N1= l e n ( data1 )
N2= l e n ( data2 )

mu1=sample_mean ( data1 )
mu2=sample_mean ( data2 )
p r i n t mu1 , mu2

1.194 1.406

S1=sample_deviation ( data1 )
S2=sample_deviation ( data2 )
print S1 , S2

0.289681817786 0.428309337849

sigma1=S1/ s q r t (N1)
sigma2=S2/ s q r t (N2)
print sigma1 , sigma2

0.091605434094 0.135443305072

d i s t 1 =normal (mu1 , sigma1 )                                    [ microns ] ' )
d i s t 2 =normal (mu2 , sigma2 )
distplot2 ([ dist1 , dist2 ] , show_quartiles=False , label= ' size
legend ( [ r ' $\mu_1$ ' , r ' $\mu_2$ ' ] )
                                                 applications of parameter estimation and inference 191

<matplotlib.figure.Figure at 0x105d61390>
<matplotlib.legend.Legend at 0x108ca1ad0>
10 Multi-parameter Models

We have already met examples of multiple parameter estimation in
the case of unknown uncertainty, where we have to estimate both
the "true" value, µ, and the uncertainty, . In this chapter, we intro-
duce the model of linear regression, which has multiple "true" value
parameters and their uncertainty. In the simple cases, we can calcu-
late the estimates by hand and apply the same testing procedures
as described in Chapter 8 (Common Statistical Significance Tests on
page 159). In the more complex cases we will have to rely on the
computer to give us the estimates, but we can still interpret them in
the same way as before.

10.1 Simple Linear Regression

In simple linear regression, we are given data consisting of two vari-    1
ables, typically denoted x and y, where the value of one (y) depends
on the other (x). For example, consider the following data of heights     Table 10.1: Heights (in inches) and
(x) and shoe sizes (y) of a small number of individuals1 shown in         shoe sizes from a subset of McLaren
Table 10.1 and Figure 10.1. By eye we can see a direct correlation - the  (2012) data.
taller the person the larger shoe size.

Height [inches]                Shoe Size
                                    7
       64.0                         9
       70.0                         8
       64.0                        11
       71.0                        12
       69.0                         9
       68.0                        10
       69.0                         6
       61.0                        10
       68.0                         9
       70.0

We propose a model of this data of the following linear form:
                                y = mx + b
194 statistical inference for everyone                                                    Figure 10.1: Heights (in inches) and
                                                                                          shoe sizes from a subset of McLaren
     14                                                                                   (2012) data.
     12
Shoe Size10
      8
      6
      4

       60 62 64 66 68 70 72
                           Height [inches]

where m is the slope and b is the intercept. Clearly this data doesn't
form a perfect line, so there is some uncertainty in the slope, inter-
cept, and predicted y values. We assume a Normal distribution for
the uncertainties in the data, so the statistical model looks like, for
each data point,

                        yi = mxi + b + Normal(0, )

where we want to obtain estimates, m^ and b^, of the "true" values of
the slope and intercept, respectively, as well as their uncertainties.
This is obtained by getting the posterior probability of the parame-
ters,

                                  P(m, b|data)

Following our standard procedure,

1 Specify the prior probabilities for the parameters being considered.
   For most simple cases we begin with absolutely no knowledge of
   its value, and thus use a uniform prior probability for each parame-
   ter.

2 Write the top of Bayes' Rule,

                P(m, b|data)  P(data|m, b) × P(m, b)

                                                      Normal uncertainties uniform prior
                                                                                multi-parameter models 195

3 Add up the values, and divide by this sum to get the final pos-
   terior probabilities. This is done by the mathematicians, and we
   simply summarize the results here.

we obtain the posterior distributions for the parameters m and b.
The calculations get too detailed to do by hand, but are very easy
with the computer. For the shoe size data in Table 10.1 we get the
distributions shown in Figures 10.2 and 10.3 for the slope and inter-
cept, respectively. The most probable values then lead to the best fit,
shown in Figure 10.4.

   The Student-t test clearly shows that the slope is non-zero (well
over 95% of the distribution lies to the right of zero), denoting a sta-
tistically significant effect on shoe size from height. The magnitude of
the slope, slope = 0.42, can be interpreted that every inch of height
leads to a 0.42 increase in shoe size on average.

                                             50%                                Figure 10.2: Posterior distribution
          4                                                                     for the slope for the linear model on
                                                                                the shoe size data subset.
                                       25% 75%
          3

P(Slope)  2                        10%            90%
                                                     95%
                               5%                            99%
          1 1%

          0        0.11  0.22 0.35 0.50 0.62                          0.73
              0.0                  0.27 0.42 0.57                          0.8
                         0.2            0.4       0.6
                                        Slope

Mean Squared Error

Another way of looking at the same idea is to introduce the notion
of Mean Squared Error (MSE). This is defined to be the number re-
sulting from taking the predicted values minus the observed values,
squaring them, and taking their mean. The squaring ensures that
deviations from the predictions both too high and too low are con-
sidered the same. The closer the prediction overall, the smaller the
resulting MSE. Mathematically this is written as

                             i                                     2
                   MSE 
                                        yi - (m^ xi + b^)
                                              N
196 statistical inference for everyone

              0.06 50%                                               Figure 10.3: Posterior distribution
                                                                     for the intercept for the linear
                                                                     model on the shoe size data subset.

              0.05                   25% 75%

P(Intercept)  0.04

              0.03              10%             90%

              0.02          5%                      95%
                                                            99%
              0.01      1%

              0.00          -32.75 -24.38 -14.13 -5.77

                        -40.27 -29.39 -19.26 -9.12 1.76
                    50  40      30   20         10      0        10
                                     Intercept

              14                                                     Figure 10.4: Best linear fit for the
                                                                     shoe size data subset.
              12 y =0.422x-19.256
Shoe Size
              10
               8
               6
               4
               60 62 64 66 68 70 72

                                   Height [inches]
                                                                           multi-parameter models 197

One can intuitively think of getting the best fit as adjusting the slopes  Figure 10.5: Minimizing the Mean
and intercepts, calculating the MSE for each, and stopping when you        Squared Error (MSE) results in the
reach a minimum value. An example of this is shown in Figure 10.5.         best linear fit for the shoe size data
                                                                           subset.
     16
               MSE=0.9Shoe Size
               MSE=1.4

     14 MSE=4.2
               MSE=9.9

     12

     10

      8

      6

      4
       60 62 64 66 68 70 72

                           Height [inches]

An Educational Example

The following example is from a data set on school expenditures and

SAT scores.2 We plot the total SAT scores as a function of expendi-        2

tures, perform a linear model fit, and present the best values and

their uncertainties in Figure 10.6. The model is

                  total = intercept + slope · expenditure

   What is immediately odd is that this result seems to suggest the
following:

1 The larger the expenditure per pupil the lower the SAT scores.

2 For each thousand dollars spent per pupil, the total SAT score goes
   down 20 points.

3 If you spent zero dollars per pupil, you'd reach a maximum of
   SAT score of 1089.
198 statistical inference for everyone

           1150

           1100                                                                     y =-20.892x +1089.294

           1050

SAT Total  1000

           950

           900

           850

           8003 4 5 6 7 8 9 10
                                                  Expenditure [per pupil, thousands]

                                     50%                             0.010 50%
           0.06

           0.05                                                      0.008

           0.04                                        P(Intercept)  0.006

P(Slope)   0.03                           95%                        0.004          5%             95%
           0.02 5%                             99%                                                      99%
           0.01 1%                                                   0.002 1%
                                          -8.60 -3.26
           0.00        -33.18             10 0                       0.000  1014.84                1163.75
                                                                                                         1196.12
                 -38.53        -20.89                                       982.47      1089.29
                 40 30 20                                            950 1000 1050 1100            1150 1200
                               Slope                                                    Intercept

                                                                                        Figure 10.6: Total SAT score vs
                                                                                        expenditure (top) and the distribu-
                                                                                        tions for the slope (bottom left) and
                                                                                        intercept (bottom right).
                                                                           multi-parameter models 199

   This seems counter intuitive to say the least. What is going on         This is perhaps the most important
here? What is happening is that there are other variables that are         lesson of regression. When you see
related to the expenditure which then lead to lower SAT scores on          an effect, make sure to think of any
average. Such a confounding variable needs to be taken into account in     variables that might also be affected
what is called controlling for a variable.                                 that may give rise to the illusion
                                                                           of an effect. It is critical that one
   For example, if we look at the relationship between expenditure         get in this habit, or you will be at
per pupil and the percent of students taking the SAT we see a pat-         the whim of every unscrupulous
tern, shown in Figure 10.7. The more that is spent per pupil, the          statistician.
more students - both bad and good - take the SAT. Thus, even if ex-
penditure helps students, the fact that the percentage of students
taking the exam increases creates the illusion of the opposite. The
next section states how you can overcome this problem.

10.2 Multiple regression

In order to control for a variable that may be affecting our result,
we simply expand our linear model, including slopes (also called
coefficients) for each of the different variables. Once we do this, visu-
alization becomes challenging because we move into three or more
dimensions. Instead of m for the slope, the multiple slopes are typ-
ically labeled with the greek letter  and numbered, such as 1, 2,
etc... The intercept is then labeled 0. The model structure, however,
is the same, and can be written

                          y = 0 + 1x1 + 2x2 · · ·

where the different x1, x2, etc... denote different variables. For the
example of the SAT scores, we might have

          total = 0 + 1 · expenditure + 2 · percent_taking

where percent_taking is a variable representing the percent of stu-
dents taking the exam. Including this variable gives the posterior
distributions shown in Figure 10.8. Notice that the effect of expendi-
ture is both statistically significant and positive. We can interpret the
values in the following way.

· For each $1000 more spent per pupil the total SAT score increases
   on average by 12.29.

· For each percent increase in students taking the SAT, the total SAT
   score decreases on average by 2.29.

   Have we controlled for all of the effects? Perhaps not! This is
where the ingenuity and expertise of the person analyzing the prob-
lem comes into play.
200 statistical inference for everyone

Percentage of Students Taking the SAT  90

                                       80 y =11.638x-33.485

                                       70
                                       60
                                       50
                                       40
                                       30
                                       20
                                       10
                                        03 4 5 6 7 8 9 10

                                                                             Expenditure [per pupil, thousands]

          0.20                         50%                          0.035                50%

                                                                    0.030

P(Slope)  0.15                              95%       P(Intercept)  0.025                               95%
                                                 99%                                                         99%
          0.10                                                      0.020
                        5%
                                                                    0.015
          0.05 1%                                                                   5%

                                                                    0.010
                                                                              1%

                                                                    0.005

          0.00 6.14 7.81 11.64 15.4717.13                           0.000        -56.68  -33.48 -10.29-0.20
                   6 8 10 12 14 16 18
                                   Slope                                   -66.77  60    40         20  0

                                                                                         Intercept

                                                                                             Figure 10.7: Percent of students
                                                                                             taking the SAT vs per pupil expen-
                                                                                             diture (top) and the distributions
                                                                                             for the slope (bottom left) and
                                                                                             intercept (bottom right).
                                                                                            multi-parameter models 201

                                                 50%                                        Figure 10.8: The posterior dis-
                                                                                            tributions for coefficients on the
                                                                                            expenditure term, the percent
                                                                                            taking term, and the intercept.

                0.10

                0.08

P(expenditure)  0.06

                0.04           5%                                        95%
                                                                                  99%
                0.02      1%

                0.00 2.11 5.20                   12.29                   19.37 22.46
                       0       5            10                  15       20            25
                                                 expenditure

                  2.0                            50%

P(percenttaking)  1.5

                  1.0                                                    95%
                                        5%                                        99%

                  0.5     1%

                  0.0     -3.37 -3.21            -2.85                   -2.49 -2.33

                          3.4     3.2       3.0       2.8           2.6      2.4       2.2
                                                 percenttaking

              0.020                              50%

P(Intercept)  0.015

              0.010                                                       95%
                                      5%                                           99%

              0.005       1%                                            1030.47 1046.41
                                                                    1020 1040 1060
              0.000       941.25 957.20           993.83
                                            980 1000
                          940  960
                                                 Intercept
202 statistical inference for everyone

10.3 Polynomial Regression

A subset multiple regression is polynomial regression, where the vari-
able you are predicting depends on the (usually single) dependent
variable with a larger exponent than linear, e.g. quadratic, cubic, etc...

10.4 Computer Examples

from s i e import *
data=load_data ( ' data/shoesize . xls ' )

data . head ( )

    Index Gender Size Height

0   1            F 5.5   60

1   2            F 6.0   60

2   3            F 7.0   60

3   4            F 8.0   60

4   5            F 8.0   60

import random

random . seed ( 1 0 2 )
rows = random . sample ( data . index , 1 0 )
newdata=data . ix [ rows ]
data=newdata
data

    Index Gender Size Height

60  61           F 7.0       64

251 252          M 9.0       70

69  70           F 8.0       64

290 291          M 11.0      71

247 248          M 12.0      69

156 157          F 9.5       68

231 232          M 10.0      69

17  18           F 6.5       61

216 217          M 10.0      68

252 253          M 9.0       70

plot ( data [ ' Height ' ] , data [ ' Size ' ] , 'o ' )
gca ( ) . set_xlim ([60 ,72])
gca ( ) . set_ylim ([4 ,14])
                                                multi-parameter models 203

xlabel ( ' Height [ inches ] ' )
ylabel ( ' Shoe Size ' )

<matplotlib.text.Text at 0x10adae0d0>

result=regression ( ' Size ~ Height ' , data )

<matplotlib.figure.Figure at 0x10d200710>

<matplotlib.figure.Figure at 0x10d702610>
204 statistical inference for everyone

plot ( data [ ' Height ' ] , data [ ' Size ' ] , 'o ' )
h= l i n s p a c e ( 6 0 , 7 2 , 1 0 )
p l o t ( h , r e s u l t [ ' _ P r e d i c t ' ] ( Height=h ) , '- ' )
gca ( ) . set_xlim ([60 ,72])
gca ( ) . set_ylim ([4 ,14])
xlabel ( ' Height [ inches ] ' )
ylabel ( ' Shoe Size ' )
b= r e s u l t . I n t e r c e p t . mean ( )
m= r e s u l t . Height . mean ( )
 if b>0:

        t e x t ( 6 2 , 1 2 , ' $y=%.3 f x + %.3 f $ ' % (m, b ) , f o n t s i z e =30)
else :

        t e x t ( 6 2 , 1 2 , ' $y=%.3 f x %.3 f $ ' % (m, b ) , f o n t s i z e =30)
                                                                                              multi-parameter models 205

data=load_data ( ' data/sat . csv ' )
result=regression ( ' total ~ expenditure ' , data )

<matplotlib.figure.Figure at 0x1109156d0>

<matplotlib.figure.Figure at 0x110953110>
206 statistical inference for everyone

plot ( data [ ' expenditure ' ] , data [ ' total ' ] , 'o ' )
xlabel ( ' Expenditure [ per pupil , thousands ] ' )
y l a b e l ( 'SAT T o t a l ' )
h= l i n s p a c e ( 3 , 1 0 , 1 0 )
p l o t ( h , r e s u l t [ ' _ P r e d i c t ' ] ( e x p e n d i t u r e=h ) , '- ' )
b= r e s u l t . I n t e r c e p t . mean ( )
m= r e s u l t . e x p e n d i t u r e . mean ( )
 if b>0:

        t e x t ( 4 . 5 , 1 1 2 5 , ' $y=%.3 f x + %.3 f $ ' % (m, b ) , f o n t s i z e =30)
else :

        t e x t ( 4 . 5 , 1 1 2 5 , ' $y=%.3 f x %.3 f $ ' % (m, b ) , f o n t s i z e =30)
                                                                                              multi-parameter models 207

result=regression ( ' percent_taking ~ expenditure ' , data )

<matplotlib.figure.Figure at 0x111cfff10>
<matplotlib.figure.Figure at 0x111867750>
208 statistical inference for everyone

plot ( data [ ' expenditure ' ] , data [ ' percent_taking ' ] , 'o ' )
xlabel ( ' Expenditure [ per pupil , thousands ] ' )
y l a b e l ( 'SAT T o t a l ' )
h= l i n s p a c e ( 3 , 1 0 , 1 0 )
p l o t ( h , r e s u l t [ ' _ P r e d i c t ' ] ( e x p e n d i t u r e=h ) , '- ' )
b= r e s u l t . I n t e r c e p t . mean ( )
m= r e s u l t . e x p e n d i t u r e . mean ( )
 if b>0:

        t e x t ( 4 . 5 , 8 5 , ' $y=%.3 f x + %.3 f $ ' % (m, b ) , f o n t s i z e =30)
else :

        t e x t ( 4 . 5 , 8 5 , ' $y=%.3 f x %.3 f $ ' % (m, b ) , f o n t s i z e =30)
                                                                                              multi-parameter models 209

result=regression ( ' total ~ expenditure + percent_taking ' , data )

<matplotlib.figure.Figure at 0x1107f5fd0>
<matplotlib.figure.Figure at 0x10d70a690>
210 statistical inference for everyone

<matplotlib.figure.Figure at 0x110fbcf90>
11 Introduction to MCMC

Once the problems get to a sufficient complexity, the analytical tools
and approximations we have employed in previous chapters will no
longer work well. In those cases, we turn to simulation techniques,
one of which is Markov Chain Monte Carlo (MCMC). It is well be-
yond this book to talk about the details of this process, but the basic
process is the following.

   We start with a model of the system, such as the bent coin model
in Section 6.3. In that system, we try to estimate the probability that a
particular doing will flip heads, quantified by the parameter  which
can take on values from  = 0 (i.e. a coin which only flips tails)
through  = 0.5 (i.e. a "fair" coin which flips tails and heads equally)
up to  = 1 (i.e. a coin which only flips heads). Our data consists of
a total number of flips, N, and how many are heads, h. Although this
problem can be done analytically, it is instructive to walk through
the solved problem with the new method before looking at more
complex models.

   MCMC proceeds, roughly, with the following steps

1 Many random "walkers" are constructed, each with a random
   value of the parameters (e.g.  in this case).

2 The random values are chosen from the prior probability of the
   parameters. (e.g. uniform in this case, P() = 1)

3 The "walkers" move around randomly, guided by the likelihood
   function (e.g. the Binomial or Bernoulli distribution, in this case)

4 Over hundreds or thousands of steps, the distribution of the val-
   ues being explored by the "walkers" matches the posterior distri-
   bution, so one can look at histograms of the resulting "walkers" to
   get estimates of the parameters, and their uncertainty

11.1 One-Dimensional Models

The model we first look at is the coin flip model: given 17 heads
in 25 flips, what is the probability distribution of the the measure
212 statistical inference for everyone

of the coin's bent-ness, . We know the solution is of the form of a
beta distribution, but we perform the same analysis with the MCMC
technique.

   h,N=data=17,25

def P_data(data,theta):
       h,N=data
       distribution=Bernoulli(h,N)
       return distribution(theta)

   model=MCMCModel(data,P_data,                                       Figure 11.1: So-called MCMC
                                theta=Uniform(0,1))                   "chains" for parameter  versus
                                                                      time. Observe that the values of 
   model.run_mcmc(500)                                                start spread evenly from 0 to 1 at
   model.plot_chains()                                                the beginning and then thin down
                                                                      to a range of about 0.5-0.8 with the
Reading the Output                                                    middle around 0.7 (17/25 = 0.68).
We can now plot the distributions of the parameters, just  in this
case, yielding best-fits, uncertainties, etc...

   model.plot_distributions()

   (see Figure 11.2)
   We can further perform some simple calculations on the probabili-
ties for the parameters, such as

   model.P('theta>0.5')

0.96173333333333333
                                                                      introduction to mcmc 213

                                                                      Figure 11.2: Distribution of , and
                                                                      the 95% credible interval.

model.P('(0.2<theta) & (theta<.5)')

0.038266666666666664

11.2 Multi-Dimensional Models

It is straightforward then to include more than one parameter and to
do regression using this technique. For example, here is an example
with some artificial data,

   def linear(x,a,b):
          return a*x+b

   model=MCMCModel_Regression(x,y,linear,
                                a=Uniform(-10,10),
                                b=Uniform(0,100),
                                )

model.run_mcmc(500)
model.plot_chains()

                                                                      Figure 11.3: Chains for parameters
                                                                      a, b, and the noise .
214 statistical inference for everyone

   plot(x,y,'o')
   model.plot_predictions(xfit,color='g')

                                                                       Figure 11.4: Data (blue) and pre-
                                                                       dictions (green) for the model - the
                                                                       width of the predictions demon-
                                                                       strates the uncertainty.

   model.plot_distributions()

   And we can look at best estimates, quartiles, and probability com-
parisons,

   model.percentiles([5,50,95])

{'_sigma': array([ 0.97143798, 1.00744104, 1.0467333 ]),
 'a': array([ 0.07063144, 0.24939562, 0.42523751]),
 'b': array([ 39.88446461, 39.98633744, 40.09010139])}
                                                                      introduction to mcmc 215

                                                                          Figure 11.5: Distributions for
                                                                          parameters a and b (slope and
                                                                          intercept).

model.P('a>0')

0.98936000000000002

11.3 Hierarchical Model Example - Kruschke BEST Test

A comparison between means is a standard statistical technique.

However, using a hierarchical model can be superior to the typical

tests.1                                                               1

In this example, we use the Kruschke BEST Test to compare the

difference between a treatment and control - we want to obtain the

best estimate of the difference between the means of variables. With

the MCMC technique, we can achieve it with the following,

from sie import *
216 statistical inference for everyone

   drug = (101,100,102,104,102,97,105,105,98,
                 101,100,123,105,103,100,95,102,106,
                 109,102,82,102,100,102,102,101,102,
                 102,103,103,97,97,103,101,97,104,
                 96,103,124,101,101,100,101,101,104,
                 100,101)

   placebo = (99,101,100,101,102,100,97,101,
                       104,101,102,102,100,105,88,101,100,
                       104,100,100,100,101,102,103,97,101,
                       101,100,101,99,101,100,100,
                       101,100,99,101,100,102,99,100,99)

   model=mcmc.BESTModel(drug,placebo)

   model.run_mcmc()

Running MCMC...
Done.
5.80 s

   model.names

['mu1', 'mu2', 'sigma1', 'sigma2', 'nu']

   model.plot_chains('mu1')

   model.plot_distribution('mu1')
introduction to mcmc 217

    Figure 11.6: Chains for parameter
    mu1, the mean of the drug group.

    Figure 11.7: Distribution for pa-
    rameter mu1, the mean of the drug
    group.
218 statistical inference for everyone

   model.plot_distribution('mu2')
         Figure 11.8: png

   model.plot_distribution(r'$\delta$=mu1-mu2')

                                                                        Figure 11.9: Distribution for pa-
                                                                        rameter , the mean of the difference
                                                                        between the drug group and the
                                                                        placebo group.

   We can clearly see from the distribution of , as well as the cred-
ible ranges, that there is significant evidence for a non-zero effect.
We would want to extend this to include the effect size, and explore
the prior probability of the the drug working, in order to reasonably
assess whether this is an effect worth pursuing.
12 Concluding Thoughts

12.1 Where have we come?

We have tried in this book to present a particular picture of the
world: everything is probability. We started with basic definitions
and applications, and followed the consequences of the rules of prob-
ability to examine more complex problems. It is our hope that the
reader sees that all of the analysis stems from a single perspective. In
this way, one can approach any problem of inference in a unified way,
applying the recipe we've used throughout:

1 Propose a model for the data you observe (which could be as
   simple as "there is an unknown true value for the observations")

2 Specify your prior knowledge of the parameters in the model, in
   the form of a prior probability (which is often as simple as "I don't
   know anything about the parameters, so all possible values are
   equally likely")

3 Specify how likely your data would be if your model were true,
   which is the likelihood part of Bayes' rule

4 Apply the rules of probability, namely Bayes' rule, to determine
   the posterior probability for the parameters in the model

5 Use the properties of probability functions to calculate answers
   to specific questions, for example "is it likely that this number is
   greater than zero?" or "are these two measurements different?"

   Although I haven't covered all possible examples, and there are
additions and clarifications still planned, this approach can be used
for all new problems one faces. The only steps that can be daunting,
at times, is the mathematical consequences and even there we have
seen that the judicious use of approximations can go a long way.
220 statistical inference for everyone

12.2 Where are we going?

Topics I'd love to add, and will when I have the chance, include (in
no particular order),
· Measurement in Science
· Linear Regression and Correlation
· Two-sample inferences
· Classification
· Model Building in Science
· Analysis of Social Science Data
· Inference for Deviation Parameters
· Experimental Design
· Computer simulations (e.g. MCMC)
Bibliography

 Paul the octopus, July 2012. URL http://en.wikipedia.org/wiki/
 Psychic_octopus.

 Alan Agresti and Brian Caffo. Simple and effective confidence
 intervals for proportions and differences of proportions result from
 adding two successes and two failures. The American Statistician, 54
 (4):280-288, 2000.

 K. Bache and M. Lichman. UCI machine learning repository, 2013.
 URL http://archive.ics.uci.edu/ml.

 D. Berry. Statistics: A Bayesian Perspective. Duxbury, 1996.

 A. Gelman, J. Hill, and Ebooks Corporation. Data analysis using
 regression and multilevel/hierarchical models, volume 625. Cambridge
 University Press Cambridge, UK:, 2007.

 David J Hand, Fergus Daly, K McConway, D Lunn, and E Os-
 trowski. A handbook of small data sets, volume 1. CRC Press, 2011.

 L Heaps. Operation morning light. Paddington, S.l, 1978. ISBN
 0709203233.

 E. T. Jaynes. Probability Theory: The Logic of Science. Cambridge
 University Press, Cambridge, 2003. Edited by G. Larry Bretthorst.

 Lord Justice Kay. R vs Sally Clark, April 2003. URL http://www.
 bailii.org/ew/cases/EWCA/Crim/2003/1020.html.

 D. V. Lindley and L. D. Phillips. Inference for a bernoulli process (a
 bayesian view). The American Statistician, 30(3):112-119, 1976.

 Sharon McGrayne. The Theory That Would Not Die: How Bayes'
 Rule Cracked the Enigma Code, Hunted Down Russian Submarines,
 and Emerged Triumphant from Two Centuries of Controversy. Yale Uni-
 versity Press, 2011. ISBN 0300169698.

 James Oberg. U.S. satellite shootdown: The inside story. IEEE
 Spectrum, 2008.
222 statistical inference for everyone

 J. Randi. Flim-flam!: psychics, ESP, unicorns, and other delusions, vol-
 ume 342. Prometheus Books Amherst, NY, 1982.

 Carl Sagan. Demon-Haunted World: Science as a Candle in the Dark.
 Random House LLC, 1996.

 J. Sullivan. People v. Collins , 68 cal.2d 319, 1968. URL http:
 //scocal.stanford.edu/opinion/people-v-collins-22583.

 A. Tversky and T. Gilovich. The cold facts about the" hot hand" in
 basketball. Anthology of statistics in sports, 16:169, 2005.

 A. Tversky and D. Kahneman. Judgment under uncertainty: Heuris-
 tics and biases. Science, 185(4157):1124, 1974.

 A. Tversky and D. Kahneman. Extensional versus intuitive reason-
 ing: The conjunction fallacy in probability judgment. Psychological
 review, 90(4):293, 1983.
Appendix A
Computational Analysis

The book is written with an accompanying software package, writ-
ten in Python. As of this writing the recommended distribution for
installing python is the Anaconda distribution, available here:

               https://store.continuum.io/cshop/anaconda/
   It is
· Free
· Easy to Use
· Easy to Extend
· Very Powerful
   The accompanying software for the book can be obtained from the
book website, http://web.bryant.edu/bblais/statistical-inference-
for-everyone-sie.html
Appendix B
Notation and Standards

B.1 Useful Greek Letters

 Alpha slope of a line                        Pi Represents the constant
                                                               3.1415· · ·, the ratio of the cir-
 Beta slope of a line, intercept
                                                                cumference to the diameter of a
 Gamma
 Gamma                                                          circle
                                              Pi A product of a series of num-
 Delta A small change in a variable
 Delta A change in a variable                                   bers
                                              Rho
  Epsilon                                     Sigma The standard width parameter

 Zeta                                                           of the normal distribution
                                              Sigma A sum of a series of numbers
 Eta                                          Tau
                                              Phi
 Theta The parameters in a binomial/-         Phi
                                              Chi A distribution related to the
                    beta distribution
 Theta                                                          sum of normally distributed

 Kappa                                                          variables
                                              Psi
 Lambda the mean in a poisson distribu-       Psi
                                              Omega
                    tion                      Omega
 Lambda

µ Mu the mean in a normal distribu-

           tion (pronounced "mew")

 Nu (pronounced "new")

  Xi

  Xi

B.2 Some Math Notation

Variables
A set of values, labeled with subscripts...

                                     x1 = 1
                                     x2 = 5
226 statistical inference for everyone

                              x3 = -3
                              x4 = 2
                              x5 = 8

referred collectively as xi.

Sums

      x1 + x2 + x3 + x4 + x5 = 1 + 5 + (-3) + 2 + 8 = 13
is equivalent to

                             5

                xi = 1 + 5 + (-3) + 2 + 8 = 13

                          i=1

Products
             x1 · x2 · x3 · x4 · x5 = 1 · 5 · (-3) · 2 · 8 = -240

   is equivalent to

                               5

                 xi = 1 · 5 · (-3) · 2 · 8 = -240

                            i=1

Sample Mean
The sample mean of a set of numbers is defined as...

                             x¯  x1 + x2 + · · · xN
                                              N

   In the example above

      x¯  x1 + x2 + x3 + x4 + x5 = 2 3
                              5                  5

It can also be written

                              x¯  iN=1 xi
                                       N

or
                                    x¯  i xi
                                              N
                                                                         notation and standards 227

Sample Standard Deviation

             2 1N 2
             s             (x - x¯)
                   N - 1 i=1

                           1N                                            Although the justification for the
             s             (x - x¯)2                                     N - 1 part is beyond this book,
                   N - 1 i=1                                             one easy way to remember it is
                                                                         that the sample distribution of a
Estimates                                                                set of numbers is an estimate for
                                                                         the  parameter of the normal
Any specific estimate of a parameter, such as , is denoted with a hat,   distribution, representing the spread
such as ^.                                                               of the data. You can think of the
                                                                         N - 1 part as a check to keep you
                                                                         from doing the crazy thing of
                                                                         estimating a spread with only 1
                                                                         data point!

Factorials

Factorials are defined as
                         N! = 1 · 2 · 3 · · · (N - 1) · N

for example

             5! = 1 · 2 · 3 · 4 · 5 = 120

   The N-choose-k notation is a shorthand for the factorials that arise
in binomial and Beta distributions.

                N  N!
                k          k!(N - k)!

B.3 Qualitative labels to probability values

Rough guide for the conversion of qualitative labels to probability
values used throughout the book.

                       term            probability
             virtually impossible      1/1,000,000
             extremely unlikely     0.01 (i.e. 1/100)
                                     0.05 (i.e. 1/20)
                 very unlikely        0.2 (i.e. 1/5)
                    unlikely          0.4 (i.e. 2/5)
                                     0.5 (i.e. 50-50)
               slightly unlikely      0.6 (i.e. 3/5)
                   even odds          0.8 (i.e. 4/5)
                                    0.95 (i.e. 19/20)
                 slightly likely   0.99 (i.e. 99/100)
                      likely       999,999/1,000,000

                   very likely
               extremely likely
               virtually certain
Appendix C
Common Distributions and Their Properties

This chapter is a reference for the standard distributions encountered
in statistical inference. Although you are encouraged to read this
chapter through, it can also be read out-of-order to look at a specific
distribution.

C.1 Discrete and Continuous

Some distributions apply to a discrete (i.e. countable) number of
possibilities while others apply to continuous values. In the case
of discrete variables, the probability is given by the actual value of
the distribution, so it makes sense to speak of the probability of an
individual label, P(coin1). In the case of continuous variables, the
probability is given by the area under the distribution, so it makes
sense only to speak of the probability if a range of labels, P(0.2 <  <
0.3).

C.2 Uniform

Discrete

Discrete uniform distribution The discrete uniform distribution is         Discrete uniform distribution The

defined to be a constant value for all possibilities. Mathematically this  discrete uniform distribution is

is written                                                                 defined to be a constant value for all

             p(xi) = 1                                                     possibilities. Mathematically this is
                        N
                                                                           written  p(xi) = 1
                                                                                               N
where N is the total number of possibilities, labeled x1 to xN. The
picture of the distribution is shown in Figure C.1                         where N is the total number of

                                                                           possibilities, labeled x1 to xN.

Continuous                                                                 Continuous uniform distribution
                                                                           The continuous uniform distri-
   Continuous uniform distribution The continuous uniform distri-          bution is defined to be a constant
bution is defined to be a constant between a minimum and maximum           between a minimum and maximum
                                                                           value, and zero everywhere else.
                                                                           Mathematically this is written

                                                                           p(x) = 1 for min < x < max
                                                                                     max - min

                                                                           .
230 statistical inference for everyone

      1.2  Min=0, Max=1                                                            Figure C.1: Discrete uniform distri-
                                                                                   bution for values 1 to 6. The value
           5% 25% 50% 75% 95%                                                      for each is p(xi) = 1/6.

      1.0                                                               1.2

      0.8

P(x)  0.6

      0.4

      0.2

      0.0 0.05 0.25 0.50 0.75 0.95
        0.2 0.0 0.2 0.4 0.6 0.8 1.0
                                  x

value, and zero everywhere else. Mathematically this is written
                    p(x) = 1 for min < x < max
                              max - min

. The picture of the distribution is shown in Figure C.2.

Example C.1 You call a plumber, and they say that they can come anytime
in the next 4 hours. The probability of them arriving at any particular time
can be represented with a uniform distribution. What is the probability that
they arrive in the first 20 minutes of the second hour?

   In order to ask questions about total probability from a continuous        The reason for the particular con-
distribution you take the area under the curve between the relevant           stant value for the uniform distri-
values. In this case it'd be the area under the curve from the time t =       bution, 1/(max - min), is simply
2hr and t = 2hr + 20minutes = 2.333hr, as shown in Figure C.3. The            that the area of the entire rectan-
area under the curve is just the area of the shaded region between            gle must be 1, which means that
times t = 2hr and t = 2.333hr, or just the area of a rectangle - A =          there is a 100% chance of the values
base × height. The base of the rectangle is the length of time, or            falling between the minimum and
                                                                              maximum values.
           base = 0.333hr

The height of the rectangle is given by the constant value of the uni-
form distribution, or

           height = 1 = 1 = 0.25 1
           max - min 4hr - 0hr          hr
                                 common distributions and their properties 231

         1.2               Min=0, Max=1                      Figure C.2: Continuous uniform
                                                             distribution between values 0 and 1
                      5% 25% 50% 75% 95%
                                                  1.2
         1.0

         0.8

P(x)     0.6

         0.4

         0.2

         0.0 0.05 0.25 0.50 0.75 0.95
           0.2 0.0 0.2 0.4 0.6 0.8 1.0
                                     x

         0.30 5%           Min=0, Max=4           Figure C.3: Continuous uniform
         0.25                                     distribution for the plumber exam-
         0.20         25%  50%           75% 95%  ple (Example C.1).
         0.15
P(time)  0.10
         0.05
         0.00                    20 minutes

                0.20  1.00 2.00 3.00 3.80
               0      1    2             3   4
                           time
232 statistical inference for everyone

So the total probability of the plumber coming in the first 20 minutes                     Binomial distribution The discrete
of the second hour is                                                                      binomial distribution is defined to
                                                                                           be the probability of achieving h
          P(2 < t < 2.25) = (0.333hr) × 0.25 1 = 0.0833                                    successes in a given N events where
                                                           hr                              each event has a given  probability
                                                                                           of success.
C.3 Binomial
                                                                                           P(h|N, ) = h h(1 - )N-h
   Binomial distribution The discrete binomial distribution is de-                                              N
fined to be the probability of achieving h successes in a given N
events where each event has a given  probability of success.

                    P(h|N, ) = h h(1 - )N-h
                                          N

            0.25                                                                           Figure C.4: Probability of getting
                                                                                           h heads in 30 flips given a possible
                                                                               p =0.1      unfair coin. One coin has p = 0.1,
                                                                               p =0.5      where the maximum is for 3 heads
                                                                                           (or 1/10 of the 30 flips), but 2
            0.20 p =0.8                                                                    heads is nearly as likely. Another
                                                                                           has p = 0.5, and is the fair coin
P(h,N =30)  0.15                                                                           considered earlier with a maximum
                                                                                           at 15 heads (or 1/2 of the 30 flips).
                                                                                           Finally, another coin shown as
                                                                                           p = 0.8 where 24 heads (or 8/10 of
                                                                                           the 30 flips) is maximum.

            0.10

            0.05

            0.000  5  10  15               20  25                                      30
                          Number of heads

   Although it may look like a Beta, the binomial distribution is used                     Beta distribution The continuous
to find the best estimate for the number of successes, h, given the                        Beta distribution is the posterior
number of events, N, and the probability of the success of a single                        probability distribution for the pa-
event, .                                                                                   rameter , where one has observed
                                                                                           h successes in a given N events, and
C.4 Beta                                                                                   each event is assumed to have a 
                                                                                           probability of success.
   Beta distribution The continuous Beta distribution is the posterior
probability distribution for the parameter , where one has observed                        P(|h, N) = (N + 1) · N h(1 - )N-h
                                                                                                                             h
                                      common distributions and their properties 233

h successes in a given N events, and each event is assumed to have a
 probability of success.

              P(|h, N) = (N + 1) · N h(1 - )N-h
                                                 h

   Although it may look like a binomial, the Beta distribution is used
to find the best estimate for the parameter  where the number of
successes and events, h and N are given.

     4           3 heads and 9 tails                                    Figure C.5: Posterior probability
                                                                        distribution for the  values of
            25%50%                                                      the bent coin - the probability
                                                                        that the coin will land heads. The
     3           75%                                                    distribution is shown for data 3
                                                                        heads and 9 tails. The various
                                                                        quartiles are shown in the plot.

P()  2  5%

     1 1%             95%
                          99%

     0 0.11 0.28 0.49
           0.07 0.20 0.36 0.59

         0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0

                                  

C.5 Normal (Gaussian)                                                   Normal distribution The Normal
                                                                        distribution is the most common
   Normal distribution The Normal distribution is the most com-         distribution found in all of statis-
mon distribution found in all of statistical inference. It is the best  tical inference. It is the best prior
prior distribution to use, when all you know is that your data has      distribution to use, when all you
a constant true value and some constant variation around that true      know is that your data has a con-
value. It is the posterior probability distribution for the unknown     stant true value and some constant
true value given N samples and the known deviation, . It is also the    variation around that true value. It
approximate form for nearly every distribution when you have many       is the posterior probability distri-
samples. The mathematical form for the normal, or Gaussian, is          bution for the unknown true value
                                                                        given N samples and the known de-
                   Normal(µ, ) =  1 e-(x-µ)2/22                         viation, . It is also the approximate
                                             22                         form for nearly every distribution
                                                                        when you have many samples. The
                                                                        mathematical form for the normal,
                                                                        or Gaussian, is

                                                                        Normal(µ, ) =  1 e-(x-µ)2/22
                                                                                                 22
234 statistical inference for everyone                              Figure C.6: The normal distribution.

p(x) =Normal(0,1)0.4
       0.3
       0.2
       0.1
      0.0

          4 3 2 10 1 2 3 4

                                             x

   Three useful properties of  for the normal distribution are the
following:

1 the normal distribution value at the maximum (i.e. at x = µ)
   is around 2.7 times larger than the value one- away from the
   maximum (at x = µ -  and x = µ + )

2 the total probability between these two points is 65%.

3 95% of the distribution lies between µ - 2 and µ + 2 (see Fig-
   ure 7.3)
Appendix D
Tables

D.1 Credible Intervals for Standard Normal Distribution

P(z)0.4 68% CI at µ ±1
    0.3
    0.2
    0.1
    0.0

       4 3 2 10 1 2 3 4
                      z

      Credible Interval    ±z    Approximately
              50.0%      0.6745
              68.0%      0.9945          1
              90.0%      1.6449          2
              95.0%      1.9600          3
              99.0%      2.5758          4
              99.8%      3.0902
                         4.0556
            99.995%

Example D.1 Usage of the Credible Interval Table for the Normal Distri-
bution

   Given a set of 10 samples with sample mean x¯ = 5.2 and known
deviation  = 0.3, the best estimate for the mean parameter µ, repre-
senting the true value of the data, is the sample mean, µ^ = 5.2 with
236 statistical inference for everyone

                  
uncertainty / N or 0.3/ 10 = 0.095. Some of the credible intervals

for this estimate then are the following

· 68% - [5.2 - 0.9945 · 0.095, 5.2 + 0.9945 · 0.095] = [5.11, 5.29]

· 95% - [5.2 - 1.9600 · 0.095, 5.2 + 1.9600 · 0.095] = [5.01, 5.39]

· 99.8% - [5.2 - 3.0902 · 0.095, 5.2 + 3.0902 · 0.095] = [4.91, 5.49]
or approximately
· 68% - [5.2 - 1 · 0.095, 5.2 + 1 · 0.095] = [5.11, 5.29]

· 95% - [5.2 - 2 · 0.095, 5.2 + 2 · 0.095] = [5.01, 5.39]

· 99.8% - [5.2 - 3 · 0.095, 5.2 + 3 · 0.095] = [4.91, 5.49]

D.2 Credible Intervals for Student's t Distribution

                                             Degrees of Freedom

Credible       1         2                3                  4         5     6           7  8
Interval
50.0%        1.000        0.816           0.765            0.741     0.727   0.718   0.711  0.706
68.0%        1.819        1.312           1.189            1.134     1.104   1.084   1.070  1.060
90.0%        6.314        2.920           2.353            2.132     2.015   1.943   1.895  1.860
95.0%       12.706        4.303           3.182            2.776     2.571   2.447   2.365  2.306
99.0%       63.657        9.925           5.841            4.604     4.032   3.707   3.499  3.355
99.8%      318.309       22.327           10.215           7.173     5.893   5.208   4.785  4.501
99.995%   12732.395      141.416          35.298           18.522    12.893  10.261  8.783  7.851

Credible                                  Degrees of Freedom
Interval
50.0%       9        10     11               12              13        14    15      16
68.0%
90.0%     0.703   0.700  0.697            0.695            0.694     0.692   0.691   0.690
95.0%     1.053   1.046  1.041            1.037            1.034     1.031   1.029   1.026
99.0%     1.833   1.812  1.796            1.782            1.771     1.761   1.753   1.746
99.8%     2.262   2.228  2.201            2.179            2.160     2.145   2.131   2.120
99.995%   3.250   3.169  3.106            3.055            3.012     2.977   2.947   2.921
          4.297   4.144  4.025            3.930            3.852     3.787   3.733   3.686
          7.215   6.757  6.412            6.143            5.928     5.753   5.607   5.484
                                                                                          tables 237

                               Degrees of Freedom

Credible  17     18     19       20   21                             22     23     24
Interval
50.0%     0.689  0.688  0.688  0.687  0.686                          0.686  0.685  0.685
68.0%     1.024  1.023  1.021  1.020  1.019                          1.017  1.016  1.015
90.0%     1.740  1.734  1.729  1.725  1.721                          1.717  1.714  1.711
95.0%     2.110  2.101  2.093  2.086  2.080                          2.074  2.069  2.064
99.0%     2.898  2.878  2.861  2.845  2.831                          2.819  2.807  2.797
99.8%     3.646  3.610  3.579  3.552  3.527                          3.505  3.485  3.467
99.995%   5.379  5.288  5.209  5.139  5.077                          5.022  4.972  4.927

                               Degrees of Freedom

Credible  25     26     27       28   29                             30     31     32
Interval
50.0%     0.684  0.684  0.684  0.683  0.683                          0.683  0.682  0.682
68.0%     1.015  1.014  1.013  1.012  1.012                          1.011  1.011  1.010
90.0%     1.708  1.706  1.703  1.701  1.699                          1.697  1.696  1.694
95.0%     2.060  2.056  2.052  2.048  2.045                          2.042  2.040  2.037
99.0%     2.787  2.779  2.771  2.763  2.756                          2.750  2.744  2.738
99.8%     3.450  3.435  3.421  3.408  3.396                          3.385  3.375  3.365
99.995%   4.887  4.849  4.816  4.784  4.756                          4.729  4.705  4.682

                               Degrees of Freedom

Credible  33     34     35       36   37                             38       39     40
Interval
50.0%     0.682  0.682  0.682  0.681  0.681                          0.681  0.681  0.681
68.0%     1.010  1.009  1.009  1.008  1.008                          1.008  1.007  1.007
90.0%     1.692  1.691  1.690  1.688  1.687                          1.686  1.685  1.684
95.0%     2.035  2.032  2.030  2.028  2.026                          2.024  2.023  2.021
99.0%     2.733  2.728  2.724  2.719  2.715                          2.712  2.708  2.704
99.8%     3.356  3.348  3.340  3.333  3.326                          3.319  3.313  3.307
99.995%   4.660  4.640  4.622  4.604  4.588                          4.572  4.558  4.544

Example D.2 Usage of the Credible Interval Table for the Student's t
Distribution

Given a set of 10 samples (9 degrees of freedom) with sample

mean x¯ = 5.2 and sample deviation s = 0.3, the best estimate for

the mean parameter µ, representing the true value of the data, is the
                                      
sample mean, µ^ = 5.2 with uncertainty s/ N or 0.3/ 10 = 0.095.

Some of the credible intervals for this estimate then are the following

· 68% - [5.2 - 1.053 · 0.095, 5.2 + 1.053 · 0.095] = [5.09, 5.3]

· 95% - [5.2 - 2.262 · 0.095, 5.2 + 2.262 · 0.095] = [4.99, 5.41]

· 99.8% - [5.2 - 4.297 · 0.095, 5.2 + 4.297 · 0.095] = [4.79, 5.61]
238 statistical inference for everyone

D.3 Cumulative Standard Normal Distribution

P(z)0.4
    0.3
    0.2 area   2 1 0z   1234
    0.1
    0.0        Area       z    Area       z    Area       z    Area       z    Area
               on Left         on Left         on Left         on Left         on Left
       43      0.0001   -3.40  0.0003   -3.10  0.0010   -2.80  0.0026   -2.50  0.0062
               0.0001   -3.39  0.0003   -3.09  0.0010   -2.79  0.0026   -2.49  0.0064
          z    0.0001   -3.38  0.0004   -3.08  0.0010   -2.78  0.0027   -2.48  0.0066
               0.0001   -3.37  0.0004   -3.07  0.0011   -2.77  0.0028   -2.47  0.0068
        -3.70  0.0001   -3.36  0.0004   -3.06  0.0011   -2.76  0.0029   -2.46  0.0069
        -3.69  0.0001   -3.35  0.0004   -3.05  0.0011   -2.75  0.0030   -2.45  0.0071
        -3.68  0.0001   -3.34  0.0004   -3.04  0.0012   -2.74  0.0031   -2.44  0.0073
        -3.67  0.0001   -3.33  0.0004   -3.03  0.0012   -2.73  0.0032   -2.43  0.0075
        -3.66  0.0001   -3.32  0.0005   -3.02  0.0013   -2.72  0.0033   -2.42  0.0078
        -3.65  0.0002   -3.31  0.0005   -3.01  0.0013   -2.71  0.0034   -2.41  0.0080
        -3.64  0.0002   -3.30  0.0005   -3.00  0.0013   -2.70  0.0035   -2.40  0.0082
        -3.63  0.0002   -3.29  0.0005   -2.99  0.0014   -2.69  0.0036   -2.39  0.0084
        -3.62  0.0002   -3.28  0.0005   -2.98  0.0014   -2.68  0.0037   -2.38  0.0087
        -3.61  0.0002   -3.27  0.0005   -2.97  0.0015   -2.67  0.0038   -2.37  0.0089
        -3.60  0.0002   -3.26  0.0006   -2.96  0.0015   -2.66  0.0039   -2.36  0.0091
        -3.59  0.0002   -3.25  0.0006   -2.95  0.0016   -2.65  0.0040   -2.35  0.0094
        -3.58  0.0002   -3.24  0.0006   -2.94  0.0016   -2.64  0.0041   -2.34  0.0096
        -3.57  0.0002   -3.23  0.0006   -2.93  0.0017   -2.63  0.0043   -2.33  0.0099
        -3.56  0.0002   -3.22  0.0006   -2.92  0.0018   -2.62  0.0044   -2.32  0.0102
        -3.55  0.0002   -3.21  0.0007   -2.91  0.0018   -2.61  0.0045   -2.31  0.0104
        -3.54  0.0002   -3.20  0.0007   -2.90  0.0019   -2.60  0.0047   -2.30  0.0107
        -3.53  0.0002   -3.19  0.0007   -2.89  0.0019   -2.59  0.0048   -2.29  0.0110
        -3.52  0.0003   -3.18  0.0007   -2.88  0.0020   -2.58  0.0049   -2.28  0.0113
        -3.51  0.0003   -3.17  0.0008   -2.87  0.0021   -2.57  0.0051   -2.27  0.0116
        -3.50  0.0003   -3.16  0.0008   -2.86  0.0021   -2.56  0.0052   -2.26  0.0119
        -3.49  0.0003   -3.15  0.0008   -2.85  0.0022   -2.55  0.0054   -2.25  0.0122
        -3.48  0.0003   -3.14  0.0008   -2.84  0.0023   -2.54  0.0055   -2.24  0.0125
        -3.47  0.0003   -3.13  0.0009   -2.83  0.0023   -2.53  0.0057   -2.23  0.0129
        -3.46  0.0003   -3.12  0.0009   -2.82  0.0024   -2.52  0.0059   -2.22  0.0132
        -3.45  0.0003   -3.11  0.0009   -2.81  0.0025   -2.51  0.0060   -2.21  0.0136
        -3.44  0.0003   -3.10  0.0010   -2.80  0.0026   -2.50  0.0062   -2.20  0.0139
        -3.43
        -3.42
        -3.41
        -3.40
                                                                        tables 239

Cumulative Normal Distribution (cont.)

0.4

P(z)0.3

    0.2 area
    0.1

0.0
   4 3 2 1 0z 1 2 3 4

      z Area        z Area                z    Area       z    Area       z    Area
                                               on Left         on Left         on Left
         on Left    on Left             -1.60  0.0548   -1.30  0.0968   -1.00  0.1587
                                        -1.59  0.0559   -1.29  0.0985   -0.99  0.1611
      -2.20 0.0139  -1.90 0.0287        -1.58  0.0571   -1.28  0.1003   -0.98  0.1635
                                        -1.57  0.0582   -1.27  0.1020   -0.97  0.1660
      -2.19 0.0143  -1.89 0.0294        -1.56  0.0594   -1.26  0.1038   -0.96  0.1685
                                        -1.55  0.0606   -1.25  0.1056   -0.95  0.1711
      -2.18 0.0146  -1.88 0.0301        -1.54  0.0618   -1.24  0.1075   -0.94  0.1736
                                        -1.53  0.0630   -1.23  0.1093   -0.93  0.1762
      -2.17 0.0150  -1.87 0.0307        -1.52  0.0643   -1.22  0.1112   -0.92  0.1788
                                        -1.51  0.0655   -1.21  0.1131   -0.91  0.1814
      -2.16 0.0154  -1.86 0.0314        -1.50  0.0668   -1.20  0.1151   -0.90  0.1841
                                        -1.49  0.0681   -1.19  0.1170   -0.89  0.1867
      -2.15 0.0158  -1.85 0.0322        -1.48  0.0694   -1.18  0.1190   -0.88  0.1894
                                        -1.47  0.0708   -1.17  0.1210   -0.87  0.1922
      -2.14 0.0162  -1.84 0.0329        -1.46  0.0721   -1.16  0.1230   -0.86  0.1949
                                        -1.45  0.0735   -1.15  0.1251   -0.85  0.1977
      -2.13 0.0166  -1.83 0.0336        -1.44  0.0749   -1.14  0.1271   -0.84  0.2005
                                        -1.43  0.0764   -1.13  0.1292   -0.83  0.2033
      -2.12 0.0170  -1.82 0.0344        -1.42  0.0778   -1.12  0.1314   -0.82  0.2061
                                        -1.41  0.0793   -1.11  0.1335   -0.81  0.2090
      -2.11 0.0174  -1.81 0.0351        -1.40  0.0808   -1.10  0.1357   -0.80  0.2119
                                        -1.39  0.0823   -1.09  0.1379   -0.79  0.2148
      -2.10 0.0179  -1.80 0.0359        -1.38  0.0838   -1.08  0.1401   -0.78  0.2177
                                        -1.37  0.0853   -1.07  0.1423   -0.77  0.2206
      -2.09 0.0183  -1.79 0.0367        -1.36  0.0869   -1.06  0.1446   -0.76  0.2236
                                        -1.35  0.0885   -1.05  0.1469   -0.75  0.2266
      -2.08 0.0188  -1.78 0.0375        -1.34  0.0901   -1.04  0.1492   -0.74  0.2296
                                        -1.33  0.0918   -1.03  0.1515   -0.73  0.2327
      -2.07 0.0192  -1.77 0.0384        -1.32  0.0934   -1.02  0.1539   -0.72  0.2358
                                        -1.31  0.0951   -1.01  0.1562   -0.71  0.2389
      -2.06 0.0197  -1.76 0.0392        -1.30  0.0968   -1.00  0.1587   -0.70  0.2420

      -2.05 0.0202  -1.75 0.0401

      -2.04 0.0207  -1.74 0.0409

      -2.03 0.0212  -1.73 0.0418

      -2.02 0.0217  -1.72 0.0427

      -2.01 0.0222  -1.71 0.0436

      -2.00 0.0228  -1.70 0.0446

      -1.99 0.0233  -1.69 0.0455

      -1.98 0.0239  -1.68 0.0465

      -1.97 0.0244  -1.67 0.0475

      -1.96 0.0250  -1.66 0.0485

      -1.95 0.0256  -1.65 0.0495

      -1.94 0.0262  -1.64 0.0505

      -1.93 0.0268  -1.63 0.0516

      -1.92 0.0274  -1.62 0.0526

      -1.91 0.0281  -1.61 0.0537

      -1.90 0.0287  -1.60 0.0548
240 statistical inference for everyone

Cumulative Normal Distribution (cont.)

0.4

P(z)0.3

    0.2 area
    0.1

0.0
   4 3 2 1 0z 1 2 3 4

      z Area        z Area                z    Area      z Area           z Area
                                               on Left          on Left          on Left
         on Left    on Left             -0.10  0.4602
                                        -0.09  0.4641   0.20 0.5793      0.50 0.6915
      -0.70 0.2420  -0.40 0.3446        -0.08  0.4681   0.21 0.5832      0.51 0.6950
                                        -0.07  0.4721   0.22 0.5871      0.52 0.6985
      -0.69 0.2451  -0.39 0.3483        -0.06  0.4761   0.23 0.5910      0.53 0.7019
                                        -0.05  0.4801   0.24 0.5948      0.54 0.7054
      -0.68 0.2483  -0.38 0.3520        -0.04  0.4840   0.25 0.5987      0.55 0.7088
                                        -0.03  0.4880   0.26 0.6026      0.56 0.7123
      -0.67 0.2514  -0.37 0.3557        -0.02  0.4920   0.27 0.6064      0.57 0.7157
                                        -0.01  0.4960   0.28 0.6103      0.58 0.7190
      -0.66 0.2546  -0.36 0.3594        0.00   0.5000   0.29 0.6141      0.59 0.7224
                                        0.01   0.5040   0.30 0.6179      0.60 0.7257
      -0.65 0.2578  -0.35 0.3632        0.02   0.5080   0.31 0.6217      0.61 0.7291
                                        0.03   0.5120   0.32 0.6255      0.62 0.7324
      -0.64 0.2611  -0.34 0.3669        0.04   0.5160   0.33 0.6293      0.63 0.7357
                                        0.05   0.5199   0.34 0.6331      0.64 0.7389
      -0.63 0.2643  -0.33 0.3707        0.06   0.5239   0.35 0.6368      0.65 0.7422
                                        0.07   0.5279   0.36 0.6406      0.66 0.7454
      -0.62 0.2676  -0.32 0.3745        0.08   0.5319   0.37 0.6443      0.67 0.7486
                                        0.09   0.5359   0.38 0.6480      0.68 0.7517
      -0.61 0.2709  -0.31 0.3783        0.10   0.5398   0.39 0.6517      0.69 0.7549
                                        0.11   0.5438   0.40 0.6554      0.70 0.7580
      -0.60 0.2743  -0.30 0.3821        0.12   0.5478   0.41 0.6591      0.71 0.7611
                                        0.13   0.5517   0.42 0.6628      0.72 0.7642
      -0.59 0.2776  -0.29 0.3859        0.14   0.5557   0.43 0.6664      0.73 0.7673
                                        0.15   0.5596   0.44 0.6700      0.74 0.7704
      -0.58 0.2810  -0.28 0.3897        0.16   0.5636   0.45 0.6736      0.75 0.7734
                                        0.17   0.5675   0.46 0.6772      0.76 0.7764
      -0.57 0.2843  -0.27 0.3936        0.18   0.5714   0.47 0.6808      0.77 0.7794
                                        0.19   0.5753   0.48 0.6844      0.78 0.7823
      -0.56 0.2877  -0.26 0.3974        0.20   0.5793   0.49 0.6879      0.79 0.7852
                                                        0.50 0.6915      0.80 0.7881
      -0.55 0.2912  -0.25 0.4013

      -0.54 0.2946  -0.24 0.4052

      -0.53 0.2981  -0.23 0.4090

      -0.52 0.3015  -0.22 0.4129

      -0.51 0.3050  -0.21 0.4168

      -0.50 0.3085  -0.20 0.4207

      -0.49 0.3121  -0.19 0.4247

      -0.48 0.3156  -0.18 0.4286

      -0.47 0.3192  -0.17 0.4325

      -0.46 0.3228  -0.16 0.4364

      -0.45 0.3264  -0.15 0.4404

      -0.44 0.3300  -0.14 0.4443

      -0.43 0.3336  -0.13 0.4483

      -0.42 0.3372  -0.12 0.4522

      -0.41 0.3409  -0.11 0.4562

      -0.40 0.3446  -0.10 0.4602
                                                                           tables 241

Cumulative Normal Distribution (cont.)

P(z)0.4
    0.3
    0.2 area          0z 1     234        z Area           z Area           z Area
    0.1                                          on Left          on Left          on Left
    0.0                    z    Area
                                on Left  1.40 0.9192      1.70 0.9554      2.00 0.9772
       4321              1.10   0.8643   1.41 0.9207      1.71 0.9564      2.01 0.9778
                         1.11   0.8665   1.42 0.9222      1.72 0.9573      2.02 0.9783
      z Area             1.12   0.8686   1.43 0.9236      1.73 0.9582      2.03 0.9788
             on Left     1.13   0.8708   1.44 0.9251      1.74 0.9591      2.04 0.9793
                         1.14   0.8729   1.45 0.9265      1.75 0.9599      2.05 0.9798
    0.80 0.7881          1.15   0.8749   1.46 0.9279      1.76 0.9608      2.06 0.9803
    0.81 0.7910          1.16   0.8770   1.47 0.9292      1.77 0.9616      2.07 0.9808
    0.82 0.7939          1.17   0.8790   1.48 0.9306      1.78 0.9625      2.08 0.9812
    0.83 0.7967          1.18   0.8810   1.49 0.9319      1.79 0.9633      2.09 0.9817
    0.84 0.7995          1.19   0.8830   1.50 0.9332      1.80 0.9641      2.10 0.9821
    0.85 0.8023          1.20   0.8849   1.51 0.9345      1.81 0.9649      2.11 0.9826
    0.86 0.8051          1.21   0.8869   1.52 0.9357      1.82 0.9656      2.12 0.9830
    0.87 0.8078          1.22   0.8888   1.53 0.9370      1.83 0.9664      2.13 0.9834
    0.88 0.8106          1.23   0.8907   1.54 0.9382      1.84 0.9671      2.14 0.9838
    0.89 0.8133          1.24   0.8925   1.55 0.9394      1.85 0.9678      2.15 0.9842
    0.90 0.8159          1.25   0.8944   1.56 0.9406      1.86 0.9686      2.16 0.9846
    0.91 0.8186          1.26   0.8962   1.57 0.9418      1.87 0.9693      2.17 0.9850
    0.92 0.8212          1.27   0.8980   1.58 0.9429      1.88 0.9699      2.18 0.9854
    0.93 0.8238          1.28   0.8997   1.59 0.9441      1.89 0.9706      2.19 0.9857
    0.94 0.8264          1.29   0.9015   1.60 0.9452      1.90 0.9713      2.20 0.9861
    0.95 0.8289          1.30   0.9032   1.61 0.9463      1.91 0.9719      2.21 0.9864
    0.96 0.8315          1.31   0.9049   1.62 0.9474      1.92 0.9726      2.22 0.9868
    0.97 0.8340          1.32   0.9066   1.63 0.9484      1.93 0.9732      2.23 0.9871
    0.98 0.8365          1.33   0.9082   1.64 0.9495      1.94 0.9738      2.24 0.9875
    0.99 0.8389          1.34   0.9099   1.65 0.9505      1.95 0.9744      2.25 0.9878
    1.00 0.8413          1.35   0.9115   1.66 0.9515      1.96 0.9750      2.26 0.9881
    1.01 0.8438          1.36   0.9131   1.67 0.9525      1.97 0.9756      2.27 0.9884
    1.02 0.8461          1.37   0.9147   1.68 0.9535      1.98 0.9761      2.28 0.9887
    1.03 0.8485          1.38   0.9162   1.69 0.9545      1.99 0.9767      2.29 0.9890
    1.04 0.8508          1.39   0.9177   1.70 0.9554      2.00 0.9772      2.30 0.9893
    1.05 0.8531          1.40   0.9192
    1.06 0.8554
    1.07 0.8577
    1.08 0.8599
    1.09 0.8621
    1.10 0.8643
242 statistical inference for everyone

Cumulative Normal Distribution (cont.)

P(z)0.4
    0.3
    0.2 area          0z 1     234        z Area           z Area           z Area
    0.1                                          on Left          on Left          on Left
    0.0                    z    Area
                                on Left  2.90 0.9981      3.20 0.9993      3.50 0.9998
       4321              2.60   0.9953   2.91 0.9982      3.21 0.9993      3.51 0.9998
                         2.61   0.9955   2.92 0.9982      3.22 0.9994      3.52 0.9998
      z Area             2.62   0.9956   2.93 0.9983      3.23 0.9994      3.53 0.9998
             on Left     2.63   0.9957   2.94 0.9984      3.24 0.9994      3.54 0.9998
                         2.64   0.9959   2.95 0.9984      3.25 0.9994      3.55 0.9998
    2.30 0.9893          2.65   0.9960   2.96 0.9985      3.26 0.9994      3.56 0.9998
    2.31 0.9896          2.66   0.9961   2.97 0.9985      3.27 0.9995      3.57 0.9998
    2.32 0.9898          2.67   0.9962   2.98 0.9986      3.28 0.9995      3.58 0.9998
    2.33 0.9901          2.68   0.9963   2.99 0.9986      3.29 0.9995      3.59 0.9998
    2.34 0.9904          2.69   0.9964   3.00 0.9987      3.30 0.9995      3.60 0.9998
    2.35 0.9906          2.70   0.9965   3.01 0.9987      3.31 0.9995      3.61 0.9998
    2.36 0.9909          2.71   0.9966   3.02 0.9987      3.32 0.9995      3.62 0.9999
    2.37 0.9911          2.72   0.9967   3.03 0.9988      3.33 0.9996      3.63 0.9999
    2.38 0.9913          2.73   0.9968   3.04 0.9988      3.34 0.9996      3.64 0.9999
    2.39 0.9916          2.74   0.9969   3.05 0.9989      3.35 0.9996      3.65 0.9999
    2.40 0.9918          2.75   0.9970   3.06 0.9989      3.36 0.9996      3.66 0.9999
    2.41 0.9920          2.76   0.9971   3.07 0.9989      3.37 0.9996      3.67 0.9999
    2.42 0.9922          2.77   0.9972   3.08 0.9990      3.38 0.9996      3.68 0.9999
    2.43 0.9925          2.78   0.9973   3.09 0.9990      3.39 0.9997      3.69 0.9999
    2.44 0.9927          2.79   0.9974   3.10 0.9990      3.40 0.9997      3.70 0.9999
    2.45 0.9929          2.80   0.9974   3.11 0.9991      3.41 0.9997      3.71 0.9999
    2.46 0.9931          2.81   0.9975   3.12 0.9991      3.42 0.9997      3.72 0.9999
    2.47 0.9932          2.82   0.9976   3.13 0.9991      3.43 0.9997      3.73 0.9999
    2.48 0.9934          2.83   0.9977   3.14 0.9992      3.44 0.9997      3.74 0.9999
    2.49 0.9936          2.84   0.9977   3.15 0.9992      3.45 0.9997      3.75 0.9999
    2.50 0.9938          2.85   0.9978   3.16 0.9992      3.46 0.9997      3.76 0.9999
    2.51 0.9940          2.86   0.9979   3.17 0.9992      3.47 0.9997      3.77 0.9999
    2.52 0.9941          2.87   0.9979   3.18 0.9993      3.48 0.9997      3.78 0.9999
    2.53 0.9943          2.88   0.9980   3.19 0.9993      3.49 0.9998      3.79 0.9999
    2.54 0.9945          2.89   0.9981   3.20 0.9993      3.50 0.9998      3.80 0.9999
    2.55 0.9946          2.90   0.9981
    2.56 0.9948
    2.57 0.9949
    2.58 0.9951
    2.59 0.9952
    2.60 0.9953
