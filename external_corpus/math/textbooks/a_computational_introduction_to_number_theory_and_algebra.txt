A Computational Introduction to Number Theory
                      and Algebra

                          (Version 2)

                                         Victor Shoup
This PDF document contains hyperlinks, and one may navigate through it by click-
ing on theorem, definition, lemma, equation, and page numbers, as well as URLs,
and chapter and section titles in the table of contents; most PDF viewers should
also display a list of "bookmarks" that allow direct access to chapters and sections.
Copyright © 2008 by Victor Shoup <victor@shoup.net>

The electronic version of this work is distributed under the terms and conditions of
a Creative Commons license (Attribution-NonCommercial-NoDerivs 3.0):

         You are free to copy, distribute, and display the electronic version
         of this work under the following conditions:
         Attribution. You must give the original author credit.
         Noncommercial. You may not use the electronic version of this

                   work for commercial purposes.
         No Derivative Works. You may not alter, transform, or build upon

                   the electronic version of this work.

         For any reuse or distribution, you must make these license terms
         clear to others.
         Any of these conditions can be waived if you get permission from
         the author.

For more information about the license, visit
                 creativecommons.org/licenses/by-nd-nc/3.0.

All other rights reserved. In particular, the right to publish or distribute this work
in print form belongs exclusively to Cambridge University Press.
Contents

Preface                                                         page x
Preliminaries                                                       xiv

1 Basic properties of the integers                                    1
       1.1 Divisibility and primality                                 1
       1.2 Ideals and greatest common divisors                        5
       1.3 Some consequences of unique factorization                 10

2 Congruences                                                        15
       2.1 Equivalence relations                                     15
       2.2 Definitions and basic properties of congruences           16
       2.3 Solving linear congruences                                19
       2.4 The Chinese remainder theorem                             22
       2.5 Residue classes                                           25
       2.6 Euler's phi function                                      31
       2.7 Euler's theorem and Fermat's little theorem               32
       2.8 Quadratic residues                                        35
       2.9 Summations over divisors                                  45

3 Computing with large integers                                      50
       3.1 Asymptotic notation                                       50
       3.2 Machine models and complexity theory                      53
       3.3 Basic integer arithmetic                                  55
       3.4 Computing in Zn                                           64
       3.5 Faster integer arithmetic ()                              69
       3.6 Notes                                                     71

4 Euclid's algorithm                                                 74
       4.1 The basic Euclidean algorithm                             74
       4.2 The extended Euclidean algorithm                          77
       4.3 Computing modular inverses and Chinese remaindering       82

                                                 v
vi                            Contents

    4.4 Speeding up algorithms via modular computation         84

    4.5 An effective version of Fermat's two squares theorem   86

    4.6 Rational reconstruction and applications               89

    4.7 The RSA cryptosystem                                   99

    4.8 Notes                                                  102

5 The distribution of primes                                   104

    5.1 Chebyshev's theorem on the density of primes           104

    5.2 Bertrand's postulate                                   108

    5.3 Mertens' theorem                                       110

    5.4 The sieve of Eratosthenes                              115

    5.5 The prime number theorem . . . and beyond              116

    5.6 Notes                                                  124

6 Abelian groups                                               126

    6.1 Definitions, basic properties, and examples            126

    6.2 Subgroups                                              132

    6.3 Cosets and quotient groups                             137

    6.4 Group homomorphisms and isomorphisms                   142

    6.5 Cyclic groups                                          153

    6.6 The structure of finite abelian groups ()              163

7 Rings                                                        166

    7.1 Definitions, basic properties, and examples            166

    7.2 Polynomial rings                                       176

    7.3 Ideals and quotient rings                              185

    7.4 Ring homomorphisms and isomorphisms                    192

    7.5 The structure of Zn                                    203

8 Finite and discrete probability distributions                207

    8.1 Basic definitions                                      207

    8.2 Conditional probability and independence               213

    8.3 Random variables                                       221

    8.4 Expectation and variance                               233

    8.5 Some useful bounds                                     241

    8.6 Balls and bins                                         245

    8.7 Hash functions                                         252

    8.8 Statistical distance                                   260

    8.9 Measures of randomness and the leftover hash lemma ()  266

    8.10 Discrete probability distributions                    270

    8.11 Notes                                                 275
                              Contents                       vii

9 Probabilistic algorithms                                   277

9.1 Basic definitions                                        278

9.2 Generating a random number from a given interval         285

9.3 The generate and test paradigm                           287

9.4 Generating a random prime                                292

9.5 Generating a random non-increasing sequence              295

9.6 Generating a random factored number                      298

9.7 Some complexity theory                                   302

9.8 Notes                                                    304

10 Probabilistic primality testing                           306

10.1 Trial division                                          306

10.2 The Miller-Rabin test                                   307

10.3 Generating random primes using the Miller-Rabin test    311

10.4 Factoring and computing Euler's phi function            320

10.5 Notes                                                   324

11 Finding generators and discrete logarithms in Zp          327

11.1 Finding a generator for Zp                              327

11.2 Computing discrete logarithms in Zp                     329

11.3 The Diffie-Hellman key establishment protocol           334

11.4 Notes                                                   340

12 Quadratic reciprocity and computing modular square roots  342

12.1 The Legendre symbol                                     342

12.2 The Jacobi symbol                                       346

12.3 Computing the Jacobi symbol                             348

12.4 Testing quadratic residuosity                           349

12.5 Computing modular square roots                          350

12.6 The quadratic residuosity assumption                    355

12.7 Notes                                                   357

13 Modules and vector spaces                                 358

13.1 Definitions, basic properties, and examples             358

13.2 Submodules and quotient modules                         360

13.3 Module homomorphisms and isomorphisms                   363

13.4 Linear independence and bases                           367

13.5 Vector spaces and dimension                             370

14 Matrices                                                  377

14.1 Basic definitions and properties                        377

14.2 Matrices and linear maps                                381

14.3 The inverse of a matrix                                 386
viii                               Contents

      14.4 Gaussian elimination                                 388

      14.5 Applications of Gaussian elimination                 392

      14.6 Notes                                                398

15 Subexponential-time discrete logarithms and factoring        399

      15.1 Smooth numbers                                       399

      15.2 An algorithm for discrete logarithms                 400

      15.3 An algorithm for factoring integers                  407

      15.4 Practical improvements                               414

      15.5 Notes                                                418

16 More rings                                                   421

      16.1 Algebras                                             421

      16.2 The field of fractions of an integral domain         427

      16.3 Unique factorization of polynomials                  430

      16.4 Polynomial congruences                               435

      16.5 Minimal polynomials                                  438

      16.6 General properties of extension fields               440

      16.7 Formal derivatives                                   444

      16.8 Formal power series and Laurent series               446

      16.9 Unique factorization domains ()                      451

      16.10 Notes                                               464

17 Polynomial arithmetic and applications                       465

      17.1 Basic arithmetic                                     465

      17.2 Computing minimal polynomials in F [X ]/(f )(I)      468

      17.3 Euclid's algorithm                                   469

      17.4 Computing modular inverses and Chinese remaindering  472

      17.5 Rational function reconstruction and applications    474

      17.6 Faster polynomial arithmetic ()                      478

      17.7 Notes                                                484

18 Linearly generated sequences and applications                486

      18.1 Basic definitions and properties                     486

      18.2 Computing minimal polynomials: a special case        490

      18.3 Computing minimal polynomials: a more general case   492

      18.4 Solving sparse linear systems                        497

      18.5 Computing minimal polynomials in F [X ]/(f )(II)     500

      18.6 The algebra of linear transformations ()             501

      18.7 Notes                                                508

19 Finite fields                                                509

      19.1 Preliminaries                                        509
                                 Contents                           ix

       19.2 The existence of finite fields                          511

       19.3 The subfield structure and uniqueness of finite fields  515

       19.4 Conjugates, norms and traces                            516

20 Algorithms for finite fields                                     522

       20.1 Tests for and constructing irreducible polynomials      522

       20.2 Computing minimal polynomials in F [X ]/(f )(III)       525

       20.3 Factoring polynomials: square-free decomposition        526

       20.4 Factoring polynomials: the Cantor-Zassenhaus algorithm  530

       20.5 Factoring polynomials: Berlekamp's algorithm            538

       20.6 Deterministic factorization algorithms ()               544

       20.7 Notes                                                   546

21 Deterministic primality testing                                  548

       21.1 The basic idea                                          548

       21.2 The algorithm and its analysis                          549

       21.3 Notes                                                   558

Appendix: Some useful facts                                         561

Bibliography                                                        566

Index of notation                                                   572

Index                                                               574
                             Preface

Number theory and algebra play an increasingly significant role in computing
and communications, as evidenced by the striking applications of these subjects
to such fields as cryptography and coding theory. My goal in writing this book
was to provide an introduction to number theory and algebra, with an emphasis
on algorithms and applications, that would be accessible to a broad audience. In
particular, I wanted to write a book that would be appropriate for typical students in
computer science or mathematics who have some amount of general mathematical
experience, but without presuming too much specific mathematical knowledge.
Prerequisites. The mathematical prerequisites are minimal: no particular math-
ematical concepts beyond what is taught in a typical undergraduate calculus
sequence are assumed.

   The computer science prerequisites are also quite minimal: it is assumed that the
reader is proficient in programming, and has had some exposure to the analysis of
algorithms, essentially at the level of an undergraduate course on algorithms and
data structures.

   Even though it is mathematically quite self contained, the text does presup-
pose that the reader is comfortable with mathematical formalism and also has
some experience in reading and writing mathematical proofs. Readers may have
gained such experience in computer science courses such as algorithms, automata
or complexity theory, or some type of "discrete mathematics for computer science
students" course. They also may have gained such experience in undergraduate
mathematics courses, such as abstract or linear algebra. The material in these math-
ematics courses may overlap with some of the material presented here; however,
even if the reader already has had some exposure to this material, it nevertheless
may be convenient to have all of the relevant topics easily accessible in one place;
moreover, the emphasis and perspective here will no doubt be different from that
in a traditional mathematical presentation of these subjects.

                                                 x
Preface  xi

Structure of the text. All of the mathematics required beyond basic calculus
is developed "from scratch." Moreover, the book generally alternates between
"theory" and "applications": one or two chapters on a particular set of purely
mathematical concepts are followed by one or two chapters on algorithms and
applications; the mathematics provides the theoretical underpinnings for the appli-
cations, while the applications both motivate and illustrate the mathematics. Of
course, this dichotomy between theory and applications is not perfectly main-
tained: the chapters that focus mainly on applications include the development
of some of the mathematics that is specific to a particular application, and very
occasionally, some of the chapters that focus mainly on mathematics include a
discussion of related algorithmic ideas as well.

   In developing the mathematics needed to discuss certain applications, I have
tried to strike a reasonable balance between, on the one hand, presenting the abso-
lute minimum required to understand and rigorously analyze the applications, and
on the other hand, presenting a full-blown development of the relevant mathemat-
ics. In striking this balance, I wanted to be fairly economical and concise, while at
the same time, I wanted to develop enough of the theory so as to present a fairly
well-rounded account, giving the reader more of a feeling for the mathematical
"big picture."

   The mathematical material covered includes the basics of number theory
(including unique factorization, congruences, the distribution of primes, and
quadratic reciprocity) and of abstract algebra (including groups, rings, fields, and
vector spaces). It also includes an introduction to discrete probability theory -- this
material is needed to properly treat the topics of probabilistic algorithms and cryp-
tographic applications. The treatment of all these topics is more or less standard,
except that the text only deals with commutative structures (i.e., abelian groups and
commutative rings with unity) -- this is all that is really needed for the purposes of
this text, and the theory of these structures is much simpler and more transparent
than that of more general, non-commutative structures.

   The choice of topics covered in this book was motivated primarily by their
applicability to computing and communications, especially to the specific areas
of cryptography and coding theory. Thus, the book may be useful for reference
or self-study by readers who want to learn about cryptography, or it could also be
used as a textbook in a graduate or upper-division undergraduate course on (com-
putational) number theory and algebra, perhaps geared towards computer science
students.

   Since this is an introduction, and not an encyclopedic reference for specialists,
some topics simply could not be covered. One such, whose exclusion will undoubt-
edly be lamented by some, is the theory of lattices, along with algorithms for and
applications of lattice basis reduction. Another omission is fast algorithms for
xii  Preface

integer and polynomial arithmetic -- although some of the basic ideas of this topic
are developed in the exercises, the main body of the text deals only with classical,
quadratic-time algorithms for integer and polynomial arithmetic. However, there
are more advanced texts that cover these topics perfectly well, and they should be
readily accessible to students who have mastered the material in this book.

   Note that while continued fractions are not discussed, the closely related prob-
lem of "rational reconstruction" is covered, along with a number of interesting
applications (which could also be solved using continued fractions).

Guidelines for using the text.

     · There are a few sections that are marked with a "()," indicating that the
         material covered in that section is a bit technical, and is not needed else-
         where.

     · There are many examples in the text, which form an integral part of the
         book, and should not be skipped.

     · There are a number of exercises in the text that serve to reinforce, as well
         as to develop important applications and generalizations of, the material
         presented in the text.

     · Some exercises are underlined. These develop important (but usually sim-
         ple) facts, and should be viewed as an integral part of the book. It is highly
         recommended that the reader work these exercises, or at the very least, read
         and understand their statements.

     · In solving exercises, the reader is free to use any previously stated results
         in the text, including those in previous exercises. However, except where
         otherwise noted, any result in a section marked with a "()," or in §5.5,
         need not and should not be used outside the section in which it appears.

     · There is a very brief "Preliminaries" chapter, which fixes a bit of notation
         and recalls a few standard facts. This should be skimmed over by the reader.

     · There is an appendix that contains a few useful facts; where such a fact is
         used in the text, there is a reference such as "see §An," which refers to the
         item labeled "An" in the appendix.

The second edition. In preparing this second edition, in addition to correcting
errors in the first edition, I have also made a number of other modifications (hope-
fully without introducing too many new errors). Many passages have been rewrit-
ten to improve the clarity of exposition, and many new exercises and examples
have been added. Especially in the earlier chapters, the presentation is a bit more
leisurely. Some material has been reorganized. Most notably, the chapter on prob-
ability now follows the chapters on groups and rings -- this allows a number of
examples and concepts in the probability chapter that depend on algebra to be
                     Preface             xiii

more fully developed. Also, a number of topics have been moved forward in the
text, so as to enliven the material with exciting applications as soon as possible;
for example, the RSA cryptosystem is now described right after Euclid's algorithm
is presented, and some basic results concerning quadratic residues are introduced
right away, in the chapter on congruences. Finally, there are numerous changes
in notation and terminology; for example, the notion of a family of objects is
now used consistently throughout the book (e.g., a pairwise independent family
of random variables, a linearly independent family of vectors, a pairwise relatively
prime family of integers, etc.).

Feedback. I welcome comments on the book (suggestions for improvement, error
reports, etc.) from readers. Please send your comments to

                     victor@shoup.net.

There is also a web site where further material and information relating to the book
(including a list of errata and the latest electronic version of the book) may be
found:

                     www.shoup.net/ntb.

Acknowledgments. I would like to thank a number of people who volunteered
their time and energy in reviewing parts of the book at various stages: Joël Alwen,
Siddhartha Annapureddy, John Black, Carl Bosley, Joshua Brody, Jan Camenisch,
David Cash, Sherman Chow, Ronald Cramer, Marisa Debowsky, Alex Dent, Nelly
Fazio, Rosario Gennaro, Mark Giesbrecht, Stuart Haber, Kristiyan Haralambiev,
Gene Itkis, Charanjit Jutla, Jonathan Katz, Eike Kiltz, Alfred Menezes, Ilya
Mironov, Phong Nguyen, Antonio Nicolosi, Roberto Oliveira, Leonid Reyzin,
Louis Salvail, Berry Schoenmakers, Hovav Shacham, Yair Sovran, Panos Toulis,
and Daniel Wichs. A very special thanks goes to George Stephanides, who trans-
lated the first edition of the book into Greek and reviewed the entire book in prepa-
ration for the second edition. I am also grateful to the National Science Foundation
for their support provided under grants CCR-0310297 and CNS-0716690. Finally,
thanks to David Tranah for all his help and advice, and to David and his colleagues
at Cambridge University Press for their progressive attitudes regarding intellectual
property and open access.

New York, June 2008                      Victor Shoup
                         Preliminaries

We establish here some terminology, notation, and simple facts that will be used
throughout the text.

                               Logarithms and exponentials
We write log x for the natural logarithm of x, and logb x for the logarithm of x to
the base b.

   We write ex for the usual exponential function, where e  2.71828 is the base of
the natural logarithm. We may also write exp[x] instead of ex.

                                       Sets and families
We use standard set-theoretic notation:  denotes the empty set; x  A means that
x is an element, or member, of the set A; for two sets A, B, A  B means that
A is a subset of B (with A possibly equal to B), and A B means that A is a
proper subset of B (i.e., A  B but A = B). Further, A  B denotes the union of
A and B, A  B the intersection of A and B, and A \ B the set of all elements of
A that are not in B. If A is a set with a finite number of elements, then we write
|A| for its size, or cardinality. We use standard notation for describing sets; for
example, if we define the set S := {-2, -1, 0, 1, 2}, then {x2 : x  S} = {0, 1, 4}
and {x  S : x is even} = {-2, 0, 2}.

   We write S1 × · · · × Sn for the Cartesian product of sets S1, . . . , Sn, which is
the set of all n-tuples (a1, . . . , an), where ai  Si for i = 1, . . . , n. We write S×n for
the Cartesian product of n copies of a set S, and for x  S, we write x×n for the
element of S×n consisting of n copies of x. (This notation is a bit non-standard,
but we reserve the more standard notation Sn for other purposes, so as to avoid
ambiguity.)

                                               xiv
Preliminaries                                                               xv

   A family is a collection of objects, indexed by some set I, called an index set.
If for each i  I we have an associated object xi, the family of all such objects
is denoted by {xi}iI . Unlike a set, a family may contain duplicates; that is, we
may have xi = xj for some pair of indices i, j with i = j. Note that while {xi}iI
denotes a family, {xi : i  I} denotes the set whose members are the (distinct)
xi's. If the index set I has some natural order, then we may view the family {xi}iI
as being ordered in the same way; as a special case, a family indexed by a set of
integers of the form {m, . . . , n} or {m, m+1, . . .} is a sequence, which we may write
as {xi}in=m or {xi}i=m. On occasion, if the choice of index set is not important, we
may simply define a family by listing or describing its members, without explicitly
describing an index set; for example, the phrase "the family of objects a, b, c" may
be interpreted as "the family {xi}i3=1, where x1 := a, x2 := b, and x3 := c."

   Unions and intersections may be generalized to arbitrary families of sets. For a
family {Si}iI of sets, the union is

                               Si := {x : x  Si for some i  I},

                                     iI

and for I = , the intersection is

                                Si := {x : x  Si for all i  I}.

                                       iI

Note that if I = , the union is by definition , but the intersection is, in general,
not well defined. However, in certain applications, one might define it by a spe-
cial convention; for example, if all sets under consideration are subsets of some
"ambient space," , then the empty intersection is usually taken to be .

   Two sets A and B are called disjoint if A  B = . A family {Si}iI of sets is
called pairwise disjoint if SiSj =  for all i, j  I with i = j. A pairwise disjoint
family of non-empty sets whose union is S is called a partition of S; equivalently,
{Si}iI is a partition of a set S if each Si is a non-empty subset of S, and each
element of S belongs to exactly one Si.

                                            Numbers

We use standard notation for various sets of numbers:

              Z := the set of integers = {. . . , -2, -1, 0, 1, 2, . . .},
              Q := the set of rational numbers = {a/b : a, b  Z, b = 0},
              R := the set of real numbers,
              C := the set of complex numbers.
xvi  Preliminaries

   We sometimes use the symbols  and - in simple arithmetic expressions

involving real numbers. The interpretation given to such expressions should be
obvious: for example, for every x  R, we have - < x < , x +  = ,
x -  = -,  +  = , and (-) + (-) = -. Expressions such as
x · (±) also make sense, provided x = 0. However, the expressions  -  and

0 ·  have no sensible interpretation.

   We use standard notation for specifying intervals of real numbers: for a, b  R
with a  b,

     [a, b] := {x  R : a  x  b},   (a, b) := {x  R : a < x < b},
     [a, b) := {x  R : a  x < b},  (a, b] := {x  R : a < x  b}.

As usual, this notation is extended to allow a = - for the intervals (a, b] and
(a, b), and b =  for the intervals [a, b) and (a, b).

                                           Functions

We write f : A  B to indicate that f is a function (also called a map) from
a set A to a set B. If A  A, then f (A ) := {f (a) : a  A } is the image of

A under f , and f (A) is simply referred to as the image of f; if B  B, then
f -1(B ) := {a  A : f (a)  B } is the pre-image of B under f .

   A function f : A  B is called one-to-one or injective if f (a) = f (b) implies
a = b. The function f is called onto or surjective if f (A) = B. The function f

is called bijective if it is both injective and surjective; in this case, f is called a

bijection, or a one-to-one correspondence. If f is bijective, then we may define
the inverse function f -1 : B  A, where for b  B, f -1(b) is defined to be
the unique a  A such that f (a) = b; in this case, f -1 is also a bijection, and
(f -1)-1 = f .

   If A  A, then the inclusion map from A to A is the function i : A  A given
by i(a) := a for a  A ; when A = A, this is called the identity map on A. If
A  A, f : A  B, f : A  B, and f (a) = f (a) for all a  A , then we say

that f is the restriction of f to A , and that f is an extension of f to A.

   If f : A  B and g : B  C are functions, their composition is the function
g  f : A  C given by (g  f )(a) := g(f (a)) for a  A. If f : A  B is a
bijection, then f -1  f is the identity map on A, and f  f -1 is the identity map on

B. Conversely, if f : A  B and g : B  A are functions such that g  f is the

identity map on A and f  g is the identity map on B, then f and g are bijections,

each being the inverse of the other. If f : A  B and g : B  C are bijections,
then so is g  f , and (g  f )-1 = f -1  g-1.

   Function composition is associative; that is, for all functions f : A  B,
g : B  C, and h : C  D, we have (h  g)  f = h  (g  f ). Thus, we
Preliminaries  xvii

can simply write h  g  f without any ambiguity. More generally, if we have
functions fi : Ai  Ai+1 for i = 1, . . . , n, where n  2, then we may write their

composition as fn  · · ·  f1 without any ambiguity. If each fi is a bijection, then so
is fn  · · ·  f1, its inverse being f1-1  · · ·  fn-1. As a special case of this, if Ai = A
and fi = f for i = 1, . . . , n, then we may write fn  · · ·  f1 as f n. It is understood
that f 1 = f , and that f 0 is the identity map on A. If f is a bijection, then so is f n
for every non-negative integer n, the inverse function of f n being (f -1)n, which
one may simply write as f -n.

   If f : I  S is a function, then we may view f as the family {xi}iI , where
xi := f (i). Conversely, a family {xi}iI , where all of the xi's belong to some set
S, may be viewed as the function f : I  S given by f (i) := xi for i  I. Really,
functions and families are the same thing, the difference being just one of notation

and emphasis.

                                      Binary operations

A binary operation on a set S is a function from S × S to S, where the value
of the function at (a, b)  S × S is denoted a b.

   A binary operation on S is called associative if for all a, b, c  S, we have
(a b) c = a (b c). In this case, we can simply write a b c without
any ambiguity. More generally, for a1, . . . , an  S, where n  2, we can write
a1 · · · an without any ambiguity.

   A binary operation on S is called commutative if for all a, b  S, we have
a b = b a. If the binary operation is both associative and commutative, then not
only is the expression a1 · · · an unambiguous, but its value remains unchanged
even if we re-order the ai's.

   If is a binary operation on S, and S  S, then S is called closed under if
a b  S for all a, b  S .
                             1

              Basic properties of the integers

This chapter discusses some of the basic properties of the integers, including the
notions of divisibility and primality, unique factorization into primes, greatest com-
mon divisors, and least common multiples.

                              1.1 Divisibility and primality
A central concept in number theory is divisibility.

   Consider the integers Z = {. . . , -2, -1, 0, 1, 2, . . .}. For a, b  Z, we say that a
divides b if az = b for some z  Z. If a divides b, we write a | b, and we may say
that a is a divisor of b, or that b is a multiple of a, or that b is divisible by a. If a
does not divide b, then we write a b.

   We first state some simple facts about divisibility:
Theorem 1.1. For all a, b, c  Z, we have

    (i) a | a, 1 | a, and a | 0;
   (ii) 0 | a if and only if a = 0;
   (iii) a | b if and only if -a | b if and only if a | -b;
   (iv) a | b and a | c implies a | (b + c);
    (v) a | b and b | c implies a | c.
Proof. These properties can be easily derived from the definition of divisibility,
using elementary algebraic properties of the integers. For example, a | a because
we can write a · 1 = a; 1 | a because we can write 1 · a = a; a | 0 because we can
write a·0 = 0. We leave it as an easy exercise for the reader to verify the remaining
properties. 2
   We make a simple observation: if a | b and b = 0, then 1  |a|  |b|. Indeed,
if az = b = 0 for some integer z, then a = 0 and z = 0; it follows that |a|  1,
|z|  1, and so |a|  |a||z| = |b|.

                                                 1
2  Basic properties of the integers

Theorem 1.2. For all a, b  Z, we have a | b and b | a if and only if a = ±b. In
particular, for every a  Z, we have a | 1 if and only if a = ±1.

Proof. Clearly, if a = ±b, then a | b and b | a. So let us assume that a | b and
b | a, and prove that a = ±b. If either of a or b are zero, then the other must be zero

as well. So assume that neither is zero. By the above observation, a | b implies
|a|  |b|, and b | a implies |b|  |a|; thus, |a| = |b|, and so a = ±b. That proves the
first statement. The second statement follows from the first by setting b := 1, and

noting that 1 | a. 2

   The product of any two non-zero integers is again non-zero. This implies the
usual cancellation law: if a, b, and c are integers such that a = 0 and ab = ac, then
we must have b = c; indeed, ab = ac implies a(b - c) = 0, and so a = 0 implies
b - c = 0, and hence b = c.

Primes and composites. Let n be a positive integer. Trivially, 1 and n divide n.
If n > 1 and no other positive integers besides 1 and n divide n, then we say n is
prime. If n > 1 but n is not prime, then we say that n is composite. The number 1
is not considered to be either prime or composite. Evidently, n is composite if and
only if n = ab for some integers a, b with 1 < a < n and 1 < b < n. The first few
primes are

                                   2, 3, 5, 7, 11, 13, 17, . . . .

While it is possible to extend the definition of prime and composite to negative
integers, we shall not do so in this text: whenever we speak of a prime or composite
number, we mean a positive integer.

   A basic fact is that every non-zero integer can be expressed as a signed product
of primes in an essentially unique way. More precisely:

Theorem 1.3 (Fundamental theorem of arithmetic). Every non-zero integer n
can be expressed as

                                       n = ±p1e1 · · · prer ,

where p1, . . . , pr are distinct primes and e1, . . . , er are positive integers. Moreover,
this expression is unique, up to a reordering of the primes.

   Note that if n = ±1 in the above theorem, then r = 0, and the product of zero
terms is interpreted (as usual) as 1.

   The theorem intuitively says that the primes act as the "building blocks" out
of which all non-zero integers can be formed by multiplication (and negation).
The reader may be so familiar with this fact that he may feel it is somehow "self
evident," requiring no proof; however, this feeling is simply a delusion, and most
       1.1 Divisibility and primality                3

of the rest of this section and the next are devoted to developing a proof of this
theorem. We shall give a quite leisurely proof, introducing a number of other very
important tools and concepts along the way that will be useful later.

   To prove Theorem 1.3, we may clearly assume that n is positive, since otherwise,
we may multiply n by -1 and reduce to the case where n is positive.

   The proof of the existence part of Theorem 1.3 is easy. This amounts to showing
that every positive integer n can be expressed as a product (possibly empty) of
primes. We may prove this by induction on n. If n = 1, the statement is true, as
n is the product of zero primes. Now let n > 1, and assume that every positive
integer smaller than n can be expressed as a product of primes. If n is a prime,
then the statement is true, as n is the product of one prime. Assume, then, that n
is composite, so that there exist a, b  Z with 1 < a < n, 1 < b < n, and n = ab.
By the induction hypothesis, both a and b can be expressed as a product of primes,
and so the same holds for n.

   The uniqueness part of Theorem 1.3 is the hard part. An essential ingredient in
this proof is the following:

Theorem 1.4 (Division with remainder property). Let a, b  Z with b > 0.
Then there exist unique q, r  Z such that a = bq + r and 0  r < b.

Proof. Consider the set S of non-negative integers of the form a - bt with t  Z.
This set is clearly non-empty; indeed, if a  0, set t := 0, and if a < 0, set t := a.
Since every non-empty set of non-negative integers contains a minimum, we define
r to be the smallest element of S. By definition, r is of the form r = a - bq for
some q  Z, and r  0. Also, we must have r < b, since otherwise, r - b would be
an element of S smaller than r, contradicting the minimality of r; indeed, if r  b,
then we would have 0  r - b = a - b(q + 1).

   That proves the existence of r and q. For uniqueness, suppose that a = bq + r
and a = bq + r , where 0  r < b and 0  r < b. Then subtracting these two
equations and rearranging terms, we obtain

                                      r - r = b(q - q ).

Thus, r - r is a multiple of b; however, 0  r < b and 0  r < b implies
|r - r| < b; therefore, the only possibility is r - r = 0. Moreover, 0 = b(q - q )
and b = 0 implies q - q = 0. 2

Theorem 1.4 can be visualized as follows:

0  rb  2b                                  3b  a 4b
4  Basic properties of the integers

Starting with a, we subtract (or add, if a is negative) the value b until we end up
with a number in the interval [0, b).

Floors and ceilings. Let us briefly recall the usual floor and ceiling functions,

denoted · and · , respectively. These are functions from R (the real numbers)
to Z. For x  R, x is the greatest integer m  x; equivalently, x is the unique
integer m such that m  x < m + 1, or put another way, such that x = m +  for

some   [0, 1). Also, x is the smallest integer m  x; equivalently, x is the
unique integer m such that m - 1 < x  m, or put another way, such that x = m - 

for some   [0, 1).

The mod operator. Now let a, b  Z with b > 0. If q and r are the unique integers
from Theorem 1.4 that satisfy a = bq + r and 0  r < b, we define

                                         a mod b := r;

that is, a mod b denotes the remainder in dividing a by b. It is clear that b | a if
and only if a mod b = 0. Dividing both sides of the equation a = bq + r by b, we
obtain a/b = q + r/b. Since q  Z and r/b  [0, 1), we see that q = a/b . Thus,

                                  (a mod b) = a - b a/b .

One can use this equation to extend the definition of a mod b to all integers a and
b, with b = 0; that is, for b < 0, we simply define a mod b to be a - b a/b .

   Theorem 1.4 may be generalized so that when dividing an integer a by a positive

integer b, the remainder is placed in an interval other than [0, b). Let x be any
real number, and consider the interval [x, x + b). As the reader may easily verify,
this interval contains precisely b integers, namely, x , . . . , x + b - 1. Applying

Theorem 1.4 with a - x in place of a, we obtain:

Theorem 1.5. Let a, b  Z with b > 0, and let x  R. Then there exist unique
q, r  Z such that a = bq + r and r  [x, x + b).

EXERCISE 1.1. Let a, b, d  Z with d = 0. Show that a | b if and only if da | db.

EXERCISE 1.2. Let n be a composite integer. Show that there exists a prime p
dividing n, with p  n1/2.

EXERCISE 1.3. Let m be a positive integer. Show that for every real number x  1,
the number of multiples of m in the interval [1, x] is x/m ; in particular, for every
integer n  1, the number of multiples of m among 1, . . . , n is n/m .

EXERCISE 1.4. Let x  R. Show that 2 x  2x  2 x + 1.
1.2 Ideals and greatest common divisors  5

EXERCISE 1.5. Let x  R and n  Z with n > 0. Show that x /n = x/n ; in
particular, a/b /c = a/bc for all positive integers a, b, c.

EXERCISE 1.6. Let a, b  Z with b < 0. Show that (a mod b)  (b, 0].

EXERCISE 1.7. Show that Theorem 1.5 also holds for the interval (x, x + b]. Does
it hold in general for the intervals [x, x + b] or (x, x + b)?

                       1.2 Ideals and greatest common divisors

To carry on with the proof of Theorem 1.3, we introduce the notion of an ideal of
Z, which is a non-empty set of integers that is closed under addition, and closed
under multiplication by an arbitrary integer. That is, a non-empty set I  Z is an
ideal if and only if for all a, b  I and all z  Z, we have

                                   a + b  I and az  I.

Besides its utility in proving Theorem 1.3, the notion of an ideal is quite useful in
a number of contexts, which will be explored later.

   It is easy to see that every ideal I contains 0: since a  I for some integer a,
we have 0 = a · 0  I. Also, note that if an ideal I contains an integer a, it also
contains -a, since -a = a · (-1)  I. Thus, if an ideal contains a and b, it also
contains a - b. It is clear that {0} and Z are ideals. Moreover, an ideal I is equal
to Z if and only if 1  I; to see this, note that 1  I implies that for every z  Z,
we have z = 1 · z  I, and hence I = Z; conversely, if I = Z, then in particular,
1  I.

   For a  Z, define aZ := {az : z  Z}; that is, aZ is the set of all multiples of a.
If a = 0, then clearly aZ = {0}; otherwise, aZ consists of the distinct integers

                             . . . , -3a, -2a, -a, 0, a, 2a, 3a, . . . .

It is easy to see that aZ is an ideal: for all az, az  aZ and z  Z, we have
az + az = a(z + z )  aZ and (az)z = a(zz )  aZ. The ideal aZ is called
the ideal generated by a, and an ideal of the form aZ for some a  Z is called a
principal ideal.

   Observe that for all a, b  Z, we have b  aZ if and only if a | b. Also
observe that for every ideal I, we have b  I if and only if bZ  I. Both of
these observations are simple consequences of the definitions, as the reader may
verify. Combining these two observations, we see that bZ  aZ if and only if a | b.

   Suppose I1 and I2 are ideals. Then it is not hard to see that the set

                          I1 + I2 := {a1 + a2 : a1  I1, a2  I2}
6  Basic properties of the integers

is also an ideal. Indeed, suppose a1 + a2  I1 + I2 and b1 + b2  I1 + I2. Then we
have (a1 + a2) + (b1 + b2) = (a1 + b1) + (a2 + b2)  I1 + I2, and for every z  Z,
we have (a1 + a2)z = a1z + a2z  I1 + I2.

Example 1.1. Consider the principal ideal 3Z. This consists of all multiples of 3;
that is, 3Z = {. . . , -9, -6, -3, 0, 3, 6, 9, . . .}. 2

Example 1.2. Consider the ideal 3Z + 5Z. This ideal contains 3 · 2 + 5 · (-1) = 1.
Since it contains 1, it contains all integers; that is, 3Z + 5Z = Z. 2

Example 1.3. Consider the ideal 4Z + 6Z. This ideal contains 4 · (-1) + 6 · 1 = 2,
and therefore, it contains all even integers. It does not contain any odd integers,
since the sum of two even integers is again even. Thus, 4Z + 6Z = 2Z. 2

   In the previous two examples, we defined an ideal that turned out upon closer
inspection to be a principal ideal. This was no accident: the following theorem
says that all ideals of Z are principal.

Theorem 1.6. Let I be an ideal of Z. Then there exists a unique non-negative
integer d such that I = dZ.

Proof. We first prove the existence part of the theorem. If I = {0}, then d = 0
does the job, so let us assume that I = {0}. Since I contains non-zero integers, it
must contain positive integers, since if a  I then so is -a. Let d be the smallest
positive integer in I. We want to show that I = dZ.

   We first show that I  dZ. To this end, let a be any element in I. It suffices
to show that d | a. Using the division with remainder property, write a = dq + r,
where 0  r < d. Then by the closure properties of ideals, one sees that r = a - dq
is also an element of I, and by the minimality of the choice of d, we must have
r = 0. Thus, d | a.

   We have shown that I  dZ. The fact that dZ  I follows from the fact that
d  I. Thus, I = dZ.

   That proves the existence part of the theorem. For uniqueness, note that if
dZ = eZ for some non-negative integer e, then d | e and e | d, from which it
follows by Theorem 1.2 that d = ±e; since d and e are non-negative, we must have
d = e. 2

Greatest common divisors. For a, b  Z, we call d  Z a common divisor of a
and b if d | a and d | b; moreover, we call such a d a greatest common divisor of
a and b if d is non-negative and all other common divisors of a and b divide d.

Theorem 1.7. For all a, b  Z, there exists a unique greatest common divisor d of
a and b, and moreover, aZ + bZ = dZ.
                1.2 Ideals and greatest common divisors  7

Proof. We apply the previous theorem to the ideal I := aZ + bZ. Let d  Z with
I = dZ, as in that theorem. We wish to show that d is a greatest common divisor
of a and b. Note that a, b, d  I and d is non-negative.

   Since a  I = dZ, we see that d | a; similarly, d | b. So we see that d is a
common divisor of a and b.

   Since d  I = aZ + bZ, there exist s, t  Z such that as + bt = d. Now suppose
a = a d and b = b d for some a , b , d  Z. Then the equation as + bt = d implies
that d (a s + b t) = d, which says that d | d. Thus, any common divisor d of a and

b divides d.

   That proves that d is a greatest common divisor of a and b. For uniqueness, note

that if e is a greatest common divisor of a and b, then d | e and e | d, and hence
d = ±e; since both d and e are non-negative by definition, we have d = e. 2

   For a, b  Z, we write gcd(a, b) for the greatest common divisor of a and b. We
say that a, b  Z are relatively prime if gcd(a, b) = 1, which is the same as saying
that the only common divisors of a and b are ±1.

   The following is essentially just a restatement of Theorem 1.7, but we state it
here for emphasis:

Theorem 1.8. Let a, b, r  Z and let d := gcd(a, b). Then there exist s, t  Z such
that as + bt = r if and only if d | r. In particular, a and b are relatively prime if
and only if there exist integers s and t such that as + bt = 1.

Proof. We have

                      as + bt = r for some s, t  Z
                 r  aZ + bZ
                 r  dZ (by Theorem 1.7)
                 d | r.

That proves the first statement. The second statement follows from the first, setting
r := 1. 2

   Note that as we have defined it, gcd(0, 0) = 0. Also note that when at least one
of a or b are non-zero, gcd(a, b) may be characterized as the largest positive integer
that divides both a and b, and as the smallest positive integer that can be expressed
as as + bt for integers s and t.

Theorem 1.9. Let a, b, c  Z such that c | ab and gcd(a, c) = 1. Then c | b.

Proof. Suppose that c | ab and gcd(a, c) = 1. Then since gcd(a, c) = 1, by
Theorem 1.8 we have as + ct = 1 for some s, t  Z. Multiplying this equation by
8             Basic properties of the integers

b, we obtain

              abs + cbt = b.                                    (1.1)

Since c divides ab by hypothesis, and since c clearly divides cbt, it follows that c
divides the left-hand side of (1.1), and hence that c divides b. 2

   Suppose that p is a prime and a is any integer. As the only divisors of p are ±1
and ±p, we have

                               p | a = gcd(a, p) = p, and
                               p a = gcd(a, p) = 1.

Combining this observation with the previous theorem, we have:

Theorem 1.10. Let p be prime, and let a, b  Z. Then p | ab implies that p | a or
p | b.

Proof. Assume that p | ab. If p | a, we are done, so assume that p a. By the above
observation, gcd(a, p) = 1, and so by Theorem 1.9, we have p | b. 2

   An obvious corollary to Theorem 1.10 is that if a1, . . . , ak are integers, and if p
is a prime that divides the product a1 · · · ak, then p | ai for some i = 1, . . . , k. This
is easily proved by induction on k. For k = 1, the statement is trivially true. Now

let k > 1, and assume that statement holds for k - 1. Then by Theorem 1.10, either

p | a1 or p | a2 · · · ak; if p | a1, we are done; otherwise, by induction, p divides one

of a2, . . . , ak.

Finishing the proof of Theorem 1.3. We are now in a position to prove the unique-

ness part of Theorem 1.3, which we can state as follows: if p1, . . . , pr are primes

(not necessarily distinct), and q1, . . . , qs are primes (also not necessarily distinct),

such that

              p1 · · · pr = q1 · · · qs,                        (1.2)

then (p1, . . . , pr) is just a reordering of (q1, . . . , qs). We may prove this by induction
on r. If r = 0, we must have s = 0 and we are done. Now suppose r > 0, and

that the statement holds for r - 1. Since r > 0, we clearly must have s > 0.

Also, as p1 obviously divides the left-hand side of (1.2), it must also divide the

right-hand side of (1.2); that is, p1 | q1 · · · qs. It follows from (the corollary to)
Theorem 1.10 that p1 | qj for some j = 1, . . . , s, and moreover, since qj is prime,
we must have p1 = qj. Thus, we may cancel p1 from the left-hand side of (1.2)

and qj from the right-hand side of (1.2), and the statement now follows from the

induction hypothesis. That proves the uniqueness part of Theorem 1.3.
1.2 Ideals and greatest common divisors                          9

EXERCISE 1.8. Let I be a non-empty set of integers that is closed under addition
(i.e., a + b  I for all a, b  I). Show that I is an ideal if and only if -a  I for all

a  I.

EXERCISE 1.9. Show that for all integers a, b, c, we have:
    (a) gcd(a, b) = gcd(b, a);
   (b) gcd(a, b) = |a|  a | b;
    (c) gcd(a, 0) = gcd(a, a) = |a| and gcd(a, 1) = 1;
   (d) gcd(ca, cb) = |c| gcd(a, b).

EXERCISE 1.10. Show that for all integers a, b with d := gcd(a, b) = 0, we have
gcd(a/d, b/d) = 1.

EXERCISE 1.11. Let n be an integer. Show that if a, b are relatively prime integers,
each of which divides n, then ab divides n.

EXERCISE 1.12. Show that two integers are relatively prime if and only if there is
no one prime that divides both of them.

EXERCISE 1.13. Let a, b1, . . . , bk be integers. Show that gcd(a, b1 · · · bk) = 1 if
and only if gcd(a, bi) = 1 for i = 1, . . . , k.

EXERCISE 1.14. Let p be a prime and k an integer, with 0 < k < p. Show that the
binomial coefficient

p=  p!
           ,
k k!(p - k)!

which is an integer (see §A2), is divisible by p.

EXERCISE 1.15. An integer a is called square-free if it is not divisible by the
square of any integer greater than 1. Show that:

    (a) a is square-free if and only if a = ±p1 · · · pr, where the pi's are distinct
         primes;

   (b) every positive integer n can be expressed uniquely as n = ab2, where a and
         b are positive integers, and a is square-free.

EXERCISE 1.16. For each positive integer m, let Im denote {0, . . . , m - 1}. Let
a, b be positive integers, and consider the map

                              : Ib × Ia  Iab
                                      (s, t)  (as + bt) mod ab.

Show  is a bijection if and only if gcd(a, b) = 1.
10   Basic properties of the integers

EXERCISE 1.17. Let a, b, c be positive integers satisfying gcd(a, b) = 1 and

c  (a - 1)(b - 1). Show that there exist non-negative integers s, t such that
c = as + bt.

EXERCISE 1.18. For each positive integer n, let Dn denote the set of positive
divisors of n. Let n1, n2 be relatively prime, positive integers. Show that the sets
Dn1 × Dn2 and Dn1n2 are in one-to-one correspondence, via the map that sends
(d1, d2)  Dn1 × Dn2 to d1d2.

                   1.3 Some consequences of unique factorization
The following theorem is a consequence of just the existence part of Theorem 1.3:

Theorem 1.11. There are infinitely many primes.

Proof. By way of contradiction, suppose that there were only finitely many primes;
call them p1, . . . , pk. Then set M := ik=1 pi and N := M + 1. Consider a prime
p that divides N. There must be at least one such prime p, since N  2, and

every positive integer can be written as a product of primes. Clearly, p cannot

equal any of the pi's, since if it did, then p would divide M, and hence also divide
N - M = 1, which is impossible. Therefore, the prime p is not among p1, . . . , pk,

which contradicts our assumption that these are the only primes. 2

   For each prime p, we may define the function p, mapping non-zero integers to
non-negative integers, as follows: for every integer n = 0, if n = pem, where p m,

then p(n) := e. We may then write the factorization of n into primes as

     n = ± pp(n),

                 p

where the product is over all primes p; although syntactically this is an infinite
product, all but finitely many of its terms are equal to 1, and so this expression
makes sense.

   Observe that if a and b are non-zero integers, then

     p(a · b) = p(a) + p(b) for all primes p,                       (1.3)

and

     a | b  p(a)  p(b) for all primes p.                            (1.4)

From this, it is clear that

                               gcd(a, b) = pmin(p(a),p(b)).

                                                                 p
                   1.3 Some consequences of unique factorization  11

Least common multiples. For a, b  Z, a common multiple of a and b is an
integer m such that a | m and b | m; moreover, such an m is the least common
multiple of a and b if m is non-negative and m divides all common multiples of
a and b. It is easy to see that the least common multiple exists and is unique,
and we denote the least common multiple of a and b by lcm(a, b). Indeed, for all
a, b  Z, if either a or b are zero, the only common multiple of a and b is 0, and so
lcm(a, b) = 0; otherwise, if neither a nor b are zero, we have

                               lcm(a, b) = pmax(p(a),p(b)),

                                                                 p

or equivalently, lcm(a, b) may be characterized as the smallest positive integer
divisible by both a and b.

   It is convenient to extend the domain of definition of p to include 0, defining
p(0) := . If we interpret expressions involving "" appropriately (see Prelimi-
naries), then for arbitrary a, b  Z, both (1.3) and (1.4) hold, and in addition,

     p(gcd(a, b)) = min(p(a), p(b)) and p(lcm(a, b)) = max(p(a), p(b))

for all primes p.

Generalizing gcd's and lcm's to many integers. It is easy to generalize the
notions of greatest common divisor and least common multiple from two integers
to many integers. Let a1, . . . , ak be integers. We call d  Z a common divisor
of a1, . . . , ak if d | ai for i = 1, . . . , k; moreover, we call such a d the greatest
common divisor of a1, . . . , ak if d is non-negative and all other common divi-
sors of a1, . . . , ak divide d. The greatest common divisor of a1, . . . , ak is denoted
gcd(a1, . . . , ak) and is the unique non-negative integer d satisfying

                   p(d) = min(p(a1), . . . , p(ak)) for all primes p.

Analogously, we call m  Z a common multiple of a1, . . . , ak if ai | m for all
i = 1, . . . , k; moreover, such an m is called the least common multiple of a1, . . . , ak
if m divides all common multiples of a1, . . . , ak. The least common multiple of
a1, . . . , ak is denoted lcm(a1, . . . , ak) and is the unique non-negative integer m sat-
isfying

                   p(m) = max(p(a1), . . . , p(ak)) for all primes p.

Finally, we say that the family {ai}ik=1 is pairwise relatively prime if for all indices
i, j with i = j, we have gcd(ai, aj) = 1. Certainly, if {ai}ik=1 is pairwise relatively
prime, and k > 1, then gcd(a1, . . . , ak) = 1; however, gcd(a1, . . . , ak) = 1 does not
imply that {ai}ik=1 is pairwise relatively prime.
12  Basic properties of the integers

Rational numbers. Consider the rational numbers Q = {a/b : a, b  Z, b = 0}.
Given any rational number a/b, if we set d := gcd(a, b), and define the integers
a0 := a/d and b0 := b/d, then we have a/b = a0/b0 and gcd(a0, b0) = 1. More-
over, if a1/b1 = a0/b0, then we have a1b0 = a0b1, and so b0 | a0b1; also, since
gcd(a0, b0) = 1, we see that b0 | b1; writing b1 = b0c, we see that a1 = a0c. Thus,
we can represent every rational number as a fraction in lowest terms, which means

a fraction of the form a0/b0 where a0 and b0 are relatively prime; moreover, the

values of a0 and b0 are uniquely determined up to sign, and every other fraction

that represents the same rational number is of the form a0c/b0c, for some non-zero

integer c.

EXERCISE 1.19. Let n be an integer. Generalizing Exercise 1.11, show that if
{ai}ik=1 is a pairwise relatively prime family of integers, where each ai divides n,
then their product ik=1 ai also divides n.

EXERCISE 1.20. Show that for all integers a, b, c, we have:
    (a) lcm(a, b) = lcm(b, a);
   (b) lcm(a, b) = |a|  b | a;
    (c) lcm(a, a) = lcm(a, 1) = |a|;
   (d) lcm(ca, cb) = |c| lcm(a, b).

EXERCISE 1.21. Show that for all integers a, b, we have:
    (a) gcd(a, b) · lcm(a, b) = |ab|;
   (b) gcd(a, b) = 1 = lcm(a, b) = |ab|.

EXERCISE 1.22. Let a1, . . . , ak  Z with k > 1. Show that:
     gcd(a1, . . . , ak) = gcd(a1, gcd(a2, . . . , ak)) = gcd(gcd(a1, . . . , ak-1), ak);
     lcm(a1, . . . , ak) = lcm(a1, lcm(a2, . . . , ak)) = lcm(lcm(a1, . . . , ak-1), ak).

EXERCISE 1.23. Let a1, . . . , ak  Z with d := gcd(a1, . . . , ak). Show that dZ =
a1Z + · · · + akZ; in particular, there exist integers z1, . . . , zk such that d = a1z1 +
· · · + akzk.

EXERCISE 1.24. Show that if {ai}ik=1 is a pairwise relatively prime family of inte-
gers, then lcm(a1, . . . , ak) = |a1 · · · ak|.

EXERCISE 1.25. Show that every non-zero x  Q can be expressed as
                                       x = ±p1e1 · · · prer ,

where the pi's are distinct primes and the ei's are non-zero integers, and that this
expression in unique up to a reordering of the primes.
1.3 Some consequences of unique factorization                                13

EXERCISE 1.26. Let n and k be positive integers, and suppose x  Q such that
xk = n for some x  Q. Show that x  Z. In other words, k n is either an integer
or is irrational.

EXERCISE 1.27. Show that gcd(a + b, lcm(a, b)) = gcd(a, b) for all a, b  Z.

EXERCISE 1.28. Show that for every positive integer k, there exist k consecutive
composite integers. Thus, there are arbitrarily large gaps between primes.

EXERCISE 1.29. Let p be a prime. Show that for all a, b  Z, we have p(a + b) 
min{p(a), p(b)}, and p(a + b) = p(a) if p(a) < p(b).

EXERCISE 1.30. For a given prime p, we may extend the domain of definition of
p from Z to Q: for non-zero integers a, b, let us define p(a/b) := p(a) - p(b).
Show that:

    (a) this definition of p(a/b) is unambiguous, in the sense that it does not
         depend on the particular choice of a and b;

   (b) for all x, y  Q, we have p(xy) = p(x) + p(y);
    (c) for all x, y  Q, we have p(x + y)  min{p(x), p(y)}, and p(x + y) =

         p(x) if p(x) < p(y);
   (d) for all non-zero x  Q, we have x = ± p pp(x), where the product is over

         all primes, and all but a finite number of terms in the product are equal to 1;

    (e) for all x  Q, we have x  Z if and only if p(x)  0 for all primes p.

EXERCISE 1.31. Let n be a positive integer, and let 2k be the highest power of 2
in the set S := {1, . . . , n}. Show that 2k does not divide any other element in S.

EXERCISE 1.32. Let n  Z with n > 1. Show that in=1 1/i is not an integer.

EXERCISE 1.33. Let n be a positive integer, and let Cn denote the number of pairs
of integers (a, b) with a, b  {1, . . . , n} and gcd(a, b) = 1, and let Fn be the number

of distinct rational numbers a/b, where 0  a < b  n.

(a) Show that Fn = (Cn + 1)/2.

(b) Show that Cn  n2/4. Hint: first show that Cn  n2(1 -  d2 1/d2), and
     then show that d2 1/d2  3/4.

EXERCISE 1.34. This exercise develops a characterization of least common mul-
tiples in terms of ideals.

    (a) Arguing directly from the definition of an ideal, show that if I and J are
         ideals of Z, then so is I  J .

   (b) Let a, b  Z, and consider the ideals I := aZ and J := bZ. By part
14  Basic properties of the integers

    (a), we know that I  J is an ideal. By Theorem 1.6, we know that
    I  J = mZ for some uniquely determined non-negative integer m. Show
    that m = lcm(a, b).
                             2

                         Congruences

This chapter introduces the basic properties of congruences modulo n, along with
the related notion of residue classes modulo n. Other items discussed include the
Chinese remainder theorem, Euler's phi function, Euler's theorem, Fermat's little
theorem, quadratic residues, and finally, summations over divisors.

                                 2.1 Equivalence relations
Before discussing congruences, we review the definition and basic properties of
equivalence relations.

   Let S be a set. A binary relation  on S is called an equivalence relation if it is
         reflexive: a  a for all a  S,
         symmetric: a  b implies b  a for all a, b  S, and
         transitive: a  b and b  c implies a  c for all a, b, c  S.

   If  is an equivalence relation on S, then for a  S one defines its equivalence
class as the set {x  S : x  a}.
Theorem 2.1. Let  be an equivalence relation on a set S, and for a  S, let [a]
denote its equivalence class. Then for all a, b  S, we have:

    (i) a  [a];
   (ii) a  [b] implies [a] = [b].
Proof. (i) follows immediately from reflexivity. For (ii), suppose a  [b], so that
a  b by definition. We want to show that [a] = [b]. To this end, consider any

                                                15
16  Congruences

x  S. We have

           x  [a] = x  a (by definition)
                     = x  b (by transitivity, and since x  a and a  b)
                     = x  [b].

Thus, [a]  [b]. By symmetry, we also have b  a, and reversing the roles of a and
b in the above argument, we see that [b]  [a]. 2

   This theorem implies that each equivalence class is non-empty, and that each
element of S belongs to a unique equivalence class; in other words, the distinct
equivalence classes form a partition of S (see Preliminaries). A member of an
equivalence class is called a representative of the class.

EXERCISE 2.1. Consider the relations =, , and < on the set R. Which of these
are equivalence relations? Explain your answers.

EXERCISE 2.2. Let S := (R × R) \ {(0, 0)}. For (x, y), (x , y )  S, let us say
(x, y)  (x , y ) if there exists a real number  > 0 such that (x, y) = (x , y ).
Show that  is an equivalence relation; moreover, show that each equivalence class
contains a unique representative that lies on the unit circle (i.e., the set of points
(x, y) such that x2 + y2 = 1).

                2.2 Definitions and basic properties of congruences

Let n be a positive integer. For integers a and b, we say that a is congruent to b
modulo n if n | (a - b), and we write a  b (mod n). If n (a - b), then we write
a  b (mod n). Equivalently, a  b (mod n) if and only if a = b + ny for some
y  Z. The relation a  b (mod n) is called a congruence relation, or simply, a
congruence. The number n appearing in such congruences is called the modulus
of the congruence. This usage of the "mod" notation as part of a congruence is not
to be confused with the "mod" operation introduced in §1.1.

   If we view the modulus n as fixed, then the following theorem says that the
binary relation "·  · (mod n)" is an equivalence relation on the set Z.

Theorem 2.2. Let n be a positive integer. For all a, b, c  Z, we have:
    (i) a  a (mod n);

   (ii) a  b (mod n) implies b  a (mod n);

   (iii) a  b (mod n) and b  c (mod n) implies a  c (mod n).

Proof. For (i), observe that n divides 0 = a - a. For (ii), observe that if n divides
2.2 Definitions and basic properties of congruences                 17

a - b, then it also divides -(a - b) = b - a. For (iii), observe that if n divides a - b
and b - c, then it also divides (a - b) + (b - c) = a - c. 2

   Another key property of congruences is that they are "compatible" with integer
addition and multiplication, in the following sense:

Theorem 2.3. Let a, a , b, b , n  Z with n > 0. If
                           a  a (mod n) and b  b (mod n),

then
                  a + b  a + b (mod n) and a · b  a · b (mod n).

Proof. Suppose that a  a (mod n) and b  b (mod n). This means that there
exist integers x and y such that a = a + nx and b = b + ny. Therefore,

                                 a + b = a + b + n(x + y),

which proves the first congruence of the theorem, and
                 ab = (a + nx)(b + ny) = a b + n(a y + b x + nxy),

which proves the second congruence. 2

   Theorems 2.2 and 2.3 allow one to work with congruence relations modulo n
much as one would with ordinary equalities: one can add to, subtract from, or
multiply both sides of a congruence modulo n by the same integer; also, if b is
congruent to a modulo n, one may substitute b for a in any simple arithmetic
expression (involving addition, subtraction, and multiplication) appearing in a con-
gruence modulo n.

   Now suppose a is an arbitrary, fixed integer, and consider the set of integers z
that satisfy the congruence z  a (mod n). Since z satisfies this congruence if
and only if z = a + ny for some y  Z, we may apply Theorems 1.4 and 1.5
(with a as given, and b := n) to deduce that every interval of n consecutive integers
contains exactly one such z. This simple fact is of such fundamental importance
that it deserves to be stated as a theorem:

Theorem 2.4. Let a, n  Z with n > 0. Then there exists a unique integer z such
that z  a (mod n) and 0  z < n, namely, z := a mod n. More generally, for
every x  R, there exists a unique integer z  [x, x + n) such that z  a (mod n).

Example 2.1. Let us find the set of solutions z to the congruence

3z + 4  6 (mod 7).                                                  (2.1)
18  Congruences

Suppose that z is a solution to (2.1). Subtracting 4 from both sides of (2.1), we
obtain

    3z  2 (mod 7).                                                         (2.2)

Next, we would like to divide both sides of this congruence by 3, to get z by itself

on the left-hand side. We cannot do this directly, but since 5 · 3  1 (mod 7), we
can achieve the same effect by multiplying both sides of (2.2) by 5. If we do this,

and then replace 5 · 3 by 1, and 5 · 2 by 3, we obtain

    z  3 (mod 7).

Thus, if z is a solution to (2.1), we must have z  3 (mod 7); conversely, one can
verify that if z  3 (mod 7), then (2.1) holds. We conclude that the integers z that
are solutions to (2.1) are precisely those integers that are congruent to 3 modulo 7,
which we can list as follows:

    . . . , -18, -11, -4, 3, 10, 17, 24, . . . 2

   In the next section, we shall give a systematic treatment of the problem of solving
linear congruences, such as the one appearing in the previous example.

EXERCISE 2.3. Let a, b, n  Z with n > 0. Show that a  b (mod n) if and only if
(a mod n) = (b mod n).

EXERCISE 2.4. Let a, b, n  Z with n > 0 and a  b (mod n).                  Also, let
c0, c1, . . . , ck  Z. Show that

                c0 + c1a + · · · + ckak  c0 + c1b + · · · + ckbk (mod n).

EXERCISE 2.5. Let a, b, n, n  Z with n > 0, n > 0, and n | n. Show that if
a  b (mod n), then a  b (mod n ).

EXERCISE 2.6. Let a, b, n, n  Z with n > 0, n > 0, and gcd(n, n ) = 1. Show
that if a  b (mod n) and a  b (mod n ), then a  b (mod nn ).

EXERCISE 2.7. Let a, b, n  Z with n > 0 and a  b (mod n). Show that
gcd(a, n) = gcd(b, n).

EXERCISE 2.8. Let a be a positive integer whose base-10 representation is a =
(ak-1 · · · a1a0)10. Let b be the sum of the decimal digits of a; that is, let b :=
a0 + a1 + · · · + ak-1. Show that a  b (mod 9). From this, justify the usual "rules of
thumb" for determining divisibility by 9 and 3: a is divisible by 9 (respectively, 3)
if and only if the sum of the decimal digits of a is divisible by 9 (respectively, 3).
                   2.3 Solving linear congruences                      19

EXERCISE 2.9. Let e be a positive integer. For a  {0, . . . , 2e - 1}, let a~ denote

the integer obtained by inverting the bits in the e-bit, binary representation of a
(note that a~  {0, . . . , 2e - 1}). Show that a~ + 1  -a (mod 2e). This justifies the

usual rule for computing negatives in 2's complement arithmetic (which is really
just arithmetic modulo 2e).

EXERCISE 2.10. Show that the equation 7y3 + 2 = z3 has no solutions y, z  Z.

EXERCISE 2.11. Show that there are 14 distinct, possible, yearly (Gregorian)
calendars, and show that all 14 calendars actually occur.

                             2.3 Solving linear congruences

In this section, we consider the general problem of solving linear congruences.
More precisely, for a given positive integer n, and arbitrary integers a and b, we
wish to determine the set of integers z that satisfy the congruence

                   az  b (mod n).                                      (2.3)

Observe that if (2.3) has a solution z, and if z  z (mod n), then z is also a
solution to (2.3). However, (2.3) may or may not have a solution, and if it does,
such solutions may or may not be uniquely determined modulo n. The following
theorem precisely characterizes the set of solutions of (2.3); basically, it says that
(2.3) has a solution if and only if d := gcd(a, n) divides b, in which case the
solution is uniquely determined modulo n/d.

Theorem 2.5. Let a, n  Z with n > 0, and let d := gcd(a, n).
    (i) For every b  Z, the congruence az  b (mod n) has a solution z  Z if
         and only if d | b.

   (ii) For every z  Z, we have az  0 (mod n) if and only if z  0 (mod n/d).
   (iii) For all z, z  Z, we have az  az (mod n) if and only if z  z (mod n/d).

Proof. For (i), let b  Z be given. Then we have

                 az  b (mod n) for some z  Z
           az = b + ny for some z, y  Z (by definition of congruence)
           az - ny = b for some z, y  Z
           d | b (by Theorem 1.8).

For (ii), we have

                   n | az  n/d | (a/d)z  n/d | z.

All of these implications follow rather trivially from the definition of divisibility,
20  Congruences

except that for the implication n/d | (a/d)z = n/d | z, we use Theorem 1.9
and the fact that gcd(a/d, n/d) = 1.

   For (iii), we have

    az  az (mod n)  a(z - z )  0 (mod n)
                           z - z  0 (mod n/d) (by part (ii))
                           z  z (mod n/d). 2

   We can restate Theorem 2.5 in more concrete terms as follows. Let a, n  Z
with n > 0, and let d := gcd(a, n). Let In := {0, . . . , n - 1} and consider the
"multiplication by a" map

    a : In  In
            z  az mod n.

The image of a consists of the n/d integers
                                  i · d (i = 0, . . . , n/d - 1).

Moreover, every element b in the image of a has precisely d pre-images
                             z0 + j · (n/d) (j = 0, . . . , d - 1),

where z0  {0, . . . , n/d - 1}. In particular, a is a bijection if and only if a and n
are relatively prime.

Example 2.2. The following table illustrates what Theorem 2.5 says for n = 15
and a = 1, 2, 3, 4, 5, 6.

           z 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14
2z mod 15 0 2 4 6 8 10 12 14 1 3 5 7 9 11 13
3z mod 15 0 3 6 9 12 0 3 6 9 12 0 3 6 9 12
4z mod 15 0 4 8 12 1 5 9 13 2 6 10 14 3 7 11
5z mod 15 0 5 10 0 5 10 0 5 10 0 5 10 0 5 10
6z mod 15 0 6 12 3 9 0 6 12 3 9 0 6 12 3 9

   In the second row, we are looking at the values 2z mod 15, and we see that this

row is just a permutation of the first row. So for every b, there exists a unique z
such that 2z  b (mod 15). This is implied by the fact that gcd(2, 15) = 1.

   In the third row, the only numbers hit are the multiples of 3, which follows from
the fact that gcd(3, 15) = 3. Also note that the pattern in this row repeats every five

columns; that is, 3z  3z (mod 15) if and only if z  z (mod 5).

   In the fourth row, we again see a permutation of the first row, which follows
from the fact that gcd(4, 15) = 1.
2.3 Solving linear congruences                        21

   In the fifth row, the only numbers hit are the multiples of 5, which follows from
the fact that gcd(5, 15) = 5. Also note that the pattern in this row repeats every

three columns; that is, 5z  5z (mod 15) if and only if z  z (mod 3).
   In the sixth row, since gcd(6, 15) = 3, we see a permutation of the third row.

The pattern repeats after five columns, although the pattern is a permutation of the

pattern in the third row. 2

We develop some further consequences of Theorem 2.5.

A cancellation law. Let a, n  Z with n > 0. Part (iii) of Theorem 2.5 gives us a
cancellation law for congruences:

         if gcd(a, n) = 1 and az  az (mod n), then z  z (mod n).

More generally, if d := gcd(a, n), then we can cancel a from both sides of a con-
gruence modulo n, as long as we replace the modulus by n/d.

Example 2.3. Observe that

                           5 · 2  5 · (-4) (mod 6).   (2.4)

Part (iii) of Theorem 2.5 tells us that since gcd(5, 6) = 1, we may cancel the
common factor of 5 from both sides of (2.4), obtaining 2  -4 (mod 6), which
one can also verify directly.

   Next observe that

                           15 · 5  15 · 3 (mod 6).    (2.5)

We cannot simply cancel the common factor of 15 from both sides of (2.5); indeed,
5  3 (mod 6). However, gcd(15, 6) = 3, and as part (iii) of Theorem 2.5 guaran-

tees, we do indeed have 5  3 (mod 2). 2

Modular inverses. Again, let a, n  Z with n > 0. We say that z  Z is a
multiplicative inverse of a modulo n if az  1 (mod n). Part (i) of Theorem 2.5
says that a has a multiplicative inverse modulo n if and only if gcd(a, n) = 1.

Moreover, part (iii) of Theorem 2.5 says that the multiplicative inverse of a, if

it exists, is uniquely determined modulo n; that is, if z and z are multiplicative

inverses of a modulo n, then z  z (mod n). Note that if z is a multiplicative

inverse of a modulo n, then a is a multiplicative inverse of z modulo n. Also note

that if a  a (mod n), then z is a multiplicative inverse of a modulo n if and only

if z is a multiplicative inverse of a modulo n.
   Now suppose that a, b, n  Z with n > 0, a = 0, and gcd(a, n) = 1. Theorem 2.5

says that there exists a unique integer z satisfying

az  b (mod n) and 0  z < n.
22  Congruences

Setting s := b/a  Q, we may generalize the "mod" operation, defining s mod
n to be this value z. As the reader may easily verify, this definition of s mod n

does not depend on the particular choice of fraction used to represent the rational
number s. With this notation, we can simply write a-1 mod n to denote the unique

multiplicative inverse of a modulo n that lies in the interval 0, . . . , n - 1.

Example 2.4. Looking back at the table in Example 2.2, we see that
                          2-1 mod 15 = 8 and 4-1 mod 15 = 4,

and that neither 3, 5, nor 6 have modular inverses modulo 15. 2

Example 2.5. Let a, b, n  Z with n > 0. We can describe the set of solutions z  Z
to the congruence az  b (mod n) very succinctly in terms of modular inverses.

   If gcd(a, n) = 1, then setting t := a-1 mod n, and z0 := tb mod n, we see that

z0 is the unique solution to the congruence az  b (mod n) that lies in the interval

{0, . . . , n - 1}.
   More generally, if d := gcd(a, n), then the congruence az  b (mod n) has

a solution if and only if d | b. So suppose that d | b. In this case, if we set
a := a/d, b := b/d, and n := n/d, then for each z  Z, we have az  b (mod n)
if and only if a z  b (mod n ). Moreover, gcd(a , n ) = 1, and therefore, if
we set t := (a )-1 mod n and z0 := tb mod n , then the solutions to the con-

gruence az  b (mod n) that lie in the interval {0, . . . , n - 1} are the d integers
z0, z0 + n , . . . , z0 + (d - 1)n . 2

EXERCISE 2.12. Let a1, . . . , ak, b, n be integers with n > 0. Show that the con-
gruence

                              a1z1 + · · · + akzk  b (mod n)
has a solution z1, . . . , zk  Z if and only if d | b, where d := gcd(a1, . . . , ak, n).

EXERCISE 2.13. Let p be a prime, and let a, b, c, e be integers, such that e > 0,
a  0 (mod pe+1), and 0  c < pe. Define N to be the number of integers
z  {0, . . . , p2e - 1} such that

                                 (az + b) mod p2e pe = c.

Show that N = pe.

                          2.4 The Chinese remainder theorem

Next, we consider systems of linear congruences with respect to moduli that are
relatively prime in pairs. The result we state here is known as the Chinese remain-
der theorem, and is extremely useful in a number of contexts.
                              2.4 The Chinese remainder theorem                                                23

Theorem 2.6 (Chinese remainder theorem). Let {ni}ik=1 be a pairwise relatively
prime family of positive integers, and let a1, . . . , ak be arbitrary integers. Then
there exists a solution a  Z to the system of congruences

                              a  ai (mod ni) (i = 1, . . . , k).

Moreover, any a  Z is a solution to this system of congruences if and only if
a  a (mod n), where n := ik=1 ni.

Proof. To prove the existence of a solution a to the system of congruences, we first
show how to construct integers e1, . . . , ek such that for i, j = 1, . . . , k, we have

                               ej  1 (mod ni) if j = i, (2.6)
                                         0 (mod ni) if j = i.

If we do this, then setting

                                                                   k

                                         a := aiei,

                                                                  i=1

one sees that for j = 1, . . . , k, we have

                                             k

                              a  aiei  aj (mod nj),

                                     i=1

since all the terms in this sum are zero modulo nj, except for the term i = j, which

is congruent to aj modulo nj.

    To construct e1, . . . , ek satisfying (2.6), let n := ik=1 ni as in the statement of
the theorem, and for i = 1, . . . , k, let ni := n/ni; that is, ni is the product of all
the moduli nj with j = i. From the fact that {ni}ik=1 is pairwise relatively prime,
                           =                                                          =
it  follows  that  for  i     1, . . . , k,  we  have  gcd  (  n       i  ,  n  i  )     1,  and  so  we  may  define

ti := (ni )-1 mod ni and ei := ni ti. One sees that ei  1 (mod ni), while for j = i,
we have ni | nj , and so ej  0 (mod ni). Thus, (2.6) is satisfied.

    That proves the existence of a solution a to the given system of congruences. If

a  a (mod n), then since ni | n for i = 1, . . . , k, we see that a  a  ai (mod ni)

for i = 1, . . . , k, and so a also solves the system of congruences.

   Finally, if a is a solution to the given system of congruences, then a  ai 
a (mod ni) for i = 1, . . . , k. Thus, ni | (a - a ) for i = 1, . . . , k. Since {ni}ik=1 is
pairwise relatively prime, this implies n | (a-a ), or equivalently, a  a (mod n). 2

   We can restate Theorem 2.6 in more concrete terms, as follows. For each positive
integer m, let Im denote {0, . . . , m - 1}. Suppose {ni}ik=1 is a pairwise relatively
24               Congruences

prime family of positive integers, and set n := n1 · · · nk. Then the map

                  : In  In1 × · · · × Ink
                        a  (a mod n1, . . . , a mod nk)

is a bijection.

Example 2.6. The following table illustrates what Theorem 2.6 says for n1 = 3
and n2 = 5.

            a 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14
    a mod 3 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2
    a mod 5 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4

   We see that as a ranges from 0 to 14, the pairs (a mod 3, a mod 5) range over
all pairs (a1, a2) with a1  {0, 1, 2} and a2  {0, . . . , 4}, with every pair being hit
exactly once. 2

EXERCISE 2.14. Compute the values e1, e2, e3 in the proof of Theorem 2.6 in the
case where k = 3, n1 = 3, n2 = 5, and n3 = 7. Also, find an integer a such that
a  1 (mod 3), a  -1 (mod 5), and a  5 (mod 7).

EXERCISE 2.15. If you want to show that you are a real nerd, here is an age-
guessing game you might play at a party. You ask a fellow party-goer to divide his
age by each of the numbers 3, 4, and 5, and tell you the remainders. Show how to
use this information to determine his age.

EXERCISE 2.16. Let {ni}ik=1 be a pairwise relatively prime family of positive
integers. Let a1, . . . , ak and b1, . . . , bk be integers, and set di := gcd(ai, ni) for
i = 1, . . . , k. Show that there exists an integer z such that aiz  bi (mod ni) for
i = 1, . . . , k if and only if di | bi for i = 1, . . . , k.

EXERCISE 2.17. For each prime p, let p(·) be defined as in §1.3. Let p1, . . . , pr
be distinct primes, a1, . . . , ar be arbitrary integers, and e1, . . . , er be arbitrary non-
negative integers. Show that there exists an integer a such that pi (a - ai) = ei for
i = 1, . . . , r.

EXERCISE 2.18. Suppose n1 and n2 are positive integers, and let d := gcd(n1, n2).
Let a1 and a2 be arbitrary integers. Show that there exists an integer a such that
a  a1 (mod n1) and a  a2 (mod n2) if and only if a1  a2 (mod d).
           2.5 Residue classes                                               25

           2.5 Residue classes

As we already observed in Theorem 2.2, for any fixed positive integer n, the binary

relation "·  · (mod n)" is an equivalence relation on the set Z. As such, this
relation partitions the set Z into equivalence classes. We denote the equivalence
class containing the integer a by [a]n, and when n is clear from context, we simply
write [a]. By definition, we have

z  [a]  z  a (mod n)  z = a + ny for some y  Z,

and hence

           [a] = a + nZ := {a + ny : y  Z}.

Historically, these equivalence classes are called residue classes modulo n, and we

shall adopt this terminology here as well. Note that a given residue class modulo n
has many different "names"; for example, the residue class [n - 1] is the same as
the residue class [-1]. Any member of a residue class is called a representative

of that class.
   We define Zn to be the set of residue classes modulo n. The following is simply

a restatement of Theorem 2.4:

Theorem 2.7. Let n be a positive integer. Then Zn consists of the n distinct residue
classes [0], [1], . . . , [n - 1]. Moreover, for every x  R, each residue class modulo
n contains a unique representative in the interval [x, x + n).

   When working with residue classes modulo n, one often has in mind a partic-
ular set of representatives. Typically, one works with the set of representatives
{0, 1, . . . , n - 1}. However, sometimes it is convenient to work with another set
of representatives, such as the representatives in the interval [-n/2, n/2). In this
case, if n is odd, we can list the elements of Zn as

                    [-(n - 1)/2], . . . , [-1], [0], [1], . . . , [(n - 1)/2],

and when n is even, we can list the elements of Zn as
                         [-n/2], . . . , [-1], [0], [1], . . . , [n/2 - 1].

   We can "equip" Zn with binary operations defining addition and multiplication
in a natural way as follows: for a, b  Z, we define

                                     [a] + [b] := [a + b],
                                      [a] · [b] := [a · b].

   Of course, one has to check that this definition is unambiguous, in the sense that
the sum or product of two residue classes should not depend on which particular
26  Congruences

representatives of the classes are chosen in the above definitions. More precisely,
one must check that if [a] = [a ] and [b] = [b ], then [a + b] = [a + b ] and
[a · b] = [a · b ]. However, this property follows immediately from Theorem 2.3.

   Observe that for all a, b, c  Z, we have

                         [a] + [b] = [c]  a + b  c (mod n),

and
                          [a] · [b] = [c]  a · b  c (mod n),

Example 2.7. Consider the residue classes modulo 6. These are as follows:

                             [0] = {. . . , -12, -6, 0, 6, 12, . . .}
                             [1] = {. . . , -11, -5, 1, 7, 13, . . .}
                             [2] = {. . . , -10, -4, 2, 8, 14, . . .}
                             [3] = {. . . , -9, -3, 3, 9, 15, . . .}
                             [4] = {. . . , -8, -2, 4, 10, 16, . . .}
                             [5] = {. . . , -7, -1, 5, 11, 17, . . .} .

Let us write down the addition and multiplication tables for Z6. The addition table
looks like this:

                            + [0] [1] [2] [3] [4] [5]
                           [0] [0] [1] [2] [3] [4] [5]
                           [1] [1] [2] [3] [4] [5] [0]
                           [2] [2] [3] [4] [5] [0] [1]
                           [3] [3] [4] [5] [0] [1] [2]
                           [4] [4] [5] [0] [1] [2] [3]
                           [5] [5] [0] [1] [2] [3] [4] .

The multiplication table looks like this:

                            · [0] [1] [2] [3] [4] [5]
                           [0] [0] [0] [0] [0] [0] [0]
                           [1] [0] [1] [2] [3] [4] [5]
                           [2] [0] [2] [4] [0] [2] [4]
                           [3] [0] [3] [0] [3] [0] [3]
                           [4] [0] [4] [2] [0] [4] [2]
                           [5] [0] [5] [4] [3] [2] [1] .

   Instead of using representatives in the interval [0, 6), we could just as well use
representatives from another interval, such as [-3, 3). Then, instead of naming the
residue classes [0], [1], [2], [3], [4], [5], we would name them [-3], [-2], [-1],
[0], [1], [2]. Observe that [-3] = [3], [-2] = [4], and [-1] = [5]. 2
2.5 Residue classes  27

   These addition and multiplication operations on Zn yield a very natural algebraic
structure. For example, addition and multiplication are commutative and associa-
tive; that is, for all , ,   Zn, we have

                        +  =  + , ( + ) +  =  + ( + ),
                                  = , () = ().

Note that we have adopted here the usual convention of writing  in place of  · .
Furthermore, multiplication distributes over addition; that is, for all , ,   Zn,
we have

                                     ( + ) =  + .

All of these properties follow from the definitions, and the corresponding proper-
ties for Z; for example, the fact that addition in Zn is commutative may be seen as
follows: if  = [a] and  = [b], then

               +  = [a] + [b] = [a + b] = [b + a] = [b] + [a] =  + .

   Because addition and multiplication in Zn are associative, for 1, . . . , k  Zn,
we may write the sum 1 + · · · + k and the product 1 · · · k without any paren-
theses, and there is no ambiguity; moreover, since both addition and multiplication

are commutative, we may rearrange the terms in such sums and products without

changing their values.
   The residue class [0] acts as an additive identity; that is, for all   Zn, we have

 + [0] = ; indeed, if  = [a], then a + 0  a (mod n). Moreover, [0] is the only
element of Zn that acts as an additive identity; indeed, if a + z  a (mod n) holds
for all integers a, then it holds in particular for a = 0, which implies z  0 (mod n).
The residue class [0] also has the property that  · [0] = [0] for all   Zn.

   Every   Zn has an additive inverse, that is, an element   Zn such that
 +  = [0]; indeed, if  = [a], then clearly  := [-a] does the job, since
a + (-a)  0 (mod n). Moreover,  has a unique additive inverse; indeed, if
a + z  0 (mod n), then subtracting a from both sides of this congruence yields
z  -a (mod n). We naturally denote the additive inverse of  by -. Observe
that the additive inverse of - is ; that is -(-) = . Also, we have the identities

       -( + ) = (-) + (-), (-) = -() = (-), (-)(-) = .

For ,   Zn, we naturally write  -  for  + (-).
   The residue class [1] acts as a multiplicative identity; that is, for all   Zn, we

have  ·[1] = ; indeed, if  = [a], then a·1  a (mod n). Moreover, [1] is the only
element of Zn that acts as a multiplicative identity; indeed, if a·z  a (mod n) holds
for all integers a, then in particular, it holds for a = 1, which implies z  1 (mod n).
28  Congruences

   For   Zn, we call   Zn a multiplicative inverse of  if  = [1]. Not
all   Zn have multiplicative inverses. If  = [a] and  = [b], then  is a
multiplicative inverse of  if and only if ab  1 (mod n). Theorem 2.5 implies that
 has a multiplicative inverse if and only if gcd(a, n) = 1, and that if it exists, it is
unique. When it exists, we denote the multiplicative inverse of  by -1.

   We define Zn to be the set of elements of Zn that have a multiplicative inverse.
By the above discussion, we have

                      Zn = {[a] : a = 0, . . . , n - 1, gcd(a, n) = 1}.

If n is prime, then gcd(a, n) = 1 for a = 1, . . . , n-1, and we see that Zn = Zn \{[0]}.
If n is composite, then Zn Zn \ {[0]}; for example, if d | n with 1 < d < n, we
see that [d] is not zero, nor does it belong to Zn. Observe that if ,   Zn, then so
are -1 and ; indeed,

    (-1)-1 =  and ()-1 = -1-1.

For   Zn and   Zn, we naturally write / for -1.
   Suppose , ,  are elements of Zn that satisfy the equation

                                             = .

If   Zn, we may multiply both sides of this equation by -1 to infer that
                                              = .

This is the cancellation law for Zn. We stress the requirement that   Zn, and
not just  = [0]. Indeed, consider any   Zn \ Zn. Then we have  = [a] with
d := gcd(a, n) > 1. Setting  := [n/d] and  := [0], we see that

     =  and  = .

Example 2.8. We list the elements of Z15, and for each   Z15, we also give -1:

       [1] [2] [4] [7] [8] [11] [13] [14]
    -1 [1] [8] [4] [13] [2] [11] [7] [14] . 2

   For 1, . . . , k  Zn, we may naturally write their sum as ik=1 i. By conven-
tion, this sum is [0] when k = 0. It is easy to see that - ik=1 i = ik=1(-i);
that is, the additive inverse of the sum is the sum of the additive inverses. In the
special case where all the i's have the same value , we define k ·  := ik=1 ;
thus, 0 ·  = [0], 1 ·  = , 2 ·  =  + , 3 ·  =  +  + , and so on. The additive
inverse of k · is k · (-), which we may also write as (-k) ·; thus, (-1) · = -,
(-2) ·  = (-) + (-) = -( + ), and so on. Therefore, the notation k · , or

more simply, k, is defined for all integers k. Note that for all integers k and a, we
have k[a] = [ka] = [k][a].
                                   2.5 Residue classes                                29

For all ,   Zn and k,  Z, we have the identities:
     k( ) = (k ) = (k), (k + ) = k + , k( + ) = k + k,
                               (k) = k() = (k).

   Analogously, for 1, . . . , k  Zn, we may write their product as ik=1 i. By
convention, this product is [1] when k = 0. It is easy to see that if all of the i's
belong to Zn, then so does their product, and in particular, ( ik=1 i)-1 = ik=1 i-1;

that is, the multiplicative inverse of the product is the product of the multiplicative

inverses. In the special case where all the i's have the same value , we define

k :=  k    ;  thus,  0  =    [1],  1  =  ,  2  =  ,  3  =  ,  and   so  on.  If       Zn ,
      i=1
then the multiplicative inverse of k is (-1)k, which we may also write as -k;

for example, -2 = -1-1 = ()-1. Therefore, when   Zn, the notation k is

defined for all integers k.

For all ,   Zn and all non-negative integers k and , we have the identities:

              ( )k = k = (k) , k+ = k , ()k = kk.                                   (2.7)

If ,   Zn, the identities in (2.7) hold for all k,  Z.
   For all 1, . . . , k, 1, . . . ,   Zn, the distributive property implies that

              (1 + · · · + k)(1 + · · · +  ) =               i j .

                                                        1ik
                                                        1j

   One last notational convention. As already mentioned, when the modulus n

is clear from context, we usually write [a] instead of [a]n. Although we want to
maintain a clear distinction between integers and their residue classes, occasionally

even the notation [a] is not only redundant, but distracting; in such situations, we
may simply write a instead of [a]. For example, for every   Zn, we have the
identity ( + [1]n)( - [1]n) = 2 - [1]n, which we may write more simply as
( + [1])( - [1]) = 2 - [1], or even more simply, and hopefully more clearly, as
( + 1)( - 1) = 2 - 1. Here, the only reasonable interpretation of the symbol "1"

is [1], and so there can be no confusion.

   In summary, algebraic expressions involving residue classes may be manipulated
in much the same way as expressions involving ordinary numbers. Extra compli-
cations arise only because when n is composite, some non-zero elements of Zn do
not have multiplicative inverses, and the usual cancellation law does not apply for
such elements.

   In general, one has a choice between working with congruences modulo n, or
with the algebraic structure Zn; ultimately, the choice is one of taste and conven-
ience, and it depends on what one prefers to treat as "first class objects": integers
and congruence relations, or elements of Zn.
30  Congruences

   An alternative, and somewhat more concrete, approach to constructing Zn is to
directly define it as the set of n "symbols" [0], [1], . . . , [n - 1], with addition and
multiplication defined as

               [a] + [b] := [(a + b) mod n], [a] · [b] := [(a · b) mod n],

for a, b  {0, . . . , n - 1}. Such a definition is equivalent to the one we have given
here. One should keep this alternative characterization of Zn in mind; however, we
prefer the characterization in terms of residue classes, as it is mathematically more
elegant, and is usually more convenient to work with.

   We close this section with a reinterpretation of the Chinese remainder theorem
(Theorem 2.6) in terms of residue classes.

Theorem 2.8 (Chinese remainder map). Let {ni}ik=1 be a pairwise relatively
prime family of positive integers, and let n := ik=1 ni. Define the map

                                : Zn  Zn1 × · · · × Znk
                                     [a]n  ([a]n1, . . . , [a]nk ).

    (i) The definition of  is unambiguous.

   (ii)  is bijective.
   (iii) For all ,   Zn, if () = (1, . . . , k) and () = (1, . . . , k), then:

             (a) ( + ) = (1 + 1, . . . , k + k);
             (b) (-) = (-1, . . . , -k);
             (c) () = (11, . . . , kk);
             (d)   Zn if and only if i  Zni for i = 1, . . . , k, in which case

                  (-1) = (-1 1 , . . . , -1 k ).

Proof. For (i), note that a  a (mod n) implies a  a (mod ni) for i = 1, . . . , k,
and so the definition of  is unambiguous (it does not depend on the choice of a).

   (ii) follows directly from the statement of the Chinese remainder theorem.
   For (iii), let  = [a]n and  = [b]n, so that for i = 1, . . . , k, we have i = [a]ni
and i = [b]ni. Then we have

   ( + ) = ([a + b]n) = ([a + b]n1, . . . , [a + b]nk ) = (1 + 1, . . . , k + k),
      (-) = ([-a]n) = ([-a]n1, . . . , [-a]nk ) = (-1, . . . , -k), and
      () = ([ab]n) = ([ab]n1, . . . , [ab]nk ) = (11, . . . , kk).
                                         2.6 Euler's phi function                                         31

That proves parts (a), (b), and (c). For part (d), we have

                        Zn  gcd(a, n) = 1
                                 gcd(a, ni) = 1 for i = 1, . . . , k
                                 i  Zni for i = 1, . . . , k.

Moreover, if   Zn and  = -1, then

               (11, . . . , kk) = () = ([1]n) = ([1]n1, . . . , [1]nk ),

and so for i = 1, . . . , k, we have ii = [1]ni, which is to say i = i-1. 2

   Theorem 2.8 is very powerful conceptually, and is an indispensable tool in many
situations. It says that if we want to understand what happens when we add or
multiply ,   Zn, it suffices to understand what happens when we add or multiply
their "components" i, i  Zni. Typically, we choose n1, . . . , nk to be primes or
prime powers, which usually simplifies the analysis. We shall see many applica-
tions of this idea throughout the text.

EXERCISE 2.19. Let  : Zn  Zn1 × · · · × Znk be as in Theorem 2.8, and suppose
that () = (1, . . . , k). Show that for every non-negative integer m, we have

(m)  =  (    m  ,  .  .  .  ,  km  )  .  Moreover,  if      Zn ,  show  that  this  identity  holds  for  all
             1

integers m.

EXERCISE 2.20. Let p be an odd prime. Show that                             -1 =    Zp  = 0.

                                                                        Zp

EXERCISE 2.21. Let p be an odd prime. Show that the numerator of                              p-1
divisible by p.
                                                                                              i=1 1/i is

EXERCISE 2.22. Suppose n is square-free (see Exercise 1.15), and let , ,   Zn.
Show that 2 = 2 implies  = .

                                  2.6 Euler's phi function

Euler's phi function (also called Euler's totient function) is defined for all posi-
tive integers n as

                                         (n) := |Zn|.

Equivalently, (n) is equal to the number of integers between 0 and n - 1 that are
relatively prime to n. For example, (1) = 1, (2) = 1, (3) = 2, and (4) = 2.

   Using the Chinese remainder theorem, more specifically Theorem 2.8, it is easy
to get a nice formula for (n) in terms of the prime factorization of n, as we estab-
lish in the following sequence of theorems.
32       Congruences

Theorem 2.9. Let {ni}ik=1 be a pairwise relatively prime family of positive inte-
gers, and let n := ik=1 ni. Then

                       k

         (n) = (ni).

                      i=1

Proof. Consider the map  : Zn  Zn1 × · · · × Znk in Theorem 2.8. By parts (ii)
and (iii.d) of that theorem, restricting  to Zn yields a one-to-one correspondence
between Zn and Zn1 × · · · × Znk . The theorem now follows immediately. 2

   We already know that (p) = p - 1 for every prime p, since the integers

1, . . . , p - 1 are not divisible by p, and hence are relatively prime to p. The next

theorem generalizes this, giving us a formula for Euler's phi function at prime

powers.

Theorem 2.10. Let p be a prime and e be a positive integer. Then
                                     (pe) = pe-1(p - 1).

Proof. The multiples of p among 0, 1, . . . , pe - 1 are

    0 · p, 1 · p, . . . , (pe-1 - 1) · p,

of which there are precisely pe-1. Thus, (pe) = pe - pe-1 = pe-1(p - 1). 2

   If n = p1e1 · · · prer is the factorization of n into primes, then the family of prime
powers {piei}ir=1 is pairwise relatively prime, and so Theorem 2.9 implies (n) =
(p1e1 ) · · · (prer ). Combining this with Theorem 2.10, we have:

Theorem 2.11. If n = p1e1 · · · prer is the factorization of n into primes, then

    r                 r

    (n) = piei-1(pi - 1) = n (1 - 1/pi).

    i=1               i=1

EXERCISE 2.23. Show that (nm) = gcd(n, m) · (lcm(n, m)).
EXERCISE 2.24. Show that if n is divisible by r distinct odd primes, then 2r | (n).
EXERCISE 2.25. Define 2(n) to be the number of integers a  {0, . . . , n-1} such
that gcd(a, n) = gcd(a + 1, n) = 1. Show that if n = p1e1 · · · prer is the factorization
of n into primes, then 2(n) = n ir=1(1 - 2/pi).

                  2.7 Euler's theorem and Fermat's little theorem
Let n be a positive integer, and let   Zn.
2.7 Euler's theorem and Fermat's little theorem  33

   Consider the sequence of powers of :

                                      1 = 0, 1, 2, . . . .

Since each such power is an element of Zn, and since Zn is a finite set, this sequence
of powers must start to repeat at some point; that is, there must be a positive integer
k such that k = i for some i = 0, . . . , k - 1. Let us assume that k is chosen to be
the smallest such positive integer. This value k is called the multiplicative order
of .

   We claim that k = 1. To see this, suppose by way of contradiction that k = i,
for some i = 1, . . . , k - 1; we could then cancel  from both sides of the equation
k = i, obtaining k-1 = i-1, which would contradict the minimality of k.

   Thus, we can characterize the multiplicative order of  as the smallest positive
integer k such that

                                             k = 1.

If  = [a] with a  Z (and gcd(a, n) = 1, since   Zn), then k is also called
the multiplicative order of a modulo n, and can be characterized as the smallest
positive integer k such that

                                       ak  1 (mod n).

   From the above discussion, we see that the first k powers of , that is, 0, 1,
. . . , k-1, are distinct. Moreover, other powers of  simply repeat this pattern. The
following is an immediate consequence of this observation.

Theorem 2.12. Let n be a positive integer, and let  be an element of Zn of
multiplicative order k. Then for every i  Z, we have i = 1 if and only if
k divides i. More generally, for all i, j  Z, we have i = j if and only if
i  j (mod k).

Example 2.9. Let n = 7. For each value a = 1, . . . , 6, we can compute successive
powers of a modulo n to find its multiplicative order modulo n.

         i123456
1i mod 7 1 1 1 1 1 1
2i mod 7 2 4 1 2 4 1
3i mod 7 3 2 6 4 5 1
4i mod 7 4 2 1 4 2 1
5i mod 7 5 4 6 2 3 1
6i mod 7 6 1 6 1 6 1

So we conclude that modulo 7: 1 has order 1; 6 has order 2; 2 and 4 have order 3;
and 3 and 5 have order 6. 2
34                           Congruences

Theorem 2.13 (Euler's theorem). Let n be a positive integer and   Zn. Then
(n) = 1. In particular, the multiplicative order of  divides (n).

Proof. Since   Zn, for every   Zn we have   Zn, and so we may define the
"multiplication by " map

                                         : Zn  Zn

                                                   .

It is easy to see that  is a bijection:
 Injectivity: If  =  , then cancel  to obtain  =  .

 Surjectivity: For every   Zn, -1 is a pre-image of  under .
Thus, as  ranges over the set Zn, so does , and we have

         = () = (n)                                       .   (2.8)

    Zn                       Zn                       Zn

Canceling the common factor  Zn   Zn from the left- and right-hand side of
(2.8), we obtain

                             1 = (n).

   That proves the first statement of the theorem. The second follows immediately
from Theorem 2.12. 2

    As a consequence of this, we obtain:

Theorem 2.14 (Fermat's little theorem). For every prime p, and every   Zp,
we have p = .
Proof. If  = 0, the statement is obviously true. Otherwise,   Zp, and by
Theorem 2.13 we have p-1 = 1. Multiplying this equation by  yields p = . 2

   In the language of congruences, Fermat's little theorem says that for every prime
p and every integer a, we have

                                        ap  a (mod p).

   For a given positive integer n, we say that a  Z with gcd(a, n) = 1 is a primitive
root modulo n if the multiplicative order of a modulo n is equal to (n). If this is
the case, then for  := [a]  Zn, the powers i range over all elements of Zn as
i ranges over the interval 0, . . . , (n) - 1. Not all positive integers have primitive
roots --we will see in §7.5 that the only positive integers n for which there exists a
primitive root modulo n are

                                      n = 1, 2, 4, pe, 2pe,

where p is an odd prime and e is a positive integer.
2.8 Quadratic residues  35

   The following theorem is sometimes useful in determining the multiplicative
order of an element in Zn.

Theorem 2.15. Suppose   Zn has multiplicative order k. Then for every m  Z,
the multiplicative order of m is k/ gcd(m, k).

Proof. Applying Theorem 2.12 to m, we see that the multiplicative order of m is
the smallest positive integer such that m = 1. But we have

    m = 1  m  0 (mod k) (applying Theorem 2.12 to )

                 0 (mod k/ gcd(m, k)) (by part (ii) of Theorem 2.5). 2

EXERCISE 2.26. Find all elements of Z19  of multiplicative order 18.

EXERCISE 2.27. Let n  Z with n > 1. Show that n is prime if and only if
n-1 = 1 for every non-zero   Zn.
EXERCISE 2.28. Let n = pq, where p and q are distinct primes. Show that if
m := lcm(p - 1, q - 1), then m = 1 for all   Zn.

EXERCISE 2.29. Let p be any prime other than 2 or 5. Show that p divides
infinitely many of the numbers 9, 99, 999, etc.

EXERCISE 2.30. Let n be an integer greater than 1. Show that n does not divide
2n - 1.

EXERCISE 2.31. Prove the following generalization of Fermat's little theorem: for
every positive integer n, and every   Zn, we have n = n-(n).

EXERCISE 2.32. This exercise develops an alternative proof of Fermat's little the-
orem.

    (a) Using Exercise 1.14, show that for all primes p and integers a, we have
         (a + 1)p  ap + 1 (mod p).

   (b) Now derive Fermat's little theorem from part (a).

                                  2.8 Quadratic residues

In §2.3, we studied linear congruences. It is natural to study congruences of higher
degree as well. In this section, we study a special case of this more general prob-
lem, namely, congruences of the form z2  a (mod n). The theory we develop here
nicely illustrates many of the ideas we have discussed earlier, and has a number of
interesting applications as well.
36  Congruences

   We begin with some general, preliminary definitions and general observations
about powers in Zn. For each integer m, we define

                                  (Zn)m := {m :   Zn},

the set of mth powers in Zn. The set (Zn)m is non-empty, as it obviously contains
[1].

Theorem 2.16. Let n be a positive integer, let ,   Zn, and let m be any integer.
    (i) If   (Zn)m, then -1  (Zn)m.
   (ii) If   (Zn)m and   (Zn)m, then   (Zn)m.

   (iii) If   (Zn)m and  / (Zn)m, then  / (Zn)m.

Proof. For (i), if  = m, then -1 = (-1)m.                   Then by (i),
   For (ii), if  = m and  = m, then  = ()m.
   For (iii), suppose that   (Zn)m,  / (Zn)m, and   (Zn)m.

-1  (Zn)m, and by (ii),  = -1()  (Zn)m, a contradiction. 2

Theorem 2.17. Let n be a positive integer. For each   Zn, and all , m  Z with
gcd( , m) = 1, if   (Zn)m, then   (Zn)m.

Proof. Suppose  = m  (Zn)m. Since gcd( , m) = 1, there exist integers s and t
such that s + mt = 1. We then have

     =  s+mt =  smt = msmt = (st)m  (Zn)m. 2

   We now focus on the squares in Zn, rather than general powers. An integer a
is called a quadratic residue modulo n if gcd(a, n) = 1 and a  b2 (mod n) for
some integer b; in this case, we say that b is a square root of a modulo n. In terms
of residue classes, a is a quadratic residue modulo n if and only if [a]  (Zn)2.

   To avoid some annoying technicalities, from now on, we shall consider only the

case where n is odd.

                           2.8.1 Quadratic residues modulo p
We first study quadratic residues modulo an odd prime p, and we begin by deter-
mining the square roots of 1 modulo p.

Theorem 2.18. Let p be an odd prime and   Zp. Then 2 = 1 if and only if
 = ±1.

Proof. Clearly, if  = ±1, then 2 = 1. Conversely, suppose that 2 = 1. Write
 = [b], where b  Z. Then we have b2  1 (mod p), which means that

                                p | (b2 - 1) = (b - 1)(b + 1),
2.8 Quadratic residues                                                 37

and since p is prime, we must have p | (b - 1) or p | (b + 1). This implies
b  ±1 (mod p), or equivalently,  = ±1. 2

   This theorem says that modulo p, the only square roots of 1 are 1 and -1, which
obviously belong to distinct residue classes (since p > 2). From this seemingly
trivial fact, a number of quite interesting and useful results may be derived.

Theorem 2.19. Let p be an odd prime and ,   Zp. Then 2 = 2 if and only if
 = ±.

Proof. This follows from the previous theorem:

            2 = 2  (/)2 = 1  / = ±1   = ±. 2

   This theorem says that if  = 2 for some   Zp, then  has precisely two
square roots:  and -.

Theorem 2.20. Let p be an odd prime. Then |(Zp)2| = (p - 1)/2.

Proof. By the previous theorem, the "squaring map"  : Zp  Zp that sends 
to 2 is a two-to-one map: every element in the image of  has precisely two pre-
images. As a general principle, if we have a function f : A  B, where A is a
finite set and every element in f (A) has exactly d pre-images, then |f (A)| = |A|/d.
Applying this general principle to our setting, we see that the image of  is half the
size of Zp. 2

   Thus, for every odd prime p, exactly half the elements of Zp are squares, and half
are non-squares. If we choose our representatives for the residue classes modulo p
from the interval [-p/2, p/2), we may list the elements of Zp as

                    [-(p - 1)/2], . . . , [-1], [0], [1], . . . , [(p - 1)/2].

We then see that Zp consists of the residue classes

                                   [±1], . . . , [±(p - 1)/2],

and so (Zp)2 consists of the residue classes
                                    [1]2, . . . , [(p - 1)/2]2,

which must be distinct, since we know that |(Zp)2| = (p - 1)/2.
Example 2.10. Let p = 7. We can list the elements of Zp as

                                       [±1], [±2], [±3].

Squaring these, we see that
                      (Zp)2 = {[1]2, [2]2, [3]2} = {[1], [4], [2]}. 2
38                   Congruences

    We next derive an extremely important characterization of quadratic residues.

Theorem 2.21 (Euler's criterion). Let p be an odd prime and   Zp.
    (i) (p-1)/2 = ±1.

   (ii) If   (Zp)2 then (p-1)/2 = 1.
   (iii) If  / (Zp)2 then (p-1)/2 = -1.

Proof. For (i), let  = (p-1)/2. By Euler's theorem (Theorem 2.13), we have

                     2 = p-1 = 1,

and hence by Theorem 2.18, we have  = ±1.
   For (ii), suppose that  = 2. Then again by Euler's theorem, we have

    (p-1)/2 = (2)(p-1)/2 = p-1 = 1.

    For (iii), let   Zp \ (Zp)2. We study the product

                      := .

                             Zp

We shall show that, on the one hand,  = (p-1)/2, while on the other hand,  = -1.

   To show that  = (p-1)/2, we group elements of Zp into pairs of distinct ele-
ments whose product is . More precisely, let P := {S  Zp : |S| = 2}, and
define C := { {, }  P :  = }. Note that for every   Zp, there is a unique
  Zp such that  = , namely,  := /; moreover,  = , since otherwise,
we would have 2 = , contradicting the assumption that  / (Zp)2. Thus, every
element of Zp belongs to exactly one pair in C; in other words, the elements of C
form a partition of Zp. It follows that

    =                ( · ) =         = (p-1)/2.

       {,}C                   {,}C

   To show that  = -1, we group elements of Zp into pairs of distinct elements
whose product is [1]. Define D := { {, }  P :  = 1}. For every   Zp,
there exists a unique   Zp such that  = 1, namely,  := -1; moreover,  = 
if and only if 2 = 1, and by Theorem 2.18, this happens if and only if  = ±1.

Thus, every element of Zp except for [±1] belongs to exactly one pair in D; in
other words, the elements of D form a partition of Zp \ {[±1]}. It follows that

     = [1] · [-1] ·  ( · ) = [-1] ·                    [1] = -1. 2

       {,}D                         {,}D
                    2.8 Quadratic residues                                  39

Thus, Euler's criterion says that for every   Zp, we have (p-1)/2 = ±1 and

                      (Zp)2  (p-1)/2 = 1.

In the course of proving Euler's criterion, we proved the following result, which
we state here for completeness:

Theorem 2.22 (Wilson's theorem). Let p be an odd prime. Then Zp  = -1.
   In the language of congruences, Wilson's theorem may be stated as follows:

                    (p - 1)!  -1 (mod p).

We also derive the following simple consequence of Theorem 2.21:

Theorem 2.23. Let p be an odd prime and ,   Zp. If  / (Zp)2 and  / (Zp)2,
then   (Zp)2.

Proof. Suppose  / (Zp)2 and  / (Zp)2. Then by Euler's criterion, we have

                            (p-1)/2 = -1 and (p-1)/2 = -1.

Therefore,

            ()(p-1)/2 = (p-1)/2 · (p-1)/2 = [-1] · [-1] = 1,

which again by Euler's criterion implies that   (Zp)2. 2

   This theorem, together with parts (ii) and (iii) of Theorem 2.16, gives us the
following simple rules regarding squares in Zp:

            square  × square  = square,

            square  × non-square = non-square,

            non-square × non-square = square.

                           2.8.2 Quadratic residues modulo pe
We next study quadratic residues modulo pe, where p is an odd prime. The key is
to establish the analog of Theorem 2.18:

Theorem 2.24. Let p be an odd prime, e be a positive integer, and   Zpe. Then
2 = 1 if and only if  = ±1.

Proof. Clearly, if  = ±1, then 2 = 1. Conversely, suppose that 2 = 1. Write
 = [b], where b  Z. Then we have b2  1 (mod pe), which means that

                               pe | (b2 - 1) = (b - 1)(b + 1).
40  Congruences

In particular, p | (b - 1)(b + 1), and so p | (b - 1) or p | (b + 1). Moreover, p
cannot divide both b - 1 and b + 1, as otherwise, it would divide their difference
(b + 1) - (b - 1) = 2, which is impossible (because p is odd). It follows that
pe | (b - 1) or pe | (b + 1), which means  = ±1. 2

   Theorems 2.19-2.23 generalize immediately from Zp to Zpe  : we really used
nothing in the proofs of these theorems other than the fact that ±1 are the only

square roots of 1 modulo p. As such, we state the analogs of these theorems for
Zpe  without proof.

Theorem 2.25. Let p be an odd prime, e be a positive integer, and ,   Zpe  .
Then 2 = 2 if and only if  = ±.

Theorem 2.26. Let p be an odd prime and e be a positive integer. Then we have
|(Zpe  )2| = (pe)/2.

Theorem 2.27. Let p be an odd prime, e be a positive integer, and   Zpe  .
    (i) (pe)/2 = ±1.
   (ii) If   (Zpe )2 then (pe)/2 = 1.

   (iii) If  / (Zpe )2 then (pe)/2 = -1.

Theorem 2.28. Let p be an odd prime and e be a positive integer. Then we have
   Ze  = -1.

            p

Theorem 2.29. Let p be an odd prime, e be a positive integer, and ,   Zpe  . If
 / (Zpe )2 and  / (Zpe )2, then   (Zpe )2.

   It turns out that an integer is a quadratic residue modulo pe if and only if it is a
quadratic residue modulo p.

Theorem 2.30. Let p be an odd prime, e be a positive integer, and a be any integer.
Then a is a quadratic residue modulo pe if and only if a is a quadratic residue

modulo p.

Proof. Suppose that a is a quadratic residue modulo pe. Then a is not divisible by
p and a  b2 (mod pe) for some integer b. It follows that a  b2 (mod p), and so a
is a quadratic residue modulo p.

   Suppose that a is not a quadratic residue modulo pe. If a is divisible by p, then
by definition a is not a quadratic residue modulo p. So suppose a is not divisible
by p. By Theorem 2.27, we have

                                 ape-1(p-1)/2  -1 (mod pe).

This congruence holds modulo p as well, and by Fermat's little theorem (applied
               2.8 Quadratic residues                                         41

e - 1 times),

               a  ap  ap2  · · ·  ape-1 (mod p),

and so

               -1  ape-1(p-1)/2  a(p-1)/2 (mod p).

Theorem 2.21 therefore implies that a is not a quadratic residue modulo p. 2

                           2.8.3 Quadratic residues modulo n

We now study quadratic residues modulo n, where n is an arbitrary, odd integer,
with n > 1. Let

                                         n = p1e1 · · · prer

be the prime factorization of n. Our main tools here are the Chinese remainder map

                                  : Zn  Zpe1 × · · · × Zperr ,

                                                                                       1

introduced in Theorem 2.8, together with the results developed so far for quadratic
residues modulo odd prime powers.

   Let   Zn with () = (1, . . . , r).
     · On the one hand, suppose  = 2 for some   Zn. If () = (1, . . . , r),
         we have
                            (1, . . . , r) = () = (2) = (12, . . . , r2),
         where we have used part (iii.c) of Theorem 2.8. It follows that i = i2 for
         each i.
     · On the other hand, suppose that for each i, i = i2 for some i  Zpiei  .
         Then setting  := -1(1, . . . , r), we have

                           (2) = (12, . . . , r2) = (1, . . . , r) = (),

         where we have again used part (iii.c) of Theorem 2.8, along with the fact
         that  is bijective (to define ). Thus, () = (2), and again since  is
         bijective, it follows that  = 2.

We have shown that

                        (Zn)2  i  Zpei  2 for i = 1, . . . , r. i

In particular, restricting  to (Zn)2 yields a one-to-one correspondence between
(Zn)2 and

                                    Zpe1  2 × · · · × Z 1 prer  2,
42  Congruences

and therefore, by Theorem 2.26 (and Theorem 2.9), we have

                                                         r

                           |(Zn)2| = ((piei )/2) = (n)/2r.

                                                       i=1

   Now suppose that  = 2, with   Zn and () = (1, . . . , r). Consider an
arbitrary element   Zn, with () = (1, . . . , r). Then we have

          2 = 2  (2) = (2)
                      (12, . . . , r2) = (12, . . . , r2)
                      (1, . . . , r) = (±1, . . . , ±r) (by Theorem 2.25).

Therefore,  has precisely 2r square roots, namely, -1(±1, . . . , ±r).

                           2.8.4 Square roots of -1 modulo p
Using Euler's criterion, we can easily characterize those primes modulo which -1
is a quadratic residue. This turns out to have a number of nice applications.

   Consider an odd prime p. The following theorem says that the question of
whether -1 is a quadratic residue modulo p is decided by the residue class of p
modulo 4. Since p is odd, either p  1 (mod 4) or p  3 (mod 4).

Theorem 2.31. Let p be an odd prime. Then -1 is a quadratic residue modulo p
if and only p  1 (mod 4).

Proof. By Euler's criterion, -1 is a quadratic residue modulo p if and only if
(-1)(p-1)/2  1 (mod p). If p  1 (mod 4), then (p - 1)/2 is even, and so
(-1)(p-1)/2 = 1. If p  3 (mod 4), then (p-1)/2 is odd, and so (-1)(p-1)/2 = -1. 2

   In fact, when p  1 (mod 4), any non-square in Zp yields a square root of -1
modulo p, as follows:

Theorem 2.32. Let p be a prime with p  1 (mod 4),   Zp \ (Zp)2, and
 := (p-1)/4. Then 2 = -1.

Proof. This is a simple calculation, based on Euler's criterion:

                                   2 = (p-1)/2 = -1. 2

   The fact that -1 is a quadratic residue modulo primes p  1 (mod 4) can be
used to prove Fermat's theorem that such primes may be written as the sum of two
squares. To do this, we first need the following technical lemma:
                 2.8 Quadratic residues                                            43

Theorem 2.33 (Thue's lemma). Let n, b, r, t  Z, with 0 < r  n < rt.
Then there exist r, t  Z with

                      r  bt (mod n), |r| < r, and 0 < |t| < t.

Proof. For i = 0, . . . , r -1 and j = 0, . . . , t -1, we define the number vij := i-bj.
Since we have defined rt numbers, and rt > n, two of these numbers must lie
in the same residue class modulo n; that is, for some (i1, j1) = (i2, j2), we have
vi1j1  vi2j2 (mod n). Setting r := i1-i2 and t := j1-j2, this implies r  bt (mod n),
|r| < r, |t| < t, and that either r = 0 or t = 0. It only remains to show that t = 0.
Suppose to the contrary that t = 0. This would imply that r  0 (mod n) and r = 0,

which is to say that r is a non-zero multiple of n; however, this is impossible, since
|r| < r  n. 2

Theorem 2.34 (Fermat's two squares theorem). Let p be an odd prime. Then
p = r2 + t2 for some r, t  Z if and only if p  1 (mod 4).

Proof. One direction is easy. Suppose p  3 (mod 4). It is easy to see that the

square of every integer is congruent to either 0 or 1 modulo 4; therefore, the sum of

two squares is congruent to either 0, 1, or 2 modulo 4, and so can not be congruent

to p modulo 4 (let alone equal to p).

For the other direction, suppose p  1 (mod 4). We know that -1 is a quadratic

residue modulo p, so let b be an integer such that b2  -1 (mod p). Now apply
Theorem 2.33 with n := p, b as just defined, and r := t := p + 1. Evidently,
 p + 1 > p, and hence rt > p. Also, since p is prime, p is not an integer,
and so p < p < p; in particular, r = p + 1  p. Thus, the hypotheses of

that theorem are satisfied, and therefore, there exist integers r and t such that

                                         
r  bt (mod p), |r|  p < p, and 0 < |t|  p < p.

It follows that

                 r2  b2t2  -t2 (mod p).

Thus, r2 + t2 is a multiple of p and 0 < r2 + t2 < 2p. The only possibility is that
r2 + t2 = p. 2

   The fact that -1 is a quadratic residue modulo an odd prime p only if p 
1 (mod 4) can be used so show there are infinitely many such primes.

Theorem 2.35. There are infinitely many primes p  1 (mod 4).

Proof. Suppose there were only finitely many such primes, p1, . . . , pk. Set M :=
   ik=1 pi and N := 4M2 + 1. Let p be any prime dividing N. Evidently, p

is not among the pi's, since if it were, it would divide both N and 4M2, and
44  Congruences

so also N - 4M2 = 1. Also, p is clearly odd, since N is odd. Moreover,
(2M)2  -1 (mod p); therefore, -1 is a quadratic residue modulo p, and so

p  1 (mod 4), contradicting the assumption that p1, . . . , pk are the only such
primes. 2

    For completeness, we also state the following fact:

Theorem 2.36. There are infinitely many primes p  3 (mod 4).

Proof. Suppose there were only finitely many such primes, p1, . . . , pk. Set M :=
   ik=1 pi and N := 4M - 1. Since N  3 (mod 4), there must be some prime

p  3 (mod 4) dividing N (if all primes dividing N were congruent to 1 modulo 4,

then so too would be their product N). Evidently, p is not among the pi's, since if
it were, it would divide both N and 4M, and so also 4M -N = 1. This contradicts
the assumption that p1, . . . , pk are the only primes congruent to 3 modulo 4. 2

EXERCISE 2.33. Let n, m  Z, where n > 0, and let d := gcd(m, (n)). Show
that:

    (a) if d = 1, then (Zn)m = (Zn);
   (b) if   (Zn)m, then (n)/d = 1.

EXERCISE 2.34. Calculate the sets C and D in the proof of Theorem 2.21 in the
case p = 11 and  = -1.

EXERCISE 2.35. Calculate the square roots of 1 modulo 4, 8, and 16.

EXERCISE 2.36. Let n  Z with n > 1. Show that n is prime if and only if
(n - 1)!  -1 (mod n).

EXERCISE 2.37. Let p be a prime with p  1 (mod 4), and b := ((p - 1)/2)!.
Show that b2  -1 (mod p).

EXERCISE 2.38. Let n := pq, where p and q are distinct, odd primes. Show that
there exist ,   Zn such that  / (Zn)2,  / (Zn)2, and  / (Zn)2.

EXERCISE 2.39. Let n be an odd positive integer, and let a be any integer. Show
that a is a quadratic residue modulo n if and only if a is a quadratic residue modulo
p for each prime p | n.

EXERCISE 2.40. Show that if p is an odd prime, with p  3 (mod 4), then
(Zp)4 = (Zp)2. More generally, show that if n is an odd positive integer, where
p  3 (mod 4) for each prime p | n, then (Zn)4 = (Zn)2.

EXERCISE 2.41. Let p be an odd prime, and let e  Z with e > 1. Let a be an
               2.9 Summations over divisors                                45

integer of the form a = pf b, where 0  f < e and p b. Consider the integer
solutions z to the congruence z2  a (mod pe). Show that a solution exists if and

only if f is even and b is a quadratic residue modulo p, in which case there are
exactly 2pf distinct solutions modulo pe.

EXERCISE 2.42. Suppose p is an odd prime, and that r2 + t2 = p for some integers
r, t. Show that if x, y are integers such that x2 +y2 = p, then (x, y) must be (±r, ±t)

or (±t, ±r).

EXERCISE 2.43. Show that if both u and v are the sum of two squares of integers,
then so is their product uv.

EXERCISE 2.44. Suppose r2 + t2  0 (mod n), where n is a positive integer, and
suppose p is an odd prime dividing n. Show that:

    (a) if p divides neither r nor t, then p  1 (mod 4);
   (b) if p divides one of r or t, then it divides the other, and moreover, p2 divides

         n, and (r/p)2 + (t/p)2  0 (mod n/p2).

EXERCISE 2.45. Let n be a positive integer, and write n = ab2 where a and b are
positive integers, and a is square-free (see Exercise 1.15). Show that n is the sum
of two squares of integers if and only if no prime p  3 (mod 4) divides a. Hint:
use the previous two exercises.

               2.9 Summations over divisors

We close this chapter with a brief treatment of summations over divisors. To this
end, we introduce some terminology and notation. By an arithmetic function,
we simply mean a function from the positive integers into the reals (actually, one
usually considers complex-valued functions as well, but we shall not do so here).
Let f and g be arithmetic functions. The Dirichlet product of f and g, denoted
f g, is the arithmetic function whose value at n is defined by the formula

               (f g)(n) := f (d)g(n/d),

                                       d|n

the sum being over all positive divisors d of n. Another, more symmetric, way to

write this is

               (f g)(n) =          f (d1)g(d2),

                           n=d1d2

the sum being over all pairs (d1, d2) of positive integers with d1d2 = n.
46                               Congruences

   The Dirichlet product is clearly commutative (i.e., f g = g f ), and is asso-
ciative as well, which one can see by checking that

    (f (g h))(n) =               f (d1)g(d2)h(d3) = ((f g) h)(n),

                       n=d1d2d3

the sum being over all triples (d1, d2, d3) of positive integers with d1d2d3 = n.
   We now introduce three special arithmetic functions: I, 1, and µ. The functions

I and 1 are defined as follows:

    I (n) :=           1 if n = 1;            1(n) := 1.
                       0 if n > 1;

The Möbius function µ is defined as follows: if n = p1e1 · · · prer is the prime factor-
ization of n, then

    µ(n) :=         0            if ei > 1 for some i = 1, . . . , r;

                    (-1)r otherwise.

In other words, µ(n) = 0 if n is not square-free (see Exercise 1.15); otherwise,
µ(n) is (-1)r where r is the number of distinct primes dividing n. Here are some

examples:

    µ(1) = 1, µ(2) = -1, µ(3) = -1, µ(4) = 0, µ(5) = -1, µ(6) = 1.

    It is easy to see from the definitions that for every arithmetic function f , we have
                           I f = f and (1 f )(n) = f (d).

                                                                                     d|n

Thus, I acts as a multiplicative identity with respect to the Dirichlet product, while
"1 " acts as a "summation over divisors" operator.

   An arithmetic function f is called multiplicative if f (1) = 1 and for all positive
integers n, m with gcd(n, m) = 1, we have f (nm) = f (n)f (m).

   The reader may easily verify that I, 1, and µ are multiplicative functions. Theo-
rem 2.9 says that Euler's function  is multiplicative. The reader may also verify
the following:

Theorem 2.37. If f is a multiplicative arithmetic function, and if n = p1e1 · · · prer
is the prime factorization of n, then f (n) = f (p1e1 ) · · · f (prer ).

Proof. Exercise. 2

    A key property of the Möbius function is the following:
2.9 Summations over divisors                                       47

Theorem 2.38. Let f be a multiplicative arithmetic function. If n = p1e1 · · · prer is
the prime factorization of n, then

   µ(d)f (d) = (1 - f (p1)) · · · (1 - f (pr)).                    (2.9)

d|n

Proof. The only non-zero terms appearing in the sum on the left-hand side of (2.9)

are those corresponding to divisors d of the form pi1 · · · pi , where pi1, . . . , pi are
distinct; the value contributed to the sum by such a term is (-1) f (pi1 · · · pi ) =
(-1) f (pi1 ) · · · f (pi ). These are the same as the terms in the expansion of the
product on the right-hand side of (2.9). 2

If we set f := 1 in the previous theorem, then we see that

   µ(d) =  1 if n = 1;
           0 if n > 1.
d|n

Translating this into the language of Dirichlet products, we have

1 µ = I.

Thus, with respect to the Dirichlet product, the functions 1 and µ are multiplicative
inverses of one another. Based on this, we may easily derive the following:

Theorem 2.39 (Möbius inversion formula). Let f and F be arithmetic functions.
Then F = 1 f if and only if f = µ F .

Proof. If F = 1 f , then
                    µ F = µ (1 f ) = (µ 1) f = I f = f,

and conversely, if f = µ F , then
                  1 f = 1 (µ F ) = (1 µ) F = I F = F . 2

   The Möbius inversion formula says this:
                       F (n) = f (d) for all positive integers n

                                              d|n

                f (n) = µ(d)F (n/d) for all positive integers n.

                                             d|n

   The Möbius inversion formula is a useful tool. As an application, we use it to
obtain a simple proof of the following fact:

Theorem 2.40. For every positive integer n, we have d|n (d) = n.
48                              Congruences

Proof. Let us define the arithmetic functions N (n) := n and M (n) := 1/n. Our

goal is to show that N = 1 , and by Möbius inversion, it suffices to show that
µ N = . If n = p1e1 · · · prer is the prime factorization of n, we have

    (µ N)(n) = µ(d)(n/d) = n µ(d)/d

    d|n                         d|n

            r

    = n (1 - 1/pi) (applying Theorem 2.38 with f := M)

           i=1

    = (n) (by Theorem 2.11). 2

EXERCISE 2.46. In our definition of a multiplicative function f , we made the
requirement that f (1) = 1. Show that if we dropped this requirement, the only
other function that would satisfy the definition would be the zero function (i.e., the
function that is everywhere zero).

EXERCISE 2.47. Let f be a polynomial with integer coefficients, and for each
positive integer n, define f (n) to be the number of integers x  {0, . . . , n - 1}
such that f (x)  0 (mod n). Show that f is multiplicative.

EXERCISE 2.48. Show that if f and g are multiplicative, then so is f g. Hint:
use Exercise 1.18.

EXERCISE 2.49. Let (n) be the number of positive divisors of n. Show that:

   (a)  is a multiplicative function;
   (b) (n) = ir=1(ei + 1), where n = p1e1 · · · prer is the prime factorization of n;
   (c) d|n µ(d)(n/d) = 1;
   (d) d|n µ(d)(d) = (-1)r, where n = p1e1 · · · prer is the prime factorization of n.

EXERCISE 2.50. Define (n) := d|n d. Show that:

   (a)  is a multiplicative function;
   (b) (n) = ir=1(piei+1 - 1)/(pi - 1), where n = p1e1 · · · prer is the prime factor-

         ization of n;

   (c) d|n µ(d)(n/d) = n;
   (d) d|n µ(d)(d) = (-1)rp1 · · · pr, where n = p1e1 · · · prer is the prime factor-

         ization of n.

EXERCISE 2.51. The Mangoldt function (n) is defined for all positive integers

n as follows: (n) := log p, if n = pk for some prime p and positive integer k, and

(n) := 0, otherwise. Show that  d|n (d) = log n, and from this, deduce that
(n) = - d|n µ(d) log d.
2.9 Summations over divisors                                                49

EXERCISE 2.52. Show that if f is multiplicative, and if n = p1e1 · · · prer is the prime
factorization of n, then d|n µ(d)2f (d) = (1 + f (p1)) · · · (1 + f (pr)).

EXERCISE 2.53. Show that n is square-free if and only if d|n µ(d)2(d) = n.

EXERCISE 2.54. Show that for every arithmetic function f with f (1) = 0, there

is a unique arithmetic function g, called the Dirichlet inverse of f, such that
f g = I. Also, show that if f (1) = 0, then f has no Dirichlet inverse.

EXERCISE 2.55. Show that if f is a multiplicative function, then so is its Dirichlet
inverse (as defined in the previous exercise).

EXERCISE 2.56. This exercise develops an alternative proof of Theorem 2.40 that
does not depend on Theorem 2.11. Let n be a positive integer. Define

                            Fn := {i/n  Q : i = 0, . . . , n - 1}.
Also, for each positive integer d, define

                        Gd := {a/d  Q : a  Z, gcd(a, d) = 1}.
    (a) Show that for each x  Fn, there exists a unique positive divisor d of n such

         that x  Gd.
   (b) Show that for each positive divisor d of n, we have

                       Fn  Gd = {a/d : a = 0, . . . , d - 1, gcd(a, d) = 1}.

    (c) Using (a) and (b), show that d|n (d) = n.

EXERCISE 2.57. Using Möbius inversion, directly derive Theorem 2.11 from The-
orem 2.40.
                             3

               Computing with large integers

In this chapter, we review standard asymptotic notation, introduce the formal com-
putational model that we shall use throughout the rest of the text, and discuss basic
algorithms for computing with large integers.

                                 3.1 Asymptotic notation
We review some standard notation for relating the rate of growth of functions.
This notation will be useful in discussing the running times of algorithms, and in a
number of other contexts as well.

   Let f and g be real-valued functions. We shall assume that each is defined on
the set of non-negative integers, or, alternatively, that each is defined on the set
of non-negative reals. Actually, as we are only concerned about the behavior of
f (x) and g(x) as x  , we only require that f (x) and g(x) are defined for all
sufficiently large x (the phrase "for all sufficiently large x" means "for some x0
and all x  x0"). We further assume that g is eventually positive, meaning that
g(x) > 0 for all sufficiently large x. Then

     · f = O(g) means that |f (x)|  cg(x) for some positive constant c and all
         sufficiently large x (read, "f is big-O of g"),

     · f = (g) means that f (x)  cg(x) for some positive constant c and all
         sufficiently large x (read, "f is big-Omega of g"),

     · f = (g) means that cg(x)  f (x)  dg(x) for some positive constants c
         and d and all sufficiently large x (read, "f is big-Theta of g"),

     · f = o(g) means that f (x)/g(x)  0 as x   (read, "f is little-o of g"),
         and

     · f  g means that f (x)/g(x)  1 as x   (read, "f is asymptotically
         equal to g").

                                                50
3.1 Asymptotic notation                                                 51

Example 3.1. Let f (x) := x2 and g(x) := 2x2 - 10x + 1. Then f = O(g) and
f = (g). Indeed, f = (g). 2

Example 3.2. Let f (x) := x2 and g(x) := x2 - 10x + 1. Then f  g. 2

Example 3.3. Let f (x) := 100x2 and g(x) := x3. Then f = o(g). 2

   Note that by definition, if we write f = (g), f = (g), or f  g, it must be the
case that f (in addition to g) is eventually positive; however, if we write f = O(g)
or f = o(g), then f need not be eventually positive.

   When one writes "f = O(g)," one should interpret "· = O(·)" as a binary rela-
tion between f with g. Analogously for "f = (g)," "f = (g)," and "f = o(g)."

   One may also write "O(g)" in an expression to denote an anonymous function
f such that f = O(g). Analogously, (g), (g), and o(g) may denote anonymous

functions. The expression O(1) denotes a function bounded in absolute value by

a constant, while the expression o(1) denotes a function that tends to zero in the

limit.

Example 3.4. Let f (x) := x3 - 2x2 + x - 3. One could write f (x) = x3 + O(x2).
Here, the anonymous function is g(x) := -2x2 + x - 3, and clearly g(x) = O(x2).
One could also write f (x) = x3 - (2 + o(1))x2. Here, the anonymous function
is g(x) := -1/x + 3/x2. While g = o(1), it is only defined for x > 0. This

is acceptable, since we will only regard statements such as this asymptotically, as

x  . 2

   As an even further use (abuse?) of the notation, one may use the big-O, big-
Omega, and big-Theta notation for functions on an arbitrary domain, in which case
the relevant inequalities should hold throughout the entire domain. This usage
includes functions of several independent variables, as well as functions defined
on sets with no natural ordering.

EXERCISE 3.1. Show that:
   (a) f = o(g) implies f = O(g) and g = O(f );
   (b) f = O(g) and g = O(h) implies f = O(h);
   (c) f = O(g) and g = o(h) implies f = o(h);
   (d) f = o(g) and g = O(h) implies f = o(h).

EXERCISE 3.2. Let f and g be eventually positive functions. Show that:
   (a) f  g if and only if f = (1 + o(1))g;
   (b) f  g implies f = (g);
   (c) f = (g) if and only if f = O(g) and f = (g);
52  Computing with large integers

    (d) f = (g) if and only if g = O(f ).

EXERCISE 3.3. Suppose f1 = O(g1) and f2 = O(g2). Show that f1 + f2 =
O(max(g1, g2)), f1f2 = O(g1g2), and that for every constant c, cf1 = O(g1).

EXERCISE 3.4. Suppose that f (x)  c + dg(x) for some positive constants c and
d, and for all sufficiently large x. Show that if g = (1), then f = O(g).

EXERCISE 3.5. Suppose f and g are defined on the integers i  k, and that
g(i) > 0 for all i  k. Show that if f = O(g), then there exists a positive constant

c such that |f (i)|  cg(i) for all i  k.

EXERCISE 3.6. Let f and g be eventually positive functions, and assume that
f (x)/g(x) tends to a limit L (possibly L = ) as x  . Show that:

    (a) if L = 0, then f = o(g);
   (b) if 0 < L < , then f = (g);
    (c) if L = , then g = o(f ).

EXERCISE 3.7. Let f (x) := x (log x) and g(x) := x (log x), where , , , 
are non-negative constants. Show that if  < , or if  =  and  < , then
f = o(g).

EXERCISE 3.8. Order the following functions in x so that for each adjacent pair
f , g in the ordering, we have f = O(g), and indicate if f = o(g), f  g, or
g = O(f ):

        x3, exx2, 1/x, x2(x + 100) + 1/x, x + x, log2 x, log3 x, 2x2, x,

                                                              

               e-x, 2x2 - 10x + 4, ex+ x, 2x, 3x, x-2, x2(log x)1000.

EXERCISE 3.9. Show that:

   (a) the relation "" is an equivalence relation on the set of eventually positive
         functions;

   (b) for all eventually positive functions f1, f2, g1, g2, if f1  g1 and f2  g2,
         then f1 f2  g1 g2, where " " denotes addition, multiplication, or
         division;

   (c) for all eventually positive functions f , g, and every  > 0, if f  g, then
         f  g;

   (d) for all eventually positive functions f, g, and every function h such that
         h(x)   as x  , if f  g, then f  h  g  h, where "" denotes
         function composition.

EXERCISE 3.10. Show that all of the claims in the previous exercise also hold
when the relation "" is replaced with the relation "· = (·)."
                      3.2 Machine models and complexity theory                          53

EXERCISE 3.11. Let f , g be eventually positive functions. Show that:
   (a) f = (g) if and only if log f = log g + O(1);
   (b) f  g if and only if log f = log g + o(1).

EXERCISE 3.12. Suppose that f and g are functions defined on the integers
k, k + 1, . . . , and that g is eventually positive. For n  k, define F (n) := in=k f (i)
and G(n) := in=k g(i). Show that if f = O(g) and G is eventually positive, then
F = O(G).

EXERCISE 3.13. Suppose that f and g are piece-wise continuous on [a, ) (see
§A4), and that g is eventually positive. For x  a, define F (x) := ax f (t) dt and
G(x) := ax g(t) dt. Show that if f = O(g) and G is eventually positive, then
F = O(G).

EXERCISE 3.14. Suppose that f and g are functions defined on the integers

k, k + 1, . . . , and that both f and g are eventually positive. For n  k, define

F (n) :=  n    f (i)  and  G(n)  :=  n    g(i).  Show  that  if  f    g  and  G(n)      as
          i=k                        i=k

n  , then F  G.

EXERCISE 3.15. Suppose that f and g are piece-wise continuous on [a, ) (see

§A4), and that both f and g are eventually positive. For x  a, define F (x) :=
 ax f (t) dt and G(x) := ax g(t) dt. Show that if f  g and G(x)   as x  ,
then F  G.

EXERCISE 3.16. Give an example of two non-decreasing functions f and g, each
mapping positive integers to positive integers, such that f = O(g) and g = O(f ).

                     3.2 Machine models and complexity theory

When presenting an algorithm, we shall always use a high-level, and somewhat
informal, notation. However, all of our high-level descriptions can be routinely
translated into the machine-language of an actual computer. So that our theorems
on the running times of algorithms have a precise mathematical meaning, we for-
mally define an "idealized" computer: the random access machine or RAM.

   A RAM consists of an unbounded sequence of memory cells

                                     m[0], m[1], m[2], . . . ,

each of which can store an arbitrary integer, together with a program. A program
consists of a finite sequence of instructions I0, I1, . . . , where each instruction is of
one of the following types:
54  Computing with large integers

arithmetic This type of instruction is of the form    , where represents one
         of the operations addition, subtraction, multiplication, or integer division
          (i.e., ·/· ). The values  and  are of the form c, m[a], or m[m[a]], and
          is of the form m[a] or m[m[a]], where c is an integer constant and a is a
         non-negative integer constant. Execution of this type of instruction causes
         the value   to be evaluated and then stored in .

branching This type of instruction is of the form IF  3  GOTO i, where i is
         the index of an instruction, and where 3 is one of the comparison opera-
         tions =, =, <, >, , , and  and  are as above. Execution of this type of
         instruction causes the "flow of control" to pass conditionally to instruction
         Ii.

halt The HALT instruction halts the execution of the program.

   A RAM works by executing instruction I0, and continues to execute instruc-
tions, following branching instructions as appropriate, until a HALT instruction is
reached.

   We do not specify input or output instructions, and instead assume that the input
and output are to be found in memory cells at some prescribed locations, in some
standardized format.

   To determine the running time of a program on a given input, we charge 1 unit
of time to each instruction executed.

   This model of computation closely resembles a typical modern-day computer,
except that we have abstracted away many annoying details. However, there are
two details of real machines that cannot be ignored; namely, any real machine has
a finite number of memory cells, and each cell can store numbers only in some
fixed range.

   The first limitation must be dealt with by either purchasing sufficient memory or
designing more space-efficient algorithms.

   The second limitation is especially annoying, as we will want to perform compu-
tations with quite large integers -- much larger than will fit into any single memory
cell of an actual machine. To deal with this limitation, we shall represent such large
integers as vectors of digits in some fixed base, so that each digit is bounded in
order to fit into a memory cell. This is discussed in more detail in the next section.
The only other numbers we actually need to store in memory cells are "small"
numbers representing array indices, counters, and the like, which we hope will fit
into the memory cells of actual machines. Below, we shall make a more precise,
formal restriction on the magnitude of numbers that may be stored in memory cells.

   Even with these caveats and restrictions, the running time as we have defined
it for a RAM is still only a rough predictor of performance on an actual machine.
On a real machine, different instructions may take significantly different amounts
3.3 Basic integer arithmetic  55

of time to execute; for example, a division instruction may take much longer than
an addition instruction. Also, on a real machine, the behavior of the cache may
significantly affect the time it takes to load or store the operands of an instruction.
Finally, the precise running time of an algorithm given by a high-level description
will depend on the quality of the translation of this algorithm into "machine code."
However, despite all of these problems, it still turns out that measuring the running
time on a RAM as we propose here is a good "first order" predictor of performance
on real machines in many cases. Also, we shall only state the running time of an
algorithm using a big-O estimate, so that implementation-specific constant factors
are anyway "swept under the rug."

   If we have an algorithm for solving a certain problem, we expect that "larger"
instances of the problem will require more time to solve than "smaller" instances,
and a general goal in the analysis of any algorithm is to estimate the rate of growth
of the running time of the algorithm as a function of the size of its input. For this
purpose, we shall simply measure the size of an input as the number of memory
cells used to represent it. Theoretical computer scientists sometimes equate the
notion of "efficient" with "polynomial time" (although not everyone takes theo-
retical computer scientists very seriously, especially on this point): a polynomial-
time algorithm is one whose running time on inputs of size n is at most anb + c,
for some constants a, b, and c (a "real" theoretical computer scientist will write
this as nO(1)). Furthermore, we also require that for a polynomial-time algorithm,
all numbers stored in memory are at most a nb + c in absolute value, for some
constants a , b , and c . Even for algorithms that are not polynomial time, we shall
insist that after executing t instructions, all numbers stored in memory are at most
a (n + t)b + c in absolute value, for some constants a , b , and c .

   Note that in defining the notion of polynomial time on a RAM, it is essential
that we restrict the magnitude of numbers that may be stored in the machine's
memory cells, as we have done above. Without this restriction, a program could
perform arithmetic on huge numbers, being charged just one unit of time for each
arithmetic operation-- not only is this intuitively "wrong," it is possible to come up
with programs that solve some problems using a polynomial number of arithmetic
operations on huge numbers, and these problems cannot otherwise be solved in
polynomial time (see §3.6).

                               3.3 Basic integer arithmetic

We will need algorithms for performing arithmetic on very large integers. Since
such integers will exceed the word-size of actual machines, and to satisfy the for-
mal requirements of our random access model of computation, we shall represent
56  Computing with large integers

large integers as vectors of digits in some base B, along with a bit indicating the
sign. That is, for a  Z, if we write

                                                k-1

                           a = ± aiBi = ±(ak-1 · · · a1a0)B,

                                                 i=0

where 0  ai < B for i = 0, . . . , k - 1, then a will be represented in memory as
a data structure consisting of the vector of base-B digits a0, . . . , ak-1, along with
a "sign bit" to indicate the sign of a. To ensure a unique representation, if a is
non-zero, then the high-order digit ak-1 in this representation should be non-zero.

   For our purposes, we shall consider B to be a constant, and moreover, a power of
2. The choice of B as a power of 2 is convenient for a number of technical reasons.

   A note to the reader: If you are not interested in the low-level details of algo-
rithms for integer arithmetic, or are willing to take them on faith, you may safely
skip ahead to §3.3.5, where the results of this section are summarized.

   We now discuss in detail basic arithmetic algorithms for unsigned (i.e., non-
negative) integers -- these algorithms work with vectors of base-B digits, and
except where explicitly noted, we do not assume that the high-order digits of the
input vectors are non-zero, nor do these algorithms ensure that the high-order digit
of the output vector is non-zero. These algorithms can be very easily adapted to
deal with arbitrary signed integers, and to take proper care that the high-order digit
of the vector representing a non-zero number is itself non-zero (the reader is asked
to fill in these details in some of the exercises below). All of these algorithms
can be implemented directly in a programming language that provides a "built-in"
signed integer type that can represent all integers of absolute value less than B2, and
that supports the basic arithmetic operations (addition, subtraction, multiplication,
integer division). So, for example, using the C or Java programming language's
int type on a typical 32-bit computer, we could take B = 215. The resulting
software would be reasonably efficient and portable, but certainly not the fastest
possible.

   Suppose we have the base-B representations of two unsigned integers a and b.
We present algorithms to compute the base-B representation of a + b, a - b, a · b,
 a/b , and a mod b. To simplify the presentation, for integers x, y with y = 0, we

denote by QuoRem(x, y) the quotient/remainder pair ( x/y , x mod y).

                                        3.3.1 Addition

Let a = (ak-1 · · · a0)B and b = (b -1 · · · b0)B be unsigned integers. Assume that
k   1 (if k < , then we can just swap a and b). The sum c := a + b is of the
3.3 Basic integer arithmetic                             57

form c = (ckck-1 · · · c0)B. Using the standard "paper-and-pencil" method (adapted

from base-10 to base-B, of course), we can compute the base-B representation of
a + b in time O(k), as follows:

carry  0

for i  0 to - 1 do
      tmp  ai + bi + carry, (carry, ci)  QuoRem(tmp, B)

for i  to k - 1 do
      tmp  ai + carry, (carry, ci)  QuoRem(tmp, B)

ck  carry

   Note that in every loop iteration, the value of carry is 0 or 1, and the value tmp
lies between 0 and 2B - 1.

                                      3.3.2 Subtraction

Let a = (ak-1 · · · a0)B and b = (b -1 · · · b0)B be unsigned integers. Assume that
k   1. To compute the difference c := a - b, we may use the same algorithm
as above, but with the expression "ai + bi" replaced by "ai - bi." In every loop

iteration, the value of carry is 0 or -1, and the value of tmp lies between -B and
B - 1. If a  b, then ck = 0 (i.e., there is no carry out of the last loop iteration);
otherwise, ck = -1 (and b - a = Bk - (ck-1 · · · c0)B, which can be computed with

another execution of the subtraction routine).

                                     3.3.3 Multiplication
Let a = (ak-1 · · · a0)B and b = (b -1 · · · b0)B be unsigned integers, with k  1 and

   1. The product c := a · b is of the form (ck+ -1 · · · c0)B, and may be computed
in time O(k ) as follows:

         for i  0 to k + - 1 do ci  0
         for i  0 to k - 1 do

               carry  0
               for j  0 to - 1 do

                      tmp  aibj + ci+j + carry
                      (carry, ci+j)  QuoRem(tmp, B)
               ci+  carry

   Note that at every step in the above algorithm, the value of carry lies between 0
and B - 1, and the value of tmp lies between 0 and B2 - 1.
58              Computing with large integers

                              3.3.4 Division with remainder

Let a = (ak-1 · · · a0)B and b = (b -1 · · · b0)B be unsigned integers, with k  1,
   1, and b -1 = 0. We want to compute q and r such that a = bq + r and

0  r < b. Assume that k  ; otherwise, a < b, and we can just set q  0 and
r  a. The quotient q will have at most m := k - + 1 base-B digits. Write
q = (qm-1 · · · q0)B.

   At a high level, the strategy we shall use to compute q and r is the following:

         ra

         for i  m - 1 down to 0 do
               qi  r/Bib
               r  r - Bi · qib

   One easily verifies by induction that at the beginning of each loop iteration, we
have 0  r < Bi+1b, and hence each qi will be between 0 and B - 1, as required.

   Turning the above strategy into a detailed algorithm takes a bit of work. In
particular, we want an easy way to compute r/Bib . Now, we could in theory

just try all possible choices for qi -- this would take time O(B ), and viewing B
as a constant, this is O( ). However, this is not really very desirable from either a

practical or theoretical point of view, and we can do much better with just a little
effort.

   We shall first consider a special case; namely, the case where = 1. In this case,
the computation of the quotient r/Bib is facilitated by the following theorem,

which essentially tells us that this quotient is determined by the two high-order

digits of r:

Theorem 3.1. Let x and y be integers such that
                           0  x = x 2n + s and 0 < y = y 2n

for some integers n, s, x , y , with n  0 and 0  s < 2n. Then x/y = x /y .

Proof. We have

                xy = xy + y 2n s  xy .

It follows immediately that x/y  x /y .
   We also have

    xy = xy + y 2n s < xy + 1y   x + y - 1 + 1  x + 1.
                                 y       y     yy

Thus, we have x/y < x /y + 1, and hence, x/y  x /y . 2
3.3 Basic integer arithmetic  59

   From this theorem, one sees that the following algorithm correctly computes the
quotient and remainder in time O(k) (in the case = 1):

         hi  0
         for i  k - 1 down to 0 do

               tmp  hi ·B + ai
               (qi, hi)  QuoRem(tmp, b0)
         output the quotient q = (qk-1 · · · q0)B and the remainder hi

   Note that in every loop iteration, the value of hi lies between 0 and b0  B - 1,
and the value of tmp lies between 0 and B · b0 + (B - 1)  B2 - 1.

   That takes care of the special case where = 1. Now we turn to the general case
   1. In this case, we cannot so easily get the digits qi of the quotient, but we can
still fairly easily estimate these digits, using the following:

Theorem 3.2. Let x and y be integers such that
                         0  x = x 2n + s and 0 < y = y 2n + t

for some integers n, s, t, x , y with n  0, 0  s < 2n, and 0  t < 2n. Further,
suppose that 2y  x/y. Then

                                x/y  x /y  x/y + 2.

Proof. We have x/y  x/y 2n, and so x/y  x/y 2n , and by the previous
theorem, x/y 2n = x /y . That proves the first inequality.

   For the second inequality, first note that from the definitions, we have x/y 
x /(y +1), which implies x y-xy -x  0. Further, 2y  x/y implies 2yy -x  0.

So we have 2yy - x  0  x y - xy - x, which implies x/y  x /y - 2, and

hence x/y  x /y - 2. 2

   Based on this theorem, we first present an algorithm for division with remain-
der that works if we assume that b is appropriately "normalized," meaning that
b -1  2w-1, where B = 2w. This algorithm is shown in Fig. 3.1.

   Some remarks are in order.

     1. In line 4, we compute qi, which by Theorem 3.2 is greater than or equal to
         the true quotient digit, but exceeds this value by at most 2.

     2. In line 5, we reduce qi if it is obviously too big.

     3. In lines 6-10, we compute

                       (ri+ · · · ri)B  (ri+ · · · ri)B - qib.

In each loop iteration, the value of tmp lies between -(B2 - B) and B - 1,
and the value carry lies between -(B - 1) and 0.
60            Computing with large integers

    1. for i  0 to k - 1 do ri  ai

    2. rk  0

    3. for i  k - down to 0 do

    4.   qi  (ri+ B + ri+ -1)/b -1

    5.   if qi  B then qi  B - 1

    6.   carry  0

    7.   for j  0 to - 1 do

    8.        tmp  ri+j - qibj + carry

    9.        (carry, ri+j)  QuoRem(tmp, B)

    10.  ri+  ri+ + carry

    11.  while ri+ < 0 do

    12.       carry  0

    13.       for j  0 to - 1 do

    14.            tmp  ri+j + bi + carry

    15.            (carry, ri+j)  QuoRem(tmp, B)

    16.       ri+  ri+ + carry

    17.       qi  qi - 1

    18. output the quotient q = (qk- · · · q0)B

         and the remainder r = (r -1 · · · r0)B

                      Fig. 3.1. Division with Remainder Algorithm

     4. If the estimate qi is too large, this is manifested by a negative value of ri+
         at line 10. Lines 11-17 detect and correct this condition: the loop body
         here executes at most twice; in lines 12-16, we compute

                          (ri+ · · · ri)B  (ri+ · · · ri)B + (b -1 · · · b0)B.

         Just as in the algorithm in §3.3.1, in every iteration of the loop in lines
         13-15, the value of carry is 0 or 1, and the value tmp lies between 0 and
         2B - 1.
   It is easily verified that the running time of the above algorithm is O( ·(k- +1)).

   Finally, consider the general case, where b may not be normalized. We multiply
both a and b by an appropriate value 2w , with 0  w < w, obtaining a := a2w
and b := b2w , where b is normalized; alternatively, we can use a more efficient,
special-purpose "left shift" algorithm to achieve the same effect. We then compute
q and r such that a = b q + r , using the division algorithm in Fig. 3.1. Observe
that q = a /b = a/b , and r = r2w , where r = a mod b. To recover r, we
3.3 Basic integer arithmetic  61

simply divide r by 2w , which we can do either using the above "single precision"

division algorithm, or by using a special-purpose "right shift" algorithm. All of
this normalizing and denormalizing takes time O(k + ). Thus, the total running
time for division with remainder is still O( · (k - + 1)).

EXERCISE 3.17. Work out the details of algorithms for arithmetic on signed inte-
gers, using the above algorithms for unsigned integers as subroutines. You should
give algorithms for addition, subtraction, multiplication, and division with remain-
der of arbitrary signed integers (for division with remainder, your algorithm should
compute a/b and a mod b). Make sure your algorithms correctly compute the
sign bit of the results, and also strip any leading zero digits from the results.

EXERCISE 3.18. Work out the details of an algorithm that compares two signed
integers a and b, determining which of a < b, a = b, or a > b holds.

EXERCISE 3.19. Suppose that we run the division with remainder algorithm in
Fig. 3.1 for > 1 without normalizing b, but instead, we compute the value qi in
line 4 as follows:

                  qi  (ri+ B2 + ri+ -1B + ri+ -2)/(b -1B + b -2) .

Show that qi is either equal to the correct quotient digit, or the correct quotient digit
plus 1. Note that a limitation of this approach is that the numbers involved in the
computation are larger than B2.

EXERCISE 3.20. Work out the details for an algorithm that shifts a given unsigned
integer a to the left by a specified number of bits s (i.e., computes b := a · 2s).
The running time of your algorithm should be linear in the number of digits of the
output.

EXERCISE 3.21. Work out the details for an algorithm that shifts a given unsigned
integer a to the right by a specified number of bits s (i.e., computes b := a/2s ).
The running time of your algorithm should be linear in the number of digits of the
output. Now modify your algorithm so that it correctly computes a/2s for signed
integers a.

EXERCISE 3.22. This exercise is for C/Java programmers. Evaluate the C/Java
expressions

   (-17) % 4; (-17) & 3;

and compare these values with (-17) mod 4. Also evaluate the C/Java expressions

   (-17) / 4; (-17) >> 2;
62  Computing with large integers

and compare with -17/4 . Explain your findings.

EXERCISE 3.23. This exercise is also for C/Java programmers. Suppose that
values of type int are stored using a 32-bit 2's complement representation, and
that all basic arithmetic operations are computed correctly modulo 232, even if an
"overflow" happens to occur. Also assume that double precision floating point
has 53 bits of precision, and that all basic arithmetic operations give a result with
a relative error of at most 2-53. Also assume that conversion from type int to
double is exact, and that conversion from double to int truncates the fractional
part. Now, suppose we are given int variables a, b, and n, such that 1 < n < 230,
0  a < n, and 0  b < n. Show that after the following code sequence is
executed, the value of r is equal to (a · b) mod n:

   int q;
   q = (int) ((((double) a) * ((double) b)) / ((double) n));
   r = a*b - q*n;
   if (r >= n)

       r = r - n;
   else if (r < 0)

       r = r + n;

               3.3.5 Summary

We now summarize the results of this section. For an integer a, we define its bit
length, or simply, its length, which we denote by len(a), to be the number of bits
in the binary representation of |a|; more precisely,

    len(a) :=  log2|a| + 1 if a = 0,
                              if a = 0.
               1

If len(a) = , we say that a is an -bit integer. Notice that if a is a positive, -bit
integer, then log2 a <  log2 a + 1, or equivalently, 2 -1  a < 2 .

   Assuming that arbitrarily large integers are represented as described at the begin-

ning of this section, with a sign bit and a vector of base-B digits, where B is a

constant power of 2, we may state the following theorem.

Theorem 3.3. Let a and b be arbitrary integers.           and the remainder
    (i) We can compute a ± b in time O(len(a) + len(b)).

   (ii) We can compute a · b in time O(len(a) len(b)).
   (iii) If b = 0, we can compute the quotient q := a/b

         r := a mod b in time O(len(b) len(q)).
3.3 Basic integer arithmetic  63

   Note the bound O(len(b) len(q)) in part (iii) of this theorem, which may be
significantly less than the bound O(len(a) len(b)). A good way to remember this
bound is as follows: the time to compute the quotient and remainder is roughly the
same as the time to compute the product bq appearing in the equality a = bq + r.

   This theorem does not explicitly refer to the base B in the underlying implemen-
tation. The choice of B affects the values of the implied big-O constants; while in
theory, this is of no significance, it does have a significant impact in practice.

   From now on, we shall (for the most part) not worry about the implementa-
tion details of long-integer arithmetic, and will just refer directly to this theorem.
However, we will occasionally exploit some trivial aspects of our data structure for
representing large integers. For example, it is clear that in constant time, we can
determine the sign of a given integer a, the bit length of a, and any particular bit of
the binary representation of a; moreover, as discussed in Exercises 3.20 and 3.21,
multiplications and divisions by powers of 2 can be computed in linear time via
"left shifts" and "right shifts." It is also clear that we can convert between the base-
2 representation of a given integer and our implementation's internal representation
in linear time (other conversions may take longer -- see Exercise 3.32).

   We wish to stress the point that efficient algorithms on large integers should
run in time bounded by a polynomial in the bit lengths of the inputs, rather than
their magnitudes. For example, if the input to an algorithm is an -bit integer n,
and if the algorithm runs in time O( 2), it will easily be able to process 1000-bit
inputs in a reasonable amount of time (a fraction of a second) on a typical, modern
computer. However, if the algorithm runs in time, say, O(n1/2), this means that
on 1000-bit inputs, it will take roughly 2500 computing steps, which even on the
fastest computer available today or in the foreseeable future, will still be running
long after our solar system no longer exists.

         A note on notation: "len" and "log." In expressing the running times
         of algorithms in terms of an input a, we generally prefer to write len(a)
         rather than log a. One reason is esthetic: writing len(a) stresses the fact
         that the running time is a function of the bit length of a. Another reason is
         technical: for big-O estimates involving functions on an arbitrary domain,
         the appropriate inequalities should hold throughout the domain, and for
         this reason, it is very inconvenient to use functions, like log, which vanish
         or are undefined on some inputs.

EXERCISE 3.24. Let a, b  Z with a  b > 0, and let q := a/b . Show that
len(a) - len(b) - 1  len(q)  len(a) - len(b) + 1.
64       Computing with large integers

EXERCISE 3.25. Let n1, . . . , nk be positive integers. Show that

    k    k                         k

         len(ni) - k  len ni   len(ni).

    i=1  i=1                      i=1

EXERCISE 3.26. Show that given integers n1, . . . , nk, with each ni > 1, we can
compute the product n := i ni in time O(len(n)2).

EXERCISE 3.27. Show that given integers a, n1, . . . , nk, with each ni > 1, where
0  a < n := i ni, we can compute (a mod n1, . . . , a mod nk) in time O(len(n)2).

EXERCISE 3.28. Show that given integers n1, . . . , nk, with each ni > 1, we can
compute (n/n1, . . . , n/nk), where n := i ni, in time O(len(n)2).

                                                                                 
EXERCISE 3.29. This exercise develops an algorithm to compute n for a given

positive integer n. Consider the following algorithm:

         k  (len(n) - 1)/2 , m  2k
         for i  k - 1 down to 0 do

                if (m + 2i)2  n then m  m + 2i
         output m

                                                                 
    (a) Show that this algorithm correctly computes n .

    (b) In a straightforward implementation of this algorithm, each loop itera-
         tion takes time O(len(n)2), yielding a total running time of O(len(n)3).

         Give a more careful implementation, so that each loop iteration takes time
         O(len(n)), yielding a total running time is O(len(n)2).

EXERCISE 3.30. Modify the algorithm in the previous exercise so that given pos-
itive integers n and e, with n  2e, it computes n1/e in time O(len(n)3/e).

EXERCISE 3.31. An integer n > 1 is called a perfect power if n = ab for some
integers a > 1 and b > 1. Using the algorithm from the previous exercise, design
an efficient algorithm that determines if a given n is a perfect power, and if it is,
also computes a and b such that n = ab, where a > 1, b > 1, and a is as small as
possible. Your algorithm should run in time O( 3 len( )), where := len(n).

EXERCISE 3.32. Show how to convert (in both directions) in time O(len(n)2)
between the base-10 representation and our implementation's internal representa-
tion of an integer n.

                                    3.4 Computing in Zn
Let n be a positive integer. For every   Zn, there exists a unique integer
a  {0, . . . , n - 1} such that  = [a]n; we call this integer a the canonical
                 3.4 Computing in Zn                              65

representative of , and denote it by rep(). For computational purposes, we

represent elements of Zn by their canonical representatives.
   Addition and subtraction in Zn can be performed in time O(len(n)): given

,   Zn, to compute rep( + ), we first compute the integer sum rep() + rep(),
and then subtract n if the result is greater than or equal to n; similarly, to com-
pute rep( - ), we compute the integer difference rep() - rep(), adding n if
the result is negative. Multiplication in Zn can be performed in time O(len(n)2):
given ,   Zn, we compute rep( · ) as rep() rep() mod n, using one integer
multiplication and one division with remainder.

         A note on notation: "rep," "mod," and "[·]n." In describing algorithms,
         as well as in other contexts, if ,  are elements of Zn, we may write, for
         example,    +  or   , and it is understood that elements of
         Zn are represented by their canonical representatives as discussed above,
         and arithmetic on canonical representatives is done modulo n. Thus, we
         have in mind a "strongly typed" language for our pseudo-code that makes
         a clear distinction between integers in the set {0, . . . , n - 1} and elements
         of Zn. If a  Z, we can convert a to an object   Zn by writing   [a]n,
         and if a  {0, . . . , n-1}, this type conversion is purely conceptual, involv-
         ing no actual computation. Conversely, if   Zn, we can convert  to
         an object a  {0, . . . , n - 1}, by writing a  rep(); again, this type
         conversion is purely conceptual, and involves no actual computation. It
         is perhaps also worthwhile to stress the distinction between a mod n and
         [a]n -- the former denotes an element of the set {0, . . . , n - 1}, while the
         latter denotes an element of Zn.

   Another interesting problem is exponentiation in Zn: given   Zn and a non-
negative integer e, compute e  Zn. Perhaps the most obvious way to do this is to
iteratively multiply by  a total of e times, requiring time O(e len(n)2). For small

values of e, this is fine; however, a much faster algorithm, the repeated-squaring

algorithm, computes e using just O(len(e)) multiplications in Zn, thus taking
time O(len(e) len(n)2).

   This method is based on the following observation. Let e = (b -1 · · · b0)2 be
the binary expansion of e (where b0 is the low-order bit). For i = 0, . . . , , define
ei := e/2i ; the binary expansion of ei is ei = (b -1 · · · bi)2. Also define i := ei
for i = 0, . . . , , so  = 1 and 0 = e. Then we have

ei = 2ei+1 + bi  and  i = 2 · bi     for i = 0, . . . ,  - 1.

                                i+1

This observation yields the following algorithm for computing e:

The repeated-squaring algorithm. On input , e, where   Zn and e is a non-
negative integer, do the following, where e = (b -1 · · · b0)2 is the binary expansion
of e:
66              Computing with large integers

           [1]n
         for i  - 1 down to 0 do

                 2
               if bi = 1 then    · 
         output 

   It is clear that when this algorithm terminates, we have  = e, and that the
running-time estimate is as claimed above. Indeed, the algorithm uses squarings
in Zn, and at most additional multiplications in Zn.

Example 3.5. Suppose e = 37 = (100101)2. The above algorithm performs the
following operations in this case:

      [1]       // computed exponent (in binary)
      2,    ·   // 0
      2         // 1
      2         // 10
      2,    ·   // 100
      2         // 1001
      2,    ·   // 10010
                // 100101 . 2

   The repeated-squaring algorithm has numerous applications. We mention a few
here, but we will see many more later on.

Computing multiplicative inverses in Zp. Suppose we are given a prime p and an
element   Zp, and we want to compute -1. By Euler's theorem (Theorem 2.13),
we have p-1 = 1, and multiplying this equation by -1, we obtain p-2 = -1.
Thus, we can use the repeated-squaring algorithm to compute -1 by raising  to
the power p - 2. This algorithm runs in time O(len(p)3). While this is reasonably
efficient, we will develop an even more efficient method in the next chapter, using
Euclid's algorithm (which also works with any modulus, not just a prime modulus).

Testing quadratic residuosity. Suppose we are given an odd prime p and an
element   Zp, and we want to test whether   (Zp)2. By Euler's criterion
(Theorem 2.21), we have   (Zp)2 if and only if (p-1)/2 = 1. Thus, we can
use the repeated-squaring algorithm to test if   (Zp)2 by raising  to the power
(p - 1)/2. This algorithm runs in time O(len(p)3). While this is also reasonably
efficient, we will develop an even more efficient method later in the text (in Chap-
ter 12).

Testing for primality. Suppose we are given an integer n > 1, and we want
to determine whether n is prime or composite. For large n, searching for prime
factors of n is hopelessly impractical. A better idea is to use Euler's theorem,
                      3.4 Computing in Zn  67

combined with the repeated-squaring algorithm: we know that if n is prime, then
every non-zero   Zn satisfies n-1 = 1. Conversely, if n is composite, there
exists a non-zero   Zn such that n-1 = 1 (see Exercise 2.27). This suggests the
following "trial and error" strategy for testing if n is prime:

repeat k times

      choose   Zn \ {[0]}
      compute   n-1
      if  = 1 output "composite" and halt

output "maybe prime"

As stated, this is not a fully specified algorithm: we have to specify the loop-
iteration parameter k, and more importantly, we have to specify a procedure for
choosing  in each loop iteration. One approach might be to just try  = [1], [2],
[3], . . . . Another might be to choose  at random in each loop iteration: this would
be an example of a probabilistic algorithm (a notion we shall discuss in detail in
Chapter 9). In any case, if the algorithm outputs "composite," we may conclude
that n is composite (even though the algorithm does not find a non-trivial factor of
n). However, if the algorithm completes all k loop iterations and outputs "maybe
prime," it is not clear what we should conclude: certainly, we have some reason to
suspect that n is prime, but not really a proof; indeed, it may be the case that n is
composite, but we were just unlucky in all of our choices for . Thus, while this
rough idea does not quite give us an effective primality test, it is not a bad start, and
is the basis for several effective primality tests (a couple of which we shall discuss
in detail in Chapters 10 and 21).

EXERCISE 3.33. The repeated-squaring algorithm we have presented here
processes the bits of the exponent from left to right (i.e., from high order to low
order). Develop an algorithm for exponentiation in Zn with similar complexity that
processes the bits of the exponent from right to left.

EXERCISE 3.34. Show that given a prime p,   Zp, and an integer e  p, we can
compute e in time O(len(e) len(p) + len(p)3).

   The following exercises develop some important efficiency improvements to the
basic repeated-squaring algorithm.

EXERCISE 3.35. The goal of this exercise is to develop a "2t-ary" variant of the
above repeated-squaring algorithm, in which the exponent is effectively treated as
a number in base 2t, for some parameter t, rather than in base 2. Let   Zn and
let e be a positive integer of length . Let us write e in base 2t as e = (ek · · · e0)2t,
where ek = 0. Consider the following algorithm:
68  Computing with large integers

    compute a table of values T [0 . . . 2t - 1],
          where T [j] := j for j = 0, . . . , 2t - 1

      T [ek]

    for i  k - 1 down to 0 do
            2t · T [ei]

    (a) Show that this algorithm correctly computes e, and work out the imple-
         mentation details; in particular, show that it may be implemented in such a
         way that it uses at most squarings and 2t + /t + O(1) additional multi-
         plications in Zn.

   (b) Show that, by appropriately choosing the parameter t, we can bound the
         number of multiplications in Zn (besides the squarings) by O( / len( )).
         Thus, from an asymptotic point of view, the cost of exponentiation is essen-
         tially the cost of about squarings in Zn.

    (c) Improve the algorithm so that it only uses no more than squarings and
         2t-1 + /t + O(1) additional multiplications in Zn. Hint: build a table that
         contains only the odd powers of  among 0, 1, . . . , 2t-1.

EXERCISE 3.36. Suppose we are given 1, . . . , k  Zn, along with non-negative
integers e1, . . . , ek, where len(ei)  for i = 1, . . . , k. Show how to compute
 := 1e1 · · · kek , using at most squarings and + 2k additional multiplications
in Zn. Your algorithm should work in two phases: the first phase uses only the
values 1, . . . , k, and performs at most 2k multiplications in Zn; in the second
phase, the algorithm computes , using the exponents e1, . . . , ek, along with the
data computed in the first phase, and performs at most squarings and additional
multiplications in Zn.

EXERCISE 3.37. Suppose that we are to compute e, where   Zn, for many
exponents e of length at most , but with  fixed. Show that for every positive
integer parameter k, we can make a pre-computation (depending on , , and k)
that uses at most squarings and 2k additional multiplications in Zn, so that after
the pre-computation, we can compute e for every exponent e of length at most
using at most /k + O(1) squarings and /k + O(1) additional multiplications in
Zn. Hint: use the algorithm in the previous exercise.

EXERCISE 3.38. Suppose we are given   Zn, along with non-negative integers
e1, . . . , er, where len(ei)  for i = 1, . . . , r, and r = O(len( )). Using the
previous exercise, show how to compute (e1, . . . , er ) using O( ) multiplications
in Zn.

EXERCISE 3.39. Suppose we are given   Zn, along with integers m1, . . . , mr,
3.5 Faster integer arithmetic ()  69

with each mi > 1. Let m := i mi. Also, for i = 1, . . . , r, let mi := m/mi.
Show how to compute (m1, . . . , mr ) using O(len(r) ) multiplications in Zn, where

  := len(m). Hint: divide and conquer. Note that if r = O(len( )), then using the
previous exercise, we can solve this problem using just O( ) multiplications.

EXERCISE 3.40. Let k be a constant, positive integer. Suppose we are given

1, . . . , k  Zn, along with non-negative integers e1, . . . , ek, where len(ei)  for
i = 1, . . . , k. Show how to compute the value 1e1 · · · kek , using at most squarings
and O( / len( )) additional multiplications in Zn. Hint: develop a 2t-ary version
of the algorithm in Exercise 3.36.

                            3.5 Faster integer arithmetic ()

The quadratic-time algorithms presented in §3.3 for integer multiplication and divi-
sion are by no means the fastest possible. The next exercise develops a faster
multiplication algorithm.

EXERCISE 3.41. Suppose we have two positive integers a and b, each of length
at most , such that a = a12k + a0 and b = b12k + b0, where 0  a0 < 2k and
0  b0 < 2k. Then

                         ab = a1b122k + (a0b1 + a1b0)2k + a0b0.

Show how to compute the product ab in time O( ), given the products a0b0, a1b1,
and (a0 - a1)(b0 - b1). From this, design a recursive algorithm that computes ab
in time O( log2 3). (Note that log2 3  1.58.)

   The algorithm in the previous exercise is also not the best possible. In fact, it is
possible to multiply two integers of length at most on a RAM in time O( ), but
we do not explore this any further for the moment (see §3.6).

   The following exercises explore the relationship between integer multiplication
and related problems. We assume that we have an algorithm that multiplies two
integers of length at most in time at most M ( ). It is convenient (and reasonable)
to assume that M is a well-behaved complexity function. By this, we mean that
M maps positive integers to positive real numbers, such that for some constant
  1, and all positive integers a and b, we have

                                           M(a + b)
                                   1  M(a) + M(b)  .

EXERCISE 3.42. Show that if M is a well-behaved complexity function, then it is
strictly increasing.
70  Computing with large integers

EXERCISE 3.43. Show that if N ( ) := M ( )/ is a non-decreasing function, and
M (2 )/M ( ) = O(1), then M is a well-behaved complexity function.

EXERCISE 3.44. Let  > 0,   1,   0,   0 be real constants. Show that
                             M ( ) :=   len( ) len(len( ))

is a well-behaved complexity function.

EXERCISE 3.45. Show that given integers n > 1 and e > 1, we can compute ne in
time O(M (len(ne))).

EXERCISE 3.46. Give an algorithm for Exercise 3.26 whose running time is
O(M (len(n)) len(k)). Hint: divide and conquer.

EXERCISE 3.47. In the previous exercise, suppose all the inputs ni have the same
length, and that M ( ) =  , where  and  are constants with  > 0 and  > 1.

Show that your algorithm runs in time O(M (len(n))).

EXERCISE 3.48. We can represent a "floating point" number z^ as a pair (a, e),
where a and e are integers -- the value of z^ is the rational number a2e, and we

call len(a) the precision of z^. We say that z^ is a k-bit approximation of a real
number z if z^ has precision k and z^ = (1 + )z for some ||  2-k+1. Show

that given positive integers b and k, we can compute a k-bit approximation of 1/b

in time O(M (k)). Hint: using Newton iteration, show how to go from a t-bit

approximation of 1/b to a (2t - 2)-bit approximation of 1/b, making use of just

the high-order O(t) bits of b, in time O(M (t)). Newton iteration is a general
method of iteratively approximating a root of an equation f (x) = 0 by starting

with an initial approximation x0, and computing subsequent approximations by
the formula xi+1 = xi - f (xi)/f (xi), where f (x) is the derivative of f (x). For
this exercise, apply Newton iteration to the function f (x) = x-1 - b.

EXERCISE 3.49. Using the result of the previous exercise, show that, given pos-
itive integers a and b of bit length at most , we can compute a/b and a mod b
in time O(M ( )). From this we see that, up to a constant factor, division with
remainder is no harder than multiplication.

EXERCISE 3.50. Using the result of the previous exercise, give an algorithm for
Exercise 3.27 that runs in time O(M (len(n)) len(k)). Hint: divide and conquer.

EXERCISE 3.51. Give an algorithm for Exercise 3.29 whose running time is
O(M (len(n))). Hint: Newton iteration.

EXERCISE 3.52. Suppose we have an algorithm that computes the square of an
 -bit integer in time at most S( ), where S is a well-behaved complexity function.
3.6 Notes  71

Show how to use this algorithm to compute the product of two arbitrary integers of
length at most in time O(S( )).

EXERCISE 3.53. Give algorithms for Exercise 3.32 whose running times are
O(M ( ) len( )), where := len(n). Hint: divide and conquer.

                                           3.6 Notes

Shamir [89] shows how to factor an integer in polynomial time on a RAM, but
where the numbers stored in the memory cells may have exponentially many
bits. As there is no known polynomial-time factoring algorithm on any realistic
machine, Shamir's algorithm demonstrates the importance of restricting the sizes
of numbers stored in the memory cells of our RAMs to keep our formal model
realistic.

   The most practical implementations of algorithms for arithmetic on large inte-
gers are written in low-level "assembly language," specific to a particular machine's
architecture (e.g., the GNU Multi-Precision library GMP, available at gmplib.
org). Besides the general fact that such hand-crafted code is more efficient than
that produced by a compiler, there is another, more important reason for using
assembly language. A typical 32-bit machine often comes with instructions that
allow one to compute the 64-bit product of two 32-bit integers, and similarly,
instructions to divide a 64-bit integer by a 32-bit integer (obtaining both the quo-
tient and remainder). However, high-level programming languages do not (as a
rule) provide any access to these low-level instructions. Indeed, we suggested in
§3.3 using a value for the base B of about half the word-size of the machine, in
order to avoid overflow. However, if one codes in assembly language, one can
take B to be much closer, or even equal, to the word-size of the machine. Since
our basic algorithms for multiplication and division run in time quadratic in the
number of base-B digits, the effect of doubling the bit-length of B is to decrease
the running time of these algorithms by a factor of four. This effect, combined
with the improvements one might typically expect from using assembly-language
code, can easily lead to a five- to ten-fold decrease in the running time, compared
to an implementation in a high-level language. This is, of course, a significant
improvement for those interested in serious "number crunching."

   The "classical," quadratic-time algorithms presented here for integer multiplica-
tion and division are by no means the best possible: there are algorithms that are
asymptotically faster. We saw this in the algorithm in Exercise 3.41, which was
originally invented by Karatsuba [54] (although Karatsuba is one of two authors
on this paper, the paper gives exclusive credit for this particular result to Karat-
suba). That algorithm allows us to multiply two integers of length at most in time
72  Computing with large integers

O( log2 3). The fastest known algorithm for multiplying such integers on a RAM
runs in time O( ), and is due to Schönhage. It actually works on a very restricted
type of RAM called a "pointer machine" (see Exercise 12, Section 4.3.3 of Knuth
[56]). See Exercise 17.25 later in this text for a much simpler (but heuristic) O( )
multiplication algorithm.

   Another model of computation is that of Boolean circuits. In this model of
computation, one considers families of Boolean circuits (with, say, the usual "and,"
"or," and "not" gates) that compute a particular function -- for every input length,
there is a different circuit in the family that computes the function on inputs that
are bit strings of that length. One natural notion of complexity for such circuit
families is the size of the circuit (i.e., the number of gates and wires in the circuit),
which is measured as a function of the input length. For many years, the smallest
known Boolean circuit that multiplies two integers of length at most was of size
O( len( ) len(len( ))). This result was due to Schönhage and Strassen [86]. More
recently, Fürer showed how to reduce this to O( len( )2O(log )) [38]. Here, the
value of log n is defined as the minimum number of applications of the function
log2 to the number n required to obtain a number that is less than or equal to 1.
The function log is an extremely slow growing function, and is a constant for all
practical purposes.

   It is hard to say which model of computation, the RAM or circuits, is "better."
On the one hand, the RAM very naturally models computers as we know them
today: one stores small numbers, like array indices, counters, and pointers, in
individual words of the machine, and processing such a number typically takes
a single "machine cycle." On the other hand, the RAM model, as we formally
defined it, invites a certain kind of "cheating," as it allows one to stuff O(len( ))-
bit integers into memory cells. For example, even with the simple, quadratic-time
algorithms for integer arithmetic discussed in §3.3, we can choose the base B to
have len( ) bits, in which case these algorithms would run in time O(( / len( ))2).
However, just to keep things simple, we have chosen to view B as a constant (from
a formal, asymptotic point of view).

   In the remainder of this text, unless otherwise specified, we shall always use
the classical O( 2) bounds for integer multiplication and division. These have the
advantages of being simple and of being reasonably reliable predictors of actual
performance for small to moderately sized inputs. For relatively large numbers,
experience shows that the classical algorithms are definitely not the best -- Karat-
suba's multiplication algorithm, and related algorithms for division, are superior
on inputs of a thousand bits or so (the exact crossover depends on myriad imple-
mentation details). The even "faster" algorithms discussed above are typically not
interesting unless the numbers involved are truly huge, of bit length around 105-
106. Thus, the reader should bear in mind that for serious computations involving
3.6 Notes  73

very large numbers, the faster algorithms are very important, even though this text
does not discuss them at great length.

   For a good survey of asymptotically fast algorithms for integer arithmetic, see
Chapter 9 of Crandall and Pomerance [30], as well as Chapter 4 of Knuth [56].
                             4

                      Euclid's algorithm

In this chapter, we discuss Euclid's algorithm for computing greatest common
divisors, which, as we will see, has applications far beyond that of just computing
greatest common divisors.

                           4.1 The basic Euclidean algorithm
We consider the following problem: given two non-negative integers a and b, com-
pute their greatest common divisor, gcd(a, b). We can do this using the well-known
Euclidean algorithm, also called Euclid's algorithm.

   The basic idea is the following. Without loss of generality, we may assume that
a  b  0. If b = 0, then there is nothing to do, since in this case, gcd(a, 0) = a.
Otherwise, b > 0, and we can compute the integer quotient q := a/b and remain-
der r := a mod b, where 0  r < b. From the equation

                                          a = bq + r,
it is easy to see that if an integer d divides both b and r, then it also divides a; like-
wise, if an integer d divides a and b, then it also divides r. From this observation, it
follows that gcd(a, b) = gcd(b, r), and so by performing a division, we reduce the
problem of computing gcd(a, b) to the "smaller" problem of computing gcd(b, r).

   The following theorem develops this idea further:
Theorem 4.1. Let a, b be integers, with a  b  0. Using the division with
remainder property, define the integers r0, r1, . . . , r+1 and q1, . . . , q, where   0,
as follows:

                                                74
4.1 The basic Euclidean algorithm                75

   a = r0,                     (0 < r2 < r1),
   b = r1,
  r0 = r1q1 + r2               (0 < ri+1 < ri),

      ..                       (0 < r < r-1),
      .                        (r+1 = 0).
ri-1 = riqi + ri+1
      ..
      .
r-2 = r-1q-1 + r
r-1 = rq

Note that by definition,  = 0 if b = 0, and  > 0, otherwise. Then we
have r =gcd(a, b). Moreover, if b > 0, then   log b/ log  + 1, where
 := (1 + 5)/2  1.62.

Proof. For the first statement, one sees that for i = 1, . . . , , we have ri-1 =
riqi + ri+1, from which it follows that the common divisors of ri-1 and ri are the
same as the common divisors of ri and ri+1, and hence gcd(ri-1, ri) = gcd(ri, ri+1).

From this, it follows that

           gcd(a, b) = gcd(r0, r1) = · · · = gcd(r, r+1) = gcd(r, 0) = r.

   To prove the second statement, assume that b > 0, and hence  > 0. If  = 1, the
statement is obviously true, so assume  > 1. We claim that for i = 0, . . . ,  - 1,
we have r-i  i. The statement will then follow by setting i =  - 1 and taking
logarithms.

   We now prove the above claim. For i = 0 and i = 1, we have

                       r  1 = 0 and r-1  r + 1  2  1.

For i = 2, . . . ,  - 1, using induction and applying the fact that 2 =  + 1, we
have

            r-i  r-(i-1) + r-(i-2)  i-1 + i-2 = i-2(1 + ) = i,

which proves the claim. 2

Example 4.1. Suppose a = 100 and b = 35. Then the numbers appearing in
Theorem 4.1 are easily computed as follows:

                           i 0 1 234

                           ri 100 35 30 5 0

                           qi  2 16
76            Euclid's algorithm

So we have gcd(a, b) = r3 = 5. 2

   We can easily turn the scheme described in Theorem 4.1 into a simple algorithm:

Euclid's algorithm. On input a, b, where a and b are integers such that a  b  0,
compute d = gcd(a, b) as follows:

         r  a, r  b
         while r = 0 do

               r  r mod r
               (r, r )  (r , r )
         dr
         output d

   We now consider the running time of Euclid's algorithm. Naively, one could
estimate this as follows. Suppose a and b are -bit numbers. The number of
divisions performed by the algorithm is the number  in Theorem 4.1, which is
O( ). Moreover, each division involves numbers of bits or fewer in length, and
so takes time O( 2). This leads to a bound on the running time of O( 3). However,
as the following theorem shows, this cubic running time bound is well off the mark.
Intuitively, this is because the cost of performing a division depends on the length
of the quotient: the larger the quotient, the more expensive the division, but also,
the more progress the algorithm makes towards termination.

Theorem 4.2. Euclid's algorithm runs in time O(len(a) len(b)).

Proof. We may assume that b > 0. With notation as in Theorem 4.1, the running
time is O(T ), where

                                  

    T = len(ri) len(qi)  len(b) len(qi)

    i=1                           i=1

                    

     len(b) (len(ri-1) - len(ri) + 1) (see Exercise 3.24)

         i=1

    = len(b)(len(r0) - len(r) + ) (telescoping the sum)

     len(b)(len(a) + log b/ log  + 1) (by Theorem 4.1)

    = O(len(a) len(b)). 2

EXERCISE 4.1. With notation as in Theorem 4.1, give a direct and simple proof
that for each i = 1, . . . , , we have ri+1  ri-1/2. Thus, with every two division
steps, the bit length of the remainder drops by at least 1. Based on this, give an

alternative proof that the number of divisions is O(len(b)).
                         4.2 The extended Euclidean algorithm                        77

EXERCISE 4.2. Show how to compute lcm(a, b) in time O(len(a) len(b)).

EXERCISE 4.3. Let a, b  Z with a  b  0, let d := gcd(a, b), and assume

d > 0. Suppose that on input a, b, Euclid's algorithm performs  division steps,
                                                   +1
and  computes  the  remainder  sequence                and  the  quotient  sequence  {qi}i=1
                                             {ri}i=0

(as in Theorem 4.1). Now suppose we run Euclid's algorithm on input a/d, b/d.

Show that on these inputs, the number of division steps performed is also , the
                                    +1
remainder  sequence  is                 and  the  quotient  sequence  is  {qi}i=1.
                         {ri/d}i=0 ,

EXERCISE 4.4. Show that if we run Euclid's algorithm on input a, b, where a 
b > 0, then its running time is O(len(a/d) len(b)), where d := gcd(a, b).

EXERCISE 4.5. Let  be a positive integer. Show that there exist integers a, b with
a > b > 0 and   log b/ log , such that Euclid's algorithm on input a, b performs
at least  divisions. Thus, the bound in Theorem 4.1 on the number of divisions is
essentially tight.

EXERCISE 4.6. This exercise looks at an alternative algorithm for computing
gcd(a, b), called the binary gcd algorithm. This algorithm avoids complex opera-
tions, such as division and multiplication; instead, it relies only on subtraction, and
division and multiplication by powers of 2, which, assuming a binary representa-
tion of integers (as we are), can be very efficiently implemented using "right shift"
and "left shift" operations. The algorithm takes positive integers a and b as input,
and runs as follows:

     r  a, r  b, e  0                              r /2,    ee+1
     while 2 | r and 2 | r do r  r/2, r
     repeat

           while 2 | r do r  r/2
           while 2 | r do r  r /2
           if r < r then (r, r )  (r , r)
           r r -r
     until r = 0
     d  2e · r
     output d

Show that this algorithm correctly computes gcd(a, b), and runs in time O( 2),
where := max(len(a), len(b)).

                        4.2 The extended Euclidean algorithm

Let a and b be integers, and let d := gcd(a, b). We know by Theorem 1.8 that there
exist integers s and t such that as + bt = d. The extended Euclidean algorithm
78                              Euclid's algorithm

allows us to efficiently compute s and t. The next theorem defines the quantities
computed by this algorithm, and states a number of important facts about them;
these facts will play a crucial role, both in the analysis of the running time of the
algorithm, as well as in applications of the algorithm that we will discuss later.

Theorem 4.3. Let a, b, r0, . . . , r+1 and q1, . . . , q be as in Theorem 4.1. Define
integers s0, . . . , s+1 and t0, . . . , t+1 as follows:

            s0 := 1,              t0 := 0,           (i = 1, . . . , ).
            s1 := 0,              t1 := 1,
          si+1 := si-1 - siqi,  ti+1 := ti-1 - tiqi

Then:     for i = 0, . . . , +1, we have asi+bti = ri; in particular, as+bt = gcd(a, b);
    (i)   for i = 0, . . . , , we have siti+1 - tisi+1 = (-1)i;
   (ii)   for i = 0, . . . ,  + 1, we have gcd(si, ti) = 1;
          for i = 0, . . . , , we have titi+1  0 and |ti|  |ti+1|; for i = 1, . . . , , we
   (iii)  have sisi+1  0 and |si|  |si+1|;
   (iv)   for i = 1, . . . ,  + 1, we have ri-1|ti|  a and ri-1|si|  b;
          if a > 0, then for i = 1, . . . ,  + 1, we have |ti|  a and |si|  b; if a > 1
    (v)   and b > 0, then |t|  a/2 and |s|  b/2.
   (vi)

Proof. (i) is easily proved by induction on i. For i = 0, 1, the statement is clear.
For i = 2, . . . ,  + 1, we have

          asi + bti = a(si-2 - si-1qi-1) + b(ti-2 - ti-1qi-1)
                     = (asi-2 + bti-2) - (asi-1 + bti-1)qi-1
                     = ri-2 - ri-1qi-1 (by induction)
                     = ri.

   (ii) is also easily proved by induction on i. For i = 0, the statement is clear. For
i = 1, . . . , , we have

          siti+1 - tisi+1 = si(ti-1 - tiqi) - ti(si-1 - siqi)
                           = -(si-1ti - ti-1si) (after expanding and simplifying)
                           = -(-1)i-1 (by induction)
                           = (-1)i.

   (iii) follows directly from (ii).

   For (iv), one can easily prove both statements by induction on i. The state-
ment involving the ti's is clearly true for i = 0. For i = 1, . . . , , we have
4.2 The extended Euclidean algorithm                               79

ti+1 = ti-1 - tiqi; moreover, by the induction hypothesis, ti-1 and ti have opposite
signs and |ti|  |ti-1|; it follows that |ti+1| = |ti-1| + |ti|qi  |ti|, and that the sign

of ti+1 is the opposite of that of ti. The proof of the statement involving the si's is
the same, except that we start the induction at i = 1.

   For (v), one considers the two equations:

                             asi-1 + bti-1 = ri-1,
                                  asi + bti = ri.

Subtracting ti-1 times the second equation from ti times the first, and applying
(ii), we get ±a = tiri-1 - ti-1ri; consequently, using the fact that ti and ti-1 have
opposite sign, we obtain

                    a = |tiri-1 - ti-1ri| = |ti|ri-1 + |ti-1|ri  |ti|ri-1.

The inequality involving si follows similarly, subtracting si-1 times the second

equation from si times the first.

   (vi) follows from (v) and the following observations: if a > 0, then ri-1 > 0 for
i = 1, . . . ,  + 1; if a > 1 and b > 0, then  > 0 and r-1  2. 2

Example 4.2. We continue with Example 4.1. The si's and ti's are easily computed
from the qi's:

i 0 1 23 4

ri 100 35 30 5 0

qi                           2 16

si 1 0 1 -1 7

ti 0 1 -2 3 -20

So we have gcd(a, b) = 5 = -a + 3b. 2

We can easily turn the scheme described in Theorem 4.3 into a simple algorithm:

The extended Euclidean algorithm. On input a, b, where a and b are integers
such that a  b  0, compute integers d, s, and t, such that d = gcd(a, b) and
as + bt = d, as follows:

r  a, r  b                    r mod r         , s - s q, t - t q)
s  1, s  0                    (r , s , t , r
t  0, t  1
while r = 0 do

      q  r/r , r
      (r, s, t, r , s , t )
dr
output d, s, t
80              Euclid's algorithm

Theorem 4.4. The extended Euclidean algorithm runs in time O(len(a) len(b)).

Proof. We may assume that b > 0. It suffices to analyze the cost of computing the
coefficient sequences {si} and {ti}. Consider first the cost of computing all of the
ti's, which is O(T ), where T = i=1 len(ti) len(qi). We have t1 = 1 and, by part
(vi) of Theorem 4.3, we have |ti|  a for i = 2, . . . , . Arguing as in the proof of
Theorem 4.2, we have

                                                 

         T  len(q1) + len(a) len(qi)

                                                i=2

             len(a) + len(a)(len(r1) - len(r) +  - 1) = O(len(a) len(b)).

An analogous argument shows that one can also compute all of the si's in time
O(len(a) len(b)), and in fact, in time O(len(b)2). 2

   For the reader familiar with the basics of the theory of matrices and determinants,
it is instructive to view Theorem 4.3 as follows. For i = 1, . . . , , we have

          ri = 0 1            ri-1 .
          ri+1  1 -qi          ri

Recursively expanding the right-hand side of this equation, we have

                       Mi :=

    ri = 0 1 · · · 0 1 a .
    ri+1        1 -qi         1 -q1 b

This defines the 2 × 2 matrix Mi for i = 1, . . . , . If we additionally define M0 to
be the 2 × 2 identity matrix, then it is easy to see that for i = 0, . . . , , we have

                                     Mi = si ti .
                                               si+1 ti+1

From these observations, part (i) of Theorem 4.3 is immediate, and part (ii) follows
from the fact that Mi is the product of i matrices, each of determinant -1, and the
determinant of Mi is evidently siti+1 - tisi+1.

EXERCISE 4.7. In our description of the extended Euclidean algorithm, we made
the restriction that the inputs a and b satisfy a  b  0. Using this restricted
algorithm as a subroutine, give an algorithm that works without any restrictions on
its input.

EXERCISE 4.8. With notation and assumptions as in Exercise 4.3, suppose that on
input a, b, the extended Euclidean algorithm computes the coefficient sequences
                          4.2 The extended Euclidean algorithm                           81

       +1  and        +1  (as  in  Theorem  4.3).  Show  that  the  extended  Euclidean  algo-

{si}i=0         {ti}i=0
rithm on input a/d, b/d computes the same coefficient sequences.

EXERCISE 4.9. Assume notation as in Theorem 4.3. Show that:

   (a) for all i = 2, . . . , , we have |ti| < |ti+1| and ri-1|ti| < a, and that for all
         i = 3, . . . , , we have |si| < |si+1| and ri-1|si| < b;

   (b) siti  0 for i = 0, . . . ,  + 1;
   (c) if d := gcd(a, b) > 0, then |s+1| = b/d and |t+1| = a/d.

EXERCISE 4.10. One can extend the binary gcd algorithm discussed in Exer-
cise 4.6 so that in addition to computing d = gcd(a, b), it also computes s and
t such that as + bt = d. Here is one way to do this (again, we assume that a and b
are positive integers):

         r  a, r  b, e  0
         while 2 | r and 2 | r do r  r/2, r  r /2, e  e + 1
         a~  r, b~  r , s  1, t  0, s  0, t  1
         repeat

               while 2 | r do
                      r  r/2
                      if 2 | s and 2 | t then s  s/2, t  t/2
                                            else s  (s + b~)/2, t  (t - a~)/2

               while 2 | r do
                      r  r /2
                      if 2 | s and 2 | t then s  s /2, t  t /2
                                            else s  (s + b~)/2, t  (t - a~)/2

               if r < r then (r, s, t, r , s , t )  (r , s , t , r, s, t)
               r  r - r, s  s - s, t  t - t
         until r = 0
         d  2e · r, output d, s, t

Show that this algorithm is correct and that its running time is O( 2), where
  := max(len(a), len(b)). In particular, you should verify that all of the divisions

by 2 performed by the algorithm yield integer results. Moreover, show that the
outputs s and t are of length O( ).

EXERCISE 4.11. Suppose we modify the extended Euclidean algorithm so that it
computes balanced remainders; that is, for i = 1, . . . , , the values qi and ri+1 are
computed so that ri-1 = riqi + ri+1 and -|ri|/2  ri+1 < |ri|/2. Assume that
the si's and the ti's are computed by the same formula as in Theorem 4.3. Give

a detailed analysis of the running time of this algorithm, which should include an

analysis of the number of division steps, and the sizes of the si's and ti's.
82  Euclid's algorithm

          4.3 Computing modular inverses and Chinese remaindering

An important application of the extended Euclidean algorithm is to the problem of
computing multiplicative inverses in Zn.

Theorem 4.5. Suppose we are given integers n, b, where 0  b < n. Then in
time O(len(n)2), we can determine if b is relatively prime to n, and if so, compute
b-1 mod n.

Proof. We may assume n > 1, since when n = 1, we have b = 0 = b-1 mod n. We
run the extended Euclidean algorithm on input n, b, obtaining integers d, s, and t,
such that d = gcd(n, b) and ns + bt = d. If d = 1, then b does not have a multi-
plicative inverse modulo n. Otherwise, if d = 1, then t is a multiplicative inverse
of b modulo n; however, it may not lie in the range {0, . . . , n - 1}, as required. By
part (vi) of Theorem 4.3, we have |t|  n/2 < n. Thus, if t  0, then b-1 mod n is
equal to t; otherwise, b-1 mod n is equal to t + n. Based on Theorem 4.4, it is clear
that all the computations can be performed in time O(len(n)2). 2

Example 4.3. Suppose we are given integers a, b, n, where 0  a < n, and
0  b < n, and we want to compute a solution z to the congruence az  b (mod n),
or determine that no such solution exists. Based on the discussion in Example 2.5,
the following algorithm does the job:

    d  gcd(a, n)                 n/d
    if d b then

          output "no solution"
    else

          a  a/d, b  b/d, n
          t  (a )-1 mod n
          z  tb mod n
          output z

Using Euclid's algorithm to compute d, and the extended Euclidean algorithm

to compute t (as in Theorem 4.5), the running time of this algorithm is clearly
O(len(n)2). 2

   We also observe that the Chinese remainder theorem (Theorem 2.6) can be made
computationally effective:

Theorem 4.6 (Effective Chinese remainder theorem). Suppose we are given
integers n1, . . . , nk and a1, . . . , ak, where the family {ni}ik=1 is pairwise relatively
prime, and where ni > 1 and 0  ai < ni for i = 1, . . . , k. Let n := ik=1 ni. Then
in time O(len(n)2), we can compute the unique integer a satisfying 0  a < n and
a  ai (mod ni) for i = 1, . . . , k.
4.3 Computing modular inverses and Chinese remaindering  83

Proof. The algorithm is a straightforward implementation of the proof of Theo-
rem 2.6, and runs as follows:

         n  ik=1 ni
         for i  1 to k do

               ni  n/ni, bi  ni mod ni, ti  bi-1 mod ni, ei  ni ti
         a  ik=1 aiei mod n

We leave it to the reader to verify the running time bound. 2

EXERCISE 4.12. In Example 4.3, show that one can easily obtain the quantities
d, a , n , and t from the data computed in just a single execution of the extended
Euclidean algorithm.

EXERCISE 4.13. In this exercise, you are to make the result of Theorem 2.17
effective. Suppose that we are given a positive integer n, two elements ,   Zn,
and integers and m, such that  = m and gcd( , m) = 1. Show how to compute
  Zn such that  = m in time O(len( ) len(m) + (len( ) + len(m)) len(n)2).

EXERCISE 4.14. In this exercise and the next, you are to analyze an "incremental
Chinese remaindering algorithm." Consider the following algorithm, which takes
as input integers a1, n1, a2, n2 satisfying

                   0  a1 < n1, 0  a2 < n2, and gcd(n1, n2) = 1.

It outputs integers a, n satisfying

          n = n1n2, 0  a < n, a  a1 (mod n1), and a  a2 (mod n2),

and runs as follows:
         b  n1 mod n2, t  b-1 mod n2, h  (a2 - a1)t mod n2
         a  a1 + n1h, n  n1n2
         output a, n

Show that the algorithm correctly computes a and n as specified, and runs in time
O(len(n) len(n2)).

EXERCISE 4.15. Using the algorithm in the previous exercise as a subroutine, give
a simple O(len(n)2) algorithm that takes as input integers n1, . . . , nk and a1, . . . , ak,
where the family {ni}ik=1 is pairwise relatively prime, and where ni > 1 and
0  ai < ni for i = 1, . . . , k, and outputs integers a and n such that 0  a < n,
n = ik=1 ni, and a  ai (mod ni) for i = 1, . . . , k. The algorithm should
be "incremental," in that it processes the pairs (ai, ni) one at a time, using time
O(len(n) len(ni)) per pair.
84               Euclid's algorithm

EXERCISE 4.16. Suppose we are given 1, . . . , k  Zn. Show how to compute
1-1, . . . , k-1 by computing one multiplicative inverse modulo n, and performing
fewer than 3k multiplications modulo n. This result is useful, as in practice, if n is

several hundred bits long, it may take 10-20 times longer to compute multiplicative

inverses modulo n than to multiply modulo n.

    4.4 Speeding up algorithms via modular computation

An important practical application of the above "computational" version (Theo-

rem 4.6) of the Chinese remainder theorem is a general algorithmic technique that

can significantly speed up certain types of computations involving long integers.

Instead of trying to describe the technique in some general form, we simply illus-

trate the technique by means of a specific example: integer matrix multiplication.

   Suppose we have two m × m matrices A and B whose entries are large integers,
and we want to compute the product matrix C := AB. Suppose that for r, s =
1, . . . , m, the entry of A at row r and column s is ars, and that for s, t = 1, . . . , m,
the entry of B at row s and column t is bst. Then for r, t = 1, . . . , m, the entry of C at
row r and column t is crt, which is given by the usual rule for matrix multiplication:

                           m                            (4.1)

                 crt = arsbst.

                          s=1

Suppose further that M is the maximum absolute value of the entries in A and
B, so that the entries in C are bounded in absolute value by M := M2m. Let

  := len(M). To simplify calculations, let us also assume that m  M (this is
reasonable, as we want to consider large values of M, greater than say 2100, and
certainly, we cannot expect to work with 2100 × 2100 matrices).

   By just applying the formula (4.1), we can compute the entries of C using m3
multiplications of numbers of length at most , and m3 additions of numbers of
length at most len(M ), where len(M )  2 + len(m) = O( ). This yields a

running time of

                 O(m3 2).                               (4.2)

Using the Chinese remainder theorem, we can actually do much better than this, as

follows.
   For every integer n > 1, and for all r, t = 1, . . . , m, we have

                           m                            (4.3)

                 crt  arsbst (mod n).

                          s=1
4.4 Speeding up algorithms via modular computation            85

Moreover, if we compute integers crt such that                (4.4)

                                                           m

                                   crt  arsbst (mod n)

                                                         s=1

and if we also have

                   - n/2  crt < n/2 and n > 2M ,              (4.5)

then we must have

                   crt = crt.                                 (4.6)

To see why (4.6) follows from (4.4) and (4.5), observe that (4.3) and (4.4) imply
that crt  crt (mod n), which means that n divides (crt - crt). Then from the bound
|crt|  M and from (4.5), we obtain

                |crt - crt|  |crt| + |crt|  M + n/2 < n/2 + n/2 = n.

So we see that the quantity (crt - crt) is a multiple of n, while at the same time this
quantity is strictly less than n in absolute value; hence, this quantity must be zero.
That proves (4.6).

   So from the above discussion, to compute C, it suffices to compute the entries
of C modulo n, where we have to make sure that we compute "balanced" remain-
ders in the interval [-n/2, n/2), rather than the more usual "least non-negative"
remainders.

   To compute C modulo n, we choose a number of small integers n1, . . . , nk, such
that the family {ni}ik=1 is pairwise relatively prime, and the product n := ik=1 ni
is just a bit larger than 2M . In practice, one would choose the ni's to be small
primes, and a table of such primes could easily be computed in advance, so that
all problems up to a given size could be handled. For example, the product of all
primes of at most 16 bits is a number that has more than 90,000 bits. Thus, by
simply pre-computing and storing a table of small primes, we can handle input
matrices with quite large entries (up to about 45,000 bits).

   Let us assume that we have pre-computed appropriate small primes n1, . . . , nk.
Further, we shall assume that addition and multiplication modulo each ni can be
done in constant time. This is reasonable from a practical (and theoretical) point
of view, since such primes easily "fit" into a machine word, and we can perform
modular addition and multiplication using a constant number of built-in machine
operations. Finally, we assume that we do not use more ni's than are necessary, so
that len(n) = O( ) and k = O( ).

   To compute C, we execute the following steps:
86                         Euclid's algorithm

    1. For each i = 1, . . . , k, do the following:

    (a) compute a^rs(i)  ars mod ni for r, s = 1, . . . , m,

    (b)             ^ (i)   bst mod ni for s, t = 1, . . . , m,
         compute bst

    (c) for r, t = 1, . . . , m, compute

                                          m

                           c^rt(i)              (i) ^(i)
                                               a^rs bst mod ni.

                                          s=1

     2. For each r, t = 1, . . . , m, apply the Chinese remainder theorem to c^rt(1), c^rt(2),
         . . . , c^rt(k), obtaining an integer crt, which should be computed as a balanced
         remainder modulo n, so that -n/2  crt < n/2.

     3. Output the matrix C, whose entry in row r and column t is crt.

   Note that in step 2, if our Chinese remainder algorithm happens to be imple-
mented to return an integer a with 0  a < n, we can easily get a balanced
remainder by just subtracting n from a if a  n/2.

   The correctness of the above algorithm has already been established. Let us now
analyze its running time. The running time of steps 1a and 1b is easily seen to be
O(m2 2). Under our assumption about the cost of arithmetic modulo small primes,
the cost of step 1c is O(m3k), and since k = O( ), the cost of this step is O(m3 ).
Finally, by Theorem 4.6, the cost of step 2 is O(m2 2). Thus, the total running time
of this algorithm is

                                        O(m2 2 + m3 ).

This is a significant improvement over (4.2); for example, if  m, then the run-
ning time of the original algorithm is O(m5), while the running time of the modular
algorithm is O(m4).

EXERCISE 4.17. Apply the ideas above to the problem of computing the product
of two polynomials whose coefficients are large integers. First, determine the run-

ning time of the "obvious" algorithm for multiplying two such polynomials, then

design and analyze a "modular" algorithm.

            4.5 An effective version of Fermat's two squares theorem

We proved in Theorem 2.34 (in §2.8.4) that every prime p  1 (mod 4) can be
expressed as a sum of two squares of integers. In this section, we make this theorem
computationally effective; that is, we develop an efficient algorithm that takes as
input a prime p  1 (mod 4), and outputs integers r and t such that p = r2 + t2.
                4.5 An effective version of Fermat's two squares theorem                       87

   One essential ingredient in the proof of Theorem 2.34 was Thue's lemma (The-
orem 2.33). This lemma asserts the existence of certain numbers, and we proved
it using the "pigeonhole principle," which unfortunately does not translate directly
into an efficient algorithm to actually find these numbers. However, we can show
that these numbers arise as a "natural by-product" of the extended Euclidean algo-
rithm. To make this more precise, let us introduce some notation. For integers a, b,
with a  b  0, let us define

                               EEA(a, b) := (ri, si, ti) i=0 +1,
where ri, si, and ti, for i = 0, . . . ,  + 1, are defined as in Theorem 4.3.

Theorem 4.7 (Effective Thue's lemma). Let n, b, r, t  Z, with 0  b < n
                                                     =                    +1
and  0  <  r    n  <  rt.  Further,  let  EEA(n, b)                           and  let  j  be  the
                                                        {(ri, si, ti)}i=0 ,
smallest index (among 0, . . . ,  + 1) such that rj < r. Then, setting r := rj and

t := tj, we have

                   r  bt (mod n), 0  r < r, and 0 < |t| < t.

Proof. Since r0 = n  r > 0 = r+1, the value of the index j is well defined;
moreover, j  1 and rj-1  r. It follows that

                      |tj|  n/rj-1 (by part (v) of Theorem 4.3)
                           n/r
                          < t (since n < rt).

Since j  1, by part (iv) of Theorem 4.3, we have |tj|  |t1| > 0. Finally, since
rj = nsj + btj, we have rj  btj (mod n). 2

   What this theorem says is that given n, b, r, t, to find the desired values r and t,

we run the extended Euclidean algorithm on input n, b. This generates a sequence
of remainders r0 > r1 > r2 > · · · , where r0 = n and r1 = b. If rj is the first
remainder in this sequence that falls below r, and if sj and tj are the corresponding
numbers computed by the extended Euclidean algorithm, then r := rj and t := tj
do the job.

   The other essential ingredient in the proof of Theorem 2.34 was Theorem 2.31,

which guarantees the existence of a square root of -1 modulo p when p is a prime
congruent to 1 modulo 4. We need an effective version of this result as well. Later,

in Chapter 12, we will study the general problem of computing square roots modulo

primes. Right now, we develop an algorithm for this special case.
   Assume we are given a prime p  1 (mod 4), and we want to compute   Zp

such that 2 = -1. By Theorem 2.32, it suffices to find   Zp \ (Zp)2, since then
 := (p-1)/4 (which we can efficiently compute via repeated squaring) satisfies
88  Euclid's algorithm

2 = -1. While there is no known efficient, deterministic algorithm to find such
a , we do know that half the elements of Zp are squares and half are not (see
Theorem 2.20), which suggests the following simple "trial and error" strategy to

compute :

    repeat
          choose   Zp
          compute   (p-1)/4

    until 2 = -1

    output 

   As an algorithm, this is not fully specified, as we have to specify a procedure

for selecting  in each loop iteration. A reasonable approach is to simply choose
 at random: this would be an example of a probabilistic algorithm, a notion that

we will study in detail in Chapter 9. Let us assume for the moment that this makes

sense from a mathematical and algorithmic point of view, so that with each loop

iteration, we have a 50% chance of picking a "good" , that is, one that is not in
(Zp)2. From this, it follows that with high probability, we should find a "good"
 in just a few loop iterations (the probability that after k loop iterations we still
have not found one is 1/2k), and that the expected number of loop iterations is just

2. The running time of each loop iteration is dominated by the cost of repeated
squaring, which is O(len(p)3). It follows that the expected running time of this
algorithm (we will make this notion precise in Chapter 9) is O(len(p)3).

   Let us now put all the ingredients together to get an algorithm to find r, t such
that p = r2 + t2.

     1. Find   Zp such that 2 = -1, using the above "trial and error" strategy.
     2. Set b  rep() (so that  = [b] and b  {0, . . . , p - 1}).

     3. Run the extended Euclidean algorithm on input p, b to obtain EEA(p, b),
         and then apply Theorem 4.7 with n := p, b, and r := t := p + 1, to

         obtain the values r and t.

     4. Output r, t.
   When this algorithm terminates, we have r2 + t2 = p, as required: as we argued
in the proof of Theorem 2.34, since r  bt (mod p) and b2  -1 (mod p), it
follows that r2 + t2  0 (mod p), and since 0 < r2 + t2 < 2p, we must have
r2 + t2 = p. The (expected) running time of step 1 is O(len(p)3). The running
time of step 3 is O(len(p)2) (note that we can compute p in time O(len(p)2),

using the algorithm in Exercise 3.29). Thus, the total (expected) running time is
O(len(p)3).

Example 4.4. One can check that p := 1009 is prime and p  1 (mod 4). Let us
express p as a sum of squares using the above algorithm. First, we need to find a
4.6 Rational reconstruction and applications  89

square root of -1 modulo p. Let us just try a random number, say 17, and raise this
to the power (p - 1)/4 = 252. One can calculate that 17252  469 (mod 1009),
and 4692  -1 (mod 1009). So we were lucky with our first try. Now we run
the extended Euclidean algorithm on input p = 1009 and b = 469, obtaining the

following data:

i  ri qi  si  ti

0 1009    1   0

1 469 2   0   1

2 71 6 1 -2

3 43 1 -6 13

4 28 1 7 -15

5 15 1 -13 28

6 13 1 20 -43

7  2 6 -33 71

8  1 2 218 -469

9  0      -469 1009

The first rj that falls below the threshold r = 1009 + 1 = 32 is at j = 4, and so

we set r := 28 and t := -15. One verifies that r2 + t2 = 282 + 152 = 1009 = p. 2

   It is natural to ask whether one can solve this problem without resorting to ran-
domization. The answer is "yes" (see §4.8), but the only known deterministic
algorithms for this problem are quite impractical (albeit polynomial time). This
example illustrates the utility of randomization as an algorithm design technique,
one that has proved to be invaluable in solving numerous algorithmic problems
in number theory; indeed, in §3.4 we already mentioned its use in connection with
primality testing, and we will explore many other applications as well (after putting
the notion of a probabilistic algorithm on firm mathematical ground in Chapter 9).

                    4.6 Rational reconstruction and applications

In the previous section, we saw how to apply the extended Euclidean algorithm
to obtain an effective version of Thue's lemma. This lemma asserts that for given
integers n and b, there exists a pair of integers (r, t) satisfying r  bt (mod n),
and contained in a prescribed rectangle, provided the area of the rectangle is large
enough, relative to n. In this section, we first prove a corresponding uniqueness the-
orem, under the assumption that the area of the rectangle is not too large; of course,
if r  bt (mod n), then for any non-zero integer q, we also have rq  b(tq) (mod n),
and so we can only hope to guarantee that the ratio r/t is unique. After proving this
uniqueness theorem, we show how to make this theorem computationally effective,
and then develop several very neat applications.
90                             Euclid's algorithm

    The basic uniqueness statement is as follows:

Theorem 4.8. Let n, b, r, t  Z with r  0, t > 0, and n > 2rt. Further,
suppose that r, t, r , t  Z satisfy

                        r  bt (mod n), |r|  r, 0 < |t|  t,                                     (4.7)

                        r  bt (mod n), |r |  r, 0 < |t |  t.                                   (4.8)

Then r/t = r /t .

Proof. Consider the two congruences

                               r  bt (mod n),
                               r  bt (mod n).

Subtracting t times the second from t times the first, we obtain

                               rt - r t  0 (mod n).

However, we also have
                          |rt - r t|  |r||t | + |r ||t|  2rt < n.

Thus, rt -r t is a multiple of n, but less than n in absolute value; the only possibility
is that rt - r t = 0, which means r/t = r /t . 2

   Now suppose that we are given n, b, r, t  Z as in the above theorem; more-
over, suppose that there exist r, t  Z satisfying (4.7), but that these values are not
given to us. Note that under the hypothesis of Theorem 4.8, Thue's lemma cannot

be used to ensure the existence of such r and t, but in our eventual applications,

we will have other reasons that will guarantee this. We would like to find r , t  Z
satisfying (4.8), and if we do this, then by the theorem, we know that r/t = r /t .

We call this the rational reconstruction problem. We can solve this problem
efficiently using the extended Euclidean algorithm; indeed, just as in the case of
our effective version of Thue's lemma, the desired values of r and t appear as

"natural by-products" of that algorithm. To state the result precisely, let us recall

the notation we introduced in the last section: for integers a, b, with a  b  0, we

defined

                               EEA(a, b) := (ri, si, ti) i=0 +1,

where ri, si, and ti, for i = 0, . . . ,  + 1, are defined as in Theorem 4.3.

Theorem 4.9 (Rational reconstruction). Let n, b, r, t  Z with 0  b < n,
                                                         =                    +1
0    r   <  n,  and  t  >  0.  Further,  let  EEA(n, b)                           and  let  j  be  the
                                                            {(ri, si, ti)}i=0 ,
smallest index (among 0, . . . ,  + 1) such that rj  r, and set

                               r := rj, s := sj, and t := tj.
4.6 Rational reconstruction and applications                       91

Finally, suppose that there exist r, s, t  Z such that
                         r = ns + bt, |r|  r, and 0 < |t|  t.

Then we have:
    (i) 0 < |t |  t;
   (ii) if n > 2rt, then for some non-zero integer q,

                                   r = r q, s = s q, and t = t q.

Proof. Since r0 = n > r  0 = r+1, the value of j is well defined, and moreover,
j  1, and we have the inequalities

0  rj  r < rj-1, 0 < |tj|, |r|  r, and 0 < |t|  t,                 (4.9)

along with the identities

                           rj-1 = nsj-1 + btj-1,                   (4.10)
                             rj = nsj + btj,                       (4.11)
                              r = ns + bt.                         (4.12)

We now turn to part (i) of the theorem. Our goal is to prove that  (4.13)
                                         |tj|  t.

This is the hardest part of the proof. To this end, let

 := sjtj-1 - sj-1tj, µ := (tj-1s - sj-1t)/,  := (sjt - tjs)/.

Since  = ±1, the numbers µ and  are integers; moreover, one may easily verify
that they satisfy the equations

                           sjµ + sj-1 = s,                         (4.14)
                           tjµ + tj-1 = t.                         (4.15)

We now use these identities to prove (4.13). We consider three cases:
    (i) Suppose  = 0. In this case, (4.15) implies tj | t, and since t = 0, this
         implies |tj|  |t|  t.

   (ii) Suppose µ < 0. In this case, since tj and tj-1 have opposite sign, (4.15)
         implies |t| = |tjµ| + |tj-1|  |tj|, and so again, we have |tj|  |t|  t.

  (iii) The only remaining possibility is that  = 0 and µ  0. We argue that
         this is impossible. Adding n times (4.14) to b times (4.15), and using the
         identities (4.10), (4.11), and (4.12), we obtain

                                            rjµ + rj-1 = r.
92  Euclid's algorithm

         If  = 0 and µ and  had the same sign, we would have |r| = |rjµ|+|rj-1| 
         rj-1, and hence rj-1  |r|  r; however, this contradicts the fact that
         rj-1 > r.

   That proves the inequality (4.13). We now turn to the proof of part (ii) of the

theorem, which relies critically on this inequality. Assume that

                                 n > 2rt.        (4.16)

From (4.11) and (4.12), we have

    rj  btj (mod n) and r  bt (mod n).

Combining this with the inequalities (4.9), (4.13), and (4.16), we see that the
hypotheses of Theorem 4.8 are satisfied, and so we may conclude that

                                 rtj - rjt = 0.  (4.17)

   Subtracting tj times (4.12) from t times (4.11), and using the identity (4.17), we
obtain n(stj - sjt) = 0, and hence

                                 stj - sjt = 0.  (4.18)

From (4.18), we see that tj | sjt, and since gcd(sj, tj) = 1, we must have tj | t. So
t = tjq for some q, and we must have q = 0 since t = 0. Substituting tjq for t in
equations (4.17) and (4.18) yields r = rjq and s = sjq. That proves part (ii) of the

theorem. 2

   In our applications in this text, we shall only directly use part (ii) of this theorem;
however, part (i) has applications as well (see Exercise 4.18).

     4.6.1 Application: recovering fractions from their decimal expansions

It should be a familiar fact to the reader that every real number has a decimal
expansion, and that this decimal expansion is unique, provided one rules out those
expansions that end in an infinite run of 9's (e.g., 1/10 = 0.1000 · · · = 0.0999 · · ·).

   Now suppose that Alice and Bob play a game. Alice thinks of a rational number
z := s/t, where s and t are integers with 0  s < t, and tells Bob some of the high-
order digits in the decimal expansion of z. Bob's goal in the game is to determine
z. Can he do this?

   The answer is "yes," provided Bob knows an upper bound M on t, and provided
Alice gives Bob enough digits. Of course, Bob probably remembers from grade
school that the decimal expansion of z is ultimately periodic, and that given enough
digits of z so that the periodic part is included, he can recover z; however, this
technique is quite useless in practice, as the length of the period can be huge --
                 4.6 Rational reconstruction and applications  93

(M) in the worst case (see Exercises 4.21-4.23 below). The method we discuss
here requires only O(len(M)) digits.

   Suppose Alice gives Bob the high-order k digits of z, for some k  1. That is, if

                       z = 0 . z1z2z3 · · ·                    (4.19)

is the decimal expansion of z, then Alice gives Bob z1, . . . , zk. Now, if 10k is
much smaller than M2, the number z is not even uniquely determined by these
digits, since there are (M2) distinct rational numbers of the form s/t, with
0  s < t  M (see Exercise 1.33). However, if 10k > 2M2, then not only
is z uniquely determined by z1, . . . , zk, but using Theorem 4.9, Bob can efficiently
compute it.

   We shall presently describe efficient algorithms for both Alice and Bob, but

before doing so, we make a few general observations about the decimal expansion

of z. Let e be an arbitrary non-negative integer, and suppose that the decimal

expansion of z is as in (4.19). Observe that

                 10ez = z1 · · · ze . ze+1ze+2 · · · .

It follows that

                       10ez = z1 · · · ze . 0 .                (4.20)

Since z = s/t, if we set r := 10es mod t, then 10es = 10ez t + r, and dividing
this by t, we have 10ez = 10ez + r/t, where r/t  [0, 1). Therefore,

                 10es mod t = 0 . ze+1ze+2ze+3 · · · .         (4.21)
                       t

   Next, consider Alice. Based on the above discussion, Alice may use the follow-
ing simple, iterative algorithm to compute z1, . . . , zk, for arbitrary k  1, after she
chooses s and t:

x1  s
for i  1 to k do

      yi  10xi
      zi  yi/t
      xi+1  yi mod t
output z1, . . . , zk

Correctness follows easily from the observation that for each i = 1, 2, . . . , we
have xi = 10i-1s mod t; indeed, applying (4.21) with e = i - 1, we have xi/t =
0 . zizi+1zi+2 · · · , and consequently, by (4.20) with e = 1 and xi/t in the role of z,
we have 10xi/t = zi. The total time for Alice's computation is O(k len(M)),

since each loop iteration takes time O(len(M)).
94  Euclid's algorithm

   Finally, consider Bob. Given the high-order digits z1, . . . , zk of z = s/t, along
with the upper bound M on t, he can compute z as follows:

     1. Compute n  10k and b  ik=1 zi10k-i.
     2. Run the extended Euclidean algorithm on input n, b to obtain EEA(n, b),

         and then apply Theorem 4.9 with n, b, and r := t := M, to obtain the

         values r , s , t .

     3. Output the rational number -s /t .

   Let us analyze this algorithm, assuming that 10k > 2M2.
   For correctness, we must show that z = -s /t . To prove this, observe that by
(4.20) with e = k, we have b = nz = ns/t . Moreover, if we set r := ns mod t,

then we have

    r = ns - bt, 0  r < t  r, 0 < t  t, and n > 2rt.

It follows that the integers s , t from Theorem 4.9 satisfy s = s q and -t = t q
for some non-zero integer q. Thus, s/t = -s /t , as required. As a bonus, since
the extended Euclidean algorithm guarantees that gcd(s , t ) = 1, not only do we

obtain z, but we obtain z expressed as a fraction in lowest terms.

   We leave it to the reader to verify that Bob's computation may be performed in
time O(k2).

   We conclude that both Alice and Bob can successfully play this game with
k chosen so that k = O(len(M)), in which case, their algorithms run in time
O (len(M )2 ).

Example 4.5. Alice chooses integers s, t, with 0  s < t  1000, and tells
Bob the high-order seven digits in the decimal expansion of z := s/t, from
which Bob should be able to compute z. Suppose s = 511 and t = 710. Then
s/t = 0.7197183098591549 · · · . Bob receives the digits 7, 1, 9, 7, 1, 8, 3, and com-
putes n = 107 and b = 7197183. Running the extended Euclidean algorithm on

input n, b, Bob obtains the data in Fig. 4.1. The first rj that meets the threshold
r = 1000 is at j = 10, and Bob reads off s = 511 and t = -710, from which he
obtains z = -s /t = 511/710.

   Another interesting phenomenon to observe in Fig. 4.1 is that the fractions -si/ti
are very good approximations to the fraction b/n = 7197183/10000000; indeed,
if we compute the error terms b/n + si/ti for i = 1, . . . , 5, we get (approximately)

    0.72, -0.28, 0.053, -0.03, 0.0054.

Thus, we can approximate the "complicated" fraction 7197183/10000000 by the
"very simple" fraction 5/7, introducing an absolute error of less than 0.006. Exer-
cise 4.18 explores this "data compression" capability of Euclid's algorithm in more
generality. 2
    4.6 Rational reconstruction and applications            95

i   ri qi    si                                   ti

0 10000000   1                                    0

1 7197183 1  0                                    1

2 2802817 2  1                                    -1

3 1591549 1  -2                                   3

4 1211268 1  3                                    -4

5 380281 3   -5                                   7

6   70425 5  18    -25

7   28156 2  -95   132

8   14113 1  208   -289

9   14043 1  -303  421

10  70 200   511   -710

11  43 1 -102503 142421

12  27 1 103014 -143131

13  16 1 -205517 285552

14  11 1 308531 -428683

15  5 2 -514048 714235

16  1 5 1336627 -1857153

17  0        -7197183 10000000

Fig. 4.1. Bob's data from the extended Euclidean algorithm

                4.6.2 Application: Chinese remaindering with errors

One interpretation of the Chinese remainder theorem is that if we "encode" an
integer a, with 0  a < n, as the sequence (a1, . . . , ak), where ai = a mod ni for
i = 1, . . . , k, then we can efficiently recover a from this encoding. Here, of course,
n = n1 · · · nk, and the family {ni}ik=1 is pairwise relatively prime.

   Suppose that Alice encodes a as (a1, . . . , ak), and sends this encoding to Bob
over some communication network; however, because the network is not perfect,

during the transmission of the encoding, some (but hopefully not too many) of
the values a1, . . . , ak may be corrupted. The question is, can Bob still efficiently
recover the original a from its corrupted encoding?

   To make the problem more precise, suppose that the original, correct encod-

ing of a is (a1, . . . , ak), and the corrupted encoding is (b1, . . . , bk). Let us define
G  {1, . . . , k} to be the set of "good" positions i with ai = bi, and B  {1, . . . , k}
to be the set of "bad" positions i with ai = bi. We shall assume that |B|  , where

  is some specified parameter.

   Of course, if Bob hopes to recover a, we need to build some redundancy into

the system; that is, we must require that 0  a  M for some bound M that is
96  Euclid's algorithm

somewhat smaller than n. Now, if Bob knew the location of bad positions, and if

the product of the ni's at the good positions exceeds M, then Bob could simply
discard the errors, and reconstruct a by applying the Chinese remainder theorem to

the ai's and ni's at the good positions. However, in general, Bob will not know a
priori the locations of the bad positions, and so this approach will not work.

   Despite these apparent difficulties, Theorem 4.9 may be used to solve the prob-

lem quite easily, as follows. Let P be an upper bound on the product of any of the

integers n1, . . . , nk (e.g., we could take P to be the product of the largest numbers
among n1, . . . , nk). Further, let us assume that n > 2MP 2.

   Now, suppose Bob obtains the corrupted encoding (b1, . . . , bk). Here is what
Bob does to recover a:

    1. Apply the Chinese remainder theorem, obtaining the integer b satisfying
       0  b < n and b  bi (mod ni) for i = 1, . . . , k.

    2. Run the extended Euclidean algorithm on input n, b to obtain EEA(n, b),
       and then apply Theorem 4.9 with n, b, r := MP and t := P , to obtain

       values r , s , t .

    3. If t | r , output the integer r /t ; otherwise, output "error."

   We claim that the above procedure outputs a, under our assumption that the set B
of bad positions is of size at most . To see this, let t := iB ni. By construction,
we have 1  t  P . Also, let r := at, and note that 0  r  r and 0 < t  t. We

claim that

    r  bt (mod n).                                                               (4.22)

To show that (4.22) holds, it suffices to show that

                                       at  bt (mod ni)                           (4.23)

for all i = 1, . . . , k. To show this, for each index i we consider two cases:
Case 1: i  G. In this case, we have ai = bi, and therefore,

    at  ait  bit  bt (mod ni).

Case 2: i  B. In this case, we have ni | t, and therefore,
                                         at  0  bt (mod ni).

Thus, (4.23) holds for all i = 1, . . . , k, and so it follows that (4.22) holds. There-
fore, the values r , t obtained from Theorem 4.9 satisfy

                                       r = r = at = a.
                                       ttt
4.6 Rational reconstruction and applications  97

One easily checks that both the procedures to encode and decode a value a run in
time O(len(n)2).

   The above scheme is an example of an error correcting code, and is actually
the integer analog of a Reed-Solomon code.

Example 4.6. Suppose we want to encode a 1024-bit message as a sequence of 16-

bit blocks, so that the above scheme can correct up to 3 corrupted blocks. Without
any error correction, we would need just 1024/16 = 64 blocks. However, to correct

this many errors, we need a few extra blocks; in fact, 7 will do.

   Of course, a 1024-bit message can naturally be viewed as an integer a in the
set {0, . . . , 21024 - 1}, and the ith 16-bit block in the encoding can be viewed as
an integer ai in the set {0, . . . , 216 - 1}. Setting k := 71, we select k primes,

n1, . . . , nk, each 16-bits in length. In fact, let us choose n1, . . . , nk to be the largest
k primes under 216. If we do this, then the smallest prime among the ni's turns out
to be 64717, which is greater than 215.98. We may set M := 21024, and since we
want to correct up to 3 errors, we may set P := 23·16. Then with n := i ni, we
have

               n > 271·15.98 = 21134.58 > 21121 = 21+1024+6·16 = 2M P 2.

Thus, with these parameter settings, the above scheme will correct up to 3 cor-

rupted blocks. This comes at a cost of increasing the length of the message from
1024 bits to 71 · 16 = 1136 bits, an increase of about 11%. 2

                         4.6.3 Applications to symbolic algebra

Rational reconstruction also has a number of applications in symbolic algebra. We
briefly sketch one such application here. Suppose that we want to find the solution
v to the equation vA = w, where we are given as input a non-singular square
integer matrix A and an integer vector w. The solution vector v will, in general,
have rational entries. We stress that we want to compute the exact solution v, and
not some floating point approximation to it. Now, we could solve for v directly
using Gaussian elimination; however, the intermediate quantities computed by that
algorithm would be rational numbers whose numerators and denominators might
get quite large, leading to a rather lengthy computation (however, it is possible to
show that the overall running time is still polynomial in the input length).

   Another approach is to compute a solution vector modulo n, where n is a power
of a prime that does not divide the determinant of A. Provided n is large enough,
one can then recover the solution vector v using rational reconstruction. With this
approach, all of the computations can be carried out using arithmetic on integers
not too much larger than n, leading to a more efficient algorithm. More of the
details of this procedure are developed later, in Exercise 14.18.
98                           Euclid's algorithm

EXERCISE  4.18.  Let n, b    Z with 0    b  <  n, and let EEA(n, b)  =                    +1

                                                                        {(ri, si, ti)}i=0 .

This exercise develops some key properties of the fractions -si/ti as approxima-
tions to b/n. For i = 1, . . . ,  + 1, let i := b/n + si/ti.

    (a) Show that i = ri /tin for i = 1, . . . ,  + 1.

    (b) Show that successive i's strictly decrease in absolute value, and alternate
         in sign.

    (c) Show that |i| < 1/t2i for i = 1, . . . , , and +1 = 0.
    (d) Show that for all s, t  Z with t = 0, if |b/n - s/t| < 1/2t2, then

         s/t = -si/ti for some i = 1, . . . ,  + 1. Hint: use part (ii) of Theorem 4.9.
    (e) Consider a fixed index i  {2, . . . ,  + 1}. Show that for all s, t  Z, if

         0 < |t|  |ti| and |b/n - s/t|  |i|, then s/t = -si/ti. In this sense, -si/ti
         is the unique, best approximation to b/n among all fractions of denominator
         at most |ti|. Hint: use part (i) of Theorem 4.9.

EXERCISE 4.19. Using the decimal approximation   3.141592654, apply

Euclid's algorithm to calculate a rational number of denominator less than 1000
that is within 10-6 of . Illustrate the computation with a table as in Fig. 4.1.

EXERCISE 4.20. Show that given integers s, t, k, with 0  s < t, and k > 0, we
can compute the kth digit in the decimal expansion of s/t in time O(len(k) len(t)2).

   For the following exercises, we need a definition. Let  = {zi}i=1 be a sequence
of elements drawn from some arbitrary set. For integers k  0 and  1, we say
that  is (k, )-periodic if zi = zi+ for all i > k; in addition, we say that  is
ultimately periodic if it is (k, )-periodic for some (k, ).

EXERCISE 4.21. Show that if a sequence  is ultimately periodic, then it is
(k, )-periodic for some uniquely determined pair (k, ) for which the follow-
ing holds: for every pair (k, ) such that  is (k, )-periodic, we have k  k and
 | .

   The value  in the above exercise is called the period of , and k is called the
pre-period of . If its pre-period is zero, then  is called purely periodic.

EXERCISE 4.22. Let z be a real number whose decimal expansion is an ultimately
periodic sequence. Show that z is rational.

EXERCISE 4.23. Let z = s/t  Q, where s and t are relatively prime integers with
0  s < t. Show that:

    (a) there exist integers k, k such that 0  k < k and s10k  s10k (mod t);

   (b) for all integers k, k with 0  k < k , the decimal expansion of z is
         (k, k - k)-periodic if and only if s10k  s10k (mod t);
4.7 The RSA cryptosystem  99

(c) if gcd(10, t) = 1, then the decimal expansion of z is purely periodic with
     period equal to the multiplicative order of 10 modulo t;

(d) more generally, if k is the smallest non-negative integer such that 10 and
     t := t/ gcd(10k, t) are relatively prime, then the decimal expansion of z is

     ultimately periodic with pre-period k and period equal to the multiplicative

     order of 10 modulo t .

   A famous conjecture of Artin postulates that for every integer d, not equal to -1
or to the square of an integer, there are infinitely many primes t such that d has
multiplicative order t - 1 modulo t. If Artin's conjecture is true, then by part (c)
of the previous exercise, there are infinitely many primes t such that the decimal
expansion of s/t, for every s with 0 < s < t, is a purely periodic sequence of period
t - 1. In light of these observations, the "grade school" method of computing a
fraction from its decimal expansion using the period is hopelessly impractical.

                                4.7 The RSA cryptosystem

One of the more exciting uses of number theory in recent decades is its application
to cryptography. In this section, we give a brief overview of the RSA cryptosystem,
named after its inventors Rivest, Shamir, and Adleman. At this point in the text,
we already have the concepts and tools at our disposal necessary to understand the
basic operation of this system, even though a full understanding of the system will
require other ideas that will be developed later in the text.

   Suppose that Alice wants to send a secret message to Bob over an insecure net-
work. An adversary may be able to eavesdrop on the network, and so sending the
message "in the clear" is not an option. Using older, more traditional cryptographic
techniques would require that Alice and Bob share a secret key between them;
however, this creates the problem of securely generating such a shared secret. The
RSA cryptosystem is an example of a public key cryptosystem. To use the system,
Bob simply places a "public key" in the equivalent of an electronic telephone book,
while keeping a corresponding "private key" secret. To send a secret message to
Bob, Alice obtains Bob's public key from the telephone book, and uses this to
encrypt her message. Upon receipt of the encrypted message, Bob uses his private
key to decrypt it, obtaining the original message.

   Here is how the RSA cryptosystem works. To generate a public key/private key
pair, Bob generates two very large, random primes p and q, with p = q. To be
secure, p and q should be quite large; in practice, they are chosen to be around 512
bits in length. Efficient algorithms for generating such primes exist, and we shall
discuss them in detail later in the text (that there are sufficiently many primes of a
given bit length will be discussed in Chapter 5; algorithms for generating them will
100                            Euclid's algorithm

be discussed at a high level in §9.4, and in greater detail in Chapter 10). Next, Bob
computes n := pq. Bob also selects an integer e > 1 such that gcd(e, (n)) = 1,
where  is Euler's phi function. Here, (n) = (p-1)(q-1). Finally, Bob computes
d := e-1 mod (n), using the extended Euclidean algorithm. The public key is the

pair (n, e), and the private key is the pair (n, d). The integer e is called the "encryp-

tion exponent" and d is called the "decryption exponent." In practice, the integers

n and d are about 1024 bits in length, while e is usually significantly shorter.

After Bob publishes his public key (n, e), Alice may send a secret message to

Bob as follows. Suppose that a message is encoded in some canonical way as a

number between 0 and n - 1 -- we can always interpret a bit string of length less

than len(n) as such a number. Thus, we may assume that a message is an element
 of Zn. To encrypt the message , Alice simply computes  := e using repeated
squaring. The encrypted message is . When Bob receives , he computes  := d,

and interprets  as a message.

   The most basic requirement of any encryption scheme is that decryption should
"undo" encryption. In this case, this means that for all   Zn, we should have

                               (e)d = .            (4.24)

If   Zn, then this is clearly the case, since we have ed = 1 + (n)k for some
positive integer k, and hence by Euler's theorem (Theorem 2.13), we have

     (e)d = ed = 1+(n)k =  · (n)k = .

To argue that (4.24) holds in general, let  be an arbitrary element of Zn, and
suppose  = [a]n. If a  0 (mod p), then trivially aed  0 (mod p); otherwise,

                         aed  a1+(n)k  a · a(n)k  a (mod p),

where the last congruence follows from the fact that (n)k is a multiple of p - 1,
which is a multiple of the multiplicative order of a modulo p (again by Euler's the-
orem). Thus, we have shown that aed  a (mod p). The same argument shows that
aed  a (mod q), and these two congruences together imply that aed  a (mod n).
Thus, we have shown that equation (4.24) holds for all   Zn.

   Of course, the interesting question about the RSA cryptosystem is whether or not
it really is secure. Now, if an adversary, given only the public key (n, e), were able
to factor n, then he could easily compute the decryption exponent d himself using
the same algorithm used by Bob. It is widely believed that factoring n is computa-
tionally infeasible, for sufficiently large n, and so this line of attack is ineffective,
barring a breakthrough in factorization algorithms. Indeed, while trying to factor
n by brute-force search is clearly infeasible, there are much faster algorithms, but
even these are not fast enough to pose a serious threat to the security of the RSA
4.7 The RSA cryptosystem  101

cryptosystem. We shall discuss some of these faster algorithms in some detail later
in the text (in Chapter 15).

   Can one break the RSA cryptosystem without factoring n? For example, it is
natural to ask whether one can compute the decryption exponent d without having
to go to the trouble of factoring n. It turns out that the answer to this question is
"no": if one could compute the decryption exponent d, then ed - 1 would be a
multiple of (n), and as we shall see later in §10.4, given any multiple of (n),
we can easily factor n. Thus, computing the decryption exponent is equivalent to
factoring n, and so this line of attack is also ineffective. But there still could be
other lines of attack. For example, even if we assume that factoring large numbers
is infeasible, this is not enough to guarantee that for a given encrypted message ,
the adversary is unable to compute d (although nobody actually knows how to do
this without first factoring n).

   The reader should be warned that the proper notion of security for an encryp-
tion scheme is quite subtle, and a detailed discussion of this is well beyond the
scope of this text. Indeed, the simple version of RSA presented here suffers from a
number of security problems (because of this, actual implementations of public-
key encryption schemes based on RSA are somewhat more complicated). We
mention one such problem here (others are examined in some of the exercises
below). Suppose an eavesdropping adversary knows that Alice will send one of
a few, known, candidate messages. For example, an adversary may know that
Alice's message is either "let's meet today" or "let's meet tomorrow." In this case,
the adversary can encrypt for himself each of the candidate messages, intercept
Alice's actual encrypted message, and then by simply comparing encryptions, the
adversary can determine which particular message Alice encrypted. This type of
attack works simply because the encryption algorithm is deterministic, and in fact,
any deterministic encryption algorithm will be vulnerable to this type of attack. To
avoid this type of attack, one must use a probabilistic encryption algorithm. In the
case of the RSA cryptosystem, this is often achieved by padding the message with
some random bits before encrypting it (but even this must be done carefully).

EXERCISE 4.24. This exercise develops a method to speed up RSA decryption.
Suppose that we are given two distinct -bit primes, p and q, an element   Zn,
where n := pq, and an integer d, where 1 < d < (n). Using the algorithm from
Exercise 3.35, we can compute d at a cost of essentially 2 squarings in Zn. Show
how this can be improved, making use of the factorization of n, so that the total cost
is essentially that of squarings in Zp and squarings in Zq, leading to a roughly
four-fold speed-up in the running time.

EXERCISE 4.25. Alice submits a bid to an auction, and so that other bidders cannot
102  Euclid's algorithm

see her bid, she encrypts it under the public key of the auction service. Suppose
that the auction service provides a public key for an RSA encryption scheme, with
a modulus n. Assume that bids are encoded simply as integers between 0 and n - 1
prior to encryption. Also, assume that Alice submits a bid that is a "round number,"
which in this case means that her bid is a number that is divisible by 10. Show how
an eavesdropper can submit an encryption of a bid that exceeds Alice's bid by 10%,
without even knowing what Alice's bid is. In particular, your attack should work
even if the space of possible bids is very large.

EXERCISE 4.26. To speed up RSA encryption, one may choose a very small

encryption exponent. This exercise develops a "small encryption exponent attack"

on RSA. Suppose Bob, Bill, and Betty have RSA public keys with moduli n1, n2,
and n3, and all three use encryption exponent 3. Assume that {ni}i3=1 is pairwise
relatively prime. Suppose that Alice sends an encryption of the same message to

Bob, Bill, and Betty -- that is, Alice encodes her message as an integer a, with
0  a < min{n1, n2, n3}, and computes the three encrypted messages i := [a3]ni,
for i = 1, . . . , 3. Show how to recover Alice's message from these three encrypted

messages.

EXERCISE 4.27. To speed up RSA decryption, one might choose a small decryp-

tion exponent, and then derive the encryption exponent from this. This exercise
develops a "small decryption exponent attack" on RSA. Suppose n = pq, where
p and q are distinct primes with len(p) = len(q). Let d and e be integers such

that 1 < d < (n), 1 < e < (n), and de  1 (mod (n)). Further, assume
that d < n1/4/3. Show how to efficiently compute d, given n and e. Hint: since
ed  1 (mod (n)), it follows that ed = 1 + (n)k for an integer k with 0 < k < d;
let r := nk - ed, and show that |r| < n3/4; next, show how to recover d (along with

r and k) using Theorem 4.9.

                                           4.8 Notes

The Euclidean algorithm as we have presented it here is not the fastest known
algorithm for computing greatest common divisors. The asymptotically fastest
known algorithm for computing the greatest common divisor of two numbers of
bit length at most runs in time O( len( )) on a RAM, which is due to Schönhage
[85]. The same algorithm leads to Boolean circuits of size O( len( )2 len(len( ))),
which using Fürer's result [38], can be reduced to O( len( )2 2O(log n)). The same
complexity results also hold for the extended Euclidean algorithm, as well as for
Chinese remaindering, Thue's lemma, and rational reconstruction.

   Experience suggests that such fast algorithms for greatest common divisors are
not of much practical value, unless the integers involved are very large -- at least
4.8 Notes  103

several tens of thousands of bits in length. The extra "log" factor and the rather
large multiplicative constants seem to slow things down too much.

   The binary gcd algorithm (Exercise 4.6) is due to Stein [100]. The extended
binary gcd algorithm (Exercise 4.10) was first described by Knuth [56], who
attributes it to M. Penk. Our formulation of both of these algorithms closely follows
that of Menezes, van Oorschot, and Vanstone [66]. Experience suggests that the
binary gcd algorithm is faster in practice than Euclid's algorithm.

   Schoof [87] presents (among other things) a deterministic, polynomial-time
algorithm that computes a square root of -1 modulo p for any given prime p 
1 (mod 4). If we use this algorithm in §4.5, we get a deterministic, polynomial-
time algorithm to compute integers r and t such that p = r2 + t2.

   Our Theorem 4.9 is a generalization of one stated in Wang, Guy, and Davenport
[103]. One can generalize Theorem 4.9 using the theory of continued fractions.
With this, one can generalize Exercise 4.18 to deal with rational approximations to
irrational numbers. More on this can be found, for example, in the book by Hardy
and Wright [46].

   The application of Euclid's algorithm to computing a rational number from the
first digits of its decimal expansion was observed by Blum, Blum, and Shub [17],
where they considered the possibility of using such sequences of digits as a pseudo-
random number generator -- the conclusion, of course, is that this is not such a
good idea.

   The RSA cryptosystem was invented by Rivest, Shamir, and Adleman [82].
There is a vast literature on cryptography. One starting point is the book by
Menezes, van Oorschot, and Vanstone [66]. The attack in Exercise 4.27 is due
to Wiener [110]; this attack was recently strengthened by Boneh and Durfee [19].
                             5

                 The distribution of primes

This chapter concerns itself with the question: how many primes are there? In
Chapter 1, we proved that there are infinitely many primes; however, we are inter-
ested in a more quantitative answer to this question; that is, we want to know how
"dense" the prime numbers are.

   This chapter has a bit more of an "analytical" flavor than other chapters in this
text. However, we shall not make use of any mathematics beyond that of elemen-
tary calculus.

                 5.1 Chebyshev's theorem on the density of primes
The natural way of measuring the density of primes is to count the number of
primes up to a bound x, where x is a real number. To this end, we introduce
the function (x), whose value at each real number x  0 is defined to be the
number of primes up to (and including) x. For example, (1) = 0, (2) = 1,
and (7.5) = 4. The function (x) is an example of a "step function," that is, a
function that changes values only at a discrete set of points. It might seem more
natural to define (x) only on the integers, but it is the tradition to define it over
the real numbers (and there are some technical benefits in doing so).

   Let us first take a look at some values of (x). Table 5.1 shows values of (x) for
x = 103i and i = 1, . . . , 6. The third column of this table shows the value of x/(x)
(to five decimal places). One can see that the differences between successive rows
of this third column are roughly the same -- about 6.9 -- which suggests that the
function x/(x) grows logarithmically in x. Indeed, as log(103)  6.9, it would
not be unreasonable to guess that x/(x)  log x, or equivalently, (x)  x/ log x
(as discussed in the Preliminaries, log x denotes the natural logarithm of x).

   The following theorem is a first -- and important -- step towards making the
above guesswork more rigorous (the statements of this and many other results in
this chapter make use of the asymptotic notation introduced in §3.1):

                                               104
               5.1 Chebyshev's theorem on the density of primes           105

                          Table 5.1. Some values of (x)

                       x                     (x) x/(x)

                    103                      168 5.95238

                    106            78498 12.73918

                    109            50847534 19.66664

                    1012  37607912018 26.59015

                    1015  29844570422669 33.50693

                    1018 24739954287740860 40.42045

Theorem 5.1 (Chebyshev's theorem). We have

                                     (x) = (x/ log x).

   It is not too difficult to prove this theorem, which we now proceed to do in several
steps. We begin with some elementary bounds on binomial coefficients (see §A2):

Lemma 5.2. If m is a positive integer, then  2m + 1  < 22m.
                         2m  22m /2m and        m
                          m

Proof. As  2m  is the largest binomial coefficient in the binomial expansion of
           m
(1 + 1)2m, we have

22m =  2m 2m        =1+   2m-1 2m  + 1  2 + (2m - 1)     2m       2m  2m  .

       i=0 i              i=1 i                           m           m

The proves the first inequality. For the second, observe that the binomial coefficient
2m+1
  m   occurs twice in the binomial expansion of (1 + 1)2m+1, and is therefore less

than 22m+1/2 = 22m. 2

   Next, recalling that p(n) denotes the power to which a prime p divides an integer
n, we continue with the following observation:

Lemma 5.3. Let n be a positive integer. For every prime p, we have

                                     p(n!) = n/pk .

                                                                  k1

Proof. For all positive integers j, k, define djk := 1 if pk | j, and djk := 0,
otherwise. Observe that p(j) = k1 djk (this sum is actually finite, since djk = 0
106                           The distribution of primes

for all sufficiently large k). So we have

                              n                     n                                 n

              p(n!) = p(j) =                                     djk =                   djk .

                        j=1                      j=1 k1                  k1 j=1

Finally, note that jn=1 djk is equal to the number of multiples of pk among the
integers 1, . . . , n, which by Exercise 1.3 is equal to n/pk . 2

The following theorem gives a lower bound on (x).

Theorem 5.4.  (n)    1  (log  2)n/    log     n  for   every     integer           n        2.
                     2

Proof. Let m be a positive integer, and consider the binomial coefficient

                                 N := 2mm = (m!)2 (2m)! .

It is clear that N is divisible only by primes p up to 2m. Applying Lemma 5.3 to
the identity N = (2m)!/(m!)2, we have

                     p(N ) = ( 2m/pk - 2 m/pk ).

                                    k1

Each term in this sum is either 0 or 1 (see Exercise 1.4), and for k > log(2m)/ log p,
each term is zero. Thus, p(N)  log(2m)/ log p. So we have

              (2m) log(2m) =                           log(2m)
                                                                 log p
                                         p2m log p

                                          p(N) log p = log N,

                                         p2m

where the summations are over the primes p up to 2m. By Lemma 5.2, we have
N  22m /2m  2m, and hence

                   (2m) log(2m)               m log 2         =  1  (  log   2  )  (  2m    )  .
                                                                 2

That proves the theorem for even n. Now consider odd n  3, so n = 2m - 1 for

some m  2. It is easily verified that the function x/ log x is increasing for x  3;

therefore,

              (2m - 1) = (2m)

                                   1  (  log  2  )  (  2m  )  /  log  (  2m  )
                                   2

                                   1  (  log  2  )  (  2m  -     1)/ log(2m              -  1).
                                   2

That proves the theorem for odd n. 2
                    5.1 Chebyshev's theorem on the density of primes                             107

As a consequence of the above theorem, we have (x) = (x/ log x) for real

numbers  x.  Indeed,  setting  c  :=  1  (  log  2  )  ,  for  every  real  number  x    2,  we  have
                                      2

                   (x) = ( x )  c x / log x  c(x - 1)/ log x;

from this, it is clear that (x) = (x/ log x).
   To obtain a corresponding upper bound for (x), we introduce an auxiliary func-

tion, called Chebyshev's theta function:

                                      (x) := log p,

                                                   px

where the sum is over all primes p up to x.
   Chebyshev's theta function is an example of a summation over primes, and in

this chapter, we will be considering a number of functions that are defined in terms
of sums or products over primes (and indeed, such summations already cropped up
in the proof of Theorem 5.4). To avoid excessive tedium, we adopt the usual con-
vention used by number theorists: if not explicitly stated, summations and products
over the variable p are always understood to be over primes. For example, we may
write (x) = px 1.

Theorem 5.5. We have

                                  (x) = ((x) log x).

Proof. On the one hand, we have

                    (x) = log p  log x 1 = (x) log x.

                               px                            px

On the other hand, we have

(x) = log p                              log p            1  log x           1
                                                          2

                px        x1/2<px                                   x1/2<px

             =  1  log x  (x) - (x1/2)              = 12 1 - (x1/2)/(x)             (x) log x.
                2

It will therefore suffice to show that (x1/2)/(x) = o(1). Clearly, (x1/2)  x1/2.
Moreover, by the previous theorem, (x) = (x/ log x). Therefore,

                          (x1/2)/(x) = O(log x/x1/2) = o(1),

and the theorem follows. 2

Theorem 5.6. (x) < 2(log 2)x for every real number x  1.

Proof. It suffices to prove that (n) < 2(log 2)n for every positive integer n, since
then (x) = ( x ) < 2(log 2) x  2(log 2)x. We prove this by induction on n.
108                   The distribution of primes

For n = 1 and n = 2, this is clear, so assume n > 2. If n is even, then using the
induction hypothesis for n - 1, we have

     (n) = (n - 1) < 2(log 2)(n - 1) < 2(log 2)n.

Now consider the case where n is odd. Write n = 2m + 1, where m is a positive
integer, and consider the binomial coefficient

     M := 2m + 1 = (2m + 1) · · · (m + 2) .
                      m                m!

Observe that M is divisible by all primes p with m + 1 < p  2m + 1. Moreover,
be Lemma 5.2, we have M < 22m. It follows that

     (2m + 1) - (m + 1) =             log p  log M < 2(log 2)m.

                           m+1<p2m+1

Using this, and the induction hypothesis for m + 1, we obtain

     (n) = (2m + 1) - (m + 1) + (m + 1)
           < 2(log 2)m + 2(log 2)(m + 1) = 2(log 2)n. 2

   Another way of stating the above theorem is:

                                               p < 4x.

                                                           px

   Theorem 5.1 follows immediately from Theorems 5.4, 5.5 and 5.6. Note that we
have also proved:

Theorem 5.7. We have

                           (x) = (x).

EXERCISE 5.1. For each positive integer n, let pn denote the nth prime. Show that
pn = (n log n).

EXERCISE 5.2. For each positive integer n, let (n) denote the number of distinct
primes dividing n. Show that (n) = O(log n/ log log n).

EXERCISE 5.3. Show that px 1/ log p = (x/(log x)2).

                                 5.2 Bertrand's postulate

Suppose we want to know how many primes there are of a given bit length, or
more generally, how many primes there are between m and 2m for a given positive
integer m. Neither the statement, nor our proof, of Chebyshev's theorem imply that
5.2 Bertrand's postulate                                                   109

there are any primes between m and 2m, let alone a useful density estimate of such
primes.

   Bertrand's postulate is the assertion that for every positive integer m, there
exists a prime between m and 2m. We shall in fact prove a stronger result: there is
at least one prime between m and 2m, and moreover, the number of such primes is
(m/ log m).

Theorem 5.8 (Bertrand's postulate). For every positive integer m, we have

(2m) - (m) >                         m          .

                                     3 log(2m)

The proof uses Theorem 5.6, along with a more careful re-working of the proof

of Theorem 5.4. The theorem is clearly true for m  2, so we may assume that

m  3. As in the proof of the Theorem 5.4, define N :=  2m  , and recall that N  is
                                                       m

divisible only by primes less than 2m, and that we have the identity

p(N ) = ( 2m/pk - 2 m/pk ),                                                (5.1)

               k1

where each term in the sum is either 0 or 1. We can characterize the values p(N)
a bit more precisely, as follows:

Lemma 5.9. Let m  3 and N :=  2m  .  For all primes  p, we have:
                              m

pp(N)  2m;                                                                 (5.2)
                                                                           (5.3)

if p > 2m, then p(N)  1;

if 2m/3 < p  m, then p(N) = 0;                                             (5.4)

if m < p < 2m, then p(N) = 1.                                              (5.5)

Proof. For (5.2), all terms with k > log(2m)/ log p in (5.1) vanish, and hence
p(N )  log(2m)/ log p, from which it follows that pp(N)  2m.

   (5.3) follows immediately from (5.2).

   For (5.4), if 2m/3 < p  m, then 2m/p < 3, and we must also have p  3,
since p = 2 implies m < 3. We have p2 > p(2m/3) = 2m(p/3)  2m, and hence
all terms with k > 1 in (5.1) vanish. The term with k = 1 also vanishes, since
1  m/p < 3/2, from which it follows that 2  2m/p < 3, and hence m/p = 1
and 2m/p = 2.

   For (5.5), if m < p < 2m, it follows that 1 < 2m/p < 2, so 2m/p = 1. Also,
m/p < 1, so m/p = 0. It follows that the term with k = 1 in (5.1) is 1, and it is
clear that 2m/pk < 1 for all k > 1, and so all the other terms vanish. 2
110                       The distribution of primes

We now have the necessary technical ingredients to prove Theorem 5.8. Define

                          Pm :=            p,

                                   m<p<2m

and define Qm so that

                                 N = QmPm.

By (5.4) and (5.5), we see that

                          Qm =     pp (N ) .

                                                              p2m/3

                                                                   
Moreover, by (5.3), p(N) > 1 for at most those p  2m, so there are at most

  2m such primes, and by (5.2), the contribution of each such prime to the above

product is at most 2m. Combining this with Theorem 5.6, we obtain

                                                                    

                                   Qm < (2m) 2m · 42m/3.

We now apply Lemma 5.2, obtaining

     Pm                                                                          

         = NQ-1  22m(2m)-1Q-1 > 4m/3(2m)-(1+ 2m).
                       m           m

It follows that
            (2m) - (m)  log Pm/ log(2m) > m log 4 - (1 + 2m)
                                                         3 log(2m)
                              = m + m(log 4 - 1) - (1 + 2m).
                                  3 log(2m) 3 log(2m)

Clearly, for all sufficiently large m, we have

                                  m(log 4 - 1) > 1 + 2m. (5.6)
                                   3 log(2m)

That proves Theorem 5.8 for all sufficiently large m. Moreover, a simple calcula-
tion shows that (5.6) holds for all m  13,000, and one can verify by brute force
(with the aid of a computer) that the theorem holds for m < 13,000.

                                   5.3 Mertens' theorem

Our next goal is to prove the following theorem, which turns out to have a number
of applications.

Theorem 5.10. We have

                              1 = log log x + O(1).
                          px p
                             5.3 Mertens' theorem                           111

   The proof of this theorem, while not difficult, is a bit technical, and we proceed
in several steps.

Theorem 5.11. We have

                              log p = log x + O(1).
                          px p

Proof. Let n := x . The idea of the proof is to estimate log(n!) in two different
ways. By Lemma 5.3, we have

log(n!) =               n/pk log p = n/p log p +              n/pk log p.

               pn k1             pn                  k2 pn

We next show that the last sum is O(n). We have

               log p n/pk     n log p p-k

           pn       k2       pn      k2

                             = n log p · 1 = n log p
                                  pn p2 1 - 1/p pn p(p - 1)

                              n log k = O(n).
                                 k2 k(k - 1)

Thus, we have shown that

                             log(n!) = n/p log p + O(n).

                                                         pn

Since n/p = n/p + O(1), applying Theorem 5.6 (and Exercise 3.12), we obtain

log(n!) = (n/p) log p + O log p + O(n) = n log p + O(n).                    (5.7)
                                                     pn p
           pn                pn

We can also estimate log(n!) by estimating a sum by an integral (see §A5):

               n          n

log(n!) = log k = log t dt + O(log n) = n log n - n + O(log n).             (5.8)

               k=1        1

Combining (5.7) and (5.8), and noting that log x-log n = o(1) (see Exercise 3.11),

we obtain

                        log p = log n + O(1) = log x + O(1),
                    px p

which proves the theorem. 2

   We shall also need the following theorem, which is a very useful tool in its own
right; it is essentially a discrete variant of "integration by parts."
112                       The distribution of primes

Theorem 5.12 (Abel's identity). Let {ci}i=k be a sequence of real numbers, and
for each real number t, define

                                C(t) := ci.

                                             kit

Further, suppose that f (t) is a function with a continuous derivative f (t) on the

interval [k, x], where x is a real number, with x  k. Then

                                            x

                cif (i) = C(x)f (x) - C(t)f (t) dt.

           kix                              k

   Note that since C(t) is a step function, the integrand C(t)f (t) is piece-wise
continuous on [k, x], and hence the integral is well defined (see §A4).

Proof. Let n := x . We have

     n                       n

          cif (i) = C(k)f (k) + [C(i) - C(i - 1)]f (i)

     i=k                     i=k+1

               n-1

           = C(i)[f (i) - f (i + 1)] + C(n)f (n)

           i=k

               n-1

           = C(i)[f (i) - f (i + 1)] + C(n)[f (n) - f (x)] + C(x)f (x).

           i=k

Observe that for i = k, . . . , n - 1, we have C(t) = C(i) for all t  [i, i + 1), and so

                                      i+1               i+1

          C(i)[f (i) - f (i + 1)] = -C(i) f (t) dt = - C(t)f (t) dt;

                                      i                 i

likewise,

                                                          x

                C(n)[f (n) - f (x)] = - C(t)f (t) dt,

                                                          n

from which the theorem directly follows. 2

Proof of Theorem 5.10. For i  2, set

                ci := (log i)/i if i is prime,
                                0          otherwise.

By Theorem 5.11, we have

           C(t) := ci = log p = log t + R(t),
                                      pt p
                          2it
                           5.3 Mertens' theorem                          113

where R(t) = O(1). Applying Theorem 5.12 with f (t) := 1/ log t (and using

Exercise 3.13), we obtain

                 1  =                 C(x) x C(t)
                           cif (i) =       +            2 dt
px p 2ix                              log x 2 t(log t)

                    =1+    R(x) x dt       +     x R(t)
                           +                                 2 dt
                           log x 2 t log t 2 t(log t)

                    = 1 + O(1/ log x) + (log log x - log log 2) + O(1)

                    = log log x + O(1). 2

Using Theorem 5.10, we can easily show the following:

Theorem 5.13 (Mertens' theorem). We have
                                    (1 - 1/p) = (1/ log x).

                                            px

Proof. Using parts (i) and (iii) of §A1, for any fixed prime p, we have

                           - p21  1p + log(1 - 1/p)  0. (5.9)

Moreover, since

                               1              1
                                              i2 < ,
                           px  p2     i2

summing the inequality (5.9) over all primes p  x yields

                           -C  1 + log g(x)  0,
                                   px p

where C is a positive constant, and g(x) := px(1 - 1/p). From this, and
from Theorem 5.10, we obtain log g(x) = - log log x + O(1), which implies that
g(x) = (1/ log x) (see Exercise 3.11). That proves the theorem. 2

EXERCISE 5.4. For each positive integer k, let Pk denote the product of the first k
primes. Show that (Pk) = (Pk/ log log Pk) (here,  is Euler's phi function).

EXERCISE 5.5. The previous exercise showed that (n) could be as small as

(about) n/ log log n for infinitely many n. Show that this is the "worst case," in
the sense that (n) = (n/ log log n).

EXERCISE 5.6. Show that for every positive integer constant k,

                       x dt =         x +O            x
                                                        k+1 .
                                           k
                       2 (log t)k (log x)        (log x)
114                The distribution of primes

This fact may be useful in some of the following exercises.

EXERCISE 5.7. Use Chebyshev's theorem and Abel's identity to prove a stronger
version of Theorem 5.5: (x) = (x) log x + O(x/ log x).

EXERCISE 5.8. Use Chebyshev's theorem and Abel's identity to show that
                                   1 = (x) + O(x/(log x)3).

                            px log p log x

EXERCISE 5.9. Show that
                                    (1 - 2/p) = (1/(log x)2).

                                        2<px

EXERCISE 5.10. Show that if (x)  cx/ log x for some constant c, then we must
have c = 1.

EXERCISE 5.11. Strengthen Theorem 5.10: show that for some constant A, we
have px 1/p = log log x + A + o(1). You do not need to estimate A, but in fact
A  0.261497212847643.

EXERCISE 5.12. Use the result from the previous exercise to strengthen Mertens'

theorem: show that for some constant B1, we have px(1 - 1/p)  B1/(log x).
You do not need to estimate B1, but in fact B1  0.561459483566885.

EXERCISE 5.13. Strengthen the result of Exercise 5.9: show that for some con-
stant B2, we have

                                     (1 - 2/p)  B2/(log x)2.

                                          2<px

You do not need to estimate B2, but in fact B2  0.832429065662.

EXERCISE 5.14. Use Abel's identity to derive Euler's summation formula: if
f (t) has a continuous derivative f (t) on the interval [a, b], where a and b are
integers, then

     b    b                                  b

          f (i) - f (t) dt = f (a) + (t - t )f (t) dt.

     i=a  a                                  a

EXERCISE 5.15. Use Euler's summation formula (previous exercise) to show that

          log(n!)  =  n  log  n  -  n  +  1  log  n  +  O(1),
                                          2

and from this, conclude that n! = ((n/e)nn). This is a weak form of Stirling's
approximation; a sharper form states that n!  (n/e)n 2n.
5.4 The sieve of Eratosthenes                            115

EXERCISE 5.16. Use Stirling's approximation (previous exercise) to show that
                                      2m = (22m/m).
                                       m

5.4 The sieve of Eratosthenes

As an application of Theorem 5.10, consider the sieve of Eratosthenes. This is
an algorithm that generates all the primes up to a given bound n. It uses an array
A[2 . . . n], and runs as follows.

for k  2 to n do A[k]  1
                 

for k  2 to n do
      if A[k] = 1 then

             i  2k

             while i  n do
                    A[i]  0, i  i + k

   When the algorithm finishes, we have A[k] = 1 if and only if k is prime, for
k = 2, . . . , n. This can easily be proven using the fact (see Exercise 1.2) that a
composite number k between 2 and n must be divisible by a prime that is at most


  n, and by proving by induction on k that at the beginning of each iteration of
the main loop, A[i] = 0 if and only if i is divisible by a prime less than k, for
i = k, . . . , n. We leave the details of this to the reader.

   We are more interested in the running time of the algorithm. To analyze the
running time, we assume that all arithmetic operations take constant time; this
is reasonable, since all the numbers computed are used as array indices and thus
should fit in single machine words. Therefore, we can assume that built-in arith-
metic instructions are used for operating on such numbers.

   Every time we execute the inner loop of the algorithm, we perform O(n/k) steps
to clear the entries of A indexed by multiples of k. Pessimistically, then, we could
bound the total running time by O(n T (n)), where

T (n) := 1/k.

                  
              k n

Estimating the sum by an integral (see §A5), we have

n n            dy                                     1

T (n) = 1/k =                          + O(1)  log n.
k=1 1 y 2

This implies a O(n len(n)) bound on the running time of the algorithm. However,
this rather crude analysis ignores the fact that the inner loop is executed only for
116                The distribution of primes

prime values of k; taking this fact into account, we see that the running time is

O(n T1(n)), where

                   T1(n) := 1/p.

                                     
                                  p n

By Theorem 5.10, T1(n) = log log n + O(1), which implies a O(n len(len(n)))
bound on the running time of the algorithm. This is a substantial improvement

over the above, rather crude analysis.

EXERCISE 5.17. Give a detailed proof of the correctness of the above algorithm.

EXERCISE 5.18. One drawback of the above algorithm is its use of space: it
requires an array of size n. Show how to modify the algorithm, without substan-
tially increasing its running time, so that one can enumerate all the primes up to n,

                                              
using an auxiliary array of size just O( n).

EXERCISE 5.19. Design and analyze an algorithm that on input n outputs the table
of values (k) for k = 1, . . . , n, where (k) is the number of positive divisors of k.
Your algorithm should run in time O(n len(n)).

                   5.5 The prime number theorem . . . and beyond

In this section, we survey a number of theorems and conjectures related to the
distribution of primes. This is a vast area of mathematical research, with a number
of very deep results. We shall be stating a number of theorems from the literature
in this section without proof; while our intent is to keep the text as self contained as
possible, and to avoid degenerating into "mathematical tourism," it nevertheless is a
good idea to occasionally have a somewhat broader perspective. In the subsequent
chapters, we shall not make any critical use of the theorems in this section.

                            5.5.1 The prime number theorem
The main theorem in the theory of the density of primes is the following.

Theorem 5.14 (Prime number theorem). We have

                                       (x)  x/ log x.

Proof. Literature--see §5.6. 2
   As we saw in Exercise 5.10, if (x)/(x/ log x) tends to a limit as x  , then

the limit must be 1, so in fact the hard part of proving the prime number theorem
is to show that (x)/(x/ log x) does indeed tend to some limit.
5.5 The prime number theorem . . . and beyond                       117

EXERCISE 5.20. Using the prime number theorem, show that (x)  x.

EXERCISE 5.21. Using the prime number theorem, show that pn  n log n, where
pn denotes the nth prime.

EXERCISE 5.22. Using the prime number theorem, show that Bertrand's postu-
late can be strengthened (asymptotically) as follows: for every  > 0, there exist
positive constants c and x0, such that for all x  x0, we have

                               ((1 + )x) - (x)  c x .
                                                            log x

                  5.5.2 The error term in the prime number theorem
The prime number theorem says that

|(x) - x/ log x|  (x),

where (x) = o(x/ log x). A natural question is: how small is the "error term"
(x)? It can be shown that

(x) = x/ log x + O(x/(log x)2).                                     (5.10)

This bound on the error term is not very impressive, but unfortunately, cannot
be improved upon. The problem is that x/ log x is not really the best "simple"
function that approximates (x). It turns out that a better approximation to (x) is
the logarithmic integral, defined for all real numbers x  2 as

                                       x li(x) := dt .
                                                   2 log t

It is not hard to show (see Exercise 5.6) that

li(x) = x/ log x + O(x/(log x)2).                                   (5.11)

Thus, li(x)  x/ log x  (x). However, the error term in the approximation
of (x) by li(x) is much better. This is illustrated numerically in Table 5.2; for
example, at x = 1018, li(x) approximates (x) with a relative error just under
10-9, while x/ log x approximates (x) with a relative error of about 0.025.

   The sharpest proven result on the error in approximating (x) by li(x) is the
following:

Theorem 5.15. Let (x) := (log x)3/5(log log x)-1/5. Then for some c > 0, we
have

                                 (x) = li(x) + O(xe-c(x)).

Proof. Literature--see §5.6. 2
118        The distribution of primes

           Table 5.2. Values of (x), li(x), and x/ log x

     x     (x)                               li(x)            x/ log x

     103   168                               176.6            144.8

     106   78498           78626.5                            72382.4

     109   50847534        50849233.9                         48254942.4

     1012  37607912018     37607950279.8                      36191206825.3

     1015  29844570422669  29844571475286.5                   28952965460216.8

     1018 24739954287740860 24739954309690414.0 24127471216847323.8

   Note that the error term xe-c(x) is o(x/(log x)k) for every fixed k  0. Also
note that (5.10) follows directly from (5.11) and Theorem 5.15.

   Although the above estimate on the error term in the approximation of (x) by
li(x) is pretty good, it is conjectured that the actual error term is much smaller:

Conjecture 5.16. For all x  2.01, we have
                                 |(x) - li(x)| < x1/2 log x.

   Conjecture 5.16 is equivalent to the famous Riemann hypothesis, which is a
conjecture about the location of the zeros of a certain function, called Riemann's
zeta function. We give a very brief, high-level account of this conjecture, and its
connection to the theory of the distribution of primes.

   For all real numbers s > 1, the zeta function is defined as

                                     1                                 (5.12)
                            (s) := ns .

                                        n=1

Note that because s > 1, the infinite series defining (s) converges. A simple, but
important, connection between the zeta function and the theory of prime numbers
is the following:

Theorem 5.17 (Euler's identity). For every real number s > 1, we have  (5.13)
                                    (s) = (1 - p-s)-1,

                                                               p

where the product is over all primes p.

Proof. The rigorous interpretation of the infinite product on the right-hand side
of (5.13) is as a limit of finite products. Thus, if pi denotes the ith prime, for
i = 1, 2, . . . , then we are really proving that

                                                                   r

                                  (s) = lim (1 - p-s i )-1.

                                                         r
                                                                 i=1
                 5.5 The prime number theorem . . . and beyond                          119

Now, from the identity

                                                 

                        (1 - p-s i )-1 = pi-es,

                                                e=0

we have

            r

           (1 - p-i s)-1 = 1 + p-s 1 + p-2s 1 + · · · · · · 1 + p-r s + p-r 2s + · · ·

          i=1
                                         

                                  hr (n)
                          = ns ,

                                        n=1

where

       hr(n) :=  1 if n is divisible only by the primes p1, . . . , pr;
                 0 otherwise.

Here, we have made use of the fact (see §A7) that we can multiply term-wise

infinite series with non-negative terms.

Now, for every  > 0, there exists n0 such that        n-s  <    (because  the           series
                                                n=n0
defining (s) converges). Moreover, there exists an r0 such that hr(n) = 1 for all

n < n0 and r  r0. Therefore, for all r  r0, we have

                                                  

                            hr(n) -  (s)      n-s < .
                        n=1 ns
                                                n=n0

It follows that

                                  

                                     hr (n)
                             lim           s = (s),
                             r            n
                                  n=1

which proves the theorem. 2

   While Theorem 5.17 is nice, things become much more interesting if one extends
the domain of definition of the zeta function to the complex plane. For the reader
who is familiar with just a little complex analysis, it is easy to see that the infinite
series defining the zeta function in (5.12) converges absolutely for all complex
numbers s whose real part is greater than 1, and that (5.13) holds as well for such
s. However, it is possible to extend the domain of definition of (s) even further --
in fact, one can extend the definition of (s) in a "nice way " (in the language of
complex analysis, analytically continue) to the entire complex plane (except the
point s = 1, where there is a simple pole). Exactly how this is done is beyond the
scope of this text, but assuming this extended definition of (s), we can now state
the Riemann hypothesis:
120  The distribution of primes

Conjecture 5.18 (Riemann hypothesis). Suppose s is a complex number with
s = x + yi, where x, y  R, such that (s) = 0 and 0 < x < 1. Then x = 1/2.

   A lot is known about the zeros of the zeta function in the "critical strip," which
consists of those points s whose real part is greater than 0 and less than 1: it is
known that there are infinitely many such zeros, and there are even good estimates
about their density. It turns out that one can apply standard tools in complex analy-
sis, like contour integration, to the zeta function (and functions derived from it) to
answer various questions about the distribution of primes. Indeed, such techniques
may be used to prove the prime number theorem. However, if one assumes the
Riemann hypothesis, then these techniques yield much sharper results, such as the
bound in Conjecture 5.16.

EXERCISE 5.23. For any arithmetic function a (mapping positive integers to
reals), we can form the Dirichlet series

                                                   a(n)
                                      Fa(s) := ns .

                                                                    n=1

For simplicity we assume that s takes only real values, even though such series are
usually studied for complex values of s.

    (a) Show that if the Dirichlet series Fa(s) converges absolutely for some real
         s, then it converges absolutely for all real s  s.

   (b) From part (a), conclude that for any given arithmetic function a, there is
         an interval of absolute convergence of the form (s0, ), where we allow
         s0 = - and s0 = , such that Fa(s) converges absolutely for s > s0, and
         does not converge absolutely for s < s0.

    (c) Let a and b be arithmetic functions such that Fa(s) has an interval of abso-
         lute convergence (s0, ) and Fb(s) has an interval of absolute conver-
         gence (s0, ), and assume that s0 <  and s0 < . Let c := a b
         be the Dirichlet product of a and b, as defined in §2.9. Show that for all
         s  (max(s0, s0), ), the series Fc(s) converges absolutely and, moreover,
         that Fa(s)Fb(s) = Fc(s).

                                   5.5.3 Explicit estimates

Sometimes, it is useful to have explicit estimates for (x), as well as related func-
tions, like (x) and the nth prime function pn. The following theorem presents a
number of bounds that have been proved without relying on any unproved conjec-
tures.
                     5.5 The prime number theorem . . . and beyond            121

Theorem 5.19. We have:         < (x) < x 1 + 3 , for x  59;
    (i) x 1 + 1                            log x 2 log x
         log x 2 log x

(ii) n(log n + log log n - 3/2) < pn < n(log n + log log n - 1/2), for n  20;

                  1   < (x) < x 1 + 1 , for x  563;
(iii) x 1 -
       2 log x                          2 log x

(iv) log log x + A -           1        1/p < log log x + A +       1
                                    2<                                   2,
                      2(log x) px                                   2(log x)

for x  286, where A  0.261497212847643;

(v) B1 1 -           1                  1 - 1 < B1 1 +              1
log x                     2<            p log x                          2,
                     2(log x)                           2(log x)
                                px

for x  285, where B1  0.561459483566885.

Proof. Literature--see §5.6. 2

                        5.5.4 Primes in arithmetic progressions

In Theorems 2.35 and 2.36, we proved that there are infinitely many primes p 
1 (mod 4) and infinitely many primes p  3 (mod 4). These results are actually
special cases of a much more general result.

   Let d be a positive integer, and let a be any integer. An arithmetic progression
with first term a and common difference d consists of all integers of the form

                                  a + dm, m = 0, 1, 2, . . . .

The question is: under what conditions does such an arithmetic progression contain
infinitely many primes? An equivalent formulation is: under what conditions are
there infinitely many primes p  a (mod d)? If a and d have a common factor
c > 1, then every term in the progression is divisible by c, and so there can be at
most one prime in the progression. So a necessary condition for the existence of
infinitely many primes p  a (mod d) is that gcd(a, d) = 1. A famous theorem due
to Dirichlet states that this is a sufficient condition as well.

Theorem 5.20 (Dirichlet's theorem). Let a, d  Z with d > 0 and gcd(a, d) = 1.
Then there are infinitely many primes p  a (mod d).

Proof. Literature--see §5.6. 2

   We can also ask about the density of primes in arithmetic progressions. One
might expect that for a fixed value of d, the primes are distributed in roughly equal
122  The distribution of primes

measure among the (d) different residue classes [a]d with gcd(a, d) = 1 (here, 
is Euler's phi function). This is in fact the case. To formulate such assertions, we
define (x; d, a) to be the number of primes p up to x with p  a (mod d).

Theorem 5.21. Let a, d  Z with d > 0 and gcd(a, d) = 1. Then

     (x; d, a)                  x          .

                                (d) log x

Proof. Literature--see §5.6. 2

   The above theorem is only applicable in the case where d and a are fixed as
x  . For example, it says that roughly half the primes up to x are congruent
to 1 modulo 4, and roughly half the primes up to x are congruent to 3 modulo 4.
However, suppose d  , and we want to estimate, say, the number of primes
p  1 (mod d) up to d3. Theorem 5.21 does not help us here. The following
conjecture does, however:

Conjecture 5.22. Let x  R, a, d  Z with x  2, d  2, and gcd(a, d) = 1. Then
                        (x; d, a) - li(x)  x1/2(log x + 2 log d).
                                       (d)

   The above conjecture is in fact a consequence of a generalization of the Rie-
mann hypothesis -- see §5.6. This conjecture implies that for every constant
 < 1/2, if 2  d  x, then (x; d, a) is closely approximated by li(x)/(d)
(see Exercise 5.24). It can also be used to get an upper bound on the least prime
p  a (mod d) (see Exercise 5.25). The following theorem is the best rigorously
proven upper bound on the smallest prime in an arithmetic progression:

Theorem 5.23. There exists a constant c such that for all a, d  Z with d  2 and
gcd(a, d) = 1, the least prime p  a (mod d) is at most cd11/2.

Proof. Literature--see §5.6. 2

EXERCISE 5.24. Assuming Conjecture 5.22, show that for all ,  satisfying

0 <  < 1/2 and 0 <  < 1, there exists an x0, such that for all x > x0, for
all d  Z with 2  d  x, and for all a  Z relatively prime to d, the number of
primes p  x such that p  a (mod d) is at least (1 - ) li(x)/(d) and at most
(1 + ) li(x)/(d).

EXERCISE 5.25. Assuming Conjecture 5.22, show that there exists a constant
c such that for all a, d  Z with d  2 and gcd(a, d) = 1, the least prime
p  a (mod d) is at most c(d)2(log d)4.
       5.5 The prime number theorem . . . and beyond       123

                               5.5.5 Sophie Germain primes
A Sophie Germain prime is a prime p such that 2p + 1 is also prime. Such primes
are actually useful in a number of practical applications, and so we discuss them
briefly here.

   It is an open problem to prove (or disprove) that there are infinitely many Sophie
Germain primes. However, numerical evidence, and heuristic arguments, strongly
suggest not only that there are infinitely many such primes, but also a fairly precise
estimate on the density of such primes.

   Let (x) denote the number of Sophie Germain primes up to x.

Conjecture 5.24. We have

                          (x)  C (log x)2 x ,

where C is the constant

                         C := 2       p(p - 2)
                                              2  1.32032,
                                 p>2 (p - 1)

and the product is over all primes p > 2.

The above conjecture is a special case of the following, more general conjecture.

Conjecture 5.25 (Dickson's conjecture). Let (a1, b1), . . . , (ak, bk) be distinct

pairs of integers, where each ai is positive. Let P (x) be the number of positive
integers m up to x such that aim + bi are simultaneously prime for i = 1, . . . , k.

For each prime p, let (p) be the number of integers m  {0, . . . , p-1} that satisfy

                           k

                             (aim + bi)  0 (mod p).

                          i=1

If (p) < p for each prime p, then
                                                      x

                                      P (x)  D (log x)k ,

where

                          D :=        1 - (p)/p
                                                  k,
                                      p (1 - 1/p)

the product being over all primes p.

   In Exercise 5.26 below, you are asked to verify that the quantity D appearing

in Conjecture 5.25 satisfies 0 < D < . Conjecture 5.24 is implied by Con-
jecture 5.25 with k := 2, (a1, b1) := (1, 0), and (a2, b2) := (2, 1); in this case,
124      The distribution of primes

(2) = 1 and (p) = 2 for all p > 2. The above conjecture also includes (a strong

version of) the famous twin primes conjecture as a special case: the number of
primes p up to x such that p + 2 is also prime is  Cx/(log x)2, where C is the

same constant as in Conjecture 5.24.

A heuristic argument in favor of Conjecture 5.25 runs as follows. In some

sense, the chance that a large positive integer m is prime is about 1/ log m. Since
log(aim + bi)  log m, the chance that a1m + b1, . . . , akm + bk are all prime should
be about 1/(log m)k. But this ignores the fact that a1m + b1, . . . , akm + bk are

not quite random integers. For each prime p, we must apply a "correction factor"
rp/sp, where rp is the chance that for random m, none of a1m + b1, . . . , akm + bk is

divisible by p, and sp is the chance that for k truly random, large integers, none of
them is divisible by p. One sees that rp = 1 - (p)/p and sp = (1 - 1/p)k. This

implies (using §A5 and Exercise 5.6) that P (x) should be about

                                      x

     D 1/(log m)k  D dt/(log t)k  Dx/(log x)k.

     mx                               2

Although Conjecture 5.25 is well supported by numerical evidence, there seems
little hope of it being proved any time soon, even under the Riemann hypothesis or
any of its generalizations.

EXERCISE 5.26. Show that the quantity D appearing in Conjecture 5.25 satisfies
0 < D < . Hint: first show that (p) = k for all sufficiently large p.

EXERCISE 5.27. Derive Theorem 5.21 from Conjecture 5.25.

EXERCISE 5.28. Show that the constant C appearing in Conjecture 5.24 satisfies
                                         2C = B2/B12,

where B1 and B2 are the constants from Exercises 5.12 and 5.13.

                                           5.6 Notes
The prime number theorem was conjectured by Gauss in 1791. It was proven
independently in 1896 by Hadamard and de la Vallée Poussin. A proof of the prime
number theorem may be found, for example, in the book by Hardy and Wright [46].

   Theorem 5.19, as well as the estimates for the constants A, B1, and B2 mentioned
in that theorem and Exercises 5.11, 5.12, and 5.13, are from Rosser and Schoenfeld
[83].

   Theorem 5.15 is from Walfisz [102].
5.6 Notes  125

   Theorem 5.17, which made the first connection between the theory of prime
numbers and the zeta function, was discovered in the 18th century by Euler. The
Riemann hypothesis was made by Riemann in 1859, and to this day, remains one
of the most vexing conjectures in mathematics. Riemann in fact showed that his
conjecture about the zeros of the zeta function is equivalent to the conjecture that
for each fixed  > 0, (x) = li(x) + O(x1/2+). This was strengthened by von
Koch in 1901, who showed that the Riemann hypothesis is true if and only if
(x) = li(x)+O(x1/2 log x). See Chapter 1 of the book by Crandall and Pomerance
[30] for more on the connection between the Riemann hypothesis and the theory
of prime numbers; in particular, see Exercise 1.36 in that book for an outline of a
proof that Conjecture 5.16 follows from the Riemann hypothesis.

   A warning: some authors (and software packages) define the logarithmic inte-
gral using the interval of integration (0, x), rather than (2, x), which increases its
value by a constant c  1.0452.

   Theorem 5.20 was proved by Dirichlet in 1837, while Theorem 5.21 was proved
by de la Vallée Poussin in 1896. A result of Oesterlé [73] implies that Conjec-
ture 5.22 for d  3 is a consequence of an assumption about the location of the
zeros of certain generalizations of Riemann's zeta function; the case d = 2 follows
from the bound in Conjecture 5.16 under the ordinary Riemann hypothesis. Theo-
rem 5.23 is from Heath-Brown [47]. The bound in Exercise 5.25 can be improved
to c(d)2(log d)2 (see Theorem 8.5.8 of [11]).

   Conjecture 5.25 originates from Dickson [33]. In fact, Dickson only conjectured
that the quantity P (x) defined in Conjecture 5.25 tends to infinity. The conjectured
formula for the rate of growth of P (x) is a special case of a more general conjec-
ture stated by Bateman and Horn [12], which generalizes various, more specific
conjectures stated by Hardy and Littlewood [45].

   For the reader who is interested in learning more on the topics discussed in this
chapter, we recommend the books by Apostol [8] and Hardy and Wright [46];
indeed, many of the proofs presented in this chapter are minor variations on proofs
from these two books. Our proof of Bertrand's postulate is based on the presen-
tation in Section 9.2 of Redmond [80]. See also Bach and Shallit [11] (especially
Chapter 8), as well as Crandall and Pomerance [30] (especially Chapter 1), for a
more detailed overview of these topics.

   The data in Tables 5.1 and 5.2 was obtained using the computer program Maple.
       6

Abelian groups

This chapter introduces the notion of an abelian group. This is an abstraction that
models many different algebraic structures, and yet despite the level of generality,

a number of very useful results can be easily obtained.

                   6.1 Definitions, basic properties, and examples

Definition 6.1. An abelian group is a set G together with a binary operation on
G such that:

    (i) for all a, b, c  G, a (b c) = (a b) c (i.e., is associative);

   (ii) there exists e  G (called the identity element) such that for all a  G,
         a e = a = e a;

   (iii) for all a  G there exists a  G (called the inverse of a) such that
         a a = e = a a;

   (iv) for all a, b  G, a b = b a (i.e., is commutative).

   While there is a more general notion of a group, which may be defined simply
by dropping property (iv) in Definition 6.1, we shall not need this notion in this
text. The restriction to abelian groups helps to simplify the discussion significantly.
Because we will only be dealing with abelian groups, we may occasionally simply
say "group" instead of "abelian group."

   Before looking at examples, let us state some very basic properties of abelian
groups that follow directly from the definition:

Theorem 6.2. Let G be an abelian group with binary operation  . Then we have:
    (i) G contains only one identity element;
   (ii) every element of G has only one inverse.

126
6.1 Definitions, basic properties, and examples        127

Proof. Suppose e, e are both identities. Then we have
                                        e=e e =e,

where we have used part (ii) of Definition 6.1, once with e as the identity, and
once with e as the identity. That proves part (i) of the theorem.

   To prove part (ii) of the theorem, let a  G, and suppose that a has two inverses,
a and a . Then using parts (i)-(iii) of Definition 6.1, we have

                  a = a e (by part (ii))
                     = a (a a ) (by part (iii) with inverse a of a)
                     = (a a) a (by part (i))
                     = e a (by part (iii) with inverse a of a)
                     = a (by part (ii)). 2

   These uniqueness properties justify use of the definite article in Definition 6.1
in conjunction with the terms "identity element" and "inverse." Note that we never
used part (iv) of the definition in the proof of the above theorem.

   Abelian groups are lurking everywhere, as the following examples illustrate.

Example 6.1. The set of integers Z under addition forms an abelian group, with 0
being the identity, and -a being the inverse of a  Z. 2

Example 6.2. For each integer n, the set nZ = {nz : z  Z} under addition forms
an abelian group, again, with 0 being the identity, and n(-z) being the inverse of
nz. 2

Example 6.3. The set of non-negative integers under addition does not form an
abelian group, since additive inverses do not exist for any positive integers. 2

Example 6.4. The set of integers under multiplication does not form an abelian
group, since inverses do not exist for any integers other than ±1. 2

Example 6.5. The set of integers {±1} under multiplication forms an abelian
group, with 1 being the identity, and -1 its own inverse. 2

Example 6.6. The set of rational numbers Q = {a/b : a, b  Z, b = 0} under
addition forms an abelian group, with 0 being the identity, and (-a)/b being the
inverse of a/b. 2

Example 6.7. The set of non-zero rational numbers Q under multiplication forms
an abelian group, with 1 being the identity, and b/a being the inverse of a/b. 2

Example 6.8. The set Zn under addition forms an abelian group, where [0]n is the
identity, and where [-a]n is the inverse of [a]n. 2
128  Abelian groups

Example 6.9. The set Zn of residue classes [a]n with gcd(a, n) = 1 under multipli-
cation forms an abelian group, where [1]n is the identity, and if b is a multiplicative

inverse of a modulo n, then [b]n is the inverse of [a]n. 2

Example 6.10. For every positive integer n, the set of n-bit strings under the
"exclusive or" operation forms an abelian group, where the "all zero" bit string
is the identity, and every bit string is its own inverse. 2

Example 6.11. The set F of all arithmetic functions f , such that f (1) = 0, and
with the Dirichlet product as the binary operation (see §2.9) forms an abelian
group. The special function I is the identity, and inverses are guaranteed by
Exercise 2.54. 2

Example 6.12. The set of all finite bit strings under concatenation does not form
an abelian group. Although concatenation is associative and the empty string acts
as an identity element, inverses do not exist (except for the empty string), nor is
concatenation commutative. 2

Example 6.13. The set of 2 × 2 integer matrices with determinant ±1, together
with the binary operation of matrix multiplication, is an example of a non-abelian
group; that is, it satisfies properties (i)-(iii) of Definition 6.1, but not property
(iv). 2

Example 6.14. The set of all permutations on a given set of size n  3, together

with the binary operation of function composition, is another example of a non-
abelian group (for n = 1, 2, it is an abelian group). 2

   Consider an abelian group G with binary operation . Since the group operation

is associative, for all a1, . . . , ak  G, we may write a1 · · · ak without parentheses,
and there can be no ambiguity as to the value of such an expression: any explicit

parenthesization of this expression yields the same value. Furthermore, since the

group operation is commutative, reordering the ai's does not change this value.
   Note that in specifying a group, one must specify both the underlying set G as

well as the binary operation; however, in practice, the binary operation is often

implicit from context, and by abuse of notation, one often refers to G itself as the
group. For example, when talking about the abelian groups Z and Zn, it is under-
stood that the group operation is addition, while when talking about the abelian
group Zn, it is understood that the group operation is multiplication.

   Typically, instead of using a special symbol like " " for the group operation, one
uses the usual addition ("+") or multiplication ("·") operations.

   Additive notation. If an abelian group G is written additively, using "+" as

the group operation, then the identity element is denoted by 0G (or just 0 if G is
6.1 Definitions, basic properties, and examples                            129

clear from context), and is also called the zero element. The inverse of an element
a  G is denoted by -a. For a, b  G, a - b denotes a + (-b).

   Multiplicative notation. If an abelian group G is written multiplicatively, using

"·" as the group operation, then the identity element is denoted by 1G (or just 1 if
G is clear from context). The inverse of an element a  G is denoted by a-1. As
usual, one may write ab in place of a · b. Also, one may write a/b for ab-1.

   For any particular, concrete abelian group, the most natural choice of notation is
clear (e.g., addition for Z and Zn, multiplication for Zn); however, for a "generic"
group, the choice is largely a matter of taste. By convention, whenever we con-
sider a "generic" abelian group, we shall use additive notation for the group

operation, unless otherwise specified.

   The next theorem states a few simple but useful properties of abelian groups

(stated using our default, additive notation).

Theorem 6.3. Let G be an abelian group. Then for all a, b, c  G, we have:
    (i) if a + b = a + c, then b = c;
   (ii) the equation a + x = b has a unique solution x  G;

   (iii) -(a + b) = (-a) + (-b);
   (iv) -(-a) = a.

Proof. These statements all follow easily from Definition 6.1 and Theorem 6.2.
For (i), just add -a to both sides of the equation a + b = a + c. For (ii), the solution
is x = b - a. For (iii), we have

       (a + b) + ((-a) + (-b)) = (a + (-a)) + (b + (-b)) = 0G + 0G = 0G,

which shows that (-a) + (-b) is indeed the inverse of a + b. For (iv), we have
(-a) + a = 0G, which means that a is the inverse of -a. 2

   Part (i) of the above theorem is the cancellation law for abelian groups.

   If a1, . . . , ak are elements of an abelian group G, we naturally write ik=1 ai for
their sum a1 + · · · + ak. By convention, the sum is 0G when k = 0. Part (iii) of
Theorem 6.3 obviously generalizes, so that - ik=1 ai = ik=1(-ai). In the special
case where all the ai's have the same value a, we define k · a := ik=1 a, whose
inverse is k · (-a), which we may write as (-k) · a. Thus, the notation k · a, or
more simply, ka, is defined for all integers k. Observe that by definition, 1a = a
and (-1)a = -a.

Theorem 6.4. Let G be an abelian group. Then for all a, b  G and k,   Z, we
have:

    (i) k( a) = (k )a = (ka);
130                           Abelian groups

   (ii) (k + )a = ka + a;
   (iii) k(a + b) = ka + kb.

Proof. The proof of this is easy, but tedious. We leave the details as an exercise to
the reader. 2

   Multiplicative notation: It is perhaps helpful to translate the above discussion
from additive to multiplicative notation. If a group G is written using multi-
plicative notation, then Theorem 6.3 says that (i) ab = ac implies b = c, (ii)
ax = b has a unique solution, (iii) (ab)-1 = a-1b-1, and (iv) (a-1)-1 = a. If
a1, . . . , ak  G, we write their product a1 · · · ak as ik=1 ai, which is 1G when
k = 0. We have ( ik=1 ai)-1 = ik=1 ai-1. We also define ak := ik=1 a, and
we have (ak)-1 = (a-1)k, which we may write as a-k. Theorem 6.4 says that (i)
(a )k = ak = (ak) , (ii) ak+ = aka , and (iii) (ab)k = akbk.

   An abelian group G may be trivial, meaning that it consists of just the zero
element 0G, with 0G +0G = 0G. An abelian group G may be infinite or finite: if the
group is finite, we define its order to be the number of elements in the underlying
set G; otherwise, we say that the group has infinite order.

Example 6.15. The order of the additive group Zn is n. If n = 1, then Zn is the
trivial group. 2

Example 6.16. The order of the multiplicative group Zn is (n), where  is Euler's
phi function, defined in §2.6. 2

Example 6.17. The additive group Z has infinite order. 2

   We close this section with two simple constructions for combining groups to
build new groups.

Example 6.18. If G1, . . . , Gk are abelian groups, we can form the direct product
H := G1 × · · · × Gk, which consists of all k-tuples (a1, . . . , ak) with a1  G1,
. . . , ak  Gk. We can view H in a natural way as an abelian group if we define the
group operation component-wise:

                  (a1, . . . , ak) + (b1, . . . , bk) := (a1 + b1, . . . , ak + bk).

Of course, the groups G1, . . . , Gk may be different, and the group operation applied
in the ith component corresponds to the group operation associated with Gi. We
leave it to the reader to verify that H is in fact an abelian group, where 0H =
(0G1, . . . , 0Gk ) and -(a1, . . . , ak) = (-a1, . . . , -ak). As a special case, if G =
G1 = · · · = Gk, then the k-wise direct product of G is denoted G×k. 2
6.1 Definitions, basic properties, and examples            131

Example 6.19. Let G be an abelian group. An element (a1, . . . , ak) of G×k may be
identified with the function f : {1, . . . , k}  G given by f (i) = ai for i = 1, . . . , k.
We can generalize this, replacing {1, . . . , k} by an arbitrary set I. We define
Map(I, G) to be the set of all functions f : I  G, which we naturally view
as a group by defining the group operation point-wise: for f , g  Map(I, G), we
define

                          (f + g)(i) := f (i) + g(i) for all i  I.

Again, we leave it to the reader to verify that Map(I, G) is an abelian group,

where the identity element is the function that maps each i  I to 0G, and for
f  Map(I, G), we have (-f )(i) = -(f (i)) for all i  I. 2

EXERCISE 6.1. For a finite abelian group, one can completely specify the group
by writing down the group operation table. For instance, Example 2.7 presented an
addition table for Z6.

    (a) Write down group operation tables for the following finite abelian groups:
         Z5, Z5, and Z3 × Z4.

   (b) Show that the group operation table for every finite abelian group is a Latin
         square; that is, each element of the group appears exactly once in each row
         and column.

(c) Below is an addition table for an abelian group that consists of the elements

{a, b, c, d}; however, some entries are missing. Fill in the missing entries.

+abcd

aa

b ba

c     a

d

EXERCISE 6.2. Let G := {x  R : x > 1}, and define a  b := ab - a - b + 2 for
all a, b  R. Show that:

    (a) G is closed under ;

(b) the set G under the operation forms an abelian group.

EXERCISE 6.3. Let G be an abelian group, and let g be an arbitrary, fixed element

of G. Assume that the group operation of G is written additively. We define a new
binary operation on G, as follows: for a, b  G, let a b := a + b + g. Show that

the set G under forms an abelian group.

EXERCISE 6.4. Let G be a finite abelian group of even order. Show that there
exists a  G with a = 0G and 2a = 0G.
132  Abelian groups

EXERCISE 6.5. Let be a binary operation on a non-empty, finite set G. Assume
that is associative, commutative, and satisfies the cancellation law: a b = a c
implies b = c. Show that G under forms an abelian group.

EXERCISE 6.6. Show that the result of the previous exercise need not hold if G is
infinite.

                                        6.2 Subgroups

We next introduce the notion of a subgroup.

Definition 6.5. Let G be an abelian group, and let H be a non-empty subset of G
such that

    (i) a + b  H for all a, b  H, and

   (ii) -a  H for all a  H.
Then H is called a subgroup of G.

   In words: H is a subgroup of G if it is closed under the group operation and
taking inverses.

   Multiplicative notation: if the abelian group G in the above definition is written
using multiplicative notation, then H is a subgroup if ab  H and a-1  H for all
a, b  H.

Theorem 6.6. If G is an abelian group, and H is a subgroup of G, then H
contains 0G; moreover, the binary operation of G, when restricted to H, yields
a binary operation that makes H into an abelian group whose identity is 0G.

Proof. First, to see that 0G  H, just pick any a  H, and using both properties of
the definition of a subgroup, we see that 0G = a + (-a)  H.

   Next, note that by property (i) of Definition 6.5, H is closed under addition,
which means that the restriction of the binary operation "+" on G to H induces a
well-defined binary operation on H. So now it suffices to show that H, together
with this operation, satisfies the defining properties of an abelian group. Associa-
tivity and commutativity follow directly from the corresponding properties for G.
Since 0G acts as the identity on G, it does so on H as well. Finally, property (ii) of
Definition 6.5 guarantees that every element a  H has an inverse in H, namely,
-a. 2

   Clearly, for an abelian group G, the subsets G and {0G} are subgroups, though
not very interesting ones. Other, more interesting subgroups may sometimes be
found by using the following two theorems.
6.2 Subgroups                                                          133

Theorem 6.7. Let G be an abelian group, and let m be an integer. Then
                                     mG := {ma : a  G}

is a subgroup of G.

Proof. The set mG is non-empty, since 0G = m0G  mG. For ma, mb  mG, we
have ma + mb = m(a + b)  mG, and -(ma) = m(-a)  mG. 2

Theorem 6.8. Let G be an abelian group, and let m be an integer. Then
                               G{m} := {a  G : ma = 0G}

is a subgroup of G.

Proof. The set G{m} is non-empty, since m0G = 0G, and so G{m} contains 0G.
If ma = 0G and mb = 0G, then m(a + b) = ma + mb = 0G + 0G = 0G and
m(-a) = -(ma) = -0G = 0G. 2

   Multiplicative notation: if the abelian group G in the above two theorems is
written using multiplicative notation, then we write the subgroup of the first theo-
rem as Gm := {am : a  G}. The subgroup in the second theorem is denoted in the
same way: G{m} := {a  G : am = 1G}.

Example 6.20. We already proved that (Zn)m is a subgroup of Zn in Theorem 2.16.
Also, the proof of Theorem 2.17 clearly works for an arbitrary abelian group G:
for each a  G, and all , m  Z with gcd( , m) = 1, if a  mG, then a  mG. 2

Example 6.21. Let p be an odd prime. Then by Theorem 2.20, (Zp)2 is a subgroup
of Zp of order (p - 1)/2, and as we saw in Theorem 2.18, Zp{2} = {[±1]}. 2

Example 6.22. For every integer m, the set mZ is the subgroup of the additive group
Z consisting of all multiples of m. This is the same as the ideal of Z generated by
m, which we already studied in some detail in §1.2. Two such subgroups mZ and
m Z are equal if and only if m = ±m . The subgroup Z{m} is equal to Z if m = 0,
and is equal to {0} otherwise. 2

Example 6.23. Let n be a positive integer, let m  Z, and consider the subgroup
mZn of the additive group Zn. Now, for every residue class [z]  Zn, we have
m[z] = [mz]. Therefore, [b]  mZn if and only if there exists z  Z such that
mz  b (mod n). By part (i) of Theorem 2.5, such a z exists if and only if d | b,
where d := gcd(m, n). Thus, mZn consists precisely of the n/d distinct residue
classes

                                 [i · d] (i = 0, . . . , n/d - 1),

and in particular, mZn = dZn.
134  Abelian groups

   Now consider the subgroup Zn{m} of Zn. The residue class [z] is in Zn{m} if
and only if mz  0 (mod n). By part (ii) of Theorem 2.5, this happens if and only if
z  0 (mod n/d), where d := gcd(m, n) as above. Thus, Zn{m} consists precisely
of the d residue classes

                                 [i · n/d] (i = 0, . . . , d - 1),

and in particular, Zn{m} = Zn{d} = (n/d)Zn. 2

Example 6.24. For n = 15, consider again the table in Example 2.2. For m = 1,
2, 3, 4, 5, 6, the elements appearing in the mth row of that table form the subgroup
mZn of Zn, and also the subgroup Zn{n/d}, where d := gcd(m, n). 2

   Because the abelian groups Z and Zn are of such importance, it is a good idea
to completely characterize all subgroups of these abelian groups. As the following
two theorems show, the subgroups in Examples 6.22 and 6.23 are the only ones.

Theorem 6.9. If G is a subgroup of Z, then there exists a unique non-negative
integer m such that G = mZ. Moreover, for two non-negative integers m1 and m2,
we have m1Z  m2Z if and only if m2 | m1.

Proof. Actually, we have already proven this. One only needs to observe that a
subset G of Z is a subgroup if and only if it is an ideal of Z, as defined in §1.2
(see Exercise 1.8). The first statement of the theorem then follows from Theo-
rem 1.6. The second statement follows easily from the definitions, as was observed
in §1.2. 2

Theorem 6.10. If G is a subgroup of Zn, then there exists a unique positive integer
d dividing n such that G = dZn. Also, for all positive divisors d1, d2 of n, we have
d1Zn  d2Zn if and only if d2 | d1.

Proof. Note that the second statement implies the uniqueness part of the first state-
ment, so it suffices to prove just the existence part of the first statement and the

second statement.
   Let G be an arbitrary subgroup of Zn, and let H := {z  Z : [z]  G}. We

claim that H is a subgroup of Z. To see this, observe that if a, b  H, then [a] and
[b] belong to G, and hence so do [a + b] = [a] + [b] and [-a] = -[a], and thus
a + b and -a belong to H. That proves the claim, and Theorem 6.9 implies that
H = dZ for some non-negative integer d. It follows that

                      G = {[y] : y  H} = {[dz] : z  Z} = dZn.

Evidently, n  H = dZ, and hence d | n. That proves the existence part of the first
statement of the theorem.
6.2 Subgroups                                                             135

   To prove the second statement of the theorem, observe that if d1 and d2 are
arbitrary integers, then

          d1Zn  d2Zn  d2z  d1 (mod n) for some z  Z
                             gcd(d2, n) | d1 (by part (i) of Theorem 2.5).

In particular, if d2 is a positive divisor of n, then gcd(d2, n) = d2, which proves the
second statement. 2

Of course, not all abelian groups have such a simple subgroup structure.

Example 6.25. Consider the group G = Z2 × Z2. For every non-zero   G,
 +  = 0G. From this, it is clear that the set H = {0G, } is a subgroup of G.
However, for every integer m, mG = G if m is odd, and mG = {0G} if m is even.

Thus, the subgroup H is not of the form mG for any m. 2

Example 6.26. Consider the group Z15. We can enumerate its elements as

                                   [±1], [±2], [±4], [±7].

Therefore, the elements of (Z15)2 are
             [1]2 = [1], [2]2 = [4], [4]2 = [16] = [1], [7]2 = [49] = [4];

thus, (Z15)2 has order 2, consisting as it does of the two distinct elements [1] and
[4].

   Going further, one sees that (Z15)4 = {[1]}. Thus, 4 = [1] for all   Z15.
   By direct calculation, one can determine that (Z15)3 = Z15; that is, cubing sim-
ply permutes Z15.
   For any given integer m, write m = 4q + r, where 0  r < 4. Then for every
  Z15, we have m = 4q+r = 4qr = r. Thus, (Z15)m is either Z15, (Z15)2, or
{[1]}.
   However, there are certainly other subgroups of Z15  -- for example, the subgroup
{[±1]}. 2

Example 6.27. Consider the group Z5 = {[±1], [±2]}. The elements of (Z5)2 are
                               [1]2 = [1], [2]2 = [4] = [-1];

thus, (Z5)2 = {[±1]} and has order 2.
   There are in fact no other subgroups of Z5 besides Z5, {[±1]}, and {[1]}.

Indeed, if H is a subgroup containing [2], then we must have H = Z5: [2]  H
implies [2]2 = [4] = [-1]  H, which implies [-2]  H as well. The same holds
if H is a subgroup containing [-2]. 2
136  Abelian groups

Example 6.28. Consider again the abelian group F of arithmetic functions f ,
such that f (1) = 0, and with the Dirichlet product as the binary operation, as

discussed in Example 6.11. Exercises 2.48 and 2.55 imply that the subset of all

multiplicative functions is a subgroup. 2

   We close this section with two theorems that provide useful ways to build new
subgroups out of old ones.

Theorem 6.11. If H1 and H2 are subgroups of an abelian group G, then so is

                       H1 + H2 := {a1 + a2 : a1  H1, a2  H2}.

Proof. It is evident that H1 + H2 is non-empty, as it contains 0G + 0G = 0G.
Consider two elements in H1 + H2, which we can write as a1 + a2 and b1 + b2,
where a1, b1  H1 and a2, b2  H2. Then by the closure properties of subgroups,
a1+b1  H1 and a2+b2  H2, and hence (a1+a2)+(b1+b2) = (a1+b1)+(a2+b2) 
H1 + H2. Similarly, -(a1 + a2) = (-a1) + (-a2)  H1 + H2. 2

   Multiplicative notation: if the abelian group G in the above theorem is written
multiplicatively, then the subgroup defined in the theorem is written H1H2 :=
{a1a2 : a1  H1, a2  H2}.

Theorem 6.12. If H1 and H2 are subgroups of an abelian group G, then so is
H1  H2.

Proof. It is evident that H1  H2 is non-empty, as both H1 and H2 contain 0G,

and hence so does their intersection. If a  H1  H2 and b  H1  H2, then
since a, b  H1, we have a + b  H1, and since a, b  H2, we have a + b  H2;
therefore, a + b  H1  H2. Similarly, -a  H1 and -a  H2, and therefore,

-a  H1  H2. 2

   Let G be an abelian group and H1, H2, H3 subgroups of G. The reader may
verify that H1 + H2 = H2 + H1 and (H1 + H2) + H3 = H1 + (H2 + H3). It follows
that if H1, . . . , Hk are subgroups of G, then we can write H1 + · · · + Hk without

any parentheses, and there can be no ambiguity; moreover, the order of the Hi's
does not matter. The same holds with "+" replaced by "."

   A warning: If H is a subgroup of an abelian group G, then in general, we have
H + H = 2H. For example, Z + Z = Z, while 2Z = Z.

EXERCISE 6.7. Let G be an abelian group.

   (a) Suppose that H is a non-empty subset of G. Show that H is a subgroup of
         G if and only if a - b  H for all a, b  H.
6.3 Cosets and quotient groups     137

(b) Suppose that H is a non-empty, finite subset of G such that a + b  H for
     all a, b  H. Show that H is a subgroup of G.

EXERCISE 6.8. Let G be an abelian group.

   (a) Show that if H is a subgroup of G, h  H, and g  G \ H, then
         h + g  G \ H.

   (b) Suppose that H is a non-empty subset of G such that for all h, g  G: (i)
         h  H implies -h  H, and (ii) h  H and g  G\H implies h+g  G\H.
         Show that H is a subgroup of G.

EXERCISE 6.9. Show that if H is a subgroup of an abelian group G, then a set
K  H is a subgroup of G if and only if K is a subgroup of H.

EXERCISE 6.10. Let G be an abelian group with subgroups H1 and H2. Show
that every subgroup H of G that contains H1  H2 must contain all of H1 + H2,
and that H1  H2 if and only if H1 + H2 = H2.

EXERCISE 6.11. Let H1 be a subgroup of an abelian group G1 and H2 a subgroup
of an abelian group G2. Show that H1 × H2 is a subgroup of G1 × G2.

EXERCISE 6.12. Show that if G1 and G2 are abelian groups, and m is an integer,
then m(G1 × G2) = mG1 × mG2.

EXERCISE 6.13. Let G1 and G2 be abelian groups, and let H be a subgroup of
G1 × G2. Define

                  H1 := {a1  G1 : (a1, a2)  H for some a2  G2}.

Show that H1 is a subgroup of G1.

EXERCISE 6.14. Let I be a set and G be an abelian group, and consider the

group Map(I, G) of functions f : I  G. Let Map#(I, G) be the set of functions
f  Map(I, G) such that f (i) = 0G for at most finitely many i  I. Show that
Map#(I, G) is a subgroup of Map(I, G).

                             6.3 Cosets and quotient groups

We now generalize the notion of a congruence relation.
   Let G be an abelian group, and let H be a subgroup of G. For a, b  G, we

write a  b (mod H) if a - b  H. In other words, a  b (mod H) if and only if
a = b + h for some h  H.

   Analogous to Theorem 2.2, if we view the subgroup H as fixed, then the fol-
lowing theorem says that the binary relation "·  · (mod H)" is an equivalence
relation on the set G:
138        Abelian groups

Theorem 6.13. Let G be an abelian group and H a subgroup of G.  For all
a, b, c  G, we have:

    (i) a  a (mod H);
   (ii) a  b (mod H) implies b  a (mod H);
   (iii) a  b (mod H) and b  c (mod H) implies a  c (mod H).

Proof. For (i), observe that H contains 0G = a - a. For (ii), observe that if H
contains a - b, then it also contains -(a - b) = b - a. For (iii), observe that if H

contains a - b and b - c, then it also contains (a - b) + (b - c) = a - c. 2

   Since the binary relation "·  · (mod H)" is an equivalence relation, it parti-
tions G into equivalence classes (see Theorem 2.1). For a  G, we denote the
equivalence class containing a by [a]H . By definition, we have

         x  [a]H  x  a (mod H)  x = a + h for some h  H,

and hence

           [a]H = a + H := {a + h : h  H}.

It is also clear that [0G]H = H.
   Historically, these equivalence classes are called cosets of H in G, and we shall

adopt this terminology here as well. Any member of a coset is called a represen-
tative of the coset.

   Multiplicative notation: if G is written multiplicatively, then a  b (mod H)
means ab-1  H, and [a]H = aH := {ah : h  H}.

Example 6.29. Let G := Z and H := nZ for some positive integer n. Then
a  b (mod H) if and only if a  b (mod n). The coset [a]H is exactly the same
thing as the residue class [a]n  Zn. 2

Example 6.30. Let G := Z6, which consists of the residue classes [0], [1], [2], [3],
[4], [5]. Let H be the subgroup 3G = {[0], [3]} of G. The coset of H containing
the residue class [1] is [1] + H = {[1], [4]}, and the coset of H containing the
residue class [2] is [2] + H = {[2], [5]}. The cosets {[0], [3]}, {[1], [4]}, and
{[2], [5]} are the only cosets of H in G, and they clearly partition the set Z6. Note
that each coset of H in G contains two elements, each of which is itself a coset of
6Z in Z (i.e., a residue classes modulo 6). 2

   In the previous example, we saw that each coset contained the same number of
elements. As the next theorem shows, this was no accident.
6.3 Cosets and quotient groups                                  139

Theorem 6.14. Let G be an abelian group and H a subgroup of G.  For all
a, b  G, the function

                                     f : GG
                                            xb-a+x

is a bijection, which, when restricted to the coset [a]H , yields a bijection from
[a]H to the coset [b]H . In particular, every two cosets of H in G have the same
cardinality.

Proof. First, we claim that f is a bijection. Indeed, if f (x) = f (x ), then
b - a + x = b - a + x , and subtracting b and adding a to both sides of this equation
yields x = x . That proves that f is injective. To prove that f is surjective, observe
that for any given x  G, we have f (a - b + x ) = x .

   Second, we claim that for all x  G, we have x  [a]H if and only if f (x)  [b]H .
On the one hand, suppose that x  [a]H , which means that x = a+h for some h  H.
Subtracting a and adding b to both sides of this equation yields b - a + x = b + h,

which means f (x)  [b]H . Conversely, suppose that f (x)  [b]H , which means
that b - a + x = b + h for some h  H. Subtracting b and adding a to both sides of
this equation yields x = a + h, which means that x  [a]H .

   The theorem is now immediate from these two claims. 2

An incredibly useful consequence of the above theorem is:

Theorem 6.15 (Lagrange's theorem). If G is a finite abelian group, and H is a
subgroup of G, then the order of H divides the order of G.

Proof. This is an immediate consequence of the previous theorem, and the fact that
the cosets of H in G partition G. 2

Analogous to Theorem 2.3, we have:

Theorem 6.16. Suppose G is an abelian group and H is a subgroup of G. For
all a, a , b, b  G, if a  a (mod H) and b  b (mod H), then we have
a + b  a + b (mod H).

Proof. Now, a  a (mod H) and b  b (mod H) means that a = a + x and
b = b +y for some x, y  H. Therefore, a+b = (a +x)+(b +y) = (a +b )+(x+y),
and since x + y  H, this means that a + b  a + b (mod H). 2

   Let G be an abelian group and H a subgroup. Let G/H denote the set of all
cosets of H in G. Theorem 6.16 allows us to define a binary operation on G/H in
the following natural way: for a, b  G, define

                                  [a]H + [b]H := [a + b]H .
140  Abelian groups

That this definition is unambiguous follows immediately from Theorem 6.16: if
[a]H = [a ]H and [b]H = [b ]H , then [a + b]H = [a + b ]H .

   We can easily verify that this operation makes G/H into an abelian group. We
need to check that the four properties of Definition 6.1 are satisfied:

    (i) Associativity:

                  [a]H + ([b]H + [c]H ) = [a]H + [b + c]H = [a + (b + c)]H
                                             = [(a + b) + c]H = [a + b]H + [c]H
                                             = ([a]H + [b]H ) + [c]H .

         Here, we have used the definition of addition of cosets, and the correspond-
         ing associativity property for G.
   (ii) Identity element: the coset [0G]H = H acts as the identity element, since

              [a]H + [0G]H = [a + 0G]H = [a]H = [0G + a]H = [0G]H + [a]H .

(iii) Inverses: the inverse of the coset [a]H is [-a]H , since
       [a]H + [-a]H = [a + (-a)]H = [0G]H = [(-a) + a]H = [-a]H + [a]H .

(iv) Commutativity:
                   [a]H + [b]H = [a + b]H = [b + a]H = [b]H + [a]H .

   The group G/H is called the quotient group of G modulo H. The order of

the group G/H is sometimes denoted [G : H] and is called the index of H in
G. Note that if H = G, then the quotient group G/H is the trivial group, and so
[G : H] = 1.

   Multiplicative notation: if G is written multiplicatively, then the definition of the
group operation of G/H is expressed [a]H · [b]H := [a · b]H ; the identity element
of G/H is [1G]H = H, and the inverse of [a]H is [a-1]H .

Theorem 6.17. Suppose G is a finite abelian group and H is a subgroup of G.
Then [G : H] = |G|/|H|. Moreover, if K is a subgroup of H, then

                                 [G : K] = [G : H][H : K].

Proof. The fact that [G : H] = |G|/|H| follows directly from Theorem 6.14. The
fact that [G : K] = [G : H][H : K] follows from a simple calculation:

                       [G : H] = |G| = |G|/|K| = [G : K] . 2
                                     |H| |H|/|K| [H : K]

Example 6.31. For each n  1, the group Zn is precisely the quotient group
Z/nZ. 2
6.3 Cosets and quotient groups                                      141

Example 6.32. Continuing with Example 6.30, let G := Z6 and H := 3G =
{[0], [3]}. The quotient group G/H has order 3, and consists of the cosets

 := {[0], [3]},  := {[1], [4]},  := {[2], [5]}.

If we write out an addition table for G, grouping together elements in cosets of H
in G, then we also get an addition table for the quotient group G/H:

                            + [0] [3] [1] [4] [2] [5]
                           [0] [0] [3] [1] [4] [2] [5]
                           [3] [3] [0] [4] [1] [5] [2]
                           [1] [1] [4] [2] [5] [3] [0]
                           [4] [4] [1] [5] [2] [0] [3]
                           [2] [2] [5] [3] [0] [4] [1]
                           [5] [5] [2] [0] [3] [1] [4]

   This table illustrates quite graphically the point of Theorem 6.16: for every two
cosets, if we take any element from the first and add it to any element of the second,
we always end up in the same coset.

   We can also write down just the addition table for G/H:

                                         + 
                                         
                                         
                                          

   Note that by replacing  with [0]3,  with [1]3, and  with [2]3, the addition table
for G/H becomes the addition table for Z3. In this sense, we can view G/H as
essentially just a "renaming" of Z3. 2

Example 6.33. Let us return to Example 6.26. The multiplicative group Z15, as we
saw, is of order 8. The subgroup (Z15)2 of Z15  has order 2. Therefore, the quotient
group Z15/(Z15)2 has order 4. Indeed, the cosets are

00 := (Z15)2 = {[1], [4]},      01 := [-1](Z15)2 = {[-1], [-4]},
10 := [2](Z15)2 = {[2], [-7]},  11 := [-2](Z15)2 = {[-2], [7]}.

We can write down the multiplication table for the quotient group:

                                  · 00 01 10 11
                                 00 00 01 10 11
                                 01 01 00 11 10
                                 10 10 11 00 01
                                 11 11 10 01 00

Note that this group is essentially just a "renaming" of the additive group Z2×Z2. 2
142  Abelian groups

Example 6.34. As we saw in Example 6.27, (Z5)2 = {[±1]}. Therefore, the
quotient group Z5/(Z5)2 has order 2. The cosets of (Z5)2 in Z5 are 0 := {[±1]}
and 1 := {[±2]}, and the multiplication table looks like this:

     · 0 1
     0 0 1
     1 1 0

We see that the quotient group is essentially just a "renaming" of Z2. 2

EXERCISE 6.15. Write down the cosets of (Z35)2 in Z35, along with the multipli-
cation table for the quotient group Z35/(Z35)2.

EXERCISE 6.16. Let n be an odd, positive integer whose factorization into primes
is n = p1e1 · · · prer . Show that [Zn : (Zn)2] = 2r.

EXERCISE 6.17. Let n be a positive integer, and let m be any integer. Show that
[Zn : mZn] = n/ gcd(m, n).

EXERCISE 6.18. Let G be an abelian group and H a subgroup with [G : H] = 2.
Show that if a, b  G \ H, then a + b  H.

EXERCISE 6.19. Let H be a subgroup of an abelian group G, and let a, b  G
with a  b (mod H). Show that ka  kb (mod H) for all k  Z.

EXERCISE 6.20. Let G be an abelian group, and let  be an equivalence relation
on G. Further, suppose that for all a, a , b  G, if a  a , then a + b  a + b. Let
H := {a  G : a  0G}. Show that H is a subgroup of G, and that for all a, b  G,
we have a  b if and only if a  b (mod H).

EXERCISE 6.21. Let H be a subgroup of an abelian group G, and let a, b  G.
Show that [a + b]H = {x + y : x  [a]H , y  [b]H }.

                   6.4 Group homomorphisms and isomorphisms

In this section, we study maps that relate the structure of one group to another. Such
maps are often very useful, as they may allow us to transfer hard-won knowledge
about one group to another, perhaps more mysterious, group.

Definition 6.18. A group homomorphism is a function  from an abelian group
G to an abelian group G such that (a + b) = (a) + (b) for all a, b  G.

   Note that in the equality (a + b) = (a) + (b) in the above definition, the
addition on the left-hand side is taking place in the group G while the addition on
the right-hand side is taking place in the group G .
6.4 Group homomorphisms and isomorphisms                        143

   Two sets play a critical role in the study of a group homomorphism  : G  G .
The first set is the image of , that is, the set (G) = {(a) : a  G}. The second

set is the kernel of , defined as the set of all elements of G that are mapped to
0G by , that is, the set -1({0G }) = {a  G : (a) = 0G }. We introduce the
following notation for these sets: Im  denotes the image of , and Ker  denotes

the kernel of .

Example 6.35. If H is a subgroup of an abelian group G, then the inclusion map
i : H  G is obviously a group homomorphism. 2

Example 6.36. Suppose H is a subgroup of an abelian group G. We define the
map

                                         : G  G/H
                                               a  [a]H .

It is not hard to see that this is a group homomorphism. Indeed, this follows almost
immediately from the way we defined addition in the quotient group G/H:

                  (a + b) = [a + b]H = [a]H + [b]H = (a) + (b).

It is clear that  is surjective. It is also not hard to see that Ker  = H; indeed, H
is the identity element in G/H, and [a]H = H if and only if a  H. The map  is
called the natural map from G to G/H. 2

Example 6.37. For a given positive integer n, the natural map from Z to Zn sends
a  Z to the residue class [a]n. This map is a surjective group homomorphism with
kernel nZ. 2

Example 6.38. Suppose G is an abelian group and m is an integer. The map

: GG
      a  ma

is a group homomorphism, since
                     (a + b) = m(a + b) = ma + mb = (a) + (b).

The image of this homomorphism is the subgroup mG and the kernel is the sub-

group G{m}. We call this map the m-multiplication map on G. If G is written
multiplicatively, then this map, which sends a  G to am  G, is called the m-
power map on G, and its image is Gm. 2

Example 6.39. Let p be an odd prime. Consider the 2-power, or squaring, map on
Zp. Then as we saw in Example 6.21, the image (Zp)2 of this map is a subgroup
of Zp of order (p - 1)/2, and its kernel is Zp{2} = {[±1]}. 2
144                         Abelian groups

Example 6.40. Consider the m-multiplication map on Z. As we saw in Exam-
ple 6.22, its image mZ is equal to Z if and only if m = ±1, while its kernel Z{m}
is equal to Z if m = 0, and is equal to {0} otherwise. 2

Example 6.41. Consider the m-multiplication map on Zn. As we saw in Exam-
ple 6.23, if d := gcd(m, n), the image mZn of this map is a subgroup of Zn of order
n/d, while its kernel Zn{m} is a subgroup of order d. 2

Example 6.42. Suppose G is an abelian group and a is an element of G. It is easy
to see that the map

                                          : ZG
                                                z  za

is a group homomorphism, since
                  (z + z ) = (z + z )a = za + z a = (z) + (z ). 2

Example 6.43. As a special case of the previous example, let n be a positive integer

and let  be an element of Zn. Let  : Z  Zn be the group homomorphism that
sends z  Z to z  Zn. That  is a group homomorphism means that z+z = zz
for all z, z  Z (note that the group operation is addition in Z and multiplication
in Zn). If the multiplicative order of  is equal to k, then as discussed in §2.7, the
image of  consists of the k distinct group elements 0, 1, . . . , k-1. The kernel
of  consists of those integers z such that z = 1. Again by the discussion in §2.7,

the kernel of  is equal to the subgroup kZ. 2

Example 6.44. Generalizing Example 6.42, the reader may verify that if a1, . . . , ak
are fixed elements of an abelian group G, then the map

     :                      Z×k  G

        (z1, . . . , zk)  z1a1 + · · · + zkak

is a group homomorphism. 2

Example 6.45. Suppose that H1, . . . , Hk are subgroups of an abelian group G. The
reader may easily verify that the map

                            : H1 × · · · × Hk  G
                                     (a1, . . . , ak)  a1 + · · · + ak

is a group homomorphism whose image is the subgroup H1 + · · · + Hk. 2

   The following theorem summarizes some of the most important properties of
group homomorphisms.
6.4 Group homomorphisms and isomorphisms                          145

Theorem 6.19. Let  be a group homomorphism from G to G . Then:
    (i) (0G) = 0G ;
   (ii) (-a) = -(a) for all a  G;

   (iii) (na) = n(a) for all n  Z and a  G;

   (iv) if H is a subgroup of G, then (H) is a subgroup of G ; in particular
         (setting H := G), Im  is a subgroup of G ;

    (v) if H is a subgroup of G , then -1(H ) is a subgroup of G; in particular
         (setting H := {0G }), Ker  is a subgroup of G;

   (vi) for all a, b  G, (a) = (b) if and only if a  b (mod Ker );
  (vii)  is injective if and only if Ker  = {0G}.

Proof. These are all straightforward calculations.
    (i) We have
                     0G + (0G) = (0G) = (0G + 0G) = (0G) + (0G).

     Now cancel (0G) from both sides.
(ii) We have

                       0G = (0G) = (a + (-a)) = (a) + (-a),

      and hence (-a) is the inverse of (a).
(iii) For n = 0, this follows from part (i). For n > 0, this follows from the

      definitions by induction on n. For n < 0, this follows from the positive case
      and part (ii).
(iv) For all a, b  H, we have a + b  H and -a  H; hence, (H) contains
      (a + b) = (a) + (b) and (-a) = -(a).
 (v) -1(H ) is non-empty, since (0G) = 0G  H . If (a)  H and
      (b)  H , then (a + b) = (a) + (b)  H , and (-a) = -(a)  H .

(vi) We have

                (a) = (b)  (a) - (b) = 0G  (a - b) = 0G

                                 a - b  Ker   a  b (mod Ker ).

(vii) If  is injective, then in particular, -1({0G }) cannot contain any other ele-
      ment besides 0G. If  is not injective, then there exist two distinct elements
      a, b  G with (a) = (b), and by part (vi), Ker  contains the element

      a - b, which is non-zero. 2

   Part (vii) of the above theorem is particularly useful: to check that a group
homomorphism is injective, it suffices to determine if Ker  = {0G}. Thus, the
146  Abelian groups

injectivity and surjectivity of a given group homomorphism  : G  G             may be
characterized in terms of its kernel and image:

     ·  is injective if and only if its kernel is trivial (i.e. Ker  = {0G});
     ·  is surjective if and only if Im  = G .

   We next present two very easy theorems that allow us to compose group homo-
morphisms in simple ways.

Theorem 6.20. If  : G  G and  : G  G are group homomorphisms, then
so is their composition    : G  G .

Proof. For all a, b  G, we have

                ((a + b)) =  ((a) + (b)) =  ((a)) +  ((b)). 2

Theorem 6.21. Let i : G  Gi, for i = 1, . . . , k, be group homomorphisms.
Then the map

                                 : G  G1 × · · · × Gk
                                       a  (1(a), . . . , k(a))

is a group homomorphism.

Proof. For all a, b  G, we have

    (a + b) = (1(a + b), . . . , k(a + b)) = (1(a) + 1(b), . . . , k(a) + k(b))
               = (a) + (b). 2

   Consider a group homomorphism  : G  G . If  is bijective, then  is called

a group isomorphism of G with G . If such a group isomorphism  exists, we say
that G is isomorphic to G , and write G = G . Moreover, if G = G , then  is
called a group automorphism on G.

Theorem 6.22. If  is a group isomorphism of G with G , then the inverse func-
tion -1 is a group isomorphism of G with G.

Proof. For all a , b  G , we have
               (-1(a ) + -1(b )) = (-1(a )) + (-1(b )) = a + b ,

and hence -1(a ) + -1(b ) = -1(a + b ). 2

   Because of this theorem, if G is isomorphic to G , we may simply say that "G
and G are isomorphic."

   We stress that a group isomorphism  : G  G is essentially just a "renaming"
of the group elements. This can be visualized as follows. Imagine the addition
table for G written out with rows and columns labeled by elements of G, with the
6.4 Group homomorphisms and isomorphisms               147

entry in row a and column b being a + b. Now suppose we use the function 

to consistently rename all the elements of G appearing in this table: the label on

row a is replaced by (a), the label on column b by (b), and the entry in row a
and column b by (a + b). Because  is bijective, every element of G appears

exactly once as a label on a row and as a label on a column; moreover, because
(a + b) = (a) + (b), what we end up with is an addition table for G . It follows

that all structural properties of the group are preserved, even though the two groups
might look quite different syntactically.

Example 6.46. As was shown in Example 6.32, the quotient group G/H discussed
in that example is isomorphic to Z3. As was shown in Example 6.33, the quotient
group Z15/(Z15)2 is isomorphic to Z2 × Z2. As was shown in Example 6.34, the
quotient group Z5/(Z5)2 is isomorphic to Z2. 2

Example 6.47. If gcd(m, n) = 1, then the m-multiplication map on Zn is a group
automorphism. 2

   The next theorem tells us that corresponding to any group homomorphism, there
is a natural group isomomorphism. As group isomorphisms are much nicer than
group homomorphisms, this is often very useful.

Theorem 6.23 (First isomorphism theorem). Let  : G  G be a group homo-
morphism with kernel K and image H . Then we have a group isomorphism

                                          G/K = H .

Specifically, the map

                        : G/K  G
                             [a]K  (a)

is an injective group homomorphism whose image is H .

Proof. Using part (vi) of Theorem 6.19, we see that for all a, b  G, we have

                 [a]K = [b]K  a  b (mod K)  (a) = (b).

This immediately implies that the definition of  is unambiguous ([a]K = [b]K
implies (a) = (b)), and that  is injective ((a) = (b) implies [a]K = [b]K ).
It is clear that  maps onto H , since every element of H is of the form (a) for
some a  G, and the map  sends [a]K to (a). Finally, to see that  is a group
homomorphism, note that

([a]K + [b]K ) = ([a + b]K ) = (a + b) = (a) + (b) = ([a]K ) + ([b]K ). 2

We can generalize the previous theorem, as follows:
148               Abelian groups

Theorem 6.24. Let  : G  G be a group homomorphism. Then for every
subgroup H of G with H  Ker , we may define a group homomorphism

                                       : G/H  G
                                             [a]H  (a).

Moreover, Im  = Im , and  is injective if and only if H = Ker .

Proof. Using the assumption that H  Ker , we see that  is unambiguously
defined, since for all a, b  G, we have

  [a]H = [b]H = a  b (mod H) = a  b (mod Ker ) = (a) = (b).

That  is a group homomorphism, with Im  = Im , follows as in the proof of The-
orem 6.23. If H = Ker , then by Theorem 6.23,  is injective, and if H Ker ,
then  is not injective, since if we choose a  Ker  \ H, we see that ([a]H ) = 0G ,
and hence Ker  is non-trivial. 2

   The next theorem gives us another important construction of a group isomor-
phism.

Theorem 6.25 (Internal direct product). Let G be an abelian group with sub-
groups H1, H2, where H1  H2 = {0G}. Then we have a group isomorphism

                                    H1 × H2 = H1 + H2

given by the map

                   : H1 × H2  H1 + H2
                          (a1, a2)  a1 + a2.

Proof. We already saw that  is a surjective group homomorphism in Example 6.45.
To see that  is injective, it suffices to show that Ker  is trivial; that is, it suffices
to show that for all a1  H1 and a2  H2, if a1 + a2 = 0G, then a1 = a2 = 0G. But
a1 + a2 = 0G implies a1 = -a2  H2, and hence a1  H1  H2 = {0G}, and so
a1 = 0G. Similarly, one shows that a2 = 0G, and that finishes the proof. 2

   If H1, H2 are as in the above theorem, then H1 + H2 is sometimes called the

internal direct product of H1 and H2.

Example 6.48. We can use the general theory developed so far to get a quick-
and-dirty proof of the Chinese remainder theorem (Theorem 2.6). Let {ni}ik=1 be a
pairwise relatively prime family of positive integers, and let n := ik=1 ni. Consider
the map

                                  : Z  Zn1 × · · · × Znk

                                       a  ([a]n1, . . . , [a]nk ).
6.4 Group homomorphisms and isomorphisms  149

It is easy to see that this map is a group homomorphism; indeed, it is the map
constructed in Theorem 6.21 applied with the natural maps i : Z  Zni, for
i = 1, . . . , k. Evidently, a  Ker  if and only if ni | a for i = 1, . . . , k, and since
{ni}ik=1 is pairwise relatively prime, it follows that a  Ker  if and only if n | a;
that is, Ker  = nZ. Theorem 6.23 then gives us an injective group homomorphism

                                : Zn  Zn1 × · · · × Znk
                                     [a]n  ([a]n1, . . . , [a]nk ).

But since the sets Zn and Zn1 × · · · × Znk have the same size, injectivity implies
surjectivity. From this, Theorem 2.6 is immediate.

   The map  is a group isomorphism

                                    Zn = Zn1 × · · · × Znk .

In fact, the map  is the same as the map  in Theorem 2.8, and so we also imme-
diately obtain parts (i), (ii), (iii.a), and (iii.b) of that theorem.

   Observe that parts (iii.c) and (iii.d) of Theorem 2.8 imply that restricting the
map  to Zn yields an isomorphism of multiplicative groups

                                    Zn = Zn1  × · · · × Znk  .

This fact does not follow from the general theory developed so far; however, in the
next chapter, we will see how this fact fits into the broader algebraic picture.

   One advantage of our original proof of Theorem 2.6 is that it gives us an explicit
formula for the inverse map -1, which is useful in computations. 2

Example 6.49. Let n1, n2 be positive integers with n1 | n2. Consider the natural
map  : Z  Zn1. This is a surjective group homomorphism with Ker  = n1Z.
Since H := n2Z  n1Z, we may apply Theorem 6.24 with the subgroup H,

obtaining the surjective group homomorphism

                                     : Zn2  Zn1
                                          [a]n2  [a]n1 . 2

Example 6.50. Let us revisit Example 6.23. Let n be a positive integer, and let m
be any integer. Let 1 : Z  Zn be the natural map, and let 2 : Zn  Zn be
the m-multiplication map. The composed map  := 2  1 from Z to Zn is also
a group homomorphism. For each z  Z, we have (z) = m[z]n = [mz]n. The
kernel of  consists of those integers z such that mz  0 (mod n), and so part (ii)
of Theorem 2.5 implies that Ker  = (n/d)Z, where d := gcd(m, n). The image of
 is mZn. Theorem 6.23 therefore implies that the map

                                      : Zn/d  mZn
                                           [z]n/d  m[z]n
150                        Abelian groups

is a group isomorphism. 2

Example 6.51. Consider the group Zp where p is an odd prime, and let  : Zp  Zp
be the squaring map. By definition, Im  = (Zp)2, and we proved in Theorem 2.18
that Ker  = {[±1]}. Theorem 2.19 says that for all ,   Zp, 2 = 2 if
and only if  = ±. This fact can also be seen to be a special case of part
(vi) of Theorem 6.19. Theorem 6.23 says that Zp/Ker  = Im , and since
|Zp/Ker | = |Zp|/|Ker | = (p - 1)/2, we see that Theorem 2.20, which says
that |(Zp)2| = (p - 1)/2, follows from this.

   Let H := (Zp)2, and consider the quotient group Zp/H. Since |H| = (p - 1)/2,
we know that |Zp/H| = |Zp|/|H| = 2, and hence Zp/H consists of the two cosets
H and H := Zp \ H.

   Let  be an arbitrary, fixed element of H, and consider the map

                                        : Z  Zp/H
                                              z  [z]H .

It is easy to see that  is a group homomorphism; indeed, it is the composition
of the homomorphism discussed in Example 6.43 and the natural map from Zp to
Zp/H. Moreover, it is easy to see (for example, as a special case of Theorem 2.17)
that

                                   z  H  z is even.

From this, it follows that Ker  = 2Z; also, since Zp/H consists of just the two
cosets H and H, it follows that  is surjective. Therefore, Theorem 6.23 says that
the map

                                       : Z2  Zp/H
                                            [z]2  [z]H

is a group isomorphism, under which [0]2 corresponds to H, and [1]2 corresponds
to H.

   This isomorphism gives another way to derive Theorem 2.23, which says that
in Zp, the product of two non-squares is a square; indeed, the statement "non-zero
plus non-zero equals zero in Z2" translates via the isomorphism  to the statement
"non-square times non-square equals square in Zp." 2

Example 6.52. Let Q be the multiplicative group of non-zero rational numbers.
Let H1 be the subgroup {±1}, and let H2 be the subgroup of positive rationals. It
is easy to see that Q = H1 · H2 and that H1  H2 = {1}. Thus, Q is the internal
direct product of H1 and H2, and Theorem 6.25 gives us a group isomorphism
Q = H1 × H2. 2
6.4 Group homomorphisms and isomorphisms  151

   Let G and G be abelian groups. Recall from Example 6.19 that Map(G, G )
is the group of all functions  : G  G , where the group operation is defined
point-wise using the group operation of G :

                    ( + )(a) = (a) + (a) and (-)(a) = -(a)

for all ,   Map(G, G ) and all a  G. The following theorem isolates an impor-
tant subgroup of this group.

Theorem 6.26. Let G and G be abelian groups, and consider the group of func-
tions Map(G, G ). Then

          Hom(G, G ) := {  Map(G, G ) :  is a group homomorphism}

is a subgroup of Map(G, G ).

Proof. First, observe that Hom(G, G ) is non-empty, as it contains the map that

sends everything in G to 0G (this is the identity element of Map(G, G )).

   Next, we have to show that if  and  are homomorphisms from G to G , then
so are  +  and -. But  +  = 2  1, where 1 : G  G × G is the map
constructed in Theorem 6.21, applied with  and , and 2 : G × G  G is as in
Example 6.45. Also, - = -1  , where -1 is the (-1)-multiplication map. 2

EXERCISE 6.22. Verify that the "is isomorphic to" relation on abelian groups is
an equivalence relation; that is, for all abelian groups G1, G2, G3, we have:

    (a) G1 = G1;
   (b) G1 = G2 implies G2 = G1;
    (c) G1 = G2 and G2 = G3 implies G1 = G3.

EXERCISE 6.23. Let i : Gi  Gi, for i = 1, . . . , k, be group homomorphisms.
Show that the map

                        : G1 × · · · × Gk  G1 × · · · × Gk
                                 (a1, . . . , ak)  (1(a1), . . . , k(ak))

is a group homomorphism. Also show that if each i is an isomorphism, then so is
.

EXERCISE 6.24. Let  : G  G be a group homomorphism. Let H, K be sub-
groups of G and let m be a positive integer. Show that (H + K) = (H) + (K)
and (mH) = m(H).

EXERCISE 6.25. Let  : G  G be a group homomorphism. Let H be a subgroup
of G, and let  : H  G be the restriction of  to H. Show that  is a group
homomorphism and that Ker  = Ker   H.
152  Abelian groups

EXERCISE 6.26. Suppose G1, . . . , Gk are abelian groups. Show that for each
i = 1, . . . , k, the projection map i : G1 × · · · × Gk  Gi that sends (a1, . . . , ak) to
ai is a surjective group homomorphism.

EXERCISE 6.27. Show that if G = G1 × G2 for abelian groups G1 and G2, and H1
is a subgroup of G1 and H2 is a subgroup of G2, then we have a group isomorphism
G/(H1 × H2) = G1/H1 × G2/H2.

EXERCISE 6.28. Let G be an abelian group with subgroups H and K.
   (a) Show that we have a group isomorphism (H + K)/K = H/(H  K).
   (b) Show that if H and K are finite, then |H + K| = |H||K|/|H  K|.

EXERCISE 6.29. Let G be an abelian group with subgroups H, K, and A, where
K  H. Show that (H  A)/(K  A) is isomorphic to a subgroup of H/K.

EXERCISE 6.30. Let  : G  G be a group homomorphism with kernel K. Let
H be a subgroup of G. Show that we have a group isomorphism G/(H + K) =

(G)/(H ).

EXERCISE 6.31. Let  : G  G be a surjective group homomorphism. Let S be
the set of all subgroups of G that contain Ker , and let S be the set of all subgroups
of G . Show that the sets S and S are in one-to-one correspondence, via the map
that sends H  S to (H)  S . Also show that this correspondence preserves
inclusions; that is, for all H1, H2  S, we have H1  H2  (H1)  (H2).

EXERCISE 6.32. Use the previous exercise, together with Theorem 6.9, to get a
short proof of Theorem 6.10.

EXERCISE 6.33. Show that the homomorphism of Example 6.44 arises by direct
application of Example 6.42, combined with Theorems 6.20 and 6.21.

EXERCISE 6.34. Suppose that G, G1, and G2 are abelian groups, and that  :
G1 × G2  G is a group isomorphism. Let H1 := (G1 × {0G2}) and H2 :=
({0G1} × G2). Show that G is the internal direct product of H1 and H2.

EXERCISE 6.35. Let Z+ denote the set of positive integers, and let Q be the
multiplicative group of non-zero rational numbers. Consider the abelian groups
Map#(Z+, Z) and Map#(Z+, Z2), as defined in Exercise 6.14. Show that we have
group isomorphisms

    (a) Q = Z2 × Map#(Z+, Z), and
   (b) Q/(Q)2 = Map#(Z+, Z2).

EXERCISE 6.36. Let n be an odd, positive integer whose factorization into primes
is n = p1e1 · · · prer . Show that:
6.5 Cyclic groups  153

(a) we have a group isomorphism Zn/(Zn)2 = Z2×r;
(b) if pi  3 (mod 4) for each i = 1, . . . , r, then the squaring map on (Zn)2 is a

     group automorphism.

EXERCISE 6.37. Which of the following pairs of groups are isomorphic? Why or

why not? (a) Z2 × Z2 and Z4, (b) Z12  and Z8, (c) Z5 and Z4, (d) Z2 × Z and Z, (e)
Q and Z, (f) Z × Z and Z.

                                      6.5 Cyclic groups
Let G be an abelian group. For a  G, define a := {za : z  Z}. It is easy
to see that a is a subgroup of G; indeed, it is the image of the group homo-
morphism discussed in Example 6.42. Moreover, a is the smallest subgroup of
G containing a; that is, a contains a, and every subgroup of G that contains a
must contain everything in a . Indeed, if a subgroup contains a, it must contain
a + a = 2a, a + a + a = 3a, and so on; it must also contain 0G = 0a, -a = (-1)a,
(-a) + (-a) = (-2)a, and so on. The subgroup a is called the subgroup (of G)
generated by a. Also, one defines the order of a to be the order of the subgroup
 a.

   More generally, for a1, . . . , ak  G, we define

                  a1, . . . , ak := {z1a1 + · · · + zkak : z1, . . . , zk  Z}.

It is easy to see that a1, . . . , ak is a subgroup of G; indeed, it is the image of
the group homomorphism discussed in Example 6.44. Moreover, this subgroup is
the smallest subgroup of G that contains a1, . . . , ak; that is, a1, . . . , ak contains
the elements a1, . . . , ak, and every subgroup of G that contains these elements
must contain everything in a1, . . . , ak . The subgroup a1, . . . , ak is called the
subgroup (of G) generated by a1, . . . , ak.

   An abelian group G is called cyclic if G = a for some a  G, in which case,
a is called a generator for G. An abelian group G is called finitely generated if
G = a1, . . . , ak for some a1, . . . , ak  G.

   Multiplicative notation: if G is written multiplicatively, then a := {az : z  Z},
and a1, . . . , ak := {a1z1 · · · akzk : z1, . . . , zk  Z}; also, for emphasis and clarity,
we use the term multiplicative order of a.

Example 6.53. Consider the additive group Z. This is a cyclic group, with 1 being
a generator:

                          1 = {z · 1 : z  Z} = {z : z  Z} = Z.
154                        Abelian groups

For every m  Z, we have
                        m = {zm : z  Z} = {mz : z  Z} = mZ.

It follows that the only elements of Z that generate Z are 1 and -1: every other
element generates a subgroup that is strictly contained in Z. 2

Example 6.54. For n > 0, consider the additive group Zn. This is a cyclic group,
with [1] being a generator:

                       [1] = {z[1] : z  Z} = {[z] : z  Z} = Zn.

For every m  Z, we have
         [m] = {z[m] : z  Z} = {[zm] : z  Z} = {m[z] : z  Z} = mZn.

By Example 6.23, the subgroup mZn has order n/ gcd(m, n). Thus, [m] has order
n/ gcd(m, n); in particular, [m] generates Zn if and only if m is relatively prime to
n, and hence, the number of generators of Zn is (n). 2

Implicit in Examples 6.53 and 6.54 is the following general fact:

Theorem 6.27. Let G be a cyclic group generated by a. Then for every m  Z,
we have

                                           ma = mG.

Proof. We have

     ma = {z(ma) : z  Z} = {m(za) : z  Z} = m a = mG. 2

The following two examples present some groups that are not cyclic.

Example 6.55. Consider the additive group G := Z × Z. Set
                         1 := (1, 0)  G and 2 := (0, 1)  G.

It is not hard to see that G = 1, 2 , since for all z1, z2  Z, we have
                        z11 + z22 = (z1, 0) + (0, z2) = (z1, z2).

However, G is not cyclic. To see this, let  = (b1, b2) be an arbitrary element of G.
We claim that one of 1 or 2 does not belong to  . Suppose to the contrary that
both 1 and 2 belong to  . This would imply that there exist integers z and z
such that

                zb1 = 1,                   zb2 = 0,
                z b1 = 0,                  z b2 = 1.
                        6.5 Cyclic groups          155

Multiplying the upper left equality by the lower right, and the upper right by the
lower left, we obtain

                                       1 = zz b1b2 = 0,

which is impossible. 2

Example 6.56. Consider the additive group G := Zn1 × Zn2. Set
                 1 := ([1]n1, [0]n2 )  G and 2 := ([0]n1, [1]n2 )  G.

It is not hard to see that G = 1, 2 , since for all z1, z2  Z, we have
           z11 + z22 = ([z1]n1 , [0]n2 ) + ([0]n1 , [z2]n2 ) = ([z1]n1 , [z2]n2 ).

However, G may or may not be cyclic: it depends on d := gcd(n1, n2).
   If d = 1, then G is cyclic, with  := ([1]n1, [1]n2 ) being a generator. One can

see this easily using the Chinese remainder theorem: for all z1, z2  Z, there exists
z  Z such that

                         z  z1 (mod n1) and z  z2 (mod n2),

which implies
                            z = ([z]n1 , [z]n2 ) = ([z1]n1 , [z2]n2 ).

   If d > 1, then G is not cyclic. To see this, let  = ([b1]n1, [b2]n2 ) be an arbitrary
element of G. We claim that one of 1 or 2 does not belong to  . Suppose to
the contrary that both 1 and 2 belong to  . This would imply that there exist
integers z and z such that

zb1  1 (mod n1),        zb2  0 (mod n2),
z b1  0 (mod n1),       z b2  1 (mod n2).

All of these congruences hold modulo d as well, and multiplying the upper left
congruence by the lower right, and the upper right by the lower left, we obtain

                        1  zz b1b2  0 (mod d),

which is impossible. 2

   It should be clear that since a group isomorphism preserves all structural prop-
erties of groups, it preserves the property of being cyclic. We state this, along with
related facts, as a theorem.

Theorem 6.28. Let  : G  G be a group isomorphism.
    (i) For all a  G, we have ( a ) = (a) .
156  Abelian groups

(ii) For all a  G, a and (a) have the same order.
(iii) G is cyclic if and only if G is cyclic.

Proof. For all a  G, we have

                ( a ) = {(za) : z  Z} = {z(a) : z  Z} = (a) .

That proves (i).
   (ii) follows from (i) and the fact that  is injective.
   (iii) follows from (i), as follows. If G is cyclic, then G = a , and since  is

surjective, we have G = (G) = (a) . The converse follows by applying the
same argument to the inverse isomorphism -1 : G  G. 2

Example 6.57. Consider again the additive group G := Zn1 × Zn2, discussed in
Example 6.56. If gcd(n1, n2) = 1, then one can also see that G is cyclic as follows:
by the discussion in Example 6.48, we know that G is isomorphic to Zn1n2, and
since Zn1n2 is cyclic, so is G. 2

Example 6.58. Consider again the subgroup mZn of Zn, discussed in Exam-
ple 6.54. One can also see that this is cyclic of order n/d, where d := gcd(m, n), as
follows: in Example 6.50, we constructed an isomorphism between Zn/d and mZn,
and this implies mZn is cyclic of order n/d. 2

Classification of cyclic groups. Examples 6.53 and 6.54 are extremely important
examples of cyclic groups. Indeed, as we shall now demonstrate, every cyclic
group is isomorphic either to Z or to Zn for some n > 0.

   Suppose that G is a cyclic group with generator a. Consider the map  : Z  G
that sends z  Z to za  G. As discussed in Example 6.42, this map is a group
homomorphism, and since a is a generator for G, it must be surjective. There are
two cases to consider.
Case 1: Ker  = {0}. In this case,  is an isomorphism of Z with G.
Case 2: Ker  = {0}. In this case, since Ker  is a subgroup of Z different from

         {0}, by Theorem 6.9, it must be of the form nZ for some n > 0. Hence, by
         Theorem 6.23, the map  : Zn  G that sends [z]n to za is an isomorphism
         of Zn with G.

   Based on this isomorphism, we immediately obtain:

Theorem 6.29. Let G be an abelian group and let a  G. If there exists a positive
integer m such that ma = 0G, then the least such positive integer n is the order of

a; in this case, we have:

     · for every integer z, za = 0G if and only if n divides z, and more generally,
         for all integers z1, z2, we have z1a = z2a if and only if z1  z2 (mod n);
6.5 Cyclic groups                                     157

· the subgroup a consists of the n distinct elements

0 · a, 1 · a, . . . , (n - 1) · a.

Otherwise, a has infinite order, and every element of a can be expressed as za
for some unique integer z.

   In the case where the group is finite, we can say more:

Theorem 6.30. Let G be a finite abelian group and let a  G. Then |G|a = 0G
and the order of a divides |G|.

Proof. Since a is a subgroup of G, by Lagrange's theorem (Theorem 6.15), the
order of a divides |G|. It then follows by Theorem 6.29 that |G|a = 0G. 2

Example 6.59. Let a, n  Z with n > 0 and gcd(a, n) = 1, and let  := [a]  Zn.
Theorem 6.29 implies that the definition given in this section of the multiplicative
order of  is consistent with that given in §2.7. Moreover, Euler's theorem (Theo-
rem 2.13) can be seen as just a special case of Theorem 6.30. Also, note that  is a
generator for Zn if and only if a is a primitive root modulo p. 2

Example 6.60. As we saw in Example 6.26, all elements of Z15  have multiplicative
order dividing 4, and since Z15  has order 8, we conclude that Z15  is not cyclic. 2
Example 6.61. The group Z5 is cyclic, with [2] being a generator:

                    [2]2 = [4] = [-1], [2]3 = [-2], [2]4 = [1]. 2

Example 6.62. Based on the calculations in Example 2.9, we may conclude that
Z7 is cyclic, with both [3] and [5] being generators. 2
Example 6.63. Consider again the additive group G := Zn1 × Zn2, discussed in
Example 6.56. If d := gcd(n1, n2) > 1, then one can also see that G is not cyclic as
follows: for every   G, we have (n1n2/d) = 0G, and hence by Theorem 6.29,
the order of  divides n1n2/d. 2

   The following two theorems completely characterize the subgroup structure of
cyclic groups. Actually, we have already proven most of the results in these two
theorems, but nevertheless, they deserve special emphasis.

Theorem 6.31. Let G be a cyclic group of infinite order.

    (i) G is isomorphic to Z.
   (ii) There is a one-to-one correspondence between the non-negative integers

         and the subgroups of G, where each such integer m corresponds to the
         cyclic group mG.
158  Abelian groups

   (iii) For every two non-negative integers m, m , we have mG  m G if and only
         if m | m.

Proof. That G = Z was established in our classification of cyclic groups, and so
it suffices to prove the other statements of the theorem for G = Z. As we saw in
Example 6.53, for every integer m, the subgroup mZ is cyclic, as it is generated by
m. This fact, together with Theorem 6.9, establishes all the other statements. 2

Theorem 6.32. Let G be a cyclic group of finite order n.

    (i) G is isomorphic to Zn.
   (ii) There is a one-to-one correspondence between the positive divisors of n

         and the subgroups of G, where each such divisor d corresponds to the
         subgroup dG; moreover, dG is a cyclic group of order n/d.
   (iii) For each positive divisor d of n, we have dG = G{n/d}; that is, the
         kernel of the (n/d)-multiplication map is equal to the image of the d-
         multiplication map; in particular, G{n/d} has order n/d.

   (iv) For every two positive divisors d, d of n, we have dG  d G if and only if
         d | d.

    (v) For every positive divisor d of n, the number of elements of order d in G
         is (d).

   (vi) For every integer m, we have mG = dG and G{m} = G{d}, where
         d := gcd(m, n).

Proof. That G = Zn was established in our classification of cyclic groups, and so
it suffices to prove the other statements of the theorem for G = Zn.

   The one-to-one correspondence in part (ii) was established in Theorem 6.10. By
the discussion in Example 6.54, it is clear that dZn is generated by [d] and has
order n/d.

   Part (iii) was established in Example 6.23.
   Part (iv) was established in Theorem 6.10.
   For part (v), the elements of order d in Zn are all contained in Zn{d}, and so
the number of such elements is equal to the number of generators of Zn{d}. The
group Zn{d} is cyclic of order d, and so is isomorphic to Zd, and as we saw in
Example 6.54, this group has (d) generators.
   Part (vi) was established in Example 6.23. 2

   Since cyclic groups are in some sense the simplest kind of abelian group, it is
nice to establish some sufficient conditions under which a group must be cyclic.

The following three theorems provide such conditions.

Theorem 6.33. If G is an abelian group of prime order, then G is cyclic.
6.5 Cyclic groups  159

Proof. Let |G| = p, which, by hypothesis, is prime. Let a  G with a = 0G, and

let k be the order of a. As the order of an element divides the order of the group,
we have k | p, and so k = 1 or k = p. Since a = 0G, we must have k = 1, and so
k = p, which implies that a generates G. 2

Theorem 6.34. If G1 and G2 are finite cyclic groups of relatively prime order, then
G1 × G2 is also cyclic. In particular, if G1 is generated by a1 and G2 is generated
by a2, then G1 × G2 is generated by (a1, a2).

Proof. We give a direct proof, based on Theorem 6.29. Let n1 := |G1| and
n2 := |G2|, where gcd(n1, n2) = 1. Also, let a1  G1 have order n1 and a2  G2
have order n2. We want to show that (a1, a2) has order n1n2. Applying The-
orem 6.29 to (a1, a2), we see that the order of (a1, a2) is the smallest positive
integer k such that k(a1, a2) = (0G1, 0G2 ). Now, for every integer k, we have
k(a1, a2) = (ka1, ka2), and

        (ka1, ka2) = (0G1, 0G2 )  n1 | k and n2 | k

                                             (applying Theorem 6.29 to a1 and a2)
                                       n1n2 | k (since gcd(n1, n2) = 1). 2

Theorem 6.35. Let G be a cyclic group. Then for every subgroup H of G, both
H and G/H are cyclic.

Proof. The fact that H is cyclic follows from part (ii) of Theorem 6.31 in the case
where G is infinite, and part (ii) of Theorem 6.32 in the case where G is finite. If
G is generated by a, then it is easy to see that G/H is generated by [a]H . 2

   The next three theorems are often useful in calculating the order of a group
element. The first generalizes Theorem 2.15.

Theorem 6.36. Let G be an abelian group, let a  G be of finite order n, and let
m be an arbitrary integer. Then the order of ma is n/ gcd(m, n).

Proof. Let H := a , and d := gcd(m, n). By Theorem 6.27, we have ma = mH,
and by Theorem 6.32, we have mH = dH, which has order n/d.

   That proves the theorem. Alternatively, we can give a direct proof, based on
Theorem 6.29. Applying Theorem 6.29 to ma, we see that the order of ma is the
smallest positive integer k such that k(ma) = 0G. Now, for every integer k, we
have k(ma) = (km)a, and

  (km)a = 0G  km  0 (mod n) (applying Theorem 6.29 to a)

                   k  0 (mod n/ gcd(m, n)) (by part (ii) of Theorem 2.5). 2
160                 Abelian groups

Theorem 6.37. Suppose that a is an element of an abelian group, and for some
prime p and integer e  1, we have pea = 0G and pe-1a = 0G. Then a has order
pe.

Proof. If m is the order of a, then since pea = 0G, we have m | pe. So m = pf for
some f = 0, . . . , e. If f < e, then pe-1a = 0G, contradicting the assumption that
pe-1a = 0G. 2

Theorem 6.38. Suppose G is an abelian group with a1, a2  G such that a1 is
of finite order n1, a2 is of finite order n2, and gcd(n1, n2) = 1. Then the order of
a1 + a2 is n1n2.

Proof. Let H1 := a1 and H2 := a2 so that |H1| = n1 and |H2| = n2.
   First, we claim that H1  H2 = {0G}. To see this, observe that H1  H2 is a

subgroup of H1, and so |H1 H2| divides n1; similarly, |H1 H2| divides n2. Since
gcd(n1, n2) = 1, we must have |H1  H2| = 1, and that proves the claim.

   Using the claim, we can apply Theorem 6.25, obtaining a group isomorphism
between H1 + H2 and H1 × H2. Under this isomorphism, the group element
a1 + a2  H1 + H2 corresponds to (a1, a2)  H1 × H2, which by Theorem 6.34
(again using the fact that gcd(n1, n2) = 1) has order n1n2. 2

   For an abelian group G, we say that an integer k kills G if kG = {0G}. Consider
the set KG of integers that kill G. Evidently, KG is a subgroup of Z, and hence of
the form mZ for a uniquely determined non-negative integer m. This integer m is
called the exponent of G. If m = 0, then we see that m is the least positive integer

that kills G.

   The following two theorems state some simple properties of the exponent of a

group.

Theorem 6.39. Let G be an abelian group of exponent m.
    (i) For every integer k, k kills G if and only if m | k.
   (ii) If G has finite order, then m divides |G|.

   (iii) If m = 0, then for every a  G, the order of a is finite and divides m.
   (iv) If G is cyclic, then the exponent of G is 0 if G is infinite, and is |G| if G

         is finite.

Proof. Exercise. 2

Theorem 6.40. If G1 and G2 are abelian groups of exponents m1 and m2, then the
exponent of G1 × G2 is lcm(m1, m2).

Proof. Exercise. 2
6.5 Cyclic groups  161

Example 6.64. The additive group Z has exponent 0. 2
Example 6.65. The additive group Zn has exponent n. 2
Example 6.66. The additive group Zn1 × Zn2 has exponent lcm(n1, n2). 2
Example 6.67. The multiplicative group Z15  has exponent 4 (see Example 6.26). 2

   The next two theorems develop some crucial properties about the structure of
finite abelian groups.

Theorem 6.41. If an abelian group G has non-zero exponent m, then G contains
an element of order m. In particular, a finite abelian group is cyclic if and only if
its order equals its exponent.

Proof. The second statement follows immediately from the first. For the first state-
ment, let m = ir=1 piei be the prime factorization of m.

   First, we claim that for each i = 1, . . . , r, there exists ai  G such that (m/pi)ai =
0G. Suppose the claim were false: then for some i, (m/pi)a = 0G for all a  G;
however, this contradicts the minimality property in the definition of the exponent

m. That proves the claim.
   Let a1, . . . , ar be as in the above claim. Then by Theorem 6.37, (m/piei )ai has

order piei for each i = 1, . . . , r. Finally, by Theorem 6.38, the group element

                                (m/p1e1 )a1 + · · · + (m/prer )ar

has order m. 2

Theorem 6.42. Let G be a finite abelian group of order n. If p is a prime dividing
n, then G contains an element of order p.

Proof. We can prove this by induction on n.
   If n = 1, then the theorem is vacuously true.
   Now assume n > 1 and that the theorem holds for all groups of order strictly less

than n. Let a be any non-zero element of G, and let m be the order of a. Since a is
non-zero, we must have m > 1. If p | m, then (m/p)a is an element of order p, and
we are done. So assume that p m and consider the quotient group G/H, where H
is the subgroup of G generated by a. Since H has order m, G/H has order n/m,
which is strictly less than n, and since p m, we must have p | (n/m). So we can
apply the induction hypothesis to the group G/H and the prime p, which says that
there is an element b  G such that the coset [b]H  G/H has order p. If is the
order of b, then b = 0G, and so b  0G (mod H), which implies that the order of
[b]H divides . Thus, p | , and so ( /p)b is an element of G of order p. 2

   As a corollary, we have:
162  Abelian groups

Theorem 6.43. Let G be a finite abelian group. Then the primes dividing the
exponent of G are the same as the primes dividing its order.

Proof. Since the exponent divides the order, every prime dividing the exponent
must divide the order. Conversely, if a prime p divides the order, then since there
is an element of order p in the group, the exponent must be divisible by p. 2

EXERCISE 6.38. Find 1, 2  Z15  such that Z15  = 1, 2 .
EXERCISE 6.39. Show that Q is not finitely generated.

EXERCISE 6.40. Let G be an abelian group, a  G, and m  Z, such that m > 0
and ma = 0G. Let m = p1e1 · · · prer be the prime factorization of m. For i = 1, . . . , r,
let fi be the largest non-negative integer such that fi  ei and m/pifi · a = 0G.
Show that the order of a is equal to p1e1-f1 · · · prer-fr .

EXERCISE 6.41. Let G be an abelian group of order n, and let m be an integer.
Show that mG = G if and only if gcd(m, n) = 1.

EXERCISE 6.42. Let H be a subgroup of an abelian group G. Show that:
    (a) if H and G/H are both finitely generated, then so is G;
   (b) if G is finite, gcd(|H|, |G/H|) = 1, and H and G/H are both cyclic, then
         G is cyclic.

EXERCISE 6.43. Let G be an abelian group of exponent m1m2, where m1 and m2
are relatively prime. Show that G is the internal direct product of m1G and m2G.

EXERCISE 6.44. Show how Theorem 2.40 easily follows from Theorem 6.32.

EXERCISE 6.45. As additive groups, Z is clearly a subgroup of Q. Consider the
quotient group G := Q/Z, and show that:

    (a) all elements of G have finite order;
   (b) G has exponent 0;
    (c) for all positive integers m, we have mG = G and G{m} = Zm;
   (d) all finite subgroups of G are cyclic.

EXERCISE 6.46. Suppose that G is an abelian group that satisfies the following
properties:

    (i) for all m  Z, G{m} is either equal to G or is of finite order;
   (ii) for some m  Z, {0G} G{m} G.
Show that G{m} is finite for all non-zero m  Z.
6.6 The structure of finite abelian groups ()               163

                    6.6 The structure of finite abelian groups ()
We next state a theorem that classifies all finite abelian groups up to isomorphism.

Theorem 6.44 (Fundamental theorem of finite abelian groups). A finite abelian
group (with more than one element) is isomorphic to a direct product of cyclic
groups

                                       Zpe1 × · · · × Zperr ,

                                                                             1

where the pi's are primes (not necessarily distinct) and the ei's are positive integers.
This direct product of cyclic groups is unique up to the order of the factors.

An alternative statement of this theorem is the following:

Theorem 6.45. A finite abelian group (with more than one element) is isomorphic
to a direct product of cyclic groups

                                       Zm1 × · · · × Zmt ,

where each mi > 1, and where for i = 1, . . . , t - 1, we have mi | mi+1. Moreover,
the integers m1, . . . , mt are uniquely determined, and mt is the exponent of the
group.

   The statements of these theorems are much more important than their proofs,
which are a bit technical. Even if the reader does not study the proofs, he is urged
to understand what the theorems actually say.

   In an exercise below, you are asked to show that these two theorems are equiv-
alent. We now prove Theorem 6.45, which we break into two lemmas, the first of
which proves the existence part of the theorem, and the second of which proves the
uniqueness part.

Lemma 6.46. A finite abelian group (with more than one element) is isomorphic
to a direct product of cyclic groups

                                       Zm1 × · · · × Zmt ,

where each mi > 1, and where for i = 1, . . . , t - 1, we have mi | mi+1; moreover,
mt is the exponent of the group.

Proof. Let G be a finite abelian group with more than one element, and let m be

the exponent of G. By Theorem 6.41, there exists an element a  G of order m.
Let A = a . Then A = Zm. Now, if A = G, the lemma is proved. So assume that
A G.

   We will show that there exists a subgroup B of G such that G = A + B and
A  B = {0G}. From this, Theorem 6.25 gives us an isomorphism of G with
164  Abelian groups

A × B. Moreover, the exponent of B is clearly a divisor of m, and so the lemma

will follow by induction (on the order of the group).
   So it suffices to show the existence of a subgroup B as above. We prove this by

contradiction. Suppose that there is no such subgroup, and among all subgroups
B such that A  B = {0G}, assume that B is maximal, meaning that there is
no subgroup B of G such that B B and A  B = {0G}. By assumption
C := A + B G.

   Let d be any element of G that lies outside of C. Consider the quotient group

G/C, and let r be the order of [d]C  G/C. Note that r > 1 and r | m. We shall
define a group element d with slightly nicer properties than d, as follows. Since
rd  C, we have rd = sa + b for some s  Z and b  B. We claim that r | s. To see
this, note that 0G = md = (m/r)rd = (m/r)sa + (m/r)b, and since A  B = {0G},
we have (m/r)sa = 0G, which can only happen if r | s. That proves the claim.
This allows us to define d := d - (s/r)a. Since d  d (mod C), we see not only
that [d ]C  G/C has order r, but also that rd  B.

   We next show that A  (B + d ) = {0G}, which will yield the contradiction
we seek, and thus prove the lemma. Because A  B = {0G}, it will suffice to show
that A  (B + d )  B. Now, suppose we have a group element b + xd  A, with
b  B and x  Z. Then in particular, xd  C, and so r | x, since [d ]C  G/C has
order r. Further, since rd  B, we have xd  B, whence b + xd  B. 2

Lemma 6.47. Suppose that G := Zm1 × · · · × Zmt and H := Zn1 × · · · × Znt
are isomorphic, where the mi's and ni's are positive integers (possibly 1) such that
mi | mi+1 and ni | ni+1 for i = 1, . . . , t - 1. Then mi = ni for i = 1, . . . , t.

Proof. Clearly, i mi = |G| = |H| = i ni. We prove the lemma by induction on
the order of the group. If the group order is 1, then clearly all the mi's and ni's must
be 1, and we are done. Otherwise, let p be a prime dividing the group order. Now,

suppose that p divides mr, . . . , mt but not m1, . . . , mr-1, and that p divides ns, . . . , nt
but not n1, . . . , ns-1, where r  t and s  t. Evidently, the groups pG and pH are
isomorphic. Moreover,

                     pG = Zm1 × · · · × Zmr-1 × Zmr/p × · · · × Zmt/p,

and

                     pH = Zn1 × · · · × Zns-1 × Zns/p × · · · × Znt/p.

Thus, we see that |pG| = |G|/pt-r+1 and |pH| = |H|/pt-s+1, from which it follows
that r = s, and the lemma then follows by induction. 2

EXERCISE 6.47. Show that Theorems 6.44 and 6.45 are equivalent; that is, show
6.6 The structure of finite abelian groups ()  165

that each one implies the other. To do this, give a natural one-to-one correspond-
ence between sequences of prime powers (as in Theorem 6.44) and sequences of
integers m1, . . . , mt (as in Theorem 6.45).

EXERCISE 6.48. Using the fundamental theorem of finite abelian groups (either
form), give short and simple proofs of Theorems 6.41 and 6.42.

EXERCISE 6.49. In our proof of Euler's criterion (Theorem 2.21), we really only
used the fact that Zp has a unique element of multiplicative order 2. This exercise
develops a proof of a generalization of Euler's criterion, based on the fundamental

theorem of finite abelian groups. Suppose G is an abelian group of even order n

that contains a unique element of order 2.
    (a) Show that G = Z2e × Zm1 × · · · × Zmk , where e > 0 and the mi's are odd
         integers.

   (b) Using part (a), show that 2G = G{n/2}.

EXERCISE 6.50. Let G be a non-trivial, finite abelian group. Let s be the smallest
positive integer such that G = a1, . . . , as for some a1, . . . , as  G. Show that s
is equal to the value of t in Theorem 6.45. In particular, G is cyclic if and only if
t = 1.

EXERCISE 6.51. Suppose G = Zm1 × · · · × Zmt. Let p be a prime, and let s be the
number of mi's divisible by p. Show that G{p} = Zp×s.

EXERCISE 6.52. Suppose G = Zm1 × · · · × Zmt with mi | mi+1 for i = 1, . . . , t - 1,
and that H is a subgroup of G. Show that H = Zn1 × · · · × Znt, where ni | ni+1 for
i = 1, . . . , t - 1 and ni | mi for i = 1, . . . , t.

EXERCISE 6.53. Suppose that G is an abelian group such that for all m > 0,
we have mG = G and |G{m}| = m2 (note that G is not finite). Show that
G{m} = Zm × Zm for all m > 0. Hint: use induction on the number of prime

factors of m.
                             7

                              Rings

This chapter introduces the notion of a ring, more specifically, a commutative ring
with unity. While there is a lot of terminology associated with rings, the basic ideas
are fairly simple. Intuitively speaking, a ring is an algebraic structure with addition
and multiplication operations that behave as one would expect.

                   7.1 Definitions, basic properties, and examples
Definition 7.1. A commutative ring with unity is a set R together with addition
and multiplication operations on R, such that:

    (i) the set R under addition forms an abelian group, and we denote the additive
         identity by 0R;

   (ii) multiplication is associative; that is, for all a, b, c  R, we have a(bc) =
         (ab)c;

   (iii) multiplication distributes over addition; that is, for all a, b, c  R, we have
         a(b + c) = ab + ac and (b + c)a = ba + ca;

   (iv) there exists a multiplicative identity; that is, there exists an element 1R  R,
         such that 1R · a = a = a · 1R for all a  R;

    (v) multiplication is commutative; that is, for all a, b  R, we have ab = ba.
   There are other, more general (and less convenient) types of rings -- one can
drop properties (iv) and (v), and still have what is called a ring. We shall not,
however, be working with such general rings in this text. Therefore, to simplify
terminology, from now on, by a "ring," we shall always mean a commutative
ring with unity.
   Let R be a ring. Notice that because of the distributive law, for any fixed a  R,
the map from R to R that sends b  R to ab  R is a group homomorphism with
respect to the underlying additive group of R. We call this the a-multiplication
map.

                                               166
7.1 Definitions, basic properties, and examples          167

We first state some simple facts:

Theorem 7.2. Let R be a ring. Then:

    (i) the multiplicative identity 1R is unique;
   (ii) 0R · a = 0R for all a  R;
   (iii) (-a)b = -(ab) = a(-b) for all a, b  R;
   (iv) (-a)(-b) = ab for all a, b  R;
    (v) (ka)b = k(ab) = a(kb) for all k  Z and a, b  R.

Proof. Part (i) may be proved using the same argument as was used to prove
part (i) of Theorem 6.2. Parts (ii), (iii), and (v) follow directly from parts (i),
(ii), and (iii) of Theorem 6.19, using appropriate multiplication maps, discussed
above. Part (iv) follows from part (iii), along with part (iv) of Theorem 6.3:
(-a)(-b) = -(a(-b)) = -(-(ab)) = ab. 2

Example 7.1. The set Z under the usual rules of multiplication and addition forms
a ring. 2

Example 7.2. For n  1, the set Zn under the rules of multiplication and addition
defined in §2.5 forms a ring. 2

Example 7.3. The set Q of rational numbers under the usual rules of multiplication
and addition forms a ring. 2

Example 7.4. The set R of real numbers under the usual rules of multiplication
and addition forms a ring. 2

Example 7.5. The set C of complex numbers under the usual rules of multiplica-
tion and addition forms a ring. Every   C can be written (uniquely) as  = a+bi,
where a, b  R and i = -1. If  = a + b i is another complex number, with
a , b  R, then

          +  = (a + a ) + (b + b )i and  = (aa - bb ) + (ab + a b)i.

The fact that C is a ring can be verified by direct calculation; however, we shall see
later that this follows easily from more general considerations.

   Recall the complex conjugation operation, which sends  to  := a - bi. One

can verify by direct calculation that complex conjugation is both additive and mul-
tiplicative; that is,  +  =  +  and  ·  =  ·  .

   The norm of  is N () :=  = a2 + b2. So we see that N () is a non-negative
real number, and is zero if and only if  = 0. Moreover, from the multiplicativity

of complex conjugation, it is easy to see that the norm is multiplicative as well:
N( ) =   =   =   = N()N( ). 2
168                                    Rings

Example 7.6. Consider the set F of all arithmetic functions, that is, functions
mapping positive integers to reals. Let us define addition of arithmetic functions
point-wise (i.e., (f + g)(n) = f (n) + g(n) for all positive integers n) and multi-
plication using the Dirichlet product, introduced in §2.9. The reader should verify
that with addition and multiplication so defined, F forms a ring, where the all-zero
function is the additive identity, and the special function I defined in §2.9 is the
multiplicative identity. 2

Example 7.7. Generalizing Example 6.18, if R1, . . . , Rk are rings, then we can
form the direct product S := R1 × · · · × Rk, which consists of all k-tuples
(a1, . . . , ak) with a1  R1, . . . , ak  Rk. We can view S in a natural way as
a ring, with addition and multiplication defined component-wise. The additive

identity is (0R1, . . . , 0Rk ) and the multiplicative identity is (1R1, . . . , 1Rk ). When
R = R1 = · · · = Rk, the k-wise direct product of R is denoted R×k. 2

Example 7.8. Generalizing Example 6.19, if I is an arbitrary set and R is a ring,
then Map(I, R), which is the set of all functions f : I  R, may be natu-
rally viewed as a ring, with addition and multiplication defined point-wise: for
f , g  Map(I, R), we define

        (f + g)(i) := f (i) + g(i) and (f · g)(i) := f (i) · g(i) for all i  I.

We leave it to the reader to verify that Map(I, R) is indeed a ring, where the addi-
tive identity is the all-zero function, and the multiplicative identity is the all-one
function. 2

   A ring R may be trivial, meaning that it consists of the single element 0R, with
0R +0R = 0R and 0R ·0R = 0R. Certainly, if R is trivial, then 1R = 0R. Conversely,
if 1R = 0R, then for all a  R, we have a = 1R · a = 0R · a = 0R, and hence R

is trivial. Trivial rings are not very interesting, but they naturally arise in certain

constructions.

For a1, . . . , ak  R, the product a1 · · · ak needs no parentheses, because mul-

tiplication is associative; moreover, we can reorder the ai's without changing the

value of the product, since multiplication is commutative. We can also write this

product as  k    ai.  By  convention,  such  a  product  is  defined  to  be  1R  when  k  =  0.
            i=1
When a = a1 = · · · = ak, we can write this product as ak. The reader may verify

the usual power laws: for all a, b  R, and all non-negative integers k and , we

have

                 (a )k = ak = (ak) , ak+ = aka , (ab)k = akbk.                             (7.1)
7.1 Definitions, basic properties, and examples                         169

For all a1, . . . , ak, b1, . . . , b  R, the distributive law implies

(a1 + · · · + ak)(b1 + · · · + b ) =       ai bj .

                                      1ik
                                      1j

   A ring R is in particular an abelian group with respect to addition. We shall call
a subgroup of the additive group of R an additive subgroup of R. The charac-
teristic of R is defined as the exponent of this group (see §6.5). Note that for all
m  Z and a  R, we have

                                ma = m(1R · a) = (m · 1R)a,

so that if m · 1R = 0R, then ma = 0R for all a  R. Thus, if the additive order of
1R is infinite, the characteristic of R is zero, and otherwise, the characteristic of R
is equal to the additive order of 1R.

Example 7.9. The ring Z has characteristic zero, Zn has characteristic n, and
Zn1 × Zn2 has characteristic lcm(n1, n2). 2

   When there is no possibility for confusion, one may write "0" instead of "0R"
and "1" instead of "1R." Also, one may also write, for example, 2R to denote 2·1R,
3R to denote 3 · 1R, and so on; moreover, where the context is clear, one may use
an implicit "type cast," so that m  Z really means m · 1R.

EXERCISE 7.1. Show that the familiar binomial theorem (see §A2) holds in an
arbitrary ring R; that is, for all a, b  R and every positive integer n, we have

(a + b)n =  n

                 n n-k k
                   a b.
            k=0 k

EXERCISE 7.2. Let R be a ring. For additive subgroups A and B of R, we
define their ring-theoretic product AB as the set of all elements of R that can
be expressed as

                                      a1b1 + · · · + akbk

for some a1, . . . , ak  A and b1, . . . , bk  B; by definition, this set includes the
"empty sum" 0R. Show that for all additive subgroups A, B, and C of R:

    (a) AB is also an additive subgroup of R;
   (b) AB = BA;
    (c) A(BC) = (AB)C;
   (d) A(B + C) = AB + AC.
170  Rings

                            7.1.1 Divisibility, units, and fields

For elements a, b in a ring R, we say that a divides b if ar = b for some r  R. If

a divides b, we write a | b, and we may say that a is a divisor of b, or that b is a

multiple of a, or that b is divisible by a. If a does not divide b, then we write a b.

Note that Theorem 1.1 holds for an arbitrary ring.
   We call a  R a unit if a | 1R, that is, if ar = 1R for some r  R. Using the

same argument as was used to prove part (ii) of Theorem 6.2, it is easy to see that r

is uniquely determined; it is called the multiplicative inverse of a, and we denote
it by a-1. Also, for b  R, we may write b/a to denote ba-1. Evidently, if a is a

unit, then a | b for every b  R.
   We denote the set of units by R. It is easy to see that 1R  R. Moreover,

R is closed under multiplication; indeed, if a and b are elements of R, then
(ab)-1 = a-1b-1. It follows that with respect to the multiplication operation of
the ring, R is an abelian group, called the multiplicative group of units of R.
If a  R and k is a positive integer, then ak  R; indeed, the multiplicative
inverse of ak is (a-1)k, which we may also write as a-k (which is consistent with
our notation for abelian groups). For all a, b  R, the identities (7.1) hold for all

integers k and .

   If R is non-trivial and every non-zero element of R has a multiplicative inverse,

then R is called a field.

Example 7.10. The only units in the ring Z are ±1. Hence, Z is not a field. 2

Example 7.11. Let n be a positive integer. The units in Zn are the residue classes
[a]n with gcd(a, n) = 1. In particular, if n is prime, all non-zero residue classes are
units, and if n is composite, some non-zero residue classes are not units. Hence, Zn
is a field if and only if n is prime. The notation Zn introduced in this section for the
group of units of the ring Zn is consistent with the notation introduced in §2.5. 2

Example 7.12. Every non-zero element of Q is a unit. Hence, Q is a field. 2

Example 7.13. Every non-zero element of R is a unit. Hence, R is a field. 2

Example 7.14. For non-zero  = a + bi  C, with a, b  R, we have c := N () =
a2 + b2 > 0. It follows that the complex number c-1 = (ac-1) + (-bc-1)i is the
multiplicative inverse of , since  · c-1 = ()c-1 = 1. Hence, every non-zero
element of C is a unit, and so C is a field. 2

Example 7.15. For rings R1, . . . , Rk, it is easy to see that the multiplicative group
of units of the direct product R1 × · · · × Rk is equal to R1 × · · · × Rk. Indeed, by
definition, (a1, . . . , ak) has a multiplicative inverse if and only if each individual ai

does. 2
7.1 Definitions, basic properties, and examples  171

Example 7.16. If I is a set and R is a ring, then the units in Map(I, R) are those
functions f : I  R such that f (i)  R for all i  I. 2

Example 7.17. Consider the ring F of arithmetic functions defined in Example 7.6.
By the result of Exercise 2.54, F = {f  F : f (1) = 0}. 2

                        7.1.2 Zero divisors and integral domains
Let R be a ring. If a and b are non-zero elements of R such that ab = 0, then a
and b are both called zero divisors. If R is non-trivial and has no zero divisors,
then it is called an integral domain. Note that if a is a unit in R, it cannot be a
zero divisor (if ab = 0, then multiplying both sides of this equation by a-1 yields
b = 0). In particular, it follows that every field is an integral domain.

Example 7.18. Z is an integral domain. 2

Example 7.19. For n > 1, Zn is an integral domain if and only if n is prime. In
particular, if n is composite, so n = ab with 1 < a < n and 1 < b < n, then [a]n
and [b]n are zero divisors: [a]n[b]n = [0]n, but [a]n = [0]n and [b]n = [0]n. 2

Example 7.20. Q, R, and C are fields, and hence are also integral domains. 2

Example 7.21. For two non-trivial rings R1, R2, an element (a1, a2)  R1 × R2 is
a zero divisor if and only if a1 is a zero divisor, a2 is a zero divisor, or exactly one
of a1 or a2 is zero. In particular, R1 × R2 is not an integral domain. 2

   The next two theorems establish certain results that are analogous to familiar
facts about integer divisibility. These results hold in a general ring, provided one
avoids zero divisors. The first is a cancellation law:

Theorem 7.3. If R is a ring, and a, b, c  R such that a = 0 and a is not a zero
divisor, then ab = ac implies b = c.

Proof. ab = bc implies a(b - c) = 0. The fact that a = 0 and a is not a zero divisor
implies that we must have b - c = 0, and so b = c. 2

Theorem 7.4. Let R be a ring.

    (i) Suppose a, b  R, and that either a or b is not a zero divisor. Then a | b
         and b | a if and only if ar = b for some r  R.

   (ii) Suppose a, b  R, a | b, a = 0, and a is not a zero divisor. Then there
         exists a unique r  R such that ar = b, which we denote by b/a.

Proof. For the first statement, if ar = b for some r  R, then we also have
br-1 = a; thus, a | b and b | a. For the converse, suppose that a | b and b | a. We
172                 Rings

may assume that b is not a zero divisor (otherwise, exchange the roles of a and b).
We may also assume that b is non-zero (otherwise, b | a implies a = 0, and so the
conclusion holds with any r). Now, a | b implies ar = b for some r  R, and b | a
implies br = a for some r  R, and hence b = ar = br r. Canceling b from both
sides of the equation b = br r, we obtain 1 = r r, and so r is a unit.

   For the second statement, a | b means ar = b for some r  R. Moreover, this
value of r is unique: if ar = b = ar , then we may cancel a, obtaining r = r . 2

   Of course, in the previous two theorems, if the ring is an integral domain, then

there are no zero divisors, and so the hypotheses may be simplified in this case,

dropping the explicit requirement that certain elements are not zero divisors. In
particular, if a, b, and c are elements of an integral domain, such that ab = ac and
a = 0, then we can cancel a, obtaining b = c.

   The next two theorems state some facts which pertain specifically to integral

domains.

Theorem 7.5. The characteristic of an integral domain is either zero or a prime.

Proof. By way of contradiction, suppose that D is an integral domain with char-

acteristic m that is neither zero nor prime. Since, by definition, D is not a trivial
ring, we cannot have m = 1, and so m must be composite. Say m = st, where

1 < s < m and 1 < t < m. Since m is the additive order of 1D, it follows that
(s · 1D) = 0D and (t · 1D) = 0D; moreover, since D is an integral domain, it follows
that (s · 1D)(t · 1D) = 0D. So we have

                    0D = m · 1D = (st) · 1D = (s · 1D)(t · 1D) = 0D,

a contradiction. 2

Theorem 7.6. Every finite integral domain is a field.

Proof. Let D be a finite integral domain, and let a be any non-zero element of
D. Consider the a-multiplication map that sends b  D to ab, which is a group
homomorphism on the additive group of D. Since a is not a zero-divisor, it follows
that the kernel of the a-multiplication map is {0D}, hence the map is injective, and
by finiteness, it must be surjective as well. In particular, there must be an element
b  D such that ab = 1D. 2

Theorem 7.7. Every finite field F must be of cardinality pw, where p is prime, w
is a positive integer, and p is the characteristic of F .

Proof. By Theorem 7.5, the characteristic of F is either zero or a prime, and since
F is finite, it must be prime. Let p denote the characteristic. By definition, p is
the exponent of the additive group of F , and by Theorem 6.43, the primes dividing
7.1 Definitions, basic properties, and examples                173

the exponent are the same as the primes dividing the order, and hence F must have
cardinality pw for some positive integer w. 2

   Of course, for every prime p, Zp is a finite field of cardinality p. As we shall
see later (in Chapter 19), for every prime p and positive integer w, there exists a
field of cardinality pw. Later in this chapter, we shall see some specific examples
of finite fields of cardinality p2 (Examples 7.40, 7.59, and 7.60).

EXERCISE 7.3. Let R be a ring, and let a, b  R such that ab = 0. Show that ab is
a zero divisor if and only if a is a zero divisor or b is a zero divisor.

EXERCISE 7.4. Suppose that R is a non-trivial ring in which the cancellation law
holds in general: for all a, b, c  R, if a = 0 and ab = ac, then b = c. Show that R
is an integral domain.

EXERCISE 7.5. Let R be a ring of characteristic m > 0, and let n be an integer.
Show that:

    (a) if gcd(n, m) = 1, then n · 1R is a unit;
   (b) if 1 < gcd(n, m) < m, then n · 1R is a zero divisor;
    (c) otherwise, n · 1R = 0.

EXERCISE 7.6. Let D be an integral domain, m  Z, and a  D. Show that
ma = 0 if and only if m is a multiple of the characteristic of D or a = 0.

EXERCISE 7.7. Show that for all n  1, and for all a, b  Zn, if a | b and b | a,
then ar = b for some r  Zn. Hint: this result does not follow from part (i) of
Theorem 7.4, as we allow a and b to be zero divisors here; first consider the case
where n is a prime power.

EXERCISE 7.8. Show that the ring F of arithmetic functions defined in Exam-
ple 7.6 is an integral domain.

EXERCISE 7.9. This exercise depends on results in §6.6. Using the fundamental
theorem of finite abelian groups, show that the additive group of a finite field of
characteristic p and cardinality pw is isomorphic to Zp×w.

                                        7.1.3 Subrings
Definition 7.8. A subset S of a ring R is called a subring if

    (i) S is an additive subgroup of R,
   (ii) S is closed under multiplication, and
   (iii) 1R  S.
174          Rings

   It is clear that the operations of addition and multiplication on a ring R make
a subring S of R into a ring, where 0R is the additive identity of S and 1R is the
multiplicative identity of S. One may also call R an extension ring of S.

   Some texts do not require that 1R belongs to a subring S, and instead require
only that S contains a multiplicative identity, which may be different than that of
R. This is perfectly reasonable, but for simplicity, we restrict ourselves to the case
where 1R  S.

   Expanding the above definition, we see that a subset S of R is a subring if and
only if 1R  S and for all a, b  S, we have

                            a + b  S, -a  S, and ab  S.

In fact, to verify that S is a subring, it suffices to show that -1R  S and that S is
closed under addition and multiplication; indeed, if -1R  S and S is closed under
multiplication, then S is closed under negation, and further, 1R = -(-1R)  S.

Example 7.22. Z is a subring of Q. 2

Example 7.23. Q is a subring of R. 2

Example 7.24. R is a subring of C. Note that for all  := a + bi  C, with a, b  R,
we have  =   a + bi = a - bi  b = 0. That is,  =     R. 2

Example 7.25. The set Z[i] of complex numbers of the form a + bi, with a, b  Z,
is a subring of C. It is called the ring of Gaussian integers. Since C is a field, it
contains no zero divisors, and hence Z[i] contains no zero divisors either. Hence,
Z[i] is an integral domain.

   Let us determine the units of Z[i]. Suppose   Z[i] is a unit, so that there exists
  Z[i] such that  = 1. Taking norms, we obtain

                            1 = N(1) = N( ) = N()N( ).

Since the norm of any Gaussian integer is itself a non-negative integer, and since
N ()N ( ) = 1, we must have N () = 1. Now, if  = a + bi, with a, b  Z, then
1 = N () = a2 + b2, which implies that  = ±1 or  = ±i. Conversely, it is easy

to see that ±1 and ±i are indeed units, and so these are the only units in Z[i]. 2

Example 7.26. Let m be a positive integer, and let Q(m) be the set of rational
numbers which can be written as a/b, where a and b are integers, and b is rela-
tively prime to m. Then Q(m) is a subring of Q, since for all a, b, c, d  Z with
gcd(b, m) = 1 and gcd(d, m) = 1, we have

     a + c = ad + bc and a · c = ac ,
     b d bd         b d bd

and since gcd(bd, m) = 1, it follows that the sum and product of any two elements
7.1 Definitions, basic properties, and examples  175

of Q(m) are again in Q(m). Clearly, Q(m) contains -1, and so it follows that Q(m) is
a subring of Q. The units of Q(m) are precisely those rational numbers of the form
a/b, where gcd(a, m) = gcd(b, m) = 1. 2

Example 7.27. Suppose R is a non-trivial ring. Then the set {0R} is not a subring
of R: although it satisfies the first two requirements of the definition of a subring,
it does not satisfy the third. 2

   Generalizing the argument in Example 7.25, it is clear that every subring of an

integral domain is itself an integral domain. However, it is not the case that a
subring of a field is always a field: the subring Z of Q is a counter-example. If F
is a subring of a field F , and F is itself a field, then we say that F is a subfield of
F , and that F is an extension field of F . For example, Q is a subfield of R, which
in turn is a subfield of C.

EXERCISE 7.10. Show that if S is a subring of a ring R, then a set T  S is a
subring of R if and only if T is a subring of S.

EXERCISE 7.11. Show that if S and T are subrings of R, then so is S  T .

EXERCISE 7.12. Let S1 be a subring of R1, and S2 a subring of R2. Show that
S1 × S2 is a subring of R1 × R2.

EXERCISE 7.13. Suppose that S and T are subrings of a ring R. Show that their
ring-theoretic product ST (see Exercise 7.2) is a subring of R that contains S  T ,
and is the smallest such subring.

EXERCISE 7.14. Show that the set Q[i] of complex numbers of the form a + bi,
with a, b  Q, is a subfield of C.

EXERCISE 7.15. Consider the ring Map(R, R) of functions f : R  R, with
addition and multiplication defined point-wise.

    (a) Show that Map(R, R) is not an integral domain, and that Map(R, R) con-
         sists of those functions that never vanish.

   (b) Let a, b  Map(R, R). Show that if a | b and b | a, then ar = b for some
         r  Map(R, R).

    (c) Let C be the subset of Map(R, R) of continuous functions. Show that C is
         a subring of Map(R, R), and that all functions in C are either everywhere
         positive or everywhere negative.

   (d) Find elements a, b  C, such that in the ring C, we have a | b and b | a, yet
         there is no r  C such that ar = b.
176        Rings

                                    7.2 Polynomial rings

If R is a ring, then we can form the ring of polynomials R[X ], consisting of
all polynomials g = a0 + a1X + · · · + akX k in the indeterminate, or "formal"
variable, X , with coefficients ai in R, and with addition and multiplication defined
in the usual way.

Example 7.28. Let us define a few polynomials over the ring Z:
     a := 3 + X 2, b := 1 + 2X - X 3, c := 5, d := 1 + X , e := X , f := 4X 3.

We have
a + b = 4 + 2X + X 2 - X 3, a · b = 3 + 6X + X 2 - X 3 - X 5, cd + ef = 5 + 5X + 4X 4. 2

   As illustrated in the previous example, elements of R are also considered to be
polynomials. Such polynomials are called constant polynomials. The set R of
constant polynomials forms a subring of R[X ]. In particular, 0R is the additive
identity in R[X ] and 1R is the multiplicative identity in R[X ]. Note that if R is the
trivial ring, then so is R[X ]; also, if R is a subring of E, then R[X ] is a subring of
E[X ].

   So as to keep the distinction between ring elements and indeterminates clear, we
shall use the symbol "X " only to denote the latter. Also, for a polynomial g  R[X ],
we shall in general write this simply as "g," and not as "g(X )." Of course, the
choice of the symbol "X " is arbitrary; occasionally, we may use another symbol,
such as "Y ," as an alternative.

           7.2.1 Formalities

For completeness, we present a more formal definition of the ring R[X ]. The

reader should bear in mind that this formalism is rather tedious, and may be more

distracting than it is enlightening. Formally, a polynomial g  R[X ] is an infinite
sequence {ai}i=0, where each ai  R, but only finitely many of the ai's are non-
zero (intuitively, ai represents the coefficient of X i). For each non-negative integer
j, it will be convenient to define the function j : R  R[X ] that maps c  R to
the sequence {ci}i=0  R[X ], where cj := c and ci := 0R for i = j (intuitively,
j(c) represents the polynomial cX j).

   For

                    g = {ai}i=0  R[X ] and h = {bi}i=0  R[X ],

we define

           g + h := {si}i=0 and gh := {pi}i=0,
                                        7.2 Polynomial rings                                             177

where for i = 0, 1, 2, . . . ,

                                              si := ai + bi                                          (7.2)

and

                                           pi :=         aj bk ,                                     (7.3)

                                                  i=j+k

the sum being over all pairs (j, k) of non-negative integers such that i = j + k
(which is a finite sum). We leave it to the reader to verify that g + h and gh are

polynomials (i.e., only finitely many of the si's and pi's are non-zero). The reader
may also verify that all the requirements of Definition 7.1 are satisfied: the additive

identity is the all-zero sequence 0(0R), and the multiplicative identity is 0(1R).
   One can easily verify that for all c, d  R, we have

             0(c + d) = 0(c) + 0(d) and 0(cd) = 0(c)0(d).

We shall identify c  R with 0(c)  R[X ], viewing the ring element c as simply

"shorthand" for the polynomial 0(c) in contexts where a polynomial is expected.

Note that while c and 0(c) are not the same mathematical object, there will be no

confusion in treating them as such. Thus, from a narrow, legalistic point of view, R

is not a subring of R[X ], but we shall not let such annoying details prevent us from

continuing to speak of it as such. Indeed, by appropriately renaming elements, we

can make R a subring of R[X ] in the literal sense of the term.

We also define X := 1(1R). One can verify that X i = i(1R) for all i  0.

More generally, for any polynomial g = {ai}i=0, if ai = 0R for all i exceeding
some value k, then we have g =                 k
                                               i=0  0  (ai  )X  i  .   Writing ai in place of 0(ai),

we have g =  k    aiX i,        and  so    we  can  return   to       the  standard  practice  of    writing
             i=0

polynomials as we did in Example 7.28, without any loss of precision.

                  7.2.2 Basic properties of polynomial rings

Let R be a ring. For non-zero g  R[X ], if g =                    k    aiX i  with  ak  =  0,  then  we  call
                                                                  i=0
k the degree of g, denoted deg(g), we call ak the leading coefficient of g, denoted

lc(g), and we call a0 the constant term of g. If lc(g) = 1, then g is called monic.

Suppose g =  k    aiX           i  and  h  =   i=0 biX i are polynomials such that ak = 0 and
             i=0
b = 0, so that deg(g) = k and lc(g) = ak, and deg(h) = and lc(h) = b . When

we multiply these two polynomials, we get

             gh = a0b0 + (a0b1 + a1b0)X + · · · + akb X k+ .

In particular, deg(gh)  deg(g) + deg(h). If either of ak or b are not zero divisors,
then akb is not zero, and hence deg(gh) = deg(g) + deg(h). However, if both ak
178                 Rings

and b are zero divisors, then we may have akb = 0, in which case, the product gh
may be zero, or perhaps gh = 0 but deg(gh) < deg(g) + deg(h).

   For the zero polynomial, we establish the following conventions: its leading
coefficient and constant term are defined to be 0R, and its degree is defined to be

-. With these conventions, we may succinctly state that

     for all g, h  R[X ], we have deg(gh)  deg(g) + deg(h), with
     equality guaranteed to hold unless the leading coefficients of both g

     and h are zero divisors.

In particular, if the leading coefficient of a polynomial is not a zero divisor, then
the polynomial is not a zero divisor. In the case where the ring of coefficients is an

integral domain, we can be more precise:

Theorem 7.9. Let D be an integral domain. Then:
    (i) for all g, h  D[X ], we have deg(gh) = deg(g) + deg(h);

   (ii) D[X ] is an integral domain;
  (iii) (D[X ]) = D.

Proof. Exercise. 2

   An extremely important property of polynomials is a division with remainder
property, analogous to that for the integers:

Theorem 7.10 (Division with remainder property). Let R be a ring. For all
g, h  R[X ] with h = 0 and lc(h)  R, there exist unique q, r  R[X ] such that
g = hq + r and deg(r) < deg(h).

Proof. Consider the set S := {g - ht : t  R[X ]}. Let r = g - hq be an element
of S of minimum degree. We must have deg(r) < deg(h), since otherwise, we
could subtract an appropriate multiple of h from r so as to eliminate the leading
coefficient of r, obtaining

                     r := r - h · (lc(r) lc(h)-1X deg(r)-deg(h))  S,

where deg(r ) < deg(r), contradicting the minimality of deg(r).
   That proves the existence of r and q. For uniqueness, suppose that g = hq + r

and g = hq + r , where deg(r) < deg(h) and deg(r ) < deg(h). This implies
r - r = h · (q - q ). However, if q = q , then

    deg(h) > deg(r - r) = deg(h · (q - q )) = deg(h) + deg(q - q )  deg(h),

which is impossible. Therefore, we must have q = q , and hence r = r . 2

   If g = hq + r as in the above theorem, we define g mod h := r. Clearly, h | g if
                                    7.2 Polynomial rings                                       179

and only if g mod h = 0. Moreover, note that if deg(g) < deg(h), then q = 0 and
r = g; otherwise, if deg(g)  deg(h), then q = 0 and deg(g) = deg(h) + deg(q).

                            7.2.3 Polynomial evaluation

A polynomial g =    k    aiX i      R[X ]  naturally  defines   a  polynomial    function      on  R
                    i=0
                       k    aixi
that sends x  R to     i=0          R,  and   we  denote  the   value  of  this  function  as  g(x)

(note that "X " denotes an indeterminate, while "x" denotes an element of R). It is

important to regard polynomials over R as formal expressions, and not to identify

them with their corresponding functions. In particular, two polynomials are equal
if and only if their coefficients are equal, while two functions are equal if and only

if their values agree at all points in R. This distinction is important, since there are
rings R over which two different polynomials define the same function. One can

of course define the ring of polynomial functions on R, but in general, that ring has
a different structure from the ring of polynomials over R.

Example 7.29. In the ring Zp, for prime p, by Fermat's little theorem (Theo-
rem 2.14), we have xp = x for all x  Zp. However, the polynomials X p and
X are not the same polynomials (in particular, the former has degree p, while the

latter has degree 1). 2

More generally, suppose R is a subring of a ring E. Then every polynomial

g = ik=0 aiX i  R[X ] defines a polynomial function from E to E that sends
        k    aii
  E to  i=0            E,   and,  again,  the  value  of  this  function   is  denoted  g().   We

say that  is a root of g if g() = 0.

An obvious, yet important, fact is the following:

Theorem 7.11. Let R be a subring of a ring E. For all g, h  R[X ] and   E, if
s := g + h  R[X ] and p := gh  R[X ], then we have

                       s() = g() + h() and p() = g()h().

Also, if c  R is a constant polynomial, then c() = c for all   E.

Proof. The statement about evaluating a constant polynomial is clear from the

definitions. The proof of the statements about evaluating the sum or product of

polynomials is really just symbol pushing. Indeed, suppose g =                   i aiX i and
h = i biX i. Then s = i(ai + bi)X i, and so

        s() = (ai + bi)i = aii + bii = g() + h().

                    i                      i          i
180                            Rings

Also, we have

                 p=     aiX i    bjX j  = aibjX i+j,

                     i         j             i,j

and employing the result for evaluating sums of polynomials, we have

     p() = aibji+j =             aii      bjj = g()h(). 2

                  i,j          i        j

Example 7.30. Consider the polynomial g := 2X 3 - 2X 2 + X - 1  Z[X ]. We can
write g = (2X 2 + 1)(X - 1). For any element  of Z, or an extension ring of Z, we
have g() = (22 + 1)( - 1). From this, it is clear that in Z, g has a root only at
1; moreover, it has no other roots in R, but in C, it also has roots ±i/ 2. 2

Example 7.31. If E = R[X ], then evaluating a polynomial g  R[X ] at a point
  E amounts to polynomial composition. For example, if g := X 2 + X and
 := X + 1, then

     g() = g X + 1 = (X + 1)2 + (X + 1) = X 2 + 3X + 2. 2

   The reader is perhaps familiar with the fact that over the real or the complex
numbers, every polynomial of degree k has at most k distinct roots, and the fact
that every set of k points can be interpolated by a unique polynomial of degree less
than k. As we will now see, these results extend to much more general, though not
completely arbitrary, coefficient rings.

Theorem 7.12. Let R be a ring, g  R[X ], and x  R. Then there exists a unique
polynomial q  R[X ] such that g = (X - x)q + g(x). In particular, x is a root of g

if and only if (X - x) divides g.

Proof. If R is the trivial ring, there is nothing to prove, so assume that R is non-

trivial. Using the division with remainder property for polynomials, there exist
unique q, r  R[X ] such that g = (X - x)q + r, with q, r  R[X ] and deg(r) < 1,
which means that r  R. Evaluating at x, we see that g(x) = (x - x)q(x) + r = r.

That proves the first statement. The second follows immediately from the first. 2

  Note that the above theorem says that X - x divides g - g(x), and the polynomial
q in the theorem may be expressed (using the notation introduced in part (ii) of

Theorem 7.4) as

                               q = g - g(x) .
                                     X -x

Theorem 7.13. Let D be an integral domain, and let x1, . . . , xk be distinct ele-

ments of D. Then for every polynomial g  D[X ], the elements x1, . . . , xk are
roots of g if and only if the polynomial ik=1(X - xi) divides g.
                                 7.2 Polynomial rings                                    181

Proof. One direction is trivial: if ik=1(X - xi) divides g, then it is clear that each
xi is a root of g. We prove the converse by induction on k. The base case k = 1 is

just Theorem 7.12. So assume k > 1, and that the statement holds for k - 1. Let

g  D[X ] and let x1, . . . , xk be distinct roots of g. Since xk is a root of g, then by
Theorem 7.12, there exists q  D[X ] such that g = (X - xk)q. Moreover, for each
i = 1, . . . , k - 1, we have

                           0 = g(xi) = (xi - xk)q(xi),

and since xi - xk = 0 and D is an integral domain, we must have q(xi) = 0. Thus,
q has roots x1, . . . , xk-1, and by induction ik=1-1(X - xi) divides q, from which it
then follows that ik=1(X - xi) divides g. 2

   Note that in this theorem, we can slightly weaken the hypothesis: we do not need
to assume that the coefficient ring is an integral domain; rather, all we really need
is that for all i = j, the difference xi - xj is not a zero divisor.

   As an immediate consequence of this theorem, we obtain:

Theorem 7.14. Let D be an integral domain, and suppose that g  D[X ], with
deg(g) = k  0. Then g has at most k distinct roots.

Proof. If g had k + 1 distinct roots x1, . . . , xk+1, then by the previous theorem,
                k+1
the polynomial          -  xi),  which  has  degree  k  +  1,  would  divide  g,  which  has
                i=1 (X

degree k --an impossibility. 2

Theorem 7.15 (Lagrange interpolation). Let F be a field, let x1, . . . , xk be dis-

tinct elements of F , and let y1, . . . , yk be arbitrary elements of F . Then there
exists a unique polynomial g  F [X ] with deg(g) < k such that g(xi) = yi for
i = 1, . . . , k, namely

                                     k       j=i(X - xj)
                                                           .
                           g := yi
                                             j=i(xi - xj)
                                   i=1

Proof. For the existence part of the theorem, one just has to verify that g(xi) = yi

for the given g, which clearly has degree less than k. This is easy to see: for
i = 1, . . . , k, evaluating the ith term in the sum defining g at xi yields yi, while

evaluating any other term at xi yields 0. The uniqueness part of the theorem follows

almost immediately from Theorem 7.14: if g and h are polynomials of degree less
than k such that g(xi) = yi = h(xi) for i = 1, . . . , k, then g - h is a polynomial

of degree less than k with k distinct roots, which, by the previous theorem, is

impossible. 2

Again, we can slightly weaken the hypothesis of this theorem: we do not need
182  Rings

to assume that the coefficient ring is a field; rather, all we really need is that for all
i = j, the difference xi - xj is a unit.

EXERCISE 7.16. Let D be an infinite integral domain, and let g, h  D[X ]. Show
that if g(x) = h(x) for all x  D, then g = h. Thus, for an infinite integral
domain D, there is a one-to-one correspondence between polynomials over D and
polynomial functions on D.

EXERCISE 7.17. Let F be a field.
    (a) Show that for all b  F , we have b2 = 1 if and only if b = ±1.
   (b) Show that for all a, b  F , we have a2 = b2 if and only if a = ±b.

    (c) Show that the familiar quadratic formula holds for F , assuming F has
         characteristic other than 2, so that 2F = 0F . That is, for all a, b, c  F with
        a = 0, the polynomial g := aX 2 + bX + c  F [X ] has a root in F if and
         only if there exists e  F such that e2 = d, where d is the discriminant of
         g, defined as d := b2 - 4ac, and in this case the roots of g are (-b ± e)/2a.

EXERCISE 7.18. Let R be a ring, let g  R[X ], with deg(g) = k  0, and let x be
an element of R. Show that:

   (a) there exist an integer m, with 0  m  k, and a polynomial q  R[X ], such
         that
                                 g = (X - x)mq and q(x) = 0,

         and moreover, the values of m and q are uniquely determined;
   (b) if we evaluate g at X + x, we have

                                                                                k

                                     g X + x = biX i,

                                                                              i=0

         where b0 = · · · = bm-1 = 0 and bm = q(x) = 0.

   Let mx(g) denote the value m in the previous exercise; for completeness, one
can define mx(g) :=  if g is the zero polynomial. If mx(g) > 0, then x is called a
root of g of multiplicity mx(g); if mx(g) = 1, then x is called a simple root of g,
and if mx(g) > 1, then x is called a multiple root of g.

   The following exercise refines Theorem 7.14, taking into account multiplicities.

EXERCISE 7.19. Let D be an integral domain, and suppose that g  D[X ], with
deg(g) = k  0. Show that

                                             mx(g)  k.

                                                       xD
       7.2 Polynomial rings                     183

EXERCISE 7.20. Let D be an integral domain, let g, h  D[X ], and let x  D.
Show that mx(gh) = mx(g) + mx(h).

       7.2.4 Multi-variate polynomials

One can naturally generalize the notion of a polynomial in a single variable to that
of a polynomial in several variables.

  Consider the ring R[X ] of polynomials over a ring R. If Y is another indeter-
minate, we can form the ring R[X ][Y ] of polynomials in Y whose coefficients are
themselves polynomials in X over the ring R. One may write R[X , Y ] instead of
R[X ][Y ]. An element of R[X , Y ] is called a bivariate polynomial.

  Consider a typical element g  R[X , Y ], which may be written

              k

       g=           aijX i Y j.                 (7.4)

           j=0 i=0

Rearranging terms, this may also be written as

       g=          aijX iY j,                   (7.5)
                                                (7.6)
              0ik
              0j

or as

           k

       g=           aijY j X j.

           i=0 j=0

  If g is written as in (7.5), the terms X iY j are called monomials. The total degree
of such a monomial X iY j is defined to be i + j, and if g is non-zero, then the total
degree of g, denoted Deg(g), is defined to be the maximum total degree among all
monomials X iY j appearing in (7.5) with a non-zero coefficient aij. We define the
total degree of the zero polynomial to be -.

   When g is written as in (7.6), one sees that we can naturally view g as an element
of R[Y ][X ], that is, as a polynomial in X whose coefficients are polynomials in Y .

From a strict, syntactic point of view, the rings R[Y ][X ] and R[X ][Y ] are not the

same, but there is no harm done in blurring this distinction when convenient. We

denote by degX (g) the degree of g, viewed as a polynomial in X , and by degY (g)
the degree of g, viewed as a polynomial in Y .

Example 7.32. Let us illustrate, with a particular example, the three different
forms -- as in (7.4), (7.5), and (7.6) -- of expressing a bivariate polynomial. In
184  Rings

the ring Z[X , Y ] we have

                       g = (5X 2 - 3X + 4)Y + (2X 2 + 1)

                          = 5X 2Y + 2X 2 - 3XY + 4Y + 1

                          = (5Y + 2)X 2 + (-3Y )X + (4Y + 1).

We have Deg(g) = 3, degX (g) = 2, and degY (g) = 1. 2

  More generally, we can form the ring R[X1, . . . , Xn] of multi-variate polynomi-
als over R in the variables X1, . . . , Xn. Formally, we can define this ring recursively
as R[X1, . . . , Xn-1][Xn], that is, the ring of polynomials in the variable Xn, with
coefficients in R[X1, . . . , Xn-1]. A monomial is a term of the form X1e1 · · · Xnen,
and the total degree of such a monomial is e1 + · · · + en. Every non-zero multi-
variate polynomial g can be expressed uniquely (up to a re-ordering of terms) as
a1µ1 + · · · + akµk, where each ai is a non-zero element of R, and each µi is a
monomial; we define the total degree of g, denoted Deg(g), to be the maximum of
the total degrees of the µi's. As usual, the zero polynomial is defined to have total
degree -.

   Just as for bivariate polynomials, the order of the indeterminates is not important,
and for every i = 1, . . . , n, one can naturally view any g  R[X1, . . . , Xn] as a
polynomial in Xi over the ring R[X1, . . . , Xi-1, Xi+1, . . . , Xn], and define degXi (g)
to be the degree of g when viewed in this way.

   Just as polynomials in a single variable define polynomial functions, so do
polynomials in several variables. If R is a subring of E, g  R[X1, . . . , Xn],
and 1, . . . , n  E, we define g(1, . . . , n) to be the element of E obtained by
evaluating the expression obtained by substituting i for Xi in g. Theorem 7.11
carries over directly to the multi-variate case.

EXERCISE 7.21. Let R be a ring, and consider the ring of multi-variate polyno-

mials R[X1, . . . , Xn]. For m  0, define Hm to be the subset of polynomials that
can be expressed as a1µ1 + · · · + akµk, where each ai belongs to R and each µi is a

monomial of total degree m (by definition, Hm includes the zero polynomial, and
H0 = R). Polynomials that belong to Hm for some m are called homogeneous
polynomials. Show that:

    (a) if g, h  Hm, then g + h  Hm;

   (b) if g  H and h  Hm, then gh  H +m;
    (c) every non-zero polynomial g can be expressed uniquely as g0 + · · · + gd,

         where gi  Hi for i = 0, . . . , d, gd = 0, and d = Deg(g);
   (d) for all polynomials g, h, we have Deg(gh)  Deg(g) + Deg(h), and if R is

         an integral domain, then Deg(gh) = Deg(g) + Deg(h).
7.3 Ideals and quotient rings             185

EXERCISE 7.22. Suppose that D is an integral domain, and g, h are non-zero,
multi-variate polynomials over D such that gh is homogeneous. Show that g and h
are also homogeneous.

EXERCISE 7.23. Let R be a ring, and let x1, . . . , xn be elements of R. Show that
every polynomial g  R[X1, . . . , Xn] can be expressed as

                g = (X1 - x1)q1 + · · · + (Xn - xn)qn + g(x1, . . . , xn),

where q1, . . . , qn  R[X1, . . . , Xn].

EXERCISE 7.24. This exercise generalizes Theorem 7.14. Let D be an integral

domain, and let g  D[X1, . . . , Xn], with Deg(g) = k  0. Let S be a finite, non-
empty subset of D. Show that the number of elements (x1, . . . , xn)  S×n such that
g(x1, . . . , xn) = 0 is at most k|S|n-1.

                               7.3 Ideals and quotient rings

Definition 7.16. Let R be a ring. An ideal of R is an additive subgroup I of R
such that ar  I for all a  I and r  R (i.e., I is closed under multiplication by
elements of R).

   Expanding the above definition, we see that a non-empty subset I of R is an
ideal of R if and only if for all a, b  I and r  R, we have

                             a + b  I, -a  I, and ar  I.

Since R is commutative, the condition ar  I is equivalent to ra  I. The condi-
tion -a  I is redundant, as it is implied by the condition ar  I with r := -1R.
In the case when R is the ring Z, this definition of an ideal is consistent with that
given in §1.2.

   Clearly, {0R} and R are ideals of R. From the fact that an ideal I is closed under
multiplication by elements of R, it is easy to see that I = R if and only if 1R  I.

Example 7.33. For each m  Z, the set mZ is not only an additive subgroup of the
ring Z, it is also an ideal of this ring. 2

Example 7.34. For each m  Z, the set mZn is not only an additive subgroup of
the ring Zn, it is also an ideal of this ring. 2

Example 7.35. In the previous two examples, we saw that for some rings, the
notion of an additive subgroup coincides with that of an ideal. Of course, that is
the exception, not the rule. Consider the ring of polynomials R[X ]. Suppose g is a
non-zero polynomial in R[X ]. The additive subgroup generated by g contains only
polynomials whose degrees are at most that of g. However, this subgroup is not an
186  Rings

ideal, since every ideal containing g must also contain g · X i for all i  0, and must
therefore contain polynomials of arbitrarily high degree. 2

Example 7.36. Let R be a ring and x  R. Consider the set
                             I := {g  R[X ] : g(x) = 0}.

It is not hard to see that I is an ideal of R[X ]. Indeed, for all g, h  I and q  R[X ],
we have
 (g + h)(x) = g(x) + h(x) = 0 + 0 = 0 and (gq)(x) = g(x)q(x) = 0 · q(x) = 0.
Moreover, by Theorem 7.12, we have I = {(X - x)q : q  R[X ]}. 2

   We next develop some general constructions of ideals.

Theorem 7.17. Let R be a ring and let a  R. Then aR := {ar : r  R} is an
ideal of R.

Proof. This is an easy calculation. For all ar, ar  aR and r  R, we have
ar + ar = a(r + r )  aR and (ar)r = a(rr )  aR. 2

   The ideal aR in the previous theorem is called the ideal of R generated by a.
An ideal of this form is called a principal ideal. Since R is commutative, one
could also write this ideal as Ra := {ra : r  R}. This ideal is the smallest ideal
of R containing a; that is, aR contains a, and every ideal of R that contains a must
contain everything in aR.

   Corresponding to Theorems 6.11 and 6.12, we have:

Theorem 7.18. If I1 and I2 are ideals of a ring R, then so are I1 + I2 and I1  I2.

Proof. We already know that I1 + I2 and I1  I2 are additive subgroups of R, so
it suffices to show that they are closed under multiplication by elements of R. The
reader may easily verify that this is the case. 2

   Let a1, . . . , ak be elements of a ring R. The ideal a1R + · · · + akR is called the
ideal of R generated by a1, . . . , ak. When the ring R is clear from context, one
often writes (a1, . . . , ak) to denote this ideal. This ideal is that smallest ideal of R
containing a1, . . . , ak.

Example 7.37. Let n be a positive integer, and let x be any integer. Define
I := {g  Z[X ] : g(x)  0 (mod n)}. We claim that I is the ideal (X - x, n)
of Z[X ]. To see this, consider any fixed g  Z[X ]. Using Theorem 7.12, we have
g = (X - x)q + g(x) for some q  Z[X ]. Using the division with remainder
property for integers, we have g(x) = nq + r for some r  {0, . . . , n - 1} and

q  Z. Thus, g(x)  r (mod n), and if g(x)  0 (mod n), then we must have
7.3 Ideals and quotient rings  187

r = 0, and hence g = (X - x)q + nq  (X - x, n). Conversely, if g  (X - x, n),
we can write g = (X - x)q + nq for some q, q  Z[X ], and from this, it is clear
that g(x) = nq (x)  0 (mod n). 2

   Let I be an ideal of a ring R. Since I is an additive subgroup of R, we may adopt
the congruence notation in §6.3, writing a  b (mod I) to mean a - b  I, and we
can form the additive quotient group R/I of cosets. Recall that for a  R, the coset
of I containing a is denoted [a]I , and that [a]I = a + I = {a + x : x  I}. Also
recall that addition in R/I was defined in terms of addition of coset representatives;
that is, for a, b  I, we defined

                                   [a]I + [b]I := [a + b]I .

Theorem 6.16 ensured that this definition was unambiguous.
   Our goal now is to make R/I into a ring by similarly defining multiplication in

R/I in terms of multiplication of coset representatives. To do this, we need the
following multiplicative analog of Theorem 6.16, which exploits in an essential
way the fact that an ideal is closed under multiplication by elements of R; in fact,
this is one of the main motivations for defining the notion of an ideal as we did.

Theorem 7.19. Suppose I is an ideal of a ring R. For all a, a , b, b  R, if
a  a (mod I) and b  b (mod I), then ab  a b (mod I).

Proof. If a = a + x for some x  I and b = b + y for some y  I, then
ab = a b + a y + b x + xy. Since I is closed under multiplication by elements of R,
we see that a y, b x, xy  I, and since I is closed under addition, a y+b x+xy  I.
Hence, ab - a b  I. 2

   Using this theorem we can now unambiguously define multiplication on R/I as
follows: for a, b  R,

                                     [a]I · [b]I := [ab]I .

Once that is done, it is straightforward to verify that all the properties that make

R a ring are inherited by R/I -- we leave the details of this to the reader. The

multiplicative identity of R/I is the coset [1R]I .
   The ring R/I is called the quotient ring or residue class ring of R modulo I.

Elements of R/I may be called residue classes.
   Note that if I = dR, then a  b (mod I) if and only if d | (a - b), and as a matter

of notation, one may simply write this congruence as a  b (mod d). We may also

write [a]d instead of [a]I .
   Finally, note that if I = R, then R/I is the trivial ring.

Example 7.38. For each n  1, the ring Zn is precisely the quotient ring Z/nZ. 2
188  Rings

Example 7.39. Let f be a polynomial over a ring R with deg(f ) =  0 and
lc(f )  R, and consider the quotient ring E := R[X ]/f R[X ]. By the division
with remainder property for polynomials (Theorem 7.10), for every g  R[X ],
there exists a unique polynomial h  R[X ] such that g  h (mod f ) and deg(h) < .
From this, it follows that every element of E can be written uniquely as [h]f , where
h  R[X ] is a polynomial of degree less than . Note that in this situation, we will
generally prefer the more compact notation R[X ]/(f ), instead of R[X ]/fR[X ]. 2

Example 7.40. Consider the polynomial f := X 2 + X + 1  Z2[X ] and the quotient
ring E := Z2[X ]/(f ). Let us name the elements of E as follows:

                00 := [0]f , 01 := [1]f , 10 := [X ]f , 11 := [X + 1]f .

With this naming convention, addition of two elements in E corresponds to just
computing the bit-wise exclusive-or of their names. More precisely, the addition
table for E is the following:

                                   + 00 01 10 11
                                   00 00 01 10 11
                                   01 01 00 11 10
                                   10 10 11 00 01
                                   11 11 10 01 00

Note that 00 acts as the additive identity for E, and that as an additive group, E is
isomorphic to the additive group Z2 × Z2.

   As for multiplication in E, one has to compute the product of two polynomials,
and then reduce modulo f . For example, to compute 10 · 11, using the identity
X 2  X + 1 (mod f ), one sees that

                 X · (X + 1)  X 2 + X  (X + 1) + X  1 (mod f );

thus, 10 · 11 = 01. The reader may verify the following multiplication table for E:

      · 00 01 10 11
     00 00 00 00 00
     01 00 01 10 11
     10 00 10 11 01
     11 00 11 01 10

Observe that 01 acts as the multiplicative identity for E. Notice that every non-zero
element of E has a multiplicative inverse, and so E is in fact a field. Observe that
E is cyclic: the reader may verify that both 10 and 11 have multiplicative order 3.

   This is the first example we have seen of a finite field whose cardinality is not
prime. 2
7.3 Ideals and quotient rings                               189

EXERCISE 7.25. Show that if F is a field, then the only ideals of F are {0F } and
F.

EXERCISE 7.26. Let a, b be elements of a ring R. Show that

a | b  b  aR  bR  aR.

EXERCISE 7.27. Let R be a ring. Show that if I is a non-empty subset of R[X ]
that is closed under addition, multiplication by elements of R, and multiplication
by X , then I is an ideal of R[X ].

EXERCISE 7.28. Let I be an ideal of R, and S a subring of R. Show that I  S is
an ideal of S.

EXERCISE 7.29. Let I be an ideal of R, and S a subring of R. Show that I + S is
a subring of R, and that I is an ideal of I + S.

EXERCISE 7.30. Let I1 be an ideal of R1, and I2 an ideal of R2. Show that I1 × I2
is an ideal of R1 × R2.

EXERCISE 7.31. Write down the multiplication table for Z2[X ]/(X 2 + X ). Is this
a field?

EXERCISE 7.32. Let I be an ideal of a ring R, and let x and y be elements of R
with x  y (mod I). Let g  R[X ]. Show that g(x)  g(y) (mod I).

EXERCISE 7.33. Let R be a ring, and fix x1, . . . , xn  R. Let

                    I := {g  R[X1, . . . , Xn] : g(x1, . . . , xn) = 0}.

Show that I is an ideal of R[X1, . . . , Xn], and that I = (X1 - x1, . . . , Xn - xn).

EXERCISE 7.34. Let p be a prime, and consider the ring Q(p) (see Example 7.26).
Show that every non-zero ideal of Q(p) is of the form (pi), for some uniquely deter-
mined integer i  0.

EXERCISE 7.35. Let p be a prime. Show that in the ring Z[X ], the ideal (X , p) is
not a principal ideal.

EXERCISE 7.36. Let F be a field. Show that in the ring F [X , Y ], the ideal (X , Y )
is not a principal ideal.

EXERCISE 7.37. Let R be a ring, and let {Ii}i=0 be a sequence of ideals of R such
that Ii  Ii+1 for all i = 0, 1, 2, . . . . Show that the union i=0 Ii is also an ideal of
R.

EXERCISE 7.38. Let R be a ring. An ideal I of R is called prime if I R and if
190               Rings

for all a, b  R, ab  I implies a  I or b  I. An ideal I of R is called maximal
if I R and there are no ideals J of R such that I J R. Show that:

    (a) an ideal I of R is prime if and only if R/I is an integral domain;
   (b) an ideal I of R is maximal if and only if R/I is a field;
    (c) all maximal ideals of R are also prime ideals.

EXERCISE 7.39. This exercise explores some examples of prime and maximal
ideals. Show that:

    (a) in the ring Z, the ideal {0} is prime but not maximal, and that the maximal
         ideals are precisely those of the form pZ, where p is prime;

   (b) in an integral domain D, the ideal {0} is prime, and this ideal is maximal if
         and only if D is a field;

   (c) if p is a prime, then in the ring Z[X ], the ideal (X , p) is maximal, while the
        ideals (X ) and (p) are prime, but not maximal;

   (d) if F is a field, then in the ring F [X , Y ], the ideal (X , Y ) is maximal, while
        the ideals (X ) and (Y ) are prime, but not maximal.

EXERCISE 7.40. It is a fact that every non-trivial ring R contain at least one max-
imal ideal. Showing this in general requires some fancy set-theoretic notions. This
exercise develops a simple proof in the case where R is countable (see §A3).

(a) Show that if R is non-trivial but finite, then it contains a maximal ideal.

(b) Assume that R is countably infinite, and let a1, a2, a3, . . . be an enumeration

     of the elements of R. Define a sequence of ideals I0, I1, I2, . . . , as follows.
     Set I0 := {0R}, and for each i  0, define

     Ii+1 :=  Ii + aiR if Ii + aiR R;

              Ii         otherwise.

     Finally, set I := i=0 Ii, which by Exercise 7.37 is an ideal of R. Show
     that I is a maximal ideal of R. Hint: first, show that I R by assuming

     that 1R  I and deriving a contradiction; then, show that I is maximal
     by assuming that for some i = 1, 2, . . . , we have I I + aiR R, and

     deriving a contradiction.

EXERCISE 7.41. Let R be a ring, and let I and J be ideals of R. With the ring-
theoretic product as defined in Exercise 7.2, show that:

    (a) IJ is an ideal;
   (b) if I and J are principal ideals, with I = aR and J = bR, then IJ = abR,

         and so is also a principal ideal;

    (c) IJ  I  J;
7.3 Ideals and quotient rings       191

(d) if I + J = R, then IJ = I  J .

EXERCISE 7.42. Let R be a subring of E, and I an ideal of R. Show that the
ring-theoretic product IE is an ideal of E that contains I, and is the smallest such
ideal.

EXERCISE 7.43. Let M be a maximal ideal of a ring R, and let a, b  R. Show
that if ab  M2 and b / M, then a  M2. Here, M2 := MM, the ring-theoretic

product.

EXERCISE 7.44. Let F be a field, let f  F [X , Y ], and let E := F [X , Y ]/(f ).
Define V (f ) := {(x, y)  F × F : f (x, y) = 0}.

    (a) Every element  of E naturally defines a function from V (f ) to F , as fol-
        lows: if  = [g]f , with g  F [X , Y ], then for P = (x, y)  V (f ), we
         define (P ) := g(x, y). Show that this definition is unambiguous, that is,
         g  h (mod f ) implies g(x, y) = h(x, y).

   (b) For P = (x, y)  V (f ), define MP := {  E : (P ) = 0}. Show that MP
        is a maximal ideal of E, and that MP = µE + E, where µ := [X - x]f
        and  := [Y - y]f .

EXERCISE 7.45. Continuing with the previous exercise, now assume that the char-
acteristic of F is not 2, and that f = Y 2 - , where   F [X ] is a non-zero

polynomial with no multiple roots in F (see definitions after Exercise 7.18).

    (a) Show that if P = (x, y)  V (f ), then so is P := (x, -y), and that
         P = P  y = 0  (x) = 0.

   (b) Let P = (x, y)  V (f ) and µ := [X - x]f  E. Show that µE = MP MP
         (the ring-theoretic product). Hint: use Exercise 7.43, and treat the cases
         P = P and P = P separately.

EXERCISE 7.46. Let R be a ring, and I an ideal of R. Define Rad(I) to be the set
of all a  R such that an  I for some positive integer n.

    (a) Show that Rad(I) is an ideal of R containing I. Hint: show that if an  I
         and bm  I, then (a + b)n+m  I.

   (b) Show that if R = Z and I = (d), where d = p1e1 · · · prer is the prime factor-
         ization of d, then Rad(I) = (p1 · · · pr).
192  Rings

                    7.4 Ring homomorphisms and isomorphisms

Definition 7.20. A function  from a ring R to a ring R is called a ring homo-
morphism if

    (i)  is a group homomorphism with respect to the underlying additive groups
         of R and R ,

   (ii) (ab) = (a)(b) for all a, b  R, and
   (iii) (1R) = 1R .

   Expanding the definition, the requirements that  must satisfy in order to be a
ring homomorphism are that for all a, b  R, we have (a + b) = (a) + (b) and
(ab) = (a)(b), and that (1R) = 1R .

   Note that some texts do not require that a ring homomorphism satisfies part (iii)

of our definition (which is not redundant -- see Examples 7.49 and 7.50 below).

Since a ring homomorphism is also an additive group homomorphism, we use the

same notation and terminology for image and kernel.

Example 7.41. If S is a subring of a ring R, then the inclusion map i : S  R is
obviously a ring homomorphism. 2

Example 7.42. Suppose I is an ideal of a ring R. Analogous to Example 6.36, we
may define the natural map from the ring R to the quotient ring R/I as follows:

                                         : R  R/I
                                               a  [a]I .

Not only is this a surjective homomorphism of additive groups, with kernel I, it is
a ring homomorphism. Indeed, we have

                        (ab) = [ab]I = [a]I · [b]I = (a) · (b),
and (1R) = [1R]I , which is the multiplicative identity in R/I. 2

Example 7.43. For a given positive integer n, the natural map from Z to Zn sends
a  Z to the residue class [a]n. This is a surjective ring homomorphism, whose
kernel is nZ. 2

Example 7.44. Let R be a subring of a ring E, and fix   E. The polynomial
evaluation map

                                    : R[X ]  E

                                                 g  g()

is a ring homomorphism (see Theorem 7.11). The image of  consists of all poly-
nomial expressions in  with coefficients in R, and is denoted R[]. As the reader
              7.4 Ring homomorphisms and isomorphisms                    193

may verify, R[] is a subring of E containing  and all of R, and is the smallest
such subring of E. 2

Example 7.45. We can generalize the previous example to multi-variate polyno-
mials. If R is a subring of a ring E and 1, . . . , n  E, then the map

                           : R[X1, . . . , Xn]  E

                                                 g  g(1, . . . , n)

is a ring homomorphism. Its image consists of all polynomial expressions in
1, . . . , n with coefficients in R, and is denoted R[1, . . . , n]. Moreover, this
image is a subring of E containing 1, . . . , n and all of R, and is the smallest such
subring of E. Note that R[1, . . . , n] = R[1, . . . , n-1][n]. 2

Example 7.46. Let  : R  R be a ring homomorphism. We can extend the
domain of definition of  from R to R[X ] by defining ( i aiX i) := i (ai)X i.
This yields a ring homomorphism from R[X ] into R [X ]. To verify this, suppose
g = i aiX i and h = i biX i are polynomials in R[X ]. Let s := g + h  R[X ] and
p := gh  R[X ], and write s = i siX i and p = i piX i, so that

              si = ai + bi and pi =                aj bk .

                                     i=j+k

Then we have

              (si) = (ai + bi) = (ai) + (bi),

which is the coefficient of X i in (g) + (h), and

(pi) =               ajbk =         (ajbk) =                (aj )(bk ),

              i=j+k          i=j+k                 i=j+k

which is the coefficient of X i in (g)(h).

   Sometimes a more compact notation is convenient: we may prefer to write a for
the image of a  R under , and if we do this, then for g = i aiX i  R[X ], we
write g for the image i aiX i of g under the extension of  to R[X ]. 2

Example 7.47. Consider the natural map that sends a  Z to a := [a]n  Zn (see
Example 7.43). As in the previous example, we may extend this to a ring homomor-
phism from Z[X ] to Zn[X ] that sends g = i aiX i  Z[X ] to g = i aiX i  Zn[X ].
This homomorphism is clearly surjective. Let us determine its kernel. Observe that
if g = i aiX i, then g = 0 if and only if n | ai for each i; therefore, the kernel is
the ideal nZ[X ] of Z[X ]. 2
194                             Rings

Example 7.48. Let R be a ring of prime characteristic p. For all a, b  R, we have

(see Exercise 7.1)

                    (a + b)p =  p

                                     p p-k k
                                       a b.
                                k=0 k

However, by Exercise 1.14, all of the binomial coefficients are multiples of p,
except for k = 0 and k = p, and hence in the ring R, all of these terms vanish,

leaving us with

                    (a + b)p = ap + bp.

This result is often jokingly referred to as the "freshman's dream," for somewhat
obvious reasons.

   Of course, as always, we have

                                (ab)p = apbp and 1Rp = 1R,
and so it follows that the map that sends a  R to ap  R is a ring homomorphism
from R into R. 2

Example 7.49. Suppose R is a non-trivial ring, and let  : R  R map everything
in R to 0R. Then  satisfies parts (i) and (ii) of Definition 7.20, but not part (iii). 2

Example 7.50. In special situations, part (iii) of Definition 7.20 may be redundant.
One such situation arises when  : R  R is surjective. In this case, we know that
1R = (a) for some a  R, and by part (ii) of the definition, we have

           (1R) = (1R) · 1R = (1R)(a) = (1R · a) = (a) = 1R . 2

   For a ring homomorphism  : R  R , all of the results of Theorem 6.19 apply.
In particular, (0R) = 0R , (a) = (b) if and only if a  b (mod Ker ), and  is
injective if and only if Ker  = {0R}. However, we may strengthen Theorem 6.19

as follows:

Theorem 7.21. Let  : R  R be a ring homomorphism.

    (i) If S is a subring of R, then (S) is a subring of R ; in particular (setting
         S := R), Im  is a subring of R .

   (ii) If S is a subring of R , then -1(S ) is a subring of R.

   (ii) If I is an ideal of R, then (I) is an ideal of Im .
   (iv) If I is an ideal of Im , then -1(I ) is an ideal of R; in particular (setting

         I := {0R }), Ker  is an ideal of R.

Proof. In each part, we already know that the relevant object is an additive sub-
group, and so it suffices to show that the appropriate additional properties are sat-

isfied.
                    7.4 Ring homomorphisms and isomorphisms                      195

 (i) For all a, b  S, we have ab  S, and hence (S) contains (ab) = (a)(b).
      Also, 1R  S, and hence (S) contains (1R) = 1R .

(ii) If (a)  S and (b)  S , then (ab) = (a)(b)  S . Moreover,
      (1R) = 1R  S .

(iii) For all a  I and r  R, we have ar  I, and hence (I) contains
      (ar) = (a)(r).

(iv) For all a  -1(I ) and r  R, we have (ar) = (a)(r), and since (a)
      belongs to the ideal I , so does (a)(r), and hence -1(I ) contains ar. 2

   Theorems 6.20 and 6.21 have natural ring analogs -- one only has to show that
the corresponding group homomorphisms satisfy the additional requirements of a
ring homomorphism, which we leave to the reader to verify:

Theorem 7.22. If  : R  R and  : R  R are ring homomorphisms, then
so is their composition    : R  R .

Theorem 7.23. Let i : R  Ri, for i = 1, . . . , k, be ring homomorphisms. Then
the map

                                 : R  R1 × · · · × Rk
                                       a  (1(a), . . . , k(a))

is a ring homomorphism.

   If a ring homomorphism  : R  R is a bijection, then it is called a ring
isomorphism of R with R . If such a ring isomorphism  exists, we say that R is
isomorphic to R , and write R = R . Moreover, if R = R , then  is called a ring
automorphism on R.

   Analogous to Theorem 6.22, we have:

Theorem 7.24. If  is a ring isomorphism of R with R , then the inverse function
-1 is a ring isomorphism of R with R.

Proof. Exercise. 2

   Because of this theorem, if R is isomorphic to R , we may simply say that "R
and R are isomorphic." We stress that a ring isomorphism is essentially just a
"renaming" of elements; in particular, we have:

Theorem 7.25. Let  : R  R be a ring isomorphism.
    (i) For all a  R, a is a zero divisor if and only if (a) is a zero divisor.
   (ii) For all a  R, a is a unit if and only if (a) is a unit.

   (iii) The restriction of R to R is a group isomorphism of R with (R ).
196                    Rings

Proof. Exercise. 2

   An injective ring homomorphism  : R  E is called an embedding of R in E.
In this case, Im  is a subring of E and R = Im . If the embedding is a natural one
that is clear from context, we may simply identify elements of R with their images
in E under the embedding; that is, for a  R, we may simply write "a," and it is
understood that this really means "(a)" if the context demands an element of E.
As a slight abuse of terminology, we shall say that R is a subring of E. Indeed,
by appropriately renaming elements, we can always make R a subring of E in the
literal sense of the term.

   This practice of identifying elements of a ring with their images in another ring
under a natural embedding is very common. We have already seen an example of
this, namely, when we formally defined the ring of polynomials R[X ] over R in
§7.2.1, we defined the map 0 : R  R[X ] that sends c  R to the polynomial
whose constant term is c, with all other coefficients zero. This map 0 is an embed-
ding, and it was via this embedding that we identified elements of R with elements
of R[X ], and so viewed R as a subring of R[X ]. We shall see more examples of
this later (in particular, Example 7.55 below).

   Theorems 6.23 and 6.24 also have natural ring analogs -- again, one only has to
show that the corresponding group homomorphisms are also ring homomorphisms:

Theorem 7.26 (First isomorphism theorem). Let  : R  R be a ring homo-
morphism with kernel K and image S . Then we have a ring isomorphism

                                          R/K = S .

Specifically, the map

                        : R/K  R
                             [a]K  (a)

is an injective ring homomorphism whose image is S .

Theorem 7.27. Let  : R  R be a ring homomorphism. Then for every ideal I
of R with I  Ker , we may define a ring homomorphism

                                        : R/I  R
                                             [a]I  (a).

Moreover, Im  = Im , and  is injective if and only if I = Ker .

Example 7.51. Returning again to the Chinese remainder theorem and the discus-
sion in Example 6.48, if {ni}ik=1 is a pairwise relatively prime family of positive
7.4 Ring homomorphisms and isomorphisms   197

integers, and n := ik=1 ni, then the map

 : Z  Zn1 × · · · × Znk
      a  ([a]n1, . . . , [a]nk )

is not just a surjective group homomorphism with kernel nZ, it is also a ring homo-
morphism. Applying Theorem 7.26, we get a ring isomorphism

                                : Zn  Zn1 × · · · × Znk

                                     [a]n  ([a]n1, . . . , [a]nk ),

which is the same function as the function  in Theorem 2.8. By part (iii) of
Theorem 7.25, the restriction of  to Zn is a group isomorphism of Zn with the
multiplicative group of units of Zn1 × · · · × Znk , which (according to Example 7.15)
is Zn1 × · · · × Znk . Thus, part (iii) of Theorem 2.8 is an immediate consequence of
the above observations. 2

Example 7.52. Extending Example 6.49, if n1 and n2 are positive integers with
n1 | n2, then the map

                                        : Zn2  Zn1
                                             [a]n2  [a]n1

is a surjective ring homomorphism. 2

Example 7.53. For a ring R, consider the map  : Z  R that sends m  Z
to m · 1R in R. It is easily verified that  is a ring homomorphism. Since Ker 
is an ideal of Z, it is either {0} or of the form nZ for some n > 0. In the first
case, if Ker  = {0}, then Im  = Z, and so the ring Z is embedded in R, and R
has characteristic zero. In the second case, if Ker  = nZ for some n > 0, then
by Theorem 7.26, Im  = Zn, and so the ring Zn is embedded in R, and R has
characteristic n.

   Note that Im  is the smallest subring of R: any subring of R must contain 1R
and be closed under addition and subtraction, and so must contain Im . 2

Example 7.54. We can generalize Example 7.44 by evaluating polynomials at sev-
eral points. This is most fruitful when the underlying coefficient ring is a field, and
the evaluation points belong to the same field. So let F be a field, and let x1, . . . , xk
be distinct elements of F . Define the map

                            : F [X ]  F ×k

                                        g  (g(x1), . . . , g(xk)).

This is a ring homomorphism (as seen by applying Theorem 7.23 to the polynomial
evaluation maps at the points x1, . . . , xk). By Theorem 7.13, Ker  = (f ), where
198          Rings

f := ik=1(X -xi). By Theorem 7.15,  is surjective. Therefore, by Theorem 7.26,
we get a ring isomorphism

              : F [X ]/(f )  F ×k

             [g]f  (g(x1), . . . , g(xk)). 2

Example 7.55. As in Example 7.39, let f be a polynomial over a ring R with
deg(f ) = and lc(f )  R, but now assume that > 0. Consider the natural
map  from R[X ] to the quotient ring E := R[X ]/(f ) that sends g  R[X ] to

[g]f . Let  be the restriction of  to the subring R of R[X ]. Evidently,  is a ring
homomorphism from R into E. Moreover, since distinct polynomials of degree

less than belong to distinct residue classes modulo f , we see that  is injective.

Thus,  is an embedding of R into E. As  is a very natural embedding, we can

identify elements of R with their images in E under , and regard R as a subring
of E. Taking this point of view, we see that if g = i aiX i, then

     [g]f =    aiX i = [ai]f ([X ]f )i = aii = g(),

             i fi i

where  := [X ]f  E. Therefore, the natural map  may be viewed as the polyno-

mial evaluation map (see Example 7.44) that sends g  R[X ] to g()  E.
   Note that we have E = R[]; moreover, every element of E can be expressed

uniquely as g() for some g  R[X ] of degree less than , and more generally, for
arbitrary g, h  R[X ], we have g() = h() if and only if g  h (mod f ). Finally,
note that f () = [f ]f = [0]f ; that is,  is a root of f . 2

Example 7.56. As a special case of Example 7.55, let f := X 2 + 1  R[X ],
and consider the quotient ring R[X ]/(f ). If we set i := [X ]f  R[X ]/(f ), then
every element of R[X ]/(f ) can be expressed uniquely as a + bi, where a, b  R.
Moreover, we have i2 = -1, and more generally, for all a, b, a , b  R, we have

                        (a + bi) + (a + b i) = (a + a ) + (b + b )i

and
                     (a + bi) · (a + b i) = (aa - bb ) + (ab + a b)i.

Thus, the rules for arithmetic in R[X ]/(f ) are precisely the familiar rules of com-
plex arithmetic, and so C and R[X ]/(f ) are essentially the same, as rings. Indeed,
the "algebraically correct" way of defining the field of complex numbers C is sim-
ply to define it to be the quotient ring R[X ]/(f ) in the first place. This will be our
point of view from now on. 2
7.4 Ring homomorphisms and isomorphisms                                199

Example 7.57. Consider the polynomial evaluation map
                            : R[X ]  C = R[X ]/(X 2 + 1)
                                        g  g(-i).

For every g  R[X ], we may write g = (X 2 + 1)q + a + bX , where q  R[X ] and
a, b  R. Since (-i)2 + 1 = i2 + 1 = 0, we have

                      g(-i) = ((-i)2 + 1)q(-i) + a - bi = a - bi.

Clearly, then,  is surjective and the kernel of  is the ideal of R[X ] generated by
the polynomial X 2 + 1. By Theorem 7.26, we therefore get a ring automorphism 
on C that sends a + bi  C to a - bi. In fact,  is none other than the complex con-
jugation map. Indeed, this is the "algebraically correct" way of defining complex

conjugation in the first place. 2

Example 7.58. We defined the ring Z[i] of Gaussian integers in Example 7.25 as
a subring of C. Let us verify that the notation Z[i] introduced in Example 7.25 is
consistent with that introduced in Example 7.44. Consider the polynomial evalua-

tion map  : Z[X ]  C that sends g  Z[X ] to g(i)  C. For every g  Z[X ], we
may write g = (X 2 + 1)q + a + bX , where q  Z[X ] and a, b  Z. Since i2 + 1 = 0,
we have g(i) = (i2 + 1)q(i) + a + bi = a + bi. Clearly, then, the image of  is
the set {a + bi : a, b  Z}, and the kernel of  is the ideal of Z[X ] generated by
the polynomial X 2 + 1. This shows that Z[i] in Example 7.25 is the same as Z[i]
in Example 7.44, and moreover, Theorem 7.26 implies that Z[i] is isomorphic to
Z[X ]/(X 2 + 1).

   Therefore, we can directly construct the Gaussian integers as the quotient ring
Z[X ]/(X 2 + 1). Likewise the field Q[i] (see Exercise 7.14) can be constructed
directly as Q[X ]/(X 2 + 1). 2

Example 7.59. Let p be a prime, and consider the quotient ring E := Zp[X ]/(f ),
where f := X 2 + 1. If we set i := [X ]f  E, then E = Zp[i] = {a + bi : a, b  Zp}.
In particular, E is a ring of cardinality p2. Moreover, we have i2 = -1, and the

rules for addition and multiplication in E look exactly the same as they do in C:
for all a, b, a , b  Zp, we have

                        (a + bi) + (a + b i) = (a + a ) + (b + b )i

and
                     (a + bi) · (a + b i) = (aa - bb ) + (ab + a b)i.

   The ring E may or may not be a field. We now determine for which primes p we
get a field.
200  Rings

   If p = 2, then 0 = 1 + i2 = (1 + i)2 (see Example 7.48), and so in this case, 1 + i

is a zero divisor and E is not a field.

   Now suppose p is odd. There are two subcases to consider: p  1 (mod 4) and

p  3 (mod 4).
   Suppose p  1 (mod 4). By Theorem 2.31, there exists c  Zp such that

c2 = -1, and therefore f = X 2 + 1 = X 2 - c2 = (X - c)(X + c), and by Exam-
ple 7.45, we have a ring isomorphism E = Zp × Zp (which maps a + bi  E to
(a + bc, a - bc)  Zp × Zp); in particular, E is not a field. Indeed, c + i is a zero
divisor, since (c + i)(c - i) = c2 - i2 = c2 + 1 = 0.

   Suppose p  3 (mod 4). By Theorem 2.31, there is no c  Zp such that c2 = -1.
It follows that for all a, b  Zp, not both zero, we must have a2 + b2 = 0; indeed,
suppose that a2 + b2 = 0, and that, say, b = 0; then we would have (a/b)2 = -1,
contradicting the assumption that -1 has no square root in Zp. Therefore, a2 + b2
has a multiplicative inverse in Zp, from which it follows that the formula for mul-
tiplicative inverses in C applies equally well in E; that is,

                                    (a + bi)-1 = a - bi .
                                                     a2 + b2

Therefore, in this case, E is a field. 2

   In Example 7.40, we saw a finite field of cardinality 4. The previous example
provides us with an explicit construction of a finite field of cardinality p2, for every
prime p congruent to 3 modulo 4. As the next example shows, there exist finite
fields of cardinality p2 for all primes p.

Example 7.60. Let p an odd prime, and let d  Zp. Let f := X 2 - d  Zp[X ],
and consider the ring E := Zp[X ]/(f ) = Zp[], where  := [X ]f  E. We have
E = {a + b : a, b  Zp} and |E| = p2. Note that 2 = d, and the general rules for
arithmetic in E look like this: for all a, b, a , b  Zp, we have

                       (a + b) + (a + b ) = (a + a ) + (b + b )

and

                   (a + b) · (a + b ) = (aa + bb d) + (ab + a b).

  Suppose that d  (Zp)2, so that d = c2 for some c  Zp. Then f = (X -c)(X +c),
and like in previous example, we have a ring isomorphism E = Zp × Zp (which
maps a + b  E to (a + bc, a - bc)  Zp × Zp); in particular, E is not a field.

   Suppose that d / (Zp)2. This implies that for all a, b  Zp, not both zero, we
have a2 - b2d = 0. Using this, we get the following formula for multiplicative
inverses in E:

                                   (a + b)-1 = a - b .
                                                    a2 - b2d
7.4 Ring homomorphisms and isomorphisms  201

Therefore, E is a field in this case.

   By Theorem 2.20, we know that |(Zp)2| = (p - 1)/2, and hence there exists
d  Zp \ (Zp)2 for all odd primes p. Thus, we have a general (though not explicit)
construction for finite fields of cardinality p2 for all odd primes p. 2

EXERCISE 7.47. Show that if  : F  R is a ring homomorphism from a field F
into a ring R, then either R is trivial or  is injective. Hint: use Exercise 7.25.

EXERCISE 7.48. Verify that the "is isomorphic to" relation on rings is an equiva-
lence relation; that is, for all rings R1, R2, R3, we have:

    (a) R1 = R1;
   (b) R1 = R2 implies R2 = R1;
    (c) R1 = R2 and R2 = R3 implies R1 = R3.

EXERCISE 7.49. Let i : Ri  Ri, for i = 1, . . . , k, be ring homomorphisms.
Show that the map

                        : R1 × · · · × Rk  R1 × · · · × Rk
                                 (a1, . . . , ak)  (1(a1), . . . , k(ak))

is a ring homomorphism.

EXERCISE 7.50. Let  : R  R be a ring homomorphism, and let a  R. Show
that (aR) = (a)(R).

EXERCISE 7.51. Let  : R  R be a ring homomorphism. Let S be a subring
of R, and let  : S  R be the restriction of  to S. Show that  is a ring
homomorphism and that Ker  = Ker   S.

EXERCISE 7.52. Suppose R1, . . . , Rk are rings. Show that for each i = 1, . . . , k,
the projection map i : R1 × · · · × Rk  Ri that sends (a1, . . . , ak) to ai is a
surjective ring homomorphism.

EXERCISE 7.53. Show that if R = R1 × R2 for rings R1 and R2, and I1 is an ideal
of R1 and I2 is an ideal of R2, then we have a ring isomorphism R/(I1 × I2) =
R1/I1 × R2/I2.

EXERCISE 7.54. Let I be an ideal of R, and S a subring of R. As we saw in
Exercises 7.28, and 7.29, I  S is an ideal of S, and I is an ideal of the subring
I + S. Show that we have a ring isomorphism (I + S)/I = S/(I  S).

EXERCISE 7.55. Let  : R  R be a ring homomorphism with kernel K. Let I
be an ideal of R. Show that we have a ring isomorphism R/(I + K) = (R)/(I).
202  Rings

EXERCISE 7.56. Let n be a positive integer, and consider the natural map that
sends a  Z to a := [a]n  Zn, which we may extend coefficient-wise to a ring
homomorphism from Z[X ] to Zn[X ], as in Example 7.47. Show that for every
f  Z[X ], we have a ring isomorphism Z[X ]/(f , n) = Zn[X ]/(f ).

EXERCISE 7.57. Let n be a positive integer. Show that we have ring isomorphisms
Z[X ]/(n) = Zn[X ], Z[X ]/(X ) = Z, and Z[X ]/(X , n) = Zn.

EXERCISE 7.58. Let n = pq, where p and q are distinct primes. Show that we
have a ring isomorphism Zn[X ] = Zp[X ] × Zq[X ].

EXERCISE 7.59. Let p be a prime with p  1 (mod 4). Show that we have a ring
isomorphism Z[X ]/(X 2 + 1, p) = Zp × Zp.

EXERCISE 7.60. Let  : R  R be a surjective ring homomorphism. Let S be
the set of all ideals of R that contain Ker , and let S be the set of all ideals of
R . Show that the sets S and S are in one-to-one correspondence, via the map that
sends I  S to (I)  S . Moreover, show that under this correspondence, prime
ideals in S correspond to prime ideals in S , and maximal ideals in S correspond
to maximal ideals in S . (See Exercise 7.38.)

EXERCISE 7.61. Let n be a positive integer whose factorization into primes is
n = p1e1 · · · prer . What are the prime ideals of Zn? (See Exercise 7.38.)

EXERCISE 7.62. Let  : R  S be a ring homomorphism. Show that (R)  S,
and that the restriction of  to R yields a group homomorphism  : R  S.

EXERCISE 7.63. Let R be a ring, and let x1, . . . , xn be elements of R. Show that
the rings R and R[X1, . . . , Xn]/(X1 - x1, . . . , Xn - xn) are isomorphic.

EXERCISE 7.64. This exercise and the next generalize the Chinese remainder the-

orem to arbitrary rings. Suppose I and J are two ideals of a ring R such that
I + J = R. Show that the map  : R  R/I × R/J that sends a  R to
([a]I , [a]J ) is a surjective ring homomorphism with kernel IJ (see Exercise 7.41).
Conclude that R/(IJ ) is isomorphic to R/I × R/J .

EXERCISE 7.65. Generalize the previous exercise, showing that R/(I1 · · · Ik) is
isomorphic to R/I1 × · · · × R/Ik, where R is a ring, and I1, . . . , Ik are ideals of
R, provided Ii + Ij = R for all i, j such that i = j.

EXERCISE 7.66. Let Q(m) be the subring of Q defined in Example 7.26. Let us
define the map  : Q(m)  Zm as follows. For a/b  Q with b relatively prime
to m, (a/b) := [a]m([b]m)-1. Show that  is unambiguously defined, and is a
surjective ring homomorphism. Also, describe the kernel of .
7.5 The structure of Zn                            203

EXERCISE 7.67. Let R be a ring, a  R, and b  R. Define the map  : R[X ] 
R[X ] that sends g  R[X ] to g aX + b . Show that  is a ring automorphism.

EXERCISE 7.68. Consider the subring Z[1/2] of Q. Show that Z[1/2] = {a/2i :
a, i  Z, i  0}, that (Z[1/2]) = {2i : i  Z}, and that every non-zero ideal of
Z[1/2] is of the form (m), for some uniquely determined, odd integer m.

                                  7.5 The structure of Zn
We are now in a position to precisely characterize the structure of the group Zn, for
an arbitrary integer n > 1. This characterization will prove to be very useful in a

number of applications.
   Suppose n = p1e1 · · · prer is the factorization of n into primes. By the Chinese

remainder theorem (see Theorem 2.8 and Example 7.51), we have the ring isomor-

phism

 : Zn  Zpe1 × · · · × Zperr

                                 1

[a]n  ([a]pe1 , . . . , [a]perr ),

                           1

and restricting  to Zn yields a group isomorphism

Zn = Z e1  × · · · × Z er  .
p1  pr

Thus, to determine the structure of the group Zn for general n, it suffices to deter-
mine the structure for n = pe, where p is prime. By Theorem 2.10, we already
know the order of the group Zpe  , namely, (pe) = pe-1(p - 1), where  is Euler's
phi function.

The main result of this section is the following:

Theorem 7.28. If p is an odd prime, then for every positive integer e, the group
Zpe  is cyclic. The group Z2e  is cyclic for e = 1 or 2, but not for e  3. For e  3,
Z2e  is isomorphic to the additive group Z2 × Z2e-2.

   In the case where e = 1, this theorem is a special case of the following, more
general, theorem:

Theorem 7.29. Let D be an integral domain and G a subgroup of D of finite
order. Then G is cyclic.

Proof. Suppose G is not cyclic. If m is the exponent of G, then by Theorem 6.41,
we know that m < |G|. Moreover, by definition, am = 1 for all a  G; that is, every
element of G is a root of the polynomial X m - 1  D[X ]. But by Theorem 7.14, a

polynomial of degree m over an integral domain has at most m distinct roots, and

this contradicts the fact that m < |G|. 2
204      Rings

   This theorem immediately implies that Zp is cyclic for every prime p, since Zp
is a field; however, we cannot directly use this theorem to prove that Zpe  is cyclic
for e > 1 (and p odd), because Zpe is not a field. To deal with the case e > 1, we
need a few simple facts.

Lemma 7.30. Let p be a prime. For every positive integer e, if a  b (mod pe),
then ap  bp (mod pe+1).

Proof. Suppose a  b (mod pe), so that a = b + cpe for some c  Z. Then
ap = bp +pbp-1cpe +dp2e for some d  Z, and it follows that ap  bp (mod pe+1). 2

Lemma 7.31. Let p be a prime, and let e be a positive integer such that pe > 2. If
a  1 + pe (mod pe+1), then ap  1 + pe+1 (mod pe+2).

Proof. Suppose a  1+pe (mod pe+1). By Lemma 7.30, ap  (1+pe)p (mod pe+2).
Expanding (1 + pe)p, we have

     (1 + pe)p = 1 + p · pe +  p-1

                                     p ek ep
                                      p +p .
                               k=2 k

By Exercise 1.14, all of the terms in the sum on k are divisible by p1+2e, and
1 + 2e  e + 2 for all e  1. For the term pep, the assumption that pe > 2 means
that either p  3 or e  2, which implies ep  e + 2. 2

   Now consider Theorem 7.28 in the case where p is odd. As we already know

that Zp is cyclic, assume e > 1. Let x  Z be chosen so that [x]p generates Zp.
Suppose the multiplicative order of [x]pe  Zpe  is m. We have xm  1 (mod pe);
hence, xm  1 (mod p), and so it must be the case that p - 1 divides m; thus,

[xm/(p-1)]pe has multiplicative order exactly p - 1. By Theorem 6.38, if we find
an integer y such that [y]pe has multiplicative order pe-1, then [xm/(p-1)y]pe has
multiplicative order (p - 1)pe-1, and we are done. We claim that y := 1 + p does

the job. Any integer between 0 and pe - 1 can be expressed as an e-digit number in
base p; for example, y = (0 · · · 0 1 1)p. If we compute successive pth powers of y
modulo pe, then by Lemma 7.31 we have

     y mod pe = (0 · · ·              0 1 1)p,

     yp mod pe = ( · · ·  1 0 1)p,

     yp2 mod pe = ( · · ·  1 0 0 1)p,

     ..
     .

     ype-2 mod pe = (1 0 · · ·        0 1)p,

     ype-1 mod pe = (0 · · ·          0 1)p.

Here, "" indicates an arbitrary digit. From this table of values, it is clear (see
7.5 The structure of Zn            205

Theorem 6.37) that [y]pe has multiplicative order pe-1. That proves Theorem 7.28
for odd p.

   We now prove Theorem 7.28 in the case p = 2. For e = 1 and e = 2, the theorem
is easily verified. Suppose e  3. Consider the subgroup G  Z2e  generated by
[5]2e. Expressing integers between 0 and 2e - 1 as e-digit binary numbers, and

applying Lemma 7.31, we have

5 mod 2e = (0 · · · 0 1 0 1)2,

52 mod 2e = ( · · ·  1 0 0 1)2,

..
.

52e-3 mod 2e = (1 0 · · ·  0 1)2,

52e-2 mod 2e = (0 · · ·    0 1)2.

So it is clear (see Theorem 6.37) that [5]2e has multiplicative order 2e-2. We claim

that [-1]2e / G. If it were, then since it has multiplicative order 2, and since every

cyclic group of even order has precisely one element of order 2 (see Theorem 6.32),
it must be equal to [52e-3]2e; however, it is clear from the above calculation that
52e-3  -1 (mod 2e). Let H  Z2e  be the subgroup generated by [-1]2e. Then
from the above, GH = {[1]2e}, and hence by Theorem 6.25, G×H is isomorphic
to the subgroup G · H of Z2e  . But since the orders of G × H and Z2e  are equal, we
must have G · H = Z2e  . That proves the theorem.

Example 7.61. Let p be an odd prime, and let d be a positive integer dividing p - 1.

Since Zp is a cyclic group of order p - 1, Theorem 6.32, implies that (Zp)d is the
unique subgroup of Zp of order (p - 1)/d, and moreover, (Zp)d = Zp{(p - 1)/d};
that is, for all   Zp, we have

                        = d for some   Zp  (p-1)/d = 1.

Setting d = 2, we arrive again at Euler's criterion (Theorem 2.21), but by a very
different, and perhaps more elegant, route than that taken in our original proof of
that theorem. 2

EXERCISE 7.69. Show that if n is a positive integer, the group Zn is cyclic if and
only if

                                    n = 1, 2, 4, pe, or 2pe,

where p is an odd prime and e is a positive integer.

EXERCISE 7.70. Let n = pq, where p and q are distinct primes such that p = 2p +1
and q = 2q + 1, where p and q are themselves prime. Show that the subgroup
(Zn)2 of squares is a cyclic group of order p q .
206  Rings

EXERCISE 7.71. Let n = pq, where p and q are distinct primes such that p (q -1)

and q (p - 1).

    (a) Show that the map that sends [a]n  Zn to [an]n2  (Zn2 )n is a group iso-
         morphism (in particular, you need to show that this map is unambiguously

         defined).

   (b) Consider the element  := [1+n]n2  Zn2  ; show that for every non-negative
         integer k, k = [1 + kn]n2; deduce that  has multiplicative order n, and
         also that the identity k = [1 + kn]n2 holds for all integers k.

    (c) Show that the map that sends ([k]n, [a]n)  Zn × Zn to [(1 + kn)an]n2  Zn2 
         is a group isomorphism.

EXERCISE 7.72. This exercise develops an alternative proof of Theorem 7.29 that
relies on less group theory. Let n be the order of the group G. Using Theorem 7.14,
show that for all d | n, there are at most d elements in the group whose multiplica-
tive order divides d. From this, deduce that for all d | n, the number of elements
of multiplicative order d is either 0 or (d). Now use Theorem 2.40 to deduce that
for all d | n (and in particular, for d = n), the number of elements of multiplicative
order d is equal to (d).
                      8

Finite and discrete probability distributions

To understand the algorithmic aspects of number theory and algebra, and appli-
cations such as cryptography, a firm grasp of the basics of probability theory is
required. This chapter introduces concepts from probability theory, starting with
the basic notions of probability distributions on finite sample spaces, and then
continuing with conditional probability and independence, random variables, and
expectation. Applications such as "balls and bins," "hash functions," and the "left-
over hash lemma" are also discussed. The chapter closes by extending the basic
theory to probability distributions on countably infinite sample spaces.

                                    8.1 Basic definitions
Let  be a finite, non-empty set. A probability distribution on  is a function
P :   [0, 1] that satisfies the following property:

     P() = 1.                                  (8.1)



The set  is called the sample space of P.
   Intuitively, the elements of  represent the possible outcomes of a random

experiment, where the probability of outcome    is P(). For now, we
shall only consider probability distributions on finite sample spaces. Later in this
chapter, in §8.10, we generalize this to allow probability distributions on countably
infinite sample spaces.

Example 8.1. If we think of rolling a fair die, then setting  := {1, 2, 3, 4, 5, 6},
and P() := 1/6 for all   , gives a probability distribution that naturally

describes the possible outcomes of the experiment. 2

Example 8.2. More generally, if  is any non-empty, finite set, and P() := 1/||
for all   , then P is called the uniform distribution on . 2

207
208  Finite and discrete probability distributions

Example 8.3. A coin toss is an example of a Bernoulli trial, which in general

is an experiment with only two possible outcomes: success, which occurs with
probability p; and failure, which occurs with probability q := 1 - p. Of course,

success and failure are arbitrary names, which can be changed as convenient. In the

case of a coin, we might associate success with the outcome that the coin comes up
heads. For a fair coin, we have p = q = 1/2; for a biased coin, we have p = 1/2. 2

An event is a subset A of , and the probability of A is defined to be

     P[A] := P().                                                      (8.2)

                    A

While an event is simply a subset of the sample space, when discussing the proba-
bility of an event (or other properties to be introduced later), the discussion always
takes place relative to a particular probability distribution, which may be implicit
from context.

   For events A and B, their union A  B logically represents the event that either
the event A or the event B occurs (or both), while their intersection A  B logi-
cally represents the event that both A and B occur. For an event A, we define its
complement A :=  \ A, which logically represents the event that A does not
occur.

   In working with events, one makes frequent use of the usual rules of Boolean
logic. De Morgan's law says that for all events A and B,

     A  B = A  B and A  B = A  B.

We also have the Boolean distributive law: for all events A, B, and C,
   A  (B  C) = (A  B)  (A  C) and A  (B  C) = (A  B)  (A  C).

Example 8.4. Continuing with Example 8.1, the event that the die has an odd
value is A := {1, 3, 5}, and we have P[A] = 1/2. The event that the die has a
value greater than 2 is B := {3, 4, 5, 6}, and P[B] = 2/3. The event that the die
has a value that is at most 2 is B = {1, 2}, and P[B] = 1/3. The event that the
value of the die is odd or exceeds 2 is A  B = {1, 3, 4, 5, 6}, and P[A  B] = 5/6.
The event that the value of the die is odd and exceeds 2 is A  B = {3, 5}, and
P[A  B] = 1/3. 2

Example 8.5. If P is the uniform distribution on a set , and A is a subset of ,
then P[A] = |A|/||. 2

   We next derive some elementary facts about probabilities of certain events, and
relations among them. It is clear from the definitions that

                                  P[] = 0 and P[] = 1,
               8.1 Basic definitions                                            209

and that for every event A, we have
                                      P[A] = 1 - P[A].

Now consider events A and B, and their union A  B. We have

               P[A  B]  P[A] + P[B];                                            (8.3)

moreover,

           P[A  B] = P[A] + P[B] if A and B are disjoint,                       (8.4)

that is, if A  B = . The exact formula for arbitrary events A and B is:

           P[A  B] = P[A] + P[B] - P[A  B].                                     (8.5)

(8.3), (8.4), and (8.5) all follow from the observation that in the expression

           P[A] + P[B] = P() + P(),

                   A                                    B

the value P() is counted once for each   A  B, except for those   A  B,
for which P() is counted twice.

Example 8.6. Alice rolls two dice, and asks Bob to guess a value that appears on
either of the two dice (without looking). Let us model this situation by considering
the uniform distribution on  := {1, . . . , 6} × {1, . . . , 6}, where for each pair
(s, t)  , s represents the value of the first die, and t the value of the second.

   For k = 1, . . . , 6, let Ak be the event that the first die is k, and Bk the event that
the second die is k. Let Ck = Ak  Bk be the event that k appears on either of the
two dice. No matter what value k Bob chooses, the probability that this choice is
correct is

                P[Ck] = P[Ak  Bk] = P[Ak] + P[Bk] - P[Ak  Bk]
                        = 1/6 + 1/6 - 1/36 = 11/36,

which is slightly less than the estimate P[Ak] + P[Bk] obtained from (8.3). 2

   If {Ai}iI is a family of events, indexed by some set I, we can naturally form the
union iI Ai and intersection iI Ai. If I = , then by definition, the union is ,
and by special convention, the intersection is the entire sample space . Logically,

the union represents the event that some Ai occurs, and the intersection represents

the event that all the Ai's occur. De Morgan's law generalizes as follows:

               Ai = Ai and Ai = Ai,

           iI  iI     iI                                iI
210                Finite and discrete probability distributions

and if B is an event, then the Boolean distributive law generalizes as follows:

     B  Ai = (B  Ai) and B  Ai = (B  Ai).

              iI   iI                             iI              iI

   We now generalize (8.3), (8.4), and (8.5) from pairs of events to families of
events. Let {Ai}iI be a finite family of events (i.e., the index set I is finite).
Using (8.3), it follows by induction on |I| that

                       P Ai  P[Ai],                                              (8.6)

                           iI    iI

which is known as Boole's inequality (and sometimes called the union bound).
Analogously, using (8.4), it follows by induction on |I| that

              P Ai = P[Ai] if {Ai}iI is pairwise disjoint,                       (8.7)

              iI       iI

that is, if Ai  Aj =  for all i, j  I with i = j. We shall refer to (8.7) as Boole's

equality. Both (8.6) and (8.7) are invaluable tools in calculating or estimating the

probability of an event A by breaking A up into a family {Ai}iI of smaller, and

hopefully simpler, events, whose union is A. We shall make frequent use of them.

The generalization of (8.5) is messier. Consider first the case of three events, A,

B, and C. We have

     P[A  B  C] = P[A] + P[B] + P[C] - P[A  B] - P[A  C] - P[B  C]
                            + P[A  B  C].

Thus, starting with the sum of the probabilities of the individual events, we have
to subtract a "correction term" that consists of the sum of probabilities of all inter-
sections of pairs of events; however, this is an "over-correction," and we have to
correct the correction by adding back in the probability of the intersection of all
three events. The general statement is as follows:

Theorem 8.1 (Inclusion/exclusion principle). Let {Ai}iI be a finite family of

events. Then

                   P Ai =        (-1)|J|-1 P Aj ,

                   iI       J I                       jJ

the sum being over all non-empty subsets J of I.

Proof. For    and B  , define [B] := 1 if   B, and [B] := 0

if  / B. As a function of , [B] is simply the characteristic function of

B. One may easily verify that for all   , B  , and C  , we have
[B] = 1 - [B] and [B  C] = [B][C]. It is also easily seen that for
every B  , we have  P()[B] = P[B].
                          8.1 Basic definitions                                 211

Let A := iI Ai, and for J  I, let AJ := jJ Aj. For every   ,

        1 - [A] = [A] =  Ai = [Ai] = (1 - [Ai])

                             iI          iI                 iI

             = (-1)|J| [Aj] = (-1)|J|[AJ ],

                 J I      jJ                J I

and so

                      [A] =        (-1)|J|-1[AJ ].                              (8.8)

                              J I

Multiplying (8.8) by P(), and summing over all   , we have

     P[A] = P()[A] = P()                     (-1)|J|-1[AJ ]

                                    J I

        =        (-1)|J|-1 P()[AJ ] =                  (-1)|J|-1 P[AJ ]. 2

            J I                                   J I

   One can also state the inclusion/exclusion principle in a slightly different way,
splitting the sum into terms with |J | = 1, |J | = 2, etc., as follows:

                                   |I |

        P Ai          = P[Ai] + (-1)k-1 P Aj ,

            iI        iI           k=2           J I jJ
                                                 |J |=k

where the last sum in this formula is taken over all subsets J of I of size k.

   We next consider a useful way to "glue together" probability distributions. Sup-
pose one conducts two physically separate and unrelated random experiments, with
each experiment modeled separately as a probability distribution. What we would
like is a way to combine these distributions, obtaining a single probability dis-
tribution that models the two experiments as one grand experiment. This can be
accomplished in general, as follows.

   Let P1 : 1  [0, 1] and P2 : 2  [0, 1] be probability distributions. Their
product distribution P := P1 P2 is defined as follows:

                      P : 1 × 2  [0, 1]
                             (1, 2)  P1(1) P2(2).

It is easily verified that P is a probability distribution on the sample space 1 × 2:

     P(1, 2) =        P1(1) P2(2) = P1(1)                   P2(2) = 1 · 1 = 1.

1,2             1,2                      1               2

More generally, if Pi : i  [0, 1], for i = 1, . . . , n, are probability distributions,
212  Finite and discrete probability distributions

then their product distribution is P := P1 · · · Pn, where

                        P : 1 × · · · × n  [0, 1]

                                 (1, . . . , n)  P1(1) · · · Pn(n).

If P1 = P2 = · · · = Pn, then we may write P = P1n. It is clear from the definitions
that if each Pi is the uniform distribution on i, then P is the uniform distribution
on 1 × · · · × n.

Example 8.7. We can view the probability distribution P in Example 8.6 as P21,
where P1 is the uniform distribution on {1, . . . , 6}. 2

Example 8.8. Suppose we have a coin that comes up heads with some probability
p, and tails with probability q := 1 - p. We toss the coin n times, and record the
outcomes. We can model this as the product distribution P = P1n, where P1 is the
distribution of a Bernoulli trial (see Example 8.3) with success probability p, and

where we identify success with heads, and failure with tails. The sample space 
of P is the set of all 2n tuples  = (1, . . . , n), where each i is either heads or
tails. If the tuple  has k heads and n - k tails, then P() = pkqn-k, regardless of

the positions of the heads and tails in the tuple.
   For each k = 0, . . . , n, let Ak be the event that our coin comes up heads exactly

k times. As a set, Ak consists of all those tuples in the sample space with exactly

k heads, and so

                                         |Ak| = n ,
                                                     k

from which it follows that
                                    P[Ak] = n pkqn-k.
                                                  k

If our coin is a fair coin, so that p = q = 1/2, then P is the uniform distribution on
, and for each k = 0, . . . , n, we have

                                    P[Ak] = n 2-n. 2
                                                  k

   Suppose P :   [0, 1] is a probability distribution. The support of P is defined
to be the set {   : P() = 0}. Now consider another probability distribution
P :   [0, 1]. Of course, these two distributions are equal if and only if  = 
and P() = P () for all   . However, it is natural and convenient to have a

more relaxed notion of equality. We shall say that P and P are essentially equal if

the restriction of P to its support is equal to the restriction of P to its support. For

example, if P is the probability distribution on {1, 2, 3, 4} that assigns probability
              8.2 Conditional probability and independence         213

1/3 to 1, 2, and 3, and probability 0 to 4, we may say that P is essentially the
uniform distribution on {1, 2, 3}.

EXERCISE 8.1. Show that P[A  B] P[A  B]  P[A] P[B] for all events A, B.

EXERCISE 8.2. Suppose A, B, C are events such that A  C = B  C. Show that
|P[A] - P[B]|  P[C].

EXERCISE 8.3. Let m be a positive integer, and let (m) be the probability that a

number chosen at random from {1, . . . , m} is divisible by either 4, 5, or 6. Write
down an exact formula for (m), and also show that (m) = 14/30 + O(1/m).

EXERCISE 8.4. This exercise asks you to generalize Boole's inequality (8.6),

proving Bonferroni's inequalities. Let {Ai}iI be a finite family of events, where
n := |I|. For m = 0, . . . , n, define

              m

              m := (-1)k-1 P Aj .

              k=1            J I jJ
                             |J |=k

Also, define

                           := P Ai .

                                       iI

Show that   m if m is odd, and   m if m is even. Hint: use induction on n.

                   8.2 Conditional probability and independence

Let P be a probability distribution on a sample space .
   For a given event B   with P[B] = 0, and for   , let us define

              P( | B) :=  P()/ P[B] if   B,

                          0                otherwise.

Viewing B as fixed, the function P(· | B) is a new probability distribution on the
sample space , called the conditional distribution (derived from P) given B.

   Intuitively, P(· | B) has the following interpretation. Suppose a random exper-
iment produces an outcome according to the distribution P. Further, suppose we
learn that the event B has occurred, but nothing else about the outcome. Then the
distribution P(· | B) assigns new probabilities to all possible outcomes, reflecting
the partial knowledge that the event B has occurred.
214  Finite and discrete probability distributions

   For a given event A  , its probability with respect to the conditional distri-
bution given B is

                          P[A | B] = P( | B) = P[A  B] .
                                        A P[B]

The value P[A | B] is called the conditional probability of A given B. Again,
the intuition is that this is the probability that the event A occurs, given the partial
knowledge that the event B has occurred.

   For events A and B, if P[A  B] = P[A] P[B], then A and B are called inde-
pendent events. If P[B] = 0, one easily sees that A and B are independent if
and only if P[A | B] = P[A]; intuitively, independence means that the partial
knowledge that event B has occurred does not affect the likelihood that A occurs.

Example 8.9. Suppose P is the uniform distribution on , and that B   with
P[B] = 0. Then the conditional distribution given B is essentially the uniform

distribution on B. 2

Example 8.10. Consider again Example 8.4, where A is the event that the value

on the die is odd, and B is the event that the value of the die exceeds 2. Then as
we calculated, P[A] = 1/2, P[B] = 2/3, and P[A  B] = 1/3; thus, P[A  B] =
P[A] P[B], and we conclude that A and B are independent. Indeed, P[A | B] =
(1/3)/(2/3) = 1/2 = P[A]; intuitively, given the partial knowledge that the value

on the die exceeds 2, we know it is equally likely to be either 3, 4, 5, or 6, and so

the conditional probability that it is odd is 1/2.

   However, consider the event C that the value on the die exceeds 3. We have
P[C] = 1/2 and P[A  C] = 1/6 = 1/4, from which we conclude that A and C
are not independent. Indeed, P[A | C] = (1/6)/(1/2) = 1/3 = P[A]; intuitively,

given the partial knowledge that the value on the die exceeds 3, we know it is

equally likely to be either 4, 5, or 6, and so the conditional probability that it is odd

is just 1/3, and not 1/2. 2

Example 8.11. In Example 8.6, suppose that Alice tells Bob the sum of the two
dice before Bob makes his guess. The following table is useful for visualizing the
situation:

                                  6 7 8 9 10 11 12
                                  5 6 7 8 9 10 11
                                  4 5 6 7 8 9 10
                                  3456 7 8 9
                                  2345 6 7 8
                                  1234 5 6 7

                                        123 4 5 6

For example, suppose Alice tells Bob the sum is 4. Then what is Bob's best strategy
       8.2 Conditional probability and independence                215

in this case? Let D be the event that the sum is , for = 2, . . . , 12, and consider
the conditional distribution given D4. This conditional distribution is essentially
the uniform distribution on the set {(1, 3), (2, 2), (3, 1)}. The numbers 1 and 3 both
appear in two pairs, while the number 2 appears in just one pair. Therefore,

       P[C1 | D4] = P[C3 | D4] = 2/3,

while

       P[C2 | D4] = 1/3

and
                        P[C4 | D4] = P[C5 | D4] = P[C6 | D4] = 0.

Thus, if the sum is 4, Bob's best strategy is to guess either 1 or 3, which will be

correct with probability 2/3.

   Similarly, if the sum is 5, then we consider the conditional distribution given D5,
which is essentially the uniform distribution on {(1, 4), (2, 3), (3, 2), (4, 1)}. In this
case, Bob should choose one of the numbers k = 1, . . . , 4, each of which will be
correct with probability P[Ck | D5] = 1/2. 2

   Suppose {Bi}iI is a finite, pairwise disjoint family of events, whose union is
. Now consider an arbitrary event A. Since {A  Bi}iI is a pairwise disjoint
family of events whose union is A, Boole's equality (8.7) implies

       P[A] = P[A  Bi].                                            (8.9)

                     iI

Furthermore, if each Bi occurs with non-zero probability (so that, in particular,
{Bi}iI is a partition of ), then we have

                                P[A] = P[A | Bi] P[Bi].            (8.10)

                                                           iI

If, in addition, P[A] = 0, then for each j  I, we have

       P[Bj | A] = P[A  Bj] =   P[A | Bj] P[Bj]                    (8.11)
                          P[A]                         .

                                iI P[A | Bi] P[Bi]

Equations (8.9) and (8.10) are sometimes called the law of total probability, while
equation (8.11) is known as Bayes' theorem. Equation (8.10) (resp., (8.11)) is
useful for computing or estimating P[A] (resp., P[Bj | A]) by conditioning on the
events Bi.

Example 8.12. Let us continue with Example 8.11, and compute Bob's overall
probability of winning, assuming he follows an optimal strategy. If the sum is 2 or
12, clearly there is only one sensible choice for Bob to make, and it will certainly
216         Finite and discrete probability distributions

be correct. If the sum is any other number , and there are N pairs in the sample
space that sum to that number, then there will always be a value that appears in
exactly 2 of these N pairs, and Bob should choose such a value (see the diagram
in Example 8.11). Indeed, this is achieved by the simple rule of choosing the value
1 if  7, and the value 6 if > 7. This is an optimal strategy for Bob, and if C is
the event that Bob wins following this strategy, then by total probability (8.10), we
have

                                                           12

                                P[C] = P[C | D ] P[D ].

                                                           =2

Moreover,

     P[C | D2] P[D2] = 1 · 1 = 1 , P[C | D12] P[D12] = 1 · 1 = 1 ,
            36 36                                          36 36

and for = 3, . . . , 11, we have

            P[C | D ] P[D ] = 2 · N = 1 .
                                   N 36 18

Therefore,

            P[C] = 1 + 1 + 9 = 10 . 2
                     36 36 18 18

Example 8.13. Suppose that the rate of incidence of disease X in the overall pop-
ulation is 1%. Also suppose that there is a test for disease X; however, the test is
not perfect: it has a 5% false positive rate (i.e., 5% of healthy patients test positive
for the disease), and a 2% false negative rate (i.e., 2% of sick patients test negative
for the disease). A doctor gives the test to a patient and it comes out positive. How
should the doctor advise his patient? In particular, what is the probability that the
patient actually has disease X, given a positive test result?

   Amazingly, many trained doctors will say the probability is 95%, since the test
has a false positive rate of 5%. However, this conclusion is completely wrong.

   Let A be the event that the test is positive and let B be the event that the patient
has disease X. The relevant quantity that we need to estimate is P[B | A]; that is,
the probability that the patient has disease X, given a positive test result. We use
Bayes' theorem to do this:

P[B | A] = P[A | B] P[B] P[A | B] P[B] + P[A | B] P[B] = 0.98 · 0.01 0.98 · 0.01 + 0.05 · 0.99  0.17.

Thus, the chances that the patient has disease X given a positive test result are just
17%. The correct intuition here is that it is much more likely to get a false positive
than it is to actually have the disease.

   Of course, the real world is a bit more complicated than this example suggests:
8.2 Conditional probability and independence  217

the doctor may be giving the patient the test because other risk factors or symp-
toms may suggest that the patient is more likely to have the disease than a random
member of the population, in which case the above analysis does not apply. 2

Example 8.14. This example is based on the TV game show "Let's make a deal,"
which was popular in the 1970's. In this game, a contestant chooses one of three
doors. Behind two doors is a "zonk," that is, something amusing but of little or
no value, such as a goat, and behind one of the doors is a "grand prize," such
as a car or vacation package. We may assume that the door behind which the
grand prize is placed is chosen at random from among the three doors, with equal
probability. After the contestant chooses a door, the host of the show, Monty Hall,
always reveals a zonk behind one of the two doors not chosen by the contestant.
The contestant is then given a choice: either stay with his initial choice of door, or
switch to the other unopened door. After the contestant finalizes his decision on
which door to choose, that door is opened and he wins whatever is behind it. The
question is, which strategy is better for the contestant: to stay or to switch?

   Let us evaluate the two strategies. If the contestant always stays with his initial
selection, then it is clear that his probability of success is exactly 1/3.

   Now consider the strategy of always switching. Let B be the event that the
contestant's initial choice was correct, and let A be the event that the contestant
wins the grand prize. On the one hand, if the contestant's initial choice was correct,
then switching will certainly lead to failure (in this case, Monty has two doors to
choose from, but his choice does not affect the outcome). Thus, P[A | B] = 0.
On the other hand, suppose that the contestant's initial choice was incorrect, so
that one of the zonks is behind the initially chosen door. Since Monty reveals the
other zonk, switching will lead with certainty to success. Thus, P[A | B] = 1.
Furthermore, it is clear that P[B] = 1/3. So using total probability (8.10), we
compute

      P[A] = P[A | B] P[B] + P[A | B] P[B] = 0 · (1/3) + 1 · (2/3) = 2/3.

   Thus, the "stay" strategy has a success probability of 1/3, while the "switch"
strategy has a success probability of 2/3. So it is better to switch than to stay.

   Of course, real life is a bit more complicated. Monty did not always reveal a
zonk and offer a choice to switch. Indeed, if Monty only revealed a zonk when
the contestant had chosen the correct door, then switching would certainly be the
wrong strategy. However, if Monty's choice itself was a random decision made
independently of the contestant's initial choice, then switching is again the pre-
ferred strategy. 2

   We next generalize the notion of independence from pairs of events to families
of events. Let {Ai}iI be a finite family of events. For a given positive integer k,
218      Finite and discrete probability distributions

we say that the family {Ai}iI is k-wise independent if the following holds:

     P Aj = P[Aj] for all J  I with |J |  k.

     jJ      jJ

   The family {Ai}iI is called pairwise independent if it is 2-wise independent.
Equivalently, pairwise independence means that for all i, j  I with i = j, we have
P[Ai  Aj] = P[Ai] P[Aj], or put yet another way, that for all i, j  I with i = j,

the events Ai and Aj are independent.

   The family {Ai}iI is called mutually independent if it is k-wise independent

for all positive integers k. Equivalently, mutual independence means that

         P Aj = P[Aj] for all J  I.

         jJ      jJ

If n := |I| > 0, mutual independence is equivalent to n-wise independence; more-

over, if 0 < k  n, then {Ai}iI is k-wise independent if and only if {Aj}jJ is
mutually independent for every J  I with |J | = k.

In defining independence, the choice of the index set I plays no real role, and

we can rename elements of I as convenient.

Example 8.15. Suppose we toss a fair coin three times, which we formally model
using the uniform distribution on the set of all 8 possible outcomes of the three
coin tosses: (heads, heads, heads), (heads, heads, tails), etc., as in Example 8.8.
For i = 1, 2, 3, let Ai be the event that the ith toss comes up heads. Then {Ai}i3=1
is a mutually independent family of events, where each individual Ai occurs with
probability 1/2.

   Now let B12 be the event that the first and second tosses agree (i.e., both heads
or both tails), let B13 be the event that the first and third tosses agree, and let B23
be the event that the second and third tosses agree. Then the family of events
B12, B13, B23 is pairwise independent, but not mutually independent. Indeed, the
probability that any given individual event occurs is 1/2, and the probability that
any given pair of events occurs is 1/4; however, the probability that all three events
occur is also 1/4, since if any two events occur, then so does the third. 2

   We close this section with some simple facts about independence of events and
their complements.

Theorem 8.2. If A and B are independent events, then so are A and B.

Proof. We have

         P[A] = P[A  B] + P[A  B] (by total probability (8.9))
                = P[A] P[B] + P[A  B] (since A and B are independent).
8.2 Conditional probability and independence                        219

Therefore,
        P[A  B] = P[A] - P[A] P[B] = P[A](1 - P[B]) = P[A] P[B]. 2

This theorem implies that

A and B are independent  A and B are independent
                                 A and B " "
                                 A and B " " .

   The following theorem generalizes this result to families of events. It says that
if a family of events is k-wise independent, then the family obtained by comple-
menting any number of members of the given family is also k-wise independent.

Theorem 8.3. Let {Ai}iI be a finite, k-wise independent family of events. Let
J be a subset of I, and for each i  I, define Ai := Ai if i  J , and Ai := Ai if
i / J . Then {Ai}iI is also k-wise independent.

Proof. It suffices to prove the theorem for the case where J = I \ {d}, for an

arbitrary d  I: this allows us to complement any single member of the family
that we wish, without affecting independence; by repeating the procedure, we can

complement any number of them.
   To this end, it will suffice to show the following: if J  I, |J | < k, d  I \ J ,

and AJ := jJ Aj, we have

P[Ad  AJ ] = (1 - P[Ad]) P[Aj].                                     (8.12)

                                               jJ

Using total probability (8.9), along with the independence hypothesis (twice), we
have

    P[Aj] = P[AJ ] = P[Ad  AJ ] + P[Ad  AJ ]

jJ

            = P[Ad] · P[Aj] + P[Ad  AJ ],

                                   jJ

from which (8.12) follows immediately. 2

EXERCISE 8.5. For events A1, . . . , An, define 1 := P[A1], and for i = 2, . . . , n,
define i := P[Ai | A1  · · ·  Ai-1] (assume that P[A1  · · ·  An-1] = 0). Show
that P[A1  · · ·  An] = 1 · · · n.

EXERCISE 8.6. Let B be an event, and let {Bi}iI be a finite, pairwise disjoint
family of events whose union is B. Generalizing the law of total probability
220  Finite and discrete probability distributions

(equations (8.9) and (8.10)), show that for every event A, we have P[A  B] =
  iI P[A  Bi], and if P[B] = 0 and I := {i  I : P[Bi] = 0}, then

                          P[A | B] P[B] = P[A | Bi] P[Bi].

                                                                 iI 

Also show that if P[A | Bi]   for each i  I, then P[A | B]  .

EXERCISE 8.7. Let B be an event with P[B] = 0, and let {Ci}iI be a finite, pair-
wise disjoint family of events whose union contains B. Again, generalizing the law
of total probability, show that for every event A, if I := {i  I : P[B  Ci] = 0},
then we have

                         P[A | B] = P[A | B  Ci] P[Ci | B].

                                                       iI 

EXERCISE 8.8. Three fair coins are tossed. Let A be the event that at least two
coins are heads. Let B be the event that the number of heads is odd. Let C be the
event that the third coin is heads. Are A and B independent? A and C? B and C?

EXERCISE 8.9. Consider again the situation in Example 8.11, but now suppose
that Alice only tells Bob the value of the sum of the two dice modulo 6. Describe
an optimal strategy for Bob, and calculate his overall probability of winning.

EXERCISE 8.10. Consider again the situation in Example 8.13, but now suppose
that the patient is visiting the doctor because he has symptom Y . Furthermore, it
is known that everyone who has disease X exhibits symptom Y , while 10% of the
population overall exhibits symptom Y . Assuming that the accuracy of the test
is not affected by the presence of symptom Y , how should the doctor advise his
patient should the test come out positive?

EXERCISE 8.11. This exercise develops an alternative proof, based on probability
theory, of Theorem 2.11. Let n be a positive integer and consider an experiment
in which a number a is chosen uniformly at random from {0, . . . , n - 1}. If
n = p1e1 · · · prer is the prime factorization of n, let Ai be the event that a is divisible
by pi, for i = 1, . . . , r.

    (a) Show that (n)/n = P[A1  · · ·  Ar], where  is Euler's phi function.

   (b) Show that if J  {1, . . . , r}, then

     P Aj = 1      pj .

         jJ    jJ

     Conclude that {Ai}ir=1 is mutually independent, and that P[Ai] = 1/pi for
     each i = 1, . . . , r.
8.3 Random variables                                                           221

(c) Using part (b), deduce that

                                                                            r

                             P[A1  · · ·  Ar] = (1 - 1/pi).

                                                                          i=1

(d) Combine parts (a) and (c) to derive the result of Theorem 2.11 that

                                                                   r

                                   (n) = n (1 - 1/pi).

                                                                 i=1

                                   8.3 Random variables

It is sometimes convenient to associate a real number, or other mathematical object,
with each outcome of a random experiment. The notion of a random variable
formalizes this idea.

   Let P be a probability distribution on a sample space . A random variable
X is a function X :   S, where S is some set, and we say that X takes values
in S. We do not require that the values taken by X are real numbers, but if this
is the case, we say that X is real valued. For s  S, "X = s" denotes the event
{   : X() = s}. It is immediate from this definition that

P[X = s] =             P().

            X-1 ({s})

More generally, for any predicate  on S, we may write "(X)" as shorthand for the
event {   : (X())}. When we speak of the image of X, we simply mean its
image in the usual function-theoretic sense, that is, the set X() = {X() :   }.
While a random variable is simply a function on the sample space, any discussion
of its properties always takes place relative to a particular probability distribution,
which may be implicit from context.

   One can easily combine random variables to define new random variables. Sup-
pose X1, . . . , Xn are random variables, where Xi :   Si for i = 1, . . . , n. Then
(X1, . . . , Xn) denotes the random variable that maps    to (X1(), . . . , Xn()) 
S1 ×· · ·×Sn. If f : S1 ×· · ·×Sn  T is a function, then f (X1, . . . , Xn) denotes the
random variable that maps    to f (X1(), . . . , Xn()). If f is applied using a
special notation, the same notation may be applied to denote the resulting random
variable; for example, if X and Y are random variables taking values in a set S,
and is a binary operation on S, then X Y denotes the random variable that maps
   to X() Y()  S.

   Let X be a random variable whose image is S. The variable X determines a
probability distribution PX : S  [0, 1] on the set S, where PX(s) := P[X = s] for
222  Finite and discrete probability distributions

each s  S. We call PX the distribution of X. If PX is the uniform distribution on

S, then we say that X is uniformly distributed over S.
   Suppose X and Y are random variables that take values in a set S. If P[X = s] =

P[Y = s] for all s  S, then the distributions of X and Y are essentially equal even

if their images are not identical.

Example 8.16. Again suppose we roll two dice, and model this experiment as the
uniform distribution on  := {1, . . . , 6} × {1, . . . , 6}. We can define the random
variable X that takes the value of the first die, and the random variable Y that takes
the value of the second; formally, X and Y are functions on , where

                       X(s, t) := s and Y(s, t) := t for (s, t)  .

For each value s  {1, . . . , 6}, the event X = s is {(s, 1), . . . , (s, 6)}, and so
P[X = s] = 6/36 = 1/6. Thus, X is uniformly distributed over {1, . . . , 6}. Like-
wise, Y is uniformly distributed over {1, . . . , 6}, and the random variable (X, Y) is
uniformly distributed over . We can also define the random variable Z := X + Y,
which formally is the function on the sample space defined by

                               Z(s, t) := s + t for (s, t)  .

The image of Z is {2, . . . , 12}, and its distribution is given by the following table:

               u 2 3 4 5 6 7 8 9 10 11 12
     P[Z = u] 1/36 2/36 3/36 4/36 5/36 6/36 5/36 4/36 3/36 2/36 1/36 . 2

Example 8.17. If A is an event, we may define a random variable X as follows:
X := 1 if the event A occurs, and X := 0 otherwise. The variable X is called the

indicator variable for A. Formally, X is the function that maps   A to 1, and

  \A to 0; that is, X is simply the characteristic function of A. The distribution
of X is that of a Bernoulli trial: P[X = 1] = P[A] and P[X = 0] = 1 - P[A].

   It is not hard to see that 1 - X is the indicator variable for A. Now suppose B is

another event, with indicator variable Y. Then it is also not hard to see that XY is
the indicator variable for A  B, and that X + Y - XY is the indicator variable for
A  B; in particular, if A  B = , then X + Y is the indicator variable for A  B. 2

Example 8.18. Consider again Example 8.8, where we have a coin that comes up
heads with probability p, and tails with probability q := 1-p, and we toss it n times.
For each i = 1, . . . , n, let Ai be the event that the ith toss comes up heads, and let
Xi be the corresponding indicator variable. Let us also define X := X1 + · · · + Xn,

which represents the total number of tosses that come up heads. The image of X
is {0, . . . , n}. By the calculations made in Example 8.8, for each k = 0, . . . , n, we
                          8.3 Random variables                  223

have
                                  P[X = k] = n pkqn-k.
                                                   k

The distribution of the random variable X is called a binomial distribution. Such
a distribution is parameterized by the success probability p of the underlying
Bernoulli trial, and by the number of times n the trial is repeated. 2

   Uniform distributions are very nice, simple distributions. It is therefore good to
have simple criteria that ensure that certain random variables have uniform distri-
butions. The next theorem provides one such criterion. We need a definition: if S
and T are finite sets, then we say that a given function f : S  T is a regular
function if every element in the image of f has the same number of pre-images
under f.

Theorem 8.4. Suppose f : S  T is a surjective, regular function, and that X
is a random variable that is uniformly distributed over S. Then f (X) is uniformly
distributed over T .

Proof. The assumption that f is surjective and regular implies that for every t  T ,
the set St := f -1({t}) has size |S|/|T |. So, for each t  T , working directly from

the definitions, we have

P[f (X) = t] =             P() =                P() = P[X = s]

                X-1 (St )         sSt X-1({s})  sSt

= 1/|S| = (|S|/|T |)/|S| = 1/|T |. 2

                sSt

As a corollary, we have:

Theorem 8.5. Suppose that  : G  G is a surjective homomorphism of finite
abelian groups G and G , and that X is a random variable that is uniformly dis-
tributed over G. Then (X) is uniformly distributed over G .

Proof. It suffices to show that  is regular. Recall that the kernel K of  is a
subgroup of G, and that for every g  G , the set -1({g }) is a coset of K (see
Theorem 6.19); moreover, every coset of K has the same size (see Theorem 6.14).
These facts imply that  is regular. 2

Example 8.19. Let us continue with Example 8.16. Recall that for a given integer
a, and positive integer n, [a]n  Zn denotes the residue class of a modulo n. Let
us define X := [X]6 and Y := [Y]6. It is not hard to see that both X and Y are
uniformly distributed over Z6, while (X , Y ) is uniformly distributed over Z6 × Z6.
Let us define Z := X + Y (where addition here is in Z6). We claim that Z is
224  Finite and discrete probability distributions

uniformly distributed over Z6. This follows immediately from the fact that the map
that sends (a, b)  Z6 × Z6 to a + b  Z6 is a surjective group homomorphism (see
Example 6.45). Further, we claim that (X , Z ) is uniformly distributed over Z6×Z6.
This follows immediately from the fact that the map that sends (a, b)  Z6 × Z6
to (a, a + b)  Z6 × Z6 is a surjective group homomorphism (indeed, it is a group
isomorphism). 2

   Let X be a random variable whose image is S. Let B be an event with P[B] = 0.
The conditional distribution of X given B is defined to be the distribution of X rel-
ative to the conditional distribution P(·|B), that is, the distribution PX|B : S  [0, 1]
defined by PX|B(s) := P[X = s | B] for s  S.

   Suppose X and Y are random variables, with images S and T , respectively. We
say X and Y are independent if for all s  S and all t  T , the events X = s and
Y = t are independent, which is to say,

                        P[(X = s)  (Y = t)] = P[X = s] P[Y = t].

Equivalently, X and Y are independent if and only if the distribution of (X, Y) is
essentially equal to the product of the distribution of X and the distribution of Y. As
a special case, if X is uniformly distributed over S, and Y is uniformly distributed
over T , then X and Y are independent if and only if (X, Y) is uniformly distributed
over S × T .

   Independence can also be characterized in terms of conditional probabilities.
From the definitions, it is immediate that X and Y are independent if and only if for
all values t taken by Y with non-zero probability, we have

                                P[X = s | Y = t] = P[X = s]

for all s  S; that is, the conditional distribution of X given Y = t is the same
as the distribution of X. From this point of view, an intuitive interpretation of
independence is that information about the value of one random variable does not
reveal any information about the value of the other.

Example 8.20. Let us continue with Examples 8.16 and 8.19. The random vari-

ables X and Y are independent: each is uniformly distributed over {1, . . . , 6}, and

(X, Y) is uniformly distributed over {1, . . . , 6} × {1, . . . , 6}. Let us calculate the
conditional distribution of X given Z = 4. We have P[X = s | Z = 4] = 1/3
for s = 1, 2, 3, and P[X = s | Z = 4] = 0 for s = 4, 5, 6. Thus, the con-
ditional distribution of X given Z = 4 is essentially the uniform distribution on
{1, 2, 3}. Let us calculate the conditional distribution of Z given X = 1. We have
P[Z = u | X = 1] = 1/6 for u = 2, . . . , 7, and P[Z = u | X = 1] = 0 for
u = 8, . . . , 12. Thus, the conditional distribution of Z given X = 1 is essentially

the uniform distribution on {2, . . . , 7}. In particular, it is clear that X and Z are
    8.3 Random variables                                         225

not independent. The random variables X and Y are independent, as are X and

Z : each of X , Y , and Z is uniformly distributed over Z6, and each of (X , Y ) and
(X , Z ) is uniformly distributed over Z6 × Z6. 2

   We now generalize the notion of independence to families of random variables.

Let {Xi}iI be a finite family of random variables. Let us call a corresponding
family of values {si}iI an assignment to {Xi}iI if si is in the image of Xi for
each i  I. For a given positive integer k, we say that the family {Xi}iI is k-
wise independent if for every assignment {si}iI to {Xi}iI , the family of events
{Xi = si}iI is k-wise independent.

   The notions of pairwise and mutual independence for random variables are

defined following the same pattern that was used for events. The family {Xi}iI is
called pairwise independent if it is 2-wise independent, which means that for all
i, j  I with i = j, the variables Xi and Xj are independent. The family {Xi}iI is
called mutually independent if it is k-wise independent for all positive integers

k. Equivalently, and more explicitly, mutual independence means that for every

assignment {si}iI to {Xi}iI , we have

P (Xj = sj) = P[Xj = sj] for all J  I.                           (8.13)

jJ  jJ

If n := |I| > 0, mutual independence is equivalent to n-wise independence; more-

over, if 0 < k  n, then {Xi}iI is k-wise independent if and only if {Xj}jJ is
mutually independent for every J  I with |J | = k.

Example 8.21. Returning again to Examples 8.16, 8.19, and 8.20, we see that
the family of random variables X , Y , Z is pairwise independent, but not mutually
independent; for example,

                   P (X = [0]6)  (Y = [0]6)  (Z = [0]6) = 1/62,

but
                 P X = [0]6 · P Y = [0]6 · P Z = [0]6 = 1/63. 2

Example 8.22. Suppose {Ai}iI is a finite family of events. Let {Xi}iI be the
corresponding family of indicator variables, so that for each i  I, Xi = 1 if Ai
occurs, and Xi = 0, otherwise. Theorem 8.3 immediately implies that for every

positive integer k, {Ai}iI is k-wise independent if and only if {Xi}iI is k-wise

independent. 2

Example 8.23. Consider again Example 8.15, where we toss a fair coin 3 times.
For i = 1, 2, 3, let Xi be the indicator variable for the event Ai that the ith toss
comes up heads. Then {Xi}i3=1 is a mutually independent family of random vari-
ables. Let Y12 be the indicator variable for the event B12 that tosses 1 and 2 agree;
226      Finite and discrete probability distributions

similarly, let Y13 be the indicator variable for the event B13, and Y23 the indicator
variable for B23. Then the family of random variables Y12, Y13, Y23 is pairwise
independent, but not mutually independent. 2

We next present a number of useful tools for establishing independence.

Theorem 8.6. Let X be a random variable with image S, and Y be a random

variable with image T . Further, suppose that f : S  [0, 1] and g : T  [0, 1]

are functions such that

                             f (s) = g(t) = 1,                           (8.14)

                         sS  tT

and that for all s  S and t  T , we have

                         P[(X = s)  (Y = t)] = f (s)g(t).                (8.15)

Then X and Y are independent, the distribution of X is f , and the distribution of
Y is g.

Proof. Since {Y = t}tT is a partition of the sample space, making use of total
probability (8.9), along with (8.15) and (8.14), we see that for all s  S, we have

P[X = s] = P[(X = s)  (Y = t)] = f (s)g(t) = f (s) g(t) = f (s).

     tT                                   tT               tT

Thus, the distribution of X is indeed f . Exchanging the roles of X and Y in the
above argument, we see that the distribution of Y is g. Combining this with (8.15),
we see that X and Y are independent. 2

   The generalization of Theorem 8.6 to families of random variables is a bit messy,
but the basic idea is the same:

Theorem 8.7. Let {Xi}iI be a finite family of random variables, where each Xi

has image Si. Also, let {fi}iI be a family of functions, where for each i  I,
fi : Si  [0, 1] and siSi fi(si) = 1. Further, suppose that

                         P (Xi = si) = fi(si)

                         iI                   iI

for each assignment {si}iI to {Xi}iI . Then the family {Xi}iI is mutually inde-
pendent, and for each i  I, the distribution of Xi is fi.

Proof. To prove the theorem, it suffices to prove the following statement: for every
subset J of I, and every assignment {sj}jJ to {Xj}jJ , we have

                         P (Xj = sj) = fj (sj).

                         jJ                   jJ
                 8.3 Random variables                                       227

Moreover, it suffices to prove this statement for the case where J = I \ {d}, for

an arbitrary d  I: this allows us to eliminate any one variable from the family,
without affecting the hypotheses, and by repeating this procedure, we can eliminate

any number of variables.
   Thus, let d  I be fixed, let J := I \ {d}, and let {sj}jJ be a fixed assignment

to {Xj}jJ . Then, since {Xd = sd}sdSd is a partition of the sample space, we have

P (Xj = sj) = P                                 (Xi = si) =    P (Xi = si)

jJ               sdSd iI                                     sdSd iI

=            fi(si) = fj (sj) · fd(sd) = fj (sj). 2

    sdSd iI      jJ                             sdSd         jJ

   This theorem has several immediate consequences. First of all, mutual inde-
pendence may be more simply characterized:

Theorem 8.8. Let {Xi}iI be a finite family of random variables. Suppose that for
every assignment {si}iI to {Xi}iI , we have

                             P (Xi = si)        = P[Xi = si].

                                            iI      iI

Then {Xi}iI is mutually independent.

   Theorem 8.8 says that to check for mutual independence, we only have to con-
sider the index set J = I in (8.13). Put another way, it says that a family of
random variables {Xi}in=1 is mutually independent if and only if the distribution of
(X1, . . . , Xn) is essentially equal to the product of the distributions of the individual
Xi's.

   Based on the definition of mutual independence, and its characterization in The-

orem 8.8, the following is also immediate:

Theorem 8.9. Suppose {Xi}in=1 is a family of random variables, and that m is an
integer with 0 < m < n. Then the following are equivalent:

    (i) {Xi}in=1 is mutually independent;
   (ii) {Xi}im=1 is mutually independent, {Xi}in=m+1 is mutually independent, and

         the two variables (X1, . . . , Xm) and (Xm+1, . . . , Xn) are independent.

   The following is also an immediate consequence of Theorem 8.7 (it also follows
easily from Theorem 8.4).

Theorem 8.10. Suppose that X1, . . . , Xn are random variables, and that S1, . . . , Sn
are finite sets. Then the following are equivalent:

    (i) (X1, . . . , Xn) is uniformly distributed over S1 × · · · × Sn;
228      Finite and discrete probability distributions

(ii) {Xi}in=1 is mutually independent, with each Xi uniformly distributed over
     Si.

Another immediate consequence of Theorem 8.7 is the following:

Theorem 8.11. Suppose P is the product distribution P1 · · · Pn, where each Pi

is a probability distribution on a sample space i, so that the sample space of P
is  = 1 × · · · × n. For each i = 1, . . . , n, let Xi be the random variable
that projects on the ith coordinate, so that Xi(1, . . . , n) = i. Then {Xi}in=1 is
mutually independent, and for each i = 1, . . . , n, the distribution of Xi is Pi.

   Theorem 8.11 is often used to synthesize independent random variables "out
of thin air," by taking the product of appropriate probability distributions. Other
arguments may then be used to prove the independence of variables derived from
these.

Example 8.24. Theorem 8.11 immediately implies that in Example 8.18, the fam-
ily of indicator variables {Xi}in=1 is mutually independent. 2

The following theorem gives us yet another way to establish independence.

Theorem 8.12. Suppose {Xi}in=1 is a mutually independent family of random vari-
ables. Further, suppose that for i = 1, . . . , n, Yi := gi(Xi) for some function gi.
Then {Yi}in=1 is mutually independent.

Proof. It suffices to prove the theorem for n = 2. The general case follows easily
by induction, using Theorem 8.9. For i = 1, 2, let ti be any value in the image of
Yi, and let Si := gi-1({ti}). We have

P[(Y1 = t1)  (Y2 = t2)] = P        (X1 = s1)         (X2 = s2)

                             s1S1              s2S2

     =P             (X1 = s1)  (X2 = s2)

         s1S1 s2S2

     =             P[(X1 = s1)  (X2 = s2)]

        s1S1 s2S2

     =             P[X1 = s1] P[X2 = s2]

        s1S1 s2S2

     =         P[X1 = s1]          P[X2 = s2]

         s1S1                s2S2

     =P        (X1 = s1) P         (X2 = s2) = P[Y1 = t1] P[Y2 = t2]. 2

         s1S1                s2S2
    8.3 Random variables                                             229

   As a special case of the above theorem, if each gi is the characteristic function
for some subset Si of the image of Xi, then X1  S1, . . . , Xn  Sn form a mutually
independent family of events.

   The next theorem is quite handy in proving the independence of random vari-

ables in a variety of algebraic settings.

Theorem 8.13. Suppose that G is a finite abelian group, and that W is a random
variable uniformly distributed over G. Let Z be another random variable, tak-
ing values in some finite set U , and suppose that W and Z are independent. Let
 : U  G be some function, and define Y := W + (Z). Then Y is uniformly
distributed over G, and Y and Z are independent.

Proof. Consider any fixed values t  G and u  U . Evidently, the events
(Y = t)  (Z = u) and (W = t - (u))  (Z = u) are the same, and therefore,

because W and Z are independent, we have

P[(Y = t)  (Z = u)] = P[W = t - (u)] P[Z = u] = 1 P[Z = u].          (8.16)
                                                                |G|

Since this holds for every u  U , making use of total probability (8.9), we have

P[Y = t] = P[(Y = t)  (Z = u)] = 1 P[Z = u] = 1 .
    |G|                       |G|
uU                        uU

Thus, Y is uniformly distributed over G, and by (8.16), Y and Z are independent.
(This conclusion could also have been deduced directly from (8.16) using Theo-
rem 8.6--we have repeated the argument here.) 2

   Note that in the above theorem, we make no assumption about the distribution
of Z, or any properties of the function .

Example 8.25. Theorem 8.13 may be used to justify the security of the one-time
pad encryption scheme. Here, the variable W represents a random, secret key --
the "pad"--that is shared between Alice and Bob; U represents a space of possible
messages; Z represents a "message source," from which Alice draws her message
according to some distribution; finally, the function  : U  G represents some
invertible "encoding transformation" that maps messages into group elements.

   To encrypt a message drawn from the message source, Alice encodes the mes-
sage as a group element, and then adds the pad. The variable Y := W + (Z)
represents the resulting ciphertext. Since Z = -1(Y - W), when Bob receives the
ciphertext, he decrypts it by subtracting the pad, and converting the resulting group
element back into a message. Because the message source Z and ciphertext Y are
independent, an eavesdropping adversary who learns the value of Y does not learn
230  Finite and discrete probability distributions

anything about Alice's message: for any particular ciphertext t, the conditional
distribution of Z given Y = t is the same as the distribution of Z.

   The term "one time" comes from the fact that a given encryption key should

be used only once; otherwise, security may be compromised. Indeed, suppose the

key is used a second time, encrypting a message drawn from a second source Z .
The second ciphertext is represented by the random variable Y := W + (Z ). In

general, the random variables (Z, Z ) and (Y, Y ) will not be independent, since
Y - Y = (Z) - (Z ). To illustrate this more concretely, suppose Z is uniformly

distributed over a set of 1000 messages, Z is uniformly distributed over a set of

two messages, say, {u1, u2}, and that Z and Z are independent. Now, without
any further information about Z, an adversary would have at best a 1-in-a-1000
chance of guessing its value. However, if he sees that Y = t and Y = t , for

particular values t, t  G, then he has a 1-in-2-chance, since the value of Z is
equally likely to be one of just two messages, namely, u1 := -1(t - t + (u1)) and
u2 := -1(t - t + (u2)); more formally, the conditional distribution of Z given
(Y = t)  (Y = t ) is essentially the uniform distribution on {u1, u2}.

   In practice, it is convenient to define the group G to be the group of all bit

strings of some fixed length, with bit-wise exclusive-or as the group operation.

The encoding function  simply "serializes" a message as a bit string. 2

Example 8.26. Theorem 8.13 may also be used to justify a very simple type of
secret sharing. A colorful, if militaristic, motivating scenario is the following.
To launch a nuclear missile, two officers who carry special keys must insert their
keys simultaneously into the "authorization device" (at least, that is how it works in
Hollywood). In the digital version of this scenario, an authorization device contains
a secret, digital "launch code," and each officer holds a digital "share" of this code,
so that (i) individually, each share reveals no information about the launch code,
but (ii) collectively, the two shares may be combined in a simple way to derive the
launch code. Thus, to launch the missile, both officers must input their shares into
the authorization device; hardware in the authorization device combines the two
shares, and compares the resulting code against the launch code it stores -- if they
match, the missile flies.

   In the language of Theorem 8.13, the launch code is represented by the random
variable Z, and the two shares by W and Y := W + (Z), where (as in the previous
example)  : U  G is some simple, invertible encoding function. Because W and
Z are independent, information about the share W leaks no information about the
launch code Z; likewise, since Y and Z are independent, information about Y leaks
no information about Z. However, by combining both shares, the launch code is
easily constructed as Z = -1(Y - W). 2
8.3 Random variables                                                                  231

Example 8.27. Let k be a positive integer. This example shows how we can take a

mutually independent family of k random variables, and, from it, construct a much

larger, k-wise independent family of random variables.

Let p be a prime, with p  k. Let {Hi}ik=0-1 be a mutually independent fam-

ily of random variables, each of which is uniformly distributed over Zp. Let us
set H := (H0, . . . , Hk-1), which, by assumption, is uniformly distributed over

Zp×k. For each s  Zp, we define the function s : Zp×k  Zp as follows: for
r = (r0, . . . , rk-1)  Zp×k, s(r) :=  k-1
                                       i=0  risi;  that  is,  s (r )  is  the  value  obtained  by

evaluating the polynomial r0 + r1X + · · · + rk-1X k-1  Zp[X ] at the point s.
Each s  Zp defines a random variable s(H) = H0 + H1s + · · · + Hk-1sk-1. We

claim that the family of random variables {s(H)}sZp is k-wise independent, with

each individual s(H) uniformly distributed over Zp. By Theorem 8.10, it suffices

to show the following: for all distinct points s1, . . . , sk  Zp, the random variable
W := (s1 (H), . . . , sk (H)) is uniformly distributed over Zp×k. So let s1, . . . , sk be

fixed, distinct elements of Zp, and define the function

 : Z×k p  Z×k p                                                                       (8.17)
         r  (s1 (r), . . . , sk (r)).

Thus, W = (H), and by Lagrange interpolation (Theorem 7.15), the function  is
a bijection; moreover, since H is uniformly distributed over Zp×k, so is W.

   Of course, the field Zp may be replaced by an arbitrary finite field. 2

Example 8.28. Consider again the secret sharing scenario of Example 8.26. Sup-
pose at the critical moment, one of the officers is missing in action. The military

planners would perhaps like a more flexible secret sharing scheme; for example,
perhaps shares of the launch code should be distributed to three officers, in such a
way that no single officer can authorize a launch, but any two can. More generally,
for positive integers k and , with  k + 1, the scheme should distribute shares
among officers, so that no coalition of k (or fewer) officers can authorize a launch,
yet any coalition of k + 1 officers can. Using the construction of the previous

example, this is easily achieved, as follows.

   Let us model the secret launch code as a random variable Z, taking values in

a finite set U . Assume that p is prime, with p  , and that  : U  Zp is
a simple, invertible encoding function. To construct the shares, we make use of

random variables H0, . . . , Hk-1, where each Hi is uniformly distributed over Zp,
and the family of random variables H0, . . . , Hk-1, Z is mutually independent. For
each s  Zp, we define the random variable

                      Ys := H0 + H1s + · · · + Hk-1sk-1 + (Z)sk.
232  Finite and discrete probability distributions

We can pick any subset S  Zp of size that we wish, so that for each s  S, an
officer gets the secret share Ys (along with the public value s).

   First, we show how any coalition of k+1 officers can reconstruct the launch code

from their collection of shares, say, Ys1, . . . , Ysk+1. This is easily done by means of
the Lagrange interpolation formula (again, Theorem 7.15). Indeed, we only need
to recover the high-order coefficient, (Z), which we can obtain via the formula

                  k+1     Ysi .
                       j=i(si - sj)
     (Z) =

                  i=1

Second, we show that no coalition of k officers learn anything about the launch

code, even if they pool their shares. Formally, this means that if s1, . . . , sk are

fixed, distinct points, then Ys1, . . . , Ysk , Z form a mutually independent family of
random variables. This is easily seen, as follows. Define H := (H0, . . . , Hk-1), and
W := (H), where  : Zp×k  Zp×k is as defined in (8.17), and set Y := (Ys1, . . . , Ysk ).

Now, by hypothesis, H and Z are independent, and H is uniformly distributed over
Zp×k. As we noted in Example 8.27,  is a bijection, and hence, W is uniformly
distributed over Zp×k; moreover (by Theorem 8.12), W and Z are independent.
Observe that Y = W +  (Z), where  maps u  U to ((u)sk1 , . . . , (u)skk)  Zp×k,
and so applying Theorem 8.13 (with the group Zp×k, the random variables W and

Z, and the function  ), we see that Y and Z are independent, where Y is uniformly
distributed over Zp×k. From this, it follows (using Theorems 8.9 and 8.10) that the

family of random variables Ys1, . . . , Ysk , Z is mutually independent, with each Ysi

uniformly distributed over Zp.
   Finally, we note that when k = 1, = 2, and S = {0, 1}, this construction

degenerates to the construction in Example 8.26 (with the additive group Zp). 2

EXERCISE 8.12. Suppose X and X are random variables that take values in a set
S and that have essentially the same distribution. Show that if f : S  T is a
function, then f (X) and f (X ) have essentially the same distribution.

EXERCISE 8.13. Let {Xi}in=1 be a family of random variables, and let Si be the
image of Xi for i = 1, . . . , n. Show that {Xi}in=1 is mutually independent if and only
if for each i = 2, . . . , n, and for all s1  S1, . . . , si  Si, we have

               P[Xi = si | (X1 = s1)  · · ·  (Xi-1 = si-1)] = P[Xi = si].

EXERCISE 8.14. Suppose that  : G  G is a surjective group homomor-
phism, where G and G are finite abelian groups. Show that if g , h  G , and
X and Y are independent random variables, where X is uniformly distributed over
-1({g }), and Y takes values in -1({h }), then X + Y is uniformly distributed over
-1({g + h }).
8.4 Expectation and variance  233

EXERCISE 8.15. Suppose X and Y are random variables, where X takes values in

S, and Y takes values in T . Further suppose that Y is uniformly distributed over

T , and that (X, Y) and Y are independent. Let  be a predicate on S × T . Show
that P[(X, Y)  (Y = Y )] = P[(X, Y)]/|T |.

EXERCISE 8.16. Let X and Y be independent random variables, where X is uni-

formly distributed over a set S, and Y is uniformly distributed over a set T  S.
Define a third random variable Z as follows: if X  T , then Z := X; otherwise,
Z := Y. Show that Z is uniformly distributed over T .

EXERCISE 8.17. Let n be a positive integer, and let X be a random variable, uni-
formly distributed over {0, . . . , n-1}. For each positive divisor d of n, let us define
the random variable Xd := X mod d. Show that:

    (a) if d is a divisor of n, then the variable Xd is uniformly distributed over
         {0, . . . , d - 1};

   (b) if d1, . . . , dk are divisors of n, then {Xdi}ik=1 is mutually independent if and
         only if {di}ik=1 is pairwise relatively prime.

EXERCISE 8.18. Suppose X and Y are random variables, each uniformly dis-
tributed over Z2, but not necessarily independent. Show that the distribution of
(X, Y) is the same as the distribution of (X + 1, Y + 1).

EXERCISE 8.19. Let I := {1, . . . , n}, where n  2, let B := {0, 1}, and let G be a
finite abelian group, with |G| > 1. Suppose that {Xib}(i,b)I×B is a mutually inde-
pendent family of random variables, each uniformly distributed over G. For each
 = (b1, . . . , bn)  B×n, let us define the random variable Y := X1b1 + · · · + Xnbn.
Show that each Y is uniformly distributed over G, and that {Y}B×n is 3-wise
independent, but not 4-wise independent.

8.4 Expectation and variance

Let P be a probability distribution on a sample space . If X is a real-valued
random variable, then its expected value, or expectation, is

E[X] := X() P().              (8.18)

              

If S is the image of X, and if for each s  S we group together the terms in (8.18)
with X() = s, then we see that

E[X] = s P[X = s].            (8.19)

             sS
234  Finite and discrete probability distributions

From (8.19), it is clear that E[X] depends only on the distribution of X: if X is

another random variable with the same (or essentially the same) distribution as X,
then E[X] = E[X ].

   More generally, suppose X is an arbitrary random variable (not necessarily real

valued) whose image is S, and f is a real-valued function on S. Then again, if for
each s  S we group together the terms in (8.18) with X() = s, we see that

       E[f (X)] = f (s) P[X = s].                                      (8.20)

           sS

   We make a few trivial observations about expectation, which the reader may
easily verify. First, if X is equal to a constant c (i.e., X() = c for every   ),
then E[X] = E[c] = c. Second, if X and Y are random variables such that X  Y

(i.e., X()  Y() for every   ), then E[X]  E[Y]. Similarly, if X > Y, then

E[X] > E[Y].

   In calculating expectations, one rarely makes direct use of (8.18), (8.19), or

(8.20), except in rather trivial situations. The next two theorems develop tools that
are often quite effective in calculating expectations.

Theorem 8.14 (Linearity of expectation). If X and Y are real-valued random
variables, and a is a real number, then

                     E[X + Y] = E[X] + E[Y] and E[aX] = a E[X].

Proof. It is easiest to prove this using the defining equation (8.18) for expectation.
For   , the value of the random variable X+Y at  is by definition X()+Y(),

and so we have

     E[X + Y] = (X() + Y()) P()

       

       = X() P() + Y() P()

                       

       = E[X] + E[Y].

For the second part of the theorem, by a similar calculation, we have

     E[aX] = (aX()) P() = a X() P() = a E[X]. 2

               

   More generally, the above theorem implies (using a simple induction argument)
that if {Xi}iI is a finite family of real-valued random variables, then we have

       E Xi = E[Xi].                                                   (8.21)

       iI      iI

   So we see that expectation is linear; however, expectation is not in general mul-
tiplicative, except in the case of independent random variables:
8.4 Expectation and variance                                              235

Theorem 8.15. If X and Y are independent, real-valued random variables, then
E[XY] = E[X] E[Y].

Proof. It is easiest to prove this using (8.20), with the function f (s, t) := st applied
to the random variable (X, Y). We have

E[XY] = st P[(X = s)  (Y = t)]

                s,t

        = st P[X = s] P[Y = t]

                s,t

= s P[X = s]      t P[Y = t]

        s       t

= E[X] E[Y]. 2

   More generally, the above theorem implies (using a simple induction argument)
that if {Xi}iI is a finite, mutually independent family of real-valued random vari-
ables, then

E Xi = E[Xi].                                                             (8.22)

iI   iI

   The following simple facts are also sometimes quite useful in calculating expec-
tations:

Theorem 8.16. Let X be a 0/1-valued random variable. Then E[X] = P[X = 1].

Proof. E[X] = 0 · P[X = 0] + 1 · P[X = 1] = P[X = 1]. 2

Theorem 8.17. If X is a random variable that takes only non-negative integer
values, then

                                     E[X] = P[X  i].

                                                                i1

Note that since X has a finite image, the sum appearing above is finite.

Proof. Suppose that the image of X is contained in {0, . . . , n}, and for i = 1, . . . , n,
let Xi be the indicator variable for the event X  i. Then X = X1 + · · · + Xn, and

by linearity of expectation and Theorem 8.16, we have

n    n

E[X] = E[Xi] = P[X  i]. 2

i=1  i=1

   Let X be a real-valued random variable with µ := E[X]. The variance of X is
Var[X] := E[(X -µ)2]. The variance provides a measure of the spread or dispersion

of the distribution of X around its expected value. Note that since (X - µ)2 takes

only non-negative values, variance is always non-negative.
236  Finite and discrete probability distributions

Theorem 8.18. Let X be a real-valued random variable, with µ := E[X], and let a
and b be real numbers. Then we have

    (i) Var[X] = E[X2] - µ2,
   (ii) Var[aX] = a2 Var[X], and
   (iii) Var[X + b] = Var[X].

Proof. For part (i), observe that

                Var[X] = E[(X - µ)2] = E[X2 - 2µX + µ2]
                         = E[X2] - 2µ E[X] + E[µ2] = E[X2] - 2µ2 + µ2
                         = E[X2] - µ2,

where in the third equality, we used the fact that expectation is linear, and in the
fourth equality, we used the fact that E[c] = c for constant c (in this case, c = µ2).

   For part (ii), observe that

                    Var[aX] = E[a2X2] - E[aX]2 = a2 E[X2] - (aµ)2
                              = a2(E[X2] - µ2) = a2 Var[X],

where we used part (i) in the first and fourth equality, and the linearity of expecta-
tion in the second.

   Part (iii) follows by a similar calculation:

                  Var[X + b] = E[(X + b)2] - (µ + b)2
                                = (E[X2] + 2bµ + b2) - (µ2 + 2bµ + b2)
                                = E[X2] - µ2 = Var[X]. 2

   The following is an immediate consequence of part (i) of Theorem 8.18, and the
fact that variance is always non-negative:

Theorem 8.19. If X is a real-valued random variable, then E[X2]  E[X]2.

   Unlike expectation, the variance of a sum of random variables is not equal to the
sum of the variances, unless the variables are pairwise independent:

Theorem 8.20. If {Xi}iI is a finite, pairwise independent family of real-valued
random variables, then

     Var Xi = Var[Xi].

     iI  iI
                          8.4 Expectation and variance                    237

Proof. We have

                =E        2              2

Var Xi                    Xi - E Xi

        iI          iI            iI

                = E[Xi2] + E[XiXj] - E[Xi] E[Xj]         - E[Xi]2

                iI        i,jI                               iI
                             i=j

                (by linearity of expectation and rearranging terms)

                = E[Xi2] - E[Xi]2

                iI        iI

                (by pairwise independence and Theorem 8.15)

                = Var[Xi]. 2

                    iI

Corresponding to Theorem 8.16, we have:

Theorem 8.21. Let X be a 0/1-valued random variable, with p := P[X = 1] and
q := P[X = 0] = 1 - p. Then Var[X] = pq.

Proof. We have E[X] = p and E[X2] = P[X2 = 1] = P[X = 1] = p. Therefore,

Var[X] = E[X2] - E[X]2 = p - p2 = p(1 - p) = pq. 2

   Let B be an event with P[B] = 0, and let X be a real-valued random variable.
We define the conditional expectation of X given B, denoted E[X | B], to be the
expected value of the X relative to the conditional distribution P(· | B), so that

E[X | B] = X() P( | B) = P[B]-1 X() P().

                                                      B

Analogous to (8.19), if S is the image of X, we have

                          E[X | B] = s P[X = s | B].                 (8.23)

                                             sS

Furthermore, suppose I is a finite index set, and {Bi}iI is a partition of the sample
space, where each Bi occurs with non-zero probability. If for each i  I we group
together the terms in (8.18) with   Bi, we obtain the law of total expectation:

                          E[X] = E[X | Bi] P[Bi].                    (8.24)

                                       iI

Example 8.29. Let X be uniformly distributed over {1, . . . , m}. Let us compute

E[X] and Var[X]. We have

                m E[X] = s · 1 = m(m + 1) · 1 = m + 1 .
                          s=1 m   2      m2
238           Finite and discrete probability distributions

We also have

     E[X2] = s2 m · 1 = m(m + 1)(2m + 1) · 1 = (m + 1)(2m + 1) .
              s=1 m  6  m                                     6

Therefore,

              Var[X] = E[X2] - E[X]2 = m . 2 2 - 1
                                                  12

Example 8.30. Let X denote the value of a roll of a die. Let A be the event that X
is even. Then the conditional distribution of X given A is essentially the uniform
distribution on {2, 4, 6}, and hence

                                 E[X | A] = 2 + 4 + 6 = 4.
                                                     3

Similarly, the conditional distribution of X given A is essentially the uniform dis-
tribution on {1, 3, 5}, and so

                                 E[X | A] = 1 + 3 + 5 = 3.
                                                     3

Using the law of total expectation, we can compute the expected value of X as
follows:

            E[X] = E[X | A] P[A] + E[X | A] P[A] = 4 · 1 + 3 · 1 = 7 ,
                           2                                  22

which agrees with the calculation in the previous example. 2

Example 8.31. Let X be a random variable with a binomial distribution, as in

Example 8.18, that counts the number of successes among n Bernoulli trials, each

of which succeeds with probability p. Let us compute E[X] and Var[X]. We can
write X as the sum of indicator variables, X = in=1 Xi, where Xi is the indicator
variable for the event that the ith trial succeeds; each Xi takes the value 1 with
probability p and 0 with probability q := 1 - p, and the family of random variables
{Xi}in=1 is mutually independent (see Example 8.24). By Theorems 8.16 and 8.21,
we have E[Xi] = p and Var[Xi] = pq for i = 1, . . . , n. By linearity of expectation,

we have

                                                               n

                                   E[X] = E[Xi] = np.

                                                              i=1

By Theorem 8.20, and the fact that {Xi}in=1 is mutually independent (and hence
pairwise independent), we have

                                                            n

                              Var[X] = Var[Xi] = npq. 2

                                                          i=1
                8.4 Expectation and variance                           239

Example 8.32. Our proof of Theorem 8.1 could be elegantly recast in terms of

indicator variables. For B  , let XB be the indicator variable for B, so that
XB() = [B] for each   . Equation (8.8) then becomes

                XA =        (-1)|J|-1XAJ ,

                       J I

and by Theorem 8.16 and linearity of expectation, we have

P[A] = E[XA] =        (-1)|J|-1 E[XAJ ] =        (-1)|J|-1 P[XAJ ]. 2

                 J I                        J I

EXERCISE 8.20. Suppose X is a real-valued random variable. Show that |E[X]| 
E[|X|]  E[X2]1/2.

EXERCISE 8.21. Suppose X and Y take non-negative real values, and that Y  c
for some constant c. Show that E[XY]  c E[X]

EXERCISE 8.22. Let X be a 0/1-valued random variable. Show that Var[X]  1/4.

EXERCISE 8.23. Let B be an event with P[B] = 0, and let {Bi}iI be a finite,
pairwise disjoint family of events whose union is B. Generalizing the law of
total expectation (8.24), show that for every real-valued random variable X, if
I := {i  I : P[Bi] = 0}, then we have

                           E[X | B] P[B] = E[X | Bi] P[Bi].

                                                                 iI 

Also show that if E[X | Bi]   for each i  I, then E[X | B]  .

EXERCISE 8.24. Let B be an event with P[B] = 0, and let {Ci}iI be a finite,
pairwise disjoint family of events whose union contains B. Again, generalizing
the law of total expectation, show that for every real-valued random variable X, if
I := {i  I : P[B  Ci] = 0}, then we have

                          E[X | B] = E[X | B  Ci] P[Ci | B].

                                                       iI 

EXERCISE 8.25. This exercise makes use of the notion of convexity (see §A8).
    (a) Prove Jensen's inequality: if f is convex on an interval, and X is a random
         variable taking values in that interval, then E[f (X)]  f (E[X]). Hint: use
         induction on the size of the image of X. (Note that Theorem 8.19 is a special
         case of this, with f (s) := s2.)

   (b) Using part (a), show that if X takes non-negative real values, and  is a
         positive number, then E[X]  E[X] if   1, and E[X]  E[X] if
           1.
240  Finite and discrete probability distributions

(c) Using part (a), show that if X takes positive real values, then E[X]  eE[log X].
(d) Using part (c), derive the arithmetic/geometric mean inequality: for all

     positive numbers x1, . . . , xn, we have

     (x1 + · · · + xn)/n  (x1 · · · xn)1/n.

EXERCISE 8.26. For real-valued random variables X and Y, their covariance is
defined as Cov[X, Y] := E[XY] - E[X] E[Y]. Show that:

    (a) if X, Y, and Z are real-valued random variables, and a is a real number, then
         Cov[X + Y, Z] = Cov[X, Z] + Cov[Y, Z] and Cov[aX, Z] = a Cov[X, Z];

   (b) if {Xi}iI is a finite family of real-valued random variables, then

     Var Xi                       = Var[Xi] + Cov[Xi, Xj].

             iI                   iI      i,jI
                                          i=j

EXERCISE 8.27. Let f : [0, 1]  R be a function that is "nice" in the following
sense: for some constant c, we have |f (s) - f (t)|  c|s - t| for all s, t  [0, 1]. This

condition is implied, for example, by the assumption that f has a derivative that

is bounded in absolute value by c on the interval [0, 1]. For each positive integer

n, define the polynomial Bn,f :=  n    n  f (k/n)T k(1 - T )n-k    R[T ].  Show
                                  k=0  k

that |Bn,f (p) - f (p)|  c/2 n for all positive integers n and all p  [0, 1]. Hint:

let X be a random variable with a binomial distribution that counts the number of

successes among n Bernoulli trials, each of which succeeds with probability p, and
begin by observing that Bn,f (p) = E[f (X/n)]. The polynomial Bn,f is called the

nth Bernstein approximation to f , and this result proves a classical result that

any "nice" function can approximated to arbitrary precision by a polynomial of
sufficiently high degree.

EXERCISE 8.28. Consider again the game played between Alice and Bob in
Example 8.11. Suppose that to play the game, Bob must place a one dollar bet.
However, after Alice reveals the sum of the two dice, Bob may elect to double his
bet. If Bob's guess is correct, Alice pays him his bet, and otherwise Bob pays Alice
his bet. Describe an optimal playing strategy for Bob, and calculate his expected
winnings.

EXERCISE 8.29. A die is rolled repeatedly until it comes up "1," or until it is rolled
n times (whichever comes first). What is the expected number of rolls of the die?
                   8.5 Some useful bounds     241

                                  8.5 Some useful bounds

In this section, we present several theorems that can be used to bound the prob-
ability that a random variable deviates from its expected value by some specified
amount.

Theorem 8.22 (Markov's inequality). Let X be a random variable that takes only
non-negative real values. Then for every  > 0, we have

                   P[X  ]  E[X]/.

Proof. We have

E[X] = s P[X = s] = s P[X = s] + s P[X = s],

                s  s<                      s

where the summations are over elements s in the image of X. Since X takes only
non-negative values, all of the terms are non-negative. Therefore,

E[X]  s P[X = s]   P[X = s] =  P[X  ]. 2

                s  s

   Markov's inequality may be the only game in town when nothing more about
the distribution of X is known besides its expected value. However, if the variance
of X is also known, then one can get a better bound.

Theorem 8.23 (Chebyshev's inequality). Let X be a real-valued random variable,
with µ := E[X] and  := Var[X]. Then for every  > 0, we have

                                   P[|X - µ|  ]  /2.

Proof. Let Y := (X - µ)2. Then Y is always non-negative, and E[Y] = . Applying
Markov's inequality to Y, we have

                         P[|X - µ|  ] = P[Y  2]  /2. 2

   An important special case of Chebyshev's inequality is the following. Suppose
that {Xi}iI is a finite, non-empty, pairwise independent family of real-valued ran-
dom variables, each with the same distribution. Let µ be the common value of
E[Xi],  be the common value of Var[Xi], and n := |I|. Set

                                         X := 1 Xi.
                                                n

                                                                     iI

The variable X is called the sample mean of {Xi}iI . By the linearity of expecta-
tion, we have E[X] = µ, and since {Xi}iI is pairwise independent, it follows from
242        Finite and discrete probability distributions

Theorem 8.20 (along with part (ii) of Theorem 8.18) that Var[X] = /n. Applying

Chebyshev's inequality, for every  > 0, we have

                                                                         (8.25)
                              P[|X - µ|  ]  n2 .

The inequality (8.25) says that for all  > 0, and for all  > 0, there exists n0
(depending on  and , as well as the variance ) such that n  n0 implies

                              P[|X - µ|  ]  .                            (8.26)

In words:

     As n gets large, the sample mean closely approximates the expected
     value µ with high probability.

This fact, known as the law of large numbers, justifies the usual intuitive interpre-

tation given to expectation.

Let us now examine an even more specialized case of the above situation, where

each Xi is a 0/1-valued random variable, taking the value 1 with probability p, and
0 with probability q := 1 - p. By Theorems 8.16 and 8.21, the Xi's have a common

expected value p and variance pq. Therefore, by (8.25), for every  > 0, we have

                                                    pq                   (8.27)
                              P[|X - p|  ]  n2 .

The bound on the right-hand side of (8.27) decreases linearly in n. If one makes

the stronger assumption that the family {Xi}iI is mutually independent (so that
X := i Xi has a binomial distribution), one can obtain a much better bound that

decreases exponentially in n:

Theorem 8.24 (Chernoff bound). Let {Xi}iI be a finite, non-empty, and mutu-

ally independent family of random variables, such that each Xi is 1 with probability
p and 0 with probability q := 1 - p. Assume that 0 < p < 1. Also, let n := |I| and

X be the sample mean of {Xi}iI . Then for every  > 0, we have:
    (i) P[X - p  ]  e-n2 /2q;
   (ii) P[X - p  -]  e-n2 /2p;

   (iii) P[|X - p|  ]  2e-n2/2.

Proof. First, we observe that (ii) follows directly from (i) by replacing Xi by 1 - Xi

and exchanging the roles of p and q. Second, we observe that (iii) follows directly
from (i) and (ii). Thus, it suffices to prove (i).

   Let  > 0 be a parameter, whose value will be determined later. Define the
random variable Z := en(X-p). Since the function x  enx is strictly increasing,
we have X - p   if and only if Z  en. By Markov's inequality, it follows that

           P[X - p  ] = P[Z  en]  E[Z]e-n.                               (8.28)
                                8.5 Some useful bounds                   243

So our goal is to bound E[Z] from above.
   For each i  I, define the random variable Zi := e(Xi-p). Observe that

Z = iI Zi, that {Zi}iI is a mutually independent family of random variables
(see Theorem 8.12), and that for each i  I, we have

                       E[Zi] = e(1-p)p + e(0-p)q = peq + qe-p.

It follows that

                 E[Z] = E Zi = E[Zi] = (peq + qe-p)n.

                            iI  iI

We will prove below that

                                peq + qe-p  e2q/2.      (8.29)

From this, it follows that

                                E[Z]  e2qn/2.           (8.30)

Combining (8.30) with (8.28), we obtain                 (8.31)
                                 P[X - p  ]  e2qn/2-n.

Now we choose the parameter  so as to minimize the quantity 2qn/2 - n. The
optimal value of  is easily seen to be  = /q, and substituting this value of  into

(8.31) yields (i).

   To finish the proof of the theorem, it remains to prove the inequality (8.29). Let

                                 := peq + qe-p.

We want to show that   e2q/2, or equivalently, that log   2q/2. We have

                  = eq (p + qe-) = eq (1 - q(1 - e-)),

and taking logarithms and applying parts (i) and (ii) of §A1, we obtain
 log  = q + log(1 - q(1 - e-))  q - q(1 - e-) = q(e- +  - 1)  q2/2.

This establishes (8.29) and completes the proof of the theorem. 2
   Thus, the Chernoff bound is a quantitatively superior version of the law of large

numbers, although its range of application is clearly more limited.

Example 8.33. Suppose we toss a fair coin 10,000 times. The expected number
of heads is 5,000. What is an upper bound on the probability  that we get 6,000
or more heads? Using Markov's inequality, we get   5/6. Using Chebyshev's
inequality, and in particular, the inequality (8.27), we get

                                      10410-2 1/4 = 1400 .
244      Finite and discrete probability distributions

Finally, using the Chernoff bound, we obtain

           e-10410-2 /2(0.5) = e-100  10-43.4. 2

EXERCISE 8.30. With notation and assumptions as in Theorem 8.24, and with

p := q := 1/2, show that there exist constants c1 and c2 such that

                                                        
         P[|X - 1/2|  c1/ n]  1/2 and P[|X - 1/2|  c2/ n]  1/2.

Hint: for the second inequality, use Exercise 5.16.

EXERCISE 8.31. In each step of a random walk, we toss a coin, and move either

one unit to the right, or one unit to the left, depending on the outcome of the

coin toss. The question is, after n steps, what is our expected distance from the

starting point? Let us model this using a mutually independent family of ran-

dom variables {Yi}in=1, with each Yi uniformly distributed over {-1, 1}, and define
                                                     
Y := Y1 + · · · + Yn. Show that the c1 n  E[|Y|]  c2 n, for some constants c1

and c2.

EXERCISE 8.32. The goal of this exercise is to prove that with probability very
close to 1, a random number between 1 and m has very close to log log m prime
factors. To prove this result, you will need to use appropriate theorems from Chap-
ter 5. Suppose N is a random variable that is uniformly distributed over {1, . . . , m},
where m  3. For i = 1, . . . , m, let Di be the indicator variable for the event that i
divides N. Also, define X := pm Dp, where the sum is over all primes p  m, so
that X counts the number of distinct primes dividing N. Show that:

    (a) 1/i - 1/m < E[Di]  1/i, for each i = 1, . . . , m;

   (b) |E[X] - log log m|  c1 for some constant c1;
    (c) for all primes p, q, where p  m, q  m, and p = q, we have

                                     Cov[Dp, Dq]  1 1 + 1 ,
                                                        mp q

         where Cov is the covariance, as defined in Exercise 8.26;
   (d) Var[X]  log log m + c2 for some constant c2;

    (e) for some constant c3, and for every   1, we have

            P |X - log log m|  (log log m)1/2  -2 1 + c3(log log m)-1/2 .

EXERCISE 8.33. For each positive integer n, let (n) denote the number of positive

divisors of n. Suppose that N is uniformly distributed over {1, . . . , m}. Show that
E[(N)] = log m + O(1).
8.6 Balls and bins  245

EXERCISE 8.34. You are given three biased coins, where for i = 1, 2, 3, coin i

comes up heads with probability pi. The coins look identical, and all you know is
the following: (1) |p1 - p2| > 0.01 and (2) either p3 = p1 or p3 = p2. Your goal
is to determine whether p3 is equal to p1, or to p2. Design a random experiment
to determine this. The experiment may produce an incorrect result, but this should
happen with probability at most 10-12. Try to use a reasonable number of coin

tosses.

EXERCISE 8.35. Consider the following game, parameterized by a positive integer
n. One rolls a pair of dice, and records the value of their sum. This is repeated until
some value is recorded n times, and this value is declared the "winner." It is
intuitively clear that 7 is the most likely winner. Let n be the probability that 7
does not win. Give a careful argument that n  0 as n  . Assume that the
rolls of the dice are mutually independent.

                                      8.6 Balls and bins

This section and the next discuss applications of the theory developed so far.
   Our first application is a brief study of "balls and bins." Suppose you throw n

balls into m bins. A number of questions naturally arise, such as:

     · What is the probability that a collision occurs, that is, two balls land in the
         same bin?

     · What is the expected value of the maximum number of balls that land in
         any one bin?

   To formalize these questions, we introduce some notation that will be used
throughout this section. Let I be a finite set of size n > 0, and S a finite set
of size m > 0. Let {Xi}iI be a family of random variables, where each Xi is
uniformly distributed over the set S. The idea is that I represents a set of labels
for our n balls, S represents the set of m bins, and Xi represents the bin into which
ball i lands.

   We define C to be the event that a collision occurs; formally, this is the event that
Xi = Xj for some i, j  I with i = j. We also define M to be the random variable
that measures that maximum number of balls in any one bin; formally,

                                   M := max{Ns : s  S},

where for each s  S, Ns is the number of balls that land in bin s; that is,

                                  Ns := |{i  I : Xi = s}|.

   The questions posed above can now be stated as the problems of estimating P[C]
246         Finite and discrete probability distributions

and E[M]. However, to estimate these quantities, we have to make some assump-
tions about the independence of the Xi's. While it is natural to assume that the
family of random variables {Xi}iI is mutually independent, it is also interesting
and useful to estimate these quantities under weaker independence assumptions.
We shall therefore begin with an analysis under the weaker assumption that {Xi}iI
is pairwise independent. We start with a simple observation:

Theorem 8.25. Suppose {Xi}iI is pairwise independent. Then for all i, j  I
with i = j, we have P[Xi = Xj] = 1/m.

Proof. The event Xi = Xj occurs if and only if Xi = s and Xj = s for some s  S.
Therefore,

        P[Xi = Xj] = P[(Xi = s)  (Xj = s)] (by Boole's equality (8.7))

                                  sS

                      = 1/m2 (by pairwise independence)

                                  sS

                      = 1/m. 2

Theorem 8.26. Suppose {Xi}iI is pairwise independent. Then

                        P[C]          n(n - 1)    .

                                      2m

Proof. Let I (2) := {J  I : |J | = 2}. Then using Boole's inequality (8.6) and
Theorem 8.25, we have

     P[C]               P[Xi = Xj] =              1 = |I (2)| = n(n - 1) . 2
                                                  mm       2m
            {i,j}I (2)                {i,j}I (2)

Theorem 8.27. Suppose {Xi}iI is pairwise independent. Then

                        E[M]  n2/m + n.

Proof. To prove this, we use the fact that E[M]2  E[M2] (see Theorem 8.19), and
that M2  Z := sS N2s. It will therefore suffice to show that

                        E[Z]  n2/m + n.                                       (8.32)

To this end, for i  I and s  S, let Lis be the indicator variable for the event that
ball i lands in bin s (i.e., Xi = s), and for i, j  I, let Cij be the indicator variable
for the event that balls i and j land in the same bin (i.e., Xi = Xj). Observing that
                                  8.6 Balls and bins                                247

Cij = sS LisLjs, we have

Z = Ns2 = Lis 2 = Lis                                     Ljs  =           Lis Ljs

sS      sS iI                     sS iI               jI          i,jI sS

= Cij.

i,jI

For i, j  I, we have E[Cij] = P[Xi = Xj] (see Theorem 8.16), and so by The-
orem 8.25, we have E[Cij] = 1/m if i = j, and clearly, E[Cij] = 1 if i = j. By

linearity of expectation, we have

E[Z] = E[Cij] = E[Cij] + E[Cii] = n(n - 1) + n  n2/m + n,
                                                      m
i,jI                    i,jI      iI
                        i=j

which proves (8.32). 2

   We next consider the situation where {Xi}iI is mutually independent. Of
course, Theorem 8.26 is still valid in this case, but with our stronger assumption,
we can derive a lower bound on P[C].

Theorem 8.28. Suppose {Xi}iI is mutually independent. Then

                             P[C]  1 - e-n(n-1)/2m.

Proof. Let  := P[C]. We want to show   e-n(n-1)/2m. We may assume that
I = {1, . . . , n} (the labels make no difference) and that n  m (otherwise,  = 0).

Under the hypothesis of the theorem, the random variable (X1, . . . , Xn) is uniformly
distributed over S×n. Among all mn sequences (s1, . . . , sn)  S×n, there are a total
of m(m - 1) · · · (m - n + 1) that contain no repetitions: there are m choices for s1,

and for any fixed value of s1, there are m - 1 choices for s2, and so on. Therefore

 = m(m - 1) · · · (m - n + 1)/mn = 1 - 1 1 - 2 · · · 1 - n - 1 .
                                         m            m                    m

Using part (i) of §A1, we obtain

                          e- ni=-11 i/m = e-n(n-1) /2m. 2

   Theorem 8.26 implies that if n(n - 1)  m, then the probability of a collision is

at most 1/2; moreover, Theorem 8.28 implies that if n(n - 1)  (2 log 2)m, then
                                                                           

the probability of a collision is at least 1/2. Thus, for n near m, the probability

of a collision is roughly 1/2. A colorful illustration of this is the following fact: in

a room with 23 or more people, the odds are better than even that two people in the
room have birthdays on the same day of the year. This follows by setting n = 23
and m = 365 in Theorem 8.28. Here, we are ignoring leap years, and the fact that
248            Finite and discrete probability distributions

birthdays are not uniformly distributed over the calendar year (however, any skew
in the birthday distribution only increases the odds that two people share the same
birthday -- see Exercise 8.40 below). Because of this fact, Theorem 8.28 is often
called the birthday paradox (the "paradox" being the perhaps surprisingly small
number of people in the room).

   The hypothesis that {Xi}iI is mutually independent is crucial in Theorem 8.28.
Indeed, assuming just pairwise independence, we may have P[C] = 1/m, even
when n = m (see Exercise 8.42 below). However, useful, non-trivial lower bounds
on P[C] can still be obtained under assumptions weaker than mutual independence
(see Exercise 8.43 below).

   Assuming {Xi}iI is mutually independent, we can get a much sharper upper
bound on E[M] than that provided by Theorem 8.27. For simplicity, we only
consider the case where m = n; in this case, Theorem 8.27 gives us the bound
E[M]  2n (which cannot be substantially improved assuming only pairwise
independence--see Exercise 8.44 below).

Theorem 8.29. Suppose {Xi}iI is mutually independent and that m = n. Then

                                E[M]  (1 + o(1)) log n .
                                                       log log n

Proof. We use Theorem 8.17, which says that E[M] = k1 P[M  k].
   Claim 1. For k  1, we have P[M  k]  n/k!.
   To prove Claim 1, we may assume that k  n (as otherwise, P[M  k] = 0).

Let I (k) := {J  I : |J | = k}. Now, M  k if and only if there is an s  S and a
subset J  I (k), such that Xj = s for all j  J . Therefore,

     P[M  k]   P (Xj = s) (by Boole's inequality (8.6))

               sS J I (k) jJ

     =                        P[Xj = s] (by mutual independence)

               sS J I (k) jJ

     = n n n-k  n/k!.
           k

That proves Claim 1.
   Of course, Claim 1 is only interesting when n/k!  1, since P[M  k] is always

at most 1. Define F (n) to be the smallest positive integer k such that k!  n.
   Claim 2. F (n)  log n/ log log n.
   To prove this, let us set k := F (n). It is clear that n  k!  nk, and taking
                               8.6 Balls and bins                              249

logarithms, log n  log k!  log n + log k. Moreover, we have

k                 k

log k! = log = log x dx + O(log k) = k log k - k + O(log k)  k log k,

=1                1

where we have estimated the sum by an integral (see §A5). Thus,

                  log n = log k! + O(log k)  k log k.

Taking logarithms again, we see that
                       log log n = log k + log log k + o(1)  log k,

and so log n  k log k  k log log n, from which Claim 2 follows.
   Finally, observe that each term in the sequence {n/k!}k=1 is at most half the

previous term. Combining this observation with Claims 1 and 2, and the fact that

P[M  k] is always at most 1, we have

E[M] = P[M  k] =                          P[M  k] +                 P[M  k]

    k1                         kF (n)                      k>F (n)

     F (n) + 2- = F (n) + 1  log n/ log log n. 2

                       1

EXERCISE 8.36. Let 1, . . . , m be real numbers that sum to 1. Show that 0 

sm=1(s - 1/m)2 =  m    2    -  1/m,  and  in  particular,    m    2      1/m.
                  s=1                                        s=1
                         s                                          s

EXERCISE 8.37. Let X and X be independent random variables, both having the
same distribution on a set S of size m. Show that P[X = X ] = sS P[X = s]2 
1/m.

EXERCISE 8.38. Suppose that the family of random variables X, Y, Y is mutually

independent, where X has image S, and where Y and Y have the same distribution
on a set T . Let  be a predicate on S × T , and let  := P[(X, Y)]. Show that
P[(X, Y)  (X, Y )]  2. In addition, show that if Y and Y are both uniformly
distributed over T , then P[(X, Y)  (X, Y )  (Y = Y )]  2 - /|T |.

EXERCISE 8.39. Let 1, . . . , m be non-negative real numbers that sum to 1. Let
S := {1, . . . , m}, and for n = 1, . . . , m, let S(n) := {T  S : |T | = n}, and define

                     Pn(1, . . . , m) :=                 t.

                                              T S(n) tT

Show that Pn(1, . . . , m) is maximized when 1 = · · · = m = 1/m. Hint: first

argue that if s < t, then for every   [0, t - s], replacing the pair (s, t) by
(s + , t - ) does not decrease the value of Pn(1, . . . , m).
250  Finite and discrete probability distributions

EXERCISE 8.40. Suppose that {Xi}iI is a finite, non-empty, mutually independ-
ent family of random variables, where each Xi is uniformly distributed over a finite
set S. Suppose that {Yi}iI is another finite, non-empty, mutually independent
family of random variables, where each Yi has the same distribution and takes
values in the set S. Let  be the probability that the Xi's are distinct, and  be the
probability that the Yi's are distinct. Using the previous exercise, show that   .

EXERCISE 8.41. Suppose n balls are thrown into m bins. Let A be the event that

there is some bin that is empty. Assuming that the throws are mutually independent,
and that n  m(log m + t) for some t  0, show that P[A]  e-t.

EXERCISE 8.42. Show that for every prime p, there exists a pairwise independent

family of random variables {Xi}iZp, where each Xi is uniformly distributed over
Zp, and yet the probability that all the Xi's are distinct is 1 - 1/p.

EXERCISE 8.43. Let {Xi}in=1 be a finite, non-empty, 4-wise independent family of
random variables, each uniformly distributed over a set S. Let  be the probability
that the Xi's are distinct. For i, j = 1, . . . , n, let Cij be the indicator variable for the
event that Xi = Xj, and define K := {(i, j) : 1  i  n - 1, i + 1  j  n} and
Z := (i,j)K Cij. Show that:

    (a) {Cij}(i,j)K is pairwise independent;
   (b) E[Z] = n(n - 1)/2m and Var[Z] = (1 - 1/m) E[Z];

    (c)   1/ E[Z];

   (d)   1/2, provided n(n - 1)  2m (hint: Exercise 8.4).

EXERCISE 8.44. Let k be a positive integer, let n := k2 - k + 1, let I and S be sets
of size n, and let s0 be a fixed element of S. Also, let I (k) := {J  I : |J | = k},
and let  be the set of all permutations on S. For each J  I (k), let fJ be some
function that maps J to s0, and maps I \ J injectively into S \ {s0}. For   ,
J  I (k), and i  I, define i(, J ) := (fJ (i)). Finally, let Y be uniformly
distributed over  × I (k), and for i  I, define Xi := i(Y). Show that {Xi}iI
is pairwise independent, with each Xi uniformly distributed over S, and yet the

                                                                
number of Xi's with the same value is always at least n.

EXERCISE 8.45. Let S be a set of size m  1, and let s0 be an arbitrary, fixed
element of S. Let F be a random variable that is uniformly distributed over the
set of all mm functions from S into S. Let us define random variables Xi, for
i = 0, 1, 2, . . . , as follows:

                        X0 := s0, Xi+1 := F(Xi) (i = 0, 1, 2, . . .).

Thus, the value of Xi is obtained by applying the function F a total of i times to the
                               8.6 Balls and bins                251

starting value s0. Since S has size m, the sequence {Xi}i=0 must repeat at some
point; that is, there exists a positive integer n (with n  m) such that Xn = Xi for
some i = 0, . . . , n - 1. Define the random variable Y to be the smallest such value

n.

(a) Show that for every i  0 and for all s1, . . . , si  S such that s0, s1, . . . , si
     are distinct, the conditional distribution of Xi+1 given the event (X1 = s1) 
     · · ·  (Xi = si) is the uniform distribution on S.

(b) Show that for every integer n  1, we have Y  n if and only if the random

     variables X0, X1, . . . , Xn-1 take on distinct values.
(c) From parts (a) and (b), show that for each n = 1, . . . , m, we have

P[Y  n | Y  n - 1] = 1 - (n - 1)/m,

and conclude that

                                                n-1

                    P[Y  n] = (1 - i/m)  e-n(n-1)/2m.

                                                 i=1

(d) Using part (c), show that

E[Y] = P[Y  n]  e-n(n-1)/2m = O(m1/2).

n1                             n1

(e) Modify the above argument to show that E[Y] = (m1/2).

EXERCISE 8.46. The setup for this exercise is identical to that of the previous
exercise, except that now, F is uniformly distributed over the set of all m! permuta-
tions of S.

    (a) Show that if Y = n, then Xn = X0.

(b) Show that for every i  0 and all s1, . . . , si  S such that s0, s1, . . . , si are
     distinct, the conditional distribution of Xi+1 given (X1 = s1)· · ·(Xi = si)

     is essentially the uniform distribution on S \ {s1, . . . , si}.

(c) Show that for each n = 2, . . . , m, we have

                     P[Y  n | Y  n - 1] = 1 - 1 m - n + 2 ,
and conclude that for all n = 1, . . . , m, we have

n-2 P[Y  n] = 1 - 1                                = 1 - n - 1.
               i=0 m - i                                     m

(d) From part (c), show that Y is uniformly distributed over {1, . . . , m}, and in
     particular, E[Y] = (m + 1)/2.
252  Finite and discrete probability distributions

                                     8.7 Hash functions

In this section, we apply the tools we have developed thus far to a particularly
important area of computer science: the theory and practice of hashing.

   Let R, S, and T be finite, non-empty sets. Suppose that for each r  R, we have
a function r : S  T . We call r a hash function (from S to T ). Elements of
R are called keys, and if r(s) = t, we say that s hashes to t under r.

   In applications of hash functions, we are typically interested in what happens
when various inputs are hashed under a randomly chosen key. To model such
situations, let H be a random variable that is uniformly distributed over R, and for
each s  S, let us define the random variable H(s), which takes the value r(s)
when H = r.

     · We say that the family of hash functions {r}rR is pairwise independent
         if the family of random variables {H(s)}sS is pairwise independent, with
         each H(s) uniformly distributed over T .

     · We say that {r}rR is universal if

                                     P[H(s) = H(s )]  1/|T |

         for all s, s  S with s = s .

   We make a couple of simple observations. First, by Theorem 8.25, if the family
of hash functions {r}rR is pairwise independent, then it is universal. Second, by
Theorem 8.10, if |S| > 1, then {r}rR is pairwise independent if and only if the
following condition holds:

         the random variable (H(s), H(s )) is uniformly distributed over
         T × T , for all s, s  S with s = s ;

or equivalently,

         P[H(s) = t  H(s ) = t ] = 1/|T |2 for all s, s  S with s = s ,
         and for all t, t  T .

   Before looking at constructions of pairwise independent and universal families
of hash functions, we briefly discuss two important applications.

Example 8.34. Suppose {r}rR is a universal family of hash functions from S
to T . One can implement a "dictionary" using a so-called hash table, which is
basically an array A indexed by T , where each entry in A is a list. Entries in the
dictionary are drawn from the set S. To insert a word s  S into the dictionary, s
is first hashed to an index t, and then s is appended to the list A[t]; likewise, to see
if an arbitrary word s  S is in the dictionary, s is first hashed to an index t, and
then the list A[t] is searched for s.

   Usually, the set of entries in the dictionary is much smaller than the set S. For
8.7 Hash functions  253

example, S may consist of all bit strings of length up to, say 2048, but the dic-
tionary may contain just a few thousand, or a few million, entries. Also, to be
practical, the set T should not be too large.

   Of course, all entries in the dictionary could end up hashing to the same index,
in which case, looking up a word in the dictionary degenerates into linear search.
However, we hope that this does not happen, and that entries hash to indices that
are nicely spread out over T . As we will now see, in order to ensure reasonable
performance (in an expected sense), T needs to be of size roughly equal to the
number of entries in the dictionary,

   Suppose we create a dictionary containing n entries. Let m := |T |, and let I  S
be the set of entries (so n = |I|). These n entries are inserted into the hash table
using a randomly chosen hash key, which we model as a random variable H that
is uniformly distributed over R. For each s  S, we define the random variable
Ls to be the number of entries in I that hash to the same index as s under the key
H; that is, Ls := |{i  I : H(s) = H(i)}|. Intuitively, Ls measures the cost of
looking up the particular word s in the dictionary. We want to bound E[Ls]. To this
end, we write Ls as a sum of indicator variables: Ls = iI Csi, where Csi is the
indicator variable for the event that H(s) = H(i). By Theorem 8.16, we have
E[Csi] = P[H(s) = H(i)]; moreover, by the universal property, E[Csi]  1/m if
s = i, and clearly, E[Csi] = 1 if s = i. By linearity of expectation, we have

                                      E[Ls] = E[Csi].

                                                                  iI

If s / I, then each term in the sum is  1/m, and so E[Ls]  n/m. If s  I,
then one term in the sum is 1, and the other n - 1 terms are  1/m, and so
E[Ls]  1 + (n - 1)/m. In any case, we have

                                       E[Ls]  1 + n/m.

In particular, this means that if m  n, then the expected cost of looking up any
particular word in the dictionary is bounded by a constant. 2

Example 8.35. Suppose Alice wants to send a message to Bob in such a way that
Bob can be reasonably sure that the message he receives really came from Alice,
and was not modified in transit by some malicious adversary. We present a solution
to this problem here that works assuming that Alice and Bob share a randomly
generated secret key, and that this key is used to authenticate just a single message
(multiple messages can be authenticated using multiple keys).

   Suppose that {r}rR is a pairwise independent family of hash functions from
S to T . We model the shared random key as a random variable H, uniformly
distributed over R. We also model Alice's message as a random variable X, taking
values in the set S. We make no assumption about the distribution of X, but we do
254            Finite and discrete probability distributions

assume that X and H are independent. When Alice sends the message X to Bob,
she also sends the "authentication tag" Y := H(X). Now, when Bob receives a
message X and tag Y , he checks that H(X ) = Y ; if this holds, he accepts the

message X as authentic; otherwise, he rejects it. Here, X and Y are also random

variables; however, they may have been created by a malicious adversary who may

have even created them after seeing X and Y. We can model such an adversary as
a pair of functions f : S × T  S and g : S × T  T , so that X := f (X, Y) and
Y := g(X, Y). The idea is that after seeing X and Y, the adversary computes X and

Y and sends X and Y to Bob instead of X and Y. Let us say that the adversary
fools Bob if H(X ) = Y and X = X. We will show that P[F]  1/m, where F is
the event that the adversary fools Bob, and m := |T |. Intuitively, this bound holds

because the pairwise independence property guarantees that after seeing the value
of H at one input, the value of H at any other input is completely unpredictable,

and cannot be guessed with probability any better than 1/m. If m is chosen to be

suitably large, the probability that Bob gets fooled can be made acceptably small.

For example, S may consist of all bit strings of length up to, say, 2048, while the set

T may be encoded using much shorter bit strings, of length, say, 64. This is nice,

as it means that the authentication tags consume very little additional bandwidth.

   A straightforward calculation justifies the claim that P[F]  1/m:

P[F] =         P (X = s)  (Y = t)  F (law of total probability (8.9))

        sS tT

     =         P (X = s)  (H(s) = t)  (H(f (s, t)) = g(s, t)) 

        sS tT (f (s, t) = s)

     =         P[X = s] P (H(s) = t)  (H(f (s, t)) = g(s, t)) 

        sS tT      (f (s, t) = s) (since X and H are independent)

               P[X = s] · (1/m2) (since {r}rR is pairwise independent)

        sS tT

     = (1/m) P[X = s] = 1/m. 2

               sS

   We now present several constructions of pairwise independent and universal
families of hash functions.

Example 8.36. By setting k := 2 in Example 8.27, for each prime p, we immedi-
ately get a pairwise independent family of hash functions {r}rR from Zp to Zp,
     8.7 Hash functions                                                      255

where R = Zp × Zp, and for r = (r0, r1)  R, the hash function r is given by

                                   r : Zp  Zp
                                            s  r0 + r1s. 2

   While very simple and elegant, the family of hash functions in Example 8.36 is
not very useful in practice. As we saw in Examples 8.34 and 8.35, what we would
really like are families of hash functions that hash long inputs to short outputs. The
next example provides us with a pairwise independent family of hash functions that
satisfies this requirement.

Example 8.37. Let p be a prime, and let be a positive integer. Let S := Zp× and
R := Zp×( +1). For each r = (r0, r1, . . . , r )  R, we define the hash function

r :  S  Zp

     (s1, . . . , s )  r0 + r1s1 + · · · + r s .

We will show that {r}rR is a pairwise independent family of hash functions

from S to Zp. To this end, let H be a random variable uniformly distributed over
R. We want to show that for each s, s  S with s = s , the random variable
(H(s), H(s )) is uniformly distributed over Zp × Zp. So let s = s be fixed, and
define the function

      : R  Zp × Zp
           r  (r(s), r(s )).

Because  is a group homomorphism, it will suffice to show that  is surjective (see
Theorem 8.5). Suppose s = (s1, . . . , s ) and s = (s1, . . . , s ). Since s = s , we
must have sj = sj for some j = 1, . . . , . For this j, consider the function

:    R  Zp × Zp

     (r0, r1, . . . , r )  (r0 + rjsj, r0 + rjsj).

Evidently, the image of  includes the image of  , and by Example 8.36, the func-
tion  is surjective. 2

   To use the construction in Example 8.37 in applications where the set of inputs
consists of bit strings of a given length, one can naturally split such a bit string up
into short bit strings which, when viewed as integers, lie in the set {0, . . . , p - 1},
and which can in turn be viewed as elements of Zp. This gives us a natural, injective
map from bit strings to elements of Zp× . The appropriate choice of the prime p
depends on the application. Of course, the requirement that p is prime limits our
choice in the size of the output set; however, this is usually not a severe restric-
tion, as Bertrand's postulate (Theorem 5.8) tells us that we can always choose p
256  Finite and discrete probability distributions

to within a factor of 2 of any desired value of the output set size. Nevertheless,
the construction in the following example gives us a universal (but not pairwise
independent) family of hash functions with an output set of any size we wish.

Example 8.38. Let p be a prime, and let m be an arbitrary positive integer. Let
us introduce some convenient notation: for   Zp, let [[]]m := [rep()]m  Zm
(recall that rep() denotes the unique integer a  {0, . . . , p-1} such that  = [a]p).
Let R := Zp × Zp, and for each r = (r0, r1)  R, define the hash function

                                  r : Zp  Zm
                                            s  [[r0 + r1s]]m.

Our goal is to show that {r}rR is a universal family of hash functions from Zp to
Zm. So let s, s  Zp with s = s , let H0 and H1 be independent random variables,
with H0 uniformly distributed over Zp and H1 uniformly distributed over Zp, and let
H := (H0, H1). Also, let C be the event that H(s) = H(s ). We want to show that
P[C]  1/m. Let us define random variables Y := H0 + H1s and Y := H0 + H1s .
Also, let s^ := s - s = 0. Then we have

P[C] = P [[Y]]m = [[Y ]]m

     = P [[Y]]m = [[Y + H1s^]]m (since Y = Y + H1s^)

     = P [[Y]]m = [[Y + H1s^]]m  (Y = ) (law of total probability (8.9))

        Zp

     = P [[]]m = [[ + H1s^]]m  (Y = )

        Zp

     = P [[]]m = [[ + H1s^]]m P[Y = ]

        Zp

     (by Theorem 8.13, Y and H1 are independent).

It will suffice to show that

                           P [[]]m = [[ + H1s^]]m  1/m  (8.33)

for each   Zp, since then

     P[C]  (1/m) P[Y = ] = (1/m) P[Y = ] = 1/m.

     Zp                                Zp

So consider a fixed   Zp. As s^ = 0 and H1 is uniformly distributed over Zp, it
follows that H1s^ is uniformly distributed over Zp, and hence  + H1s^ is uniformly
distributed over the set Zp \ {}. Let M := {  Zp : [[]]m = [[]]m}. To prove
           8.7 Hash functions                         257

(8.33), we need to show that |M \ {}|  (p - 1)/m. But it is easy to see that
|M|  p/m , and since M certainly contains , we have

|M \ {}|  p - 1  p + m - 1 - 1 = p - 1 . 2
        m  mm                                     m

   One drawback of the family of hash functions in the previous example is that the
prime p may need to be quite large (at least as large as the size of the set of inputs)
and so to evaluate a hash function, we have to perform modular multiplication of
large integers. In contrast, in Example 8.37, the prime p can be much smaller
(only as large as the size of the set of outputs), and so these hash functions can be
evaluated much more quickly.

   Another consideration in designing families of hash functions is the size of key
set. The following example gives a variant of the family in Example 8.37 that uses
somewhat a smaller key set (relative to the size of the input), but is only a universal
family, and not a pairwise independent family.

Example 8.39. Let p be a prime, and let be a positive integer. Let S := Zp×( +1)
and R := Zp× . For each r = (r1, . . . , r )  R, we define the hash function

r :        S  Zp

     (s0, s1, . . . , s )  s0 + r1s1 + · · · + r s .

Our goal is to show that {r}rR is a universal family of hash functions from
S to Zp. So let s, s  S with s = s , and let H be a random variable that is
uniformly distributed over R. We want to show that P[H(s) = H(s )]  1/p. Let
s = (s0, s1, . . . , s ) and s = (s0, s1, . . . , s ), and set s^i := si - si for i = 0, 1, . . . , .
Let us define the function

     :     R  Zp

        (r1, . . . , r )  r1s^1 + · · · + r s^ .

Clearly, H(s) = H(s ) if and only if (H) = -s^0. Moreover,  is a group
homomorphism. There are two cases to consider. In the first case, s^i = 0 for all
i = 1, . . . , ; in this case, the image of  is {0}, but s^0 = 0 (since s = s ), and
so P[(H) = -s^0] = 0. In the second case, s^i = 0 for some i = 1, . . . , ; in

this case, the image of  is Zp, and so (H) is uniformly distributed over Zp (see
Theorem 8.5); thus, P[(H) = -s^0] = 1/p. 2

   One can get significantly smaller key sets, if one is willing to relax the defini-

tions of universal and pairwise independence. Let {r}rR be a family of hash
functions from S to T , where m := |T |. Let H be a random variable that is

uniformly distributed over R. We say that {r}rR is -almost universal if for
all s, s  S with s = s , we have P[H(s) = H(s )]  . Thus, {r}rR is
258       Finite and discrete probability distributions

universal if and only if it is 1/m-almost universal. We say that {r}rR is -almost
strongly universal if H(s) is uniformly distributed over T for each s  S, and
P[(H(s) = t)  (H(s ) = t )]  /m for all s, s  S with s = s and all t, t  T .

Constructions, properties, and applications of these types of hash functions are

developed in some of the exercises below.

EXERCISE 8.47. For each positive integer n, let In denote {0, . . . , n - 1}. Let m
be a power of a prime, be a positive integer, S := Im× , and R := Im2 ×( +1). For
each r = (r0, r1, . . . , r )  R, define the hash function

     r :  S  Im

          (s1, . . . , s )  (r0 + r1s1 + · · · + r s ) mod m2 m .

Using the result from Exercise 2.13, show that {r}rR is a pairwise independent
family of hash functions from S to Im. Note that on a typical computer, if m is a
suitable power of 2, then it is very easy to evaluate these hash functions, using just
multiplications, additions, shifts, and masks (no divisions).

EXERCISE 8.48. Let {r}rR be an -almost universal family of hash functions
from S to T . Also, let H, X, X be random variables, where H is uniformly dis-

tributed over R, and both X and X take values in S. Moreover, assume H and
(X, X ) are independent. Show that P[H(X) = H(X )]  P[X = X ] + .

EXERCISE 8.49. Let {r}rR be an -almost universal a family of hash functions
from S to T , and let H be a random variable that is uniformly distributed over R.
Let I be a subset of S of size n > 0. Let C be the event that H(i) = H(j)
for some i, j  I with i = j. We define several random variables: for each
t  T , Nt := |{i  I : H(i) = t}|; M := max{Nt : t  T }; for each s  S,
Ls := |{i  I : H(s) = H(i)}|. Show that:

    (a) P[C]  n(n - 1)/2;

   (b) E[M]  n2 + n;

    (c) for each s  S, E[Ls]  1 + n.

   The results of the previous exercise show that for many applications, the -
almost universal property is good enough, provided  is suitably small. The next
three exercises develop -almost universal families of hash functions with very
small sets of keys, even when  is quite small.

EXERCISE 8.50. Let p be a prime, and let be a positive integer. Let S := Zp×( +1).
     8.7 Hash functions                                             259

For each r  Zp, define the hash function

r :  S  Zp

     (s0, s1, . . . , s )  s0 + s1r + · · · + s r .

Show that {r}rZp is an /p-almost universal family of hash functions from S to
Zp.

EXERCISE 8.51. Let {r}rR be an -almost universal family of hash functions
from S to T . Let {r }r R be an  -almost universal family of hash functions from

S to T , where T  S . Show that

                                     {r  r}(r,r )R×R                (here, ""
is an ( +  )-almost universal family of hash functions from S to T
denotes function composition).

EXERCISE 8.52. Let m and be positive integers, and let 0 <  < 1. Given these
parameters, show how to construct an -almost universal family of hash functions
{r}rR from Zm× to Zm, such that

               (1 + )/m and log|R| = O(log m + log + log(1/)).

Hint: use the previous two exercises, and Example 8.38.

EXERCISE 8.53. Let {r}rR be an -almost universal family of hash functions
from S to T . Show that   1/|T | - 1/|S|.

EXERCISE 8.54. Let {r}rR be a family of hash functions from S to T , with
m := |T |. Show that:

    (a) if {r}rR is -almost strongly universal, then it is -almost universal;
   (b) if {r}rR is pairwise independent, then it is 1/m-almost strongly univer-

         sal;
    (c) if {r}rR is -almost universal, and {r }r R is an  -almost strongly

         universal family of hash functions from S to T , where T  S , then
         {r  r}(r,r )R×R is an ( +  )-almost strongly universal family of hash
         functions from S to T .

EXERCISE 8.55. Show that if an -almost strongly universal family of hash func-
tions is used in Example 8.35, then Bob gets fooled with probability at most .

EXERCISE 8.56. Show how to construct an -almost strongly universal family of
hash functions satisfying the same bounds as in Exercise 8.52, under the restriction
that m is a prime power.
260  Finite and discrete probability distributions

EXERCISE 8.57. Let p be a prime, and let be a positive integer. Let S := Zp×
and R := Zp × Zp. For each (r0, r1)  R, define the hash function

     r :  S  Zp

          (s1, . . . , s )  r0 + s1r1 + · · · + s r1.

Show that {r}rR is an /p-almost strongly universal family of hash functions
from S to Zp.

                                   8.8 Statistical distance

This section discusses a useful measure of "distance" between two random vari-
ables. Although important in many applications, the results of this section (and the
next) will play only a very minor role in the remainder of the text.

   Let X and Y be random variables which both take values in a finite set S. We
define the statistical distance between X and Y as

                        [X; Y] := 1 P[X = s] - P[Y = s] .
                                       2 sS

Theorem 8.30. For random variables X, Y, Z, we have
    (i) 0  [X; Y]  1,
   (ii) [X; X] = 0,

   (iii) [X; Y] = [Y; X], and
   (iv) [X; Z]  [X; Y] + [Y; Z].

Proof. Exercise. 2

   It is also clear from the definition that [X; Y] depends only on the distributions
of X and Y, and not on any other properties. As such, we may sometimes speak of
the statistical distance between two distributions, rather than between two random
variables.

Example 8.40. Suppose X has the uniform distribution on {1, . . . , m}, and Y has
the uniform distribution on {1, . . . , m - }, where   {0, . . . , m - 1}. Let us
compute [X; Y]. We could apply the definition directly; however, consider the
following graph of the distributions of X and Y:
         8.8 Statistical distance                                           261

1/(m - )
                                A

      1/m

             B                            C

   0                               m-                                    m

   The statistical distance between X and Y is just 1/2 times the area of regions A
and C in the diagram. Moreover, because probability distributions sum to 1, we
must have

                  area of B + area of A = 1 = area of B + area of C,

and hence, the areas of region A and region C are the same. Therefore,

                       [X; Y] = area of A = area of C = /m. 2

The following characterization of statistical distance is quite useful:

Theorem 8.31. Let X and Y be random variables taking values in a set S. For
every S  S, we have

                            [X; Y]  |P[X  S ] - P[Y  S ]|,

and equality holds for some S  S, and in particular, for the set
                          S := {s  S : P[X = s] < P[Y = s]},

as well as its complement.

Proof. Suppose we split the set S into two disjoint subsets: the set S0 consisting
of those s  S such that P[X = s] < P[Y = s], and the set S1 consisting of those
s  S such that P[X = s]  P[Y = s]. Consider the following rough graph of

the distributions of X and Y, where the elements of S0 are placed to the left of the

elements of S1:

Y     A                                C

X

                               B

         S0                        S1
262  Finite and discrete probability distributions

Now, as in Example 8.40,
                          [X; Y] = area of A = area of C.

Now consider any subset S of S, and observe that
                    P[X  S ] - P[Y  S ] = area of C - area of A ,

where C is the subregion of C that lies above S , and A is the subregion of A that
lies above S . It follows that |P[X  S ] - P[Y  S ]| is maximized when S = S0
or S = S1, in which case it is equal to [X; Y]. 2

   We can restate Theorem 8.31 as follows:

            [X; Y] = max{|P[(X)] - P[(Y)]| :  is a predicate on S}.

This implies that when [X; Y] is very small, then for every predicate , the events
(X) and (Y) occur with almost the same probability. Put another way, there is no
"statistical test" that can effectively distinguish between the distributions of X and
Y. For many applications, this means that the distribution of X is "for all practical
purposes" equivalent to that of Y, and hence in analyzing the behavior of X, we can
instead analyze the behavior of Y, if that is more convenient.

Theorem 8.32. If S and T are finite sets, X and Y are random variables taking
values in S, and f : S  T is a function, then [f (X); f (Y)]  [X; Y].

Proof. We have

         [f (X); f (Y)] = |P[f (X)  T ] - P[f (Y)  T ]| for some T  T
                                 (by Theorem 8.31)

                            = |P[X  f -1(T )] - P[Y  f -1(T )]|
                             [X; Y] (again by Theorem 8.31). 2

Example 8.41. Let X be uniformly distributed over the set {0, . . . , m-1}, and let Y
be uniformly distributed over the set {0, . . . , n-1}, for n  m. Let f (t) := t mod m.
We want to compute an upper bound on the statistical distance between X and f (Y).
We can do this as follows. Let n = qm - r, where 0  r < m, so that q = n/m .
Also, let Z be uniformly distributed over {0, . . . , qm - 1}. Then f (Z) is uniformly
distributed over {0, . . . , m - 1}, since every element of {0, . . . , m - 1} has the same
number (namely, q) of pre-images under f which lie in the set {0, . . . , qm - 1}.
Since statistical distance depends only on the distributions of the random variables,
by the previous theorem, we have

                         [X; f (Y)] = [f (Z); f (Y)]  [Z; Y],
               8.8 Statistical distance                                              263

and as we saw in Example 8.40,
                              [Z; Y] = r/qm < 1/q  m/n.

Therefore,

               [X; f (Y)] < m/n. 2

We close this section with two useful theorems.

Theorem 8.33. Suppose X, Y, and Z are random variables, where X and Z are
independent, and Y and Z are independent. Then [X, Z; Y, Z] = [X, Y].

   Note that [X, Z; Y, Z] is shorthand for [(X, Z); (Y, Z)].

Proof. Suppose X and Y take values in a finite set S, and Z takes values in a finite
set T . From the definition of statistical distance,

2[X, Z; Y, Z] = P[(X = s)  (Z = t)] - P[(Y = s)  (Z = t)]

                              s,t

                  = P[X = s] P[Z = t] - P[Y = s] P[Z = t]

                              s,t

                     (by independence)

            = P[Z = t] P[X = s] - P[Y = s]

                 s,t

            = P[Z = t]                                P[X = s] - P[Y = s]

            t                 s

            = 1 · 2[X; Y]. 2

Theorem 8.34. Let X1, . . . , Xn, Y1, . . . , Yn be random variables, where {Xi}in=1 is
mutually independent, and {Yi}in=1 is mutually independent. Then we have

                                                                                  n

                        [X1, . . . , Xn; Y1, . . . , Yn]  [Xi; Yi].

                                                                                 i=1

Proof. Since [X1, . . . , Xn; Y1, . . . , Yn] depends only on the individual distributions
of the random variables (X1, . . . , Xn) and (Y1, . . . , Yn), without loss of general-
ity, we may assume that (X1, . . . , Xn) and (Y1, . . . , Yn) are independent, so that
X1, . . . , Xn, Y1, . . . , Yn form a mutually independent family of random variables.
We introduce random variables Z0, . . . , Zn, defined as follows:

            Z0 := (X1, . . . , Xn),                   for i = 1, . . . , n - 1, and
            Zi := (Y1, . . . , Yi, Xi+1, . . . , Xn)
            Zn := (Y1, . . . , Yn).
264                    Finite and discrete probability distributions

By definition, [X1, . . . , Xn; Y1, . . . , Yn] = [Z0; Zn]. Moreover, by part (iv) of
Theorem 8.30, we have [Z0; Zn]  in=1 [Zi-1; Zi]. Now consider any fixed
index i = 1, . . . , n. By Theorem 8.33, we have

                   [Zi-1; Zi] = [ Xi, (Y1, . . . , Yi-1, Xi+1, . . . , Xn);

                                         Yi, (Y1, . . . , Yi-1, Xi+1, . . . , Xn)]
                                  = [Xi; Yi].

The theorem now follows immediately. 2

   The technique used in the proof of the previous theorem is sometimes called
a hybrid argument, as one considers the sequence of "hybrid" random variables
Z0, Z1, . . . , Zn, and shows that the distance between each consecutive pair of vari-
ables is small.

EXERCISE 8.58. Let X and Y be independent random variables, each uniformly
distributed over Zp, where p is prime. Calculate [X, Y; X, XY].

EXERCISE 8.59. Let n be an integer that is the product of two distinct primes of
the same bit length. Let X be uniformly distributed over Zn, and let Y be uniformly
distributed over Zn. Show that [X; Y]  3n-1/2.

EXERCISE 8.60. Let X and Y be 0/1-valued random variables. Show that
                             [X; Y] = |P[X = 1] - P[Y = 1]|.

EXERCISE 8.61. Let S be a finite set, and consider any function  : S  {0, 1}.
Let B be a random variable uniformly distributed over {0, 1}, and for b = 0, 1,

let Xb be a random variable taking values in S, and assume that Xb and B are
independent. Show that

      |P[(XB )  =  B]  -  12 |  =  12 |P[(X0)  =  1]  -  P[(X1)  =  1]|    1  [  X  0  ;  X  1  ].
                                                                           2

EXERCISE 8.62. Let X, Y be random variables taking values in a finite set S. For
an event B that occurs with non-zero probability, define the conditional statistical
distance

                  [X; Y | B] := 1 P[X = s | B] - P[Y = s | B] .
                                    2 sS

Let {Bi}iI be a finite, pairwise disjoint family of events whose union is B. Show

that

                   [X; Y | B] P[B]                    [X; Y | Bi] P[Bi].

                                     P[Bi]=0
8.8 Statistical distance  265

EXERCISE 8.63. Let {r}rR be a family of hash functions from S to T , with
m := |T |. We say {r}rR is -variationally universal if H(s) is uniformly
distributed over T for each s  S, and [H(s ); Y | H(s) = t]   for each
s, s  S with s = s and each t  T ; here, H and Y are independent random

variables, with H uniformly distributed over R, and Y uniformly distributed over

T . Show that:

    (a) if {r}rR is pairwise independent, then it is 0-variationally universal;

   (b) if {r}rR is -variationally universal, then it is (1/m + )-almost strongly

         universal;

    (c) if {r}rR is -almost universal, and {r }r R is an  -variationally uni-
         versal family of hash functions from S to T , where T  S , then
         {r  r}(r,r )R×R is an ( +  )-variationally universal family of hash
         functions from S to T .

EXERCISE 8.64. Let {r}rR be a family hash functions from S to T such that
(i) each r maps S injectively into T , and (ii) there exists   [0, 1] such that
[H(s); H(s )]   for all s, s  S, where H is uniformly distributed over R.

Show that |R|  (1 - )|S|.

EXERCISE 8.65. Let X and Y be random variables that take the same value
unless a certain event F occurs (i.e., X() = Y() for all   F ). Show that
[X; Y]  P[F].

EXERCISE 8.66. Let X and Y be random variables taking values in the interval
[0, t]. Show that |E[X] - E[Y]|  t · [X; Y].

EXERCISE 8.67. Show that Theorem 8.33 is not true if we drop the independence
assumptions.

EXERCISE 8.68. Let S be a set of size m  1. Let F be a random variable that
is uniformly distributed over the set of all functions from S into S. Let G be a
random variable that is uniformly distributed over the set of all permutations of S.
Let s1, . . . , sn be distinct, fixed elements of S. Show that

                   [F(s1), . . . , F(sn); G(s1), . . . , G(sn)]  n(n - 1) .
                                                                        2m

EXERCISE 8.69. Let m be a large integer. Consider three random experiments. In
the first, we generate a random integer X1 between 1 and m, and then a random inte-
ger Y1 between 1 and X1. In the second, we generate a random integer X2 between
2 and m, and then generate a random integer Y2 between 1 and X2. In the third,
we generate a random integer X3 between 2 and m, and then a random integer Y3
266      Finite and discrete probability distributions

between 2 and X3. Show that [X1, Y1; X2, Y2] = O(1/m) and [X2, Y2; X3, Y3] =
O(log m/m), and conclude that [X1, Y1; X3, Y3] = O(log m/m).

         8.9 Measures of randomness and the leftover hash lemma ()

In this section, we discuss different ways to measure "how random" the distribution

of a random variable is, and relations among them.

Let X be a random variable taking values in a finite set S of size m. We define

three measures of randomness:

     1. the collision probability of X is sS P[X = s]2;
     2. the guessing probability of X is max{P[X = s] : s  S};

     3.  the distance of X from uniform on S  is  1  sS|P[X = s] - 1/m|.
                                                  2

Suppose X has collision probability , guessing probability , and distance 

from uniform on S. If X is another random variable with the same distribution
as X, where X and X independent, then  = P[X = X ] (see Exercise 8.37). If Y
is a random variable that is uniformly distributed over S, then  = [X; Y]. If X
itself is uniformly distributed over S, then  =  = 1/m, and  = 0. The quantity

log2(1/) is sometimes called the min entropy of X, and the quantity log2(1/) is
sometimes called the Renyi entropy of X.

We first state some easy inequalities:

Theorem 8.35. Suppose X is a random variable that takes values in a finite set S
of size m. If X has collision probability , guessing probability , and distance 
from uniform on S, then:

    (i)   1/m;
   (ii) 2      1/m + .

Proof. Part (i) is immediate from Exercise 8.37. The other inequalities are left as
easy exercises. 2

   This theorem implies that the collision and guessing probabilities are minimal
for the uniform distribution, which perhaps agrees with one's intuition.

   While the above theorem implies that  and  are close to 1/m when  is small,
the following theorem provides a converse:

Theorem 8.36. Suppose X is a random variable that takes values in a finite set S
of size m. If X has collision probability , and distance  from uniform on S, then
  12 m - 1.

Proof. We may assume that  > 0, since otherwise the theorem is already true,
simply from the fact that   1/m.
         8.9 Measures of randomness and the leftover hash lemma ()  267

For s    S, let ps  :=  P[X  =  s].  We have   =  1  s|ps - 1/m|, and hence
                                                  2
1 = s qs, where qs := |ps - 1/m|/2. So we have

         1  qs2 (by Exercise 8.36)
         ms

         =1             (ps - 1/m)2

                 2
         4 s

         =1               ps2 - 1/m  (again by Exercise 8.36)
            42
                        s

         = 42 1 ( - 1/m),

from which the theorem follows immediately. 2

   We are now in a position to state and prove a very useful result which, intuitively,
allows us to convert a "low quality" source of randomness into a "high quality"
source of randomness, making use of an almost universal family of hash functions
(see end of §8.7).

Theorem 8.37 (Leftover hash lemma). Let {r}rR be a (1 + )/m-almost uni-
versal family of hash functions from S to T , where m := |T |. Let H and X be

independent random variables, where H is uniformly distributed over R, and X

takes values in S. If  is the collision probability of X, and  is the distance of

(H, H(X)) from uniform on R × T , then     1   m + .
                                           2

Proof. Let  be the collision probability of (H, H(X)). Our goal is to bound 
from above, and then apply Theorem 8.36 to the random variable (H, H(X)). To
this end, let := |R|, and suppose H and X are random variables, where H has

the same distribution as H, X has the same distribution as X, and H, H , X, X form

a mutually independent family of random variables. Then we have

        = P[(H = H )  (H(X) = H (X ))]
          = P[(H = H )  (H(X) = H(X ))]
          = 1 P[H(X) = H(X )] (a special case of Exercise 8.15)

          1 (P[X = X ] + (1 + )/m) (by Exercise 8.48)

               = 1 (m + 1 + ).
                    m

The theorem now follows immediately from Theorem 8.36. 2
   In the previous theorem, if {r}rR is a universal family of hash functions, then
268  Finite and discrete probability distributions

we can take  = 0. However, it is convenient to allow  > 0, as this allows for the
use of families with a smaller key set (see Exercise 8.52).

Example 8.42. Suppose S := {0, 1}×1000, T := {0, 1}×64, and that {r}rR is
a universal family of hash functions from S to T . Suppose X and H are inde-

pendent random variables, where X is uniformly distributed over some subset S
of S of size  2160, and H is uniformly distributed over R. Then the collision and
guessing probabilities of X are at most 2-160, and so the leftover hash lemma (with
 = 0) says that the distance of (H, H(X)) from uniform on R × T is  , where
  12 2642-160 = 2-49. By Theorem 8.32, it follows that the distance of H(X)
from uniform on T is at most   2-49. 2

   The leftover hash lemma allows one to convert "low quality" sources of ran-
domness into "high quality" sources of randomness. Suppose that to conduct an
experiment, we need to sample a random variable Y whose distribution is uniform
on a set T of size m, or at least, its distance from uniform on T is sufficiently small.
However, we may not have direct access to a source of "real" randomness whose
distribution looks anything like that of the desired uniform distribution, but rather,
only to a "low quality" source of randomness. For example, one could model
various characteristics of a person's typing at the keyboard, or perhaps various
characteristics of the internal state of a computer (both its software and hardware)
as a random process. We cannot say very much about the probability distribu-
tions associated with such processes, but perhaps we can conservatively estimate
the collision or guessing probabilities associated with these distributions. Using
the leftover hash lemma, we can hash the output of this random process, using
a suitably generated random hash function. The hash function acts like a "mag-
nifying glass": it "focuses" the randomness inherent in the "low quality" source
distribution onto the set T , obtaining a "high quality," nearly uniform, distribution
on T .

   Of course, this approach requires a random hash function, which may be just as
difficult to generate as a random element of T . The following theorem shows, how-
ever, that we can at least use the same "magnifying glass" many times over, with
the statistical distance from uniform of the output distribution increasing linearly
in the number of applications of the hash function.

Theorem 8.38. Let {r}rR be a (1 + )/m-almost universal family of hash
functions from S to T , where m := |T |. Let H, X1, . . . , Xn be random vari-
ables, where H is uniformly distributed over R, each Xi takes values in S, and
H, X1, . . . , Xn form a mutually independent family of random variables. If  is
an upper bound on the collision probability of each Xi, and  is the distance of
(H, H(X1), . . . , H(Xn)) from uniform on R × T ×n, then   12 n m + .
         8.9 Measures of randomness and the leftover hash lemma ()                 269

Proof. Let Y1, . . . , Yn be random variables, each uniformly distributed over T , and
assume that H, X1, . . . , Xn, Y1, . . . , Yn form a mutually independent family of ran-
dom variables. We shall make a hybrid argument (as in the proof of Theorem 8.34).
Define random variables Z0, Z1, . . . , Zn as follows:

Z0 := (H, H(X1), . . . , H(Xn)),                    for i = 1, . . . , n - 1, and
Zi := (H, Y1, . . . , Yi, H(Xi+1), . . . , H(Xn))
Zn := (H, Y1, . . . , Yn).

We have

          = [Z0; Zn]

                   n

             [Zi-1; Zi] (by part (iv) of Theorem 8.30)

                 i=1
                   n

             [ H, Y1, . . . , Yi-1, H(Xi), Xi+1, . . . , Xn;
               i=1 H, Y1, . . . , Yi-1, Yi, Xi+1, . . . , Xn ]

         (by Theorem 8.32)

              n               (by Theorem 8.33)

         = [H, H(Xi); H, Yi]

             i=1

          12 n m +  (by Theorem 8.37). 2

   Another source of "low quality" randomness arises in certain cryptographic
applications, where we have a "secret value" X, which is a random variable that
takes values in a set S, and which has small collision or guessing probability. We
want to derive from X a "secret key" whose distance from uniform on some speci-
fied "key space" T is small. Typically, T is the set of all bit strings of some given
length, as in Example 8.25. Theorem 8.38 allows us to do this using a "public"
hash function -- generated at random once and for all, published for all to see, and
used over and over to derive secret keys as needed. However, to apply this theorem,
it is crucial that the secret values (and the hash key) are mutually independent.

EXERCISE 8.70. Consider again the situation in Theorem 8.37. Suppose that

T = {0, . . . , m - 1}, but that we would rather have a nearly uniform distribution

on T = {0, . . . , m - 1}, for some m < m. While it may be possible to work with

a different family of hash functions, we do not have to if m is large enough with

respect to m , in which case we can just use the value Y := H(X) mod m . Show

that the distance of (H, Y ) from uniform on R × T  is  at  most  1  m +  + m /m.
                                                                  2
270                  Finite and discrete probability distributions

EXERCISE 8.71. Let {r}rR be a (1 + )/m-almost universal family of hash
functions from S to T , where m := |T |. Suppose H, X, Y, Z are random variables,

where H is uniformly distributed over R, X takes values in S, Y is uniformly dis-

tributed over T , and U is the set of values taken by Z with non-zero probability.

Assume that the family of random variables H, Y, (X, Z) is mutually independent.

(a) For u  U , define (u) := sS P[X = s | Z = u]2. Also, let  :=

          uU  (u) P[Z  =  u].   Show  that  [H, H(X), Z; H, Y, Z]     1  m + .
                                                                      2

(b) Suppose that X is uniformly distributed over a subset S of S, and that Z =

     f (X) for some function f : S  U . Show that [H, H(X), Z; H, Y, Z] 

     1    m|U |/|S | + .
     2

                        8.10 Discrete probability distributions

In addition to working with probability distributions over finite sample spaces, one
can also work with distributions over infinite sample spaces. If the sample space is
countable, that is, either finite or countably infinite (see §A3), then the distribution
is called a discrete probability distribution. We shall not consider any other types
of probability distributions in this text. The theory developed in §§8.1-8.5 extends
fairly easily to the countably infinite setting, and in this section, we discuss how
this is done.

                                  8.10.1 Basic definitions

To say that the sample space  is countably infinite simply means that there is a
bijection f from the set of positive integers onto ; thus, we can enumerate the
elements of  as 1, 2, 3, . . . , where i := f (i).

   As in the finite case, a probability distribution on  is a function P :   [0, 1],
where all the probabilities sum to 1, which means that the infinite series i=1 P(i)
converges to one. Luckily, the convergence properties of an infinite series whose

terms are all non-negative is invariant under a reordering of terms (see §A6), so it
does not matter how we enumerate the elements of .

Example 8.43. Suppose we toss a fair coin repeatedly until it comes up heads, and

let k be the total number of tosses. We can model this experiment as a discrete

probability distribution P, where the sample space consists of the set of all positive

integers: for each positive integer k, P(k) := 2-k. We can check that indeed

     2-k  =  1,  as  required.
k=1

One may be tempted to model this experiment by setting up a probability dis-

tribution on the sample space of all infinite sequences of coin tosses; however,

this sample space is not countably infinite, and so we cannot construct a discrete
                8.10 Discrete probability distributions                          271

probability distribution on this space. While it is possible to extend the notion of a
probability distribution to such spaces, this would take us too far afield. 2

Example 8.44. More generally, suppose we repeatedly execute a Bernoulli trial

until it succeeds, where each execution succeeds with probability p > 0 independ-

ently of the previous trials, and let k be the total number of trials executed. Then
we associate the probability P(k) := qk-1p with each positive integer k, where
q := 1 - p, since we have k - 1 failures before the one success. One can easily
check that these probabilities sum to 1. Such a distribution is called a geometric
distribution. 2

Example 8.45. The series       1/k3  converges   to  some  positive  number  c.  There-
                          k=1

fore, we can define a probability distribution on the set of positive integers, where

we associate with each k  1 the probability 1/ck3. 2

   As in the finite case, an event is an arbitrary subset A of . The probability P[A]
of A is defined as the sum of the probabilities associated with the elements of A.
This sum is treated as an infinite series when A is infinite. This series is guaranteed
to converge, and its value does not depend on the particular enumeration of the
elements of A.

Example 8.46. Consider the geometric distribution discussed in Example 8.44,
where p is the success probability of each Bernoulli trial, and q := 1 - p. For a

given integer i  1, consider the event A that the number of trials executed is at

least i. Formally, A is the set of all integers greater than or equal to i. Intuitively,
P[A] should be qi-1, since we perform at least i trials if and only if the first i - 1

trials fail. Just to be sure, we can compute

P[A] = P(k) = qk-1p = qi-1p qk = qi-1p · 1 = qi-1. 2
                                          k0 1 - q
ki              ki

   It is an easy matter to check that all the statements and theorems in §8.1 carry
over verbatim to the case of countably infinite sample spaces. Moreover, Boole's
inequality (8.6) and equality (8.7) are also valid for countably infinite families of
events:

Theorem 8.39.   Suppose A :=         Ai,  where  {Ai}i=1   is  an  infinite  sequence  of
events. Then                   i=1

    (i) P[A]    i=1 P[Ai], and
   (ii) P[A] =  i=1 P[Ai] if {Ai}i=1 is pairwise disjoint.

Proof. As in the proof of Theorem 8.1, for    and B  , define [B] := 1 if
  B, and [B] := 0 if  / B. First, suppose that {Ai}i=1 is pairwise disjoint.
272  Finite and discrete probability distributions

Evidently, [A] = i=1 [Ai] for each   , and so

                     

     P[A] = P()[A] = P() [Ai]

                     i=1

                

     =        P()[Ai] = P[Ai],

        i=1     i=1

where we use the fact that we may reverse the order of summation in an infinite

double summation of non-negative terms (see §A7). That proves (ii), and (i) fol-

lows from (ii), applied to the sequence {Ai}i=1, where Ai := Ai \  i-1
P[A] = i=1 P[Ai]  i=1 P[Ai]. 2
                                                                   j=1 Ai, as

                   8.10.2 Conditional probability and independence

All of the definitions and results in §8.2 carry over verbatim to the countably
infinite case. The law of total probability (equations (8.9) and (8.10)), as well
as Bayes' theorem (8.11), extend to families of events {Bi}iI indexed by any
countably infinite set I. The definitions of independent families of events (k-wise
and mutually) extend verbatim to infinite families.

                                 8.10.3 Random variables

All of the definitions and results in §8.3 carry over verbatim to the countably infi-
nite case. Note that the image of a random variable may be either finite or countably
infinite. The definitions of independent families of random variables (k-wise and
mutually) extend verbatim to infinite families.

                             8.10.4 Expectation and variance

We define the expected value of a real-valued random variable X exactly as in
(8.18); that is, E[X] :=  X() P(), but where this sum is now an infinite
series. If this series converges absolutely (see §A6), then we say that X has finite
expectation, or that E[X] is finite. In this case, the series defining E[X] converges
to the same finite limit, regardless of the ordering of the terms.

   If E[X] is not finite, then under the right conditions, E[X] may still exist, although
its value will be ±. Consider first the case where X takes only non-negative
values. In this case, if E[X] is not finite, then we naturally define E[X] := , as the
series defining E[X] diverges to , regardless of the ordering of the terms. In the
general case, we may define random variables X+ and X-, where

              X+() := max{0, X()} and X-() := max{0, -X()},
                         8.10 Discrete probability distributions                           273

so that X = X+ - X-, and both X+ and X- take only non-negative values. Clearly,
X has finite expectation if and only if both X+ and X- have finite expectation.
Now suppose that E[X] is not finite, so that one of E[X+] or E[X-] is infinite. If
E[X+] = E[X-] = , then we say that E[X] does not exist; otherwise, we define
E[X] := E[X+]-E[X-], which is ±; in this case, the series defining E[X] diverges

to ±, regardless of the ordering of the terms.

Example 8.47. Let X be a random variable whose distribution is as in Exam-

ple 8.45. Since the series              1/k2  converges     and   the  series  k=1 1/k diverges,
                                   k=1
the expectation E[X] is finite, while E[X2] = . One may also verify that the

random variable (-1)XX2 has no expectation. 2

All of the results in §8.4 carry over essentially unchanged, although one must

pay some attention to "convergence issues."

   If E[X] exists, then we can regroup the terms in the series  X() P(), with-
out affecting its value. In particular, equation (8.19) holds provided E[X] exists,

and equation (8.20) holds provided E[f (X)] exists.

Theorem 8.14 still holds, under the additional hypothesis that E[X] and E[Y] are

finite. Equation (8.21) also holds, provided the individual expectations E[Xi] are
finite. More generally, if E[X] and E[Y] exist, then E[X + Y] = E[X] + E[Y], unless
E[X] =  and E[Y] = -, or E[X] = - and E[Y] = . Also, if E[X] exists,
then E[aX] = a E[X], unless a = 0 and E[X] = ±.

One might consider generalizing (8.21) to countably infinite families of ran-

dom variables. To this end, suppose {Xi}i=1 is an infinite sequence of real-valued
random variables. The random variable X := i=1 Xi is well defined, provided
the series i=1 Xi() converges for each   . One might hope that E[X] =

i=1  E[Xi  ];  however,  this  is  not  in  general  true,  even  if  the  individual  expectations,

E[Xi], are non-negative, and even if the series defining X converges absolutely for

each ; nevertheless, it is true when the Xi's are non-negative:

Theorem 8.40. Let {Xi}i=1 be an infinite sequence of random variables. Suppose

that for each i  1, Xi takes non-negative values only, and has finite expectation.

Also suppose that        Xi()      converges  for    each          ,   and  define  X  :=  i=1 Xi.
                   i=1

Then we have

                                              

                                        E[X] = E[Xi].

                                              i=1

Proof. This is a calculation just like the one made in the proof of Theorem 8.39,
where, again, we use the fact that we may reverse the order of summation in an
274  Finite and discrete probability distributions

infinite double summation of non-negative terms:

                                                  

     E[X] = P()X() = P() Xi()

                                                  i=1

                

     =        P()Xi() = E[Xi]. 2

        i=1     i=1

   Theorem 8.15 holds under the additional hypothesis that E[X] and E[Y] are finite.
Equation (8.22) also holds, provided the individual expectations E[Xi] are finite.
Theorem 8.16 still holds, of course. Theorem 8.17 also holds, but where now the
sum may be infinite; it can be proved using essentially the same argument as in the
finite case, combined with Theorem 8.40.

Example 8.48. Suppose X is a random variable with a geometric distribution, as

in Example 8.44, with an associated success probability p and failure probabil-
ity q := 1 - p. As we saw in Example 8.46, for every integer i  1, we have
P[X  i] = qi-1. We may therefore apply the infinite version of Theorem 8.17 to

easily compute the expected value of X:

       11
     E[X] = P[X  i] = qi-1 =                           = .2
     i=1 i=1 1 - q p

Example 8.49. To illustrate that Theorem 8.40 does not hold in general, consider

the geometric distribution on the positive integers, where P(j) = 2-j for j  1.
For i  1, define the random variable Xi so that Xi(i) = 2i, Xi(i + 1) = -2i+1,
and Xi(j) = 0 for all j / {i, i + 1}. Then E[Xi] = 0 for all i  1, and so

  i1 E[Xi] = 0. Now define X := i1 Xi. This is well defined, and in fact
X(1) = 2, while X(j) = 0 for all j > 1. Hence E[X] = 1. 2

   The variance Var[X] of X exists only when µ := E[X] is finite, in which case
it is defined as usual as E[(X - µ)2], which may be either finite or infinite. Theo-
rems 8.18, 8.19, and 8.20 hold provided all the relevant expectations and variances
are finite.

   The definition of conditional expectation carries over verbatim. Equation (8.23)
holds, provided E[X | B] exists, and the law of total expectation (8.24) holds, pro-
vided E[X] exists. The law of total expectation also holds for a countably infinite
partition {Bi}iI , provided E[X] exists, and each of the conditional expectations
E[X | Bi] is finite.
8.11 Notes  275

                                8.10.5 Some useful bounds

All of the results in this section hold, provided the relevant expectations and vari-
ances are finite.

EXERCISE 8.72. Let {Ai}i=1 be a family of events, such that Ai  Ai+1 for each
i  1, and let A := i=1 Ai. Show that P[A] = limi P[Ai].

EXERCISE 8.73. Generalize Exercises 8.6, 8.7, 8.23, and 8.24 to the discrete set-
ting, allowing a countably infinite index set I.

EXERCISE 8.74. Suppose X is a random variable taking positive integer values,
and that for some real number q, with 0  q  1, and for all integers i  1, we
have P[X  i] = qi-1. Show that X has a geometric distribution with associated
success probability p := 1 - q.

EXERCISE 8.75. This exercise extends Jensen's inequality (see Exercise 8.25) to
the discrete setting. Suppose that f is a convex function on an interval I. Let X
be a random variable whose image is a countably infinite subset of I, and assume
that both E[X] and E[f (X)] are finite. Show that E[f (X)]  f (E[X]). Hint: use
continuity.

EXERCISE 8.76. A gambler plays a simple game in a casino: with each play of
the game, the gambler may bet any number m of dollars; a fair coin is tossed, and
if it comes up heads, the casino pays m dollars to the gambler, and otherwise, the
gambler pays m dollars to the casino. The gambler plays the game repeatedly,
using the following strategy: he initially bets a dollar, and with each subsequent
play, he doubles his bet; if he ever wins, he quits and goes home; if he runs out of
money, he also goes home; otherwise, he plays again. Show that if the gambler has
an infinite amount of money, then his expected winnings are one dollar, and if he
has a finite amount of money, his expected winnings are zero.

                                          8.11 Notes

The idea of sharing a secret via polynomial evaluation and interpolation (see Exam-
ple 8.28) is due to Shamir [90].

   Our Chernoff bound (Theorem 8.24) is one of a number of different types of
bounds that appear in the literature under the rubric of "Chernoff bound."

   Universal and pairwise independent hash functions, with applications to hash
tables and message authentication codes, were introduced by Carter and Weg-
man [25, 105]. The notions of -almost universal and -almost strongly universal
276  Finite and discrete probability distributions

hashing were developed by Stinson [101]. The notion of -variationally universal
hashing (see Exercise 8.63) is from Krovetz and Rogaway [57].

   The leftover hash lemma (Theorem 8.37) was originally stated and proved by
Impagliazzo, Levin, and Luby [48], who use it to obtain an important result in the
theory of cryptography. Our proof of the leftover hash lemma is loosely based on
one by Impagliazzo and Zuckermann [49], who also present further applications.
                             9

                   Probabilistic algorithms

It is sometimes useful to endow our algorithms with the ability to generate random
numbers. In fact, we have already seen two examples of how such probabilistic
algorithms may be useful:

     · at the end of §3.4, we saw how a probabilistic algorithm might be used to
         build a simple and efficient primality test; however, this test might incor-
         rectly assert that a composite number is prime; in the next chapter, we
         will see how a small modification to this algorithm will ensure that the
         probability of making such a mistake is extremely small;

     · in §4.5, we saw how a probabilistic algorithm could be used to make Fer-
         mat's two squares theorem constructive; in this case, the use of randomiza-
         tion never leads to incorrect results, but the running time of the algorithm
         was only bounded "in expectation."

   We will see a number of other probabilistic algorithms in this text, and it is high
time that we place them on a firm theoretical foundation. To simplify matters,
we only consider algorithms that generate random bits. Where such random bits
actually come from will not be of great concern to us here. In a practical imple-
mentation, one would use a pseudo-random bit generator, which should produce
bits that "for all practical purposes" are "as good as random." While there is a
well-developed theory of pseudo-random bit generation (some of which builds on
the ideas in §8.9), we will not delve into this here. Moreover, the pseudo-random
bit generators used in practice are not based on this general theory, and are much
more ad hoc in design. So, although we will present a rigorous formal theory of
probabilistic algorithms, the application of this theory to practice is ultimately a bit
heuristic; nevertheless, experience with these algorithms has shown that the theory
is a very good predictor of the real-world behavior of these algorithms.

                                               277
278                                 Probabilistic algorithms

                                    9.1 Basic definitions

Formally speaking, we will add a new type of instruction to our random access
machine (described in §3.2):

random bit This type of instruction is of the form   RAND, where  takes the
         same form as in arithmetic instructions. Execution of this type of instruc-
         tion assigns to  a value sampled from the uniform distribution on {0, 1},
         independently from the execution of all other random-bit instructions.

    Algorithms that use random-bit instructions are called probabilistic (or ran-

domized), while those that do not are called deterministic.

                                                                                                                         ¢

    In describing probabilistic algorithms at a high level, we shall write "y  {0, 1}"

to  denote  the  assignment  of  a  random  bit  to  the  variable  y,  and  "y   ¢  {0,  1}×  "  to

                                                                                 

denote the assignment of a random bit string of length to the variable y.

    To analyze the behavior of a probabilistic algorithm, we first need a probability

distribution that appropriately models its execution. Once we have done this, we

shall define the running time and output to be random variables associated with

this distribution.

                              9.1.1 Defining the distribution

It would be desirable to define a probability distribution that could be used for all
algorithms and all inputs. While this can be done in principle, it would require
notions from the theory of probability more advanced than those we developed in
the previous chapter. Instead, for a given probabilistic algorithm A and input x, we
shall define a discrete probability distribution that models A's execution on input
x. Thus, every algorithm/input pair yields a different distribution.

   To motivate our definition, consider Example 8.43. We could view the sample
space in that example to be the set of all bit strings consisting of zero or more
0 bits, followed by a single 1 bit, and to each such bit string  of this special
form, we assign the probability 2-||, where || denotes the length of . The
"random experiment" we have in mind is to generate random bits one at a time until
one of these special "halting" strings is generated. In developing the definition of
the probability distribution for a probabilistic algorithm, we simply consider more
general sets of "halting" strings, as determined by the algorithm and its input.

   So consider a fixed algorithm A and input x. Let  be a finite bit string of length,
say, . We can use  to "drive" the execution of A on input x for up to execution
steps, as follows: for each step i = 1, . . . , , if the ith instruction executed by A
is   RAND, the ith bit of  is assigned to . In this context, we shall refer to
 as an execution path. The reader may wish to visualize  as a finite path in an
                                9.1 Basic definitions           279

infinite binary tree, where we start at the root, branching to the left if the next bit
in  is a 0 bit, and branching to the right if the next bit in  is a 1 bit.

   After using  to drive A on input x for up to steps, we might find that the
algorithm executed a halt instruction at some point during the execution, in which
case we call  a complete execution path; moreover, if this halt instruction was the
 th instruction executed by A, then we call  an exact execution path.

   Our intent is to define the probability distribution associated with A on input x to
be P :   [0, 1], where the sample space  is the set of all exact execution paths,
and P() := 2-|| for each   . However, for this to work, all the probabilities
must sum to 1. The next theorem at least guarantees that these probabilities sum to
at most 1. The only property of  that really matters in the proof of this theorem
is that it is prefix free, which means that no exact execution path is a proper prefix
of any other.

Theorem 9.1. Let  be the set of all exact execution paths for A on input x. Then
   2-||  1.

Proof. Let k be a non-negative integer. Let k   be the set of all exact execution
paths of length at most k, and let k := k 2-||. We shall show below that

                                k  1.                           (9.1)

From this, it will follow that

                                     2-|| = lim k  1.

                                                    k
                                

   To prove the inequality (9.1), consider the set Ck of all complete execution paths
of length equal to k. We claim that

                                k = 2-k|Ck|,                    (9.2)

from which (9.1) follows, since clearly, |Ck|  2k. So now we are left to prove
(9.2). Observe that by definition, each   Ck extends some   k; that is,  is
a prefix of ; moreover,  is uniquely determined by , since no exact execution

path is a proper prefix of any other exact execution path. Also observe that for

each   k, if Ck() is the set of execution paths   Ck that extend , then
|Ck()| = 2k-||, and by the previous observation, {Ck()}k is a partition of

Ck. Thus, we have

k =     2-|| =         2-||            2-k+|| = 2-k             1 = 2-k|Ck|,

     k          k               Ck ()                  k Ck ()

which proves (9.2). 2
280  Probabilistic algorithms

   From the above theorem, if  is the set of all exact execution paths for A on
input x, then

                                       := 2-||  1,

                                                            

and we say that A halts with probability  on input x. If  = 1, we define the
distribution P :   [0, 1] associated with A on input x, where P() := 2-|| for
each   .

   We shall mainly be interested in algorithms that halt with probability 1 on all
inputs. The following four examples provide some simple criteria that guarantee
this.

Example 9.1. Suppose that on input x, A always halts within a finite number of
steps, regardless of its random choices. More precisely, this means that there is
a bound (depending on A and x), such that all execution paths of length are
complete. In this case, we say that A's running time on input x is strictly bounded
by , and it is clear that A halts with probability 1 on input x. Moreover, one can
much more simply model A's computation on input x by working with the uniform
distribution on execution paths of length . 2

Example 9.2. Suppose A and B are probabilistic algorithms that both halt with
probability 1 on all inputs. Using A and B as subroutines, we can form their serial
composition; that is, we can construct the algorithm

         C(x) : output B(A(x)),

which on input x, first runs A on input x, obtaining a value y, then runs B on input
y, obtaining a value z, and finally, outputs z. We claim that C halts with probability
1 on all inputs.

   For simplicity, we may assume that A places its output y in a location in memory
where B expects to find its input, and that B places its output in a location in
memory where C's output should go. With these assumptions, the program for C is
obtained by simply concatenating the programs for A and B, making the following
adjustments: every halt instruction in A's program is translated into an instruction
that branches to the first instruction of B's program, and every target in a branch
instruction in B's program is increased by the length of A's program.

   Let  be the sample space representing A's execution on an input x. Each
   determines an output y, and a corresponding sample space  representing
B's execution on input y. The sample space representing C's execution on input x
is

                               = { :   ,   },
                 9.1 Basic definitions                     281

where  is the concatenation of  and  . We have

        2-| | =    2-||        2-| | = 2-|| · 1 = 1,

                                        

which shows that C halts with probability 1 on input x. 2

Example 9.3. Suppose A, B, and C are probabilistic algorithms that halt with
probability 1 on all inputs, and that A always outputs either true or false. Then we
can form the conditional construct

         D(x) : if A(x) then output B(x) else output C(x).

By a calculation similar to that in the previous example, it is easy to see that D
halts with probability 1 on all inputs. 2

Example 9.4. Suppose A and B are probabilistic algorithms that halt with proba-
bility 1 on all inputs, and that A always outputs either true or false. We can form
the iterative construct

C(x) :  while A(x) do x  B(x)
        output x.

   Algorithm C may or may not halt with probability 1. To analyze C, we define
an infinite sequence of algorithms {Cn}n=0; namely, we define C0 as

         C0(x) : halt,

and for n > 0, we define Cn as

         Cn(x) : if A(x) then Cn-1(B(x)).

Essentially, Cn drives C for up to n loop iterations before halting, if necessary, in
C0. By the previous three examples, it follows by induction on n that each Cn halts
with probability 1 on all inputs. Therefore, we have a well-defined probability
distribution for each Cn and each input x.

   Consider a fixed input x. For each n  0, let n be the probability that on input
x, Cn terminates by executing algorithm C0. Intuitively, n is the probability that C
executes at least n loop iterations; however, this probability is defined with respect
to the probability distribution associated with algorithm Cn on input x. It is not hard
to see that the sequence {n}n=0 is non-increasing, and so the limit  := limn n
exists; moreover, C halts with probability 1 -  on input x.

   On the one hand, if the loop in algorithm C is guaranteed to terminate after a
finite number of iterations (as in a "for loop"), then C certainly halts with proba-
bility 1. Indeed, if on input x, there is a bound (depending on x) such that the
number of loop iterations is always at most , then  +1 =  +2 = · · · = 0. On the
other hand, if on input x, C enters into a good, old-fashioned infinite loop, then C
282                         Probabilistic algorithms

certainly does not halt with probability 1, as 0 = 1 = · · · = 1. Of course, there
may be in-between cases, which require further analysis. 2

   We now illustrate the above criteria with a couple of some simple, concrete
examples.

Example 9.5. Consider the following algorithm, which models an experiment in
which we toss a fair coin repeatedly until it comes up heads:

         repeat

                          ¢

               b  {0, 1}
         until b = 1

   For each positive integer n, let n be the probability that the algorithm executes
at least n loop iterations, in the sense of Example 9.4. It is not hard to see that
n = 2-n+1, and since n  0 as n  , the algorithm halts with probability
1, even though the loop is not guaranteed to terminate after any particular, finite

number of steps. 2

Example 9.6. Consider the following algorithm:

     i0

     repeat

     ii+1

              ¢  {0,  1}×i

             

     until  = 0×i

   For each positive integer n, let n be the probability that the algorithm executes
at least n loop iterations, in the sense of Example 9.4. It is not hard to see that

                   n-1      n-1
             n = (1 - 2-i)  e-2-i+1 = e-              n-2 2-i -2

                                                      i=0  e ,

                   i=1      i=1

where we have made use of the estimate (iii) in §A1. Therefore,

                            lim n  e-2 > 0,

                            n

and so the algorithm does not halt with probability 1, even though it never falls into
an infinite loop. 2

                      9.1.2 Defining the running time and output

Let A be a probabilistic algorithm that halts with probability 1 on a fixed input x.
We may define the random variable Z that represents A's running time on input x,
and the random variable Y that represents A's output on input x.
9.1 Basic definitions  283

   Formally, Z and Y are defined using the probability distribution on the sample
space , defined in §9.1.2. The sample space  consists of all exact execution
paths for A on input x. For each   , Z() := ||, and Y() is the output
produced by A on input x, using  to drive its execution.

   The expected running time of A on input x is defined to be E[Z]. Note that in
defining the expected running time, we view the input as fixed, rather than drawn
from some probability distribution. Also note that the expected running time may
be infinite.

   We say that A runs in expected polynomial time if there exist constants a, b,
and c, such that for all n, and for all inputs x of size n, the expected running time
of A on input x is at most anb + c. We say that A runs in strict polynomial time
if there exist constants a, b, and c, such that for all n, and for all inputs x of size n,
A's running time on input x is strictly bounded by anb + c (as in Example 9.1).

Example 9.7. Consider again the algorithm in Example 9.5. Let L be the random

variable that represents the number of loop iterations executed by the algorithm.

The distribution of L is a geometric distribution, with associated success probability
1/2 (see Example 8.44). Therefore, E[L] = 2 (see Example 8.46). Let Z be the

random variable that represents the running time of the algorithm. We have Z  cL,
for some implementation-dependent constant c. Therefore, E[Z]  c E[L] = 2c. 2

Example 9.8. Consider the following probabilistic algorithm that takes as input a
positive integer m. It models an experiment in which we toss a fair coin repeatedly
until it comes up heads m times.

         k0
         repeat

                          ¢

               b  {0, 1}
               if b = 1 then k  k + 1
         until k = m

Let L be the random variable that represents the number of loop iterations executed
the algorithm on a fixed input m. We claim that E[L] = 2m. To see this, define

random variables L1, . . . , Lm, where L1 is the number of loop iterations needed to
get b = 1 for the first time, L2 is the number of additional loop iterations needed
to get b = 1 for the second time, and so on. Clearly, we have L = L1 + · · · + Lm,
and moreover, E[Li] = 2 for i = 1, . . . , m; therefore, by linearity of expectation, we
have E[L] = E[L1] + · · · + E[Lm] = 2m. It follows that the expected running time

of this algorithm on input m is O(m). 2
284                           Probabilistic algorithms

Example 9.9. Consider the following algorithm:

     n0

                                           ¢

     repeat n  n + 1, b  {0, 1} until b = 1

     repeat     ¢  {0,  1}×n  until  = 0×n

               

The expected running time is infinite (even though it does halt with probability 1).

To see this, define random variables L1 and L2, where L1 is the number of iterations

of the first loop, and L2 is the number of iterations of the second. As in Exam-

ple 9.7, the distribution of L1 is a geometric distribution with associated success
probability 1/2, and E[L1] = 2. For each k  1, the conditional distribution of L2
given L1 = k is a geometric distribution with associated success probability 1/2k,
and so E[L2 | L1 = k] = 2k. Therefore,

     E[L2] = E[L2 | L1 = k] P[L1 = k] = 2k · 2-k = 1 = . 2

             k1                             k1          k1

   We have presented a fairly rigorous definitional framework for probabilistic
algorithms, but from now on, we shall generally reason about such algorithms at a
higher, and more intuitive, level. Nevertheless, all of our arguments can be trans-
lated into this rigorous framework, the details of which we leave to the interested
reader. Moreover, all of the algorithms we shall present halt with probability 1 on
all inputs, but we shall not go into the details of proving this (but the criteria in
Examples 9.1-9.4 can be used to easily verify this).

EXERCISE 9.1. Suppose A is a probabilistic algorithm that halts with probability
1 on input x, and let P :   [0, 1] be the corresponding probability distribution.
Let  be an execution path of length , and assume that no proper prefix of  is
exact. Let E := {   :  extends }. Show that P[E] = 2- .

EXERCISE 9.2. Let A be a probabilistic algorithm that on a given input x, halts
with probability 1, and produces an output in the set T . Let P be the correspond-
ing probability distribution, and let Y and Z be random variables representing the
output and running time, respectively. For each k  0, let Pk be the uniform
distribution on all execution paths  of length k. We define random variables Yk
and Zk, associated with Pk, as follows: if  is complete, we define Yk() to be
the output produced by A, and Zk() to be the actual number of steps executed by
A; otherwise, we define Yk() to be the special value "" and Zk() to be k. For
each t  T , let ptk be the probability (relative to Pk) that Yk = t, and let µk be the
expected value (relative to Pk) of Zk. Show that:

    (a) for each t  T , P[Y = t] = lim ptk;

                                                         k
        9.2 Generating a random number from a given interval  285

(b) E[Z] = lim µk.

                    k

EXERCISE 9.3. Let A1 and A2 be probabilistic algorithms. Let B be any proba-
bilistic algorithm that always outputs 0 or 1. For i = 1, 2, let Ai be the algorithm
that on input x computes and outputs B(Ai(x)). Fix an input x, and let Y1 and Y2

be random variables representing the outputs of A1 and A2, respectively, on input

x, and let Y1 and Y2 be random variables representing the outputs of A1 and A2,
respectively, on input x. Assume that the images of Y1 and Y2 are finite, and let
 := [Y1; Y2] be their statistical distance. Show that |P[Y1 = 1] - P[Y2 = 1]|  .

             9.2 Generating a random number from a given interval

Suppose we want to generate a number, uniformly at random from the interval
{0, . . . , m - 1}, for a given positive integer m.

   If m is a power of 2, say m = 2 , then we can do this directly as follows: generate
a random -bit string , and convert  to the integer I () whose base-2 represen-
tation is ; that is, if  = b -1b -2 · · · b0, where the bi's are bits, then

                                                                      -1

                                       I () := bi2i.

                                                                    i=0

   In the general case, we do not have a direct way to do this, since we can only
directly generate random bits. But the following algorithm does the job:

Algorithm RN. On input m, where m is a positive integer, do the following, where
  := log2 m :

repeat

         ¢  {0,  1}×

        

y  I()

until y < m

output y

Theorem 9.2. The expected running time of Algorithm RN is O(len(m)), and its
output is uniformly distributed over {0, . . . , m - 1}.

Proof. Note that m  2 < 2m. Let L denote the number of loop iterations of this

algorithm, and Z its running time. With every loop iteration, the algorithm halts

with probability m/2 , and so the distribution of L is a geometric distribution with
associated success probability m/2 > 1/2. Therefore, E[L] = 2 /m < 2. Since
Z  c len(m) · L for some constant c, it follows that E[Z] = O(len(m)).

   Next, we analyze the output distribution. Let Y denote the output of the algo-

rithm. We want to show that Y is uniformly distributed over {0, . . . , m - 1}. This
286                        Probabilistic algorithms

is perhaps intuitively obvious, but let us give a rigorous justification of this claim.
To do this, for i = 1, 2, . . . , let Yi denote the value of y in the ith loop iteration;
for completeness, if the ith loop iteration is not executed, then we define Yi := .
Also, for i = 1, 2 . . . , let Hi be the event that the algorithm halts in the ith loop
iteration (i.e., Hi is the event that L = i). Let t  {0, . . . , m - 1} be fixed.

   First, by total probability (specifically, the infinite version of (8.9), discussed in

§8.10.2), we have

     P[Y = t] = P[(Y = t)  Hi] = P[(Yi = t)  Hi].                            (9.3)

         i1                       i1

Next, observe that as each loop iteration works the same as any other, it follows
that for each i  1, we have

     P[(Yi = t)  Hi | L  i] = P[(Y1 = t)  H1] = P[Y1 = t] = 2- .

Moreover, since Hi implies L  i, we have
        P[(Yi = t)  Hi] = P[(Yi = t)  Hi  (L  i)]
                             = P[(Yi = t)  Hi | L  i] P[L  i] = 2- P[L  i],

and so using (9.3) and the infinite version of Theorem 8.17 (discussed in §8.10.4),
we have

     P[Y = t] = P[(Yi = t)  Hi] = 2- P[L  i] = 2- P[L  i]

     i1                    i1                        i1

     = 2- · E[L] = 2- · 2 /m = 1/m.

This shows that Y is uniformly distributed over {0, . . . , m - 1}. 2

   Of course, by adding an appropriate value to the output of Algorithm RN, we can
generate random numbers uniformly in the interval {m1, . . . , m2}, for any given m1
and m2. In what follows, we shall denote the execution of this algorithm as

                                                         ¢

                                      y  {m1, . . . , m2}.

More generally, if T is any finite, non-empty set for which we have an efficient
algorithm whose output is uniformly distributed over T , we shall denote the exe-
cution of this algorithm as

                                                                  ¢

                                             y  T.

For example, we may write

                               ¢

                           y  Zm

to denote assignment to y of a randomly chosen element of Zm. Of course, this
                      9.3 The generate and test paradigm                       287

is done by running Algorithm RN on input m, and viewing its output as a residue
class modulo m.

   We also mention the following alternative algorithm for generating an almost-
random number from an interval.

Algorithm RN . On input m, k, where both m and k are positive integers, do the
following, where := log2 m :

   ¢  {0,  1}×(  +k)

  

y  I () mod m

output y

   Compared with Algorithm RN, Algorithm RN has the advantage that there are
no loops--it always halts in a bounded number of steps; however, it has the disad-
vantage that its output is not uniformly distributed over the interval {0, . . . , m - 1}.
Nevertheless, the statistical distance between its output distribution and the uniform
distribution on {0, . . . , m - 1} is at most 2-k (see Example 8.41 in §8.8). Thus,
by choosing k suitably large, we can make the output distribution "as good as
uniform" for most practical purposes.

EXERCISE 9.4. Prove that if m is not a power of 2, there is no probabilistic
algorithm whose running time is strictly bounded and whose output distribution
is uniform on {0, . . . , m - 1}.

EXERCISE 9.5. You are to design and analyze an efficient probabilistic algorithm
B that takes as input two integers n and y, with n > 0 and 0  y  n, and always
outputs 0 or 1. Your algorithm should satisfy the following property. Suppose A is a
probabilistic algorithm that takes two inputs, n and x, and always outputs an integer
between 0 and n. Let Y be a random variable representing A's output on input n, x.
Then for all inputs n, x, we should have P[B(n, A(n, x)) outputs 1] = E[Y]/n.

                          9.3 The generate and test paradigm

Algorithm RN, which was discussed in §9.2, is a specific instance of a very general
type of construction that may be called the "generate and test" paradigm.

   Suppose we have two probabilistic algorithms, A and B, and we combine them
to form a new algorithm

C(x) :     repeat y  A(x) until B(x, y)
           output y.

Here, we assume that B(x, y) always outputs either true or false.
   Our goal is to answer the following questions about C for a fixed input x:
288          Probabilistic algorithms

     1. Does C halt with probability 1?

     2. What is the expected running time of C?

     3. What is the output distribution of C?

   The answer to the first question is "yes," provided (i) A halts with probability
1 on input x, (ii) for all possible outputs y of A(x), B halts with probability 1 on
input (x, y), and (iii) for some possible output y of A(x), B(x, y) outputs true with
non-zero probability. We shall assume this from now on.

   To address the second and third questions, let us define random variables L, Z,
and Y, where L is the total number of loop iterations of C, Z is the total running
time of C, and Y is the output of C. We can reduce the study of L, Z, and Y to
the study of a single iteration of the main loop. Instead of working with a new
probability distribution that directly models a single iteration of the loop, it is more
convenient to simply study the first iteration of the loop in C. To this end, we define
random variables Z1 and Y1, where Z1 is the running time of the first loop iteration
of C, and Y1 is the value assigned to y in the first loop iteration of C. Also, let H1
be the event that the algorithm halts in the first loop iteration, and let T be the set of
possible outputs of A(x). Note that by the assumption in the previous paragraph,
P[H1] > 0.

Theorem 9.3. Under the assumptions above,

    (i) L has a geometric distribution with associated success probability P[H1],
         and in particular, E[L] = 1/ P[H1];

   (ii) E[Z] = E[Z1] E[L] = E[Z1]/ P[H1];
   (iii) for every t  T , P[Y = t] = P[Y1 = t | H1].

Proof. (i) is clear.
   To prove (ii), for i  1, let Zi be the time spent by the algorithm in the ith loop

iteration, so that Z = i1 Zi. Now, the conditional distribution of Zi given L  i
is (essentially) the same as the distribution of Z1; moreover, Zi = 0 when L < i.
Therefore, by the law of total expectation (8.24), for each i  1, we have

      E[Zi] = E[Zi | L  i] P[L  i] + E[Zi | L < i] P[L < i] = E[Z1] P[L  i].

We may assume that E[Z1] is finite, as otherwise (ii) is trivially true. By Theo-
rem 8.40 and the infinite version of Theorem 8.17 (discussed in §8.10.4), we have

     E[Z] = E[Zi] = E[Z1] P[L  i] = E[Z1] P[L  i] = E[Z1] E[L].

     i1  i1  i1

  To prove (iii), for i  1, let Yi be the value assigned to y in loop iteration i, with
Yi :=  if L < i, and let Hi be the event that the algorithm halts in loop iteration i
    9.3 The generate and test paradigm                      289

(i.e., Hi is the event that L = i). By a calculation similar to that made in the proof
of Theorem 9.2, for each t  T , we have

P[Y = t] = P[(Y = t)  Hi] = P[(Yi = t)  Hi | L  i] P[L  i]

i1      i1

= P[(Y1 = t)  H1] P[L  i] = P[(Y1 = t)  H1] · E[L]

    i1

= P[(Y1 = t)  H1]/ P[H1] = P[Y1 = t | H1]. 2

Example 9.10. Suppose T is a finite set, and T is a non-empty, finite subset of T .
Consider the following generalization of Algorithm RN:

         repeat

                          ¢

               yT
         until y  T
         output y

Here, we assume that we have an algorithm to generate a random element of T (i.e.,
uniformly distributed over T ), and an efficient algorithm to test for membership in
T . Let L denote the number of loop iterations, and Y the output. Also, let Y1 be
the value of y in the first iteration, and H1 the event that the algorithm halts in the
first iteration. Since Y1 is uniformly distributed over T , and H1 is the event that
Y1  T , we have P[H1] = |T |/|T |. It follows that E[L] = |T |/|T |. As for the
output, for every t  T , we have

                    P[Y = t] = P[Y1 = t | H1] = P[Y1 = t | Y1  T ],

which is 0 if t / T and is 1/|T | if t  T . It follows that Y is uniformly distributed
over T . 2

Example 9.11. Let us analyze the following algorithm:

         repeat

                          ¢

               y  {1, 2, 3, 4}

                          ¢

               z  {1, . . . , y}
         until z = 1
         output y

With each loop iteration, the algorithm chooses y uniformly at random, and then
decides to halt with probability 1/y. Let L denote the number of loop iterations,
and Y the output. Also, let Y1 be the value of y in the first iteration, and H1 the
event that the algorithm halts in the first iteration. Y1 is uniformly distributed over
290       Probabilistic algorithms

{1, . . . , 4}, and for t = 1, . . . , 4, P[H1 | Y1 = t] = 1/t. Therefore,

     4    4

     P[H1] = P[H1 | Y1 = t] P[Y1 = t] = (1/t)(1/4) = 25/48.

     t=1  t=1

Thus, E[L] = 48/25. For the output distribution, for t = 1, . . . , 4, we have

     P[Y = t] = P[Y1 = t | H1] = P[(Y1 = t)  H1]/ P[H1]
                = P[H1 | Y1 = t] P[Y1 = t]/ P[H1] = (1/t)(1/4)(48/25) = 12 .
                                                                                         25t

This example illustrates how a probabilistic test can be used to create a biased
output distribution. 2

EXERCISE 9.6. Design and analyze an efficient probabilistic algorithm that takes
as input an integer n  2, and outputs a random element of Zn.

EXERCISE 9.7. Consider the following probabilistic algorithm that takes as input
a positive integer m:

         S
         repeat

                          ¢

               n  {1, . . . , m}, S  S  {n}
         until |S| = m

Show that the expected number of iterations of the main loop is  m log m.

EXERCISE 9.8. Consider the following algorithm (which takes no input):

         j1
         repeat

                                             ¢

               j  j + 1, n  {0, . . . , j - 1}
         until n = 0

Show that the expected running time of this algorithm is infinite (even though it
does halt with probability 1).

EXERCISE 9.9. Now consider the following modification to the algorithm in the
previous exercise:

         j2
         repeat

                                             ¢

               j  j + 1, n  {0, . . . , j - 1}
         until n = 0 or n = 1

Show that the expected running time of this algorithm is finite.
                      9.3 The generate and test paradigm  291

EXERCISE 9.10. Consider again Algorithm RN in §9.2. On input m, this algorithm
may use up to  2 random bits on average, where := log2 m . Indeed, each
loop iteration generates random bits, and the expected number of loop iterations
will be  2 when m  2 -1. This exercise asks you to analyze an alternative
algorithm that uses just + O(1) random bits on average, which may be useful in

settings where random bits are a scarce resource. This algorithm runs as follows:

    repeat

       y  0, i  1

       while y < m and i  do

()                ¢       y  y + 2 -ib,  ii+1

              b  {0, 1},

    until y < m

    output y

Define random variables K and Y, where K is the number of times the line marked
() is executed, and Y is the output. Show that E[K] = + O(1) and that Y is

uniformly distributed over {0, . . . , m - 1}.

EXERCISE 9.11. Let S and T be finite, non-empty sets, and let f : S × T 
{-1, 0, 1} be a function. Consider the following probabilistic algorithm:

    ¢         ¢
    x  S, y  T

    if f (x, y) = 0 then

       y y

    else

                   ¢

          y T

()     while f (x, y ) = 0 do y  T¢

Here, we assume we have algorithms to generate random elements in S and T , and
a deterministic algorithm to evaluate f . Define random variables X, Y, Y , and L,
where X is the value assigned to x, Y is the value assigned to y, Y is the final value
assigned to y , and L is the number of times that f is evaluated at the line marked
().

    (a) Show that (X, Y ) has the same distribution as (X, Y).

   (b) Show that E[L]  1.

    (c) Give an explicit example of S, T , and f, such that if the line marked () is
         deleted, then E[f (X, Y)] > E[f (X, Y )] = 0.
292                           Probabilistic algorithms

                            9.4 Generating a random prime

Suppose we are given an integer m  2, and want to generate a random prime
between 2 and m. One way to proceed is simply to generate random numbers
until we get a prime. This idea will work, assuming the existence of an efficient,
deterministic algorithm IsPrime that determines whether or not a given integer is
prime. We will present such an algorithm later, in Chapter 21. For the moment,
we shall just assume we have such an algorithm, and use it as a "black box." Let
us assume that on inputs of bit length at most , IsPrime runs in time at most ( ).
Let us also assume (quite reasonably) that ( ) = ( ).

Algorithm RP. On input m, where m is an integer  2, do the following:

     repeat

                   ¢

           n  {2, . . . , m}
     until IsPrime(n)
     output n

   We now wish to analyze the running time and output distribution of Algo-
rithm RP on an input m, where := len(m). This is easily done, using the results of

§9.3, and more specifically, by Example 9.10. The expected number of loop itera-

tions performed by Algorithm RP is (m - 1)/(m), where (m) is the number of
primes up to m. By Chebyshev's theorem (Theorem 5.1), (m) = (m/ ). It fol-
lows that the expected number of loop iterations is ( ). Furthermore, the expected

running time of any one loop iteration is O(( )) (the expected running time for
generating n is O( ), and this is where we use the assumption that ( ) = ( )).

It follows that the expected total running time is O( ( )). As for the output, it is

clear that it is uniformly distributed over the set of primes up to m.

                        9.4.1 Using a probabilistic primality test
In the above analysis, we assumed that IsPrime was an efficient, deterministic
algorithm. While such an algorithm exists, there are in fact simpler and far more
efficient primality tests that are probabilistic. We shall discuss such an algorithm in
detail in the next chapter. This algorithm (like several other probabilistic primality
tests) has one-sided error, in the following sense: if the input n is prime, then
the algorithm always outputs true; otherwise, if n is composite, the output may be
true or false, but the probability that the output is true is at most , where  is a
very small number (the algorithm may be easily tuned to make  quite small, e.g.,
2-100 ).

   Let us analyze the behavior of Algorithm RP under the assumption that IsPrime
is implemented by a probabilistic algorithm with an error probability for composite
                9.4 Generating a random prime                            293

inputs bounded by , as discussed in the previous paragraph. Let ( ) be a bound
on the expected running time of this algorithm for all inputs of bit length at most .
Again, we assume that ( ) = ( ).

   We use the technique developed in §9.3. Consider a fixed input m, and let
  := len(m). Let L, Z, and N be random variables representing, respectively, the
number of loop iterations, the total running time, and output of Algorithm RP on
input m. Also, let Z1 be the random variable representing the running time of
the first loop iteration, and let N1 be the random variable representing the value
assigned to n in the first loop iteration. Let H1 be the event that the algorithm halts
in the first loop iteration, and let C1 be the event that N1 is composite.
   Clearly, N1 is uniformly distributed over {2, . . . , m}. Also, by our assumptions
about IsPrime, we have

                                       E[Z1] = O(( )),

and moreover, for each j  {2, . . . , m}, we have
                           P[H1 | N1 = j]   if j is composite,

and
                             P[H1 | N1 = j] = 1 if j is prime.

In particular,

                P[H1 | C1]   and P[H1 | C1] = 1.

It follows that

          P[H1] = P[H1 | C1] P[C1] + P[H1 | C1] P[C1]  P[H1 | C1] P[C1]
                  = (m)/(m - 1).

Therefore,

                E[L]  (m - 1)/(m) = O( )

and
                              E[Z] = E[L] E[Z1] = O( ( )).

   That takes care of the running time. Now consider the output.  For every
j  {2, . . . , m}, we have

                                P[N = j] = P[N1 = j | H1].
294                  Probabilistic algorithms

If j is prime, then

     P[N = j] = P[N1 = j | H1] = P[(N1 = j)  H1]
                                        P[H1]
                     = P[H1 | N1 = j] P[N1 = j] =
                                                   1
                                                            .
                     P[H1]                         (m - 1) P[H1]

   Thus, every prime is output with equal probability; however, the algorithm may
also output a number that is not prime. Let us bound the probability of this
event. One might be tempted to say that this happens with probability at most
; however, in drawing such a conclusion, one would be committing the fallacy of
Example 8.13--to correctly analyze the probability that Algorithm RP mistakenly
outputs a composite, one must take into account the rate of incidence of the "pri-
mality disease," as well as the error rate of the test for this disease. Indeed, if C is
the event that N is composite, then we have

     P[C] = P[C1 | H1] = P[C1  H1] = P[H1 | C1] P[C1]
                     P[H1]                         P[H1]
                         = O( ).
                     P[H1] (m)/(m - 1)

   Another way of analyzing the output distribution of Algorithm RP is to consider
its statistical distance  from the uniform distribution on the set of primes between

2 and m. As we have already argued, every prime between 2 and m is equally likely

to be output, and in particular, any fixed prime is output with probability at most
1/(m). It follows from Theorem 8.31 that  = P[C] = O( ).

                         9.4.2 Generating a random -bit prime

Instead of generating a random prime between 2 and m, we may instead want to
generate a random -bit prime, that is, a prime between 2 -1 and 2 - 1. Bertrand's
postulate (Theorem 5.8) tells us that there exist such primes for every  2,
and that in fact, there are (2 / ) such primes. Because of this, we can modify
Algorithm RP, so that each candidate n is chosen at random from the interval
{2 -1, . . . , 2 - 1}, and all of the results for that algorithm carry over essentially
without change. In particular, the expected number of trials until the algorithm
halts is O( ), and if a probabilistic primality test as in §9.4.1 is used, with an error
probability of , the probability that the output is not prime is O( ).

EXERCISE 9.12. Suppose Algorithm RP is implemented using an imperfect ran-
dom number generator, so that the statistical distance between the output distribu-
tion of the random number generator and the uniform distribution on {2, . . . , m} is
9.5 Generating a random non-increasing sequence    295

equal to  (e.g., Algorithm RN in §9.2). Assume that 2 < (m)/(m - 1). Also,

let µ denote the expected number of iterations of the main loop of Algorithm RP,
let  denote the statistical distance between its output distribution and the uniform
distribution on the primes up to m, and let := len(m).

    (a) Assuming the primality test is deterministic, show that µ = O( ) and
          = O( ).

   (b) Assuming the primality test is probabilistic, with one-sided error , as in
         §9.4.1, show that µ = O( ) and  = O(( + ) ).

                 9.5 Generating a random non-increasing sequence

The following algorithm will be used in the next section as a fundamental subrou-
tine in a beautiful algorithm (Algorithm RFN) that generates random numbers in
factored form.

Algorithm RS. On input m, where m is an integer  2, do the following:

         n0  m
         k0
         repeat

               k  k+1

                            ¢

               nk  {1, . . . , nk-1}
         until nk = 1
         output (n1, . . . , nk)

   We analyze first the output distribution, and then the running time.

9.5.1 Analysis of the output distribution

Let N1, N2, . . . be random variables denoting the choices of n1, n2, . . . (for com-
pleteness, define Ni := 1 if loop i is never entered).

   A particular output of the algorithm is a non-increasing sequence (j1, . . . , jh),
where j1  j2  · · ·  jh-1 > jh = 1. For any such sequence, we have

      h      h

P (Nv = jv)  = P[N1 = j1] · P Nv = jv | (Nw = jw)

    v=1      v=2              w<v

             = 1 · 1 ··· 1 .                       (9.4)
                m j1 jh-1

   This completely describes the output distribution, in the sense that we have
determined the probability with which each non-increasing sequence appears as
an output. However, there is another way to characterize the output distribution
296        Probabilistic algorithms

that is significantly more useful. For j = 2, . . . , m, define the random variable Oj

to be the number of occurrences of the integer j in the output sequence. The Oj's
determine the Ni's, and vice versa. Indeed, Om = em, . . . , O2 = e2 if and only if the

output of the algorithm is the sequence

     (m, . . . , m, m - 1, . . . , m - 1, . . . , 2, . . . , 2, 1).

     em times em-1 times                       e2 times

From (9.4), we can therefore directly compute

              m                                  1m 1                          (9.5)
                                              = m jej .
        P (Oj = ej)
                                                      j=2
            j=2

Moreover, we can write 1/m as a telescoping product,

     1 m-1 m-2                                21 m
     =  ·       · · · · · · = (1 - 1/j),
     m m m-1                                  3 2 j=2

and so re-write (9.5) as                           m                           (9.6)

                                           m  = j-ej (1 - 1/j).

                           P (Oj = ej)            j=2

                                         j=2

Notice that for j = 2, . . . , m,

               j-ej (1 - 1/j) = 1,

           ej 0

and so by (a discrete version of) Theorem 8.7, the family of random variables
{Oj}jm=2 is mutually independent, and for each j = 2, . . . , m and each integer
ej  0, we have

        P[Oj = ej] = j-ej (1 - 1/j).                                           (9.7)

In summary, we have shown:

      that the family {Oj}jm=2 is mutually independent, where for each
      j = 2, . . . , m, the variable Oj + 1 has a geometric distribution with
      an associated success probability of 1 - 1/j.

   Another, perhaps more intuitive, analysis of the distribution of the Oj's runs as
follows. Conditioning on the event Om = em, . . . , Oj+1 = ej+1, one sees that the

value of Oj is the number of times the value j appears in the sequence Ni, Ni+1, . . . ,
where i = em+· · ·+ej+1+1; moreover, in this conditional probability distribution, it

is not too hard to convince oneself that Ni is uniformly distributed over {1, . . . , j}.
Hence the probability that Oj = ej in this conditional probability distribution is the
        9.5 Generating a random non-increasing sequence                   297

probability of getting a run of exactly ej copies of the value j in an experiment in
which we successively choose numbers between 1 and j at random, and this latter
probability is clearly j-ej (1 - 1/j).

                            9.5.2 Analysis of the running time

Let := len(m), and let K be the random variable that represents the number of

loop iterations performed by the algorithm. With the random variables O2, . . . , Om
defined as above, we can write K = 1 + jm=2 Oj. Moreover, for each j, Oj + 1 has
a geometric distribution with associated success probability 1 - 1/j, and hence

            E[Oj] = 1 - 1 = 1 .
            1 - 1/j             j-1

Thus,

        m m-1 1 m dy
        E[K] = 1 + E[Oj] = 1 +  2+   = log m + 2,
        j=2 j=1 j 1 y

where we have estimated the sum by an integral (see §A5).
   Intuitively, this is roughly as we would expect, since with probability 1/2, each

successive ni is at most one half as large as its predecessor, and so after O( ) steps,
we expect to reach 1.

   Let Z be the total running time of the algorithm. We may bound E[Z] using
essentially the same argument that was used in the proof of Theorem 9.3. First,
write Z = i1 Zi, where Zi is the time spent in the ith loop iteration. Each loop
iteration, if executed at all, runs in expected time O( ). That is, there exists a
constant c, such that for each i  1,

        E[Zi | K  i]  c and E[Zi | K < i] = 0.

Thus,

       E[Zi] = E[Zi | K  i] P[K  i] + E[Zi | K < i] P[K < i]  c P[K  i],

and so

        E[Z] = E[Zi]  c P[K  i] = c E[K] = O( 2).

        i1  i1

In summary, we have shown:
      the expected running time of Algorithm RS on -bit inputs is O( 2).

EXERCISE 9.13. Show that when Algorithm RS runs on input m, the expected
number of (not necessarily distinct) primes in the output sequence is  log log m.
298                  Probabilistic algorithms

                     9.6 Generating a random factored number
We now present an efficient algorithm that generates a random factored number.
That is, on input m  2, the algorithm generates a number y uniformly distributed
over the interval {1, . . . , m}, but instead of the usual output format for such a num-
ber y, the output consists of the prime factorization of y.

   As far as anyone knows, there are no efficient algorithms for factoring large
numbers, despite years of active research in search of such an algorithm. So our
algorithm to generate a random factored number will not work by generating a
random number and then factoring it.

   Our algorithm will use Algorithm RS in §9.5 as a subroutine. In addition, as
we did in §9.4, we shall assume the existence of an efficient, deterministic primal-
ity test IsPrime. In the analysis of the algorithm, we shall make use of Mertens'
theorem, which we proved in Chapter 5 (Theorem 5.13).

Algorithm RFN. On input m, where m is an integer  2, do the following:

     repeat

     run Algorithm RS on input m, obtaining (n1, . . . , nk)

()   let (p1, . . . , pr) be the subsequence of primes in (n1, . . . , nk)

()   y  p1 · · · pr

     if y  m then

                  ¢

              x  {1, . . . , m}

              if x  y then output (p1, . . . , pr) and halt

     forever

Notes:
   () For i = 1, . . . , k - 1, the number ni is tested for primality using algorithm
         IsPrime. The sequence (n1, . . . , nk) may contain duplicates, and if these are
         prime, they are appear in (p1, . . . , pr) with the same multiplicity.

  () We assume that the product is computed by a simple iterative procedure
         that halts as soon as the partial product exceeds m. This ensures that the
         time spent forming the product is always O(len(m)2), which simplifies the
         analysis.

   We now analyze the running time and output distribution of Algorithm RFN on
input m, using the generate-and-test paradigm discussed in §9.3; here, the "gen-
erate" part consists of the first two lines of the loop body, which generates the
sequence (p1, . . . , pr), while the "test" part consists of the last four lines of the loop
body.

   Let := len(m). We assume that each call to IsPrime takes time at most ( ),
and for simplicity, we assume ( ) = ( ).

   Let K1 be the value of k in the first loop iteration, Z1 be the running time of
        9.6 Generating a random factored number                      299

the first loop iteration, Y1 be the value of y in the first loop iteration, and H1 be
the event that the algorithm halts in the first loop iteration. Also, let Z be the total
running time of the algorithm, and let Y be the value of y in the last loop iteration
(i.e., the number whose factorization is output).

   We begin with three preliminary calculations.
   First, let t = 1, . . . , m be a fixed integer, and let us calculate the probability that
Y1 = t. Suppose t = pm pep is the prime factorization of t. Let O2, . . . , Om be
random variables as defined in §9.5, so that Oj represents the number of occur-
rences of j in the output sequence of the first invocation of Algorithm RS. Then
Y1 = t if and only if Op = ep for all primes p  m, and so by the analysis in §9.5,
we have

                         P[Y1 = t] = p-ep (1 - 1/p) = g(m) ,
                                         pm t

where
                                    g(m) := (1 - 1/p).

                                                                pm

   Second, we calculate P[H1]. Observe that for t = 1, . . . , m, we have

                                    P[H1 | Y1 = t] = t/m,

and so

        m m t g(m)
        P[H1] = P[H1 | Y1 = t] P[Y1 = t] =         = g(m).
        t=1 t=1 m t

   Third, let t = 1, . . . , m be a fixed integer, and let us calculate the conditional
probability that Y1 = t given H1. We have

        P[Y1 = t | H1] = P[(Y1 = t)  H1] = P[H1 | Y1 = t] P[Y1 = t]
        P[H1]                               P[H1]

        = (t/m)(g(m)/t) = 1 .
        g(m)   m

   We may now easily analyze the output distribution of Algorithm RFN. By The-
orem 9.3, for each t = 1, . . . , m, we have

        P[Y = t] = P[Y1 = t | H1] = 1 ,
                                           m

which shows that the output is indeed uniformly distributed over all integers in
{1, . . . , m}, represented in factored form.

   Finally, we analyze the expected running time of Algorithm RFN. It is easy to
300  Probabilistic algorithms

see that E[Z1] = O(E[K1]( ) + 2), and by the analysis in §9.5, we know that
E[K1] = O( ), and hence E[Z1] = O( ( )). By Theorem 9.3, we have

     E[Z] = E[Z1]/ P[H1] = E[Z1]g(m)-1.

By Mertens' theorem, g(m)-1 = O( ). We conclude that

     E[Z] = O( 2( )).

That is, the expected running time of Algorithm RFN is O( 2( )).

                      9.6.1 Using a probabilistic primality test ()

Analogous to the discussion in §9.4.1, we can analyze the behavior of Algo-
rithm RFN under the assumption that IsPrime is a probabilistic algorithm which
may erroneously indicate that a composite number is prime with probability at most
. Let := len(m), and as we did in §9.4.1, let ( ) be a bound on the expected
running time of IsPrime for all inputs of bit length at most (and again, assume
( ) = ( )).

   The random variables K1, Z1, Y1, Z, Y and the event H1 are defined as above. Let
us also define F1 to be the event that the primality test makes a mistake in the first
loop iteration, and F to be the event that the output of the algorithm is not a list of
primes. Let  := P[F1].

   Again, we begin with three preliminary calculations.
   First, let t = 1, . . . , m be fixed and let us calculate P[(Y1 = t)  F1]. To do
this, define the random variable Y1 to be the product of the actual primes among
the output of the first invocation of Algorithm RS (because the primality test may
err, Y1 may contain additional factors). Evidently, the events (Y1 = t)  F1 and
(Y1 = t)  F1 are the same. Moreover, we claim that the events Y1 = t and F1 are
independent. To see this, recall that the family {Oj}jm=2 is mutually independent,
and also observe that the event Y1 = t depends only on the random variables Oj,
where j is prime, while the event F1 depends only on the random variables Oj,
where j is composite, along with the execution paths of IsPrime on corresponding
inputs. Thus, by a calculation analogous to one we made above,

                  P[(Y1 = t)  F 1] = P[Y1 = t] P[F 1] = g(m) t (1 - ).

   Second, we calculate P[H1  F1]. Observe that for t = 1, . . . , m, we have

                                P[H1 | (Y1 = t)  F 1] = t/m,
           9.6 Generating a random factored number                        301

and so

              m

           P[H1  F 1] = P[H1  (Y1 = t)  F 1]

              t=1

              m

           = P[H1 | (Y1 = t)  F 1] P[(Y1 = t)  F 1]

              t=1

           =  m t g(m)               (1 - ) = g(m)(1 - ).

              t=1 m t

   Third, let t = 1, . . . , m be a fixed integer, and let us calculate the conditional
probability that (Y1 = t)  F1 given H1. We have

         P[(Y1 = t)  F 1 | H1] = P[(Y1 = t)  F 1  H1]
                                                 P[H1]

                   = P[H1 | (Y1 = t)  F 1] P[(Y1 = t)  F 1]
                                          P[H1]

                   = (t/m)((1 - )g(m)/t) = g(m)(1 - ) .
                                     P[H1]                m P[H1]

   We may now easily analyze the output distribution of Algorithm RFN. By The-
orem 9.3, for each t = 1, . . . , m, we have

           P[(Y = t)  F] = P[(Y1 = t)  F1 | H1] = g(m)(1 - ) .
                                                                 m P[H1]

Thus, every integer between 1 and m is equally likely to be output by Algo-

rithm RFN in correct factored form.

Let us also calculate an upper bound on the probability P[F] that Algorithm RFN

outputs an integer that is not in correct factored form. Making use of Exercise 8.1,

we have

           P[F1 | H1] = P[F1  H1]  P[F1] .
                                     P[H1]  P[F1  H1]

Moreover,

           P[F1  H1] = P[F1] + P[H1  F 1] =  + g(m)(1 - )
                          g(m) + g(m)(1 - ) = g(m).

By Theorem 9.3, it follows that
                               P[F] = P[F1 | H1]  /g(m).

Now, the reader may verify that

                      · (E[K1] - 1),
302            Probabilistic algorithms

and by our calculations in §9.5, E[K1]  log m + 2. Thus,

                  · (log m + 1),

and so by Mertens' theorem,

                                        P[F] = O( 2).

   We may also analyze the statistical distance  between the output distribution
of Algorithm RFN and the uniform distribution on {1, . . . , m} (in factored form).
It follows from Theorem 8.31 that  = P[F]  /g(m) = O( 2).

   Finally, we analyze the expected running time of Algorithm RFN. We have

                           P[H1]  P[H1  F 1] = g(m)(1 - ).
We leave it to the reader to verify that E[Z1] = O( ( )), from which it follows by
Theorem 9.3 that

                       E[Z] = E[Z1]/ P[H1] = O( 2( )/(1 - )).
If  is moderately small, so that (log m + 1)  1/2, and hence   1/2, then

                                      E[Z] = O( 2( )).

                               9.7 Some complexity theory

We close this chapter with a few observations about probabilistic algorithms from
a more "complexity theoretic" point of view.

   Suppose f is a function mapping bit strings to bit strings. We may have an
algorithm A that approximately computes f in the following sense: there exists
a constant , with 0   < 1/2, such that for all inputs x, A(x) outputs f (x) with
probability at least 1 - . The value  is a bound on the error probability, which
is defined as the probability that A(x) does not output f (x).

               9.7.1 Reducing the error probability

There is a standard "trick" by which one can make the error probability very small;

namely, run A on input x some number, say k, times, and take the majority output

as the answer. Suppose  < 1/2 is a bound on the error probability. Using the
Chernoff bound (Theorem 8.24), the error probability for the iterated version of A

is bounded by

               exp[-(1/2 - )2k/2],                        (9.8)

and so the error probability decreases exponentially with the number of iterations.
This bound is derived as follows. For i = 1, . . . , k, let Xi be the indicator variable
                                 9.7 Some complexity theory                                    303

for the event that the ith iteration of A(x) does not output f (x). The expected value

of  the  sample  mean  X  :=  1  k    Xi  is  at  most  ,  and  if  the  majority  output  of  the
                              k  i=1

iterated algorithm is wrong (or indeed, if there is no majority), then X exceeds its

expectation by at least 1/2 - . The bound (9.8) follows immediately from part (i)

of Theorem 8.24.

                                9.7.2 Strict polynomial time

If we have an algorithm A that runs in expected polynomial time, and which
approximately computes a function f , then we can easily turn it into a new algo-
rithm A that runs in strict polynomial time, and also approximates f , as follows.
Suppose that  < 1/2 is a bound on the error probability, and Q(n) is a polynomial
bound on the expected running time for inputs of size n. Then A simply runs A for
at most kQ(n) steps, where k is any constant chosen so that  + 1/k < 1/2 -- if A
does not halt within this time bound, then A simply halts with an arbitrary output.
The probability that A errs is at most the probability that A errs plus the probability
that A runs for more than kQ(n) steps. By Markov's inequality (Theorem 8.22),
the latter probability is at most 1/k, and hence A approximates f as well, but with
an error probability bounded by  + 1/k.

                                9.7.3 Language recognition

An important special case of approximately computing a function is when the out-
put of the function f is either 0 or 1 (or equivalently, false or true). In this case, f
may be viewed as the characteristic function of the language L := {x : f (x) = 1}.
(It is the tradition of computational complexity theory to call sets of bit strings
"languages.") There are several "flavors" of probabilistic algorithms for approxi-
mately computing the characteristic function f of a language L that are tradition-
ally considered -- for the purposes of these definitions, we may restrict ourselves
to algorithms that output either 0 or 1:

     · We call a probabilistic, expected polynomial-time algorithm an Atlantic
         City algorithm for recognizing L if it approximately computes f with
         error probability bounded by a constant  < 1/2.

     · We call a probabilistic, expected polynomial-time algorithm A a Monte
         Carlo algorithm for recognizing L if for some constant  > 0, we have:

               - P[A(x) outputs 1]   for all x  L;
               - P[A(x) outputs 1] = 0 for all x / L.

     · We call a probabilistic, expected polynomial-time algorithm a Las Vegas
         algorithm for recognizing L if it computes f correctly on all inputs x.
304  Probabilistic algorithms

   One also says an Atlantic City algorithm has two-sided error, a Monte Carlo
algorithm has one-sided error, and a Las Vegas algorithm has zero-sided error.

EXERCISE 9.14. Show that every language recognized by a Las Vegas algorithm
is also recognized by a Monte Carlo algorithm, and that every language recognized
by a Monte Carlo algorithm is also recognized by an Atlantic City algorithm.

EXERCISE 9.15. Show that if L is recognized by an Atlantic City algorithm that
runs in expected polynomial time, then it is recognized by an Atlantic City algo-
rithm that runs in strict polynomial time, and whose error probability is at most 2-n
on inputs of size n.

EXERCISE 9.16. Show that if L is recognized by a Monte Carlo algorithm that
runs in expected polynomial time, then it is recognized by a Monte Carlo algorithm
that runs in strict polynomial time, and whose error probability is at most 2-n on
inputs of size n.

EXERCISE 9.17. Show that a language is recognized by a Las Vegas algorithm
if and only if the language and its complement are recognized by Monte Carlo
algorithms.

EXERCISE 9.18. Show that if L is recognized by a Las Vegas algorithm that runs
in strict polynomial time, then L may be recognized in deterministic polynomial
time.

EXERCISE 9.19. Suppose that for a given language L, there exists a probabilistic
algorithm A that runs in expected polynomial time, and always outputs either 0 or
1. Further suppose that for some constants  and c, where

     ·  is a rational number with 0   < 1, and
     · c is a positive integer,
and for all sufficiently large n, and all inputs x of size n, we have
     · if x / L, then P[A(x) outputs 1]  , and
     · if x  L, then P[A(x) outputs 1]   + 1/nc.
    (a) Show that there exists an Atlantic City algorithm for L.
   (b) Show that if  = 0, then there exists a Monte Carlo algorithm for L.

                                           9.8 Notes

Our approach in §9.1 to defining the probability distribution associated with the
execution of a probabilistic algorithm is not the only possible one. For example,
9.8 Notes  305

one could define the output distribution and expected running time of an algorithm
on a given input directly, using the identities in Exercise 9.2, and avoid the con-
struction of an underlying probability distribution altogether; however, we would
then have very few tools at our disposal to analyze the behavior of an algorithm.
Yet another approach is to define a distribution that models an infinite random bit
string. This can be done, but requires more advanced notions from probability
theory than those that have been covered in this text.

   The algorithm presented in §9.6 for generating a random factored number is due
to Kalai [52], although the analysis presented here is a bit different, and our anal-

ysis using a probabilistic primality test is new. Kalai's algorithm is significantly
simpler, though less efficient, than an earlier algorithm due to Bach [9], which uses
an expected number of O( ) primality tests, as opposed to the O( 2) primality tests

used by Kalai's algorithm.

   See Luby [63] for an exposition of the theory of pseudo-random bit generation.
                                   10

                     Probabilistic primality testing

In this chapter, we discuss some simple and efficient probabilistic algorithms for
testing whether a given integer is prime.

                                     10.1 Trial division

Suppose we are given an integer n > 1, and we want to determine whether n is

prime or composite. The simplest algorithm to describe and to program is trial

division. We simply divide n by 2, 3, and so on, testing if any of these numbers
                                                                                     

evenly divide n. Of course, we only need to divide by primes up to n, since if
                                                                        

n is composite, it must have a prime factor no greater than n (see Exercise 1.2).

Not only does this algorithm determine whether n is prime or composite, it also

produces a non-trivial factor of n in case n is composite.
   Of course, the drawback of this algorithm is that it is terribly inefficient: it

requires (n) arithmetic operations, which is exponential in the bit length of n.

Thus, for practical purposes, this algorithm is limited to quite small n. Suppose, for

example, that n has 100 decimal digits, and that a computer can perform 1 billion

divisions per second (this is much faster than any computer existing today). Then
                                                               
it  would  take  on  the  order  of  1033  years  to  perform    n  divisions.

    In this chapter, we discuss a much faster primality test that allows 100-decimal-

digit numbers to be tested for primality in less than a second. Unlike the above

test, however, this test does not find a factor of n when n is composite. More-

over, the algorithm is probabilistic, and may in fact make a mistake. However, the

probability that it makes a mistake can be made so small as to be irrelevant for all

practical purposes. Indeed, we can easily make the probability of error as small as

2-100 -- should one really care about an event that happens with such a miniscule

probability?

                                           306
                                        10.2 The Miller-Rabin test                                         307

                                10.2 The Miller-Rabin test

We describe in this section a fast (polynomial time) test for primality, known as
the Miller-Rabin test. As discussed above, the algorithm is probabilistic, and
may (with small probability) make a mistake.

   We assume for the remainder of this section that the number n we are testing for
primality is an odd integer greater than 1.

   We recall some basic algebraic facts that will play a critical role in this section
(see §7.5). Suppose n = p1e1 · · · prer is the prime factorization of n (since n is odd,
each pi is odd). The Chinese remainder theorem gives us a ring isomorphism

                                  : Zn  Zpe1 × · · · × Zperr

                                                                  1

                                            [a]n  ([a]pe1 , . . . , [a]perr ),

                                                                       1

and restricting  to Zn yields a group isomorphism

                                        Zn = Z e1  × · · · × Z er  .
                                                p1                 pr

Moreover, Theorem 7.28 says that each Zpei is a cyclic group, whose order, of i
course, is (piei ) = piei-1(pi - 1), where  is Euler's phi function.

Several probabilistic primality tests, including the Miller-Rabin test, have the
                                                  +
following     general    structure.     Define       to   be  the  set  of  non-zero  elements  of         Zn;
                                                Zn
          +   =                                       +  =
thus,            n   -  1,  and  if  n  is  prime,          Zn .  Suppose       also  that  we  define  a  set
       |Zn |                                        Zn
         +
Ln          such    that:
       Zn

       ·  there  is  an  efficient      algorithm   that  on  input    n  and           +   determines     if

                                                                                      Zn ,

            Ln;

       · if n is prime, then Ln = Zn;

       · if n is composite, |Ln|  c(n - 1) for some constant c < 1.

To test n for primality, we set a "repetition parameter" k, and choose random
                                +
elements 1, . . . , k                   If i    Ln   for all i    =    1, . . . , k, then we output true;
                              Zn .

otherwise, we output false.

   It is easy to see that if n is prime, this algorithm always outputs true, and if n is
composite this algorithm outputs true with probability at most ck. If c = 1/2 and k
is chosen large enough, say k = 100, then the probability that the output is wrong

is so small that for all practical purposes, it is "just as good as zero."

   We now make a first attempt at defining a suitable set Ln. Let us define
                                Ln := {  Zn+ : n-1 = 1}.

Note that Ln  Zn, since if n-1 = 1, then  has a multiplicative inverse, namely,
308            Probabilistic primality testing

n-2. We can test if   Ln in time O(len(n)3), using a repeated-squaring algo-
rithm.

Theorem 10.1. If n is prime, then Ln = Zn. If n is composite and Ln  Zn, then
|Ln|  (n - 1)/2.

Proof. Note that Ln is the kernel of the (n - 1)-power map on Zn, and hence is a
subgroup of Zn.

   If n is prime, then we know that Zn is a group of order n - 1. Since the order of
a group element divides the order of the group, we have n-1 = 1 for all   Zn.
That is, Ln = Zn.

   Suppose that n is composite and Ln Zn. Since the order of a subgroup divides
the order of the group, we have |Zn| = t|Ln| for some integer t > 1. From this, we

conclude that

               |Ln| = 1 |Zn|  1 |Zn|  n - 1 . 2t
                      2                    2

Unfortunately, there are odd composite numbers n such that Ln = Zn. Such

numbers are called Carmichael numbers. The smallest Carmichael number is

                  561 = 3 · 11 · 17.

Carmichael numbers are extremely rare, but it is known that there are infinitely
many of them, so we cannot ignore them. The following theorem puts some con-
straints on Carmichael numbers.

Theorem 10.2. Every Carmichael number n is of the form n = p1 · · · pr, where
the pi's are distinct primes, r  3, and (pi - 1) | (n - 1) for i = 1, . . . , r.

Proof. Let n = p1e1 · · · prer be a Carmichael number. By the Chinese remainder
theorem, we have an isomorphism of Zn with the group

                  Z e1  × · · · × Z er  ,
                  p1     pr

and we know that each group Zpei  is cyclic of order p i iei-1(pi - 1). Thus, the power
n - 1 kills the group Zn if and only if it kills all the groups Zpei , which happens if i
and only if piei-1(pi - 1) | (n - 1). Now, on the one hand, n  0 (mod pi). On the

other hand, if ei > 1, we would have n  1 (mod pi), which is clearly impossible.
Thus, we must have ei = 1.

   It remains to show that r  3. Suppose r = 2, so that n = p1p2. We have

               n - 1 = p1p2 - 1 = (p1 - 1)p2 + (p2 - 1).

Since (p1 -1) | (n-1), we must have (p1 -1) | (p2 -1). By a symmetric argument,
(p2 - 1) | (p1 - 1). Hence, p1 = p2, a contradiction. 2
                                 10.2 The Miller-Rabin test                         309

   To obtain a good primality test, we need to define a different set Ln, which we
do as follows. Let n - 1 = t2h, where t is odd (and h  1 since n is assumed odd),

and define

Ln          :=  {      +  :  t2h = 1 and

                     Zn      t2j+1 = 1 = t2j = ±1 for j = 0, . . . , h - 1}.

The Miller-Rabin test uses this set Ln, in place of the set Ln defined above. It is

clear from the definition that Ln  Ln.
                                     +
Testing     whether  a  given           belongs  to  Ln  can  be  done  using  the  following
                                   Zn

procedure:

  t
if  = 1 then return true

for j  0 to h - 1 do
      if  = -1 then return true
      if  = +1 then return false
        2

return false

   It is clear that using a repeated-squaring algorithm, this procedure runs in time
O(len(n)3). We leave it to the reader to verify that this procedure correctly deter-
mines membership in Ln.

Theorem 10.3. If n is prime, then Ln = Zn. If n is composite, then |Ln|  (n-1)/4.

Proof. Let n - 1 = t2h, where t is odd.

Case 1: n is prime. Let   Zn. Since Zn is a group of order n - 1, and the order
of a group element divides the order of the group, we know that t2h = n-1 = 1.
Now consider any index j = 0, . . . , h-1 such that t2j+1 = 1, and consider the value
 := t2j . Then since 2 = t2j+1 = 1, the only possible choices for  are ±1 -- this
is because Zn is cyclic of even order and so there are exactly two elements of Zn
whose multiplicative order divides 2, namely ±1. So we have shown that   Ln.
Case 2: n = pe, where p is prime and e > 1. Certainly, Ln is contained in the
kernel K of the (n-1)-power map on Zn. By Theorem 6.32, |K| = gcd((n), n-1).
Since n = pe, we have (n) = pe-1(p - 1), and so

     |Ln|  |K| = gcd(pe-1(p - 1), pe - 1) = p - 1 = pe-1p+ · · · + 1  4 . e - 1 n - 1
Case 3: n = p1e1 · · · prer is the prime factorization of n, and r > 1. Let

                                  : Zn  Zpe1 × · · · × Zperr

                                                                                        1

be the ring isomorphism provided by the Chinese remainder theorem. Also, let
310  Probabilistic primality testing

(piei ) = ti2hi, with ti odd, for i = 1, . . . , r, and let g := min{h, h1, . . . , hr}. Note
that g  1, and that each Z ei  is a cyclic group of order ti2hi.

                                            pi

                                                                                                                        g

   We first claim that for every   Ln, we have t2 = 1. To prove this, first
note that if g = h, then by definition, t2g = 1, so suppose that g < h. By
way of contradiction, suppose that t2g = 1, and let j be the smallest index in
the range g, . . . , h - 1 such that t2j+1 = 1. By the definition of Ln, we must
have t2j = -1. Since g < h, we must have g = hi for some particular index
i = 1, . . . , r. Writing () = (1, . . . , r), we have it2j = -1. This implies that
the multiplicative order of it is equal to 2j+1 (see Theorem 6.37). However, since
j  g = hi, this contradicts the fact that the order of a group element (in this case,
it) must divide the order of the group (in this case, Zpei ). i

   For j = 0, . . . , h, let us define j to be the (t2j)-power map on Zn. From the claim

in the previous paragraph, and the definition of Ln, it follows that each   Ln
satisfies t2g-1 = ±1. In other words, Ln  g-1 -1 ({±1}), and hence

     |Ln|  2|Ker g-1|.                       (10.1)

From the group isomorphism Zn = Z e1  × · · · × Z er  , and Theorem 6.32, we have
     p1                                  pr

                        r                    (10.2)

     |Ker j| = gcd(ti2hi, t2j)

                       i=1

for each j = 0, . . . , h. Since g  h, and g  hi for i = 1, . . . , r, it follows
immediately from (10.2) that

     2r|Ker g-1| = |Ker g|  |Ker h|.         (10.3)

Combining (10.3) with (10.1), we obtain

     |Ln|  2-r+1|Ker h|.                     (10.4)

If r  3, then (10.4) directly implies that |Ln|  |Zn|/4  (n - 1)/4, and we
are done. So suppose that r = 2. In this case, Theorem 10.2 implies that n is not

a Carmichael number, which implies that |Ker h|  |Zn|/2, and so again, (10.4)
implies |Ln|  |Zn|/4  (n - 1)/4. 2

EXERCISE 10.1. Show that an integer n > 1 is prime if and only if there exists an
element in Zn of multiplicative order n - 1.

EXERCISE 10.2. Show that Carmichael numbers satisfy Fermat's little theorem;
that is, if n is a Carmichael number, then n =  for all   Zn.
                  10.3 Generating random primes using the Miller-Rabin test                  311

EXERCISE 10.3. Let p be a prime. Show that n := 2p + 1 is a prime if and only if
2n-1  1 (mod n).

EXERCISE 10.4. Here is another primality test that takes as input an odd integer
                                                                                               +
n  >  1,  and  a  positive  integer  parameter  k.  The  algorithm  chooses  1, . . . , k  
                                                                                             Zn

at random, and computes

                            i := i(n-1)/2 (i = 1, . . . , k).

If (1, . . . , k) is of the form (±1, ±1, . . . , ±1), but is not equal to (1, 1, . . . , 1), the
algorithm outputs true; otherwise, the algorithm outputs false. Show that if n is
prime, then the algorithm outputs false with probability at most 2-k, and if n is
composite, the algorithm outputs true with probability at most 2-k.

   In the terminology of §9.7, the algorithm in the above exercise is an example of
an "Atlantic City" algorithm for the language of prime numbers (or equivalently,
the language of composite numbers), while the Miller-Rabin test is an example of
a "Monte Carlo" algorithm for the language of composite numbers.

          10.3 Generating random primes using the Miller-Rabin test

The Miller-Rabin test is the most practical algorithm known for testing primality,
and because of this, it is widely used in many applications, especially cryptographic
applications where one needs to generate large, random primes (as we saw in §4.7).
In this section, we discuss how one uses the Miller-Rabin test in several practically
relevant scenarios where one must generate large primes.

                 10.3.1 Generating a random prime between 2 and m

Suppose we are given an integer m  2, and want to generate a random prime
between 2 and m. We can do this by simply picking numbers at random until one
of them passes a primality test. We discussed this problem in some detail in §9.4,
where we assumed that we had a primality test IsPrime. The reader should review
§9.4, and §9.4.1 in particular. In this section, we discuss aspects of this problem
that are specific to the situation where the Miller-Rabin test is used to implement
IsPrime. To be more precise, let us define the following algorithm:

Algorithm MR. On input n, k, where n and k are integers with n > 1 and k  1,
do the following:
312  Probabilistic primality testing

     if n = 2 then return true
     if n is even then return false

     repeat k times

                   ¢+

             Zn
           if  / Ln return false

     return true

   So we shall implement IsPrime(·) as MR(·, k), where k is an auxiliary parame-

ter. By Theorem 10.3, if n is prime, the output of MR(n, k) is always true, while
if n is composite, the output is true with probability at most 4-k. Thus, this imple-
mentation of IsPrime satisfies the assumptions in §9.4.1, with  = 4-k.

   Let (m, k) be the probability that the output of Algorithm RP in §9.4 -- using

this implementation of IsPrime-- is composite. Then as we discussed in §9.4.1,

     (m, k)  4-k · m - 1 = O(4-k ),                                        (10.5)
                         (m)

where := len(m), and (m) is the number of primes up to m. Furthermore, if

the output of Algorithm RP is prime, then every prime is equally likely; that is, the

conditional distribution of the output, given that the output is prime, is (essentially)

the uniform distribution on the set of primes up to m.

Let us now consider the expected running time of Algorithm RP. As discussed

in §9.4.1, the expected number of iterations of the main loop in Algorithm RP is
O( ). Clearly, the expected running time of a single loop iteration is O(k 3), since

MR(n, k) executes at most k iterations of the Miller-Rabin test, and each such
test takes time O( 3). This leads to a bound on the expected total running time
of Algorithm RP of O(k 4). However, this estimate is overly pessimistic, because

when n is composite, we expect to perform very few Miller-Rabin tests -- only

when n is prime do we actually perform all k of them.

To make a rigorous argument, let us define random variables measuring various

quantities during the first iteration of the main loop in Algorithm RP: N1 is the

value of n; K1 is the number of Miller-Rabin tests actually performed; Z1 is the

running time. Of course, N1 is uniformly distributed over {2, . . . , m}. Let C1 be

the event that N1 is composite. Consider the conditional distribution of K1 given

C1. This is not exactly a geometric distribution, since K1 never takes on values

greater than k; nevertheless, using Theorem 8.17, we can easily calculate

     E[K1 | C1] = P[K1  i | C1]  (1/4)i-1 = 4/3.

     i1                              i1
        10.3 Generating random primes using the Miller-Rabin test        313

Using the law of total expectation (8.24), it follows that

        E[K1] = E[K1 | C1] P[C1] + E[K1 | C1] P[C1]
                4/3 + k(m)/(m - 1).

Thus, E[K1]  4/3 + O(k/ ), and hence E[Z1] = O( 3 E[K1]) = O( 3 + k 2).
Therefore, if Z is the total running time of Algorithm RP, then E[Z] = O( E[Z1]),

and so

                       E[Z] = O( 4 + k 3).                               (10.6)

   Note that the above estimate (10.5) for (m, k) is actually quite pessimistic. This
is because the error probability 4-k is a worst-case estimate; in fact, for "most"
composite integers n, the probability that MR(n, k) outputs true is much smaller
than this. In fact, (m, 1) is very small for large m. For example, the following is
known:

Theorem 10.4. We have
         (m, 1)  exp[-(1 + o(1)) log(m) log(log(log(m)))/ log(log(m))].

Proof. Literature--see §10.5. 2

   The bound in the above theorem goes to zero quite quickly: faster than (log m)-c
for every positive constant c. While the above theorem is asymptotically very good,
in practice, one needs explicit bounds. For example, the following lower bounds
for - log2((2 , 1)) are known:

                       200 300 400 500 600
                          3 19 37 55 74

Given an upper bound on (m, 1), we can bound (m, k) for k  2 using the

following inequality:

                       (m, k)  (m, 1) 4-k+1.                             (10.7)
                                   1 - (m, 1)

To prove (10.7), it is not hard to see that on input m, the output distribution of
Algorithm RP is the same as that of the following algorithm:

        repeat
              repeat

                                ¢

                     n  {2, . . . , m}
              until MR(n , 1)
              nn
        until MR(n, k - 1)
        output n
314                   Probabilistic primality testing

   Let N1 be the random variable representing the value of n in the first iteration of
the main loop in this algorithm, let C1 be the event that N1 is composite, and let
H1 be the event that this algorithm halts at the end of the first iteration of the main
loop. Using Theorem 9.3, we see that

     (m, k) = P[C1 | H1] = P[C1  H1]  P[C1  H1] = P[H1 | C1] P[C1]
                         P[H1]  P[C1]                  P[C1]

       4-k+1(m, 1)    ,

       1 - (m, 1)

which proves (10.7).

   Given that (m, 1) is so small, for large m, Algorithm RP actually exhibits the
following behavior in practice: it generates a random value n  {2, . . . , m}; if n
is odd and composite, then the very first iteration of the Miller-Rabin test will
detect this with overwhelming probability, and no more iterations of the test are
performed on this n; otherwise, if n is prime, the algorithm will perform k - 1
more iterations of the Miller-Rabin test, "just to make sure."

EXERCISE 10.5. Consider the problem of generating a random Sophie Germain
prime between 2 and m (see §5.5.5). One algorithm to do this is as follows:

         repeat

                          ¢

               n  {2, . . . , m}
               if MR(n, k) then

                      if MR(2n + 1, k) then
                             output n and halt

         forever

Assuming Conjecture 5.24, show that this algorithm runs in expected time
O( 5 + k 4), and outputs a number that is not a Sophie Germain prime with prob-
ability O(4-k 2). As usual, := len(m).

EXERCISE 10.6. Improve the algorithm in the previous exercise, so that under the
same assumptions, it runs in expected time O( 5 + k 3), and outputs a number that
is not a Sophie Germain prime with probability O(4-k 2), or even better, show
that this probability is at most (m, k)(m)/(m) = O((m, k) ), where (m) is
defined as in §5.5.5.

EXERCISE 10.7. Suppose in Algorithm RFN in §9.6 we implement algorithm
IsPrime(·) as MR(·, k), where k is a parameter satisfying 4-k(log m + 1)  1/2,
and m is the input to RFN. Show that the expected running time of Algorithm RFN
in this case is O( 5 + k 4 len( )). Hint: use Exercise 9.13.
10.3 Generating random primes using the Miller-Rabin test  315

                        10.3.2 Trial division up to a small bound

In generating a random prime, most candidates will in fact be composite, and so it
makes sense to cast these out as quickly as possible. Significant efficiency gains can
be achieved by testing if a given candidate n is divisible by any prime up to a given
bound s, before we subject n to a Miller-Rabin test. This strategy makes sense,
since for a small, "single precision" prime p, we can test if p | n essentially in time
O(len(n)), while a single iteration of the Miller-Rabin test takes time O(len(n)3).

   To be more precise, let us define the following algorithm:

Algorithm MRS. On input n, k, s, where n, k, s  Z, and n > 1, k  1, and s > 1,
do the following:

for each prime p  s do
      if p | n then
             if p = n then return true else return false

repeat k times

              ¢+

        Zn
      if  / Ln return false

return true

   In an implementation of the above algorithm, one would most likely use the
sieve of Eratosthenes (see §5.4) to generate the small primes.

   Note that MRS(n, k, 2) is equivalent to MR(n, k). Also, it is clear that the
probability that MRS(n, k, s) makes a mistake is no more than the probability that
MR(n, k) makes a mistake. Therefore, using MRS in place of MR will not increase
the probability that the output of Algorithm RP is a composite -- indeed, it is likely
that this probability decreases significantly.

   Let us now analyze the impact on the running time Algorithm RP. To do this, we
need to estimate the probability (m, s) that a randomly chosen integer between 2
and m is not divisible by any prime up to s. If m is sufficiently large with respect to
s, the following heuristic argument can be made rigorous, as we will discuss below.
The probability that a random integer is divisible by a prime p is about 1/p, so the
probability that it is not divisible by p is about 1 - 1/p. Assuming that these events
are essentially independent for different values of p (this is the heuristic part), we
estimate

(m, s)  (1 - 1/p).                                         (10.8)

                 ps

Assuming for the time being that the approximation in (10.8) is sufficiently accu-
rate, then using Mertens' theorem (Theorem 5.13), we may deduce that

(m, s) = O(1/ log s).                                      (10.9)
316           Probabilistic primality testing

Later, when we make this argument more rigorous, we shall see that (10.9) holds
provided s is not too large relative to m, and in particular, if s = O((log m)c) for

some constant c.

   The estimate (10.9) gives us a bound on the probability that a random integer

passes the trial division phase, and so must be subjected to Miller-Rabin; how-

ever, performing the trial division takes some time, so we also need to estimate the

expected number (m, s) of trial divisions performed on a random integer between

2 and m. Of course, in the worst case, we divide by all primes up to s, and so
(m, s)  (s) = O(s/ log s), but we can get a better bound, as follows. Let
p1, p2, . . . , pr be the primes up to s, and for i = 1, . . . , r, let qi be the probability
that we perform at least i trial divisions. By Theorem 8.17, we have

                                 r

              (m, s) = qi.

                   i=1

Moreover, q1 = 1, and qi = (m, pi-1) for i = 2, . . . , r. From this, and (10.9), it

follows that

              r

              (m, s) = 1 + (m, pi-1) = O 1/ log p .

              i=2                   ps

As a simple consequence of Chebyshev's theorem (in particular, see Exercise 5.3),

we obtain

              (m, s) = O(s/(log s)2).                        (10.10)

   We now derive a bound on the running time of Algorithm RP, assuming that
IsPrime(·) is implemented using MRS(·, k, s). Let := len(m). Our argument
follows the same lines as was used to derive the estimate (10.6). Let us define
random variables measuring various quantities during the first iteration of the main
loop in Algorithm RP: N1 is the value of n; K1 is the number of Miller-Rabin tests
actually performed; Z1 is the running time. Also, let C1 be the event that N1 is
composite, and let D1 be the event that N1 passes the trial division check. Then we
have

         E[K1] = E[K1 | C1  D1] P[C1  D1] + E[K1 | C1  D1] P[C1  D1]
                       + E[K1 | C1] P[C1]

                  4/3 · P[C1  D1] + 0 · P[C1  D1] + k · P[C1]
                  4/3 · P[D1] + k · P[C1].

By (10.9) and Chebyshev's theorem, it follows that           (10.11)
                                E[K1] = O(1/ len(s) + k/ ).
10.3 Generating random primes using the Miller-Rabin test             317

Let us write Z1 = Z1 + Z1, where Z1 is the amount of time spent performing
the Miller-Rabin test, and Z1 is the amount of time spent performing trial divi-
sion. By (10.11), we have E[Z1] = O( 3/ len(s) + k 2). Further, assuming
that each individual trial division step takes time O( ), then by (10.10) we have
E[Z1] = O( s/ len(s)2). Hence,

                       E[Z1] = O( 3/ len(s) + k 2 + s/ len(s)2).

It follows that if Z is the total running time of Algorithm RP, then
                       E[Z] = O( 4/ len(s) + k 3 + 2s/ len(s)2).

Clearly, we want to choose the parameter s so that the time spent performing trial

division is dominated by the time spent performing the Miller-Rabin test. To this
end, let us assume that  s  2. Then we have

E[Z] = O( 4/ len( ) + k 3).                                           (10.12)

This estimate does not take into account the time to generate the small primes
using the sieve of Eratosthenes. These values might be pre-computed, in which
case this time is zero, but even if we compute them on the fly, this takes time
O(s len(len(s))), which is dominated by the running time of the rest of the algo-
rithm for the values of s under consideration.

   Thus, by sieving up to a bound s, where  s  2, then compared to (10.6),
we effectively reduce the running time by a factor proportional to len( ), which is
a very real and noticeable improvement in practice.

   As we already mentioned, the above analysis is heuristic, but the results are
correct. We shall now discuss how this analysis can be made rigorous; however,
we should remark that any such rigorous analysis is mainly of theoretical interest
only -- in any practical implementation, the optimal choice of the parameter s is
best determined by experiment, with the analysis being used only as a rough guide.
Now, to make the analysis rigorous, we need prove that the estimate (10.8) is suf-
ficiently accurate. Proving such estimates takes us into the realm of "sieve theory."
The larger m is with respect to s, the easier it is to prove such estimates. We shall
prove only the simplest and most naive such estimate, but it is still good enough
for our purposes.

   Before stating any results, let us restate the problem slightly differently. For a
given real number y  0, let us call a positive integer "y-rough" if it is not divisible
by any prime p up to y. For all real numbers x  0 and y  0, let us define R(x, y)
to be the number of y-rough positive integers up to x. Thus, since (m, s) is the
probability that a random integer between 2 and m is s-rough, and 1 is by definition
s-rough, we have (m, s) = (R(m, s) - 1)/(m - 1).
318                  Probabilistic primality testing

Theorem 10.5. For all real x  0 and y  0, we have

                R(x, y) - x (1 - 1/p)  2(y).

                                    py

Proof. To simplify the notation, we shall use the Möbius function µ (see §2.9).
Also, for a real number u, let us write u = u + {u}, where 0  {u} < 1. Let Q
be the product of the primes up to the bound y.

   Now, there are x positive integers up to x, and of these, for each prime p divid-
ing Q, precisely x/p are divisible by p, for each pair p, p of distinct primes divid-
ing Q, precisely x/pp are divisible by pp , and so on. By inclusion/exclusion
(see Theorem 8.1), we have

           R(x, y) = µ(d) x/d = µ(d)(x/d) - µ(d){x/d}.

                d|Q                  d|Q              d|Q

Moreover,

                µ(d)(x/d) = x µ(d)/d = x (1 - 1/p),

           d|Q                  d|Q            py

and

                            µ(d){x/d}  1 = 2(y).

                     d|Q                  d|Q

That proves the theorem. 2

   This theorem says something non-trivial only when y is quite small. Neverthe-

less, using Chebyshev's theorem on the density of primes, along with Mertens'

theorem, it is not hard to see that this theorem implies that (10.9) holds when
s = O((log m)c) for some constant c (see Exercise 10.8), which implies the esti-
mate (10.12) above, when  s  2.

EXERCISE 10.8. Suppose that s is a function of m such that s = O((log m)c) for
some positive constant c. Show that (m, s) = O(1/ log s).

EXERCISE 10.9. Let f be a polynomial with integer coefficients. For real x  0
and y  0, define Rf (x, y) to be the number of positive integers t up to x such
that f (t) is y-rough. For each positive integer m, define f (m) to be the number of
integers t  {0, . . . , m - 1} such that f (t)  0 (mod m). Show that

           Rf (x, y) - x (1 - f (p)/p)  (1 + f (p)).

                            py                 py
10.3 Generating random primes using the Miller-Rabin test  319

EXERCISE 10.10. Consider again the problem of generating a random Sophie

Germain prime, as discussed in Exercises 10.5 and 10.6. A useful idea is to first
test if either n or 2n + 1 are divisible by any small primes up to some bound s,

before performing any more expensive tests. Using this idea, design and analyze

an algorithm that improves the running time of the algorithm in Exercise 10.6 to
O( 5/ len( )2 + k 3) -- under the same assumptions, and achieving the same error

probability bound as in that exercise. Hint: first show that the previous exercise
implies that the number of positive integers t up to x such that both t and 2t + 1 are

y-rough is at most

                        1   (1 - 2/p) + 3(y).
                    x·
                    2 2<py

EXERCISE 10.11. Design an algorithm that takes as input a prime q and a bound

m, and outputs a random prime p between 2 and m such that p  1 (mod q).
Clearly, we need to assume that m is sufficiently large with respect to q. Ana-

lyze your algorithm assuming Conjecture 5.22. State how large m must be with

respect to q, and under these assumptions, show that your algorithm runs in time
O( 4/ len( ) + k 3), and that its output is incorrect with probability O(4-k ). As
usual, := len(m).

                    10.3.3 Generating a random -bit prime

In some applications, we want to generate a random prime of fixed size -- a ran-
dom 1024-bit prime, for example. More generally, let us consider the following
problem: given an integer  2, generate a random -bit prime, that is, a prime in
the interval [2 -1, 2 ).

   Bertrand's postulate (Theorem 5.8) implies that there exists a constant c > 0
such that (2 ) - (2 -1)  c2 -1/ for all  2.

   Now let us modify Algorithm RP so that it takes as input an integer  2, and
repeatedly generates a random n in the interval {2 -1, . . . , 2 - 1} until IsPrime(n)
returns true. Let us call this variant Algorithm RP . Further, let us implement
IsPrime(·) as MR(·, k), for some auxiliary parameter k, and define  ( , k) to be
the probability that the output of Algorithm RP -- with this implementation of
IsPrime--is composite.

   Then using exactly the same reasoning as in §10.3.1, we have

                     ( , k)  4-k 2 -1     = O(4-k );
                           (2 ) - (2 -1)

moreover, if the output of Algorithm RP is prime, then every -bit prime is equally
320                    Probabilistic primality testing

likely, and the expected running time is O( 4 + k 3). By doing some trial division
as in §10.3.2, this can be reduced to O( 4/ len( ) + k 3).

   The function  ( , k) has been studied a good deal; for example, the following

explicit bound is known:

Theorem 10.6. For all   2, we have

                                                           

                               ( , 1)  242- .

Proof. Literature--see §10.5. 2

   Upper bounds for  ( , k) for specific values of and k have been computed.
The following table lists some known lower bounds for - log2( ( , k)) for various
values of and k:

                        k\ 200 300 400 500 600
                          1 11 19 37 56 75
                          2 25 33 46 63 82
                          3 34 44 55 70 88
                          4 41 53 63 78 95
                          5 47 60 72 85 102

Using exactly the same reasoning as the derivation of (10.7), one sees that

                              ( , k)   ( , 1) 4-k+1.
                                          1 -  ( , 1)

     10.4 Factoring and computing Euler's phi function

In this section, we use some of the ideas developed to analyze the Miller-Rabin

test to prove that the problem of factoring n and the problem of computing (n)
are equivalent. By equivalent, we mean that given an efficient algorithm to solve
one problem, we can efficiently solve the other, and vice versa.

   Clearly, one direction is easy: if we can factor n into primes, so

                       n = p1e1 · · · prer ,                 (10.13)

then we can simply compute (n) using the formula

                          (n) = p1e1-1(p1 - 1) · · · prer-1(pr - 1).
   For the other direction, first consider the special case where n = pq, for distinct
primes p and q. Suppose we are given n and (n), so that we have two equations
in the unknowns p and q:

                       n = pq and (n) = (p - 1)(q - 1).
10.4 Factoring and computing Euler's phi function                          321

Substituting n/p for q in the second equation, and simplifying, we obtain
                               p2 + ((n) - n - 1)p + n = 0,

which can be solved using the quadratic formula.

   For the general case, it is just as easy to prove a stronger result: given any non-
zero multiple of the exponent of Zn, we can efficiently factor n. In particular, this
will show that we can efficiently factor Carmichael numbers.

   Before stating the algorithm in its full generality, we can convey the main idea
by considering the special case where n = pq, where p and q are distinct primes,

with p  q  3 (mod 4). Suppose we are given such an n, along with a non-zero
multiple f of the exponent of Zn. Now, Zn = Zp × Zq, and since Zp is a cyclic
group of order p - 1 and Zq is a cyclic group of order q - 1, this means that f is
a non-zero common multiple of p - 1 and q - 1. Let f = t2h, where t is odd, and

consider the following probabilistic algorithm:

                 ¢+

           Zn
         d  gcd(rep(), n)
         if d = 1 then output d and halt
           t
         d  gcd(rep() + 1, n)

         if d / {1, n} then output d and halt

         output "failure"

Recall that rep() denotes the canonical representative of , that is, the unique
integer a such that [a]n =  and 0  a < n. We shall prove that this algorithm
outputs a non-trivial divisor of n with probability at least 1/2.

   Let  be the t-power map on Zn, and let G := -1({±1}). We shall show that
     · G Zn, and

     · if the algorithm chooses  / G, then it splits n.
Since G is a subgroup of Zn, it follows that |G|/|Z+n |  |G|/|Zn|  1/2, and this
implies the algorithm succeeds with probability at least 1/2.

   Let  : Zn  Zp × Zq be the ring isomorphism from the Chinese remainder
theorem. The assumption that p  3 (mod 4) means that (p - 1)/2 is an odd
integer, and since f is a multiple of p - 1, it follows that gcd(t, p - 1) = (p - 1)/2,
and hence the image of Zp under the t-power map is the subgroup of Zp of order 2,
which is {±1}. Likewise, the image of Zq under the t-power map is {±1}. Thus,

          (Im ) = ((Zn)t) = ((Zn))t = (Zp)t × (Zq )t = {±1} × {±1},

and so Im  consists of the four elements:

              1 = -1(1, 1), -1 = -1(-1, -1), -1(-1, 1), -1(1, -1).
322          Probabilistic primality testing

   By the observations in the previous paragraph, not all elements of Zn map to ±1
under , which means that G Zn. Suppose that the algorithm chooses   Z+n \G.
We want to show that n gets split. If  / Zn, then gcd(rep(), n) is a non-trivial
divisor of n, and the algorithm splits n. So let us assume that   Zn \ G. Consider
the value  = t = () computed by the algorithm. Since  / G, we have
 = ±1, and by the observations in the previous paragraph, we have () = (-1, 1)
or () = (1, -1). In the first case, ( + 1) = (0, 2), and so gcd(rep() + 1, n) = p,
while in the second case, ( + 1) = (2, 0), and so gcd(rep() + 1, n) = q. In either

case, the algorithm splits n.

   We now consider the general case, where n is an arbitrary positive integer. Let
(n) denote the exponent of Zn. If the prime factorization of n is as in (10.13), then
by the Chinese remainder theorem, we have

     (n) = lcm((p1e1 ), . . . , (prer )).

Moreover, for every prime power pe, by Theorem 7.28, we have

     (pe) =  pe-1(p - 1) if p = 2 or e  2,

             2e-2  if p = 2 and e  3.

In particular, if d | n, then (d) | (n).

   Now, assume we are given n, along with a non-zero multiple f of (n). We

would like to calculate the complete prime factorization of n. We may proceed
recursively: first, if n = 1, we may obviously halt; otherwise, we test if n is prime,
using an efficient primality test, and if so, halt (if we are using the Miller-Rabin

test, then we may erroneously halt even when n is composite, but we can ensure
that this happens with negligible probability); otherwise, we split n as n = d1d2,
using an algorithm to be described below, and then recursively factor both d1 and
d2; since (d1) | f and (d2) | f , we may use the same value f in the recursion.

   So let us assume that n > 1 and n is not prime, and our goal now is to use f to

obtain a non-trivial factorization of n. If n is even, then we can certainly do this.
Moreover, if n is a perfect power -- that is, if n = ab for some integers a > 1 and

b > 1 --we can also obtain a non-trivial factorization of n (see Exercise 3.31).

   So let us assume not only that n > 1 and n is not prime, but also that n is odd,
and n is not a perfect power. Let f = t2h, where t is odd. Consider the following

probabilistic algorithm:
       10.4 Factoring and computing Euler's phi function                         323

           ¢+                             and halt

         Zn
       d  gcd(rep(), n)
       if d = 1 then output d and halt
         t

       for j  0 to h - 1 do
             d  gcd(rep() + 1, n)

             if d / {1, n} then output d
               2

       output "failure"

   We want to show that this algorithm outputs a non-trivial factor of n with prob-
ability at least 1/2. To do this, suppose the prime factorization of n is as in
(10.13). Then by our assumptions about n, we have r  2 and each pi is odd.
Let (piei ) = ti2hi, where ti is odd, for i = 1, . . . , r, and let g := max{h1, . . . , hr}.
Note that since (n) | f, we have 1  g  h.

   Let  be the (t2g-1)-power map on Zn, and let G := -1({±1}). As above, we
shall show that

     · G Zn, and
     · if the algorithm chooses  / G, then it splits n,

which will prove that the algorithm splits n with probability at least 1/2.
   Let

                                  : Zn  Zpe1 × · · · × Zperr

                                                                                        1

be the ring isomorphism of the Chinese remainder theorem. We have

       (Im ) = G1 × · · · × Gr,

where

       Gi := Zpiei  t2 for i g-1 = 1, . . . , r.

Let us assume the pi's are ordered so that hi = g for i = 1, . . . , r , and hi < g
for i = r + 1, . . . , r, where we have 1  r  r. Then we have Gi = {±1} for
i = 1, . . . , r , and Gi = {1} for i = r + 1, . . . , r.

By the observations in the previous paragraph, and the fact that r  2, the image

of  contains elements other than ±1; for example, -1(-1, 1, . . . , 1) is such an
                                                                              +
element. This means that G  Zn .  Suppose  the      algorithm  chooses           \ G.
                                                                            Zn

We want to show that n gets split. If  / Zn, then gcd(rep(), n) is a non-trivial

divisor of n, and so the algorithm certainly splits n. So assume   Zn \ G. In loop
iteration j = g - 1, the value of  is equal to (), and writing () = (1, . . . , r),

we have i = ±1 for i = 1, . . . , r. Let S be the set of indices i such that i = -1.
324  Probabilistic primality testing

As  / G, we know that  = ±1, and so  S {1, . . . , r}. Thus,

                                gcd(rep() + 1, n) = piei

                                                                                iS

is a non-trivial factor of n. This means that the algorithm splits n in loop iteration
j = g - 1 (if not in some earlier loop iteration).

   So we have shown that the above algorithm splits n with probability at least 1/2.
If we iterate the algorithm until n gets split, the expected number of loop iterations
required will be at most 2. Combining this with the above recursive algorithm,
we get an algorithm that completely factors an arbitrary n in expected polynomial
time.

EXERCISE 10.12. Suppose you are given an integer n of the form n = pq, where
p and q are distinct, -bit primes, with p = 2p + 1 and q = 2q + 1, where p and
q are themselves prime. Suppose that you are also given an integer t such that
gcd(t, p q ) = 1. Show how to efficiently factor n.

EXERCISE 10.13. Suppose there is a probabilistic algorithm A that takes as input
an integer n of the form n = pq, where p and q are distinct, -bit primes, with
p = 2p + 1 and q = 2q + 1, where p and q are prime. The algorithm also takes
as input ,   (Zn)2. It outputs either "failure," or integers x, y, not both zero,
such that xy = 1. Furthermore, assume that A runs in expected polynomial
time, and that for all n of the above form, and for randomly chosen ,   (Zn)2,
A succeeds in finding x, y as above with probability (n). Here, the probability
is taken over the random choice of  and , as well as the random choices made
during the execution of A on input (n, , ). Show how to use A to construct
another probabilistic algorithm A that takes as input n as above, runs in expected
polynomial time, and that satisfies the following property:

         if (n)  0.001, then A factors n with probability at least 0.999.

                                          10.5 Notes

The Miller-Rabin test is due to Miller [67] and Rabin [79]. The paper by Miller
defined the set Ln, but did not give a probabilistic analysis. Rather, Miller showed
that under a generalization of the Riemann hypothesis, for composite n, the least
positive integer a such that [a]n  Zn \ Ln is at most O((log n)2), thus giving rise
to a deterministic primality test whose correctness depends on the above unproved
hypothesis. The later paper by Rabin re-interprets Miller's result in the context of
probabilistic algorithms.
                      10.5 Notes                                             325

   Bach [10] gives an explicit version of Miller's result, showing that under the

same assumptions, the least positive integer a such that [a]n  Zn \ Ln is at most
2(log n)2; more generally, Bach shows that the following holds under a generaliza-

tion of the Riemann hypothesis:

For every positive integer n, and every subgroup G Zn, the least
positive integer a such that [a]n  Zn\G is at most 2(log n)2, and the
least positive integer b such that [b]n  Zn \ G is at most 3(log n)2.

The first efficient probabilistic primality test was invented by Solovay and Strassen

[99] (their paper was actually submitted for publication in 1974). Later, in Chap-

ter 21, we shall discuss a recently discovered, deterministic, polynomial-time

(though not very practical) primality test, whose analysis does not rely on any

unproved hypothesis.

Carmichael numbers are named after R. D. Carmichael, who was the first to

discuss them, in work published in the early 20th century. Alford, Granville, and

Pomerance [7] proved that there are infinitely many Carmichael numbers.

Exercise 10.4 is based on Lehmann [58].

Theorem 10.4, as well as the table of values just below it, are from Kim and

Pomerance [55]. In fact, these bounds hold for the weaker test based on Ln.

Our analysis in §10.3.2 is loosely based on a similar analysis in §4.1 of Maurer

[65]. Theorem 10.5 and its generalization in Exercise 10.9 are certainly not the best

results possible in this area. The general goal of "sieve theory" is to prove useful

upper and lower bounds for quantities like Rf (x, y) that hold when y is as large as

possible with respect to x. For example, using a technique known as Brun's pure

sieve, one can show that for log y < log x, there exist  and  , both of absolute

value at most 1, such that                       (1 - f (p)/p) +  x.
                                         

                Rf (x, y) = (1 + e- log x)x

                                             py

Thus, this gives us very sharp estimates for Rf (x, y) when x tends to infinity, and
y is bounded by any fixed polynomial in log x. For a proof of this result, see §2.2
of Halberstam and Richert [44] (the result itself is stated as equation 2.16). Brun's
pure sieve is really just the first non-trivial sieve result, developed in the early 20th
century; even stronger results, extending the useful range of y (but with larger error
terms), have subsequently been proved.

   Theorem 10.6, as well as the table of values immediately below it, are from
Damgård, Landrock, and Pomerance [32].

   The algorithm presented in §10.4 for factoring an integer given a multiple of
(n) (or, for that matter, (n)) is essentially due to Miller [67]. However, just as for
his primality test, Miller presents his algorithm as a deterministic algorithm, which
326  Probabilistic primality testing

he analyzes under a generalization of the Riemann hypothesis. The probabilistic
version of Miller's factoring algorithm appears to be "folklore."
                            11

    Finding generators and discrete logarithms in Zp

As we have seen in Theorem 7.28, for a prime p, Zp is a cyclic group of order
p - 1. This means that there exists a generator   Zp, such that each   Zp can
be written uniquely as  = x, where x is an integer with 0  x < p - 1; the integer
x is called the discrete logarithm of  to the base , and is denoted log .

   This chapter discusses some computational problems in this setting; namely,
how to efficiently find a generator , and given  and , how to compute log .

   More generally, if  generates a subgroup G of Zp of order q, where q | (p - 1),
and   G, then log  is defined to be the unique integer x with 0  x < q and
 = x. In some situations it is more convenient to view log  as an element of
Zq. Also for x  Zq, with x = [a]q, one may write x to denote a. There can be
no confusion, since if x = [a ]q, then a = a. However, in this chapter, we shall
view log  as an integer.

   Although we work in the group Zp, all of the algorithms discussed in this chapter
trivially generalize to any finite cyclic group that has a suitably compact repre-
sentation of group elements and an efficient algorithm for performing the group
operation on these representations.

                             11.1 Finding a generator for Zp
In this section, we consider the problem of how to find a generator for Zp. There
is no efficient algorithm known for this problem, unless the prime factorization of
p - 1 is given, and even then, we must resort to the use of a probabilistic algorithm.
Of course, factoring in general is believed to be a very difficult problem, so it
may not be easy to get the prime factorization of p - 1. However, if our goal is
to construct a large prime p, together with a generator for Zp, then we may use
Algorithm RFN in §9.6 to generate a random factored number n in some range,
test n + 1 for primality, and then repeat until we get a factored number n such that

                                               327
328  Finding generators and discrete logarithms in Zp

p = n + 1 is prime. In this way, we can generate a random prime p in a given range

along with the factorization of p - 1.
   We now present an efficient probabilistic algorithm that takes as input an odd

prime p, along with the prime factorization

                     r

     p - 1 = qiei ,

                   i=1

and outputs a generator for Zp. It runs as follows:

     for i  1 to r do

           repeat
                  choose   Zp at random
                  compute   (p-1)/qi

           until  = 1

           i  (p-1)/qiei

       ir=1 i
     output 

   First, let us analyze the correctness of this algorithm. When the ith loop iteration

terminates, by construction, we have

                                                  qiei qiei-1

                                   i = 1 but i = 1.

It follows (see Theorem 6.37) that i has multiplicative order qiei. From this, it
follows (see Theorem 6.38) that  has multiplicative order p - 1. Thus, we have

shown that if the algorithm terminates, its output is always correct.
   Let us now analyze the running time of this algorithm. Fix i = 1, . . . , r, and

consider the repeat/until loop in the ith iteration of the outer loop. Let Li be the

random variable whose value is the number of iterations of this repeat/until loop.
Since  is chosen at random from Zp, the value of  is uniformly distributed over
the image of the (p - 1)/qi-power map (see Theorem 8.5), and since the latter is a
subgroup of Zp of order qi (see Example 7.61), we see that  = 1 with probability
1/qi. Thus, Li has a geometric distribution with associated success probability
1 - 1/qi, and E[Li] = 1/(1 - 1/qi)  2 (see Theorem 9.3).

   Now set L := L1 + · · · + Lr. By linearity of expectation (Theorem 8.14), we have
E[L] = E[L1] + · · · + E[Lr]  2r. The running time Z of the entire algorithm is
O(L · len(p)3), and hence the expected running time is E[Z] = O(r len(p)3), and
since r  log2 p, we have E[Z] = O(len(p)4).

   Although this algorithm is quite practical, there are asymptotically faster algo-

rithms for this problem (see Exercise 11.2).
11.2 Computing discrete logarithms in Zp  329

EXERCISE 11.1. Suppose we are not given the prime factorization of p - 1, but

rather, just a prime q dividing p-1, and we want to find an element of multiplicative
order q in Zp. Design and analyze an efficient algorithm to do this.

EXERCISE 11.2. Suppose we are given a prime p, along with the prime factoriza-
tion p - 1 = ir=1 qiei .

    (a) If, in addition, we are given   Zp, show how to compute the multiplica-
         tive order of  in time O(r len(p)3). Hint: use Exercise 6.40.

   (b) Improve the running time bound to O(len(r) len(p)3). Hint: use Exer-

         cise 3.39.

    (c) Modifying the algorithm you developed for part (b), show how to construct
         a generator for Zp in expected time O(len(r) len(p)3).

EXERCISE 11.3. Suppose we are given a positive integer n, along with its prime
factorization n = p1e1 · · · prer , and that for each i = 1, . . . , r, we are also given the
prime factorization of pi - 1. Show how to efficiently compute the multiplicative
order of any element   Zn.

EXERCISE 11.4. Suppose there is an efficient algorithm that takes as input a pos-
itive integer n and an element   Zn, and computes the multiplicative order of .
Show how to use this algorithm to build an efficient integer factoring algorithm.

                      11.2 Computing discrete logarithms in Zp

In this section, we consider algorithms for computing the discrete logarithm of
  Zp to a given base . The algorithms we present here are, in the worst case,
exponential-time algorithms, and are by no means the best possible; however, in

some special cases, these algorithms are not so bad.

                                 11.2.1 Brute-force search

Suppose that   Zp generates a subgroup G of Zp of order q > 1 (not necessarily
prime), and we are given p, q, , and   G, and wish to compute log .

   The simplest algorithm to solve this problem is brute-force search:

         1
         i0
         while  =  do

                ·
               ii+1
         output i
330  Finding generators and discrete logarithms in Zp

   This algorithm is clearly correct, and the main loop will always halt after at

most q iterations (assuming, as we are, that   G). So the total running time is
O(q len(p)2).

                           11.2.2 Baby step/giant step method

As above, suppose that   Zp generates a subgroup G of Zp of order q > 1 (not
necessarily prime), and we are given p, q, , and   G, and wish to compute

log .
   A faster algorithm than brute-force search is the baby step/giant step method.

It works as follows.
   Let us choose an approximation m to q1/2. It does not have to be a very good

approximation -- we just need m = (q1/2). Also, let m = q/m , so that
m = (q1/2) as well.

   The idea is to compute all the values i for i = 0, . . . , m - 1 (the "baby steps")
and to build an "associative array" (or "lookup table") T that maps the key i
to the value i. For   Zp, we shall write T [] to denote the value associated
with the key , writing T [] =  if there is no such value. We shall assume

that T is implemented so that accessing T [] is fast. Using an appropriate data

structure, T can be implemented so that accessing individual elements takes time

O(len(p)). One such data structure is a radix tree (also called a search trie). Other

data structures may be used (for example, a hash table or a binary search tree), but
these may have somewhat different access times.

   We can build the associative array T using the following algorithm:

     initialize T // T [] =  for all   Zp
     1
     for i  0 to m - 1 do

           T []  i
            ·

Clearly, this algorithm takes time O(q1/2 len(p)2).
   After building the lookup table, we execute the following procedure (the "giant

steps"):

       -m                     i  T []

       , j  0, i  T []
     while i =  do

              ·  , j  j + 1,

     x  jm + i
     output x
11.2 Computing discrete logarithms in Zp  331

   To analyze this procedure, suppose that  = x with 0  x < q. Now, x can be
written in a unique way as x = vm + u, where u and v are integers with 0  u < m
and 0  v  m . In the jth loop iteration, for j = 0, 1, . . . , we have

 = -mj = (v-j)m+u.

So we will detect i =  precisely when j = v, in which case i = u. Thus, the

output will be correct, and the total running time of the algorithm (for both the
"baby steps" and "giant steps" parts) is easily seen to be O(q1/2 len(p)2).

   While this algorithm is much faster than brute-force search, it has the draw-
back that it requires space for about q1/2 elements of Zp. Of course, there is a
"time/space trade-off" here: by choosing m smaller, we get a table of size O(m),

but the running time will be proportional to O(q/m). In §11.2.5 below, we discuss
an algorithm that runs (at least heuristically) in time O(q1/2 len(q) len(p)2), but
which requires space for only a constant number of elements of Zp.

                                 11.2.3 Groups of order qe

Suppose that   Zp generates a subgroup G of Zp of order qe, where q > 1 and
e  1, and we are given p, q, e, , and   G, and wish to compute log .

   There is a simple algorithm that allows one to reduce this problem to the problem
of computing discrete logarithms in the subgroup of Zp of order q.

   It is perhaps easiest to describe the algorithm recursively. The base case is when
e = 1, in which case, we use an algorithm for the subgroup of Zp of order q. For
this, we might employ the algorithm in §11.2.2, or if q is very small, the algorithm

in §11.2.1.
   Suppose now that e > 1. We choose an integer f with 0 < f < e. Different

strategies for choosing f yield different algorithms -- we discuss this below. Sup-
pose  = x, where 0  x < qe. Then we can write x = qf v + u, where u and v are
integers with 0  u < qf and 0  v < qe-f . Therefore,

                                         qe-f = qe-f u.

Note that qe-f has multiplicative order qf , and so if we recursively compute the
discrete logarithm of qe-f to the base qe-f , we obtain u.

   Having obtained u, observe that

                                          /u = qf v.

Note also that qf has multiplicative order qe-f , and so if we recursively compute
the discrete logarithm of /u to the base qf , we obtain v, from which we then
compute x = qf v + u.
332  Finding generators and discrete logarithms in Zp

Let us put together the above ideas succinctly in a recursive procedure:

Algorithm RDL. On input p, q, e, ,  as above, do the following:

         if e = 1 then
               return log  // base case: use a different algorithm

         else

               select f  {1, . . . , e - 1}
               u  RDL(p, q, f , qe-f , qe-f ) // 0  u < qf
               v  RDL(p, q, e - f , qf , /u) // 0  v < qe-f
               return qf v + u

   To analyze the running time of this recursive algorithm, note that the running
time of the body of one recursive invocation (not counting the running time of the
recursive calls it makes) is O(e len(q) len(p)2). To calculate the total running time,
we have to sum up the running times of all the recursive calls plus the running
times of all the base cases.

   Regardless of the strategy for choosing f, the total number of base case invoca-
tions is e. Note that all the base cases compute discrete logarithms to the base qe-1.
Assuming we implement the base case using the baby step/giant step algorithm in
§11.2.2, the total running time for all the base cases is therefore O(eq1/2 len(p)2).

   The total running time for the recursion (not including the base case computa-
tions) depends on the strategy used to choose the split f . It is helpful to represent
the behavior of the algorithm using a recursion tree. This is a binary tree, where
every node represents one recursive invocation of the algorithm; the root of the
tree represents the initial invocation of the algorithm; for every node N in the tree,
if N represents the recursive invocation I, then N's children (if any) represent
the recursive invocations made by I. We can naturally organize the nodes of the
recursion tree by levels: the root of the recursion tree is at level 0, its children are
at level 1, its grandchildren at level 2, and so on. The depth of the recursion tree is
defined to be the maximum level of any node.

   We consider two different strategies for choosing the split f:
     · If we always choose f = 1 or f = e - 1, then the depth of the recursion
         tree is O(e). The running time contributed by each level of the recursion
         tree is O(e len(q) len(p)2), and so the total running time for the recursion is
         O(e2 len(q) len(p)2). Note that if f = 1, then the algorithm is essentially
         tail recursive, and so may be easily converted to an iterative algorithm with-
         out the need for a stack.

     · If we use a "balanced" divide-and-conquer strategy, choosing f  e/2,
         then the depth of the recursion tree is O(len(e)), while the running time
11.2 Computing discrete logarithms in Zp  333

contributed by each level of the recursion tree is still O(e len(q) len(p)2).
Thus, the total running time for the recursion is O(e len(e) len(q) len(p)2).

   Assuming we use the faster, balanced recursion strategy, and that we use the
baby step/giant step algorithm for the base case, the total running time of Algo-
rithm RDL is:

                           O((eq1/2 + e len(e) len(q)) · len(p)2).

                             11.2.4 Discrete logarithms in Zp

Suppose that we are given a prime p, along with the prime factorization

                                                                       r

                                        p - 1 = qiei ,

                                                                     i=1

a generator  for Zp, and   Zp. We wish to compute log .
   Suppose that  = x, where 0  x < p - 1. Then for i = 1, . . . , r, we have

                                  (p-1)/qiei =  (p-1)/qi . ei x

Note that (p-1)/qiei has multiplicative order qiei, and if xi is the discrete logarithm
of (p-1)/qiei to the base (p-1)/qiei , then we have 0  xi < qeii and x  xi (mod qeii ).

   Thus, if we compute the values x1, . . . , xr, using Algorithm RDL in §11.2.3,
we can obtain x using the algorithm of the Chinese remainder theorem (see Theo-
rem 4.6). If we define q := max{q1, . . . , qr}, then the running time of this algorithm
will be bounded by q1/2 len(p)O(1).

   We conclude that

         the difficulty of computing discrete logarithms in Zp is determined
         by the size of the largest prime dividing p - 1.

                 11.2.5 A space-efficient square-root time algorithm

We present a more space-efficient alternative to the algorithm in §11.2.2, the anal-

ysis of which we leave as a series of exercises for the reader.

   The algorithm makes a somewhat heuristic assumption that we have a function

that "behaves" for all practical purposes like a random function. Such functions can

indeed be constructed using cryptographic techniques under reasonable intractabil-

ity assumptions; however, for the particular application here, one can get by in

practice with much simpler constructions.
   Let p be a prime, q a prime dividing p - 1,  an element of Zp that generates a

subgroup G of Zp of order q, and   G. Let F be a function mapping elements of
G to {0, . . . , q - 1}. Define H : G  G to be the function that sends  to F ().
334  Finding generators and discrete logarithms in Zp

The algorithm runs as follows:

         i1

         x  0,   ,

         x  F (),   H()
         while  =  do

               x  (x + F ()) mod q,   H ()

               repeat 2 times
                      x  (x + F ( )) mod q,   H ( )

               ii+1

         if i < q then
               output (x - x )i-1 mod q

         else

               output "fail"

   To analyze this algorithm, let us define 1, 2, . . . , as follows: 1 :=  and for
i > 1, i := H (i-1).

EXERCISE 11.5. Show that each time the main loop of the algorithm is entered,
we have  = i = xi, and  = 2i = x 2i.

EXERCISE 11.6. Show that if the loop terminates with i < q, the value output is
equal to log .

EXERCISE 11.7. Let j be the smallest index such that j = k for some index
k < j. Show that j  q + 1 and that the loop terminates with i < j (and in
particular, i  q).

EXERCISE 11.8. Assume that F is a random function, meaning that it is chosen at
random, uniformly from among all functions from G into {0, . . . , q -1}. Show that
this implies that H is a random function, meaning that it is uniformly distributed
over all functions from G into G.

EXERCISE 11.9. Assuming that F is a random function as in the previous exercise,
apply the result of Exercise 8.45 to conclude that the expected running time of the
algorithm is O(q1/2 len(q) len(p)2), and that the probability that the algorithm fails
is exponentially small in q.

                11.3 The Diffie-Hellman key establishment protocol

One of the main motivations for studying algorithms for computing discrete loga-
rithms is the relation between this problem and the problem of breaking a protocol
called the Diffie-Hellman key establishment protocol, named after its inventors.
11.3 The Diffie-Hellman key establishment protocol  335

   In this protocol, Alice and Bob need never to have talked to each other before, but
nevertheless, can establish a shared secret key that nobody else can easily compute.
To use this protocol, a third party must provide a "telephone book," which contains
the following information:

     · p, q, and , where p and q are primes with q | (p - 1), and  is an element
         generating a subgroup G of Zp of order q;

     · an entry for each user, such as Alice or Bob, that contains the user's name,
         along with a "public key" for that user, which is an element of the group G.

   To use this system, Alice posts her public key in the telephone book, which is
of the form  = x, where x  {0, . . . , q - 1} is chosen by Alice at random. The
value x is Alice's "secret key," which Alice never divulges to anybody. Likewise,
Bob posts his public key, which is of the form  = y, where y  {0, . . . , q - 1} is
chosen by Bob at random, and is his secret key.

   To establish a shared key known only between them, Alice retrieves Bob's public
key  from the telephone book, and computes A := x. Likewise, Bob retrieves
Alice's public key , and computes B := y. It is easy to see that

                       A = x = (y)x = xy = (x)y = y = B,

and hence Alice and Bob share the same secret key  := A = B.
   Using this shared secret key, they can then use standard methods for encryption

and message authentication to hold a secure conversation. We shall not go any
further into how this is done; rather, we briefly (and only superficially) discuss
some aspects of the security of the key establishment protocol itself. Clearly, if
an attacker obtains  and  from the telephone book, and computes x = log ,
then he can compute Alice and Bob's shared key as  = x -- in fact, given x, an
attacker can efficiently compute any key shared between Alice and another user.

   Thus, if this system is to be secure, it should be very difficult to compute discrete
logarithms. However, the assumption that computing discrete logarithms is hard is
not enough to guarantee security. Indeed, it is not entirely inconceivable that the
discrete logarithm problem is hard, and yet the problem of computing  from 
and  is easy. The latter problem -- computing  from  and  -- is called the
Diffie-Hellman problem.

   As in the discussion of the RSA cryptosystem in §4.7, the reader is warned that
the above discussion about security is a bit of an oversimplification. A complete
discussion of all the security issues related to the above protocol is beyond the
scope of this text.

   Note that in our presentation of the Diffie-Hellman protocol, we work with a
generator of a subgroup G of Zp of prime order, rather than a generator for Zp.
There are several reasons for doing this: one is that there are no known discrete
336  Finding generators and discrete logarithms in Zp

logarithm algorithms that are any more practical in G than in Zp, provided the order
q of G is sufficiently large; another is that by working in G, the protocol becomes
substantially more efficient. In typical implementations, p is 1024 bits long, so as
to protect against subexponential-time algorithms such as those discussed later in
§15.2, while q is 160 bits long, which is enough to protect against the square-root-
time algorithms discussed in §11.2.2 and §11.2.5. The modular exponentiations
in the protocol will run several times faster using "short," 160-bit exponents rather
than "long," 1024-bit exponents.

   For the following exercise, we need the following notions from complexity the-
ory.

     · We say problem A is deterministic poly-time reducible to problem B if
         there exists a deterministic algorithm R for solving problem A that makes
         calls to a subroutine for problem B, where the running time of R (not
         including the running time for the subroutine for B) is polynomial in the
         input length.

     · We say that problems A and B are deterministic poly-time equivalent if A
         is deterministic poly-time reducible to B and B is deterministic poly-time
         reducible to A.

EXERCISE 11.10. Consider the following problems.
    (a) Given a prime p, a prime q that divides p - 1, an element   Zp generat-
         ing a subgroup G of Zp of order q, and two elements ,   G, compute
         xy, where x := log  and y := log . (This is just the Diffie-Hellman
         problem.)
   (b) Given a prime p, a prime q that divides p - 1, an element   Zp generating
         a subgroup G of Zp of order q, and an element   G, compute x2, where
         x := log .
    (c) Given a prime p, a prime q that divides p - 1, an element   Zp generating
         a subgroup G of Zp of order q, and two elements ,   G, with  = 1,
         compute xy , where x := log , y := y-1 mod q, and y := log .
   (d) Given a prime p, a prime q that divides p - 1, an element   Zp generating
         a subgroup G of Zp of order q, and an element   G, with  = 1, compute
         x , where x := x-1 mod q and x := log .

Show that these problems are deterministic poly-time equivalent. Moreover, your
reductions should preserve the values of p, q, and ; that is, if the algorithm that
reduces one problem to another takes as input an instance of the former problem
of the form (p, q, , . . .), it should invoke the subroutine for the latter problem with
inputs of the form (p, q, , . . .).
11.3 The Diffie-Hellman key establishment protocol          337

EXERCISE 11.11. Suppose there is a probabilistic algorithm A that takes as input a
prime p, a prime q that divides p - 1, and an element   Zp generating a subgroup
G of Zp of order q. The algorithm also takes as input   G. It outputs either
"failure," or log . Furthermore, assume that A runs in expected polynomial time,
and that for all p, q, and  of the above form, and for randomly chosen   G, A

succeeds in computing log  with probability (p, q, ). Here, the probability is
taken over the random choice of , as well as the random choices made during the

execution of A. Show how to use A to construct another probabilistic algorithm

A that takes as input p, q, and  as above, as well as   G, runs in expected

polynomial time, and that satisfies the following property:

if (p, q, )  0.001, then for all   G, A computes log  with
probability at least 0.999.

   The algorithm A in the previous exercise is an example of a random self-
reduction, which means an algorithm that reduces the task of solving an arbitrary
instance of a given problem to that of solving a random instance of the same prob-
lem. Intuitively, the existence of such a reduction means that the problem is no
harder in the worst case than on average.

EXERCISE 11.12. Let p be a prime, q a prime that divides p - 1,   Zp an
element that generates a subgroup G of Zp of order q, and   G. For   G,
a representation of  with respect to  and  is a pair of integers (r, s), with
0  r < q and 0  s < q, such that rs = .

    (a) Show that for every   G, there are precisely q representations (r, s) of 
         with respect to  and , and among these, there is precisely one with s = 0.

   (b) Show that given a representation (r, s) of 1 with respect to  and  such that
         s = 0, we can efficiently compute log .

    (c) Show that given any   G, along with any two distinct representations of
          with respect to  and , we can efficiently compute log .

   (d) Suppose we are given access to an "oracle" that, when presented with any

           G, tells us some representation of  with respect to  and . Show how
         to use this oracle to efficiently compute log .

   The following two exercises examine the danger of the use of "short" exponents
in discrete logarithm based cryptographic schemes that do not work with a group
of prime order.

EXERCISE 11.13. Let p be a prime and let p - 1 = q1e1 · · · qrer be the prime fac-
torization of p - 1. Let  be a generator for Zp. Let y be a positive number, and
let Qp(y) be the product of all the prime powers qiei with qi  y. Suppose you are
338  Finding generators and discrete logarithms in Zp

given p, y, the primes qi dividing p - 1 with qi  y, along with , an element  of
Zp, and a bound x^, where x := log  < x^. Show how to compute x in time

                            (y1/2 + (x^/Qp(y))1/2) · len(p)O(1).

EXERCISE 11.14. Continuing with the previous, let Qp(y) denote the product of
all the primes qi dividing p - 1 with qi  y. Note that Qp(y) | Qp(y). The goal of
this exercise is to estimate the expected value of log Qp(y), assuming p is a large,
random prime. To this end, let R be a random variable that is uniformly distributed
over all -bit primes, and assume that y  2 /3. Assuming Conjecture 5.22, show
that asymptotically (as  ), we have E[log QR(y)] = log y + O(1).

   The results of the previous two exercises caution against the use of "short" expo-
nents in cryptographic schemes based on the discrete logarithm problem for Zp.
For example, suppose that p is a random 1024-bit prime, and that for reasons
of efficiency, one chooses x^  2160, thinking that a method such as the baby
step/giant step method would require  280 steps to recover x. However, if we
choose y  280, then the above analysis implies that Qp(y) is at least  280 with
a reasonable probability, in which case x^/Qp(y) is at most  280, and so we can
in fact recover x in  240 steps (there are known methods to find the primes up to
y that divide p - 1 quickly enough). While 280 may not be a feasible number of
steps, 240 may very well be. Of course, none of these issues arise if one works in a
subgroup of Zp of large prime order, which is the recommended practice.

   An interesting fact about the Diffie-Hellman problem is that there is no known
efficient algorithm to recognize a solution to the problem. Some cryptographic
protocols actually rely on the apparent difficulty of this decision problem, which
is called the decisional Diffie-Hellman problem. The following three exercises

develop a random self-reducibility property for this decision problem.

EXERCISE 11.15. Let p be a prime, q a prime dividing p - 1, and  an element of
Zp that generates a subgroup G of order q. Let   G, and let H be the subgroup
of G × G generated by (, ). Let ~, ~ be arbitrary elements of G, and define the
map

                               : Zq × Zq  G × G
                                     ([r]q, [s]q)  (r~s, r~s).

Show that the definition of  is unambiguous, that  is a group homomorphism,
and that

     · if (~, ~ )  H, then Im  = H, and
     · if (~, ~ ) / H, then Im  = G × G.
11.3 The Diffie-Hellman key establishment protocol  339

EXERCISE 11.16. For p, q,  as in the previous exercise, let Dp,q, be the set of all
triples of the form (x, y, xy), and let Rp,q, be the set of all triples of the form
(x, y, z). Using the result from the previous exercise, design a probabilistic

algorithm that runs in expected polynomial time, and that on input p, q, , along
with a triple   Rp,q,, outputs a triple   Rp,q, such that

     · if   Dp,q,, then  is uniformly distributed over Dp,q,, and

     · if  / Dp,q,, then  is uniformly distributed over Rp,q,.

EXERCISE 11.17. Suppose that A is a probabilistic algorithm that takes as input
p, q,  as in the previous exercise, along with a triple   Rp,q,, and outputs either
0 or 1. Furthermore, assume that A runs in expected polynomial time. Define two

random variables, Xp,q, and Yp,q,, as follows:
     · Xp,q, is defined to be the output of A on input p, q, , and , where  is
         uniformly distributed over Dp,q,, and
     · Yp,q, is defined to be the output of A on input p, q, , and , where  is
         uniformly distributed over Rp,q,.

In both cases, the value of the random variable is determined by the random choice
of , as well as the random choices made by the algorithm. Define

(p, q, ) := P[Xp,q, = 1] - P[Yp,q, = 1] .

Using the result of the previous exercise, show how to use A to design a probabilis-
tic, expected polynomial-time algorithm that takes as input p, q,  as above, along
with   Rp,q,, and outputs either "yes" or "no," so that

         if (p, q, )  0.001, then for all   Rp,q,, the probability that A
         correctly determines whether   Dp,q, is at least 0.999.

Hint: use the Chernoff bound.

   The following exercise demonstrates that the problem of distinguishing "Diffie-
Hellman triples" from "random triples" is hard only if the order of the underlying
group is not divisible by any small primes, which is another reason we have chosen
to work with groups of large prime order.

EXERCISE 11.18. Assume the notation of the previous exercise, but let us drop

the restriction that q is prime. Design and analyze a deterministic algorithm A
that takes inputs p, q,  and   Rp,q,, that outputs 0 or 1, and that satisfies
the following property: if t is the smallest prime dividing q, then A runs in time
(t + len(p))O(1), and the "distinguishing advantage" (p, q, ) for A on inputs p, q, 

is at least 1/t.
340  Finding generators and discrete logarithms in Zp

                                          11.4 Notes

The probabilistic algorithm in §11.1 for finding a generator for Zp can be made
deterministic under a generalization of the Riemann hypothesis. Indeed, as dis-

cussed in §10.5, under such a hypothesis, Bach's result [10] implies that for each
prime q | (p - 1), the least positive integer a such that [a]p  Zp \ (Zp)q is at most
2 log p.

   Related to the problem of constructing a generator for Zp is the question of how
big is the smallest positive integer g such that [g]p is a generator for Zp; that is,
how big is the smallest (positive) primitive root modulo p. The best bounds on the

least primitive root are also obtained using the same generalization of the Riemann

hypothesis mentioned above. Under this hypothesis, Wang [104] showed that the
least primitive root modulo p is O(r6 len(p)2), where r is the number of distinct
prime divisors of p-1. Shoup [95] improved Wang's bound to O(r4 len(r)4 len(p)2)

by adapting a result of Iwaniec [50, 51] and applying it to Wang's proof. The
best unconditional bound on the smallest primitive root modulo p is p1/4+o(1) (this

bound is also in Wang [104]). Of course, even if there exists a small primitive root,
there is no known way to efficiently recognize a primitive root modulo p without
knowing the prime factorization of p - 1.

   As we already mentioned, all of the algorithms presented in this chapter are
completely "generic," in the sense that they work in any finite cyclic group -- we
really did not exploit any properties of Zp other than the fact that it is a cyclic
group. In fact, as far as such "generic" algorithms go, the algorithms presented

here for discrete logarithms are optimal [71, 98]. However, there are faster, "non-

generic" algorithms (though still not polynomial time) for discrete logarithms in
Zp. We shall examine one such algorithm later, in Chapter 15.

   The "baby step/giant step" algorithm in §11.2.2 is due to Shanks [91]. See, for

example, the book by Cormen, Leiserson, Rivest, and Stein [29] for appropriate

data structures to implement the lookup table used in that algorithm. In particular,

see Problem 12-2 in [29] for a brief introduction to radix trees, which is the data

structure that yields the best running time (at least in principle) for our application.

   The algorithms in §11.2.3 and §11.2.4 are variants of an algorithm published by

Pohlig and Hellman [75]. See Chapter 4 of [29] for details on how one analyzes

recursive algorithms, such as the one presented in §11.2.3; in particular, Section

4.2 in [29] discusses in detail the notion of a recursion tree.

   The algorithm in §11.2.5 is a variant of an algorithm of Pollard [76]; in fact,
Pollard's algorithm is a bit more efficient than the one presented here, but the

analysis of its running time depends on stronger heuristics. Pollard's paper also

describes an algorithm for computing discrete logarithms that lie in a restricted

interval -- if the interval has width w, this algorithm runs (heuristically) in time
11.4 Notes  341

w1/2 len(p)O(1), and requires space for O(len(w)) elements of Zp. This algorithm
is useful in reducing the space requirement for the algorithm of Exercise 11.13.

   The key establishment protocol in §11.3 is from Diffie and Hellman [34]. That

paper initiated the study of public key cryptography, which has proved to be a

very rich field of research. Exercises 11.13 and 11.14 are based on van Oorschot
and Wiener [74]. For more on the decisional Diffie-Hellman assumption, see

Boneh [18].
                            12

 Quadratic reciprocity and computing modular square
                              roots

In §2.8, we initiated an investigation of quadratic residues. This chapter continues
this investigation. Recall that an integer a is called a quadratic residue modulo a
positive integer n if gcd(a, n) = 1 and a  b2 (mod n) for some integer b.

   First, we derive the famous law of quadratic reciprocity. This law, while his-
torically important for reasons of pure mathematical interest, also has important
computational applications, including a fast algorithm for testing if an integer is a
quadratic residue modulo a prime.

   Second, we investigate the problem of computing modular square roots: given a
quadratic residue a modulo n, compute an integer b such that a  b2 (mod n). As
we will see, there are efficient probabilistic algorithms for this problem when n is
prime, and more generally, when the factorization of n into primes is known.

                                12.1 The Legendre symbol
For an odd prime p and an integer a with gcd(a, p) = 1, the Legendre symbol
(a | p) is defined to be 1 if a is a quadratic residue modulo p, and -1 otherwise. For
completeness, one defines (a | p) = 0 if p | a. The following theorem summarizes
the essential properties of the Legendre symbol.
Theorem 12.1. Let p be an odd prime, and let a, b  Z. Then we have:

    (i) (a | p)  a(p-1)/2 (mod p); in particular, (-1 | p) = (-1)(p-1)/2;
   (ii) (a | p)(b | p) = (ab | p);
   (iii) a  b (mod p) implies (a | p) = (b | p);
   (iv) (2 | p) = (-1)(p2-1)/8;
    (v) if q is an odd prime, then (p | q) = (-1) 2 2 p (q | p). -1 q-1
   Part (i) of the theorem is just a restatement of Euler's criterion (Theorem 2.21).

                                               342
12.1 The Legendre symbol                                    343

As was observed in Theorem 2.31, this implies that -1 is a quadratic residue mod-
ulo p if and only if p  1 (mod 4). Thus, the quadratic residuosity of -1 modulo p
is determined by the residue class of p modulo 4.

   Part (ii) of the theorem follows immediately from part (i), and part (iii) is an
immediate consequence of the definition of the Legendre symbol.

   Part (iv), which we will prove below, can also be recast as saying that 2 is a
quadratic residue modulo p if and only if p  ±1 (mod 8). Thus, the quadratic
residuosity of 2 modulo p is determined by the residue class of p modulo 8.

   Part (v), which we will also prove below, is the law of quadratic reciprocity.
Note that when p = q, both (p | q) and (q | p) are zero, and so the statement of part
(v) is trivially true -- the interesting case is when p = q, and in this case, part (v)
is equivalent to saying that

                                                                                                           p-1 q-1

                                 (p | q)(q | p) = (-1) 2 2 .

Thus, the Legendre symbols (p | q) and (q | p) have the same values if and only
if either p  1 (mod 4) or q  1 (mod 4). As the following examples illustrate,
this result also shows that for a given odd prime q, the quadratic residuosity of q
modulo another odd prime p is determined by the residue class of p modulo either
q or 4q.

Example 12.1. Let us characterize those primes p modulo which 5 is a quadratic
residue. Since 5  1 (mod 4), the law of quadratic reciprocity tells us that
(5 | p) = (p | 5). Now, among the numbers ±1, ±2, the quadratic residues
modulo 5 are ±1. It follows that 5 is a quadratic residue modulo p if and only if
p  ±1 (mod 5). This example obviously generalizes, replacing 5 by any prime
q  1 (mod 4), and replacing the above congruences modulo 5 by appropriate
congruences modulo q. 2

Example 12.2. Let us characterize those primes p modulo which 3 is a quadratic

residue. Since 3  1 (mod 4), we must be careful in our application of the law of
quadratic reciprocity. First, suppose that p  1 (mod 4). Then (3 | p) = (p | 3),

and so 3 is a quadratic residue modulo p if and only if p  1 (mod 3). Second,
suppose that p  1 (mod 4). Then (3 | p) = -(p | 3), and so 3 is a quadratic

residue modulo p if and only if p  -1 (mod 3). Putting this all together, we see

that 3 is quadratic residue modulo p if and only if

p  1 (mod 4) and p  1 (mod 3)

or
                          p  -1 (mod 4) and p  -1 (mod 3).

Using the Chinese remainder theorem, we can restate this criterion in terms of
344  Quadratic reciprocity and computing modular square roots

residue classes modulo 12: 3 is quadratic residue modulo p if and only if p 
±1 (mod 12). This example obviously generalizes, replacing 3 by any prime
q  -1 (mod 4), and replacing the above congruences modulo 12 by appropriate
congruences modulo 4q. 2

   The rest of this section is devoted to a proof of parts (iv) and (v) of Theo-
rem 12.1. The proof is completely elementary, although a bit technical.

Theorem 12.2 (Gauss' lemma). Let p be an odd prime and let a be an integer not

divisible by p. Define j := ja mod p for j = 1, . . . , (p - 1)/2, and let n be the
number of indices j for which j > p/2. Then (a | p) = (-1)n.

Proof. Let r1, . . . , rn denote the values j that exceed p/2, and let s1, . . . , sk denote

the remaining values j. The ri's and si's are all distinct and non-zero. We have
0 < p - ri < p/2 for i = 1, . . . , n, and no p - ri is an sj; indeed, if p - ri = sj,
then sj  -ri (mod p), and writing sj = ua mod p and ri = va mod p, for some
u, v = 1, . . . , (p-1)/2, we have ua  -va (mod p), which implies u  -v (mod p),

which is impossible.

   It follows that the sequence of numbers s1, . . . , sk, p - r1, . . . , p - rn is just a

reordering of 1, . . . , (p - 1)/2. Then we have

     ((p - 1)/2)!  s1 · · · sk(-r1) · · · (-rn)
                     (-1)ns1 · · · skr1 · · · rn
                     (-1)n((p - 1)/2)! a(p-1)/2 (mod p),

and canceling the factor ((p - 1)/2)!, we obtain a(p-1)/2  (-1)n (mod p), and the
result follows from the fact that (a | p)  a(p-1)/2 (mod p). 2

Theorem 12.3. If p is an odd prime and gcd(a, 2p) = 1, then (a | p) = (-1)t
where t = j=1 (p-1)/2 ja/p . Also, (2 | p) = (-1)(p2-1)/8.

Proof. Let a be an integer not divisible by p, but which may be even, and let us

adopt the same notation as in the statement and proof of Theorem 12.2; in par-

ticular, 1, . . . , (p-1)/2, r1, . . . , rn, and s1, . . . , sk are as defined there. Note that
ja = p ja/p + j, for j = 1, . . . , (p - 1)/2, so we have

     (p-1)/2        (p-1)/2          n    k

              ja =           p ja/p  + rj + sj.                (12.1)

     j=1            j=1              j=1  j=1

Moreover, as we saw in the proof of Theorem 12.2, the sequence of numbers
                                12.1 The Legendre symbol                                                                  345

s1, . . . , sk, p - r1, . . . , p - rn is a reordering of 1, . . . , (p - 1)/2, and hence

                (p-1)/2      n             k              n                                                            k

                         j = (p - rj) + sj = np - rj + sj.                                                                (12.2)

                j=1      j=1               j=1            j=1                                                       j=1

Subtracting (12.2) from (12.1), we get

                         (p-1)/2       (p-1)/2                                                                      n

                (a - 1)           j=p           ja/p - n + 2 rj.                                                          (12.3)

                         j=1               j=1                                                                      j=1

Note that

                                  (p-1)/2 j = p2 - 1 ,                                                                    (12.4)
                                   j=1 8

which together with (12.3) implies

                (a - 1)      p2 - 1 (p-1)/2
                                                ja/p - n (mod 2).                                                         (12.5)
                                8 j=1

If a is odd, (12.5) implies

                                  (p-1)/2

                             n             ja/p (mod 2).                                                                  (12.6)

                                  j=1

If a = 2, then  2j/p     = 0 for j = 1, . . . , (p - 1)/2, and (12.5) implies

                                  n    p2 - 1   (mod 2).                                                                  (12.7)

                                       8

The theorem now follows from (12.6) and (12.7), together with Theorem 12.2. 2

   Note that this last theorem proves part (iv) of Theorem 12.1. The next theorem
proves part (v).

Theorem 12.4. If p and q are distinct odd primes, then

                                                                                                           p-1 q-1

                                 (p | q)(q | p) = (-1) 2 2 .

Proof. Let S be the set of pairs of integers (x, y) with 1  x  (p - 1)/2 and
1  y  (q - 1)/2. Note that S contains no pair (x, y) with qx = py, so let

us partition S into two subsets: S1 contains all pairs (x, y) with qx > py, and
S2 contains all pairs (x, y) with qx < py. Note that (x, y)  S1 if and only if
346  Quadratic reciprocity and computing modular square roots

1  x  (p - 1)/2 and 1  y  qx/p . So |S1| =                          (p-1)/2
|S2| = y=1 (q-1)/2 py/q . So we have
                                                                    x=1 qx/p . Similarly,

     p-1q-1                         (p-1)/2                         (q-1)/2

     22      = |S| = |S1| + |S2| =           qx/p +                          py/q ,

                                    x=1                             y=1

and Theorem 12.3 implies

                                                           p-1 q-1

             (p | q)(q | p) = (-1) 2 2 . 2

EXERCISE 12.1. Characterize those odd primes p for which (15 | p) = 1, in terms
of the residue class of p modulo 60.

EXERCISE 12.2. Let p be an odd prime. Show that the following are equivalent:
    (a) (-2 | p) = 1;
   (b) p  1 or 3 (mod 8);
    (c) p = r2 + 2t2 for some r, t  Z.

                                  12.2 The Jacobi symbol
Let a, n be integers, where n is positive and odd, so that n = q1 · · · qk, where the
qi's are odd primes, not necessarily distinct. Then the Jacobi symbol (a | n) is
defined as

                                (a | n) := (a | q1) · · · (a | qk),

where (a | qi) is the Legendre symbol. By definition, (a | 1) = 1 for all
a  Z. Thus, the Jacobi symbol essentially extends the domain of definition of
the Legendre symbol. Note that (a | n)  {0, ±1}, and that (a | n) = 0 if and only
if gcd(a, n) > 1. The following theorem summarizes the essential properties of the
Jacobi symbol.

Theorem 12.5. Let m, n be odd, positive integers, and let a, b  Z. Then we have:
    (i) (ab | n) = (a | n)(b | n);
   (ii) (a | mn) = (a | m)(a | n);

   (iii) a  b (mod n) implies (a | n) = (b | n);
   (iv) (-1 | n) = (-1)(n-1)/2;
    (v) (2 | n) = (-1)(n2-1)/8;
   (vi) (m | n) = (-1) 2 2 m-1 n-1 (n | m).
12.2 The Jacobi symbol  347

Proof. Parts (i)-(iii) follow directly from the definition (exercise).
   For parts (iv) and (vi), one can easily verify (exercise) that for all odd integers

n1, . . . , nk,

                                  k

                           (ni - 1)/2  (n1 · · · nk - 1)/2 (mod 2).

                                i=1

Part (iv) easily follows from this fact, along with part (ii) of this theorem and part
(i) of Theorem 12.1 (exercise). Part (vi) easily follows from this fact, along with
parts (i) and (ii) of this theorem, and part (v) of Theorem 12.1 (exercise).

   For part (v), one can easily verify (exercise) that for odd integers n1, . . . , nk,

                                 k

                           (n2i - 1)/8  (n21 · · · nk2 - 1)/8 (mod 2).

                                i=1

Part (v) easily follows from this fact, along with part (ii) of this theorem, and part
(iv) of Theorem 12.1 (exercise). 2

   As we shall see later, this theorem is extremely useful from a computational
point of view -- with it, one can efficiently compute (a | n), without having to
know the prime factorization of either a or n. Also, in applying this theorem it is
useful to observe that for all odd integers m, n,

     · (-1)(n-1)/2 = 1  n  1 (mod 4);
     · (-1)(n2-1)/8 = 1  n  ±1 (mod 8);
     · (-1)((m-1)/2)((n-1)/2) = 1  m  1 (mod 4) or n  1 (mod 4).

   Suppose a is a quadratic residue modulo n, so that a  b2 (mod n), where
gcd(a, n) = 1 = gcd(b, n). Then by parts (iii) and (i) of Theorem 12.5, we have
(a | n) = (b2 | n) = (b | n)2 = 1. Thus, if a is a quadratic residue modulo n, then
(a | n) = 1. The converse, however, does not hold: (a | n) = 1 does not imply that
a is a quadratic residue modulo n (see Exercise 12.3 below).

   It is sometimes useful to view the Jacobi symbol as a group homomorphism. Let
n be an odd, positive integer. Define the Jacobi map

                                     Jn : Zn  {±1}

                                            [a]n  (a | n).

First, we note that by part (iii) of Theorem 12.5, this definition is unambiguous.
Second, we note that since gcd(a, n) = 1 implies (a | n) = ±1, the image of Jn

is indeed contained in {±1}. Third, we note that by part (i) of Theorem 12.5, Jn

is a group homomorphism. Since Jn is a group homomorphism, it follows that its
kernel, Ker Jn, is a subgroup of Zn.
348  Quadratic reciprocity and computing modular square roots

EXERCISE 12.3. Let n be an odd, positive integer, and consider the Jacobi map

Jn.

    (a) Show that (Zn)2  Ker Jn.
   (b) Show that if n is the square of an integer, then Ker Jn = Zn.
    (c) Show that if n is not the square of an integer, then [Zn : Ker Jn] = 2 and

         [Ker Jn : (Zn)2] = 2r-1, where r is the number of distinct prime divisors of
         n.

EXERCISE 12.4. Let p and q be distinct primes, with p  q  3 (mod 4), and let
n := pq.

    (a) Show that [-1]n  Ker Jn \ (Zn)2, and from this, conclude that the cosets
         of (Zn)2 in Ker Jn are the two distinct cosets (Zn)2 and [-1]n(Zn)2.

   (b) Let   Zn \ Ker Jn. Show that the map from {0, 1} × {0, 1} × (Zn)2 to Zn
         that sends (a, b, ) to a(-1)b is a bijection.

                           12.3 Computing the Jacobi symbol

Suppose we are given an odd, positive integer n, along with an integer a, and we
want to compute the Jacobi symbol (a | n). Theorem 12.5 suggests the following
algorithm:

         1
         repeat

               // loop invariant: n is odd and positive

               a  a mod n
               if a = 0 then

                      if n = 1 then return  else return 0
               compute a , h such that a = 2ha and a is odd
               if h  0 (mod 2) and n  ±1 (mod 8) then   -
               if a  1 (mod 4) and n  1 (mod 4) then   -
               (a, n)  (n, a )
         forever

   That this algorithm correctly computes the Jacobi symbol (a | n) follows directly
from Theorem 12.5. Using an analysis similar to that of Euclid's algorithm, one
easily sees that the running time of this algorithm is O(len(a) len(n)).

EXERCISE 12.5. Develop a "binary" Jacobi symbol algorithm, that is, one that
uses only addition, subtractions, and "shift" operations, analogous to the binary
gcd algorithm in Exercise 4.6.
12.4 Testing quadratic residuosity  349

EXERCISE 12.6. This exercise develops a probabilistic primality test based on the
Jacobi symbol. For odd integer n > 1, define

                            Gn := {  Zn : (n-1)/2 = Jn()},

where Jn : Zn  {±1} is the Jacobi map.
    (a) Show that Gn is a subgroup of Zn.
   (b) Show that if n is prime, then Gn = Zn.
    (c) Show that if n is composite, then Gn Zn.
   (d) Based on parts (a)-(c), design and analyze an efficient probabilistic pri-
         mality test that works by choosing a random, non-zero element   Zn, and
         testing if   Gn.

                           12.4 Testing quadratic residuosity

In this section, we consider the problem of testing whether a is a quadratic residue
modulo n, for given integers a and n, from a computational perspective.

                                   12.4.1 Prime modulus

For an odd prime p, we can test if an integer a is a quadratic residue modulo p by
either performing the exponentiation a(p-1)/2 mod p or by computing the Legendre
symbol (a | p). Assume that 0  a < p. Using a standard repeated squaring
algorithm, the former method takes time O(len(p)3), while using the Euclidean-
like algorithm of the previous section, the latter method takes time O(len(p)2). So
clearly, the latter method is to be preferred.

                               12.4.2 Prime-power modulus
For an odd prime p, we know that a is a quadratic residue modulo pe if and only
if a is a quadratic residue modulo p (see Theorem 2.30). So this case immediately
reduces to the previous one.

                                12.4.3 Composite modulus

For odd, composite n, if we know the factorization of n, then we can also determine
if a is a quadratic residue modulo n by determining if it is a quadratic residue
modulo each prime divisor p of n (see Exercise 2.39). However, without knowledge
of this factorization (which is in general believed to be hard to compute), there is
no efficient algorithm known. We can compute the Jacobi symbol (a | n); if this
350  Quadratic reciprocity and computing modular square roots

is -1 or 0, we can conclude that a is not a quadratic residue; otherwise, we cannot
conclude much of anything.

                        12.5 Computing modular square roots

In this section, we consider the problem of computing a square root of a modulo n,
given integers a and n, where a is a quadratic residue modulo n.

                                   12.5.1 Prime modulus
Let p be an odd prime, and let a be an integer such that 0 < a < p and (a | p) = 1.
We would like to compute a square root of a modulo p. Let  := [a]p  Zp, so
that we can restate our problem as that of finding   Zp such that 2 = , given
  (Zp)2.

   We first consider the special case where p  3 (mod 4), in which it turns out that
this problem can be solved very easily. Indeed, we claim that in this case

                                          := (p+1)/4

is a square root of  -- note that since p  3 (mod 4), the number (p + 1)/4 is an
integer. To show that 2 = , suppose  = ~2 for some ~  Zp. We know that
there is such a ~, since we are assuming that   (Zp)2. Then we have

                              2 = (p+1)/2 = ~p+1 = ~2 = ,

where we used Fermat's little theorem for the third equality. Using a repeated-
squaring algorithm, we can compute  in time O(len(p)3).

   Now we consider the general case, where we may have p  3 (mod 4). Here
is one way to efficiently compute a square root of , assuming we are given, in
addition to , an auxiliary input   Zp \ (Zp)2 (how one obtains such a  is
discussed below).

   Let us write p-1 = 2hm, where m is odd. For every   Zp, m has multiplicative
order dividing 2h. Since 2h-1m = 1, m has multiplicative order dividing 2h-1.
Since 2h-1m = -1, m has multiplicative order precisely 2h. Since there is only
one subgroup of Zp of order 2h, it follows that m generates this subgroup, and that
m = mx for some integer x, where 0  x < 2h and x is even. We can find x
by computing the discrete logarithm of m to the base m, using the algorithm in
§11.2.3. Setting  = mx/2, we have

                                            2 = m.

   We are not quite done, since we now have a square root of m, and not of .
12.5 Computing modular square roots  351

Since m is odd, we may write m = 2t + 1 for some non-negative integer t. It then
follows that

                        (-t)2 = 2-2t = m-2t = m-2t = .

Thus, -t is a square root of .
   Let us summarize the above algorithm for computing a square root of   (Zp)2,

assuming we are given   Zp \ (Zp)2, in addition to :

         compute positive integers m, h such that p - 1 = 2hm with m odd
           m,   m
         compute x  log  // note that 0  x < 2h and x is even
           ( )x/2- m/2

         output 

   The work done outside the discrete logarithm calculation amounts to just a hand-
ful of exponentiations modulo p, and so takes time O(len(p)3). The time to com-
pute the discrete logarithm is O(h len(h) len(p)2). So the total running time of this
procedure is

                               O(len(p)3 + h len(h) len(p)2).

   The above procedure assumed we had at hand a non-square . If h = 1, which
means that p  3 (mod 4), then (-1 | p) = -1, and so we are done. However, we
have already seen how to efficiently compute a square root in this case.

   If h > 1, we can find a non-square  using a probabilistic search algorithm.

Simply choose  at random, test if it is a square, and if so, repeat. The proba-
bility that a random element of Zp is a square is 1/2; thus, the expected number
of trials until we find a non-square is 2; moreover, the running time per trial is
O(len(p)2), and hence the expected running time of this probabilistic search algo-
rithm is O(len(p)2).

                               12.5.2 Prime-power modulus

Let p be an odd prime, let a be an integer relatively prime to p, and let e > 1 be
an integer. We know that a is a quadratic residue modulo pe if and only if a is a

quadratic residue modulo p. Suppose that a is a quadratic residue modulo p, and
that we have found an integer b such that b2  a (mod p), using, say, one of the

procedures described in §12.5.1. From this, we can easily compute a square root
of a modulo pe using the following technique, which is known as Hensel lifting.

   More generally, suppose that for some f  1, we have computed an integer
b satisfying the congruence b2  a (mod pf ), and we want to find an integer c
satisfying the congruence c2  a (mod pf+1). Clearly, if c2  a (mod pf+1), then
352  Quadratic reciprocity and computing modular square roots

c2  a (mod pf ), and so c  ±b (mod pf ). So let us set c = b + pf h, and solve for
h. We have

         c2  (b + pf h)2  b2 + 2bpf h + p2f h2  b2 + 2bpf h (mod pf+1).

So we want to find an integer h satisfying the linear congruence

     2bpf h  a - b2 (mod pf+1).                                   (12.8)

Since p 2b, we have gcd(2bpf , pf+1) = pf . Furthermore, since b2  a (mod pf ),
we have pf | (a - b2). Therefore, Theorem 2.5 implies that (12.8) has a unique
solution h modulo p, which we can efficiently compute as in Example 4.3.

   By iterating the above procedure, starting with a square root of a modulo p, we
can quickly find a square root of a modulo pe. We leave a detailed analysis of the

running time of this procedure to the reader.

                                12.5.3 Composite modulus

To find square roots modulo n, where n is an odd composite modulus, if we know
the prime factorization of n, then we can use the above procedures for finding
square roots modulo primes and prime powers, and then use the algorithm of the
Chinese remainder theorem to get a square root modulo n.

   However, if the factorization of n is not known, then there is no efficient algo-
rithm known for computing square roots modulo n. In fact, one can show that
the problem of finding square roots modulo n is at least as hard as the problem of
factoring n, in the sense that if there is an efficient algorithm for computing square
roots modulo n, then there is an efficient (probabilistic) algorithm for factoring n.

   We now present an algorithm to factor n, using a modular square-root algorithm
A as a subroutine. For simplicity, we assume that A is deterministic, and that for all
n and for all   (Zn)2, A(n, ) outputs a square root of . Also for simplicity, we
shall assume that n is of the form n = pq, where p and q are distinct, odd primes.
In Exercise 12.15 below, you are asked to relax these restrictions. Our algorithm
runs as follows:

                 ¢+

           Zn , d  gcd(rep(), n)
         if d > 1 then

               output d
         else

                 2,   A(n, )
               if  = ±

                      then output "failure"
                      else output gcd(rep( -  ), n)
                          12.5 Computing modular square roots                 353

Here,    +  denotes  the  set  of  non-zero  elements  of  Zn.   Also, recall that rep()

       Zn

denotes the canonical representative of .

   First, we argue that the algorithm outputs either "failure" or a non-trivial factor
of n. Clearly, if  / Zn, then the value d computed by the algorithm is a non-trivial
factor. So suppose   Zn. In this case, the algorithm invokes A on inputs n and
 := 2, obtaining a square root  of . Suppose that  = ± , and set  :=  -  .

What we need to show is that gcd(rep(), n) is a non-trivial factor of n. To see this,

consider the ring isomorphism of the Chinese remainder theorem

                                    : Zn  Zp × Zq
                                         [a]n  ([a]p, [a]q).

Suppose ( ) = (1, 2). Then the four square roots of  are

        = -1(1, 2), - = -1(-1, -2), -1(-1, 2), -1(1, -2).

The assumption that  = ± implies that () = (-1, 2) or () = (1, -2). In
the first case, () = (-21, 0), which implies gcd(rep(), n) = q. In the second
case, () = (0, -22), which implies gcd(rep(), n) = p.

   Second, we argue that P[F]  1/2, where F is the event that the algorithm

outputs "failure." Viewed as a random variable,  is uniformly distributed over
  +
      Clearly, P[F |  / Zn] = 0.   Now consider any fixed         (Zn)2.  Observe
Zn .
that the conditional distribution of  given that 2 =  is (essentially) the uniform

distribution on the set of four square roots of  . Also observe that the output of A
depends only on n and 2, and so with respect to the conditional distribution given
that 2 =  , the output  of A is fixed. Thus,

                     P[F | 2 =  ] = P[ = ± | 2 =  ] = 1/2.

Putting everything together, using total probability, we have

       P[F ] = P[F |  / Zn] P[ / Zn] +                 P[F | 2 =  ] P[2 =  ]

                                              (Zn )2

            = 0 · P[ / Zn] +               1 · P[2 =  ]  1 .
                                   22                         2
                                    (Zn )

   Thus, the above algorithm fails to split n with probability at most 1/2. If we like,
we can repeat the algorithm until it succeeds. The expected number of iterations
performed will be at most 2.

EXERCISE 12.7. Let p be an odd prime, and let f  Zp[X ] be a polynomial with
0  deg(f )  2. Design and analyze an efficient, deterministic algorithm that
354  Quadratic reciprocity and computing modular square roots

takes as input p, f , and an element of Zp \ (Zp)2, and which determines if f has
any roots in Zp, and if so, finds all of the roots. Hint: see Exercise 7.17.

EXERCISE 12.8. Show how to deterministically compute square roots modulo
primes p  5 (mod 8) in time O(len(p)3).

EXERCISE 12.9. This exercise develops an alternative algorithm for computing
square roots modulo a prime. Let p be an odd prime, let   Zp, and set  := 2.
Define B := {  Zp : 2 -   (Zp)2}.

    (a) Show that B = {  Zp : g() = 0}, where
                        g := (X - )(p-1)/2 - (X + )(p-1)/2  Zp[X ].

   (b) Let   Zp \ B, and suppose 2 = . Let µ,  be the uniquely determined
         elements of Zp satisfying the polynomial congruence
                            µ + X  ( - X )(p-1)/2 (mod X 2 - ).

         Show that µ = 0 and -2 = .

    (c) Using parts (a) and (b), design and analyze a probabilistic algorithm that
         computes a square root of a given   (Zp)2 in expected time O(len(p)3).

   Note that when p - 1 = 2hm (m odd), and h is large (e.g., h  len(p)/2), the
algorithm in the previous exercise is asymptotically faster than the one in §12.5.1;
however, the latter algorithm is likely to be faster in practice for the typical case
where h is small.

EXERCISE 12.10. Show that the following two problems are deterministic, poly-
time equivalent (see discussion just above Exercise 11.10 in §11.3):

    (a) Given an odd prime p and   (Zp)2, find   Zp such that 2 = .
   (b) Given an odd prime p, find an element of Zp \ (Zp)2.

EXERCISE 12.11. Design and analyze an efficient, deterministic algorithm that
takes as input primes p and q, such that q | (p - 1), along with an element   Zp,
and determines whether or not   (Zp)q.

EXERCISE 12.12. Design and analyze an efficient, deterministic algorithm that
takes as input primes p and q, such that q | (p - 1) but q2 (p - 1), along with an
element   (Zp)q, and computes a qth root of , that is, an element   Zp such
that q = .

EXERCISE 12.13. Design and analyze an algorithm that takes as input primes p

and q, such that q | (p - 1), along with an element   (Zp)q, and computes a qth
root of . (Unlike Exercise 12.12, we now allow q2 | (p - 1).) Your algorithm may
12.6 The quadratic residuosity assumption                              355

be probabilistic, and should have an expected running time that is bounded by q1/2
times a polynomial in len(p). Hint: Exercise 4.13 may be useful.

EXERCISE 12.14. Let p be an odd prime,  be a generator for Zp, and  be any
element of Zp. Define

B(p, , ) :=  1 if log   (p - 1)/2;
             0 if log  < (p - 1)/2.

Suppose that there is an algorithm that efficiently computes B(p, , ) for all p, , 
as above. Show how to use this algorithm as a subroutine in an efficient, proba-

bilistic algorithm that computes log  for all p, ,  as above. Hint: in addition to
the algorithm that computes B, use algorithms for testing quadratic residuosity and
computing square roots modulo p, and "read off" the bits of log  one at a time.

EXERCISE 12.15. Suppose there is a probabilistic algorithm A that takes as input
a positive integer n, and an element   (Zn)2. Assume that for all n, and for a
randomly chosen   (Zn)2, A computes a square root of  with probability at least
0.001. Here, the probability is taken over the random choice of  and the random

choices of A. Show how to use A to construct another probabilistic algorithm

A that takes n as input, runs in expected polynomial time, and that satisfies the

following property:

for all n, A outputs the complete factorization of n into primes with
probability at least 0.999.

EXERCISE 12.16. Suppose there is a probabilistic algorithm A that takes as input
positive integers n and m, and an element   (Zn)m. It outputs either "failure,"
or an mth root of . Furthermore, assume that A runs in expected polynomial
time, and that for all n and m, and for randomly chosen   (Zn)m, A succeeds
in computing an mth root of  with probability (n, m). Here, the probability is

taken over the random choice of , as well as the random choices made during the

execution of A. Show how to use A to construct another probabilistic algorithm A
that takes as input n, m, and   (Zn)m, runs in expected polynomial time, and that
satisfies the following property:

         if (n, m)  0.001, then for all   (Zn)m, A computes an mth root
         of  with probability at least 0.999.

                     12.6 The quadratic residuosity assumption

Loosely speaking, the quadratic residuosity (QR) assumption is the assumption
that it is hard to distinguish squares from non-squares in Zn, where n is of the form
356  Quadratic reciprocity and computing modular square roots

n = pq, and p and q are distinct primes. This assumption plays an important role

in cryptography. Of course, since the Jacobi symbol is easy to compute, for this

assumption to make sense, we have to restrict our attention to elements of Ker Jn,
where Jn : Zn  {±1} is the Jacobi map. We know that (Zn)2  Ker Jn (see
Exercise 12.3). Somewhat more precisely, the QR assumption is the assumption
that it is hard to distinguish a random element in Ker Jn \ (Zn)2 from a random
element in (Zn)2, given n (but not its factorization!).

   To give a rough idea as to how this assumption may be used in cryptography,
assume that p  q  3 (mod 4), so that [-1]n  Ker Jn \ (Zn)2, and moreover,
Ker Jn \ (Zn)2 = [-1]n(Zn)2 (see Exercise 12.4). The value n can be used as a
public key in a public-key cryptosystem (see §4.7). Alice, knowing the public key,
can encrypt a single bit b  {0, 1} as  := (-1)b2, where Alice chooses   Zn
at random. The point is, if b = 0, then  is uniformly distributed over (Zn)2, and
if b = 1, then  is uniformly distributed over Ker Jn \ (Zn)2. Now Bob, knowing
the secret key, which is the factorization of n, can easily determine if   (Zn)2
or not, and hence deduce the value of the encrypted bit b. However, under the QR
assumption, an eavesdropper, seeing just n and , cannot effectively figure out what

b is.
   Of course, the above scheme is much less efficient than the RSA cryptosystem

presented in §4.7, but nevertheless, has attractive properties; in particular, its secu-

rity is very closely tied to the QR assumption, whereas the security of RSA is a bit

less well understood.

EXERCISE 12.17. Suppose that A is a probabilistic algorithm that takes as input n
of the form n = pq, where p and q are distinct primes such that p  q  3 (mod 4).
The algorithm also takes as input   Ker Jn, and outputs either 0 or 1. Fur-
thermore, assume that A runs in expected polynomial time. Define two random

variables, Xn and Yn, as follows: Xn is defined to be the output of A on input n and
a value  chosen at random from Ker Jn\(Zn)2, and Yn is defined to be the output of
A on input n and a value  chosen at random from (Zn)2. In both cases, the value
of the random variable is determined by the random choice of , as well as the
random choices made by the algorithm. Define (n) := |P[Xn = 1] - P[Yn = 1]|.
Show how to use A to design a probabilistic, expected polynomial time algorithm

A that takes as input n as above and   Ker Jn, and outputs either "square" or
"non-square," with the following property:

         if (n)  0.001, then for all   Ker Jn, the probability that A
         correctly identifies whether   (Zn)2 is at least 0.999.

Hint: use the Chernoff bound.
12.7 Notes  357

EXERCISE 12.18. Assume the same notation as in the previous exercise. Define

the random variable Xn to be the output of A on input n and a value  chosen
at random from Ker Jn. Show that |P[Xn = 1] - P[Yn = 1]| = (n)/2. Thus,
the problem of distinguishing Ker Jn from (Zn)2 is essentially equivalent to the
problem of distinguishing Ker Jn \ (Zn)2 from (Zn)2.

                                          12.7 Notes

The proof we present here of Theorem 12.1 is essentially the one from Niven and
Zuckerman [72]. Our proof of Theorem 12.5 follows closely the one found in Bach
and Shallit [11].

   Exercise 12.6 is based on Solovay and Strassen [99].
   The probabilistic algorithm in §12.5.1 can be made deterministic under a gen-
eralization of the Riemann hypothesis. Indeed, as discussed in §10.5, under such
a hypothesis, Bach's result [10] implies that the least positive integer that is not
a quadratic residue modulo p is at most 2 log p (this follows by applying Bach's
result with the subgroup (Zp)2 of Zp). Thus, we may find the required element
  Zp \ (Zn)2 in deterministic polynomial time, just by brute-force search. The
best unconditional bound on the smallest positive integer that is not a quadratic
residue modulo p is due to Burgess [22], who gives a bound of p+o(1), where
 := 1/(4e)  0.15163.
   Goldwasser and Micali [41] introduced the quadratic residuosity assumption to
cryptography (as discussed in §12.6). This assumption has subsequently been used
as the basis for numerous cryptographic schemes.
                            13

                 Modules and vector spaces

In this chapter, we introduce the basic definitions and results concerning modules
over a ring R and vector spaces over a field F . The reader may have seen some
of these notions before, but perhaps only in the context of vector spaces over a
specific field, such as the real or complex numbers, and not in the context of, say,
finite fields like Zp.

                  13.1 Definitions, basic properties, and examples
Throughout this section, R denotes a ring (i.e., a commutative ring with unity).
Definition 13.1. An R-module is a set M together with an addition operation on
M and a function µ : R × M  M, such that the set M under addition forms an
abelian group, and moreover, for all c, d  R and ,   M, we have:

    (i) µ(c, µ(d, )) = µ(cd, );
   (ii) µ(c + d, ) = µ(c, ) + µ(d, );
   (iii) µ(c,  + ) = µ(c, ) + µ(c, );
   (iv) µ(1R, ) = .
   One may also call an R-module M a module over R, and elements of R are
sometimes called scalars. The function µ in the definition is called a scalar mul-
tiplication map, and the value µ(c, ) is called the scalar product of c and .
Usually, we shall simply write c (or c · ) instead of µ(c, ). When we do this,
properties (i)-(iv) of the definition may be written as follows:

       c(d) = (cd), (c + d) = c + d, c( + ) = c + c, 1R = .
Note that there are two addition operations at play here: addition in R (such as
c + d) and addition in M (such as  + ). Likewise, there are two multiplication
operations at play: multiplication in R (such as cd) and scalar multiplication (such

                                               358
13.1 Definitions, basic properties, and examples  359

as c). Note that by property (i), we may write cd without any ambiguity, as both
possible interpretations, c(d) and (cd), yield the same value.

   For fixed c  R, the map that sends   M to c  M is a group homomor-
phism with respect to the additive group operation of M (by property (iii) of the
definition); likewise, for fixed   M, the map that sends c  R to c  M is
a group homomorphism from the additive group of R into the additive group of
M (by property (ii)). Combining these observations with basic facts about group
homomorphisms (see Theorem 6.19), we may easily derive the following basic
facts about R-modules:

Theorem 13.2. If M is a module over R, then for all c  R,   M, and k  Z,
we have:

    (i) 0R ·  = 0M ;
   (ii) c · 0M = 0M ;
   (iii) (-c) = -(c) = c(-);
   (iv) (kc) = k(c) = c(k).

Proof. Exercise. 2

   An R-module M may be trivial, consisting of just the zero element 0M . If R is
the trivial ring, then any R-module M is trivial, since for every   M, we have
 = 1R = 0R = 0M .

Example 13.1. The ring R itself can be viewed as an R-module in the obvious
way, with addition and scalar multiplication defined in terms of the addition and
multiplication operations of R. 2

Example 13.2. The set R×n, which consists of all of n-tuples of elements of R,
forms an R-module, with addition and scalar multiplication defined component-
wise: for  = (a1, . . . , an)  R×n,  = (b1, . . . , bn)  R×n, and c  R, we define

             +  := (a1 + b1, . . . , an + bn) and c := (ca1, . . . , can). 2

Example 13.3. The ring of polynomials R[X ] over R forms an R-module in the
natural way, with addition and scalar multiplication defined in terms of the addition
and multiplication operations of the polynomial ring. 2

Example 13.4. As in Example 7.39, let f be a non-zero polynomial over R with
lc(f )  R, and consider the quotient ring E := R[X ]/(f ). Then E is a module

over R, with addition defined in terms of the addition operation of E, and scalar
multiplication defined by c[g]f := [c]f · [g]f = [cg]f , for c  R and g  R[X ]. 2

Example 13.5. Generalizing Example 13.3, if E is any ring containing R as a
360  Modules and vector spaces

subring (i.e., E is an extension ring of R), then E is a module over R, with addi-
tion and scalar multiplication defined in terms of the addition and multiplication
operations of E. 2

Example 13.6. Any abelian group G, written additively, can be viewed as a Z-
module, with scalar multiplication defined in terms of the usual integer multiplica-

tion map (see Theorem 6.4). 2

Example 13.7. Let G be any group, written additively, whose exponent divides n.

Then we may define a scalar multiplication that maps [k]n  Zn and   G to k.
That this map is unambiguously defined follows from the fact that G has exponent
dividing n, so that if k  k (mod n), we have k - k  = (k - k ) = 0G, since
n | (k - k ). It is easy to check that this scalar multiplication map indeed makes G

into a Zn-module. 2

Example 13.8. Of course, viewing a group as a module does not depend on

whether or not we happen to use additive notation for the group operation. If
we specialize the previous example to the group G = Zp, where p is prime, then
we may view G as a Zp-1-module. However, since the group operation itself is
written multiplicatively, the "scalar product" of [k]p-1  Zp-1 and   Zp is the
power k. 2

Example 13.9. If M1, . . . , Mk are R-modules, then so is their direct product

M1 × · · · × Mk, where addition and scalar product are defined component-wise. If
M = M1 = · · · = Mk, we write this as M×k. 2

Example 13.10. If I is an arbitrary set, and M is an R-module, then Map(I, M),
which is the set of all functions f : I  M, may be naturally viewed as an R-
module, with point-wise addition and scalar multiplication: for f , g  Map(I, M)
and c  R, we define

          (f + g)(i) := f (i) + g(i) and (cf )(i) := cf (i) for all i  I. 2

                        13.2 Submodules and quotient modules
Again, throughout this section, R denotes a ring. The notions of subgroups and
quotient groups extend in the obvious way to R-modules.

Definition 13.3. Let M be an R-module. A subset N of M is a submodule (over
R) of M if

    (i) N is a subgroup of the additive group M, and
   (ii) c  N for all c  R and   N (i.e., N is closed under scalar multiplica-

         tion).
13.2 Submodules and quotient modules  361

   It is easy to see that a submodule N of an R-module M is also an R-module in
its own right, with addition and scalar multiplication operations inherited from M.

   Expanding the above definition, we see that a non-empty subset N of M is a
submodule if and only if for all c  R and all ,   N, we have

                            +   N, -  N, and c  N.

Observe that the condition -  N is redundant, as it is implied by the condition
c  N with c = -1R.

   Clearly, {0M } and M are submodules of M. For k  Z, it is easy to see that
not only are kM and M{k} subgroups of M (see Theorems 6.7 and 6.8), they are
also submodules of M. Moreover, for c  R,

              cM := {c :   M} and M{c} := {  M : c = 0M }

are also submodules of M. Further, for   M,

                                     R := {c : c  R}

is a submodule of M. Finally, if N1 and N2 are submodules of M, then N1 + N2
and N1  N2 are not only subgroups of M, they are also submodules of M. We
leave it to the reader to verify all these facts: they are quite straightforward.

   Let 1, . . . , k  M. The submodule
                                       R1 + · · · + Rk

is called the submodule (over R) generated by 1, . . . , k. It consists of all R-
linear combinations

                                      c11 + · · · + ckk,

where the ci's are elements of R, and is the smallest submodule of M that contains

the elements 1, . . . , k. We shall also write this submodule as 1, . . . , k R. As
a matter of definition, we allow k = 0, in which case this submodule is {0M }.
We say that M is finitely generated (over R) if M = 1, . . . , k R for some

1, . . . , k  M.

Example 13.11. For a given integer  0, define R[X ]< to be the set of polyno-

mials of degree less than . The reader may verify that R[X ]< is a submodule of
the R-module R[X ], and indeed, is the submodule generated by 1, X , . . . , X -1. If

  = 0, then this submodule is the trivial submodule {0R}. 2

Example 13.12. Let G be an abelian group. As in Example 13.6, we can view

G as a Z-module in a natural way. Subgroups of G are just the same thing as
submodules of G, and for a1, . . . , ak  G, the subgroup a1, . . . , ak is the same as
the submodule a1, . . . , ak Z. 2
362  Modules and vector spaces

Example 13.13. As in Example 13.1, we may view the ring R itself as an R-
module. With respect to this module structure, ideals of R are just the same thing
as submodules of R, and for a1, . . . , ak  R, the ideal (a1, . . . , ak) is the same as
the submodule a1, . . . , ak R. Note that for a  R, the ideal generated by a may
be written either as aR, using the notation introduced in §7.3, or as Ra, using the
notation introduced in this section. 2

Example 13.14. If E is an extension ring of R, then we may view E as an R-
module, as in Example 13.5. It is easy to see that every ideal of E is a submodule;
however, the converse is not true in general. Indeed, the submodule R[X ]< of
R[X ] discussed in Example 13.11 is not an ideal of the ring R[X ]. 2

   If N is a submodule of M, then in particular, it is also a subgroup of M, and
we can form the quotient group M/N in the usual way (see §6.3), which consists
of all cosets []N , where   M. Moreover, because N is closed under scalar
multiplication, we can also define a scalar multiplication on M/N in a natural
way. Namely, for c  R and   M, we define

                                      c · []N := [c]N .

As usual, one must check that this definition is unambiguous, which means that
c  c (mod N) whenever    (mod N). But this follows (as the reader
may verify) from the fact that N is closed under scalar multiplication. One can
also easily check that with scalar multiplication defined in this way, M/N is an
R-module; it is called the quotient module (over R) of M modulo N.

Example 13.15. Suppose E is an extension ring of R, and I is an ideal of E.

Viewing E as an R-module, I is a submodule of E, and hence the quotient ring

E/I may naturally be viewed as an R-module, with scalar multiplication defined
by c · []I := [c]I for c  R and   E. Example 13.4 is a special case of this,
applied to the extension ring R[X ] and the ideal (f ). 2

EXERCISE 13.1. Show that if N is a submodule of an R-module M, then a set
P  N is a submodule of M if and only if P is a submodule of N.

EXERCISE 13.2. Let M1 and M2 be R-modules, and let N1 be a submodule of
M1 and N2 a submodule of M2. Show that N1 × N2 is a submodule of M1 × M2.

EXERCISE 13.3. Show that if R is non-trivial, then the R-module R[X ] is not
finitely generated.
13.3 Module homomorphisms and isomorphisms  363

                  13.3 Module homomorphisms and isomorphisms

Again, throughout this section, R is a ring. The notion of a group homomorphism
extends in the obvious way to R-modules.

Definition 13.4. Let M and M be modules over R. An R-module homomor-
phism from M to M is a function  : M  M , such that

    (i)  is a group homomorphism from M to M , and
   (ii) (c) = c() for all c  R and   M.

   An R-module homomorphism is also called an R-linear map. We shall use

this terminology from now on. Expanding the definition, we see that a map
 : M  M is an R-linear map if and only if ( + ) = () + () and
(c) = c() for all ,   M and all c  R.

Example 13.16. If N is a submodule of an R-module M, then the inclusion map
i : N  M is obviously an R-linear map. 2

Example 13.17. Suppose N is a submodule of an R-module M. Then the natural
map (see Example 6.36)

                                        : M  M/N
                                                 []N

is not just a group homomorphism, it is also easily seen to be an R-linear map. 2

Example 13.18. Let M be an R-module, and let k be an integer. Then the k-
multiplication map on M (see Example 6.38) is not only a group homomorphism,
but it is also easily seen to be an R-linear map. Its image is the submodule kM,
and its kernel the submodule M{k}. 2

Example 13.19. Let M be an R-module, and let c be an element of R. The map

: MM
         c

is called c-multiplication map on M, and is easily seen to be an R-linear map

whose image is the submodule cM, and whose kernel is the submodule M{c}.
The set of all c  R for which cM = {0M } is called the R-exponent of M, and is
easily seen to be an ideal of R. 2

Example 13.20. Let M be an R-module, and let  be an element of M. The map

: RM
      c  c
364     Modules and vector spaces

is easily seen to be an R-linear map whose image is the submodule R (i.e., the
submodule generated by ). The kernel of this map is called the R-order of , and
is easily seen to be an ideal of R. 2

Example 13.21. Generalizing the previous example, let M be an R-module, and
let 1, . . . , k be elements of M. The map

     :  R×k  M

        (c1, . . . , ck)  c11 + · · · + ckk

is easily seen to be an R-linear map whose image is the submodule R1 +· · ·+Rk
(i.e., the submodule generated by 1, . . . , k). 2

Example 13.22. Suppose that M1, . . . , Mk are submodules of an R-module M.
Then the map

                            : M1 × · · · × Mk  M
                                     (1, . . . , k)  1 + · · · + k

is easily seen to be an R-linear map whose image is the submodule M1+· · ·+Mk. 2

Example 13.23. Let E be an extension ring of R. As we saw in Example 13.5,
E may be viewed as an R-module in a natural way. Let   E, and consider the
-multiplication map on E, which sends   E to   E. Then it is easy to see
that this is an R-linear map. 2

Example 13.24. Let E and E be extension rings of R, which may be viewed as

R-modules as in Example 13.5. Suppose that  : E  E is a ring homomorphism
whose restriction to R is the identity map (i.e., (c) = c for all c  R). Then  is an
R-linear map. Indeed, for every c  R and ,   E, we have (+) = ()+()
and (c) = (c)() = c(). 2

Example 13.25. Let G and G be abelian groups. As we saw in Example 13.6,

G and G may be viewed as Z-modules. In addition, every group homomorphism
 : G  G is also a Z-linear map. 2

   Since an R-module homomorphism is also a group homomorphism on the under-
lying additive groups, all of the statements in Theorem 6.19 apply. In particular, an
R-linear map is injective if and only if the kernel is trivial (i.e., contains only the
zero element). However, in the case of R-module homomorphisms, we can extend
Theorem 6.19, as follows:

Theorem 13.5. Let  : M  M be an R-linear map. Then:

    (i) for every submodule N of M, (N) is a submodule of M ; in particular
         (setting N := M), Im  is a submodule of M ;
13.3 Module homomorphisms and isomorphisms                       365

(ii) for every submodule N of M , -1(N ) is a submodule of M ; in partic-
     ular (setting N := {0M }), Ker  is a submodule of M.

Proof. Exercise. 2

   Theorems 6.20 and 6.21 have natural R-module analogs, which the reader may
easily verify:

Theorem 13.6. If  : M  M and  : M  M are R-linear maps, then so is
their composition    : M  M .

Theorem 13.7. Let i : M  Mi, for i = 1, . . . , k, be R-linear maps. Then the
map

                                 : M  M1 × · · · × Mk
                                         (1(), . . . , k())

is an R-linear map.

   If an R-linear map  : M  M is bijective, then it is called an R-module
isomorphism of M with M . If such an R-module isomorphism  exists, we say
that M is isomorphic to M , and write M = M . Moreover, if M = M , then  is
called an R-module automorphism on M.

   Theorems 6.22-6.26 also have natural R-module analogs, which the reader may

easily verify:

Theorem 13.8. If  is an R-module isomorphism of M with M , then the inverse
function -1 is an R-module isomorphism of M with M.

Theorem 13.9 (First isomorphism theorem). Let  : M  M be an R-linear
map with kernel K and image N . Then we have an R-module isomorphism

                                          M/K = N .

Specifically, the map

                        : M/K  M
                              []K  ()

is an injective R-linear map whose image is N .

Theorem 13.10. Let  : M  M be an R-linear map. Then for every submodule
N of M with N  Ker , we may define an R-linear map

                                       : M/N  M
                                             []N  ().

Moreover, Im  = Im , and  is injective if and only if N = Ker .
366               Modules and vector spaces

Theorem 13.11 (Internal direct product). Let M be an R-module with submod-
ules N1, N2, where N1  N2 = {0M }. Then we have an R-module isomorphism

                                    N1 × N2 = N1 + N2

given by the map

                   : N1 × N2  N1 + N2
                         (1, 2)  1 + 2.

Theorem 13.12. Let M and M be R-modules, and consider the R-module of
functions Map(M, M ) (see Example 13.10). Then

           HomR(M, M ) := {  Map(M, M ) :  is an R-linear map}

is a submodule of Map(M, M ).

Example 13.26. Consider again the R-module R[X ]/(f ) discussed in Exam-
ple 13.4, where f  R[X ] is of degree  0 and lc(f )  R. As an R-module,

R[X ]/(f ) is isomorphic to R[X ]< (see Example 13.11). Indeed, based on the
observations in Example 7.39, the map  : R[X ]<  R[X ]/(f ) that sends a
polynomial g  R[X ] of degree less than to [g]f  R[X ]/(f ) is an isomorphism

of R[X ]< with R[X ]/(f ). Furthermore, R[X ]< is isomorphic as an R-module to
R× . Indeed, the map  : R[X ]<  R× that sends g = i=0 -1 aiX i  R[X ]< to
(a0, . . . , a -1)  R× is an isomorphism of R[X ]< with R× . 2

EXERCISE 13.4. Verify that the "is isomorphic to" relation on R-modules is an
equivalence relation; that is, for all R-modules M1, M2, M3, we have:

    (a) M1 = M1;
   (b) M1 = M2 implies M2 = M1;
    (c) M1 = M2 and M2 = M3 implies M1 = M3.

EXERCISE 13.5. Let i : Mi  Mi, for i = 1, . . . , k, be R-linear maps. Show
that the map

                       : M1 × · · · × Mk  M1 × · · · × Mk
                                 (1, . . . , k)  (1(1), . . . , k(k))

is an R-linear map.

EXERCISE 13.6. Let  : M  M be an R-linear map, and let c  R. Show that
(cM) = c(M).

EXERCISE 13.7. Let  : M  M be an R-linear map. Let N be a submodule of
13.4 Linear independence and bases  367

M, and let  : N  M be the restriction of  to N. Show that  is an R-linear
map and that Ker  = Ker   N.

EXERCISE 13.8. Suppose M1, . . . , Mk are R-modules. Show that for each i =
1, . . . , k, the projection map i : M1 × · · · × Mk  Mi that sends (1, . . . , k) to
i is a surjective R-linear map.

EXERCISE 13.9. Show that if M = M1 × M2 for R-modules M1 and M2, and
N1 is a subgroup of M1 and N2 is a subgroup of M2, then we have an R-module
isomorphism M/(N1 × N2) = M1/N1 × M2/N2.

EXERCISE 13.10. Let M be an R-module with submodules N1 and N2. Show
that we have an R-module isomorphism (N1 + N2)/N2 = N1/(N1  N2).

EXERCISE 13.11. Let M be an R-module with submodules N1, N2, and A, where
N2  N1. Show that (N1  A)/(N2  A) is isomorphic to a submodule of N1/N2.

EXERCISE 13.12. Let  : M  M be an R-linear map with kernel K. Let N be
a submodule of M. Show that we have an R-module isomorphism M/(N + K) =

(M )/(N ).

EXERCISE 13.13. Let  : M  M be a surjective R-linear map. Let S be the set
of all submodules of M that contain Ker , and let S be the set of all submodules
of M . Show that the sets S and S are in one-to-one correspondence, via the map
that sends N  S to (N)  S .

                          13.4 Linear independence and bases

Throughout this section, R denotes a ring.

Definition 13.13. Let M be an R-module, and let {i}in=1 be a family of elements
of M. We say that {i}in=1

    (i) is linearly dependent (over R) if there exist c1, . . . , cn  R, not all zero,
         such that c11 + · · · + cnn = 0M ;

   (ii) is linearly independent (over R) if it is not linearly dependent;

   (iii) spans M (over R) if for every   M, there exist c1, . . . , cn  R such that
         c11 + · · · + cnn = ;

   (iv) is a basis for M (over R) if it is linearly independent and spans M.

   The family {i}in=1 always spans some submodule of M, namely, the submodule
N generated by 1, . . . , n. In this case, we may also call N the submodule (over
R) spanned by {i}ni=1.

   The family {i}in=1 may contain duplicates, in which case it is linearly dependent
368             Modules and vector spaces

(unless R is trivial). Indeed, if, say, 1 = 2, then setting c1 := 1, c2 := -1, and
c3 := · · · := cn := 0, we have the linear relation in=1 cii = 0M .

   If the family {i}in=1 contains 0M , then it is also linear dependent (unless R is
trivial). Indeed, if, say, 1 = 0M , then setting c1 := 1 and c2 := · · · := cn := 0, we
have the linear relation in=1 cii = 0M .

   The family {i}in=1 may also be empty (i.e., n = 0), in which case it is linearly
independent, and spans the submodule {0M }.

   In the above definition, the ordering of the elements 1, . . . , n makes no differ-

ence. As such, when convenient, we may apply the terminology in the definition

to any family {i}iI , where I is an arbitrary, finite index set.

Example 13.27. Consider the R-module R×n. Define 1, . . . , n  R×n as follows:
          1 := (1, 0, . . . , 0), 2 := (0, 1, 0, . . . , 0), . . . , n := (0, . . . , 0, 1);

that is, i has a 1 in position i and is zero everywhere else. It is easy to see that
{i}in=1 is a basis for R×n. Indeed, for all c1, . . . , cn  R, we have

                              c11 + · · · + cnn = (c1, . . . , cn),

from which it is clear that {i}in=1 spans R×n and is linearly independent. The
family {i}in=1 is called the standard basis for R×n. 2

Example 13.28. Consider the Z-module Z×3. In addition to the standard basis,
which consists of the tuples

                (1, 0, 0), (0, 1, 0), (0, 0, 1),

the tuples

                1 := (1, 1, 1), 2 := (0, 1, 0), 3 := (2, 0, 1)

also form a basis. To see this, first observe that for all c1, c2, c3, d1, d2, d3  Z, we
have

                             (d1, d2, d3) = c11 + c22 + c33

if and only if

                d1 = c1 + 2c3, d2 = c1 + c2, and d3 = c1 + c3.  (13.1)

If (13.1) holds with d1 = d2 = d3 = 0, then subtracting the equation c1 + c3 = 0
from c1 +2c3 = 0, we see that c3 = 0, from which it easily follows that c1 = c2 = 0.
This shows that the family {i}i3=1 is linearly independent. To show that it spans
Z×3, the reader may verify that for any given d1, d2, d3  Z, the values

                 c1 := -d1 + 2d3, c2 := d1 + d2 - 2d3, c3 := d1 - d3
13.4 Linear independence and bases                                    369

satisfy (13.1).
   The family of tuples (1, 1, 1), (0, 1, 0), (1, 0, 1) is not a basis, as it is linearly

dependent: the third tuple is equal to the first minus the second.
   The family of tuples (1, 0, 12), (0, 1, 30), (0, 0, 18) is linearly independent, but

does not span Z×3: the last component of any Z-linear combination of these tuples
must be divisible by gcd(12, 30, 18) = 6. However, this family of tuples is a basis
for the Q-module Q×3. 2

Example 13.29. Consider again the submodule R[X ]< of R[X ], where  0,
consisting of all polynomials of degree less than (see Example 13.11). Then
{X i i-1} =1 is a basis for R[X ]< over R. 2

Example 13.30. Consider again the ring E = R[X ]/(f ), where f  R[X ] with
deg(f ) =  0 and lc(f )  R. As in Example 13.4, we may naturally view E as
a module over R. From the observations in Example 7.39, it is clear that { i i-1} =1
is a basis for E over R, where  := [X ]f  E. 2

The next theorem highlights a critical property of bases:

Theorem 13.14. If {i}in=1 is a basis for an R-module M, then the map

:  R×n  M

   (c1, . . . , cn)  c11 + · · · + cnn

is an R-module isomorphism. In particular, every element of M can be expressed
in a unique way as c11 + · · · + cnn, for c1, . . . , cn  R.

Proof. We already saw that  is an R-linear map in Example 13.21. Since {i}in=1
is linearly independent, it follows that the kernel of  is trivial, so that  is injective.
That  is surjective follows immediately from the fact that {i}in=1 spans M. 2

   The following is an immediate corollary of this theorem:

Theorem 13.15. Any two R-modules with bases of the same size are isomorphic.

   The following theorem develops an important connection between bases and
linear maps.

Theorem 13.16. Let {i}in=1 be a basis for an R-module M, and let  : M  M
be an R-linear map. Then:

    (i)  is surjective if and only if {(i)}ni=1 spans M ;
   (ii)  is injective if and only if {(i)}ni=1 is linearly independent;
   (iii)  is an isomorphism if and only if {(i)}ni=1 is a basis for M .
370  Modules and vector spaces

Proof. By the previous theorem, we know that every element of M can be written

uniquely as i cii, where the ci's are in R. Therefore, every element in Im 
can be expressed as ( i cii) = i ci(i). It follows that Im  is equal to the
subspace of M spanned by {(i)}ni=1. From this, (i) is clear.

   For (ii), consider a non-zero element i cii of M, so that not all ci's are zero.
Now, i cii  Ker  if and only if i ci(i) = 0M , and thus, Ker  is non-trivial
if and only if {(i)}ni=1 is linearly dependent. That proves (ii).

   (iii) follows from (i) and (ii). 2

EXERCISE 13.14. Let M be an R-module. Suppose {i}in=1 is a linearly inde-
pendent family of elements of M. Show that for every J  {1, . . . , n}, the sub-
family {j}jJ is also linearly independent.

EXERCISE 13.15. Suppose  : M  M is an R-linear map. Show that if {i}in=1
is a linearly dependent family of elements of M, then {(i)}ni=1 is also linearly
dependent.

EXERCISE 13.16. Suppose  : M  M is an injective R-linear map and that
{i}in=1 is a linearly independent family of elements of M. Show that {(i)}ni=1 is
linearly independent.

EXERCISE 13.17. Suppose that {i}in=1 spans an R-module M and that  : M 
M is an R-linear map. Show that:

    (a)  is surjective if and only if {(i)}ni=1 spans M ;
   (b) if {(i)}ni=1 is linearly independent, then  is injective.

                           13.5 Vector spaces and dimension

Throughout this section, F denotes a field.
   A module over a field is also called a vector space. In particular, an F -module

is called an F -vector space, or a vector space over F .
   For vector spaces over F , one typically uses the terms subspace and quotient

space, instead of (respectively) submodule and quotient module; likewise, one
usually uses the terms F -vector space homomorphism, isomorphism and auto-
morphism, as appropriate.

   We now develop the basic theory of dimension for finitely generated vector
spaces. Recall that a vector space V over F is finitely generated if we have
V = 1, . . . , n F for some 1, . . . , n of V . The main results here are that

     · every finitely generated vector space has a basis, and

     · all such bases have the same number of elements.
                                  13.5 Vector spaces and dimension                                    371

   Throughout the rest of this section, V denotes a vector space over F . We begin
with a technical fact that will be used several times throughout this section:

Theorem 13.17. Suppose that {i}in=1 is a linearly independent family of elements
                                                                                                    n+1
that spans a subspace W                  V , and that    n+1        V  \ W.      Then
                                                                                             {i}i=1 is also

linearly independent.

Proof. Suppose we have a linear relation

                            0V = c11 + · · · + cnn + cn+1n+1,

where the ci's are in F . We want to show that all the ci's are zero. If cn+1 = 0, then
we have

                                                    -1

                          n+1 = -cn+1(c11 + · · · + cnn)  W ,

contradicting the assumption that n+1 / W . Therefore, we must have cn+1 = 0,
and the linear independence of {i}in=1 implies that c1 = · · · = cn = 0. 2

   The next theorem says that every finitely generated vector space has a basis, and
in fact, any family that spans a vector space contains a subfamily that is a basis for
the vector space.

Theorem 13.18. Suppose {i}in=1 is a family of elements that spans V . Then for
some subset J  {1, . . . , n}, the subfamily {j}jJ is a basis for V .

Proof. We prove this by induction on n. If n = 0, the theorem is clear, so assume
n > 0. Consider the subspace W of V spanned by {i}in=1-1. By the induction
hypothesis, for some K  {1, . . . , n - 1}, the subfamily {k}kK is a basis for W .
There are two cases to consider.

   Case 1: n  W . In this case, W = V , and the theorem clearly holds with
J := K.

   Case 2: n / W . We claim that setting J := K  {n}, the subfamily {j}jJ
is a basis for V . Indeed, since {k}kK is linearly independent, and n / W ,
Theorem 13.17 immediately implies that {j}jJ is linearly independent. Also,
since {k}kK spans W , it is clear that {j}jJ spans W + n F = V . 2

Theorem 13.19. If V is spanned by some family of n elements of V , then every
family of n + 1 elements of V is linearly dependent.

Proof. We prove this by induction on n. If n = 0, the theorem is clear, so assume
                                                                                        n+1
that  n  >   0.  Let  {i}in=1     be  a  family    that  spans  V,  and     let              be  an  arbitrary
                                                                                 {i}i=1
                                                                       n+1
family   of  elements  of  V   .  We     wish  to  show  that               is  linearly  dependent.
                                                                {i}i=1

We know that n+1 is a linear combination of the i's, say,

                                      n+1 = c11 + · · · + cnn.                                       (13.2)
372                                    Modules and vector spaces

If  all  the  ci's  were  zero,  then  we  would  have  n+1  =  0V  ,  and  so  trivially,         n+1  is

                                                                                            {i}i=1

linearly dependent. So assume that some ci is non-zero, and for concreteness, say

cn = 0. Dividing equation (13.2) through by cn, it follows that n is an F -linear

combination of 1, . . . , n-1, n+1. Therefore,

                    1, . . . , n-1, n+1 F  1, . . . , n-1 F + n F = V .

    Now consider the subspace W := n+1 F and the quotient space V /W . Since

the family of elements 1, . . . , n-1, n+1 spans V , it is easy to see that {[i]W }in=1-1
spans V /W ; therefore, by induction, {[i]W }in=1 is linearly dependent. This means
that there exist d1, . . . , dn  F , not all zero, such that d11+· · ·+dnn  0 (mod W ),

which means that for some dn+1  F , we have d11 + · · · + dnn = dn+1n+1. That
                      n+1
proves   that              is  linearly  dependent.  2
               {i}i=1

    An important corollary of Theorem 13.19 is the following:

Theorem 13.20. If V is finitely generated, then any two bases for V have the same
size.

Proof. If one basis had more elements than another, then Theorem 13.19 would
imply that the first basis was linearly dependent, which contradicts the definition
of a basis. 2

    Theorem 13.20 allows us to make the following definition:

Definition 13.21. If V is finitely generated, the common size of any basis is called
the dimension of V , and is denoted dimF (V ).

   Note that from the definitions, we have dimF (V ) = 0 if and only if V is the
trivial vector space (i.e., V = {0V }). We also note that one often refers to a
finitely generated vector space as a finite dimensional vector space. We shall give
preference to this terminology from now on.

   To summarize the main results in this section up to this point: if V is finite
dimensional, it has a basis, and any two bases have the same size, which is called
the dimension of V .

Theorem 13.22. Suppose that dimF (V ) = n, and that {i}in=1 is a family of n
elements of V . The following are equivalent:

    (i) {i}in=1 is linearly independent;
   (ii) {i}in=1 spans V ;
   (iii) {i}in=1 is a basis for V .

Proof. Let W be the subspace of V spanned by {i}ni=1.
   First, let us show that (i) implies (ii). Suppose {i}in=1 is linearly independent.
                           13.5 Vector spaces and dimension                         373

Also, by way of contradiction, suppose that W V , and choose n+1  V \ W .
                                             n+1
Then  Theorem  13.17   implies  that              is  linearly  independent.        But then we
                                      {i}i=1
have a linearly independent family of n + 1 elements of V , which is impossible by

Theorem 13.19.
   Second, let us prove that (ii) implies (i). Let us assume that {i}in=1 is linearly

dependent, and prove that W V . By Theorem 13.18, we can find a basis for
W among the i's, and since {i}in=1 is linearly dependent, this basis must con-
tain strictly fewer than n elements. Hence, dimF (W ) < dimF (V ), and therefore,

W V.

The theorem now follows from the above arguments, and the fact that, by defi-

nition, (iii) holds if and only if both (i) and (ii) hold. 2

We next examine the dimension of subspaces of finite dimensional vector spaces.

Theorem 13.23. Suppose that V is finite dimensional and W is a subspace of

V . Then W is also finite dimensional, with dimF (W )  dimF (V ). Moreover,
dimF (W ) = dimF (V ) if and only if W = V .

Proof. Suppose dimF (V ) = n. Consider the set S of all linearly independent
families of the form {i}mi=1, where m  0 and each i is in W . The set S is
certainly non-empty, as it contains the empty family. Moreover, by Theorem 13.19,

every member of S must have at most n elements. Therefore, we may choose some

particular element {i}im=1 of S, where m is as large as possible. We claim that this
family {i}im=1 is a basis for W . By definition, {i}im=1 is linearly independent

and spans some subspace W of W . If W W , we can choose an element
                                                                 m+1
m+1     W \W   ,  and  by  Theorem  13.17,  the   family              is  linearly  independent,
                                                          {i}i=1

and therefore, this family also belongs to S, contradicting the assumption that m is

as large as possible.

   That proves that W is finite dimensional with dimF (W )  dimF (V ). It remains
to show that these dimensions are equal if and only if W = V . Now, if W = V ,

then clearly dimF (W ) = dimF (V ). Conversely, if dimF (W ) = dimF (V ), then by

Theorem 13.22, any basis for W must already span V . 2

Theorem 13.24. If V is finite dimensional, and W is a subspace of V , then the
quotient space V /W is also finite dimensional, and

                          dimF (V /W ) = dimF (V ) - dimF (W ).

Proof. Suppose that {i}in=1 spans V . Then it is clear that {[i]W }in=1 spans V /W .
By Theorem 13.18, we know that V /W has a basis of the form {[i]W }i=1, where

   n (renumbering the i's as necessary). By Theorem 13.23, we know that W
has a basis, say {j}mj=1. The theorem will follow immediately from the following:
374                  Modules and vector spaces

Claim. The elements

                     1, . . . ,  , 1, . . . , m  (13.3)

form a basis for V .

   To see that this family spans V , consider any element  of V . Then since

{[i]W }i=1 spans V /W , we have   i cii (mod W ) for some c1, . . . , c  F . If
we set  :=  - i cii  W , then since {j}jm=1 spans W , we have  = j djj for
some d1, . . . , dm  F , and hence  = i cii + j djj. That proves that the family
of elements (13.3) spans V . To prove this family is linearly independent, suppose
we have a relation of the form i cii + j djj = 0V , where c1, . . . , c  F
and d1, . . . , dm  F . If any of the ci's were non-zero, this would contradict the

assumption that {[i]W }i=1 is linearly independent. So assume that all the ci's are
zero. If any of the dj's were non-zero, this would contradict the assumption that
{j}jm=1 is linearly independent. Thus, all the ci's and dj's must be zero, which
proves that the family of elements (13.3) is linearly independent. That proves the

claim. 2

Theorem 13.25. If V is finite dimensional, then every linearly independent family
of elements of V can be extended to form a basis for V .

Proof. One can prove this by generalizing the proof of Theorem 13.18. Alterna-
tively, we can adapt the proof of the previous theorem. Let {j}jm=1 be a linearly
independent family of elements that spans a subspace W of V . As in the proof of

the previous theorem, if {[i]W }i=1 is a basis for the quotient space V /W , then the
elements

                                    1, . . . ,  , 1, . . . , m

form a basis for V . 2

Example 13.31. Suppose that F is finite, say |F | = q, and that V is finite dimen-
sional, say dimF (V ) = n. Then clearly |V | = qn. If W is a subspace with
dimF (W ) = m, then |W | = qm, and by Theorem 13.24, dimF (V /W ) = n - m, and
hence |V /W | = qn-m. Just viewing V and W as additive groups, we know that the
index of W in V is [V : W ] = |V /W | = |V |/|W | = qn-m, which agrees with the

above calculations. 2

   We next consider the relation between the notion of dimension and linear maps.
First, observe that by Theorem 13.15, if two finite dimensional vector spaces have
the same dimension, then they are isomorphic. The following theorem is the con-
verse:

Theorem 13.26. If V is of finite dimension n, and V is isomorphic to V , then V
is also of finite dimension n.
13.5 Vector spaces and dimension             375

Proof. If {i}in=1 is a basis for V , then by Theorem 13.16, {(i)}in=1 is a basis for
V. 2

   Thus, two finite dimensional vector spaces are isomorphic if and only if they
have the same dimension.

   We next illustrate one way in which the notion of dimension is particularly use-
ful. In general, if we have a function f : A  B, injectivity does not imply
surjectivity, nor does surjectivity imply injectivity. If A and B are finite sets of
equal size, then these implications do indeed hold. The following theorem gives us
another important setting where these implications hold, with finite dimensionality
playing the role corresponding to finite cardinality:

Theorem 13.27. If  : V  V is an F -linear map, and if V and V are finite
dimensional with dimF (V ) = dimF (V ), then we have:

          is injective if and only if  is surjective.

Proof. Let {i}in=1 be a basis for V . Then
     is injective  {(i)}ni=1 is linearly independent (by Theorem 13.16)
                      {(i)}ni=1 spans V (by Theorem 13.22)
                       is surjective (again by Theorem 13.16). 2

This theorem may be generalized as follows:

Theorem 13.28. If V is finite dimensional, and  : V  V is an F -linear map,
then Im  is a finite dimensional vector space, and

                         dimF (V ) = dimF (Im ) + dimF (Ker ).

Proof. As the reader may verify, this follows immediately from Theorem 13.24,
together with Theorems 13.26 and 13.9. 2

   Intuitively, one way to think of Theorem 13.28 is as a "law of conservation" for
dimension: any "dimensionality" going into  that is not "lost" to the kernel of 
must show up in the image of .

EXERCISE 13.18. Show that if V1, . . . , Vn are finite dimensional vector spaces
over F , then V1 × · · · × Vn has dimension in=1 dimF (Vi).

EXERCISE 13.19. Show that if V is a finite dimensional vector space over F with
subspaces W1 and W2, then

           dimF (W1 + W2) = dimF (W1) + dimF (W2) - dimF (W1  W2).
376  Modules and vector spaces

EXERCISE 13.20. From the previous exercise, one might be tempted to think that
a more general "inclusion/exclusion principle" for dimension holds. Determine if
the following statement is true or false: if V is a finite dimensional vector space
over F with subspaces W1, W2, and W3, then

         dimF (W1 + W2 + W3) = dimF (W1) + dimF (W2) + dimF (W3)

                    - dimF (W1  W2) - dimF (W1  W3) - dimF (W2  W3)
                    + dimF (W1  W2  W3).

EXERCISE 13.21. Suppose that V and W are vector spaces over F , V is finite

dimensional, and {i}ik=1 is a linearly independent family of elements of V . In
addition, let 1, . . . , k be arbitrary elements of W . Show that there exists an F -
linear map  : V  W such that (i) = i for i = 1, . . . , k.

EXERCISE 13.22. Let V be a vector space over F with basis {i}ni=1. Let S be a
finite, non-empty subset of F , and define

     B :=   n

               cii : c1, . . . , cn  S .

           i=1

Show that if W is a subspace of V , with W V , then |B  W |  |S|n-1.

EXERCISE 13.23. The theory of dimension for finitely generated vector spaces is
quite elegant and powerful. There is a theory of dimension (of sorts) for modules
over an arbitrary, non-trivial ring R, but it is much more awkward and limited.
This exercise develops a proof of one aspect of this theory: if an R-module M has
a basis at all, then any two bases have the same size. To prove this, we need the fact
that any non-trivial ring has a maximal ideal (this was proved in Exercise 7.40 for
countable rings). Let n, m be positive integers, let 1, . . . , m be elements of R×n,
and let I be an ideal of R.

    (a) Show that if {i}im=1 spans R×n, then every element of I×n can be expressed
         as c11 + · · · + cmm, where c1, . . . , cm belong to I.

   (b) Show that if m > n and I is a maximal ideal, then there exist c1, . . . , cm  R,
         not all in I, such that c11 + · · · + cmm  I×n.

    (c) From (a) and (b), deduce that if m > n, then {i}im=1 cannot be a basis for
         R×n.

   (d) From (c), conclude that any two bases for a given R-module M must have
         the same size.
                            14

                            Matrices

In this chapter, we discuss basic definitions and results concerning matrices. We
shall start out with a very general point of view, discussing matrices whose entries
lie in an arbitrary ring R. Then we shall specialize to the case where the entries lie
in a field F , where much more can be said.

   One of the main goals of this chapter is to discuss "Gaussian elimination," which
is an algorithm that allows us to efficiently compute bases for the image and kernel
of an F -linear map.

   In discussing the complexity of algorithms for matrices over a ring R, we shall
treat a ring R as an "abstract data type," so that the running times of algorithms will
be stated in terms of the number of arithmetic operations in R. If R is a finite ring,
such as Zm, we can immediately translate this into a running time on a RAM (in
later chapters, we will discuss other finite rings and efficient algorithms for doing
arithmetic in them).

   If R is, say, the field of rational numbers, a complete running time analysis
would require an additional analysis of the sizes of the numbers that appear in the
execution of the algorithm. We shall not attempt such an analysis here -- however,
we note that all the algorithms discussed in this chapter do in fact run in poly-
nomial time when R = Q, assuming we represent rational numbers as fractions in
lowest terms. Another possible approach for dealing with rational numbers is to use
floating point approximations. While this approach eliminates the size problem, it
creates many new problems because of round-off errors. We shall not address any
of these issues here.

                          14.1 Basic definitions and properties
Throughout this section, R denotes a ring.

   For positive integers m and n, an m × n matrix A over a ring R is a rectangular

                                               377
378      Matrices

array                         

         a11 a12 · · · a1n
       a21 a22 · · · a2n 
       A =  .. ..             ..  ,
       . .                    .

       am1 am2 · · · amn

where each entry aij in the array is an element of R; the element aij is called the
(i, j) entry of A, which we denote by A(i, j). For i = 1, . . . , m, the ith row of A is

         (ai1, . . . , ain),

which we denote by Rowi(A), and for j = 1, . . . , n, the jth column of A is
         
           a1j
                   
           a2j     
          ..  ,
         .

         amj

which we denote by Colj(A). We regard a row of A as a 1 × n matrix, and a column
of A as an m × 1 matrix.

   The set of all m × n matrices over R is denoted by Rm×n. Elements of R1×n
are called row vectors (of dimension n) and elements of Rm×1 are called col-
umn vectors (of dimension m). Elements of Rn×n are called square matrices (of
dimension n). We do not make a distinction between R1×n and R×n; that is, we

view standard n-tuples as row vectors.

   We can define the familiar operations of matrix addition and scalar multipli-
cation:

     · If A, B  Rm×n, then A + B is the m × n matrix whose (i, j) entry is
         A(i, j) + B(i, j).

     · If c  R and A  Rm×n, then cA is the m × n matrix whose (i, j) entry is
         cA(i, j).

The m × n zero matrix is the m × n matrix, all of whose entries are 0R; we denote
this matrix by 0Rm×n (or just 0, when the context is clear).

Theorem 14.1. With addition and scalar multiplication as defined above, Rm×n is
an R-module. The matrix 0Rm×n is the additive identity, and the additive inverse of
a matrix A  Rm×n is the m × n matrix whose (i, j) entry is -A(i, j).

Proof. To prove this, one first verifies that matrix addition is associative and com-
mutative, which follows from the associativity and commutativity of addition in R.
The claims made about the additive identity and additive inverses are also easily
14.1 Basic definitions and properties  379

verified. These observations establish that Rm×n is an abelian group. One also has
to check that all of the properties in Definition 13.1 hold. We leave this to the
reader. 2

We can also define the familiar operation of matrix multiplication:

  · If A  Rm×n and B  Rn×p, then AB is the m × p matrix whose (i, k) entry
      is

                                                         n

                                            A(i, j)B(j, k).

                                                       j=1

The n × n identity matrix is the matrix I  Rn×n, where I (i, i) := 1R and
I (i, j) := 0R for i = j. That is, I has 1R's on the diagonal that runs from the

upper left corner to the lower right corner, and 0R's everywhere else.

Theorem 14.2.

    (i) Matrix multiplication is associative; that is, A(BC) = (AB)C for all
         A  Rm×n, B  Rn×p, and C  Rp×q.

   (ii) Matrix multiplication distributes over matrix addition; that is, A(C + D) =
         AC +AD and (A+B)C = AC +BC for all A, B  Rm×n and C, D  Rn×p.

   (iii) The n × n identity matrix I  Rn×n acts as a multiplicative identity; that
         is, AI = A and IB = B for all A  Rm×n and B  Rn×m; in particular,
         CI = C = IC for all C  Rn×n.

   (iv) Scalar multiplication and matrix multiplication associate; that is, c(AB) =
         (cA)B = A(cB) for all c  R, A  Rm×n, and B  Rn×p.

Proof. All of these are trivial, except for (i), which requires just a bit of compu-
tation to show that the (i, ) entry of both A(BC) and (AB)C is equal to (as the
reader may verify)

                                      A(i, j)B(j, k)C(k, ). 2

                                           1jn
                                          1kp

   Note that while matrix addition is commutative, matrix multiplication in general
is not. Indeed, Theorems 14.1 and 14.2 imply that Rn×n satisfies all the properties
of a ring except for commutativity of multiplication.

Some simple but useful facts to keep in mind are the following:

  · If A  Rm×n and B  Rn×p, then the ith row of AB is equal to vB,
      where v = Rowi(A); also, the kth column of AB is equal to Aw, where
      w = Colk(B).
380                       Matrices

     · If A  Rm×n and v = (c1, . . . , cm)  R1×m, then

                                  m

                       vA = ci Rowi(A).

                                 i=1

     In words: vA is a linear combination of the rows of A, with coefficients

     taken from the corresponding entries of v.

     · If A  Rm×n and        

                             d1    
                                   
                       w  =    ..      Rn×1,
                               .

                               dn

     then

                                                             n

                                     Aw = dj Colj(A).

                                                            j=1

     In words: Aw is a linear combination of the columns of A, with coefficients

     taken from the corresponding entries of w.

   If A  Rm×n, the transpose of A, denoted by A , is defined to be the n × m
matrix whose (j, i) entry is A(i, j).

Theorem 14.3. If A, B  Rm×n, C  Rn×p, and c  R, then:
    (i) (A + B) = A + B ;
   (ii) (cA) = cA ;

   (iii) (A ) = A;
   (iv) (AC) = C A .

Proof. Exercise. 2

   If Ai is an ni × ni+1 matrix, for i = 1, . . . , k, then by associativity of matrix
multiplication, we may write the product matrix A1 · · · Ak, which is an n1 × nk+1
matrix, without any ambiguity.

   For an n×n matrix A, and a positive integer k, we write Ak to denote the product
A · · · A, where there are k terms in the product. Note that A1 = A. We may extend
this notation to k = 0, defining A0 to be the n × n identity matrix. One may readily
verify the usual rules of exponent arithmetic: for all non-negative integers k, , we

have
                        (A )k = Ak = (Ak) and AkA = Ak+ .

It is easy also to see that part (iv) of Theorem 14.3 implies that for all non-negative
                     14.2 Matrices and linear maps  381

integers k, we have

                     (Ak) = (A )k.

                                      Algorithmic issues
For computational purposes, matrices are represented in the obvious way as arrays
of elements of R. As remarked at the beginning of this chapter, we shall treat R as
an "abstract data type," and not worry about how elements of R are actually rep-
resented; in discussing the complexity of algorithms, we shall simply count "oper-
ations in R," by which we mean additions, subtractions, and multiplications; we
shall sometimes also include equality testing and computing multiplicative inverses
as "operations in R." In any real implementation, there will be other costs, such
as incrementing counters, and so on, which we may safely ignore, as long as their
number is at most proportional to the number of operations in R.

   The following statements are easy to verify:
     · We can multiply an m × n matrix by a scalar using mn operations in R.
     · We can add two m × n matrices using mn operations in R.
     · We can compute the product of an m × n matrix and an n × p matrix using
         O(mnp) operations in R.

   It is also easy to see that given an n×n matrix A, and a non-negative integer e, we
can adapt the repeated squaring algorithm discussed in §3.4 so as to compute Ae
using O(len(e)) multiplications of n×n matrices, and hence O(len(e)n3) operations
in R.

EXERCISE 14.1. Let A  Rm×n. Show that if vA = 0R1×n for all v  R1×m, then
A = 0Rm×n.

                             14.2 Matrices and linear maps
Let R be a ring.

   For positive integers m and n, consider the R-modules R1×m and R1×n. If A is
an m × n matrix over R, then the map

                                     A : R1×m  R1×n
                                                  v  vA

is easily seen to be an R-linear map -- this follows immediately from parts (ii) and
(iv) of Theorem 14.2. We call A the linear map corresponding to A.
382                          Matrices

If v = (c1, . . . , cm)  R1×m, then

                                                                 m

                            A(v) = vA = ci Rowi(A).

                                                                i=1

From this, it is clear that

     · the image of A is the submodule of R1×n spanned by {Rowi(A)}mi=1; in
        particular, A is surjective if and only if {Rowi(A)}mi=1 spans R1×n;

     · A is injective if and only if {Rowi(A)}mi=1 is linearly independent.

   There is a close connection between matrix multiplication and composition of
corresponding linear maps. Specifically, let A  Rm×n and B  Rn×p, and consider
the corresponding linear maps A : R1×m  R1×n and B : R1×n  R1×p. Then

we have

                             B  A = AB.                                    (14.1)

This follows immediately from the associativity of matrix multiplication.

   We have seen how vector/matrix multiplication defines a linear map. Conversely,
we shall now see that the action of any R-linear map can be viewed as a vec-
tor/matrix multiplication, provided the R-modules involved have bases (which will
always be the case for finite dimensional vector spaces).

   Let M be an R-module, and suppose that S = {i}im=1 is a basis for M, where
m > 0. As we know (see Theorem 13.14), every element   M can be written
uniquely as c11 + · · · + cmm, where the ci's are in R. Let us define

                             VecS () := (c1, . . . , cm)  R1×m.

We call VecS () the coordinate vector of  relative to S. The function
                                      VecS : M  R1×m

is an R-module isomorphism (it is the inverse of the isomorphism  in Theo-

rem 13.14).
   Let N be another R-module, and suppose that T = {j}jn=1 is a basis for N,

where n > 0. Just as in the previous paragraph, every element   N has a unique
coordinate vector VecT ()  R1×n relative to T .

   Now let  : M  N be an arbitrary R-linear map. Our goal is to define a matrix
A  Rm×n with the following property:

         VecT (()) = VecS ()A for all   M.                                 (14.2)

In words: if we multiply the coordinate vector of  on the right by A, we get the
coordinate vector of ().
           14.2 Matrices and linear maps                    383

   Constructing such a matrix A is easy: we define A to be the matrix whose ith
row, for i = 1, . . . , m, is the coordinate vector of (i) relative to T . That is,

                        Rowi(A) = VecT ((i)) for i = 1, . . . , m.

Then for an arbitrary   M, if (c1, . . . , cm) is the coordinate vector of  relative to
S, we have

        () =  cii = ci(i)

           i     i

and so

        VecT (()) = ci VecT ((i)) = ci Rowi(A) = VecS ()A.

        i     i

  Furthermore, A is the only matrix satisfying (14.2). Indeed, if A also satisfies
(14.2), then subtracting, we obtain

                                  VecS ()(A - A ) = 0R1×n

for all   M. Since the map VecS : M  R1×m is surjective, this means that
v(A - A ) is zero for all v  R1×m, and from this, it is clear (see Exercise 14.1) that
A - A is the zero matrix, and so A = A .

   We call the unique matrix A satisfying (14.2) the matrix of  relative to S and

T , and denote it by MatS,T ().
   Recall that HomR(M, N) is the R-module consisting of all R-linear maps from

M to N (see Theorem 13.12). We can view MatS,T as a function mapping ele-
ments of HomR(M, N ) to elements of Rm×n.

Theorem 14.4. The function MatS,T : HomR(M, N )  Rm×n is an R-module
isomorphism. In particular, for every A  Rm×n, the pre-image of A under MatS,T
is VecT-1 A  VecS , where A : R1×m  R1×n is the linear map corresponding to

A.

Proof. To show that MatS,T is an R-linear map, let ,   HomR(M, N), and let
c  R. Also, let A := MatS,T () and A := MatS,T ( ). Then for all   M, we
have

       VecT (( +  )()) = VecT (() +  ()) = VecT (()) + VecT ( ())
                             = VecS ()A + VecS ()A = VecS ()(A + A ).

As this holds for all   M, and since the matrix of a linear map is uniquely

determined, we must have MatS,T ( +  ) = A + A . A similar argument shows
that MatS,T (c) = cA. This shows that MatS,T is an R-linear map.

   To show that the map MatS,T is injective, it suffices to show that its kernel is
384                         Matrices

trivial. If  is in the kernel of this map, then setting A := 0Rm×n in (14.2), we see
that VecT (()) is zero for all   M. But since the map VecT : N  R1×n is
injective, this implies () is zero for all   M. Thus,  must be the zero map.

   To show surjectivity, we show that every A  Rm×n has a pre-image under
MatS,T as described in the statement of the theorem. So let A be an m × n matrix,
and let  := VecT-1 A  VecS . Again, since the matrix of a linear map is uniquely
determined, it suffices to show that (14.2) holds for this particular A and . For
every   M, we have

              VecT (()) = VecT (Vec-T 1(A(VecS ()))) = A(VecS ())
                             = VecS ()A.

That proves the theorem. 2

As a special case of the above, suppose that M = R1×m and N = R1×n, and S

and T are the standard bases for M and N (see Example 13.27). In this case, the

functions VecS and VecT are the identity maps, and the previous theorem implies

that the function

                    : Rm×n  HomR(R1×m, R1×n)

                            A  A

is the inverse of the function MatS,T : HomR(R1×m, R1×n)  Rm×n. Thus, the
function  is also an R-module isomorphism.

   To summarize, we see that an R-linear map  from M to N, together with
particular bases for M and N, uniquely determine a matrix A such that the action
of multiplication on the right by A implements the action of  with respect to the
given bases. There may be many bases for M and N to choose from, and different
choices will in general lead to different matrices. Also, note that in general, a basis
may be indexed by an arbitrary finite set; however, in defining coordinate vectors
and matrices of linear maps, the index set must be ordered in some way. In any
case, from a computational perspective, the matrix A gives us an efficient way to
compute the map , assuming elements of M and N are represented as coordinate
vectors with respect to the given bases.

   We have taken a "row-centric" point of view. Of course, if one prefers, by simply
transposing everything, one can equally well take a "column-centric" point of view,
where the action of  corresponds to multiplication of a column vector on the left
by a matrix.

Example 14.1. Consider the quotient ring E = R[X ]/(f ), where f  R[X ] with
deg(f ) = > 0 and lc(f )  R. Let  := [X ]f  E. As an R-module, E
has a basis S := { i i-1} =1 (see Example 13.30). Let  : E  E be the -
multiplication map, which sends   E to   E. This is an R-linear map. If
                  14.2 Matrices and linear maps                385

f = c0 + c1X + · · · + c -1X -1 + c X , then the matrix of  relative to S is the ×

matrix                                                       

               0       1                      0 ···       0

               0       0                      1 ···       0  
                                                             
                                                   . . .  ,
        A=

             
        0              0                      0 ···       1

        -c0/c -c1/c -c2/c · · · -c -1/c

where for i = 1, . . . , - 1, the ith row of A contains a 1 in position i + 1, and is
zero everywhere else. The matrix A is called the companion matrix of f. 2

Example 14.2. Let x1, . . . , xk  R. Let R[X ]<k be the set of polynomials g  R[X ]
with deg(g) < k, which is an R-module with a basis S := {X i i-1}k=1 (see Exam-

ple 13.29). The multi-point evaluation map

                : R[X ]<k  R1×k
                            g  (g(x1), . . . , g(xk))

is an R-linear map. Let T be the standard basis for R1×k.    Then the matrix of 

relative to S and T is the k × k matrix                 
                  
                    1 1 ··· 1

                    x1                   x2   ···  xk   
                                                        
                   x2                    x22       x2 
               A= 1                           ···   k .
                   ..                                   
                  .                      ...       ...  

                  x1k-1 x2k-1 · · · xkk-1

The matrix A is called a Vandermonde matrix. 2

EXERCISE 14.2. Let  : M  N and  : N  P be R-linear maps, and suppose
that M, N, and P have bases S, T , and U , respectively. Show that

                        MatS,U (  ) = MatS,T () · MatT ,U ().

EXERCISE 14.3. Let V be a vector space over a field F with basis S = {i}mi=1.
Suppose that U is a subspace of V of dimension < m. Show that there exists
a matrix A  F m×(m- ) such that for all   V , we have   U if and only if
VecS ()A is zero. Such a matrix A is called a parity check matrix for U .

EXERCISE 14.4. Let F be a finite field, and let A be a non-zero m × n matrix over
F . Suppose one chooses a vector v  F 1×m at random. Show that the probability
that vA is the zero vector is at most 1/|F |.
386  Matrices

EXERCISE 14.5. Design and analyze a probabilistic algorithm that takes as input
matrices A, B, C  Zpm×m, where p is a prime. The algorithm should run in time
O(m2 len(p)2) and should output either "yes" or "no" so that the following holds:

     · if C = AB, then the algorithm should always output "yes";

     · if C = AB, then the algorithm should output "no" with probability at least

         0.999.

                               14.3 The inverse of a matrix

Let R be a ring.
   For a square matrix A  Rn×n, we call a matrix B  Rn×n an inverse of A if

BA = AB = I, where I is the n × n identity matrix. It is easy to see that if A has
an inverse, then the inverse is unique: if B and C are inverses of A, then

                        B = BI = B(AC) = (BA)C = IC = C.

Because the inverse of A is uniquely determined, we denote it by A-1. If A has an
inverse, we say that A is invertible, or non-singular. If A is not invertible, it is
sometimes called singular. We will use the terms "invertible" and "not invertible."
Observe that A is the inverse of A-1; that is, (A-1)-1 = A.

   If A and B are invertible n × n matrices, then so is their product: in fact, it is
easily verified that (AB)-1 = B-1A-1. It follows that if A is an invertible matrix,
and k is a non-negative integer, then Ak is invertible with inverse (A-1)k, which
we also denote by A-k.

   It is also easy to see that A is invertible if and only if the transposed matrix A
is invertible, in which case (A )-1 = (A-1) . Indeed, AB = I = BA holds if and
only if B A = I = A B .

   We now develop a connection between invertible matrices and R-module iso-
morphisms. Recall from the previous section the R-module isomorphism

                              : Rn×n  HomR(R1×n, R1×n)

                                        A  A,

where for each A  Rn×n, A is the corresponding R-linear map

                                     A : R1×n  R1×n

                                                  v  vA.

Evidently, I is the identity map.

Theorem 14.5. Let A  Rn×n, and let A : R1×n  R1×n be the corresponding
R-linear map. Then A is invertible if and only if A is bijective, in which case
A-1 = -1 A .
               14.3 The inverse of a matrix               387

Proof. Suppose A is invertible, and that B is its inverse. We have AB = BA = I,

and hence AB = BA = I , from which it follows (see (14.1)) that B  A =
A  B = I . Since I is the identity map, this implies A is bijective.

   Suppose A is bijective. We know that the inverse map A-1 is also an R-linear
map, and since the mapping  above is surjective, we have A-1 = B for some
B  Rn×n. Therefore, we have B  A = A  B = I , and hence (again,
see (14.1)) AB = BA = I . Since the mapping  is injective, it follows that
AB = BA = I. This implies A is invertible, with A-1 = B. 2

We also have:

Theorem 14.6. Let A  Rn×n. The following are equivalent:

    (i) A is invertible;
   (ii) {Rowi(A)}ni=1 is a basis for R1×n;
   (iii) {Colj (A)}nj=1 is a basis for Rn×1.

Proof. We first prove the equivalence of (i) and (ii). By the previous theorem,

A is invertible if and only if A is bijective. Also, in the previous section, we
observed that A is surjective if and only if {Rowi(A)}ni=1 spans R1×n, and that A
is injective if and only if {Rowi(A)}ni=1 is linearly independent.

   The equivalence of (i) and (iii) follows by considering the transpose of A. 2

EXERCISE 14.6. Let R be a ring, and let A be a square matrix over R. Let us call
B a left inverse of A if BA = I, and let us call C a right inverse of A if AC = I.

    (a) Show that if A has both a left inverse B and a right inverse C, then B = C
         and hence A is invertible.

   (b) Assume that R is a field. Show that if A has either a left inverse or a right
         inverse, then A is invertible.

   Note that part (b) of the previous exercise holds for arbitrary rings, but the proof
of this is non-trivial, and requires the development of the theory of determinants,
which we do not cover in this text.

EXERCISE 14.7. Show that if A and B are two square matrices over a field such
that their product AB is invertible, then both A and B themselves must be invert-
ible.

EXERCISE 14.8. Show that if A is a square matrix over an arbitrary ring, and Ak
is invertible for some k > 0, then A is invertible.

EXERCISE 14.9. With notation as in Example 14.1, show that the matrix A is
invertible if and only if c0  R.
388                  Matrices

EXERCISE 14.10. With notation as in Example 14.2, show that the matrix A is
invertible if and only if xi - xj  R for all i = j.

                                14.4 Gaussian elimination

Throughout this section, F denotes a field.
   A matrix B  F m×n is said to be in reduced row echelon form if there exists a

sequence of integers (p1, . . . , pr), with 0  r  m and 1  p1 < p2 < · · · < pr  n,
such that the following holds:

     · for i = 1, . . . , r, all of the entries in row i of B to the left of entry (i, pi) are
         zero; that is, B(i, j) = 0F for j = 1, . . . , pi - 1;

     · for i = 1, . . . , r, all of the entries in column pi of B above entry (i, pi) are
         zero; that is, B(i , pi) = 0F for i = 1, . . . , i - 1;

     · for i = 1, . . . , r, we have B(i, pi) = 1F ;
     · all entries in rows r + 1, . . . , m of B are zero; that is, B(i, j) = 0F for

         i = r + 1, . . . , m and j = 1, . . . , n.

   It is easy to see that if B is in reduced row echelon form, then the sequence
(p1, . . . , pr) above is uniquely determined, and we call it the pivot sequence of B.
Several further remarks are in order:

     · All of the entries of B are completely determined by the pivot sequence,
         except for the entries (i, j) with 1  i  r and j > pi with j / {pi+1, . . . , pr},
         which may be arbitrary.

     · If B is an n × n matrix in reduced row echelon form whose pivot sequence
         is of length n, then B must be the n × n identity matrix.

     · We allow for an empty pivot sequence (i.e., r = 0), which will be the case
         precisely when B = 0Fm×n.

Example 14.3. The following 4×6 matrix B over the rational numbers is in reduced

row echelon form:                

                   0 1 -2 0 0 3

                   B = 0 0 0 1 0 2  0 0 0 0 1 -4 .

                   00 0 00 0

The pivot sequence of B is (2, 4, 5). Notice that the first three rows of B form a
linearly independent family of vectors, that columns 2, 4, and 5 form a linearly
independent family of vectors, and that all of other columns of B are linear com-
binations of columns 2, 4, and 5. Indeed, if we truncate the pivot columns to their
first three rows, we get the 3 × 3 identity matrix. 2
14.4 Gaussian elimination  389

   Generalizing the previous example, if a matrix is in reduced row echelon form,
it is easy to deduce the following properties, which turn out to be quite useful:

Theorem 14.7. If B is a matrix in reduced row echelon form with pivot sequence
(p1, . . . , pr), then:

    (i) rows 1, 2, . . . , r of B form a linearly independent family of vectors;

   (ii) columns p1, . . . , pr of B form a linearly independent family of vectors, and
         all other columns of B can be expressed as linear combinations of columns
         p1, . . . , pr.

Proof. Exercise--just look at the matrix! 2

   Gaussian elimination is an algorithm that transforms a given matrix A  F m×n
into a matrix B  F m×n, where B is in reduced row echelon form, and is obtained
from A by a sequence of elementary row operations. There are three types of
elementary row operations:

Type I: swap two rows;

Type II: multiply a row by a non-zero scalar;
Type III: add a scalar multiple of one row to a different row.

   The application of any specific elementary row operation to an m × n matrix
C can be affected by multiplying C on the left by a suitable m × m matrix X.
Indeed, the matrix X corresponding to a particular elementary row operation is
simply the matrix obtained by applying the same elementary row operation to the
m × m identity matrix. It is easy to see that for every elementary row operation, the
corresponding matrix X is invertible.

   We now describe the basic version of Gaussian elimination. The input is an m×n
matrix A, and the algorithm is described in Fig. 14.1.

   The algorithm works as follows. First, it makes a copy B of A (this is not neces-
sary if the original matrix A is not needed afterwards). The algorithm proceeds col-
umn by column, starting with the left-most column, so that after processing column
j, the first j columns of B are in reduced row echelon form, and the current value
of r represents the length of the pivot sequence. To process column j, in steps 3-6
the algorithm first searches for a non-zero element among B(r + 1, j), . . . , B(m, j);
if none is found, then the first j + 1 columns of B are already in reduced row
echelon form. Otherwise, one of these non-zero elements is selected as the pivot
element (the choice is arbitrary), which is then used in steps 8-13 to bring column
j into the required form. After incrementing r, the pivot element is brought into
position (r, j), using a Type I operation in step 9. Then the entry (r, j) is set to 1F ,
using a Type II operation in step 10. Finally, all the entries above and below entry
(r, j) are set to 0F , using Type III operations in steps 11-13. Note that because
390                             Matrices

     1. B  A, r  0

     2. for j  1 to n do

     3.           0, i  r

     4.          while = 0 and i  m do

     5.            ii+1

     6.            if B(i, j) = 0F then  i

     7.          if = 0 then

     8.            r  r+1

     9.            swap rows r and of B

     10.           Rowr(B)  B(r, j)-1 Rowr(B)

     11.           for i  1 to m do

     12.            if i = r then

     13.                      Rowi(B)  Rowi(B) - B(i, j) Rowr(B)

     14. output B

                   Fig. 14.1. Gaussian elimination

columns 1, . . . , j - 1 of B were already in reduced row echelon form, none of these
operations changes any values in these columns.

   As for the complexity of the algorithm, it is easy to see that it performs O(mn)
elementary row operations, each of which takes O(n) operations in F , so a total of
O(mn2) operations in F .

Example 14.4.  Consider the execution of the Gaussian elimination algorithm on
input
                                           
                              [0] [1] [1]

                   A = [2] [1] [2]  Z3×3 3 .

                              [2] [2] [0]

After copying A into B, the algorithm transforms B as follows:

[0] [1] [1]                   [2] [1] [2]                       [1] [2] [1]

  Row1Row2   Row1[2] Row1  
[2] [1] [2] --------- [0] [1] [1] ----------- [0] [1] [1]

[2] [2] [0]                   [2] [2] [0]                       [2] [2] [0]

                   [1] [2] [1]                      [1] [0] [2]   

Row3Row3 -[2] Row1   Row1Row1 -[2] Row2  
---------------- [0] [1] [1] ---------------- [0] [1] [1]

                   [0] [1] [1]                      [0] [1] [1]
                       14.4 Gaussian elimination                   391

                     [1] [0] [2]  

Row3Row3 - Row2  
-------------- [0] [1] [1] . 2

                     [0] [0] [0]

   Suppose the Gaussian elimination algorithm performs a total of t elementary
row operations. Then as discussed above, the application of the eth elementary
row operation, for e = 1, . . . , t, amounts to multiplying the current value of the
matrix B on the left by a particular invertible m × m matrix Xe. Therefore, the final
output value of B satisfies the equation

                           B = XA where X = XtXt-1 · · · X1.

Since the product of invertible matrices is also invertible, we see that X itself is
invertible.

   Although the algorithm as presented does not compute the matrix X, it can
be easily modified to do so. The resulting algorithm, which we call extended
Gaussian elimination, is the same as plain Gaussian elimination, except that we
initialize the matrix X to be the m × m identity matrix, and we add the following
steps:

     · just before step 9: swap rows r and of X;
     · just before step 10: Rowr(X)  B(r, j)-1 Rowr(X);

     · just before step 13: Rowi(X)  Rowi(X) - B(i, j) Rowr(X).

At the end of the algorithm we output X in addition to B.
   So we simply perform the same elementary row operations on X that we perform

on B. The reader may verify that the above algorithm is correct, and that it uses
O(mn(m + n)) operations in F .

Example 14.5. Continuing with Example 14.4, the execution of the extended

Gaussian elimination algorithm initializes X to the identity matrix, and then trans-

forms X as follows:

[1] [0] [0]            [0] [1] [0]                  [0] [2] [0]    

  Row1Row2   Row1[2] Row1  
[0] [1] [0] --------- [1] [0] [0] ----------- [1] [0] [0]

[0] [0] [1]            [0] [0] [1]                  [0] [0] [1]

                     [0] [2] [0]                    [1] [2] [0]  

Row3Row3 -[2] Row1   Row1Row1 -[2] Row2  
---------------- [1] [0] [0] ---------------- [1] [0] [0]

                     [0] [2] [1]                    [0] [2] [1]
392                 Matrices

     [1] [2] [0]  

Row3Row3 - Row2  
-------------- [1] [0] [0] . 2

     [2] [2] [1]

EXERCISE 14.11. For each type of elementary row operation, describe the matrix
X which corresponds to it, as well as X-1.

EXERCISE 14.12. Given a matrix B  F m×n in reduced row echelon form, show
how to compute its pivot sequence using O(n) operations in F .

EXERCISE 14.13. In §4.4, we saw how to speed up matrix multiplication over Z
using the Chinese remainder theorem. In this exercise, you are to do the same, but
for performing Gaussian elimination over Zp, where p is a large prime. Suppose
you are given an m × m matrix A over Zp, where len(p) = (m). Straightforward
application of Gaussian elimination would require O(m3) operations in Zp, each of
which takes time O(m2), leading to a total running time of O(m5). Show how to
use the techniques of §4.4 to reduce the running time of Gaussian elimination to
O(m4).

                      14.5 Applications of Gaussian elimination
Throughout this section, A is an arbitrary m×n matrix over a field F , and XA = B,
where X is an invertible m × m matrix, and B is an m × n matrix in reduced row
echelon form with pivot sequence (p1, . . . , pr). This is precisely the information
produced by the extended Gaussian elimination algorithm, given A as input (the
pivot sequence can easily be "read" directly from B -- see Exercise 14.12). Also,
let

                                     A : F 1×m  F 1×n

                                                  v  vA

be the linear map corresponding to A.

                             Computing the image and kernel

Consider first the row space of A, by which we mean the subspace of F 1×n spanned
by {Rowi(A)}mi=1, or equivalently, the image of A.

   We claim that the row space of A is the same as the row space of B. To see this,
note that since B = XA, for every v  F 1×m, we have vB = v(XA) = (vX)A, and

so the row space of B is contained in the row space of A. For the other containment,
note that since X is invertible, we can write A = X-1B, and apply the same

argument.
14.5 Applications of Gaussian elimination  393

   Further, note that the row space of B, and hence that of A, clearly has dimension
r. Indeed, as stated in Theorem 14.7, rows 1, . . . , r of B form a basis for the row
space of B.

   Consider next the kernel K of A, or what we might call the row null space
of A. We claim that {Rowi(X)}mi=r+1 is a basis for K. Clearly, just from the
fact that XA = B and the fact that rows r + 1, . . . , m of B are zero, it follows
that rows r + 1, . . . , m of X are contained in K. Furthermore, as X is invertible,
{Rowi(X)}mi=1 is a basis for F 1×m (see Theorem 14.6). Thus, the family of vectors
{Rowi(X)}mi=r+1 is linearly independent and spans a subspace K of K. It suffices
to show that K = K. Suppose to the contrary that K K, and let v  K \ K .
As {Rowi(X)}mi=1 spans F 1×m, we may write v = im=1 ci Rowi(X); moreover, as
v / K , we must have ci = 0F for some i = 1, . . . , r. Setting v~ := (c1, . . . , cm), we
see that v = v~X, and so

                          A(v) = vA = (v~X)A = v~(XA) = v~B.

Furthermore, since {Rowi(B)}ri=1 is linearly independent, rows r + 1, . . . , m of B
are zero, and v~ has a non-zero entry in one of its first r positions, we see that v~B is
not the zero vector. We have derived a contradiction, and hence may conclude that
K = K.

   Finally, note that if m = n, then A is invertible if and only if its row space has
dimension m, which holds if and only if r = m, and in the latter case, B is the
identity matrix, and hence X is the inverse of A.

   Let us summarize the above discussion:

     · The first r rows of B form a basis for the row space of A (i.e., the image of
         A).

     · The last m - r rows of X form a basis for the row null space of A (i.e., the
         kernel of A).

     · If m = n, then A is invertible (i.e., A is an isomorphism) if and only if
         r = m, in which case X is the inverse of A (i.e., the matrix of A-1 relative
         to the standard basis).

   So we see that from the output of the extended Gaussian elimination algorithm,
we can simply "read off" bases for both the image and the kernel, as well as the
inverse (if it exists), of a linear map represented as a matrix with respect to given
bases. Also note that this procedure provides a "constructive" version of Theo-
rem 13.28.

Example 14.6. Continuing with Examples 14.4 and 14.5, we see that the vectors
([1], [0], [2]) and ([0], [1], [1]) form a basis for the row space of A, while the vector
([2], [2], [1]) is a basis for the row null space of A. 2
394  Matrices

                           Solving systems of linear equations

Suppose that in addition to the matrix A, we are given w  F 1×n, and want to find
a solution v  F 1×m (or perhaps describe all solutions), to the equation

     vA = w.   (14.3)

Equivalently, we can phrase the problem as finding an element (or describing all
elements) of the set -A1({w}).

   Now, if there exists a solution at all, say v  F 1×m, then A(v) = A(v ) if and

only if v  v (mod K), where K is the kernel of A. It follows that the set of all
solutions to (14.3) is v + K = {v + v0 : v0  K}. Thus, given a basis for K and

any solution v to (14.3), we have a complete and concise description of the set of

solutions to (14.3).

   As we have discussed above, the last m - r rows of X form a basis for K, so it
suffices to determine if w  Im A, and if so, determine a single pre-image v of w.

   Also as we discussed, Im A, that is, the row space of A, is equal to the row space

of B, and because of the special form of B, we can quickly and easily determine if

the given w is in the row space of B, as follows. By definition, w is in the row space
of B if and only if there exists a vector v  F 1×m such that vB = w. We may as well
assume that all but the first r entries of v are zero. Moreover, vB = w implies that
for i = 1, . . . , r, the ith entry of v is equal to the pith entry of w. Thus, the vector v,

if it exists, is completely determined by the entries of w at positions p1, . . . , pr. We
can construct v satisfying these conditions, and then test if vB = w. If not, then we
may conclude that (14.3) has no solutions; otherwise, setting v := vX, we see that
vA = (vX)A = v(XA) = vB = w, and so v is a solution to (14.3).

   One easily verifies that if we implement the above procedure as an algorithm,

the work done in addition to running the extended Gaussian elimination algorithm
amounts to O(m(n + m)) operations in F .

   A special case of the above procedure is when m = n and A is invertible, in
which case (14.3) has a unique solution, namely, v := wX, since in this case,
X = A-1.

                                    The rank of a matrix

We define the row rank of A to be the dimension of its row space, which is equal to
dimF (Im A). The column space of A is defined as the subspace of F m×1 spanned
by {Colj(A)}nj=1; that is, the column space of A is {Az : z  F n×1}. The column
rank of A is the dimension of its column space.

   Now, the column space of A need not be the same as the column space of B, but
from the identity B = XA, and the fact that X is invertible, it easily follows that
these two subspaces are isomorphic (via the map that sends y  F m×1 to Xy), and
14.5 Applications of Gaussian elimination  395

hence have the same dimension. Moreover, by Theorem 14.7, the column rank of
B is r, which is the same as the row rank of A.

   So we may conclude: The column rank and row rank of A are the same.
   Because of this, we may define the rank of a matrix to be the common value of
its row and column rank.

                       The orthogonal complement of a subspace

So as to give equal treatment to rows and columns, one can also define the column

null space of A to be the kernel of the linear map defined by multiplication on the
left by A; that is, the column null space of A is {z  F n×1 : Az = 0Fm×1}. By
applying the results above to the transpose of A, we see that the column null space

of A has dimension n - r, where r is the rank of A.

   Let U  F 1×n be the row space of A, and let U   F 1×n denote the set of
all vectors u  F 1×n whose transpose u belongs to the column null space of A.
Now, U is a subspace of F 1×n of dimension r and U  is a subspace of F 1×n of
dimension n - r. The space U  consists precisely of all vectors u  F 1×n that are

"orthogonal" to all vectors u  U , in the sense that the "inner product" uu is zero.
For this reason, U  is sometimes called the "orthogonal complement of U ."

   Clearly, U  is determined by the subspace U itself, and does not depend on the

particular choice of matrix A. It is also easy to see that the orthogonal complement
of U  is U ; that is, (U ) = U . This follows immediately from the fact that
U  (U ) and dimF ((U )) = n - dimF (U ) = dimF (U ).

   Now suppose that U  U  = {0}. Then by Theorem 13.11, we have an isomor-
phism of U × U  with U + U , and since U × U  has dimension n, it must be the
case that U + U  = F 1×n. It follows that every element of F 1×n can be expressed
uniquely as u + u, where u  U and u  U .

   We emphasize that the observations in the previous paragraph hinged on the
assumption that U  U  = {0}, which itself holds provided U contains no non-

zero "self-orthogonal vectors" u such that uu is zero. If F is the field of real

numbers, then of course there are no non-zero self-orthogonal vectors, since uu

is the sum of the squares of the entries of u. However, for other fields, there may
very well be non-zero self-orthogonal vectors. As an example, if F = Z2, then any
vector u with an even number of 1-entries is self orthogonal.

   So we see that while much of the theory of vector spaces and matrices carries

over without change from familiar ground fields, like the real numbers, to arbitrary

ground fields F , not everything does. In particular, the usual decomposition of a

vector space into a subspace and its orthogonal complement breaks down, as does

any other procedure that relies on properties specific to "inner product spaces."
396  Matrices

   For the following three exercises, as above, A is an arbitrary m × n matrix over a
field F , and XA = B, where X is an invertible m × m matrix, and B is in reduced

row echelon form.

EXERCISE 14.14. Show that the column null space of A is the same as the column
null space of B.

EXERCISE 14.15. Show how to compute a basis for the column null space of A
using O(r(n - r)) operations in F , given A and B.

EXERCISE 14.16. Show that the matrix B is uniquely determined by A; more
precisely, show that if X A = B , where X is an invertible m × m matrix, and B is
in reduced row echelon form, then B = B.

   In the following two exercises, the theory of determinants could be used; how-
ever, they can all be solved directly, without too much difficulty, using just the ideas

developed so far in the text.

EXERCISE 14.17. Let p be a prime. A matrix A  Zm×m is called invertible mod-
ulo p if there exists a matrix B  Zm×m such that AB  BA  I (mod p), where I
is the m × m integer identity matrix. Here, two matrices are considered congruent

with respect to a given modulus if their corresponding entries are congruent. Show
that A is invertible modulo p if and only if A is invertible over Q, and the entries
of A-1 lie in Q(p) (see Example 7.26).

EXERCISE 14.18. You are given a matrix A  Zm×m and a prime p such that A
is invertible modulo p (see previous exercise). Suppose that you are also given
w  Z1×m.

    (a) Show how to efficiently compute a vector v  Z1×m such that vA 
         w (mod p), and that v is uniquely determined modulo p.

   (b) Given a vector v as in part (a), along with an integer e  1, show how
         to efficiently compute v^  Z1×m such that v^A  w (mod pe), and that v^
         is uniquely determined modulo pe. Hint: mimic the "lifting" procedure

         discussed in §12.5.2.

    (c) Using parts (a) and (b), design and analyze an efficient algorithm that takes

         the matrix A and the prime p as input, together with a bound H on the

         absolute value of the numerator and denominator of the entries of the vector
         v that is the unique (rational) solution to the equation v A = w. Your

         algorithm should run in time polynomial in the length of H, the length of

         p, and the sum of the lengths of the entries of A and w. Hint: use rational

         reconstruction, but be sure to fully justify its application.
14.5 Applications of Gaussian elimination  397

   Note that in the previous exercise, one can use the theory of determinants to
derive good bounds, in terms of the lengths of the entries of A and w, on the size of
the least prime p such that A is invertible modulo p (assuming A is invertible over
the rationals), and on the length of the numerator and denominator of the entries of
rational solution v to the equation v A = w. The interested reader who is familiar
with the basic theory of determinants is encouraged to establish such bounds.

   The next two exercises illustrate how Gaussian elimination can be adapted, in
certain cases, to work in rings that are not necessarily fields. Let R be an arbitrary
ring. A matrix B  Rm×n is said to be in row echelon form if there exists a pivot
sequence (p1, . . . , pr), with 0  r  m and 1  p1 < p2 < · · · < pr  n, such that
the following holds:

     · for i = 1, . . . , r, all of the entries in row i of B to the left of entry (i, pi) are
         zero;

     · for i = 1, . . . , r, we have B(i, pi) = 0R;
     · all entries in rows r + 1, . . . , m of B are zero.

EXERCISE 14.19. Let R be the ring Zpe, where p is prime and e > 1. Let
 := [p]  R. The goal of this exercise is to develop an efficient algorithm for the
following problem: given a matrix A  Rm×n, with m > n, find a vector v  R1×m
such that vA = 0R1×n but v / R1×m.

    (a) Show how to modify the extended Gaussian elimination algorithm to solve
         the following problem: given a matrix A  Rm×n, compute X  Rm×m and
         B  Rm×n, such that XA = B, X is invertible, and B is in row echelon
         form. Your algorithm should run in time O(mn(m + n)e2 len(p)2). Assume

         that the input includes the values p and e. Hint: when choosing a pivot ele-

         ment, select one divisible by a minimal power of ; as in ordinary Gaussian

         elimination, your algorithm should only use elementary row operations to

         transform the input matrix.

   (b) Using the fact that the matrix X computed in part (a) is invertible, argue
         that none of its rows belong to R1×m.

    (c) Argue that if m > n and the matrix B computed in part (a) has pivot

         sequence (p1, . . . , pr), then m - r > 0 and if v is any one of the last m - r
         rows of X, then vA = 0R1×n.
   (d) Give an example that shows that {Rowi(B)}ri=1 need not be linearly inde-
         pendent, and that {Rowi(X)}mi=r+1 need not span the kernel of the linear
         map A corresponding to A.

EXERCISE 14.20. Let R be the ring Z , where > 1 is an integer. You are given
a matrix A  Rm×n. Show how to efficiently compute X  Rm×m and B  Rm×n
398            Matrices

such that XA = B, X is invertible, and B is in row echelon form. Your algorithm
should run in time O(mn(m + n) len( )2). Hint: to zero-out entries, you should use

"rotations"--for integers a, b, d, s, t with

               d = gcd(a, b) = 0 and as + bt = d,

and for row indices r, i, a rotation simultaneously updates rows r and i of a matrix

C as follows:

(Rowr(C), Rowi(C))  (s Rowr(C) + t Rowi(C), - b Rowr(C) + a Rowi(C));
                                                     d  d

observe that if C(r, j) = [a] and C(i, j) = [b] before applying the rotation, then

C(r, j) = [d] and C(i, j) = [0] after the rotation.

EXERCISE 14.21. Consider again the setting in Exercise 14.3. Show that A 
F m×(m- ) is a parity check matrix for U if and only if {Colj(A) }im=1- is a basis for
the orthogonal complement of VecS (U )  F 1×m.

EXERCISE 14.22. Let {vi}in=1 be a family of vectors, where vi  R1× for each
i = 1, . . . , n. We say that {vi}in=1 is pairwise orthogonal if vivj = 0 for all i = j.
Show that every pairwise orthogonal family of non-zero vectors over R is linearly
independent.

EXERCISE 14.23. The purpose of this exercise is to use linear algebra to prove that

any pairwise independent family of hash functions (see §8.7) must contain a large
number of hash functions. More precisely, let {r}rR be a pairwise independent

family of hash functions from S to T , with |T |  2. Our goal is to show that
|R|  |S|. Let n := |S|, and m := |T |, and := |R|. Write R = {r1, . . . , r } and
S = {s1, . . . , sn}. Without loss of generality, we may assume that T is a set of
non-zero real numbers that sum to zero (e.g., T = {1, . . . , m - 1, -m(m - 1)/2}).
Now define the matrix A  Rn× with A(i, j) := rj (si). Show that {Rowi(A)}ni=1
is a pairwise orthogonal family of non-zero vectors (see previous exercise). From

this, deduce that  n.

                                          14.6 Notes

While a trivial application of the defining formulas yields a simple algorithm for
multiplying two n × n matrices over a ring R that uses O(n3) operations in R, this
algorithm is not the best, asymptotically speaking. The currently fastest algorithm
for this problem, due to Coppersmith and Winograd [28], uses O(n) operations in
R, where  < 2.376. We note, however, that the good old O(n3) algorithm is still
the only one used in almost any practical setting.
                            15

Subexponential-time discrete logarithms and factoring

This chapter presents subexponential-time algorithms for computing discrete log-
arithms and for factoring integers. These algorithms share a common technique,
which makes essential use of the notion of a smooth number.

                                   15.1 Smooth numbers

If y is a non-negative real number and m is a positive integer, then we say that m is
y-smooth if all prime divisors of m are at most y.

   For 0  y  x, let us define (y, x) to be the number of y-smooth integers up to
x. The following theorem gives us a lower bound on (y, x), which will be crucial
in the analysis of our discrete logarithm and factoring algorithms.

Theorem 15.1. Let y be a function of x such that

              y   and u := log x  
              log x       log y

as x  . Then

              (y, x)  x · exp[(-1 + o(1))u log log x].

Proof. Let us write u = u + , where 0   < 1. Let us split the primes up to y
into two sets: the set V of "very small" primes that are at most y/2, and the set
W of other primes that are greater than y/2 but at most y. To simplify matters,

let us also include the integer 1 in the set V .

   By Bertrand's postulate (Theorem 5.8), there exists a constant C > 0 such that
|W |  Cy/ log y for sufficiently large y. By the assumption that y/ log x   as
x  , we also have |W |  2 u for sufficiently large x.

   To derive the lower bound, we shall count those integers that can be built up by

multiplying together u distinct elements of W , together with one element of V .

                     399
400  Subexponential-time discrete logarithms and factoring

These products are clearly distinct, y-smooth numbers, and each is bounded by x,
since each is at most y u y = yu = x.

   If S denotes the set of all of these products, then for x sufficiently large, we have

     |S| = |W | · |V |
                u

     = |W |(|W | - 1) · · · (|W | - u + 1) · |V |
                           u!

       |W | u               · |V |

       2u

                            Cy u  · |V |

       2u log y

     = Cy · |V |. u-
          2 log x

Taking logarithms, we have

     log|S|  (u - )(log y - log log x + log(C/2)) + log|V |            (15.1)
             = log x - u log log x + (log|V | -  log y) +
                 O(u + log log x).

To prove the theorem, it suffices to show that

     log|S|  log x - (1 + o(1))u log log x.

Under our assumption that u  , the term O(u + log log x) in (15.1) is clearly
o(u log log x), and so it will suffice to show that the term (log|V | -  log y) is also
o(u log log x). But by Chebyshev's theorem (Theorem 5.1), for some positive con-
stant D, we have

                                   Dy/ log y  |V |  y,

and taking logarithms, and again using the fact that u  , we have
                  log|V | -  log y = O(log log y) = o(u log log x). 2

                      15.2 An algorithm for discrete logarithms

We now present a probabilistic, subexponential-time algorithm for computing dis-

crete logarithms. The input to the algorithm is p, q, , , where p and q are primes,
with q | (p - 1),  is an element of Zp generating a subgroup G of Zp of order q,
and   G.

   We shall make the simplifying assumption that q2 (p - 1), which is equivalent
to saying that q m := (p - 1)/q. Although not strictly necessary, this assumption
15.2 An algorithm for discrete logarithms               401

simplifies the design and analysis of the algorithm, and moreover, for cryptographic
applications, this assumption is almost always satisfied. Exercises 15.1-15.3 below
explore how this assumption may be lifted, as well as other generalizations.

   At a high level, the main goal of our discrete logarithm algorithm is to find a ran-
dom representation of 1 with respect to  and  -- as discussed in Exercise 11.12,
this allows us to compute log  (with high probability). More precisely, our main
goal is to compute integers r and s in a probabilistic fashion, such that rs = 1
and [s]q is uniformly distributed over Zq. Having accomplished this, then with
probability 1 - 1/q, we shall have s  0 (mod q), which allows us to compute
log  as -rs-1 mod q.

   Let H be the subgroup of Zp of order m. Our assumption that q m implies
that G  H = {1}, since the multiplicative order of any element in the intersection
must divide both q and m, and so the only possibility is that the multiplicative
order is 1. Therefore, the map  : G × H  Zp that sends (, ) to  is injective
(Theorem 6.25), and since |Zp| = qm, it must be surjective as well.

   We shall use this fact in the following way: if  is chosen uniformly at random
from G, and  is chosen uniformly at random from H (and independent of ), then
 is uniformly distributed over Zp. Furthermore, since H is the image of the q-
power map on Zp, we may generate a random   H simply by choosing ^  Zp
at random, and setting  := ^q.

   The discrete logarithm algorithm uses a "smoothness parameter" y. We will

discuss choice of y below, when we analyze the running time of the algorithm; for

now, we only assume that y < p. Let p1, . . . , pk be an enumeration of the primes
up to y. Let i := [pi]p  Zp for i = 1, . . . , k.

The algorithm has two stages.
In the first stage, we find relations of the form

                           ri si i = 1ei1 . . . keik ,  (15.2)

for i = 1, . . . , k + 1, where ri, si, ei1, . . . , eik  Z and i  H for each i.
   We obtain each such relation by a randomized search, as follows: we choose

ri, si  {0, . . . , q - 1} at random, as well as ^i  Zp at random; we then compute
i := ^qi , i := risi, and mi := rep(ii). Now, the value i is uniformly dis-
tributed over G, while i is uniformly distributed over H; therefore, the product
ii is uniformly distributed over Zp, and hence mi is uniformly distributed over
{1, . . . , p - 1}. Next, we simply try to factor mi by trial division, trying all the

primes p1, . . . , pk up to y. If we are lucky, we completely factor mi in this way,

obtaining a factorization

                           mi = p1ei1 · · · pkeik ,
402    Subexponential-time discrete logarithms and factoring

for some exponents ei1, . . . , eik, and we get the relation (15.2). If we are unlucky,
then we simply keep trying until we are lucky.

   For i = 1, . . . , k + 1, let vi := (ei1, . . . , eik)  Z×k, and let vi denote the image of
vi in Zq×k (i.e., vi := ([ei1]q, . . . , [eik]q)). Since Zq×k is a vector space over the field
Zq of dimension k, the family of vectors v1, . . . , vk+1 must be linearly dependent.
The second stage of the algorithm uses Gaussian elimination over Zq (see §14.4)
to find a linear dependence among the vectors v1, . . . , vk+1, that is, to find integers

c1, . . . , ck+1  {0, . . . , q - 1}, not all zero, such that

       (e1, . . . , ek) := c1v1 + · · · + ck+1vk+1  qZ×k.

   Raising each equation (15.2) to the corresponding power ci, and multiplying
them all together, we obtain

            rs = 1e1 · · · kek ,

where

       k+1  k+1                   k+1

       r := ciri, s := cisi, and  := ici.

       i=1  i=1                   i=1

                                                                                                                          ej

   Now,   H, and since each ej is a multiple of q, we also have j  H
for j = 1, . . . , k. It follows that rs  H. But since rs  G as well, and
G  H = {1}, it follows that rs = 1. If we are lucky (and we will be with

overwhelming probability, as we discuss below), we will have s  0 (mod q), in
which case, we can compute s := s-1 mod q, obtaining

             = -rs ,

and hence -rs mod q is the discrete logarithm of  to the base . If we are very
unlucky, we will have s  0 (mod q), at which point the algorithm simply quits,
reporting "failure."

   The entire algorithm, called Algorithm SEDL, is presented in Fig. 15.1.
   As already argued above, if Algorithm SEDL does not output "failure," then
its output is indeed the discrete logarithm of  to the base . There remain three
questions to answer:

     1. What is the expected running time of Algorithm SEDL?

     2. How should the smoothness parameter y be chosen so as to minimize the
         expected running time?

     3. What is the probability that Algorithm SEDL outputs "failure"?

   Let us address these questions in turn. As for the expected running time, let
 be the probability that a random element of {1, . . . , p - 1} is y-smooth. Then
         15.2 An algorithm for discrete logarithms                     403

i0

repeat
      ii+1

repeat

             choose ri, si  {0, . . . , q - 1} at random
             choose ^i  Zp at random
             i  ri si , i  ^qi , mi  rep(ii)
             test if mi is y-smooth (trial division)
      until mi = p1ei1 · · · pkeik for some integers ei1, . . . , eik
until i = k + 1

set vi  (ei1, . . . , eik)  Z×k for i = 1, . . . , k + 1

apply Gaussian elimination over Zq to find integers c1, . . . , ck+1 

{0, . . . , q - 1}, not all zero, such that

c1v1 + · · · + ck+1vk+1  qZ×k.

    k+1          k+1
r  i=1 ciri, s  i=1 cisi

if s  0 (mod q)

then output "failure"
else output -rs-1 mod q

                 Fig. 15.1. Algorithm SEDL

the expected number of attempts needed to produce a single relation is -1, and
so the expected number of attempts to produce k + 1 relations is (k + 1)-1.

In each attempt, we perform trial division using p1, . . . , pk, along with a few
other minor computations, leading to a total expected running time in stage 1 of
k2-1 · len(p)O(1). The running time in stage 2 is dominated by the Gaussian
elimination step, which takes time k3 · len(p)O(1). Thus, if Z is the total running

time of the algorithm, then we have

                 E[Z]  (k2-1 + k3) · len(p)O(1).                       (15.3)

Let us assume for the moment that                                      (15.4)
                                 y = exp[(log p)+o(1)]

for some constant  with 0 <  < 1. Our final choice of y will indeed satisfy this
assumption. Consider the probability . We have

                = (y, p - 1)/(p - 1) = (y, p)/(p - 1)  (y, p)/p,
404            Subexponential-time discrete logarithms and factoring

where for the second equality we use the assumption that y < p, so p is not y-
smooth. With our assumption (15.4), we may apply Theorem 15.1 (with the given
value of y and x := p), obtaining

                        exp[(-1 + o(1))(log p/ log y) log log p].

By Chebyshev's theorem (Theorem 5.1), we know that k = (y/ log y), and so
log k = (1 + o(1)) log y. Moreover, assumption (15.4) implies that the factor
len(p)O(1) in (15.3) is of the form exp[o(min(log y, log p/ log y))], and so we have

   E[Z]  exp[(1 + o(1)) max{(log p/ log y) log log p + 2 log y, 3 log y}]. (15.5)

   Let us find the value of y that minimizes the right-hand side of (15.5), ignoring

the "o(1)" terms. Let µ := log y, A := log p log log p, S1 := A/µ + 2µ, and
S2 := 3µ. We want to find µ that minimizes max{S1, S2}. Using a little calculus,
one sees that S1 is minimized at µ = (A/2)1/2. With this choice of µ, we have
S1 = (22)A1/2 and S2 = (3/2)A1/2 < S1. Thus, choosing

                           y = exp[(1/2)(log p log log p)1/2],

we obtain      E[Z]  exp[(22 + o(1))(log p log log p)1/2].

   That takes care of the first two questions, although strictly speaking, we have
only obtained an upper bound for the expected running time, and we have not
shown that the choice of y is actually optimal, but we shall nevertheless content
ourselves (for now) with these results. Finally, we deal with the third question, on
the probability that the algorithm outputs "failure."

Lemma 15.2. The probability that Algorithm SEDL outputs "failure" is 1/q.

Proof. Let F be the event that the algorithm outputs "failure." For i = 1, . . . , k + 1,
we may view the final values assigned to ri, si, i, and mi as random variables,
which we shall denote by these same names (to avoid additional notation). Simi-
larly, we may view s as a random variable.

   Let m1, . . . , mk+1 be arbitrary, fixed y-smooth numbers, and let B be the event
that m1 = m1, . . . , mk+1 = mk+1. We shall show that P[F | B] = 1/q, and since this
holds for all relevant B, it follows by total probability that P[F] = 1/q.

   For the rest of the argument, we focus on the conditional distribution given
B. With respect to this conditional distribution, the distribution of each random
variable (ri, si, i) is (essentially) the uniform distribution on the set

               Pi := {(r , s ,  )  Iq × Iq × H : r s  = [mi]p},

where  Iq  :=  {0, . . . , q  - 1};  also,  the  family  of  random  variables                     k+1

                                                                                {(ri, si, i)}i=1
                       15.2 An algorithm for discrete logarithms                                     405

is mutually independent. It is easy to see that for i = 1, . . . , k + 1, and for each

s  Iq, there exist unique values r  Iq and   H such that (r , s ,  )  Pi. From

this, it easily follows that each si is uniformly distributed over Iq, and the family
                              k+1
of  random  variables              is  mutually  independent.  Also,   the  values  c1,  .  .  .  ,  ck+1
                       {si}i=1

computed by the algorithm are fixed (as they are determined by m1, . . . , mk+1), and
since s = c1s1 +· · ·+ck+1sk+1, and not all the ci's are zero modulo q, it follows that

s mod q is uniformly distributed over Iq, and so is equal to zero with probability

1/q. 2

    Let us summarize the above discussion in the following theorem.

Theorem 15.3. With the smoothness parameter set as

                       y  :=                     p  log  log  p)1/2],
                              exp[(1/ 2)(log

the expected running time of Algorithm SEDL is at most
                           exp[(22 + o(1))(log p log log p)1/2].

The probability that Algorithm SEDL outputs "failure" is 1/q.

   In the description and analysis of Algorithm SEDL, we have assumed that the
primes p1, . . . , pk were pre-computed. Of course, we can construct this list of
primes using, for example, the sieve of Eratosthenes (see §5.4), and the running
time of this pre-computation will be dominated by the running time of Algo-
rithm SEDL.

   In the analysis of Algorithm SEDL, we relied crucially on the fact that in gener-
ating a relation, each candidate element risii was uniformly distributed over Zp.
If we simply left out the i's, then the candidate element would be uniformly dis-
tributed over the subgroup G, and Theorem 15.1 simply would not apply. Although
the algorithm might anyway work as expected, we would not be able to prove this.

EXERCISE 15.1. Using the result of Exercise 14.19, show how to modify Algo-
rithm SEDL to work in the case where p - 1 = qem, e > 1, q m,  generates
the subgroup G of Zp of order qe, and   G. Your algorithm should compute
log  with roughly the same expected running time and success probability as
Algorithm SEDL.

EXERCISE 15.2. Using the algorithm of the previous exercise as a subroutine,

design and analyze an algorithm for the following problem. The input is p, q, , ,
where p is a prime, q is a prime dividing p - 1,  generates the subgroup G of Zp
of order q, and   G; note that we may have q2 | (p - 1). The output is log .
Your algorithm should always succeed in computing this discrete logarithm, and its
406  Subexponential-time discrete logarithms and factoring

expected running time should be bounded by a constant times the expected running
time of the algorithm of the previous exercise.

EXERCISE 15.3. Using the result of Exercise 14.20, show how to modify Algo-
rithm SEDL to solve the following problem: given a prime p, a generator  for
Zp, and an element   Zp, compute log . Your algorithm should work without
knowledge of the factorization of p-1; its expected running time should be roughly
the same as that of Algorithm SEDL, but its success probability may be lower. In
addition, explain how the success probability may be significantly increased at
almost no cost by collecting a few extra relations.

EXERCISE 15.4. Let n = pq, where p and q are distinct, large primes. Let e be a
prime, with e < n and e (p - 1)(q - 1). Let x be a positive integer, with x < n.
Suppose you are given n (but not its factorization!) along with e and x. In addition,
you are given access to two "oracles," which you may invoke as often as you like.

     · The first oracle is a "challenge oracle": each invocation of the oracle pro-
         duces a "challenge" a  {1, . . . , x} -- distributed uniformly, and independ-
         ent of all other challenges.

     · The second oracle is a "solution oracle": you invoke this oracle with the
         index of a previous challenge oracle; if the corresponding challenge was a,
         the solution oracle returns the eth root of a modulo n; that is, the solution
         oracle returns b  {1, . . . , n - 1} such that be  a (mod n) -- note that b
         always exists and is uniquely determined.

Let us say that you "win" if you are able to compute the eth root modulo n of any
challenge, but without invoking the solution oracle with the corresponding index
of the challenge (otherwise, winning would be trivial, of course).

    (a) Design a probabilistic algorithm that wins the above game, using an
         expected number of

                          exp[(c + o(1))(log x log log x)1/2] · len(n)O(1)

     steps, for some constant c, where a "step" is either a computation step or an
     oracle invocation (either challenge or solution). Hint: Gaussian elimination
     over the field Ze.
(b) Suppose invocations of the challenge oracle are "cheap," while invocations
     of the solution oracle are relatively "expensive." How would you modify
     your strategy in part (a)?

   Exercise 15.4 has implications in cryptography. A popular way of implementing
a public-key primitive known as a "digital signature" works as follows: to digi-
tally sign a message M (which may be an arbitrarily long bit string), first apply
15.3 An algorithm for factoring integers  407

a "hash function" or "message digest" H to M, obtaining an integer a in some
fixed range {1, . . . , x}, and then compute the signature of M as the eth root b of
a modulo n. Anyone can verify that such a signature b is correct by checking that
be  H (M) (mod n); however, it would appear to be difficult to "forge" a signature
without knowing the factorization of n. Indeed, one can prove the security of this
signature scheme by assuming that it is hard to compute the eth root of a random
number modulo n, and by making the heuristic assumption that H is a random
function (see §15.5). However, for this proof to work, the value of x must be close
to n; otherwise, if x is significantly smaller than n, as the result of this exercise,
one can break the signature scheme at a cost that is roughly the same as the cost of
factoring numbers around the size of x, rather than the size of n.

                       15.3 An algorithm for factoring integers

We now present a probabilistic, subexponential-time algorithm for factoring inte-
gers. The algorithm uses techniques very similar to those used in Algorithm SEDL
in §15.2.

   Let n > 1 be the integer we want to factor. We make a few simplifying assump-
tions. First, we assume that n is odd -- this is not a real restriction, since we can
always pull out any factors of 2 in a pre-processing step. Second, we assume that
n is not a perfect power, that is, not of the form ab for integers a > 1 and b > 1 --
this is also not a real restriction, since we can always partially factor n using the
algorithm from Exercise 3.31 if n is a perfect power. Third, we assume that n is
not prime -- this may be efficiently checked using, say, the Miller-Rabin test (see
§10.2). Fourth, we assume that n is not divisible by any primes up to a "smoothness
parameter" y -- we can ensure this using trial division, and it will be clear that the
running time of this pre-computation is dominated by that of the algorithm itself.

   With these assumptions, the prime factorization of n is of the form

                                        n = q1f1 · · · qwfw ,

where w > 1, the qi's are distinct, odd primes, each greater than y, and the fi's are
positive integers.

   The main goal of our factoring algorithm is to find a random square root of 1 in
Zn. Let

                                : Zn  Zqf1 × · · · × 1 Zqfww
                                     [a]n  ([a]qf1 , . . . , [a] 1 qfww )

be the ring isomorphism of the Chinese remainder theorem. The square roots of
1 in Zn are precisely those elements   Zn such that () = (±1, . . . , ±1). If
 is a random square root of 1, then with probability 1 - 2-w+1  1/2, we have
408    Subexponential-time discrete logarithms and factoring

() = (1, . . . , w), where the i's are neither all 1 nor all -1 (i.e.,  = ±1). If this
happens, then ( - 1) = (1 - 1, . . . , w - 1), and so we see that some, but not all,

of the values i - 1 will be zero. The value of gcd(rep( - 1), n) is precisely the
product of the prime powers qifi such that i - 1 = 0, and hence this gcd will yield
a non-trivial factorization of n, unless  = ±1.

   Let p1, . . . , pk be the primes up to the smoothness parameter y mentioned above.
Let i := [pi]n  Zn for i = 1, . . . , k.

   We first describe a simplified version of the algorithm, after which we modify

the algorithm slightly to deal with a technical problem. Like Algorithm SEDL, this

algorithm proceeds in two stages. In the first stage, we find relations of the form

       i2 = 1ei1 · · · keik ,                                 (15.6)

for i = 1, . . . , k + 1, where ei1, . . . , eik  Z and i  Zn for each i.
   We can obtain each such relation by randomized search, as follows: we select

i  Zn at random, square it, and try to factor mi := rep(i2) by trial division, trying
all the primes p1, . . . , pk up to y. If we are lucky, we obtain a factorization

       mi = p1ei1 · · · pkeik ,

for some exponents ei1, . . . , eik, yielding the relation (15.6); if not, we just keep
trying.

   For i = 1, . . . , k + 1, let vi := (ei1, . . . , eik)  Z×k, and let vi denote the image
of vi in Z2×k (i.e., vi := ([ei1]2, . . . , [eik]2)). Since Z2×k is a vector space over
the field Z2 of dimension k, the family of vectors v1, . . . , vk+1 must be linearly
dependent. The second stage of the algorithm uses Gaussian elimination over Z2
to find a linear dependence among the vectors v1, . . . , vk+1, that is, to find integers
c1, . . . , ck+1  {0, 1}, not all zero, such that

                      (e1, . . . , ek) := c1v1 + · · · + ck+1vk+1  2Z×k.

Raising each equation (15.6) to the corresponding power ci, and multiplying them
all together, we obtain

                                       2 = 1e1 · · · kek ,

where

               k+1

        := ici .

                i=1

Since each ei is even, we can compute

                                      := 1e1/2 · · · kek/2,
and we see that 2 = 2, and hence (/)2 = 1. Thus,  := / is a square root
15.3 An algorithm for factoring integers  409

of 1 in Zn. A more careful analysis (see below) shows that in fact,  is uniformly
distributed over all square roots of 1, and hence, with probability at least 1/2, if we

compute gcd(rep( - 1), n), we get a non-trivial factor of n.

   That is the basic idea of the algorithm. There is, however, a technical problem.
Namely, in the method outlined above for generating a relation, we attempt to fac-
tor mi := rep(i2). Thus, the running time of the algorithm will depend in a crucial
way on the probability that a random square modulo n is y-smooth. Unfortunately
for us, Theorem 15.1 does not say anything about this situation -- it only applies
to the situation where a number is chosen at random from an interval [1, x]. There
are (at least) three different ways to address this problem:

     1. Ignore it, and just assume that the bounds in Theorem 15.1 apply to random
         squares modulo n (taking x := n in the theorem).

     2. Prove a version of Theorem 15.1 that applies to random squares modulo n.

     3. Modify the factoring algorithm, so that Theorem 15.1 applies.

The first choice, while not unreasonable from a practical point of view, is not very
satisfying mathematically. It turns out that the second choice is indeed a viable
option (i.e., the theorem is true and is not so difficult to prove), but we opt for the
third choice, as it is somewhat easier to carry out, and illustrates a probabilistic
technique that is more generally useful.

   So here is how we modify the basic algorithm. Instead of generating relations of
the form (15.6), we generate relations of the form

i2 = 1ei1 · · · keik ,                    (15.7)

for i = 1, . . . , k + 2, where ei1, . . . , eik  Z and i  Zn for each i, and   Zn.
Note that the value  is the same in all relations.

   We generate these relations as follows. For the very first relation (i.e., i = 1),
we repeatedly choose 1 and  in Zn at random, until rep(12) is y-smooth. Then,
after having found the first relation, we find each subsequent relation (i.e., for
i > 1) by repeatedly choosing i in Zn at random until rep(i2) is y-smooth,
where  is the same value that was used in the first relation. Now, Theorem 15.1

will apply directly to determine the success probability of each attempt to generate
the first relation. When we have found this relation, the value 21 will be uniformly
distributed over all y-smooth elements of Zn (i.e., elements whose integer repre-
sentations are y-smooth). Consider the various cosets of (Zn)2 in Zn. Intuitively,
it is much more likely that a random y-smooth element of Zn lies in a coset that
contains many y-smooth elements than in a coset with very few, and indeed, it is

reasonably likely that the fraction of y-smooth elements in the coset containing 
is not much less than the overall fraction of y-smooth elements in Zn. Therefore,
410    Subexponential-time discrete logarithms and factoring

for i > 1, each attempt to find a relation should succeed with reasonably high

probability. This intuitive argument will be made rigorous in the analysis to follow.
   The second stage is then modified as follows. For i = 1, . . . , k + 2, let vi :=

(ei1, . . . , eik, 1)  Z×(k+1), and let vi denote the image of vi in Z2×(k+1). Since
Z2×(k+1) is a vector space over the field Z2 of dimension k + 1, the family of vectors
v1, . . . , vk+2 must be linearly dependent. Therefore, we use Gaussian elimination
over Z2 to find a linear dependence among the vectors v1, . . . , vk+2, that is, to find
integers c1, . . . , ck+2  {0, 1}, not all zero, such that

                  (e1, . . . , ek+1) := c1v1 + · · · + ck+2vk+2  2Z×(k+1).

Raising each equation (15.7) to the corresponding power ci, and multiplying them
all together, we obtain

                                    2ek+1 = 1e1 · · · kek ,

where

               k+2

        := ici .

                i=1

Since each ei is even, we can compute

                                  := 1e1/2 · · · kek/2-ek+1/2,

so that 2 = 2 and  := / is a square root of 1 in Zn.
   The entire algorithm, called Algorithm SEF, is presented in Fig. 15.2.
   Now the analysis. From the discussion above, it is clear that Algorithm SEF

either outputs "failure," or outputs a non-trivial factor of n. So we have the same
three questions to answer as we did in the analysis of Algorithm SEDL:

     1. What is the expected running time of Algorithm SEF?

     2. How should the smoothness parameter y be chosen so as to minimize the
        expected running time?

     3. What is the probability that Algorithm SEF outputs "failure"?

   To answer the first question, let  denote the probability that (the canonical
representative of) a random element of Zn is y-smooth. For i = 1, . . . , k + 2, let
Li denote the number of iterations of the inner loop in the ith iteration of the main

loop in stage 1; that is, Li is the number of attempts made in finding the ith relation.

Lemma 15.4. For i = 1, . . . , k + 2, we have E[Li]  -1.

Proof. We first compute E[L1]. As  is chosen uniformly from Zn and independ-
ent of 1, at each attempt to find a relation, 21 is uniformly distributed over Zn,
                  15.3 An algorithm for factoring integers                   411

      i0

      repeat
            ii+1

            repeat
                   choose i  Zn at random
                   if i = 1 then choose   Zn at random
                   mi  rep(i2)
                   test if mi is y-smooth (trial division)

            until mi = p1ei1 · · · pkeik for some integers ei1, . . . , eik
      until i = k + 2

      set vi  (ei1, . . . , eik, 1)  Z×(k+1) for i = 1, . . . , k + 2

      apply Gaussian elimination over Z2 to find integers c1, . . . , ck+2 

        {0, 1}, not all zero, such that
        (e1, . . . , ek+1) := c1v1 + · · · + ck+2vk+2  2Z×(k+1).

          k+2 ci         e1/2  ek/2 -ek+1/2  ,   /
          i=1 i ,   1 · · · k 

      if  = ±1

        then output "failure"

        else output gcd(rep( - 1), n)

                         Fig. 15.2. Algorithm SEF

and hence the probability that the attempt succeeds is precisely . This means
E[L1] = -1.

   We next compute E[Li] for i > 1. To this end, let us denote the cosets of (Zn)2
by Zn as C1, . . . , Ct. As it happens, t = 2w, but this fact plays no role in the
analysis. For j = 1, . . . , t, let j denote the probability that a random element of

Cj is y-smooth, and let j denote the probability that the final value of  belongs to

Cj .

We claim that for j = 1, . . . , t, we have j = j-1t-1. To see this, note that each

coset Cj has the same number of elements, namely, |Zn|t-1, and so the number of
y-smooth elements in Cj is equal to j|Zn|t-1. Moreover, the final value of 21
is equally likely to be any one of the y-smooth numbers in Zn, of which there are
|Zn|, and hence

                              j |Zn |t-1
                         j =    = j t ,
                                          -1 -1

                               |Zn|

which proves the claim.
412            Subexponential-time discrete logarithms and factoring

   Now, for a fixed value of  and a random choice of i  Zn, one sees that i2
is uniformly distributed over the coset containing . Therefore, for j = 1, . . . , t, if
j > 0, we have

                                     E[Li |   Cj] = j-1.

Summing over all j = 1, . . . , t with j > 0, it follows that

               E[Li] = E[Li |   Cj] · P[  Cj]

               j >0

               = -1 j · j = -1 j · j-1t-1  -1,

               j >0                         j >0

which proves the lemma. 2

   So in stage 1, the expected number of attempts made in generating a single rela-
tion is -1, each such attempt takes time k·len(n)O(1), and we have to generate k+2
relations, leading to a total expected running time in stage 1 of -1k2 · len(n)O(1).
Stage 2 is dominated by the cost of performing Gaussian elimination, which takes
time k3 · len(n)O(1). Thus, if Z is the total running time of the algorithm, we have

                             E[Z]  (-1k2 + k3) · len(n)O(1).

   By our assumption that n is not divisible by any primes up to y, all y-smooth

integers up to n - 1 are in fact relatively prime to n. Therefore, the number of
y-smooth elements of Zn is equal to (y, n - 1), and since n itself is not y-smooth,
this is equal to (y, n). From this, it follows that

                = (y, n)/|Zn|  (y, n)/n.

The rest of the running time analysis is essentially the same as in the analysis
of Algorithm SEDL; that is, assuming y = exp[(log n)+o(1)] for some constant

0 <  < 1, we obtain

E[Z]  exp[(1 + o(1)) max{(log n/ log y) log log n + 2 log y, 3 log y}].  (15.8)

Setting  y  =                  n  log  log  n)1/2],  we  obtain
               exp[(1/ 2)(log

               E[Z]  exp[(22 + o(1))(log n log log n)1/2].

   That basically takes care of the first two questions. As for the third, we have:
Lemma 15.5. Algorithm SEF outputs "failure" with probability 2-w+1  1/2.

Proof. Let F be the event that the algorithm outputs "failure." We may view

the final values assigned to  and 1, . . . , k+2 as random variables, which we
shall denote by these same names. Let   Zn and 1, . . . , k+2  (Zn)2 be
                         15.3 An algorithm for factoring integers                         413

arbitrary, fixed values such that rep(i ) is y-smooth for i = 1, . . . , k + 2. Let
B be the event that  =  and i2 = i for i = 1, . . . , k + 2. We shall show
that P[F | B] = 2-w+1, and since this holds for all relevant B, it follows by total
probability that P[F ] = 2-w+1.

For the rest of the argument, we focus on the conditional distribution given

B. With respect to this conditional distribution, the distribution of each random

variable i is (essentially) the uniform distribution on -1({i}), where  is the
                                                                                 k+2
squaring  map  on  Zn .  Moreover,  the  family  of  random    variables              is  mutually
                                                                          {i}i=1

independent. Also, the values  and c1, . . . , ck+2 computed by the algorithm are

fixed. It follows (see Exercise 8.14) that the distribution of  is (essentially) the

uniform distribution on -1({2}), and hence  := / is a random square root of
1 in Zn. Thus,  = ±1 with probability 2-w+1. 2

Let us summarize the above discussion in the following theorem.

Theorem 15.6. With the smoothness parameter set as

                         y  :=                   n   log  log  n)1/2],
                                exp[(1/ 2)(log

the expected running time of Algorithm SEF is at most
                           exp[(22 + o(1))(log n log log n)1/2].

The probability that Algorithm SEF outputs "failure" is at most 1/2.

EXERCISE 15.5. It is perhaps a bit depressing that after all that work, Algo-
rithm SEF only succeeds (in the worst case) with probability 1/2. Of course, to
reduce the failure probability, we can simply repeat the entire computation -- with

  repetitions, the failure probability drops to 2- . However, there is a better way to
reduce the failure probability. Suppose that in stage 1, instead of collecting k + 2
relations, we collect k + 1 + relations, where  1 is an integer parameter.

    (a) Show that in stage 2, we can use Gaussian elimination over Z2 to find inte-
         ger vectors

                     c(j) = (c1(j), . . . , ck+1+ (j) )  {0, 1}×(k+1+ ) (j = 1, . . . , )

         such that
               - over the field Z2, the images of the vectors c(1), . . . , c( ) in Z2×(k+1+ )
                  form a linearly independent family of vectors, and
               - for j = 1, . . . , , we have

                                   c1(j)v1 + · · · + ck+1+ (j) vk+1+  2Z×(k+2).
414  Subexponential-time discrete logarithms and factoring

(b) Show that given vectors c(1), . . . , c( ) as in part (a), if for j = 1, . . . , , we
     set

                  (e1(j), . . . , ek+1 (j) )  c1(j)v1 + · · · + ck+1+ (j) vk+1+ ,

     (j)  ci (j) k+1+ (j) e1(j)/2 ek(j)/2 -e i ,   1 · · · k  k+1 (j) /2,  (j)  (j)/(j),

                   i=1

         then the family of random variables (1), . . . , ( ) is mutually independent,
         with each (j) uniformly distributed over the set of all square roots of 1 in
         Zn, and hence at least one of gcd(rep((j) - 1), n) splits n with probability
         at least 1 - 2- .

So, for example, if we set = 20, then the failure probability is reduced to less

than one in a million, while the increase in running time over Algorithm SEF will

hardly be noticeable.

                               15.4 Practical improvements

Our presentation and analysis of algorithms for discrete logarithms and factoring
were geared towards simplicity and mathematical rigor. However, if one really
wants to compute discrete logarithms or factor numbers, then a number of impor-
tant practical improvements should be considered. In this section, we briefly sketch
some of these improvements, focusing our attention on algorithms for factoring
numbers (although some of the techniques apply to discrete logarithms as well).

                       15.4.1 Better smoothness density estimates

From an algorithmic point of view, the simplest way to improve the running times
of both Algorithms SEDL and SEF is to use a more accurate smoothness density
estimate, which dictates a different choice of the smoothness bound y in those
algorithms, speeding them up significantly. While our Theorem 15.1 is a valid
lower bound on the density of smooth numbers, it is not "tight," in the sense that
the actual density of smooth numbers is somewhat higher. We quote from the
literature the following result:

Theorem 15.7. Let y be a function of x such that for some  > 0, we have

              y = ((log x)1+) and u := log x  
                                                 log y

as x  . Then

              (y, x) = x · exp[(-1 + o(1))u log u].
                                15.4 Practical improvements                                           415

Proof. See §15.5. 2

Let us apply this result to the analysis of Algorithm SEF. Assume that
                                y = exp[(log n)1/2+o(1)].

Our choice of y will in fact be of this form. With this assumption, we have
log log y = (1/2 + o(1)) log log n, and using Theorem 15.7, we can improve the

inequality (15.8), obtaining instead (as the reader may verify)

E[Z]         exp[(1   +  o(1))  max{  1  (log  n/  log  y)  log  log  n  +  2  log  y,  3  log  y}].
                                      2

From this, if we set

                         y := exp[ 12 (log n log log n)1/2)],

we obtain

                      E[Z]  exp[(2 + o(1))(log n log log n)1/2].

An analogous improvement can be obtained for Algorithm SEDL.
   Although this improvement only reduces the constant 2 2  2.828 to 2, the

constant is in the exponent, and so this improvement is not to be scoffed at!

                          15.4.2 The quadratic sieve algorithm

We now describe a practical improvement to Algorithm SEF. This algorithm,
known as the quadratic sieve, is faster in practice than Algorithm SEF; however,
its analysis is somewhat heuristic.

   First, let us return to the simplified version of Algorithm SEF, where we collect
relations of the form (15.6). Furthermore, instead of choosing the values i at
random, we will choose them in a special way, as we now describe. Let

                                           n~ := n ,

and define the polynomial

                              F := (X + n~)2 - n  Z[X ].

In addition to the usual "smoothness parameter" y, we need a "sieving parameter"
z, whose choice will be discussed below. We shall assume that both y and z are
of the form exp[(log n)1/2+o(1)], and our ultimate choices of y and z will indeed
satisfy this assumption.

   For all s = 1, 2, . . . , z , we shall determine which values of s are "good," in
the sense that the corresponding value F (s) is y-smooth. For each good s, since
we have F (s)  (s + n~)2 (mod n), we obtain one relation of the form (15.6),
with i := [s + n~]n. If we find at least k + 1 good values of s, then we can apply
416  Subexponential-time discrete logarithms and factoring

Gaussian elimination as usual to find a square root  of 1 in Zn. Hopefully, we will
have  = ±1, allowing us to split n.

   Observe that for 1  s  z, we have
                           1  F (s)  z2 + 2zn1/2  n1/2+o(1).

Now, although the values F (s) are not at all random, we might expect heuristically
that the number of good s up to z is roughly equal to ^ z, where ^ is the probability
that a random integer in the interval [1, n1/2] is y-smooth, and by Theorem 15.7,
we have

                      ^ = exp[(- 41 + o(1))(log n/ log y) log log n].

If our heuristics are valid, this already yields an improvement over Algorithm SEF,
since now we are looking for y-smooth numbers near n1/2, which are much more
common than y-smooth numbers near n. But there is another improvement possi-
ble; namely, instead of testing each individual number F (s) for smoothness using
trial division, we can test them all at once using the following "sieving procedure."

   The sieving procedure works as follows. First, we create an array v[1 . . . z ],
and initialize v[s] to F (s), for 1  s  z. Then, for each prime p up to y, we do
the following:

     1. Compute the roots of the polynomial F modulo p.
         This can be done quite efficiently, as follows. For p = 2, F has exactly
         one root modulo p, which is determined by the parity of n~. For p > 2,
         we may use the familiar quadratic formula together with an algorithm for
         computing square roots modulo p, as discussed in Exercise 12.7. A quick
         calculation shows that the discriminant of F is 4n, and thus, F has a root
         modulo p if and only if n is a quadratic residue modulo p, in which case it
         will have two roots (under our usual assumptions, we cannot have p | n).

     2. Assume that F has vp distinct roots modulo p lying in the interval [1, p];
         call them r1, . . . , rvp.
         Note that vp = 1 for p = 2 and vp  {0, 2} for p > 2. Also note that
         F (s)  0 (mod p) if and only if s  ri (mod p) for some i = 1, . . . , vp.
         For i = 1, . . . , vp, do the following:

     s  ri
     while s  z do

           repeat v[s]  v[s]/p until p v[s]
           ss+p

At the end of this sieving procedure, the good values of s may be identified as
                           15.4 Practical improvements                                         417

precisely those such that v[s] = 1. The running time of this sieving procedure is at
most len(n)O(1) times

                       z = z 1 = O(z log log y) = z1+o(1).
           py p            py p

Here, we have made use of Theorem 5.10, although this is not really necessary --
for our purposes, the bound py 1/p = O(log y) would suffice. Note that this
sieving procedure is a factor of k1+o(1) faster than the method for finding smooth
numbers based on trial division. With just a little extra book-keeping, we can not
only identify the good values of s but also compute the factorization of F (s) into
primes, at essentially no extra cost.

   Now, let us put together all the pieces. We have to choose z just large enough
so as to find at least k + 1 good values of s up to z. So we should choose z so

that z  k/^ -- in practice, we could choose an initial estimate for z, and if this

choice of z does not yield enough relations, we could keep doubling z until we do
get enough relations. Assuming that z  k/^ , the cost of sieving is (k/^ )1+o(1), or

        exp[(1         +  o(1)  )  (  1  (log  n/  log  y  )  log  log  n  +  log  y  )].
                                      4

The cost of Gaussian elimination is still O(k3), or

                                exp[(3 + o(1)) log y].

Thus, the total running time is bounded by

exp[(1  +  o  (  1  )  )  max{  1  (  log  n/  log  y   )  log  log  n  +  log y,  3 log y}].
                                4

Let µ := log y, A := (1/4) log n log log n, S1 := A/µ + µ and S2 := 3µ, and let us

find the value of µ that minimizes max{S1, S2}. Using a little calculus, one finds
that S1 is minimized at µ = A1/2. For this value of µ, we have S1 = 2A1/2 and
S2 = 3A1/2 > S1, and so this choice of µ is a bit larger than optimal. For µ < A1/2,

S1 is decreasing (as a function of µ), while S2 is always increasing. It follows that

the optimal value of µ is obtained by setting

                                         A/µ + µ = 3µ,

and solving for µ. This yields µ = (A/2)1/2. So setting

                 y     :=                                  n  log  log  n)1/2],
                           exp[(1/2 2)(log

the total running time of the quadratic sieve factoring algorithm is bounded by
                         exp[(3/22 + o(1))(log n log log n)1/2].

Thus, we have reduced the constant in the exponent from 2(for Algorithm SEF
with the more accurate smoothness density estimates) to 3/2 2  1.061.
418      Subexponential-time discrete logarithms and factoring

   We mention one final improvement. The matrix to which we apply Gaussian
elimination in stage 2 is "sparse"; indeed, since any integer less than n has O(log n)
prime factors, the total number of non-zero entries in the matrix is k1+o(1). There
are special algorithms for working with such sparse matrices, which allow us to
perform stage 2 of the factoring algorithm in time k2+o(1), or

                                    exp[(2 + o(1)) log y].

Setting

         y := exp[ 21 (log n log log n)1/2],

the total running time is bounded by

                            exp[(1 + o(1))(log n log log n)1/2].
                                                                                     

Thus, this improvement reduces the constant in the exponent from 3/2 2  1.061
to 1. Moreover, the special algorithms designed to work with sparse matrices typ-
ically use much less space than ordinary Gaussian elimination (even if the input
to Gaussian elimination is sparse, the intermediate matrices will not be). We shall
discuss in detail later, in §18.4, one such algorithm for solving sparse systems of
linear equations.

   The quadratic sieve may fail to factor n, for one of two reasons: first, it may
fail to find k + 1 relations; second, it may find these relations, but in stage 2, it
finds only a trivial square root of 1. There is no rigorous theory to say why the
algorithm should not fail for one of these two reasons, but experience shows that
the algorithm does indeed work as expected.

                                          15.5 Notes

Many of the algorithmic ideas in this chapter were first developed for the problem
of factoring integers, and then later adapted to the discrete logarithm problem.
The first (heuristic) subexponential-time algorithm for factoring integers, called
the continued fraction method (not discussed here), was introduced by Lehmer
and Powers [59], and later refined and implemented by Morrison and Brillhart
[70]. The first rigorously analyzed subexponential-time algorithm for factoring
integers was introduced by Dixon [35]. Algorithm SEF is a variation of Dixon's
algorithm, which works the same way as Algorithm SEF, except that it generates
relations of the form (15.6) directly (and indeed, it is possible to prove a variant
of Theorem 15.1, and for that matter, Theorem 15.7, for random squares modulo
n). Algorithm SEF is based on an idea suggested by Rackoff (personal communi-
cation).

   Theorem 15.7 was proved by Canfield, Erds, and Pomerance [23].
                                       15.5 Notes                                       419

   The quadratic sieve was introduced by Pomerance [78]. Recall that the quadratic
sieve has a heuristic running time of

                            exp[(1 + o(1))(log n log log n)1/2].

This running time bound can also be achieved rigorously by a result of Lenstra

and Pomerance [61], and to date, this is the best rigorous running time bound for

factoring algorithms. We should stress, however, that most practitioners in this

field are not so much interested in rigorous running time analyses as they are in

actually factoring integers, and, for such purposes, heuristic running time estimates

are quite acceptable. Indeed, the quadratic sieve is much more practical than the

algorithm in [61], which is mainly of theoretical interest.

There are two other factoring algorithms not discussed here, but that should

anyway at least be mentioned. The first is the elliptic curve method, introduced

by Lenstra [60]. Unlike all of the other known subexponential-time algorithms, the

running time of this algorithm is sensitive to the sizes of the factors of n; in partic-

ular, if p is the smallest prime dividing n, the algorithm will find p (heuristically)

in expected time

                           +  o(1)  )  (log  p  log  log  p)1/2  ]  ·  len(n)O  (1)  .
                  exp[( 2

This algorithm is quite practical, and is the method of choice when it is known
(or suspected) that n has some small factors. It also has the advantage that it uses
only polynomial space (unlike all of the other known subexponential-time factoring
algorithms).

   The second is the number field sieve, the basic idea of which was introduced by
Pollard [77], and later generalized and refined by Buhler, Lenstra, and Pomerance
[21], as well as by others. The number field sieve will split n (heuristically) in
expected time

                          exp[(c + o(1))(log n)1/3(log log n)2/3],

where c is a constant (currently, the smallest value of c is 1.902, a result due to
Coppersmith [27]). The number field sieve is currently the asymptotically fastest
known factoring algorithm (at least, heuristically), and it is also practical, having
been used to set the latest factoring record -- the factorization of a 200-decimal-
digit integer that is the product of two primes of about the same size. See the web
page www.crypto-world.com/FactorRecords.html for more details (as well
as for announcements of new records).

   As for subexponential-time algorithms for discrete logarithms, Adleman [1]
adapted the ideas used for factoring to the discrete logarithm problem, although
it seems that some of the basic ideas were known much earlier. Algorithm SEDL
is a variation on this algorithm, and the basic technique is usually referred to as the
420  Subexponential-time discrete logarithms and factoring

index calculus method. The basic idea of the number field sieve was adapted to the
discrete logarithm problem by Gordon [42]; see also Adleman [2] and Schirokauer,
Weber, and Denny [84].

   For many more details and references for subexponential-time algorithms for
factoring and discrete logarithms, see Chapter 6 of Crandall and Pomerance [30].
Also, see the web page www.crypto-world.com/FactorWorld.html for links
to research papers and implementation reports.

   For more details regarding the security of signature schemes, as discussed fol-
lowing Exercise 15.4, see the paper by Bellare and Rogaway [13].

   Last, but not least, we should mention the fact that there are in fact polynomial-
time algorithms for factoring and for computing discrete logarithms; however,
these algorithms require special hardware, namely, a quantum computer. Shor
[92, 93] showed that these problems could be solved in polynomial time on such a
device; however, at the present time, it is unclear when and if such machines will
ever be built. Much, indeed most, of modern-day cryptography will crumble if this
happens, or if efficient "classical" algorithms for these problems are discovered
(which is still a real possibility).
                            16

                           More rings

This chapter develops a number of more advanced concepts concerning rings.
These concepts will play important roles later in the text, and we prefer to dis-
cuss them now, so as to avoid too many interruptions of the flow of subsequent
discussions.

                                        16.1 Algebras
Throughout this section, R denotes a ring (i.e., a commutative ring with unity).

   Sometimes, a ring may also be naturally viewed as an R-module, in which case,
both the theory of rings and the theory of modules may be brought to bear to study
its properties.
Definition 16.1. An R-algebra is a set E, together with addition and multiplica-
tion operations on E, and a function µ : R × E  E, such that

    (i) with respect to addition and multiplication, E forms a ring;
   (ii) with respect to addition and the scalar multiplication map µ, E forms an

         R-module;
   (iii) for all c  R, and ,   E, we have

                                   µ(c, ) = µ(c, ) = µ(c, ).
   An R-algebra E may also be called an algebra over R. As we usually do for
R-modules, we shall write c (or c · ) instead of µ(c, ). When we do this, part
(iii) of the definition states that

                                   (c) = c() = (c)
for all c  R and ,   E. In particular, we may write c without any ambiguity.
Note that there are two multiplication operations at play here: scalar multiplication

                                               421
422  More rings

(such as c), and ring multiplication (such as ). Also note that since we are

assuming E is commutative, the second equality in part (iii) is already implied

by the first. A simple consequence of the definition is that for all c, d  R and
,   E, we have (c)(d) = (cd)(). From this, it follows that for all c  R,
  E, and k  0, we have (c)k = ckk.

Example 16.1. Suppose E is a ring and  : R  E is a ring homomorphism. With
scalar multiplication defined by c := (c) for c  R and   E, one may easily

check that E is indeed an R-algebra. In this case, we say that E is an R-algebra
via the map . 2

Example 16.2. If R is a subring of E, then with  : R  E being the inclusion
map, we can view E as an R-algebra as in the previous example. In this case, we
say that E is an R-algebra via inclusion. 2

Example 16.3. If  : R  E is a natural embedding of rings, then by a slight
abuse of terminology, just as we sometimes say that R is a subring of E, we shall
also say that E is an R-algebra via inclusion. 2

In fact, all R-algebras can be viewed as special cases of Example 16.1:

Theorem 16.2. If E is an R-algebra, then the map

                                       : RE
                                              c  c · 1E,

is a ring homomorphism, and c = (c) for all c  R and   E.

Proof. Exercise. 2

   In the special situation where R is a field, we can say even more. In this situation,
and with  as in the above theorem, then either E is trivial or  is injective (see
Exercise 7.47). In the latter case, E contains an isomorphic copy of R as a subring.
To summarize:

Theorem 16.3. If R is a field, then an R-algebra is either the trivial ring or con-
tains an isomorphic copy of R as a subring.

The following examples give further important constructions of R-algebras.

Example 16.4. If E1, . . . , Ek are R-algebras, then their direct product E1 ×· · ·×Ek

is an R-algebra as well, where addition, multiplication, and scalar multiplication
are defined component-wise. As usual, if E = E1 = · · · = Ek, we write this as
E×k. 2
16.1 Algebras  423

Example 16.5. If I is an arbitrary set, and E is an R-algebra, then Map(I, E),
which is the set of all functions f : I  E, may be naturally viewed as an
R-algebra, with addition, multiplication, and scalar multiplication defined point-
wise. 2

Example 16.6. Let E be an R-algebra and let I be an ideal of E. Then it is easily
verified that I is also a submodule of E. This means that the quotient ring E/I
may also be viewed as an R-module, and indeed, it is an R-algebra, called the
quotient algebra (over R) of E modulo I. For ,   E and c  R, addition,
multiplication, and scalar multiplication in E are defined as follows:

      []I + []I := [ + ]I , []I · []I := [ · ]I , c · []I := [c · ]I . 2

Example 16.7. The ring of polynomials R[X ] is an R-algebra via inclusion. Let
f  R[X ] be a non-zero polynomial with lc(f )  R. We may form the quotient
ring E := R[X ]/(f ), which may naturally be viewed as an R-algebra, as in the
previous example. If deg(f ) = 0, then E is trivial; so assume deg(f ) > 0, and
consider the map

                                        : RE

                                               c  c · 1E

from Theorem 16.2. By definition, (c) = [c]f . As discussed in Example 7.55, the
map  is a natural embedding of rings, and so by identifying R with its image in
E under , we can view R as a subring of E; therefore, we can also view E as an
R-algebra via inclusion. 2

                                          Subalgebras
Let E be an R-algebra. A subset S of E is called a subalgebra (over R) of E if it
is both a subring of E and a submodule of E. This means that S contains 1E, and
is closed under addition, multiplication, and scalar multiplication; restricting these
operations to S, we may view S as an R-algebra in its own right.

   The following theorem gives a simple but useful characterization of subalgebras,
in relation to subrings:

Theorem 16.4. If E is an R-algebra via inclusion, and S is a subring of E, then
S is a subalgebra if and only if S contains R. More generally, if E is an arbitrary
R-algebra, and S is a subring of E, then S is a subalgebra of E if and only if S
contains c · 1E for all c  R.

Proof. Exercise. 2
424                 More rings

                                R-algebra homomorphisms

Let E and E be R-algebras. A function  : E  E is called an R-algebra
homomorphism if  is both a ring homomorphism and an R-linear map. This
means that (1E ) = 1E , and

           ( + ) = () + (), () = ()(), and (c) = c()

for all ,   E and all c  R. As usual, if  is bijective, then it is called an
R-algebra isomorphism, and if, in addition, E = E , it is called an R-algebra
automorphism.

   The following theorem gives a simple but useful characterization of R-algebra
homomorphisms, in relation to ring homomorphisms:

Theorem 16.5. If E and E are R-algebras via inclusion, and  : E  E is
a ring homomorphism, then  is an R-algebra homomorphism if and only if the
restriction of  to R is the identity map. More generally, if E and E are arbitrary
R-algebras and  : E  E is a ring homomorphism, then  is an R-algebra
homomorphism if and only if (c · 1E ) = c · 1E for all c  R.

Proof. Exercise. 2

Example 16.8. If E is an R-algebra and I is an ideal of E, then as observed in
Example 16.6, I is also a submodule of E, and we may form the quotient algebra
E/I. The natural map

                                         : E  E/I
                                                 []I

is both a ring homomorphism and an R-linear map, and hence is an R-algebra
homomorphism. 2

Example 16.9. Since C contains R as a subring, we may naturally view C as an
R-algebra via inclusion. The complex conjugation map on C that sends a + bi to
a - bi, for a, b  R, is an R-algebra automorphism on C (see Example 7.5). 2

   Many simple facts about R-algebra homomorphisms can be obtained by com-
bining corresponding facts for ring and R-module homomorphisms. For example,
the composition of two R-algebra homomorphisms is again an R-algebra homo-
morphism, since the composition is both a ring homomorphism and an R-linear
map (Theorems 7.22 and 13.6). As another example, if  : E  E is an R-
algebra homomorphism, then its image S is both a subring and a submodule of
E , and hence, S is a subalgebra of E . The kernel K of  is an ideal of E, and
we may form the quotient algebra E/K. The first isomorphism theorems for rings
and modules (Theorems 7.26 and 13.9) tell us that E/K and S are isomorphic
16.1 Algebras                                              425

both as rings and as R-modules, and hence, they are isomorphic as R-algebras.
Specifically, the map

                                       : E/K  E
                                             []K  ()

is an injective R-algebra homomorphism whose image is S .

   The following theorem isolates an important subalgebra associated with any R-
algebra homomorphism  : E  E.

Theorem 16.6. Let E be an R-algebra, and let  : E  E be an R-algebra
homomorphism. Then the set S := {  E : () = } is a subalgebra of E,
called the subalgebra of E fixed by . Moreover, if E is a field, then so is S.

Proof. Let us verify that S is closed under addition. If ,   S, then we have

             ( + ) = () + () (since  is a group homomorphism)
                        =  +  (since ,   S).

   Using the fact that  is a ring homomorphism, one can similarly show that S is
closed under multiplication, and that 1E  S. Likewise, using the fact that  is an
R-linear map, one can also show that S is closed under scalar multiplication.

   This shows that S is a subalgebra, proving the first statement. For the second
statement, suppose that E is a field. Let  be a non-zero element of S, and suppose
  E is its multiplicative inverse, so that  = 1E. We want to show that  lies in
S. Again, using the fact that  is a ring homomorphism, we have

                     = 1E = (1E ) = () = ()() = (),

and hence  = (); canceling , we obtain  = (), and so   S. 2

Example 16.10. The subalgebra of C fixed by the complex conjugation map is
R. 2

                                   Polynomial evaluation

Let E be an R-algebra. Consider the ring of polynomials R[X ] (which is an R-
algebra via inclusion). Any polynomial g  R[X ] naturally defines a function on
E: if g = i aiX i, with each ai  R, and   E, then

                                       g() := aii.

                                                                      i

Just as for rings, we say that  is a root of g if g() = 0E.
426              More rings

For fixed   E, the polynomial evaluation map

                  : R[X ]  E
                           g  g()

is easily seen to be an R-algebra homomorphism. The image of  is denoted R[],
and is a subalgebra of E. Indeed, R[] is the smallest subalgebra of E contain-
ing , and is called the subalgebra (over R) generated by . Note that if E is
an R-algebra via inclusion, then the notation R[] has the same meaning as that
introduced in Example 7.44.

   We next state a very simple, but extremely useful, fact:

Theorem 16.7. Let  : E  E be an R-algebra homomorphism. Then for all
g  R[X ] and   E, we have

                 (g()) = g(()).

Proof. Let g = i aiX i  R[X ]. Then we have

     (g()) =  aii = (aii) = ai(i) = ai()i

     i           i                           i  i

     = g(()). 2

   As a special case of Theorem 16.7, if E = R[] for some   E, then every
element of E can be expressed as g() for some g  R[X ], and (g()) = g(());

hence, the action of  is completely determined by its action on .

Example 16.11. Let f  R[X ] be a non-zero polynomial with lc(f )  R. As in
Example 16.7, we may form the quotient algebra E := R[X ]/(f ).

  Let  := [X ]f  E. Then E = R[], and moreover, every element of E can be
expressed uniquely as g(), where g  R[X ] and deg(g) < deg(f ). In addition, 

is a root of f. If deg(f ) > 0, these facts were already observed in Example 7.55,

and otherwise, they are trivial.

   Now let E be any R-algebra, and suppose that  : E  E is an R-algebra
homomorphism, and let  := (). By the previous theorem,  sends g()

to g( ), for each g  R[X ]. Thus, the image of  is R[ ]. Also, we have
f ( ) = f (()) = (f ()) = (0E ) = 0E . Therefore,  must be a root of f .

   Conversely, suppose that   E is a root of f. Then the polynomial evalu-

ation map from R[X ] to E that sends g  R[X ] to g( )  E is an R-algebra

homomorphism whose kernel contains f. Using the generalized versions of the

first isomorphism theorems for rings and R-modules (Theorems 7.27 and 13.10),
16.2 The field of fractions of an integral domain  427

we obtain the R-algebra homomorphism

: EE
     g()  g( ).

One sees that complex conjugation is just a special case of this construction (see
Example 7.57). 2

EXERCISE 16.1. Let E be an R-algebra. For   E, consider the -multiplication
map on E, which sends   E to   E. Show that this map is an R-linear map.

EXERCISE 16.2. Show that every ring may be viewed in a unique way as a Z-
algebra, and that subrings are subalgebras, and ring homomorphisms are Z-algebra
homomorphisms.

EXERCISE 16.3. Show that the only R-algebra homomorphisms from C into itself
are the identity map and the complex conjugation map.

                  16.2 The field of fractions of an integral domain

Let D be an integral domain. Just as we can construct the field of rational numbers
by forming fractions involving integers, we can construct a field consisting of frac-
tions whose numerators and denominators are elements of D. This construction is
quite straightforward, though a bit tedious.

   To begin with, let S be the set of all pairs of the form (a, b), with a, b  D
and b = 0D. Intuitively, such a pair (a, b) is a "formal fraction," with numer-
ator a and denominator b. We define a binary relation  on S as follows: for
(a1, b1), (a2, b2)  S, we say (a1, b1)  (a2, b2) if and only if a1b2 = a2b1. Our
first task is to show that this is an equivalence relation:

Lemma 16.8. For all (a1, b1), (a2, b2), (a3, b3)  S, we have

    (i) (a1, b1)  (a1, b1);

   (ii) (a1, b1)  (a2, b2) implies (a2, b2)  (a1, b1);

   (iii) (a1, b1)  (a2, b2) and (a2, b2)  (a3, b3) implies (a1, b1)  (a3, b3).

Proof. (i) and (ii) are rather trivial, and we do not comment on these any further. As
for (iii), assume that a1b2 = a2b1 and a2b3 = a3b2. Multiplying the first equation
by b3, we obtain a1b2b3 = a2b1b3 and substituting a3b2 for a2b3 on the right-hand
side of this last equation, we obtain a1b2b3 = a3b2b1. Now, using the fact that b2
is non-zero and that D is an integral domain, we may cancel b2 from both sides,
obtaining a1b3 = a3b1. 2
428                 More rings

   Since  is an equivalence relation, it partitions S into equivalence classes, and

for (a, b)  S, we denote by [a, b] the equivalence class containing (a, b), and

we denote by K the set of all such equivalence classes. Our next task is to define

addition and multiplication operations on equivalence classes, mimicking the usual

rules of arithmetic with fractions. We want to define the sum of [a1, b1] and [a2, b2]
to be [a1b2 + a2b1, b1b2], and the product of [a1, b1] and [a2, b2] to be [a1a2, b1b2].
Note that since D is an integral domain, if b1 and b2 are non-zero, then so is the
product b1b2, and therefore [a1b2 + a2b1, b1b2] and [a1a2, b1b2] are indeed equiv-
alence classes. However, to ensure that this definition is unambiguous, and does

not depend on the particular choice of representatives of the equivalence classes

[a1, b1] and [a2, b2], we need the following lemma.

Lemma 16.9. Let (a1, b1), (a1, b1), (a2, b2), (a2, b2)  S, where (a1, b1)  (a1, b1)
and (a2, b2)  (a2, b2). Then we have

                        (a1b2 + a2b1, b1b2)  (a1b2 + a2b1, b1b2)

and

                                 (a1a2, b1b2)  (a1a2, b1b2).
Proof. This is a straightforward calculation. Since a1b1 = a1b1 and a2b2 = a2b2,
we have

          (a1b2 + a2b1)b1b2 = a1b2b1b2 + a2b1b1b2 = a1b2b1b2 + a2b1b1b2
                                = (a1b2 + a2b1)b1b2

and
                           a1a2b1b2 = a1a2b1b2 = a1a2b1b2. 2

   In light of this lemma, we may unambiguously define addition and multiplication
on K as follows: for [a1, b1], [a2, b2]  K, we define

                         [a1, b1] + [a2, b2] := [a1b2 + a2b1, b1b2]

and
                              [a1, b1] · [a2, b2] := [a1a2, b1b2].

   The next task is to show that K is a ring -- we leave the details of this (which
are quite straightforward) to the reader.

Lemma 16.10. With addition and multiplication as defined above, K is a ring,
with additive identity [0D, 1D] and multiplicative identity [1D, 1D].

Proof. Exercise. 2
       16.2 The field of fractions of an integral domain  429

   Finally, we observe that K is in fact a field: it is clear that [a, b] is a non-zero
element of K if and only if a = 0D, and hence any non-zero element [a, b] of K

has a multiplicative inverse, namely, [b, a].

   The field K is called the field of fractions of D. Consider the map  : D  K
that sends a  D to [a, 1D]  K. It is easy to see that this map is a ring homomor-
phism, and one can also easily verify that it is injective. So, starting from D, we
can synthesize "out of thin air" its field of fractions K, which essentially contains
D as a subring, via the natural embedding  : D  K.

   Now suppose that we are given a field L that contains D as a subring. Consider
the set K consisting of all elements of L of the form ab-1, where a, b  D and
b = 0D -- note that here, the arithmetic operations are performed using the rules
for arithmetic in L. One may easily verify that K is a subfield of L that contains
D, and it is easy to see that this is the smallest subfield of L that contains D. The
subfield K of L may be referred to as the field of fractions of D within L. One
may easily verify that the map  : K  L that sends [a, b]  K to ab-1  L is an
unambiguously defined ring homomorphism that maps K injectively onto K . If
we view K and L as D-algebras via inclusion, and we see that the map  is in fact
a D-algebra homomorphism. Thus, K and K are isomorphic as D-algebras. It is
in this sense that the field of fractions K is the smallest field that contains D as a
subring.

From now on, we shall simply write an element [a, b] of K as the fraction a/b.

In this notation, the above rules for addition, multiplication, and testing equality in

K now look quite familiar:

a1 + a2 = a1b2 + a2b1 ,     a1 · a2 = a1a2 ,  a1 = a2  a1b2 = a2b1.
b1 b2  b1b2                 b1 b2 b1b2        b1 b2

                                        Function fields

An important special case of the above construction for the field of fractions of D
is when D = F [X ], where F is a field. In this case, the field of fractions is denoted
F (X ), and is called the field of rational functions (over F ). This terminology is
a bit unfortunate, since just as with polynomials, although the elements of F (X )
define functions, they are not (in general) in one-to-one correspondence with these
functions.

  Since F [X ] is a subring of F (X ), and since F is a subring of F [X ], we see that
F is a subfield of F (X ).

  More generally, we may apply the above construction to D = F [X1, . . . , Xn],
the ring of multi-variate polynomials over the field F , in which case the field of
430  More rings

fractions is denoted F (X1, . . . , Xn), and is also called the field of rational functions
(over F , in the variables X1, . . . , Xn).

EXERCISE 16.4. Let F be a field of characteristic zero. Show that F contains an
isomorphic copy of Q.

EXERCISE 16.5. Show that the field of fractions of Z[i] within C is Q[i]. (See
Example 7.25 and Exercise 7.14.)

                       16.3 Unique factorization of polynomials

Throughout this section, F denotes a field.
  Like the ring Z, the ring F [X ] of polynomials is an integral domain, and because

of the division with remainder property for polynomials, F [X ] has many other
properties in common with Z. Indeed, essentially all the ideas and results from
Chapter 1 can be carried over almost verbatim from Z to F [X ], and in this section,
we shall do just that.

  Recall that the units of F [X ] are precisely the units F  of F , that is, the non-
zero constants. We call two polynomials g, h  F [X ] associate if g = ch for some
c  F . It is easy to see that g and h are associate if and only if g | h and h | g --
indeed, this follows as a special case of part (i) of Theorem 7.4. Clearly, any non-
zero polynomial g is associate to a unique monic polynomial (i.e., a polynomial
with leading coefficient 1), called the monic associate of g; indeed, the monic
associate of g is lc(g)-1 · g (where, as usual, lc(g) denotes the leading coefficient
of g).

  We call a polynomial f  F [X ] irreducible if it is non-constant and all divisors
of f are associate to 1 or f . Conversely, we call f reducible if it is non-constant
and is not irreducible. Equivalently, a non-constant polynomial f is reducible if
and only if there exist polynomials g, h  F [X ] of degree strictly less than that of
f such that f = gh.

   Clearly, if g and h are associate polynomials, then g is irreducible if and only if
h is irreducible.

   The irreducible polynomials play a role similar to that of the prime numbers. Just
as it is convenient to work with only positive prime numbers, it is also convenient
to restrict attention to monic irreducible polynomials.

   Corresponding to Theorem 1.3, every non-zero polynomial can be expressed as
a unit times a product of monic irreducibles in an essentially unique way:

Theorem 16.11. Every non-zero polynomial f  F [X ] can be expressed as

                                      f = c · p1e1 · · · prer ,
16.3 Unique factorization of polynomials  431

where c  F , p1, . . . , pr are distinct monic irreducible polynomials, and e1, . . . , er
are positive integers. Moreover, this expression is unique, up to a reordering of the
irreducible polynomials.

   To prove this theorem, we may assume that f is monic, since the non-monic
case trivially reduces to the monic case.

   The proof of the existence part of Theorem 16.11 is just as for Theorem 1.3. If
f is 1 or a monic irreducible, we are done. Otherwise, there exist g, h  F [X ] of
degree strictly less than that of f such that f = gh, and again, we may assume that
g and h are monic. By induction on degree, both g and h can be expressed as a
product of monic irreducible polynomials, and hence, so can f .

   The proof of the uniqueness part of Theorem 16.11 is almost identical to that of
Theorem 1.3. The key to the proof is the division with remainder property, Theo-
rem 7.10, from which we can easily derive the following analog of Theorem 1.6:

Theorem 16.12. Let I be an ideal of F [X ]. Then there exists a unique polynomial
d  F [X ] such that I = dF [X ] and d is either zero or monic.

Proof. We first prove the existence part of the theorem. If I = {0}, then d = 0 does
the job, so let us assume that I = {0}. Since I contains non-zero polynomials, it
must contain monic polynomials, since if g is a non-zero polynomial in I, then its
monic associate lc(g)-1g is also in I. Let d be a monic polynomial of minimal
degree in I. We want to show that I = dF [X ].

  We first show that I  dF [X ]. To this end, let g be any element in I. It suf-
fices to show that d | g. Using Theorem 7.10, we may write g = dq + r, where
deg(r) < deg(d). Then by the closure properties of ideals, one sees that r = g - dq
is also an element of I, and by the minimality of the degree of d, we must have
r = 0. Thus, d | g.

  We next show that dF [X ]  I. This follows immediately from the fact that
d  I and the closure properties of ideals.

   That proves the existence part of the theorem. As for uniqueness, note that if
dF [X ] = eF [X ], we have d | e and e | d, from which it follows that d and e are
associate, and so if d and e are both either monic or zero, they must be equal. 2

  For g, h  F [X ], we call d  F [X ] a common divisor of g and h if d | g and
d | h; moreover, we call such a d a greatest common divisor of g and h if d is
monic or zero, and all other common divisors of g and h divide d. Analogous to
Theorem 1.7, we have:

Theorem 16.13. For all g, h  F [X ], there exists a unique greatest common divi-
sor d of g and h, and moreover, gF [X ] + hF [X ] = dF [X ].

Proof. We apply the previous theorem to the ideal I := gF [X ] + hF [X ]. Let
432  More rings

d  F [X ] with I = dF [X ], as in that theorem. Note that g, h, d  I and d is monic
or zero.

  It is clear that d is a common divisor of g and h. Moreover, there exist s, t  F [X ]
such that gs + ht = d. If d | g and d | h, then clearly d | (gs + ht), and hence
d | d.

   Finally, for uniqueness, if e is a greatest common divisor of g and h, then d | e
and e | d, and hence e is associate to d, and the requirement that e is monic or zero
implies that e = d. 2

  For g, h  F [X ], we denote by gcd(g, h) the greatest common divisor of g and
h. Note that as we have defined it, lc(g) gcd(g, 0) = g. Also note that when at least

one of g or h are non-zero, gcd(g, h) is the unique monic polynomial of maximal

degree that divides both g and h.

  An immediate consequence of Theorem 16.13 is that for all g, h  F [X ], there
exist s, t  F [X ] such that gs + ht = gcd(g, h), and that when at least one of g or h

are non-zero, gcd(g, h) is the unique monic polynomial of minimal degree that can
be expressed as gs + ht for some s, t  F [X ].

  We say that g, h  F [X ] are relatively prime if gcd(g, h) = 1, which is

the same as saying that the only common divisors of g and h are units. It is

immediate from Theorem 16.13 that g and h are relatively prime if and only if
gF [X ] + hF [X ] = F [X ], which holds if and only if there exist s, t  F [X ] such
that gs + ht = 1.

   Analogous to Theorem 1.9, we have:

Theorem 16.14. For f, g, h  F [X ] such that f | gh and gcd(f, g) = 1, we have
f | h.

Proof. Suppose that f | gh and gcd(f, g) = 1. Then since gcd(f , g) = 1, by
Theorem 16.13 we have fs + gt = 1 for some s, t  F [X ]. Multiplying this
equation by h, we obtain f hs + ght = h. Since f | f by definition, and f | gh by

hypothesis, it follows that f | h. 2

Analogous to Theorem 1.10, we have:

Theorem 16.15. Let p  F [X ] be irreducible, and let g, h  F [X ]. Then p | gh
implies that p | g or p | h.

Proof. Assume that p | gh. The only divisors of p are associate to 1 or p. Thus,

gcd(p, g) is either 1 or the monic associate of p. If p | g, we are done; otherwise, if
p g, we must have gcd(p, g) = 1, and by the previous theorem, we conclude that

p | h. 2
     16.3 Unique factorization of polynomials                       433

Now to prove the uniqueness part of Theorem 16.11. Suppose we have
                                  p1 · · · pr = q1 · · · qs,

where p1, . . . , pr and q1, . . . , qs are monic irreducible polynomials (with duplicates
allowed among the pi's and among the qj's). If r = 0, we must have s = 0 and
we are done. Otherwise, as p1 divides the right-hand side, by inductively applying
Theorem 16.15, one sees that p1 is equal to qj for some j. We can cancel these
terms and proceed inductively (on r).

   That completes the proof of Theorem 16.11.

Analogous to Theorem 1.11, we have:

Theorem 16.16. There are infinitely many monic irreducible polynomials in F [X ].

   If F is infinite, then this theorem is true simply because there are infinitely
many monic, linear polynomials; in any case, one can easily prove this theorem
by mimicking the proof of Theorem 1.11 (as the reader may verify).

   For a monic irreducible polynomial p, we may define the function p, mapping
non-zero polynomials to non-negative integers, as follows: for every polynomial
f = 0, if f = peg, where p g, then p(f ) := e. We may then write the factoriza-
tion of f into irreducibles as

     f = c pp(f),

                 p

where the product is over all monic irreducible polynomials p, with all but finitely

many of the terms in the product equal to 1.

   Just as for integers, we may extend the domain of definition of p to include 0,
defining p(0) := . For all polynomials g, h, we have

     p(g · h) = p(g) + p(h) for all p.                              (16.1)

From this, it follows that for all polynomials g, h, we have

     h | g  p(h)  p(g) for all p,                                   (16.2)

and

     p(gcd(g, h)) = min(p(g), p(h)) for all p.                      (16.3)

  For g, h  F [X ], a common multiple of g and h is a polynomial m such that
g | m and h | m; moreover, such an m is the least common multiple of g and h
if m is monic or zero, and m divides all common multiples of g and h. In light of
Theorem 16.11, it is clear that the least common multiple exists and is unique, and
we denote the least common multiple of g and h by lcm(a, b). Note that as we have
434  More rings

defined it, lcm(g, 0) = 0, and that when both g and h are non-zero, lcm(g, h) is
the unique monic polynomial of minimal degree that is divisible by both g and h.
Also, for all g, h  F [X ], we have

     p(lcm(g, h)) = max(p(g), p(h)) for all p.                    (16.4)

   Just as in §1.3, the notions of greatest common divisor and least common multi-

ple generalize naturally from two to any number of polynomials. We also say that
a family of polynomials {gi}ik=1 is pairwise relatively prime if gcd(gi, gj) = 1 for
all indices i, j with i = j.

  Also just as in §1.3, any rational function g/h  F (X ) can be expressed as a
fraction g0/h0 in lowest terms -- that is, g/h = g0/h0 and gcd(g0, h0) = 1 -- and
this representation is unique up to multiplication by units.

   Many of the exercises in Chapter 1 carry over naturally to polynomials -- the
reader is encouraged to look over all of the exercises in that chapter, determining
which have natural polynomial analogs, and work some of these out.

Example 16.12. Let f  F [X ] be a polynomial of degree 2 or 3. Then it is easy to
see that f is irreducible if and only if f has no roots in F . Indeed, if f is reducible,
then it must have a factor of degree 1, which we can assume is monic; thus, we can
write f = (X - x)g, where x  F and g  F [X ], and so f (x) = (x - x)g(x) = 0.
Conversely, if x  F is a root of f, then X - x divides f (see Theorem 7.12), and
so f is reducible. 2

Example 16.13. As a special case of the previous example, consider the poly-
nomials f := X 2 - 2  Q[X ] and g := X 3 - 2  Q[X ]. We claim that as
polynomials over Q, f and g are irreducible. Indeed, neither of them have integer
roots, and so neither of them have rational roots (see Exercise 1.26); therefore, they

are irreducible. 2

Example 16.14. In discussing the factorization of polynomials, one must be clear
about the coefficient domain. Indeed, if we view f and g in the previous example

as polynomials over R, then they factor into irreducibles as
            f = (X - 2)(X + 2), g = (X - 3 2)(X 2 + 3 2 X + 3 4),

and over C, g factors even further, as
           g = (X - 3 2) X - 3 2(1 + i3)/2 X - 3 2(1 - i3)/2 . 2

EXERCISE 16.6. Suppose f = i=0 ciX i is an irreducible polynomial over F ,
where c0 = 0 and c = 0. Show that the "reverse" polynomial f~ := i=0 c -iX i is
also irreducible.
16.4 Polynomial congruences  435

                              16.4 Polynomial congruences

Throughout this section, F denotes a field.
   Many of the results from Chapter 2 on congruences modulo a positive inte-

ger n carry over almost verbatim to congruences modulo a non-zero polynomial
f  F [X ]. We state these results here -- the proofs of these results are essentially
the same as in the integer case, and as such, are omitted for the most part.

   Because of the division with remainder property for polynomials, we have the
analog of Theorem 2.4:

Theorem 16.17. Let g, f  F [X ], where f = 0. Then there exists a unique
z  F [X ] such that z  g (mod f ) and deg(z) < deg(f ), namely, z := g mod f.

   Corresponding to Theorem 2.5, we have:

Theorem 16.18. Let g, f  F [X ] with f = 0, and let d := gcd(g, f ).
    (i) For every h  F [X ], the congruence gz  h (mod f ) has a solution
        z  F [X ] if and only if d | h.
   (ii) For every z  F [X ], we have gz  0 (mod f ) if and only if z  0
         (mod f/d).

  (iii) For all z, z  F [X ], we have gz  gz (mod f ) if and only if z  z
         (mod f/d).

  Let g, f  F [X ] with f = 0. Part (iii) of Theorem 16.18 gives us a cancellation
law for polynomial congruences:

         if gcd(g, f ) = 1 and gz  gz (mod f ), then z  z (mod f ).

We say that z  F [X ] is a multiplicative inverse of g modulo f if gz  1 (mod f ).
Part (i) of Theorem 16.18 says that g has a multiplicative inverse modulo f if
and only if gcd(g, f ) = 1. Moreover, part (iii) of Theorem 16.18 says that the
multiplicative inverse of g, if it exists, is uniquely determined modulo f .

   As for integers, we may generalize the "mod" operation as follows. Suppose
g, h, f  F [X ], with f = 0, g = 0, and gcd(g, f ) = 1. If s is the rational function
h/g  F (X ), then we define s mod f to be the unique polynomial z  F [X ]
satisfying

                         gz  h (mod f ) and deg(z) < deg(f ).

With this notation, we can simply write g-1 mod f to denote the unique multi-
plicative inverse of g modulo f of degree less than deg(f ).

   Corresponding to Theorem 2.6, we have:
436                     More rings

Theorem 16.19 (Chinese remainder theorem). Let {fi}ik=1 be a pairwise rela-
tively prime family of non-zero polynomials in F [X ], and let g1, . . . , gk be arbi-
trary polynomials in F [X ]. Then there exists a solution g  F [X ] to the system of
congruences

                              g  gi (mod fi) (i = 1, . . . , k).

Moreover, any g  F [X ] is a solution to this system of congruences if and only if
g  g (mod f ), where f := ik=1 fi.

Let us recall the formula for the solution g (see proof of Theorem 2.6). We have

                                  k

                        g := giei,

                                i=1

where

       ei := fiti, fi := f /fi, ti := (f i )-1 mod fi (i = 1, . . . , k).

   Now, let us consider the special case of the Chinese remainder theorem where
fi = X - xi with xi  F , and gi = yi  F , for i = 1, . . . , k. The condition that
{fi}ik=1 is pairwise relatively prime is equivalent to the condition that the xi's are
distinct. Observe that a polynomial g  F [X ] satisfies the system of congruences

                               g  gi (mod fi) (i = 1, . . . , k)

if and only if          g(xi) = yi (i = 1, . . . , k).
Moreover, we have fi =  j=i(X - xj) and ti = 1/ j=i(xi - xj)  F . So we get

                                 k  j=i(X - xj)
                                                  .
                        g = yi
                                    j=i(xi - xj)
                               i=1

The reader will recognize this as the usual Lagrange interpolation formula (see
Theorem 7.15). Thus, the Chinese remainder theorem for polynomials includes
Lagrange interpolation as a special case.

Polynomial quotient algebras. Let f  F [X ] be a polynomial of degree  0,
and consider the quotient ring E := F [X ]/(f ). As discussed in Example 16.7, we
may naturally view E as an F -algebra. Moreover, if we set  := [X ]f  E, then
E = F [], and viewing E as a vector space over F , we see that { i i-1} =1 is a basis
for E.

  Now suppose   E. We have  = [g]f = g() for some g  F [X ], and from
16.4 Polynomial congruences  437

the above discussion about polynomial congruences, we see that  is a unit if and
only if gcd(g, f ) = 1.

   If = 0, then E is trivial. If f is irreducible, then E is a field, since g  0 (mod f )
implies gcd(g, f ) = 1. If f is reducible, then E is not a field, and indeed, not even
an integral domain: for any non-trivial factor g  F [X ] of f , [g]f  E is a zero
divisor.

   The Chinese remainder theorem for polynomials also has a more algebraic inter-
pretation. Namely, if {fi}ik=1 is a pairwise relatively prime family of non-zero
polynomials in F [X ], and f := ik=1 fi, then the map

                   : F [X ]/(f )  F [X ]/(f1) × · · · × F [X ]/(fk)

                                 [g]f  ([g]f1, . . . , [g]fk )

is unambiguously defined, and is in fact an F -algebra isomorphism. This map may
be seen as a generalization of the ring isomorphism  discussed in Example 7.54.

Example 16.15. The polynomial X 2 +1 is irreducible over R, since if it were not, it
would have a root in R (see Example 16.12), which is clearly impossible, since -1
is not the square of any real number. It follows immediately that C = R[X ]/(X 2+1)
is a field, without having to explicitly calculate a formula for the inverse of a non-

zero complex number. 2

Example 16.16. Consider the polynomial f := X 4 + X 3 + 1 over Z2. We claim
that f is irreducible. It suffices to show that f has no irreducible factors of degree

1 or 2.
   If f had a factor of degree 1, then it would have a root; however, f (0) = 0 + 0 +

1 = 1 and f (1) = 1 + 1 + 1 = 1. So f has no factors of degree 1.
  Does f have a factor of degree 2? The polynomials of degree 2 are X 2, X 2 + X ,

X 2 + 1, and X 2 + X + 1. The first and second of these polynomials are divisible
by X , and hence not irreducible, while the third has a 1 as a root, and hence is also
not irreducible. The last polynomial, X 2 + X + 1, has no roots, and hence is the
only irreducible polynomial of degree 2 over Z2. So now we may conclude that if
f were not irreducible, it would have to be equal to

            (X 2 + X + 1)2 = X 4 + 2X 3 + 3X 2 + 2X + 1 = X 4 + X 2 + 1,

which it is not.
  Thus, E := Z2[X ]/(f ) is a field with 24 = 16 elements. We may think of ele-

ments E as bit strings of length 4, where the rule for addition is bit-wise "exclusive-

or." The rule for multiplication is more complicated: to multiply two given bit
strings, we interpret the bits as coefficients of polynomials (with the left-most bit
the coefficient of X 3), multiply the polynomials, reduce the product modulo f , and
438     More rings

write down the bit string corresponding to the reduced product polynomial. For
example, to multiply 1001 and 0011, we compute

                         (X 3 + 1)(X + 1) = X 4 + X 3 + X + 1,

and
                     (X 4 + X 3 + X + 1) mod (X 4 + X 3 + 1) = X .

Hence, the product of 1001 and 0011 is 0010.
   Theorem 7.29 says that E is a cyclic group. Indeed, the element  := 0010

(i.e.,  = [X ]f ) is a generator for E, as the following table of powers shows:

     i  i i         i

     1 0010 8 1110

     2 0100 9 0101

     3 1000 10 1010

     4 1001 11 1101

     5 1011 12 0011

     6 1111 13 0110

     7 0111 14 1100

        15 0001

   Such a table of powers is sometimes useful for computations in small finite fields

such as this one. Given ,   E, we can compute  by obtaining (by table
lookup) i, j such that  = i and  = j, computing k := (i + j) mod 15, and then
obtaining  = k (again by table lookup). 2

                                16.5 Minimal polynomials

Throughout this section, F denotes a field.
   Suppose that E is an arbitrary F -algebra, and let  be an element of E. Consider

the polynomial evaluation map

                                    : F [X ]  E

                                                 g  g(),

which is an F -algebra homomorphism. By definition, the image of  is F []. The
kernel of  is an ideal of F [X ], and since every ideal of F [X ] is principal, it follows
that Ker  = F [X ] for some polynomial   F [X ]; moreover, we can make the
choice of  unique by insisting that it is monic or zero. The polynomial  is called
the minimal polynomial of  (over F ).

   On the one hand, suppose  = 0. Since any polynomial that is zero at  is a
polynomial multiple of , we see that  is the unique monic polynomial of smallest
                                   16.5 Minimal polynomials                                           439

degree that vanishes at . Moreover, the first isomorphism theorems for rings and
modules tell us that F [] is isomorphic (as an F -algebra) to F [X ]/(), via the
isomorphism

                                 : F [X ]/()  F []
                                                [g]  g().

Under this isomorphism, [X ]  F [X ]/() corresponds to   F [], and we see

that { i i-1}m=1 is a basis for F [] over F , where m = deg(). In particular, every
                                                  m     cii-1,
element of F [] can be written uniquely as        i=1           where    c1,  .  .  .  ,  cm    F  .

   On the other hand, suppose  = 0. This means that no non-zero polynomial van-

ishes at . Also, it means that the map  is injective, and hence F [] is isomorphic

(as an F -algebra) to F [X ]; in particular, F [] is not finitely generated as a vector

space over F .
   Note that if   E has a minimal polynomial  = 0, then deg() > 0, unless E

is trivial (i.e., 1E = 0E ), in which case  = 1.

                                                             3
Example 16.17. Consider the real numbers 2 and 2.
   We claim that X 2 - 2 is the minimal polynomial of 2 over Q. To see this, first
observe that 2 is a root of X 2 - 2. Thus, the minimal polynomial of 2 divides

X  2  - 2.  However,  as  we  saw  in  Example  16.13,  the  polynomial  X2-2          is  irreducible
                                                                          
over Q, and hence must be equal to the minimal polynomial of 2 overQ.
   A similar argument shows that X 3 - 2 is the minimal polynomial of 3 2 over Q.
   We also see that Q[ 2] is isomorphic (as a Q-algebra) to Q[X ]/(X 2 - 2), and
since X 2 - 2 is irreducible, it follows that the ring Q[ 2] is actually afield. As a
vector space over Q, Q[ 2] has dimension 2, and every element of Q[ 2] may be
written uniquely as a + b 2 for a, b  Q. Indeed, forall a, b  Q, not both zero,
the multiplicative inverse of a + b 2 is (a/c) + (b/c) 2, where c := a2 - 2b2.
   Similarly, Q[ 2]is a field and has dimension 3 as avector space over Q, and3

every element of Q[ 3 2] may be written uniquely as a+b 3 2+c 3 4 for a, b, c  Q. 2

   A simple but important fact is the following:

Theorem 16.20. Suppose E is an F -algebra, and that as an F -vector space, E
has finite dimension n. Then every   E has a non-zero minimal polynomial of
degree at most n.

Proof. Indeed, the family of elements
                                          1E, , . . . , n

must be linearly dependent (as must any family of n + 1 elements of a vector space
440  More rings

of dimension n), and hence there exist c0, . . . , cn  F , not all zero, such that
                               c01E + c1 + · · · + cnn = 0E,

and therefore, the non-zero polynomial f := i ciX i vanishes at . 2

Example 16.18. Let f  F [X ] be a monic polynomial of degree , and consider
the F -algebra E := F [X ]/(f ) = F [], where  := [X ]f  E. Clearly, the minimal

polynomial of  over F is f. Moreover, as a vector space over F , E has dimension
 , with { i i-1} =1 being a basis. Therefore, every   E has a non-zero minimal
polynomial of degree at most . 2

EXERCISE 16.7. In the field E in Example 16.16, what is the minimal polynomial
of 1011 over Z2?

EXERCISE 16.8. Let  : E  E be an F -algebra homomorphism, let   E, let
 be the minimal polynomial of  over F , and let  be the minimal polynomial of
() over F . Show that  | , and that  =  if  is injective.

EXERCISE 16.9. Show that if the factorization of f over F [X ] into monic irre-
ducibles is f = f1e1 · · · frer , and if  = [h]f  F [X ]/(f ), then the minimal polyno-
mial  of  over F is lcm(1, . . . , r), where each i is the minimal polynomial of
[h]fei  F [X ]/(fiei ) over F .

        i

                      16.6 General properties of extension fields

We now discuss a few general notions related to extension fields. These are all quite
simple applications of the theory developed so far. Recall that if F and E are fields,
with F being a subring of E, then F is called a subfield of E, and E is called an
extension field of F . As usual, we shall blur the distinction between a subring and
a natural embedding; that is, if  : F  E is a natural embedding, we shall simply
identify elements of F with their images in E under , and in so doing, we may
view E as an extension field of F . Usually, the map  will be clear from context;
for example, if E = F [X ]/(f ) for some irreducible polynomial f  F [X ], then
we shall simply say that E is an extension field of F , although strictly speaking, F
is embedded in E via the map that sends c  F to [c]f  E.

   We start with some definitions. Let E be an extension field of a field F . Then E
is an F -algebra via inclusion, and in particular, an F -vector space. If E is a finite
dimensional F -vector space, then we say that E is a finite extension of F , and
dimF (E) is called the degree (over F ) of the extension, and is denoted (E : F );
otherwise, we say that E is an infinite extension of F .
16.6 General properties of extension fields  441

   An element   E is called algebraic over F if there exists a non-zero polyno-
mial g  F [X ] such that g() = 0, and in this case, we define the degree of  (over
F ) to be the degree of its minimal polynomial over F (see §16.5); otherwise,  is
called transcendental over F . If all elements of E are algebraic over F , then we
call E an algebraic extension of F .

   Suppose E is an extension field of a field F . For   E, we define

                   F () := {g()/h() : g, h  F [X ], h() = 0}.

It is easy to see that F () is a subfield of E, and indeed, it is the smallest subfield
of E containing F and . Clearly, the ring F [] = {g() : g  F [X ]}, which is the
smallest subring of E containing F and , is a subring of F (). We derive some
basic properties of F () and F []. The analysis naturally breaks down into two
cases, depending on whether  is algebraic or transcendental over F .

   On the one hand, suppose  is algebraic over F . Let  be the minimal polyno-
mial of  over F , so that deg() > 0, and the quotient ring F [X ]/() is isomorphic
(as an F -algebra) to the ring F [] (see §16.5). Since F [] is a subring of a field,
it must be an integral domain, which implies that F [X ]/() is an integral domain,
and so  is irreducible. This in turn implies that F [X ]/() is a field, and so F [] is
not just a subring of E, it is a subfield of E. Since F [] is itself already a subfield
of E containing F and , it follows that F () = F []. Moreover, F [] is a finite
extension of F ; indeed (F [] : F ) = deg() = the degree of  over F , and the
elements 1, , . . . , m-1, where m := deg(), form a basis for F [] over F .

   On the other hand, suppose that  is transcendental over F . In this case, the
minimal polynomial of  over F is the zero polynomial, and the ring F [] is iso-
morphic (as an F -algebra) to the ring F [X ] (see §16.5), which is definitely not a
field. But consider the "rational function evaluation map" that sends g/h  F (X )
to g()/h()  F (). Since no non-zero polynomial over F vanishes at , it is
easy to see that this map is well defined, and is in fact an F -algebra isomorphism.
Thus, we see that F () is isomorphic (as an F -algebra) to F (X ). It is also clear
that F () is an infinite extension of F .

   Let us summarize the above discussion in the following theorem:

Theorem 16.21. Let E be an extension field of a field F .
    (i) If   E is algebraic over F , then F () = F [], and F [] is isomorphic
        (as an F -algebra) to F [X ]/(), where  is the minimal polynomial of 
         over F , which is irreducible; moreover, F [] is a finite extension of F ,
         and (F [] : F ) = deg() = the degree of  over F , and the elements
         1, , . . . , m-1, where m := deg(), form a basis for F [] over F .

   (ii) If   E is transcendental over F , then F () is isomorphic (as an F -
        algebra) to the rational function field F (X ), while the subring F [] is
442     More rings

        isomorphic (as an F -algebra) to the ring of polynomials F [X ]; moreover,
        F () is an infinite extension of F .

   Suppose E is an extension field of a field K, which itself is an extension of a
field F . Then E is also an extension field of F . The following theorem examines
the relation between the degrees of these extensions, in the case where E is a finite
extension of K, and K is a finite extension of F . The proof is a simple calculation,
which we leave to the reader to verify.

Theorem 16.22. Suppose E is a finite extension of a field K, with a basis {j}jm=1
over K, and K is a finite extension of F , with a basis {i}in=1 over F . Then the
elements

                             ij (i = 1, . . . , n; j = 1, . . . , m)

form a basis for E over F . In particular, E is a finite extension of F and
                                 (E : F ) = (E : K)(K : F ).

   Now suppose that E is a finite extension of a field F . Let K be an intermediate
field, that is, a subfield of E containing F . Then evidently, E is a finite extension of
K (since any basis for E over F also spans E over K), and K is a finite extension
of F (since as F -vector spaces, K is a subspace of E). The previous theorem then
implies that (E : F ) = (E : K)(K : F ). We have proved:

Theorem 16.23. If E is a finite extension of a field F , and K is a subfield of E

containing F , then E is a finite extension of K, K is a finite extension of F , and
(E : F ) = (E : K)(K : F ).

   Again, suppose that E is a finite extension of a field F . Theorem 16.20 implies
that E is algebraic over F , and indeed, that each element of E has degree over F
bounded by (E : F ). However, we can say a bit more about these degrees. Suppose
  E. Then the degree of  over F is equal to (F [] : F ), and by the previous
theorem, applied to K := F [], we have (E : F ) = (E : F [])(F [] : F ). In
particular, the degree of  over F divides (E : F ). We have proved:

Theorem 16.24. If E is a finite extension of a field F , then it is an algebraic

extension, and for each   E, the degree of  over F divides (E : F ).

                                                                             
Example 16.19. Continuing with Example 16.17, we see that the real numbers 2
and 2 are algebraic over Q. The fields Q[ 2] and Q[ 2] are extension fields of33

Q, where (Q[2] : Q) = 2 = the degree of 2 over Q, and (Q[ 3 2] : Q) = 3 =
the degree of 3 2 over Q. As both of these fields are finite extensions of Q, they

are algebraic extensions as well. Since their degrees over Q are prime numbers,

it follows that they have no subfields other than themselves and Q. In particular,
16.6 General properties of extension fields  443

if   Q[2] \ Q, then Q[] = Q[2]. Similarly, if   Q[3 2] \ Q, then
Q[] = Q[ 3 2]. 2

Example 16.20. Continuing with Example 16.18, suppose f  F [X ] is a monic
irreducible polynomial of degree , so that E := F [X ]/(f ) = F [], where
 := [X ]f  E, is an extension field of F . The element  is algebraic of degree
over F . Moreover, E is a finite extension of F , with (E : F ) = ; in particular, E

is an algebraic extension of F , and for each   E, the degree of  over F divides

 .2

   As we have seen in Example 16.14, an irreducible polynomial over a field may
be reducible when viewed as a polynomial over an extension field. A splitting
field is a finite extension of the coefficient field in which a given polynomial splits
completely into linear factors. As the next theorem shows, splitting fields always
exist.

Theorem 16.25. Let F be a field, and f  F [X ] a non-zero polynomial of degree
n. Then there exists a finite extension E of F over which f factors as

                         f = c(X - 1)(X - 2) · · · (X - n),

where c  F and 1, . . . , n  E.

Proof. We may assume that f is monic. We prove the existence of E by induction
on the degree n of f. If n = 0, then the theorem is trivially true. Otherwise, let h be
an irreducible factor of f , and set K := F [X ]/(h), so that  := [X ]h  K is a root
of h, and hence of f . So over K, which is a finite extension of F , the polynomial

f factors as

                                      f = (X - )g,

where g  K[X ] is a monic polynomial of degree n - 1. Applying the induction
hypothesis, there exists a finite extension E of K over which g splits into linear
factors. Thus, over E, f splits into linear factors, and by Theorem 16.22, E is a
finite extension of F . 2

EXERCISE 16.10. In the field E in Example 16.16, find all the elements of degree
2 over Z2.

EXERCISE 16.11. Let E be an extension field of a field F , and let 1, . . . , n  E
be algebraic over F . Show that the ring F [1, . . . , n] (see Example 7.45) is in fact
a field, and that F [1, . . . , n] is a finite (and hence algebraic) extension of F .
444  More rings

EXERCISE 16.12. Consider the real numbers 2 and 3 2. Show that

     (Q[2, 3 2] : Q) = (Q[2 + 3 2] : Q) = 6.

                                                        
EXERCISE 16.13. Consider the real numbers 2 and 3. Show that

     (Q[2, 3] : Q) = (Q[2 + 3] : Q) = 4.

EXERCISE 16.14. Show that if E is an algebraic extension of K, and K is an
algebraic extension of F , then E is an algebraic extension of F .

EXERCISE 16.15. Let E be an extension of F . Show that the set of all elements
of E that are algebraic over F is a subfield of E containing F .

EXERCISE 16.16. Consider a field F and its field of rational functions F (X ). Let
  F (X ) \ F . Show that X is algebraic over F (), and that  is transcendental
over F .

EXERCISE 16.17. Let E be an extension field of a field F . Suppose   E is
transcendental over F , and that E is algebraic over F (). Show that for every
  E,  is transcendental over F if and only if E is algebraic over F ().

     16.7 Formal derivatives

Throughout this section, R denotes a ring.

  Consider a polynomial g  R[X ]. If Y is another indeterminate, we may evaluate
g at X + Y , and collecting monomials of like degree in Y , we may write

     g X + Y = g0 + g1Y + g2Y 2 + · · ·                         (16.5)

where gi  R[X ] for i = 0, 1, 2, . . . . Evidently, g0 = g (just substitute 0 for Y in
(16.5)), and we may write

     g X + Y  g + g1Y (mod Y 2).                                (16.6)

We define the formal derivative of g, denoted D(g), to be the unique polyno-
mial g1  R[X ] satisfying (16.6). We stress that unlike the "analytical" notion
of derivative from calculus, which is defined in terms of limits, this definition is
purely "symbolic." Nevertheless, some of the usual rules for derivatives still hold:

Theorem 16.26. We have:
    (i) D(c) = 0 for all c  R;
   (ii) D(X ) = 1;

  (iii) D(g + h) = D(g) + D(h) for all g, h  R[X ];
   (iv) D(gh) = D(g)h + gD(h) for all g, h  R[X ].
                               16.7 Formal derivatives                             445

Proof. Parts (i) and (ii) are immediate from the definition. Parts (iii) and (iv)
follow from the definition by a simple calculation. Suppose

      g X + Y  g + g1Y (mod Y 2) and h X + Y  h + h1Y (mod Y 2)
where g1 = D(g) and h1 = D(h). Then

 (g + h) X + Y  g X + Y + h X + Y  (g + h) + (g1 + h1)Y (mod Y 2),

and

     (gh) X + Y  g X + Y h X + Y  gh + (g1h + gh1)Y (mod Y 2). 2

Combining parts (i) and (iv) of this theorem, we see that D(cg) = cD(g) for

all c  R and g  R[X ]. This fact can also be easily derived directly from the

definition of the derivative.

   Combining parts (ii) and (iv) of this theorem, together with a simple induction
argument, we see that D(X n) = nX n-1 for all positive integers n. This fact can also

be easily derived directly from the definition of the derivative by considering the
binomial expansion of (X + Y )n.

Combining part (iii) of this theorem and the observations in the previous two

paragraphs, we see that for any polynomial g =  k       aiX i    R[X ],  we  have
                                                i=0

                                              k                                    (16.7)

                               D(g) = iaiX i-1,

                                            i=1

which agrees with the usual formula for the derivative of a polynomial.

   The notion of a formal derivative can be generalized to multi-variate polynomi-
als. Let g  R[X1, . . . , Xn]. For any i = 1, . . . , n, we can view g as a polynomial in
the variable Xi, whose coefficients are elements of R[X1, . . . , Xi-1, Xi+1, . . . , Xn].
Then if we formally differentiate with respect to the variable Xi, we obtain the

formal "partial" derivative DXi (g).

EXERCISE 16.18. Show that for g1, . . . , gn  R[X ], we have

     D gi = D(gi) gj

                               i  i             j=i

and that for g  R[X ], and n  1, we have

                               D(gn) = ngn-1D(g).

EXERCISE 16.19. Prove the "chain rule" for formal derivatives: if g, h  R[X ]
446                                   More rings

and f = g(h)  R[X ], then D(f ) = D(g)(h) · D(h); more generally, if g 
R[X1, ..., Xn], and h1, ..., hn  R[X ], and f = g(h1, ..., hn)  R[X ], then

                                                      n

                          DX (f ) = DXi (g)(h1, ..., hn)DX (hi).

                                                     i=1

EXERCISE 16.20. Let g  R[X ], and let x  R be a root of g. Show that x is a
multiple root of g if and only if x is also a root of D(g) (see Exercise 7.18).

EXERCISE 16.21. Let g  R[X ] with deg(g) = k  0, and let x  R. Show that
if we evaluate g at X + x, writing

     g X +x                                k

                                      = biX i,

                                          i=0

with b0, . . . , bk  R, then we have

     i! · bi = (Di(g))(x) for i = 0, . . . , k.

EXERCISE 16.22. Suppose p is a prime, g  Z[X ], and x  Z, such that
g(x)  0 (mod p) and D(g)(x)  0 (mod p). Show that for every positive integer
e, there exists an integer x^ such that g(x^)  0 (mod pe), and give an efficient

procedure to compute such an x^, given p, g, x, and e. Hint: mimic the "lifting"

procedure discussed in §12.5.2.

                    16.8 Formal power series and Laurent series

We discuss generalizations of polynomials that allow an infinite number of non-
zero coefficients. Although we are mainly interested in the case where the coeffi-
cients come from a field F , we develop the basic theory for general rings R.

                                16.8.1 Formal power series

The ring R[[X ]] of formal power series over R consists of all formal expressions
of the form

                              g = a0 + a1X + a2X 2 + · · · ,

where a0, a1, a2, . . .  R. Unlike ordinary polynomials, we allow an infinite num-
ber of non-zero coefficients. We may write such a formal power series as

                                                                 

                                      g = aiX i.

                                                                 i=0
16.8 Formal power series and Laurent series  447

Formally, such a formal power series is an infinite sequence {ai}i=0, and the rules
for addition and multiplication are exactly the same as for polynomials. Indeed,
the formulas (7.2) and (7.3) in §7.2 for addition and multiplication may be applied
directly -- all of the relevant sums are finite, and so everything is well defined.
We leave it to the reader to verify that with addition and multiplication so defined,
R[[X ]] indeed forms a ring. We shall not attempt to interpret a formal power series
as a function, and therefore, "convergence" issues shall simply not arise.

  Clearly, R[[X ]] contains R[X ] as a subring. Let us consider the group of units of
R[[X ]].

Theorem 16.27. Let g =  i=0 aiX i  R[[X ]]. Then g  (R[[X ]]) if and only if
a0  R.

Proof. If a0 is not a unit, then it is clear that g is not a unit, since the constant term
of a product of formal power series is equal to the product of the constant terms.

   Conversely, if a0 is a unit, we show how to define the coefficients of the inverse
h = i=0 biX i of g. Let f = gh = i=0 ciX i. We want f = 1, which means that
c0 = 1 and ci = 0 for all i > 0. Now, c0 = a0b0, so we set b0 := a0-1. Next, we have
c1 = a0b1+a1b0, so we set b1 := -a1b0·a0-1. Next, we have c2 = a0b2+a1b1+a2b0,
so we set b2 := -(a1b1 + a2b0) · a0-1. Continuing in this way, we see that if we
define bi := -(a1bi-1 + · · · + aib0) · a0-1 for i  1, then gh = 1. 2

Example 16.21. In the ring R[[X ]], the multiplicative inverse of 1-X is i=0 X i. 2

EXERCISE 16.23. Let F be a field. Show that every non-zero ideal of F [[X ]] is of
the form (X m) for some uniquely determined integer m  0.

                               16.8.2 Formal Laurent series

One may generalize formal power series to allow a finite number of negative pow-
ers of X . The ring R((X )) of formal Laurent series over R consists of all formal
expressions of the form

                             g = amX m + am+1X m+1 + · · · ,

where m is allowed to be any integer (possibly negative), and am, am+1, . . .  R.
Thus, elements of R((X )) may have an infinite number of terms involving positive
powers of X , but only a finite number of terms involving negative powers of X . We
may write such a formal Laurent series as

                                                                 

                                      g = aiX i.

                                                                i=m
448                                  More rings

Formally, such a formal Laurent series is a doubly infinite sequence {ai}i=-,
with the restriction that for some integer m, we have ai = 0 for all i < m. We may
again use the usual formulas (7.2) and (7.3) to define addition and multiplication
(where the indices i, j, and k now range over all integers, not just the non-negative
integers). Note that while the sum in (7.3) has an infinite number of terms, only
finitely many of them are non-zero.

  One may naturally view R[[X ]] as a subring of R((X )), and of course, R[X ] is a
subring of R[[X ]] and so also a subring of R((X )).

Theorem 16.28. If D is an integral domain, then D((X )) is an integral domain.

Proof. Let g = i=m aiX i and h = i=n biX i, where am = 0 and bn = 0. Then
gh =         ciX i,               =        =
      i=m+n          where  cm+n     ambn     0.  2

Theorem 16.29. Let g  R((X )), and suppose that g = 0 and g =       aiX i  with
am  R. Then g has a multiplicative inverse in R((X )).         i=m

Proof. We can write g = X mg , where g is a formal power series whose constant
term is a unit, and hence there is a formal power series h such that g h = 1. Thus,

X -mh is the multiplicative inverse of g in R((X )). 2

As an immediate corollary, we have:

Theorem 16.30. If F is a field, then F ((X )) is a field.

EXERCISE 16.24. Let F be a field. Show that F ((X )) is the field of fractions of
F [[X ]]; that is, there is no subfield E F ((X )) that contains F [[X ]].

                     16.8.3 Reversed Laurent series

While formal Laurent series are useful in some situations, in many others, it is
more useful and natural to consider reversed Laurent series over R. These are
formal expressions of the form

                                           m

                                     g=       aiX i,

                                         i=-

where am, am-1, . . .  R. Thus, in a reversed Laurent series, we allow an infinite
number of terms involving negative powers of X , but only a finite number of terms

involving positive powers of X . Formally, such a reversed Laurent series is a doubly
infinite sequence {ai}i=-, with the restriction that for some integer m, we have
ai = 0 for all i > m. We may again use the usual formulas (7.2) and (7.3) to define
                    16.8 Formal power series and Laurent series  449

addition and multiplication -- and again, the sum in (7.3) has only finitely many

non-zero terms.
  The ring of all reversed Laurent series is denoted R((X -1)), and as the notation

suggests, the map that sends X to X -1 (and acts as the identity on R) is an R-
algebra isomorphism of R((X )) with R((X -1)). Also, one may naturally view R[X ]
as a subring of R((X -1)).

  For g = im=- aiX i  R((X -1)) with am = 0, let us define the degree of g,
denoted deg(g), to be the value m, and the leading coefficient of g, denoted lc(g),

to be the value am. As for ordinary polynomials, we define the degree of 0 to
be -, and the leading coefficient of 0 to be 0. Note that if g happens to be a
polynomial, then these definitions of degree and leading coefficient agree with that

for ordinary polynomials.

Theorem 16.31. For g, h  R((X -1)), we have deg(gh)  deg(g) + deg(h), where
equality holds unless both lc(g) and lc(h) are zero divisors. Furthermore, if h = 0
and lc(h) is a unit, then h is a unit, and we have deg(gh-1) = deg(g) - deg(h).

Proof. Exercise. 2

It is also natural to define a floor function for reversed Laurent series: for

g  R((X -1)) with g =  m    aiX  i,  we  define
                       i=-

                                   m

                       g := aiX i  R[X ];

                                 i=0

that is, we compute the floor function by simply throwing away all terms involving
negative powers of X .

Theorem 16.32. Let g, h  R[X ] with h = 0 and lc(h)  R, and using the
usual division with remainder property for polynomials, write g = hq + r, where

q, r  R[X ] with deg(r) < deg(h). Let h-1 denote the multiplicative inverse of h
in R((X -1)). Then q = gh-1 .

Proof. Multiplying the equation g = hq + r by h-1, we obtain gh-1 = q + rh-1,
and deg(rh-1) < 0, from which it follows that gh-1 = q. 2

  Let F be a field, so that F ((X -1)) is also field (this is immediate from Theo-
rem 16.31). Now, F ((X -1)) contains F [X ] as a subring, and hence contains (an
isomorphic copy of) the rational function field F (X ). Just as F (X ) corresponds
to the field of rational numbers, F ((X -1)) corresponds to the field real numbers.
Indeed, we can think of real numbers as decimal numbers with a finite number
of digits to the left of the decimal point and an infinite number to the right, and
reversed Laurent series have a similar "syntactic" structure. In many ways, this
450            More rings

syntactic similarity between the real numbers and reversed Laurent series is more
than just superficial.

EXERCISE 16.25. Write down the rule for determining the multiplicative inverse
of an element of R((X -1)) whose leading coefficient is a unit in R.

EXERCISE 16.26. Let F be a field of characteristic other than 2. Show that a
non-zero g  F ((X -1)) has a square-root in F ((X -1)) if and only if deg(g) is even
and lc(g) has a square-root in F .

EXERCISE 16.27. Let R be a ring, and let a  R. Show that the multiplicative
inverse of X - a in R((X -1)) is j=1 aj-1X -j.

EXERCISE 16.28. Let R be an arbitrary ring, let a1, . . . , a  R, and let
                     f := (X - a1)(X - a2) · · · (X - a )  R[X ].

For j  0, define the "power sum"

               sj := aij.

                         i=1

Show that in the ring R((X -1)), we have

     D(f )     1                            

            =                               = sj-1X ,-j

     f i=1 (X - ai) j=1

where D(f ) is the formal derivative of f.

EXERCISE 16.29. Continuing with the previous exercise, derive Newton's iden-
tities, which state that if f = X + c1X -1 + · · · + c , with c1, . . . , c  R, then

                                                        s1 + c1 = 0
                                              s2 + c1s1 + 2c2 = 0
                                      s3 + c1s2 + c2s1 + 3c3 = 0

                                                                  ..
                                                                  .
                          s + c1s -1 + · · · + c -1s1 + c = 0
                  sj+ + c1sj+ -1 + · · · + c -1sj+1 + c sj = 0 (j  1).
16.9 Unique factorization domains ()    451

                        16.9 Unique factorization domains ()

As we have seen, both the ring of integers and the ring of polynomials over a field
enjoy a unique factorization property. These are special cases of a more general
phenomenon, which we explore here.

   Throughout this section, D denotes an integral domain.
   We call a, b  D associate if a = ub for some u  D. Equivalently, a and b are
associate if and only if a | b and b | a (see part (i) of Theorem 7.4). A non-zero
element p  D is called irreducible if it is not a unit, and all divisors of p are
associate to 1 or p. Equivalently, a non-zero, non-unit p  D is irreducible if and
only if it cannot be expressed as p = ab where neither a nor b are units.

Definition 16.33. We call D a unique factorization domain (UFD) if
    (i) every non-zero element of D that is not a unit can be written as a product
         of irreducibles in D, and

   (ii) such a factorization into irreducibles is unique up to associates and the order
         in which the factors appear.

   Another way to state part (ii) of the above definition is that if p1 · · · pr and
p1 · · · ps are two factorizations of some element as a product of irreducibles, then
r = s, and there exists a permutation  on the indices {1, . . . , r} such that pi and
p(i) are associate.

  As we have seen, both Z and F [X ] are UFDs. In both of those cases, we chose
to single out a distinguished irreducible element among all those associate to any

given irreducible: for Z, we always chose positive primes, and for F [X ], we chose
monic irreducible polynomials. For any specific unique factorization domain D,

there may be such a natural choice, but in the general case, there will not be (but

see Exercise 16.30 below).

Example 16.22. Having already seen two examples of UFDs, it is perhaps a good

idea to look at an example of an integral domain that is not a UFD. Consider the
subring Z[ -3] of the complex numbers, which consists of all complex numbers
of the form a + b -3, where a, b  Z.As this is a subring of the field C, it is an
integral domain (one may also view Z[ -3] as the quotient ring Z[X ]/(X 2 + 3)).
                                      
Let us first determine the units in Z[ -3]. For a, b  Z, we have N (a+b -3) =
a2 + 3b2, where N is the usual norm map on C (see Example 7.5). If   Z[ -3]
is a unit, then there exists   Z[-3] such that  = 1. Taking norms, we obtain

                            1 = N(1) = N( ) = N()N( ).
                                          

Since the norm of an element of Z[ -3] is a non-negative integer, this implies that
N () = 1. If  = a + b -3, with a, b  Z, then N () = a2 + 3b2, and it is clear
452               More rings

that N () = 1 if and only if  = ±1. We conclude that the only units in Z[-3]

are ±1.                                     

Now consider the following two factorizations of 4 in Z[ -3]:

           4 = 2 · 2 = (1 + -3)(1 - -3).                       (16.8)

We claim that 2 is irreducible. For suppose, say, that 2 =  , for ,   Z[-3],
with neither a unit. Taking norms, we have 4 = N (2) = N ()N ( ), and therefore,

N () = N ( ) = 2-- but this is impossible, since there are no integers aand b such
that a2 + 3b2 = 2. By the same reasoning, since N (1 + -3) = N (1 - -3) = 4,
           
we see that 1 + -3 and 1- -3 are both irreducible. Further, it is clear that 2 is
not associate to either 1 + -3 or 1 - -3, and so the two factorizations of 4 in

(16.8) are fundamentally different. 2

   For a, b  D, we call d  D a common divisor of a and b if d | a and d | b;
moreover, we call such a d a greatest common divisor of a and b if all other
common divisors of a and b divide d. We say that a and b are relatively prime if
the only common divisors of a and b are units. It is immediate from the definition
of a greatest common divisor that it is unique, up to multiplication by units, if
it exists at all. Unlike in the case of Z and F [X ], in the general setting, greatest
common divisors need not exist; moreover, even when they do, we shall not attempt
to "normalize" greatest common divisors, and we shall speak only of "a" greatest
common divisor, rather than "the" greatest common divisor.

   Just as for integers and polynomials, we can generalize the notion of a greatest
common divisor in an arbitrary integral domain D from two to any number of
elements of D, and we can also define a least common multiple of any number of
elements as well.

   Although these greatest common divisors and least common multiples need not
exist in an arbitrary integral domain D, if D is a UFD, they will always exist.
The existence question easily reduces to the question of the existence of a greatest
common divisor and least common multiple of a and b, where a and b are non-zero
elements of D. So assuming that D is a UFD, we may write

             r                         r

           a = u piei and b = v pifi ,

             i=1                       i=1

where u and v are units, p1, . . . , pr are non-associate irreducibles, and e1, . . . , er
and f1, . . . , fr are non-negative integers, and it is easily seen that

                   r

                      pimin(ei ,fi )

                  i=1
                    16.9 Unique factorization domains ()  453

is a greatest common divisor of a and b, while

                                                           r

                                              pimax(ei ,fi )

                                                         i=1

is a least common multiple of a and b.
   It is also evident that in a UFD D, if c | ab and c and a are relatively prime,

then c | b. In particular, if p is irreducible and p | ab, then p | a or p | b. This
is equivalent to saying that if p is irreducible, then the quotient ring D/pD is an
integral domain (and the ideal pD is a prime ideal -- see Exercise 7.38). The
converse also holds:

Theorem 16.34. Suppose D satisfies part (i) of Definition 16.33, and that D/pD
is an integral domain for every irreducible p  D. Then D is a UFD.

Proof. Exercise. 2

EXERCISE 16.30.     (a) Show that the "is associate to" relation is an equivalence
         relation.

(b) Consider an equivalence class C induced by the "is associate to" relation.
     Show that if C contains an irreducible element, then all elements of C are
     irreducible.

   (c) Suppose that for every equivalence class C that contains irreducibles, we

         choose one element of C, and call it a distinguished irreducible. Show that

         D is a UFD if and only if every non-zero element of D can be expressed as
         up1e1 · · · prer , where u is a unit, p1, . . . , pr are distinguished irreducibles, and
         this expression is unique up to a reordering of the pi's.

                                                 
EXERCISE 16.31. Show that the ring Z[ -5] is not a UFD.

EXERCISE 16.32. Let D be a UFD and F its field of fractions. Show that
   (a) every element x  F can be expressed as x = a/b, where a, b  D are
         relatively prime, and
   (b) that if x = a/b for a, b  D relatively prime, then for any other a , b  D
         with x = a /b , we have a = ca and b = cb for some c  D.

EXERCISE 16.33. Let D be a UFD and let p  D be irreducible. Show that there
is no prime ideal Q of D with {0D} Q pD (see Exercise 7.38).
454        More rings

      16.9.1 Unique factorization in Euclidean and principal ideal domains

Our proofs of the unique factorization property in both Z and F [X ] hinged on the
division with remainder property for these rings. This notion can be generalized,
as follows.

Definition 16.35. We say D is a Euclidean domain if there is a "size function" S

mapping the non-zero elements of D to the set of non-negative integers, such that
for all a, b  D with b = 0, there exist q, r  D, with the property that a = bq + r
and either r = 0 or S(r) < S(b).

Example 16.23. Both Z and F [X ] are Euclidean domains. In Z, we can take the
ordinary absolute value function |·| as a size function, and for F [X ], the function

deg(·) will do. 2

Example 16.24. Recall again the ring

                                  Z[i] = {a + bi : a, b  Z}

of Gaussian integers from Example 7.25. Let us show that this is a Euclidean
domain, using the usual norm map N on complex numbers (see Example 7.5) for
the size function. Let ,   Z[i], with  = 0. We want to show the existence
of ,   Z[i] such that  =  + , where N () < N (). Suppose that in the
field C, we compute -1 = r + si, where r, s  Q. Let m, n be integers such that
|m - r|  1/2 and |n - s|  1/2 -- such integers m and n always exist, but may not
be uniquely determined. Set  := m + ni  Z[i] and  :=  - . Then we have

                                         -1 =  + ,

where   C with N ()  1/4 + 1/4 = 1/2, and
                            =  -  =  - (-1 - ) = ,

and hence

           N() = N() = N()N()  1N(). 2
                                                  2

Theorem 16.36. If D is a Euclidean domain and I is an ideal of D, then there
exists d  D such that I = dD.

Proof. If I = {0}, then d = 0 does the job, so let us assume that I = {0}. Let d

be any non-zero element of I such that S(d) is minimal, where S is a size function
that makes D into a Euclidean domain. We claim that I = dD.

   It will suffice to show that for all c  I, we have d | c. Now, we know that
there exists q, r  D such that c = dq + r, where either r = 0 or S(r) < S(d).
If r = 0, we are done; otherwise, r is a non-zero element of I with S(r) < S(d),

contradicting the minimality of S(d). 2
                      16.9 Unique factorization domains ()                                   455

   Recall that an ideal of the form I = dD is called a principal ideal. If all ideals of
D are principal, then D is called a principal ideal domain (PID). Theorem 16.36
says that every Euclidean domain is a PID.

   PIDs enjoy many nice properties, including:

Theorem 16.37. If D is a PID, then D is a UFD.

  For the rings Z and F [X ], the proof of part (i) of Definition 16.33 was a quite
straightforward induction argument (as it also would be for any Euclidean domain).
For a general PID, however, this requires a different sort of argument. We begin

with the following fact:

Theorem 16.38. If D is a PID, and I1  I2  · · · are ideals of D, then there
exists an integer k such that Ik = Ik+1 = · · · .

Proof. Let I :=       Ii,  which  is  an  ideal  of  D  (see  Exercise  7.37).  Thus,  I  =  dD
                 i=1
                           
for some d  D. But d       i=1  Ii    implies  that  d    Ik  for  some  k,  which  shows    that

I = dD  Ik. It follows that I = Ik = Ik+1 = · · · . 2

We can now prove the existence part of Theorem 16.37:

Theorem 16.39. If D is a PID, then every non-zero, non-unit element of D can
be expressed as a product of irreducibles in D.

Proof. Let c  D, c = 0, and c not a unit. If c is irreducible, we are done.
Otherwise, we can write c = ab, where neither a nor b are units. As ideals, we
have cD aD and cD bD. If we continue this process recursively, building up
a "factorization tree" where c is at the root, a and b are the children of c, and so on,
then the recursion must stop, since any infinite path in the tree would give rise to
ideals

                                    cD = I1 I2 · · · ,

contradicting Theorem 16.38. 2

   The proof of the uniqueness part of Theorem 16.37 is essentially the same as for

proofs we gave for Z and F [X ].
   Analogous to Theorems 1.7 and 16.13, we have:

Theorem 16.40. Let D be a PID. For all a, b  D, there exists a greatest common
divisor d of a and b, and moreover, aD + bD = dD.

Proof. Exercise. 2

   As an immediate consequence of the previous theorem, we see that in a PID D,
for all a, b  D with greatest common divisor d, there exist s, t  D such that
456                             More rings

as + bt = d; moreover, a, b  D are relatively prime if and only if there exist
s, t  D such that as + bt = 1.

   Analogous to Theorems 1.9 and 16.14, we have:

Theorem 16.41. Let D be a PID. For all a, b, c  D such that c | ab and a and c
are relatively prime, we have c | b.

Proof. Exercise. 2

Analogous to Theorems 1.10 and 16.15, we have:

Theorem 16.42. Let D be a PID. Let p  D be irreducible, and let a, b  D. Then
p | ab implies that p | a or p | b.

Proof. Exercise. 2

   Theorem 16.37 now follows immediately from Theorems 16.39, 16.42, and
16.34.

                                       
EXERCISE 16.34. Show that Z[ -2] is a Euclidean domain.

EXERCISE 16.35. Consider the polynomial

                         X 3 - 1 = (X - 1)(X 2 + X + 1).

Over C, the roots of X 3 - 1 are 1, (-1 ± -3)/2. Let  := (-1 + -3)/2, and
                         
note that 2 = -1 -  = (-1 - -3)/2, and  = 1.3

(a) Show that the ring Z[] consists of all elements of the form a + b, where
     a, b  Z, and is an integral domain. This ring is called the ring of Eisenstein
     integers.

(b) Show that the only units in Z[] are ±1, ±, and ±2.

(c) Show that Z[] is a Euclidean domain.

EXERCISE 16.36. Show that in a PID, all non-zero prime ideals are maximal (see
Exercise 7.38).

   Recall that for a complex number  = a + bi, with a, b  R, the norm of
 was defined as N () =  = a2 + b2 (see Example 7.5). There are other

measures of the "size" of a complex number that are useful. The absolute value

of  is defined as || :=  N() =  a2 + b2. The max norm of  is defined as
M () := max{|a|, |b|}.

EXERCISE 16.37. Let ,   C. Prove the following statements:
   (a) || = ||||;
16.9 Unique factorization domains ()  457

(b) | + |  || + ||;
(c) N( + )  2(N() + N());

                       
(d) M ()  ||  2M ().

   The following exercises develop algorithms for computing with Gaussian inte-
gers. For computational purposes, we assume that a Gaussian integer  = a + bi,
with a, b  Z, is represented as the pair of integers (a, b).

EXERCISE 16.38. Let ,   Z[i].
    (a) Show how to compute M () in time O(len(M ())) and N () in time
         O(len(M ())2).
   (b) Show how to compute  +  in time O(len(M ()) + len(M ())).

    (c) Show how to compute  ·  in time O(len(M ()) · len(M ())).
   (d) Assuming  = 0, show how to compute ,   Z[i] such that  =  + ,

         N ()  21 N (), and N ()  4N ()/N (). Your algorithm should run
         in time O(len(M ()) · len(M ())). Hint: see Example 16.24; also, to
         achieve the stated running time bound, your algorithm should first test if
         M ()  2M ().

EXERCISE 16.39. Using the division with remainder algorithm from part (d)

of the previous exercise, adapt the Euclidean algorithm for (ordinary) integers

to work with Gaussian integers. On inputs ,   Z[i], your algorithm should
compute a greatest common divisor   Z[i] of  and  in time O( 3), where

  := max{len(M ()), len(M ())}.

EXERCISE 16.40. Extend the algorithm of the previous exercise, so that it com-
putes ,   Z[i] such that  +  = . Your algorithm should run in time O( 3),
and it should also be the case that len(M ()) and len(M ()) are O( ).

   The algorithms in the previous two exercises for computing greatest common
divisors in Z[i] run in time cubic in the length of their input, whereas the corre-
sponding algorithms for Z run in time quadratic in the length of their input. This is
essentially because the running time of the algorithm for division with remainder
discussed in Exercise 16.38 is insensitive to the size of the quotient.

   To get a quadratic-time algorithm for computing greatest common divisors in
Z[i], in the following exercises we shall develop an analog of the binary gcd algo-
rithm for Z.

EXERCISE 16.41. Let  := 1 + i  Z[i].
    (a) Show that 2 =  = -i2, that N () = 2, and that  is irreducible in Z[i].
458                More rings

(b) Let   Z[i], with  = a + bi for a, b  Z. Show that  |  if and only if
     a - b is even, in which case

                    = a + b + b - a i.
                   2           2

(c) Show that for all   Z[i], we have   0 (mod ) or   1 (mod ).
(d) Show that the quotient ring Z[i]/Z[i] is isomorphic to the ring Z2.
(e) Show that for all   Z[i] with   1 (mod ), there exists a unique

       {±1, ±i} such that    (mod 2).

(f) Show that for all ,   Z[i] with     1 (mod ), there exists a unique
       {±1, ±i} such that    (mod 2).

EXERCISE 16.42. We now present a "(1 + i)-ary gcd algorithm" for Gaussian
integers. Let  := 1 + i  Z[i]. The algorithm takes non-zero ,   Z[i] as input,
and runs as follows:

       ,   , e  0
     while  |  and  |  do   /,    /, e  e + 1

     repeat

     while  |  do   /

     while  |  do    /

     if M ( ) < M () then (,  )  ( , )

     determine   {±1, ±i} such that    (mod 2)

()      - 

     until  = 0

       e · 

     output 

Show that this algorithm correctly computes a greatest common divisor of 
and , and that it can be implemented so as to run in time O( 2), where :=
max(len(M ()), len(M ())). Hint: to analyze the running time, for i = 1, 2, . . . ,

let vi (respectively, vi) denote the value of | | just before (respectively, after) the
execution of the line marked () in loop iteration i, and show that

                                        
                 vi  (1 + 2)vi and vi+1  vi /2 2.

EXERCISE 16.43. Extend the algorithm of the previous exercise, so that it com-
putes ,   Z[i] such that  +  = . Your algorithm should run in time O( 2),
and it should also be the case that len(M ()) and len(M ()) are O( ). Hint: adapt

the algorithm in Exercise 4.10.

EXERCISE 16.44. In Exercise 16.41, we saw that 2 factors as -i(1 + i)2 in Z[i],
16.9 Unique factorization domains ()                             459

where 1 + i is irreducible. This exercise examines the factorization in Z[i] of prime
numbers p > 2. Show that:

(a) for every irreducible   Z[i], there exists a unique prime number p such
     that  divides p;

(b) for all prime numbers p  1 (mod 4), we have p = , where   Z[i]
     is irreducible, and the complex conjugate  of  is also irreducible and not

     associate to ;

    (c) all prime numbers p  3 (mod 4) are irreducible in Z[i].
Hint: for parts (b) and (c), use Theorem 2.34.

                         16.9.2 Unique factorization in D[X ]

In this section, we prove the following:

Theorem 16.43. If D is a UFD, then so is D[X ].

  This theorem implies, for example, that Z[X ] is a UFD. Applying the theorem
inductively, one also sees that Z[X1, . . . , Xn] is a UFD, as is F [X1, . . . , Xn] for every
field F .

   We begin with some simple observations. First, recall that for an integral domain
D, D[X ] is an integral domain, and the units in D[X ] are precisely the units in D.
Second, it is easy to see that an element of D is irreducible in D if and only if it is
irreducible in D[X ]. Third, for c  D and f = i ciX i  D[X ], we have c | f if
and only if c | ci for all i.

  We call a non-zero polynomial f  D[X ] primitive if the only elements of
D that divide f are units. If D is a UFD, then given any non-zero polynomial
f  D[X ], we can write it as f = cf , where c  D and f  D[X ] is a primitive
polynomial: just take c to be a greatest common divisor of all the coefficients of f.

Example 16.25. In Z[X ], the polynomial f = 4X 2 + 6X + 20 is not primitive, but
we can write f = 2f , where f = 2X 2 + 3X + 10 is primitive. 2

   It is easy to prove the existence part of Theorem 16.43:

Theorem 16.44. Let D be a UFD. Every non-zero, non-unit element of D[X ] can
be expressed as a product of irreducibles in D[X ].

Proof. Let f be a non-zero, non-unit polynomial in D[X ]. If f is a constant, then
because D is a UFD, f factors into irreducibles in D. So assume f is not constant.
If f is not primitive, we can write f = cf , where c is a non-zero, non-unit in D,
and f is a primitive, non-constant polynomial in D[X ]. Again, as D is a UFD, c
factors into irreducibles in D.
460  More rings

   From the above discussion, it suffices to prove the theorem for non-constant,
primitive polynomials f  D[X ]. If f is itself irreducible, we are done. Otherwise,
we can write f = gh, where g, h  D[X ] and neither g nor h are units. Further, by
the assumption that f is a primitive, non-constant polynomial, both g and h must
also be primitive, non-constant polynomials; in particular, both g and h have degree
strictly less than deg(f ), and the theorem follows by induction on degree. 2

   The uniqueness part of Theorem 16.43 is (as usual) more difficult. We begin
with the following fact:

Theorem 16.45. Let D be a UFD, let p be an irreducible in D, and let g, h  D[X ].
Then p | gh implies p | g or p | h.

Proof. Consider the quotient ring D/pD, which is an integral domain (because
D is a UFD), and the corresponding ring of polynomials (D/pD)[X ], which is
also an integral domain. Also consider the natural map that sends a  D to
a := [a]p  D/pD, which we can extend coefficient-wise to a ring homomorphism
from D[X ] to (D/pD)[X ] (see Example 7.46). If p | gh, then we have

                                         0 = gh = gh,

and since (D/pD)[X ] is an integral domain, it follows that g = 0 or h = 0, which
means that p | g or p | h. 2

Theorem 16.46. Let D be a UFD. The product of two primitive polynomials in
D[X ] is also primitive.

Proof. Let g, h  D[X ] be primitive polynomials, and let f := gh. If f is not
primitive, then c | f for some non-zero, non-unit c  D, and as D is a UFD, there
is some irreducible element p  D that divides c, and therefore, divides f as well.
By Theorem 16.45, it follows that p | g or p | h, which implies that either g is not
primitive or h is not primitive. 2

   Suppose that D is a UFD and that F is its field of fractions. Any non-zero
polynomial f  F [X ] can always be written as f = (c/d)f , where c, d  D,
with d = 0, and f  D[X ] is primitive. To see this, clear the denominators of the
coefficients of f, writing df = f , where 0 = d  D and f  D[X ]. Then take c
to be a greatest common divisor of the coefficients of f , so that f = cf , where
f  D[X ] is primitive. Then we have f = (c/d)f , as required. Of course, we

may assume that c and d are relatively prime -- if not, we may divide c and d by a

greatest common divisor.

Example 16.26. Let f = (3/5)X 2 + 9X + 3/2  Q[X ]. Then we can write
f = (3/10)f , where f = 2X 2 + 30X + 5  Z[X ] is primitive. 2
16.9 Unique factorization domains ()                461

As a consequence of the previous theorem, we have:

Theorem 16.47. Let D be a UFD and let F be its field of fractions. Suppose that
f, g  D[X ] and h  F [X ] are non-zero polynomials such that f = gh and g is

primitive. Then h  D[X ].

Proof. Write h = (c/d)h , where c, d  D and h  D[X ] is primitive. Let us
assume that c and d are relatively prime. Then we have

d · f = c · gh .                                    (16.9)

   We claim that d  D. To see this, note that (16.9) implies that d | (c · gh ),
and the assumption that c and d are relatively prime implies that d | gh . But by
Theorem 16.46, gh is primitive, from which it follows that d is a unit. That proves
the claim.

  It follows that c/d  D, and hence h = (c/d)h  D[X ]. 2

Theorem 16.48. Let D be a UFD and F its field of fractions. If f  D[X ] with
deg(f ) > 0 is irreducible, then f is also irreducible in F [X ].

Proof. Suppose that f is not irreducible in F [X ], so that f = gh for non-constant
polynomials g, h  F [X ], both of degree strictly less than that of f. We may write
g = (c/d)g , where c, d  D and g  D[X ] is primitive. Set h := (c/d)h, so that
f = gh = g h . By Theorem 16.47, we have h  D[X ], and this shows that f is
not irreducible in D[X ]. 2

Theorem 16.49. Let D be a UFD. Let f  D[X ] with deg(f ) > 0 be irreducible,
and let g, h  D[X ]. If f divides gh in D[X ], then f divides either g or h in
D[X ].

Proof. Suppose that f  D[X ] with deg(f ) > 0 is irreducible. This implies that f
is a primitive polynomial. By Theorem 16.48, f is irreducible in F [X ], where F is
the field of fractions of D. Suppose f divides gh in D[X ]. Then because F [X ] is
a UFD, f divides either g or h in F [X ]. But Theorem 16.47 implies that f divides
either g or h in D[X ]. 2

   Theorem 16.43 now follows immediately from Theorems 16.44, 16.45, and
16.49, together with Theorem 16.34.

   In the proof of Theorem 16.43, there is a clear connection between factorization
in D[X ] and F [X ], where F is the field of fractions of D. We should perhaps make
this connection more explicit. Let f  D[X ] be a non-zero polynomial. We may
write f as

                                 f = up1a1 · · · prar f1b1 · · · fsbs .
462                                More rings

where u  D, the pi's are non-associate, irreducible elements of D, and the fj's
are non-associate, irreducible, non-constant polynomials over D (and in particular,
primitive). For j = 1, . . . , s, let gj := lc(fj)-1fj be the monic associate of fj in
F [X ]. Then in F [X ], f factors as

                                       f = cg1b1 · · · gsbs ,

where

       c := u · piai · lc(fj)bj  F ,

                                i  j

and the gj's are distinct, irreducible, monic polynomials over F .

Example 16.27. Consider the polynomial f = 4X 2 + 2X - 2  Z[X ]. Over Z[X ],
f factors as 2(2X - 1)(X + 1), where each of these three factors is irreducible in
Z[X ]. However, over Q[X ], f factors as 4(X - 1/2)(X + 1), where 4 is a unit, and
the other two factors are irreducible. 2

   The following theorem provides a useful criterion for establishing that a polyno-
mial is irreducible.

Theorem 16.50 (Eisenstein's criterion). Let D be a UFD and F its field of frac-
tions. Let f = cnX n + cn-1X n-1 + · · · + c0  D[X ]. If there exists an irreducible
p  D such that

                            p cn, p | cn-1, · · · , p | c0, p2 c0,

then f is irreducible over F .

Proof. Let f be as above, and suppose it were not irreducible in F [X ]. Then by
Theorem 16.48, we could write f = gh, where g, h  D[X ], both of degree strictly

less than that of f . Let us write

       g = akX k + · · · + a0 and h = b X + · · · + b0,

where ak = 0 and b = 0, so that 0 < k < n and 0 < < n. Now, since cn = akb ,
and p cn, it follows that p ak and p b . Further, since c0 = a0b0, and p | c0 but
p2 c0, it follows that p divides one of a0 or b0, but not both -- for concreteness, let

us assume that p | a0 but p b0. Also, let m be the smallest positive integer such

that p am -- note that 0 < m  k < n.
   Now consider the natural map that sends a  D to a := [a]p  D/pD, which

we can extend coefficient-wise to a ring homomorphism from D[X ] to (D/pD)[X ]

(see Example 7.46). Because D is a UFD and p is irreducible, D/pD is an integral
domain. Since f = gh, we have

       cnX n = f = gh = (akX k + · · · + amX m)(b X + · · · + b0).  (16.10)
                        16.9 Unique factorization domains ()            463

But notice that when we multiply out the two polynomials on the right-hand side
of (16.10), the coefficient of X m is amb0 = 0, and as m < n, this clearly contradicts
the fact that the coefficient of X m in the polynomial on the left-hand side of (16.10)

is zero. 2

As an application of Eisenstein's criterion, we have:

Theorem 16.51. For every prime number q, the qth cyclotomic polynomial
                       q := X q - 1 = X q-1 + X q-2 + · · · + 1
                                X -1

is irreducible over Q.

Proof. Let

                        f := q X + 1 = (X + 1) (X + 1) - 1 . q - 1

It is easy to see that

                   q-1               q  (i = 0, . . . , q - 1).
                                   i+1
            f = ciX i, where ci =

                    i=0

Thus, cq-1 = 1, c0 = q, and for 0 < i < q - 1, we have q | ci (see Exercise 1.14).

Theorem 16.50 therefore applies, and we conclude that f is irreducible over Q. It
follows that q is irreducible over Q, since if q = gh were a non-trivial factoriza-
tion of q, then f = q X + 1 = g X + 1 · h X + 1 would be a non-trivial

factorization of f. 2

EXERCISE 16.45. Show that neither Z[X ] nor F [X , Y ] (where F is a field) are
PIDs (even though they are UFDs).

EXERCISE 16.46. Let f  Z[X ] be a monic polynomial. Show that if f has a root
x  Q, then x  Z, and x divides the constant term of f .

EXERCISE 16.47. Let D be a UFD, let p be an irreducible element of D, and
consider the natural map that sends a  D to a := [a]p  D/pD, which we
extend coefficient-wise to a ring homomorphism from D[X ] to (D/pD)[X ] (see
Example 7.46). Show that if f  D[X ] is a primitive polynomial such that p lc(f )
and f  (D/pD)[X ] is irreducible, then f is irreducible.

EXERCISE 16.48. Let a be a non-zero, square-free integer, with a / {±1}, and let
n be a positive integer. Show that the polynomial X n - a is irreducible in Q[X ].

EXERCISE 16.49. Show that the polynomial X 4 + 1 is irreducible in Q[X ].
464  More rings

EXERCISE 16.50. Let F be a field, and consider the ring of bivariate polynomials
F [X , Y ]. Show that in this ring, the polynomial X 2 + Y 2 - 1 is irreducible, provided

F does not have characteristic 2. What happens if F has characteristic 2?

EXERCISE 16.51. Design and analyze an efficient algorithm for the following
problem. The input is a pair of polynomials g, h  Z[X ], along with their greatest
common divisor d in the ring Q[X ]. The output is the greatest common divisor of
g and h in the ring Z[X ].

EXERCISE 16.52. Let g, h  Z[X ] be non-zero polynomials with d := gcd(g, h) 
Z[X ]. Show that for every prime p not dividing lc(g) lc(h), we have d | gcd(g, h),
and except for finitely many primes p, we have d = gcd(g, h). Here, d, g, and h
denote the images of d, g, and h in Zp[X ] under the coefficient-wise extension of
the natural map from Z to Zp (see Example 7.47).

EXERCISE 16.53. Let F be a field, and let g, h  F [X , Y ]. Define V (g, h) :=
{(x, y)  F × F : g(x, y) = h(x, y) = 0}. Show that if g and h are relatively prime,

then V (g, h) is a finite set. Hint: consider the rings F (X )[Y ] and F (Y )[X ].

                                          16.10 Notes
The "(1 + i)-ary gcd algorithm" in Exercise 16.42 for computing greatest common
divisors of Gaussian integers is based on algorithms in Weilert [106] and Damgård
and Frandsen [31]. The latter paper also develops a corresponding algorithm for
Eisenstein integers (see Exercise 16.35). Weilert [107] presents an asymptotically
fast algorithm that computes the greatest common divisor of Gaussian integers of
length at most in time O( 1+o(1)).
                             17

          Polynomial arithmetic and applications

In this chapter, we study algorithms for performing arithmetic on polynomials.

Initially, we shall adopt a very general point of view, discussing polynomials whose
coefficients lie in an arbitrary ring R, and then specialize to the case where the
coefficient ring is a field F .

  There are many similarities between arithmetic in Z and in R[X ], and the simi-
larities between Z and F [X ] run even deeper. Many of the algorithms we discuss
in this chapter are quite similar to the corresponding algorithms for integers.

   As we did in Chapter 14 for matrices, we shall treat R as an "abstract data

type," and measure the complexity of algorithms for polynomials over a ring R by

counting "operations in R."

          17.1 Basic arithmetic

Throughout this section, R denotes a non-trivial ring.

  For computational purposes, we shall assume that a polynomial g = ik=0-1 aiX i 
R[X ] is represented as a coefficient vector (a0, a1, . . . , ak-1). Further, when g is
non-zero, the coefficient ak-1 should be non-zero.

The basic algorithms for addition, subtraction, multiplication, and division of

polynomials are quite straightforward adaptations of the corresponding algorithms

for integers. In fact, because of the lack of "carries," these algorithms are actually

much simpler in the polynomial case. We briefly discuss these algorithms here --

analogous to our treatment of integer arithmetic, we do not discuss the details of
"stripping" leading zero coefficients.

   For addition and subtraction, all we need to do is to add or subtract coefficient

vectors.

For multiplication, let g = ik=0-1 aiX i  R[X ] and h = i=0 -1 biX i  R[X ],
where k  1 and  1. The product f := g · h is of the form f =  k+ -2 i

                                                              i=0 ciX , the
coefficients of which can be computed using O(k ) operations in R as follows:

          465
466  Polynomial arithmetic and applications

     for i  0 to k + - 2 do ci  0
     for i  0 to k - 1 do

           for j  0 to - 1 do
                  ci+j  ci+j + ai · bj

  For division, let g = ik=0-1 aiX i  R[X ] and h = i=0 -1 biX i  R[X ], where
b -1  R. We want to compute polynomials q, r  R[X ] such that g = hq + r,
where deg(r) < - 1. If k < , we can simply set q  0 and r  g; otherwise, we
can compute q and r using O( · (k - + 1)) operations in R using the following

algorithm:

     t  b -1 -1  R
     for i  0 to k - 1 do ri  ai
     for i  k - down to 0 do

           qi  t · ri+ -1
           for j  0 to - 1 do

                  ri+j  ri+j - qi · bj

     q  ik=0- qiX i, r  i=0 -2 riX i

   With these simple algorithms, we obtain the polynomial analog of Theorem 3.3.
Let us define the length of g  R[X ], denoted len(g), to be the length of its coeffi-

cient vector; more precisely, we define

     len(g) :=  deg(g) + 1 if g = 0,

                1                       if g = 0.

Sometimes (but not always) it is clearer and more convenient to state the running
times of algorithms in terms of the length, rather than the degree, of a polynomial
(the latter has the inconvenient habit of taking on the value 0, or worse, -).

Theorem 17.1. Let g and h be arbitrary polynomials in R[X ].
    (i) We can compute g ± h with O(len(g) + len(h)) operations in R.

   (ii) We can compute g · h with O(len(g) len(h)) operations in R.
  (iii) If lc(h)  R, we can compute q, r  R[X ] such that g = hq + r and

         deg(r) < deg(h) with O(len(h) len(q)) operations in R.

   Analogous to algorithms for modular integer arithmetic, we can also do arith-

metic in the residue class ring R[X ]/(f ), where f  R[X ] is a polynomial with
lc(f )  R. For each   R[X ]/(f ), there exists a unique polynomial g  R[X ]
with deg(g) < deg(f ) and  = [g]f ; we call this polynomial g the canonical
representative of , and denote it by rep(). For computational purposes, we

represent elements of R[X ]/(f ) by their canonical representatives.
17.1 Basic arithmetic  467

  With this representation, addition and subtraction in R[X ]/(f ) can be performed
using O(len(f )) operations in R, while multiplication takes O(len(f )2) operations

in R.

   The repeated-squaring algorithm for computing powers works equally well in
this setting: given   R[X ]/(f ) and a non-negative exponent e, we can compute
e using O(len(e)) multiplications in R[X ]/(f ), for a total of O(len(e) len(f )2)
operations in R.

EXERCISE 17.1. State and re-work the polynomial analogs of Exercises 3.26-
3.28.

EXERCISE 17.2. Given a polynomial g  R[X ] and an element x  R, a particu-
larly elegant and efficient way of computing g(x) is called Horner's rule. Suppose
g = ik=0-1 aiX i, where k  0 and ai  R for i = 0, . . . , k - 1. Horner's rule
computes g(x) as follows:

         y  0R
         for i  k - 1 down to 0 do

               y  yx + ai
         output y

Show that this algorithm correctly computes g(x) using k multiplications in R and
k additions in R.

EXERCISE 17.3. Let f  R[X ] be a polynomial of degree > 0 with lc(f )  R,
and let E := R[X ]/(f ). Suppose that in addition to f, we are given a polynomial
g  R[X ] of degree less than k and an element   E, and we want to compute
g()  E. This is called the modular composition problem.

    (a) Show that a straightforward application of Horner's rule yields an algo-
         rithm that uses O(k 2) operations in R, and requires space for storing O( )
         elements of R.

   (b) Show how to compute g() using just O(k + k1/2 2) operations in R, at
         the expense of requiring space for storing O(k1/2 ) elements of R. Hint:
         first compute a table of powers 1, , . . . , m, for m  k1/2.

EXERCISE 17.4. Given polynomials g, h  R[X ], show how to compute their
composition g(h)  R[X ] using O(len(g)2 len(h)2) operations in R.

EXERCISE 17.5. Suppose you are given three polynomials f , g, h  Zp[X ],
where p is a large prime, in particular, p  2 deg(g) deg(h). Design an effi-
cient probabilistic algorithm that tests if f = g(h) (i.e., if f equals g composed
with h). Your algorithm should have the following properties: if f = g(h), it
468                    Polynomial arithmetic and applications

should always output "true," and otherwise, it should output "false" with prob-

ability at least 0.999. The expected running time of your algorithm should be
O((len(f ) + len(g) + len(h)) len(p)2).

EXERCISE 17.6. Let x, a0, . . . , a -1  R, and let k be an integer with 0 < k  .
                                      i+k-1
For i = 0, . . . , - k, define gi :=  j=i    ajX j    R[X ].  Show  how  to  compute  the

   - k + 1 values g0(x), . . . , g -k(x) using O( ) operations in R.

           17.2 Computing minimal polynomials in F [X ]/(f ) (I)

In this section, we shall examine a computational problem to which we shall return

on several occasions, as it will serve to illustrate a number of interesting algebraic

and algorithmic concepts.

  Let F be a field, and let f  F [X ] be a monic polynomial of degree > 0. Also,
let E := F [X ]/(f ), which is an F -algebra, and in particular, an F -vector space.

As an F -vector space, E has dimension . Suppose we are given an element   E,
and want to efficiently compute the minimal polynomial of  over F -- that is, the
monic polynomial   F [X ] of least degree such that () = 0, which we know

has degree at most (see §16.5).

   We can solve this problem using polynomial arithmetic and Gaussian elimi-

nation, as follows. Consider the F -linear map  : F [X ]  E that sends a

polynomial g  F [X ] of degree at most to g(). To perform the linear algebra,

we need to specify bases for F [X ] and E. For F [X ] , let us work with the basis
           +1-i +1
S  :=  {X              With this choice of basis, for g  =    i=0 aiX i  F [X ] , the
               }i=1 .
coordinate vector of g is VecS (g) = (a , . . . , a0)  F 1×( +1). For E, let us work

with the basis T := { i i-1} =1, where  := [X ]f  E. Let

                           A := MatS,T ()  F ( +1)× ;

that is, A is the matrix of  relative to S and T (see §14.2). For i = 1, . . . , + 1,
the ith row of A is the coordinate vector VecT ( +1-i)  F 1× .

   We compute the matrix A by computing the powers 1, , . . . ,  , reading off the
ith row of A directly from the canonical representative of the  +1-i. We then
apply Gaussian elimination to A to find row vectors v1, . . . , vs  F 1×( +1) that are

coordinate vectors corresponding to a basis for the kernel of . Now, the coordinate

vector of the minimal polynomial of  is a linear combination of v1, . . . , vs. To find
it, we form the s × ( + 1) matrix B whose rows consist of v1, . . . , vs, and apply
Gaussian elimination to B, obtaining an s×( +1) matrix B in reduced row echelon

form whose row space is the same as that of B. Let  be the polynomial whose

coordinate vector is the last row of B .

  Because of the choice of basis for F [X ] , and because B is in reduced row
17.3 Euclid's algorithm  469

echelon form, it is clear that no non-zero polynomial in Ker  has degree less than

that of . Moreover, as  is already monic (again, by the fact that B is in reduced

row echelon form), it follows that  is in fact the minimal polynomial of  over F .
   The total amount of work performed by this algorithm is O( 3) operations in F

to build the matrix A (this just amounts to computing successive powers of ,
that is, O( ) multiplications in E, each of which takes O( 2) operations in F ), and
O( 3) operations in F to perform both Gaussian elimination steps.

                                  17.3 Euclid's algorithm

In this section, F denotes a field, and we consider the computation of greatest
common divisors in F [X ].

   The Euclidean algorithm for integers is easily adapted to compute gcd(g, h)
for polynomials g, h  F [X ]. Analogous to the integer case, we assume that
deg(g)  deg(h); however, we shall also assume that g = 0. This is not a serious
restriction, of course, as gcd(0, 0) = 0, and making this restriction will simplify
the presentation a bit. Recall that we defined gcd(g, h) to be either zero or monic,
and the assumption that g = 0 means that gcd(g, h) is non-zero, and hence monic.

   The following is the analog of Theorem 4.1, and is based on the division with
remainder property for polynomials.

Theorem 17.2. Let g, h  F [X ], with deg(g)  deg(h) and g = 0. Define the
polynomials r0, r1, . . . , r+1  F [X ] and q1, . . . , q  F [X ], where   0, as
follows:

                      g = r0,
                      h = r1,
                     r0 = r1q1 + r2 (0  deg(r2) < deg(r1)),

                         ..
                         .
                   ri-1 = riqi + ri+1 (0  deg(ri+1) < deg(ri)),
                         ..
                         .
                   r-2 = r-1q-1 + r (0  deg(r) < deg(r-1)),
                   r-1 = rq (r+1 = 0).

Note that by definition,  = 0 if h = 0, and  > 0 otherwise. Then we have
r/ lc(r) = gcd(g, h), and if h = 0, then   deg(h) + 1.

Proof. Arguing as in the proof of Theorem 4.1, one sees that

      gcd(g, h) = gcd(r0, r1) = · · · = gcd(r, r+1) = gcd(r, 0) = r/ lc(r).
470  Polynomial arithmetic and applications

That proves the first statement.
   For the second statement, if h = 0, then the degree sequence

                                deg(r1), deg(r2), . . . , deg(r)

is strictly decreasing, with deg(r)  0, from which it follows that deg(h) =
deg(r1)   - 1. 2

   This gives us the following polynomial version of the Euclidean algorithm:

Euclid's algorithm. On input g, h, where g, h  F [X ] with deg(g)  deg(h) and
g = 0, compute d = gcd(g, h) as follows:

         r  g, r  h
         while r = 0 do

               r  r mod r
               (r, r )  (r , r )
         d  r/ lc(r) // make monic
         output d

Theorem 17.3. Euclid's algorithm for polynomials performs O(len(g) len(h))
operations in F .

Proof. The proof is almost identical to that of Theorem 4.2. Details are left to the
reader. 2

  Just as for integers, if d = gcd(g, h), then gF [X ] + hF [X ] = dF [X ], and so there
exist polynomials s and t such that gs + ht = d. The procedure for calculating s
and t is precisely the same as in the integer case; however, in the polynomial case,
we can be much more precise about the relative sizes of the objects involved in the
calculation.

Theorem 17.4. Let g, h, r0, . . . , r+1 and q1, . . . , q be as in Theorem 17.2. Define
polynomials s0, . . . , s+1  F [X ] and t0, . . . , t+1  F [X ] as follows:

                                       s0 := 1, t0 := 0,
                                       s1 := 0, t1 := 1,
and for i = 1, . . . , ,

                          si+1 := si-1 - siqi, ti+1 := ti-1 - tiqi.

Then:
    (i) for i = 0, . . . ,  + 1, we have gsi + hti = ri; in particular, gs + ht =
         lc(r) gcd(g, h);
   (ii) for i = 0, . . . , , we have siti+1 - tisi+1 = (-1)i;
17.3 Euclid's algorithm                                  471

(iii) for i = 0, . . . ,  + 1, we have gcd(si, ti) = 1;
(iv) for i = 1, . . . ,  + 1, we have

deg(ti) = deg(g) - deg(ri-1),

and for i = 2, . . . ,  + 1, we have

deg(si) = deg(h) - deg(ri-1);

(v) for i = 1, . . . ,  + 1, we have deg(ti)  deg(g) and deg(si)  deg(h); if
     deg(g) > 0 and h = 0, then deg(t) < deg(g) and deg(s) < deg(h).

Proof. (i), (ii), and (iii) are proved just as in the corresponding parts of Theo-
rem 4.3.

   For (iv), the proof will hinge on the following facts:
     · For i = 1, . . . , , we have deg(ri-1)  deg(ri), and since qi is the quotient
         in dividing ri-1 by ri, we have deg(qi) = deg(ri-1) - deg(ri).
     · For i = 2, . . . , , we have deg(ri-1) > deg(ri).

   We prove the statement involving the ti's by induction on i, and leave the proof
of the statement involving the si's to the reader.

   One can see by inspection that this statement holds for i = 1, since deg(t1) = 0
and r0 = g. If  = 0, there is nothing more to prove, so assume that  > 0 and
h = 0.

   Now, for i = 2, we have t2 = 0 - 1 · q1 = -q1. Thus, deg(t2) = deg(q1) =
deg(r0) - deg(r1) = deg(g) - deg(r1).

   Now for the induction step. Assume i  3. Then we have

         deg(ti-1qi-1) = deg(ti-1) + deg(qi-1)
                          = deg(g) - deg(ri-2) + deg(qi-1) (by induction)
                          = deg(g) - deg(ri-1)
                              (since deg(qi-1) = deg(ri-2) - deg(ri-1))

                          > deg(g) - deg(ri-3) (since deg(ri-3) > deg(ri-1))
                          = deg(ti-2) (by induction).

   By definition, ti = ti-2 - ti-1qi-1, and from the above reasoning, we see that

                    deg(g) - deg(ri-1) = deg(ti-1qi-1) > deg(ti-2),

from which it follows that deg(ti) = deg(g) - deg(ri-1).
   (v) follows easily from (iv). 2

From this theorem, we obtain the following algorithm:
472                 Polynomial arithmetic and applications

The extended Euclidean algorithm. On input g, h, where g, h  F [X ] with
deg(g)  deg(h) and g = 0, compute d, s, and t, where d, s, t  F [X ], d = gcd(g, h)
and gs + ht = d, as follows:

         r  g, r  h
         s  1, s  0
         t  0, t  1
         while r = 0 do

               compute q, r such that r = r q + r , with deg(r ) < deg(r )
               (r, s, t, r , s , t )  (r , s , t , r , s - s q, t - t q)
         c  lc(r)
         d  r/c, s  s/c, t  t/c // make monic
         output d, s, t

Theorem 17.5. The extended Euclidean algorithm for polynomials performs
O(len(g) len(h)) operations in F .

Proof. Exercise. 2

EXERCISE 17.7. State and re-work the polynomial analogs of Exercises 4.2, 4.3,
4.4, 4.5, and 4.8.

          17.4 Computing modular inverses and Chinese remaindering

In this and the remaining sections of this chapter, we explore various applications
of Euclid's algorithm for polynomials. Most of these applications are analogous
to their integer counterparts, although there are some differences to watch for.
Throughout this section, F denotes a field.

   We begin with the obvious application of the extended Euclidean algorithm for
polynomials to the problem of computing multiplicative inverses in F [X ]/(f ).

Theorem 17.6. Suppose we are given polynomials f, h  F [X ], where deg(h) <
deg(f ). Then using O(len(f )2) operations in F , we can determine if h is relatively
prime to f , and if so, compute h-1 mod f .

Proof. We may assume deg(f ) > 0, since deg(f ) = 0 implies h = 0 = h-1 mod f .
We run the extended Euclidean algorithm on input f , h, obtaining polynomials
d, s, t such that d = gcd(f , h) and f s + ht = d. If d = 1, then h does not have
a multiplicative inverse modulo f. Otherwise, if d = 1, then t is a multiplica-
tive inverse of h modulo f . Moreover, by part (v) of Theorem 17.4, we have
deg(t) < deg(f ), and so t = h-1 mod f . Based on Theorem 17.5, it is clear that
all the computations can be performed using O(len(f )2) operations in F . 2
17.4 Computing modular inverses and Chinese remaindering              473

   We also observe that the Chinese remainder theorem for polynomials (Theo-
rem 16.19) can be made computationally effective as well:

Theorem 17.7 (Effective Chinese remainder theorem). Suppose we are given
polynomials f1, . . . , fk  F [X ] and g1, . . . , gk  F [X ], where the family {fi}ik=1
is pairwise relatively prime, and where deg(fi) > 0 and deg(gi) < deg(fi) for
i = 1, . . . , k. Let f := ik=1 fi. Then using O(len(f )2) operations in F , we
can compute the unique polynomial g  F [X ] satisfying deg(g) < deg(f ) and
g  gi (mod fi) for i = 1, . . . , k.

Proof. Exercise (just use the formulas given after Theorem 16.19). 2

                                  Polynomial interpolation

We remind the reader of the discussion following Theorem 16.19, where the point
was made that when fi = X - xi and gi = yi, for i = 1, . . . , k, then the Chinese
remainder theorem for polynomials reduces to Lagrange interpolation. Thus, The-
orem 17.7 says that given distinct elements x1, . . . , xk  F , along with elements
y1, . . . , yk  F , we can compute the unique polynomial g  F [X ] of degree less
than k such that

                                 g(xi) = yi (i = 1, . . . , k),

using O(k2) operations in F .
   It is perhaps worth noting that we could also solve the polynomial interpolation

problem using Gaussian elimination, by inverting the corresponding Vandermonde
matrix (see Example 14.2). However, this algorithm would use O(k3) operations
in F . This is a specific instance of a more general phenomenon: there are many
computational problems involving polynomials over fields that can be solved using
Gaussian elimination, but which can be solved more efficiently using more special-
ized algorithmic techniques.

                  Speeding up algorithms via modular computation

In §4.4, we discussed how the Chinese remainder theorem could be used to speed
up certain types of computations involving integers. The example we gave was the
multiplication of integer matrices. We can use the same idea to speed up certain
types of computations involving polynomials. For example, if one wants to mul-
tiply two matrices whose entries are elements of F [X ], one can use the Chinese
remainder theorem for polynomials to speed things up. This strategy is most easily
implemented if F is sufficiently large, so that we can use polynomial evaluation
474                       Polynomial arithmetic and applications

and interpolation directly, and do not have to worry about constructing irreducible
polynomials.

EXERCISE 17.8. Adapt the algorithms of Exercises 4.14 and 4.15 to obtain an
algorithm for polynomial interpolation. This algorithm is called Newton interpo-
lation.

              17.5 Rational function reconstruction and applications

Throughout this section, F denotes a field.
   We next state and prove the polynomial analog of Theorem 4.9. As we are

now "reconstituting" a rational function, rather than a rational number, we call this
procedure rational function reconstruction. Because of the relative simplicity of
polynomials compared to integers, the rational reconstruction theorem for polyno-
mials is a bit "sharper" than the rational reconstruction theorem for integers, and
much simpler to prove.

   To state the result precisely, let us introduce some notation. For polynomials
g, h  F [X ] with deg(g)  deg(h) and g = 0, let us define

                               EEA(g, h) := (ri, si, ti) i=0 +1,
where ri, si, and ti, for i = 0, . . . ,  + 1, are defined as in Theorem 17.4.

Theorem 17.8 (Rational function reconstruction). Let f, h  F [X ] be polyno-
mials, and let r, t be non-negative integers, such that

               deg(h) < deg(f ) and r + t  deg(f ).

Further,  let  EEA(f, h)  =                    +1  and  let  j  be  the  smallest  index  (among

                             {(ri, si, ti)}i=0 ,
0, . . . ,  + 1) such that deg(rj) < r, and set

                          r := rj, s := sj, and t := tj.

Finally, suppose that there exist polynomials r, s, t  F [X ] such that
                   r = f s + ht, deg(r) < r, and 0  deg(t)  t.

Then for some non-zero polynomial q  F [X ], we have
                                  r = r q, s = s q, t = t q.

Proof. Since deg(r0) = deg(f )  r > - = deg(r+1), the value of j is well
defined, and moreover, j  1, deg(rj-1)  r, and tj = 0.
             17.5 Rational function reconstruction and applications  475

From the equalities rj = f sj +htj and r = f s+ht, we have the two congruences:

                                 rj  htj (mod f ),
                                 r  ht (mod f ).

Subtracting t times the first from tj times the second, we obtain

                                 rtj  rjt (mod f ).

This says that f divides rtj - rjt.
   We want to show that, in fact, rtj - rjt = 0. To this end, first observe that by part

(iv) of Theorem 17.4 and the inequality deg(rj-1)  r, we have

                      deg(tj) = deg(f ) - deg(rj-1)  deg(f ) - r.

Combining this with the inequality deg(r) < r, we see that

                          deg(rtj) = deg(r) + deg(tj) < deg(f ).

Furthermore, using the inequalities
                   deg(rj) < r, deg(t)  t, and r + t  deg(f ),

we see that

             deg(rjt) = deg(rj) + deg(t) < deg(f ),

and it immediately follows that

             deg(rtj - rjt) < deg(f ).

Since f divides rtj - rjt and deg(rtj - rjt) < deg(f ), the only possibility is that
                                         rtj - rjt = 0.

   The rest of the proof follows exactly the same line of reasoning as in the last
paragraph in the proof of Theorem 4.9, as the reader may easily verify. 2

 17.5.1 Application: recovering rational functions from their reversed Laurent
                                                  series

We now discuss the polynomial analog of the application in §4.6.1. This is an
entirely straightforward translation of the results in §4.6.1, but we shall see in the
next chapter that this problem has its own interesting applications.

  Suppose Alice knows a rational function z = s/t  F (X ), where s and t are
polynomials with deg(s) < deg(t), and tells Bob some of the high-order coeffi-
cients of the reversed Laurent series (see §16.8) representing z in F ((X -1)). We
shall show that if deg(t)  and Bob is given the bound on deg(t), along with the
476  Polynomial arithmetic and applications

high-order 2 coefficients of z, then Bob can determine z, expressed as a rational

function in lowest terms.

So suppose that z = s/t =       ziX  -i,  and  that  Alice     tells  Bob  the  coefficients
                           i=1

z1, . . . , z2 . Equivalently, Alice gives Bob the polynomial

     h := z1X 2 -1 + · · · + z2 -1X + z2 .

Also, let us define f := X 2 . Here is Bob's algorithm for recovering z:

     1. Run the extended Euclidean algorithm on input f, h to obtain EEA(f, h),
        and apply Theorem 17.8 with f , h, r := , and t := , to obtain the

        polynomials r , s , t .

     2. Output s , t .

  We claim that z = -s /t . To prove this, first observe that h = f z = f s/t
(see Theorem 16.32). So if we set r := f s mod t, then we have

     r = f s - ht, deg(r) < r, 0  deg(t)  t, and r + t  deg(f ).

It follows that the polynomials s , t from Theorem 17.8 satisfy s = s q and -t = t q
for some non-zero polynomial q, and thus, s /t = -s/t, which proves the claim.

   We may further observe that since the extended Euclidean algorithm guarantees
that gcd(s , t ) = 1, not only do we obtain z, but we obtain z expressed as a fraction

in lowest terms.
   It is clear that this algorithm takes O( 2) operations in F .

              17.5.2 Application: polynomial interpolation with errors

We now discuss the polynomial analog of the application in §4.6.2.
  If we "encode" a polynomial g  F [X ], with deg(g) < k, as the sequence

(y1, . . . , yk)  F ×k, where yi = g(xi), then we can efficiently recover g from this
encoding, using an algorithm for polynomial interpolation. Here, of course, the
xi's are distinct elements of F .

   Now suppose that Alice encodes g as (y1, . . . , yk), and sends this encoding to
Bob, but that some, say at most , of the yi's may be corrupted during transmission.
Let (z1, . . . , zk) denote the vector actually received by Bob.

   Here is how we can use Theorem 17.8 to recover the original value of g from
(z1, . . . , zk), assuming:

     · the original polynomial g has degree less than m,

     · at most errors occur in transmission, and
     · k  2 + m.
  Let us set fi := X - xi for i = 1, . . . , k, and f := f1 · · · fk. Now, suppose Bob
obtains the corrupted encoding (z1, . . . , zk). Here is what Bob does to recover g:
17.5 Rational function reconstruction and applications  477

1. Interpolate, obtaining a polynomial h, with deg(h) < k and h(xi) = zi for
   i = 1, . . . , k.

2. Run the extended Euclidean algorithm on input f, h to obtain EEA(f, h),
   and apply Theorem 17.8 with f , h, r := m + and t := , to obtain the

   polynomials r , s , t .

3. If t | r , output r /t ; otherwise, output "error."

   We claim that the above procedure outputs g, under the assumptions listed above.

To see this, let t be the product of the fi's for those values of i where an error
occurred. Now, assuming at most errors occurred, we have deg(t)  . Also, let
r := gt, and note that deg(r) < m + . We claim that

                                       r  ht (mod f ).  (17.1)
To show that (17.1) holds, it suffices to show that

gt  ht (mod fi)                                         (17.2)

for all i = 1, . . . , k. To show this, consider first an index i at which no error
occurred, so that yi = zi. Then gt  yit (mod fi) and ht  zit  yit (mod fi), and
so (17.2) holds for this i. Next, consider an index i for which an error occurred.
Then by construction, gt  0 (mod fi) and ht  0 (mod fi), and so (17.2) holds
for this i. Thus, (17.1) holds, from which it follows that the values r , t obtained
from Theorem 17.8 satisfy

                                       r = r = gt = g.
                                       ttt

   One easily checks that both the procedures to encode and decode a value g run in
time O(k2). The above scheme is an example of an error correcting code called

a Reed-Solomon code.

                        17.5.3 Applications to symbolic algebra
Rational function reconstruction has applications in symbolic algebra, analogous
to those discussed in §4.6.3. In that section, we discussed the application of solv-
ing systems of linear equations over the integers using rational reconstruction. In
exactly the same way, one can use rational function reconstruction to solve systems
of linear equations over F [X ] -- the solution to such a system of equations will be
a vector whose entries are elements of F (X ), the field of rational functions.

EXERCISE 17.9. Consider again the secret sharing problem, as discussed in Exam-
ple 8.28. There, we presented a scheme that distributes shares of a secret among
several parties in such a way that no coalition of k or fewer parties can reconstruct
478  Polynomial arithmetic and applications

the secret, while every coalition of k+1 parties can. Now suppose that some parties
may be corrupt: in the protocol to reconstruct the secret, a corrupted party may
contribute an incorrect share. Show how to modify the protocol in Example 8.28
so that if shares are distributed among several parties, then

(a) no coalition of k or fewer parties can reconstruct the secret, and
(b) if at most k parties are corrupt, then every coalition of 3k + 1 parties (which

     may include some of the corrupted parties) can correctly reconstruct the
     secret.

   The following exercises are the polynomial analogs of Exercises 4.20, 4.22, and
4.23.

EXERCISE 17.10. Let F be a field. Show that given polynomials s, t  F [X ] and
integer k, with deg(s) < deg(t) and k > 0, we can compute the kth coefficient in
the reversed Laurent series representing s/t using O(len(k) len(t)2) operations in

F.

EXERCISE 17.11. Let F be a field. Let z  F ((X -1)) be a reversed Laurent series
whose coefficient sequence is ultimately periodic. Show that z  F (X ).

EXERCISE 17.12. Let F be a field. Let z = s/t, where s, t  F [X ], deg(s) < deg(t),
and gcd(s, t) = 1.

    (a) Show that if F is finite, there exist integers k, k such that 0  k < k and
        sX k  sX k (mod t).

   (b) Show that for integers k, k with 0  k < k , the sequence of coefficients of
         the reversed Laurent series representing z is (k, k - k)-periodic if and only
        if sX k  sX k (mod t).

   (c) Show that if F is finite and X t, then the reversed Laurent series repre-
         senting z is purely periodic with period equal to the multiplicative order of
        [X ]t  (F [X ]/(t)).

   (d) More generally, show that if F is finite and t = X kt , with X t , then the
         reversed Laurent series representing z is ultimately periodic with pre-period
        k and period equal to the multiplicative order of [X ]t  (F [X ]/(t )).

                         17.6 Faster polynomial arithmetic ()
The algorithms discussed in §3.5 for faster integer arithmetic are easily adapted to
polynomials over a ring. Throughout this section, R denotes a non-trivial ring.

EXERCISE 17.13. State and re-work the analog of Exercise 3.41 for R[X ]. Your
17.6 Faster polynomial arithmetic ()                                          479

algorithm should multiply two polynomials over R of length at most using
O( log2 3) operations in R.

   It is in fact possible to multiply polynomials over R of length at most using
O( len( ) len(len( ))) operations in R -- we shall develop some of the ideas that
lead to such a result below in Exercises 17.21-17.24 (see also the discussion in
§17.7).

   In Exercises 17.14-17.19 below, assume that we have an algorithm that multi-
plies two polynomials over R of length at most using at most M ( ) operations in
R, where M is a well-behaved complexity function (as defined in §3.5).

EXERCISE 17.14. State and re-work the analog of Exercises 3.46 and 3.47 for
R[X ].

EXERCISE 17.15. This problem is the analog of Exercise 3.48 for R[X ]. Let

us first define the notion of a "floating point" reversed Laurent series z^, which

is represented as a pair (g, e), where g  R[X ] and e  Z -- the value of z^ is
gX e  R((X -1)), and we call len(g) the precision of z^. We say that z^ is a length
k approximation of z  R((X -1)) if z^ has precision k and z^ = (1 + )z for
  R((X -1)) with deg()  -k, which is the same as saying that the high-order k
coefficients of z^ and z are equal. Show that given h  R[X ] with lc(h)  R, and
positive integer k, we can compute a length k approximation of 1/h  R((X -1))

using O(M (k)) operations in R. Hint: using Newton iteration, show how to go

from a length t approximation of 1/h to a length 2t approximation, making use of
just the high-order 2t coefficients of h, and using O(M (t)) operations in R.

EXERCISE 17.16. State and re-work the analog of Exercise 3.49 for R[X ].

EXERCISE 17.17. State and re-work the analog of Exercise 3.50 for R[X ]. Con-
clude that a polynomial of length at most k can be evaluated at k points using
O(M (k) len(k)) operations in R.

EXERCISE 17.18. State and re-work the analog of Exercise 3.52 for R[X ], assum-
ing 2R  R.

   The next two exercises develop a useful technique known as Kronecker substi-
tution.

EXERCISE 17.19. Let g, h  R[X , Y ] with g = im=0-1 giY i and h = im=0-1 hiY i,

where each gi and hi is a polynomial in X of degree less than k. The product

f := gh  R[X , Y ] may be written f =  2m-2  fiY  i,  where  each  fi  is  a  polynomial
                                       i=0

in X . Show how to compute f, given g and h, using O(M (km)) operations in R.

Hint: for an appropriately chosen integer t > 0, first convert g, h to g~, h~  R[X ],
480             Polynomial arithmetic and applications

where g~ :=     m-1 giX ti and h~ :=     im=0-1 hiX ti; next, compute f~ := g~h~  R[X ];

                i=0
finally, "read off" the fi's from the coefficients of f~.

EXERCISE 17.20. Assume that integers of length at most can be multiplied in

time M ( ), where M is a well-behaved complexity function. Let g, h  Z[X ] with
g=   im=0-1 aiX i and h =  m-1
                           i=0  biX  i,  where  each  ai  and  bi  is  a  non-negative  integer,

strictly less than 2k. The product f := gh  Z[X ] may be written f =      2m-2 i

                                                                          i=0 ciX ,

where each ci is a non-negative integer. Show how to compute f , given g and h,

using O(M ((k + len(m))m)) operations in R. Hint: for an appropriately cho-

sen integer t > 0, first convert g, h to a, b  Z, where a := im=0-1 ai2ti and
b := im=0-1 bi2ti; next, compute c := ab  Z; finally, "read off" the ci's from

the bits of c.

   The following exercises develop an important algorithm for multiplying polyno-

mials in almost-linear time. For an integer n  0, let us call   R a primitive
2nth root of unity if n  1 and 2n-1 = -1R, or n = 0 and  = 1R; if 2R = 0R,
then in particular,  has multiplicative order 2n. For n  0, and   R a prim-
itive 2nth root of unity, let us define the R-linear map En, : R×2n  R×2n that
sends the vector (a0, . . . , a2n-1) to the vector (g(1R), g(), . . . , g(2n-1)), where

            2n-1 i

g := i=0 aiX  R[X ].

EXERCISE 17.21. Suppose 2R  R and   R is a primitive 2nth root of unity.
   (a) Let k be any integer, and consider gcd(k, 2n), which must be of the form
         2m for some m = 0, . . . , n. Show that k is a primitive 2n-mth root of unity.
   (b) Show that if n  1, then  - 1R  R.
   (c) Show that k - 1R  R for all integers k  0 (mod 2n).

   (d) Show that for every integer k, we have

                           2n-1          2Rn if k  0 (mod 2n),
                                         0R if k  0 (mod 2n).
                                ki =

                           i=0

(e) Let M2 be the 2-multiplication map on R×2n, which is a bijective, R-linear
     map. Show that

                            En,  En,-1 = M2n = En,-1  En,,

     and conclude that En, is bijective, with M2-nEn,-1 being its inverse. Hint:
     write down the matrices representing the maps En, and En,-1.

EXERCISE 17.22. This exercise develops a fast algorithm, called the fast Fourier
transform or FFT, for computing the function En,. This is a recursive algorithm
                  17.6 Faster polynomial arithmetic ()                                     481

FFT(n, ; a0, . . . , a2n-1) that takes as input an integer n  0, a primitive 2nth root
of unity   R, and elements a0, . . . , a2n-1  R, and runs as follows:

         if n = 0 then

               return a0
         else

               (0, . . . , 2n-1-1)  FFT(n - 1, 2; a0, a2, . . . , a2n-2)
               (0, . . . , 2n-1-1)  FFT(n - 1, 2; a1, a3, . . . , a2n-1)
               for i  0 to 2n-1 - 1 do

                      i  i + ii, i+2n-1  i - ii
               return (0, . . . , 2n-1)

Show that this algorithm correctly computes En,(a0, . . . , a2n-1) using O(2nn) oper-
ations in R.

EXERCISE 17.23. Assume 2R  R. Suppose that we are given two polyno-
mials g, h  R[X ] of length at most , along with a primitive 2nth root of unity
  R, where 2  2n < 4 . Let us "pad" g and h, writing g = i2=0n-1 aiX i
and h = i2=0n-1 biX i, where ai and bi are zero for i  . Show that the following
algorithm correctly computes the product of g and h using O( len( )) operations
in R:

         (0, . . . , 2n-1)  FFT(n, ; a0, . . . , a2n-1)
         (0, . . . , 2n-1)  FFT(n, ; b0, . . . , b2n-1)
         (0, . . . , 2n-1)  (00, . . . , 2n-12n-1)
         (c0, . . . , c2n-1)  2R-n FFT(n, -1; 0, . . . , 2n-1)

                           2 -2 i

        output i=0 ciX

Also, argue more carefully that the algorithm performs O( len( )) additions and
subtractions in R, O( len( )) multiplications in R by powers of , and O( ) other
multiplications in R.

EXERCISE 17.24. Assume 2R  R. In this exercise, we use the FFT to develop an
algorithm that multiplies polynomials over R of length at most using O( len( ) )

operations in R, where  is a constant. Unlike the previous exercise, we do not

assume that R contains any particular primitive roots of unity; rather, the algo-

rithm will create them "out of thin air." Suppose that g, h  R[X ] are of length

at most . Set k := /2 , m := /k . We may write g = im=0-1 giX ki and
h=  m-1
    i=0  hiX ki,  where  the  gi's  and  hi's  are  polynomials  of  length  at  most  k.  Let

n be the integer determined by 2m  2n < 4m. Let q := X 2n-1 + 1R  R[X ],

E := R[X ]/(q), and  := [X ]q  E.

(a) Show that  is a primitive 2nth root of unity in E, and that given an element
482  Polynomial arithmetic and applications

       E and an integer i between 0 and 2n - 1, we can compute i  E
     using O( 1/2) operations in R.
(b) Let g := im=0-1[gi]qY i  E[Y ] and h := im=0-1[hi]qY i  E[Y ]. Using
    the FFT (over E), show how to compute f := gh  E[Y ] by computing
    O( 1/2) products in R[X ] of polynomials of length O( 1/2), along with
     O( len( )) additional operations in R.
(c) Show how to compute the coefficients of f := gh  R[X ] from the value
    f  E[Y ] computed in part (b), using O( ) operations in R.

(d) Based on parts (a)-(c), we obtain a recursive multiplication algorithm: on
     inputs of length at most , it performs at most 0 len( ) operations in R,
     and calls itself recursively on at most 1 1/2 subproblems, each of length
     at most 2 1/2; here, 0, 1 and 2 are constants. If we just perform one
     level of recursion, and immediately switch to a quadratic multiplication
     algorithm, we obtain an algorithm whose operation count is O( 1.5). If we
     perform two levels of recursion, this is reduced to O( 1.25). For practical

     purposes, this is probably enough; however, to get an asymptotically better

     complexity bound, we can let the algorithm recurse all the way down to

     inputs of some (appropriately chosen) constant length. Show that if we do
     this, the operation count of the recursive algorithm is O( len( ) ) for some
     constant  (whose value depends on 1 and 2).

   The approach used in the previous exercise was a bit sloppy. With a bit more
care, one can use the same ideas to get an algorithm that multiplies polynomials
over R of length at most using O( len( ) len(len( ))) operations in R, assuming
2R  R. The next exercise applies similar ideas, but with a few twists, to the
problem of integer multiplication.

EXERCISE 17.25. This exercise uses the FFT to develop a linear-time algorithm

for integer multiplication; however, a rigorous analysis depends on an unproven

conjecture (which follows from a generalization of the Riemann hypothesis). Sup-

pose we want to multiply two positive integers a and b, each of length at most

(represented internally using the data structure described in §3.3). Throughout this

exercise, assume that all computations are done on a RAM, and that arithmetic

on integers of length O(len( )) takes time O(1). Let k be an integer parameter

with k = (len( )), and let m := /k . We may write a = im=0-1 ai2ki and
b=   m-1 bi2ki, where 0
                            ai  <  2k and 0    bi  <  2k .  Let n be the integer
     i=0
determined by 2m  2n < 4m.

(a) Assuming Conjecture 5.22, and assuming a deterministic, polynomial-time

     primality test (such as the one to be presented in Chapter 21), show how
     to efficiently generate a prime p  1 (mod 2n) and an element   Zp of
17.6 Faster polynomial arithmetic ()  483

multiplicative order 2n, such that

22km < p  O(1).

     Your algorithm should be probabilistic, and run in expected time polyno-
     mial in len( ).
(b) Assuming you have computed p and  as in part (a), let g := im=0-1[ai]pX i 
    Zp[X ] and h := im=0-1[bi]pX i  Zp[X ], and show how to compute f := gh 
    Zp[X ] in time O( ) using the FFT (over Zp). Here, you may store elements
     of Zp in single memory cells, so that operations in Zp take time O(1).
(c) Assuming you have computed f  Zp[X ] as in part (b), show how to obtain
     c := ab in time O( ).

(d) Conclude that assuming Conjecture 5.22, we can multiply two integers of
     length at most on a RAM in time O( ).

   Note that even if one objects to our accounting practices, and insists on charging
O(len( )2) time units for arithmetic on numbers of length O(len( )), the algorithm
in the previous exercise runs in time O( len( )2), which is "almost" linear time.

EXERCISE 17.26. Continuing with the previous exercise:

   (a) Show how the algorithm presented there can be implemented on a RAM
         that has only built-in addition, subtraction, and branching instructions, but
         no multiplication or division instructions, and still run in time O( ). Also,
         memory cells should store numbers of length at most len( ) + O(1). Hint:
         represent elements of Zp as sequences of base-2t digits, where t   len( )
         for some constant  < 1; use table lookup to multiply t-bit numbers, and to
         perform 2t-by-t-bit divisions -- for  sufficiently small, you can build these
         tables in time o( ).

   (b) Using Theorem 5.23, show how to make this algorithm fully deterministic
         and rigorous, assuming that on inputs of length , it is provided with a
         certain bit string  of length O(len( )) (this is called a non-uniform algo-
         rithm).

EXERCISE 17.27. This exercise shows how the algorithm in Exercise 17.25 can

be made quite concrete, and fairly practical, as well.
    (a) The number p := 25927 + 1 is a 64-bit prime. Show how to use this value
         of p in conjunction with the algorithm in Exercise 17.25 with k = 20 and
         any value of up to 227.
   (b) The numbers p1 := 2303 + 1, p2 := 22813 + 1, and p3 := 22729 + 1 are 32-
         bit primes. Show how to use the Chinese remainder theorem to modify the

         algorithm in Exercise 17.25, so that it uses the three primes p1, p2, p3, and
484  Polynomial arithmetic and applications

     so that it works with k = 32 and any value of up to 231. This variant may
     be quite practical on a 32-bit machine with built-in instructions for 32-bit
     multiplication and 64-by-32-bit division.

   The previous three exercises indicate that we can multiply integers in essentially
linear time, both in theory and in practice. As mentioned in §3.6, there is a differ-
ent, fully deterministic and rigorously analyzed algorithm that multiplies integers
in linear time on a RAM. In fact, that algorithm works on a very restricted type
of machine called a "pointer machine," which can be simulated in "real time" on
a RAM with a very restricted instruction set (including the type in the previous
exercise). That algorithm works with finite approximations to complex roots of
unity, rather than roots of unity in a finite field.

   We close this section with a cute application of fast polynomial multiplication to
the problem of factoring integers.

EXERCISE 17.28. Let n be a large, positive integer. We can factor n using trial
division in time n1/2+o(1); however, using fast polynomial arithmetic in Zn[X ],
one can get a simple, deterministic, and rigorous algorithm that factors n in time
n1/4+o(1). Note that all of the factoring algorithms discussed in Chapter 15, while
faster, are either probabilistic, or deterministic but heuristic. Assume that we can
multiply polynomials in Zn[X ] of length at most using M ( ) operations in Zn,
where M is a well-behaved complexity function, and M ( ) = 1+o(1) (the algo-
rithm from Exercise 17.24 would suffice).

    (a) Let be a positive integer, and for i = 1, . . . , , let

                                                                 -1

                                       ai := (i - j) mod n.

                                                                j=0

         Using fast polynomial arithmetic, show how to compute (a1, . . . , a ) in time
          1+o(1) len(n)O(1).
   (b) Using the result of part (a), show how to factor n in time n1/4+o(1) using a
         deterministic algorithm.

                                          17.7 Notes

Reed-Solomon codes were first proposed by Reed and Solomon [81], although the
decoder presented here was developed later. Theorem 17.8 was proved by Mills
[68]. The Reed-Solomon code is just one way of detecting and correcting errors --
we have barely scratched the surface of this subject.

   Just as in the case of integer arithmetic, the basic "pencil and paper" quadratic-
time algorithms discussed in this chapter for polynomial arithmetic are not the best
17.7 Notes  485

possible. The fastest known algorithms for multiplication of polynomials of length
at most over a ring R take O( len( ) len(len( ))) operations in R. These algo-
rithms are all variations on the basic FFT algorithm (see Exercise 17.23), but work
without assuming that 2R  R or that R contains any particular primitive roots
of unity (we developed some of the ideas in Exercise 17.24). The Euclidean and
extended Euclidean algorithms for polynomials over a field F can be implemented
so as to take O( len( )2 len(len( ))) operations in F , as can the algorithms for
Chinese remaindering and rational function reconstruction. See the book by von
zur Gathen and Gerhard [39] for details (as well for an analysis of the Euclidean
algorithm for polynomials over the field of rational numbers and over function
fields). Depending on the setting and many implementation details, such asymptot-
ically fast algorithms for multiplication and division can be significantly faster than
the quadratic-time algorithms, even for quite moderately sized inputs of practical
interest. However, the fast Euclidean algorithms are only useful for significantly
larger inputs.

   Exercise 17.3 is based on an algorithm of Brent and Kung [20]. Using fast
matrix and polynomial arithmetic, Brent and Kung show how to solve the modular
composition problem using O( (+1)/2) operations in R, where  is the exponent
for matrix multiplication (see §14.6), and so (+1)/2 < 1.7. Modular composition
arises as a subproblem in a number of algorithms.

 Very recently, faster algorithms for modular composition have been discovered. See the papers by C. Umans
  [Fast polynomial factorization and modular composition in small characteristic, to appear in 40th Annual
  ACM Symposium on Theory of Computing, 2008] and K. Kedlaya and C. Umans [Fast modular composition in
  any characteristic, manuscript, April 2008], both of which are available at www.cs.caltech.edu/~umans/
  research.
                            18

     Linearly generated sequences and applications

In this chapter, we develop some of the theory of linearly generated sequences.
As an application, we develop an efficient algorithm for solving sparse systems
of linear equations, such as those that arise in the subexponential-time algorithms
for discrete logarithms and factoring in Chapter 15. These topics illustrate the
beautiful interplay between the arithmetic of polynomials, linear algebra, and the
use of randomization in the design of algorithms.

                          18.1 Basic definitions and properties
Let F be a field, let V be an F -vector space, and consider an infinite sequence

                                           = {i}i=0
where i  V for i = 0, 1, 2 . . . . We say that  is linearly generated (over F )
if there exist scalars c0, . . . , ck-1  F such that the following recurrence relation
holds:

                                                k-1

                          k+i = cjj+i (for i = 0, 1, 2, . . .).

                                                j=0

In this case, all of the elements of the sequence  are determined by the initial
segment 0, . . . , k-1, together with the coefficients c0, . . . , ck-1 defining the recur-
rence relation.

   The general problem we consider is this: how to determine the coefficients defin-
ing such a recurrence relation, given a sufficiently long initial segment of . To
study this problem, it turns out to be very useful to rephrase the problem slightly.
Let g  F [X ] be a polynomial of degree, say, k, and write g = jk=0 ajX j. Next,

                                               486
                        18.1 Basic definitions and properties  487

define

                                         k

                        g  := ajj.

                                       j=0

Then it is clear that  is linearly generated if and only if there exists a non-zero

polynomial g such that

                        (X ig)  = 0 (for i = 0, 1, 2, . . .).  (18.1)

Indeed, if there is such a non-zero polynomial g, then we can take

              c0 := -(a0/ak), c1 := -(a1/ak), . . . , ck-1 := -(ak-1/ak)

as coefficients defining the recurrence relation for . We call a polynomial g sat-
isfying (18.1) a generating polynomial for . The sequence  will in general
have many generating polynomials. Note that the zero polynomial is technically
considered a generating polynomial, but is not a very interesting one.

   Let G() be the set of all generating polynomials for .

Theorem 18.1. The set G() is an ideal of F [X ].

Proof. First, note that for all g, h  F [X ], we have (g +h)  = (g ) + (h ) --
this is clear from the definitions. It is also clear that for all c  F and g  F [X ],
we have (cg)  = c · (g ). From these two observations, it follows that G()
is closed under addition and scalar multiplication. It is also easy to see from the
definition that G() is closed under multiplication by X ; indeed, if (X ig)  = 0
for all i  0, then certainly, (X i(X g))  = (X i+1g)  = 0 for all i  0. But any
non-empty subset of F [X ] that is closed under addition, multiplication by elements
of F , and multiplication by X is an ideal of F [X ] (see Exercise 7.27). 2

  Since all ideals of F [X ] are principal, it follows that G() is the ideal of F [X ]
generated by some polynomial   F [X ] -- we can make this polynomial unique
by choosing the monic associate (if it is non-zero), and we call this polynomial
the minimal polynomial of . Thus, a polynomial g  F [X ] is a generating
polynomial for  if and only if  divides g; in particular,  is linearly generated if
and only if  = 0.

   We can now restate our main objective as follows: given a sufficiently long initial
segment of a linearly generated sequence, determine its minimal polynomial.

Example 18.1. One can always define a linearly generated sequence by simply
choosing an initial segment 0, 1, . . . , k-1, along with scalars c0, . . . , ck-1  F
defining the recurrence relation. One can enumerate as many elements of the
sequence as one wants by using storage for k elements of V , along with storage for
the scalars c0, . . . , ck-1, as follows:
488             Linearly generated sequences and applications

         (0, . . . , k-1)  (0, . . . , k-1)
         repeat

               output 0
                 jk=0 -1 cjj
               (0, . . . , k-1)  (1, . . . , k-1,  )
         forever

   Because of the structure of the above algorithm, linearly generated sequences are
sometimes also called shift register sequences. Also observe that if F is a finite
field, and V is finite dimensional, the value stored in the "register" (0, . . . , k-1)
must repeat at some point. It follows that the linearly generated sequence must be
ultimately periodic (see definitions above Exercise 4.21). 2

Example 18.2. Linearly generated sequences can also arise in a natural way, as this
example and the next illustrate. Let E := F [X ]/(f ), where f  F [X ] is a monic

polynomial of degree > 0, and let  be an element of E. Consider the sequence
 := { i i}=0 of powers of . For every polynomial g = jk=0 ajX j  F [X ], we
have

                                                               k

                                  g  = ajj = g().

                                                              j=0

Now, if g() = 0, then clearly (X ig)  = ig() = 0 for all i  0. Conversely,
if (X ig)  = 0 for all i  0, then in particular, g() = 0. Thus, g is a generating
polynomial for  if and only if g() = 0. It follows that the minimal polynomial
 of  is the same as the minimal polynomial of  over F , as defined in §16.5.
Furthermore,  = 0, and the degree m of  may be characterized as the smallest
positive integer m such that { i i}m=0 is linearly dependent; moreover, as E has
dimension over F , we must have m  . 2

Example 18.3. Let V be a vector space over F of dimension > 0, and let

 : V  V be an F -linear map. Let   V , and consider the sequence  := {i}i=0,
where i = i(); that is, 0 = , 1 = (), 2 = (()), and so on. For every

polynomial g =  k    ajX j    F [X ],  we  have
                j=0

                                              k

                              g  = ajj(),

                                            j=0

and for every i  0, we have

                     k                           k

     (X ig)  = aji+j () = i ajj () = i(g ).

                     j=0                         j=0
                   18.1 Basic definitions and properties   489

Thus, if g  = 0, then clearly (X ig)  = i(g ) = i(0) = 0 for all i  0.
Conversely, if (X ig)  = 0 for all i  0, then in particular, g  = 0. Thus, g is
a generating polynomial for  if and only if g  = 0. The minimal polynomial 
of  is non-zero and its degree m is at most ; indeed, m may be characterized as
the least non-negative integer such that {i()}mi=0 is linearly dependent, and since
V has dimension over F , we must have m  .

   The previous example can be seen as a special case of this one, by taking V to

be E,  to be the -multiplication map on E, and setting  to 1. 2

   The problem of computing the minimal polynomial of a linearly generated
sequence can always be solved by means of Gaussian elimination. For exam-
ple, the minimal polynomial of the sequence discussed in Example 18.2 can be
computed using the algorithm described in §17.2. The minimal polynomial of
the sequence discussed in Example 18.3 can be computed in a similar manner.
Also, Exercise 18.3 below shows how one can reformulate another special case of
the problem so that it is easily solved by Gaussian elimination. However, in the
following sections, we will present algorithms for computing minimal polynomials
for certain types of linearly generated sequences that are much more efficient than
any algorithm based on Gaussian elimination.

EXERCISE 18.1. Show that the only sequence for which 1 is a generating polyno-
mial is the "all zero" sequence.

EXERCISE 18.2. Let  = {i}i=0 be a sequence of elements of an F -vector space
V . Further, suppose that  has non-zero minimal polynomial .

   (a) Show that for all polynomials g, h  F [X ], if g  h (mod ), then
         g  = h .

   (b) Let m := deg(). Show that if g  F [X ] and (X ig)  = 0 for all
         i = 0, . . . , m - 1, then g is a generating polynomial for .

EXERCISE 18.3. This exercise develops an alternative characterization of linearly

generated sequences. Let  = {zi}i=0 be a sequence of elements of F . Further,
suppose that  has minimal polynomial  = jm=0 cjX j with m > 0 and cm = 1.

Define the matrix                                
                             z0            zm-1
                                 z1 · · ·   zm  .   F m×m
                    A :=  z1.
                          ..     z2 · · ·    .. 

                                 ..  ...
                                 .

                   zm-1 zm · · · z2m-2
490             Linearly generated sequences and applications

and the vector

                w := (zm, . . . , z2m-1)  F 1×m.

Show that

                v = (-c0, . . . , -cm-1)  F 1×m

is the unique solution to the equation
                                            vA = w.

Hint: show that the rows of A form a linearly independent family of vectors by

making use of Exercise 18.2 and the fact that no polynomial of degree less than m
is a generating polynomial for .

EXERCISE 18.4. Let c0, . . . , ck-1  F and z0, . . . , zk-1  F . For each i  0, let

                                                                  k-1

                                      zk+i := cjzj+i.

                                                                  j=0

Given n  0, along with c0, . . . , ck-1 and z0, . . . , zk-1, show how to compute zn
using O(len(n)k2) operations in F .

EXERCISE 18.5. Let V be a vector space over F , and consider the set V × of all
infinite sequences {i}i=0, where the i's are in V . Let us define the scalar product
of g  F [X ] and   V × as

                            g ·  = {(X ig) }i=0  V ×.

Show that with this scalar product, and addition defined component-wise, V × is
an F [X ]-module, and that a polynomial g  F [X ] is a generating polynomial for
  V × if and only if g ·  = 0.

               18.2 Computing minimal polynomials: a special case
We now tackle the problem of efficiently computing the minimal polynomial of a
linearly generated sequence from a sufficiently long initial segment.

   We shall first address a special case of this problem, namely, the case where the
vector space V is just the field F . In this case, we have

                                           = {zi}i=0,

where zi  F for i = 0, 1, 2, . . . .
   Suppose that we do not know the minimal polynomial  of , but we know

an upper bound M > 0 on its degree. Then it turns out that the initial segment
z0, z1, . . . z2M-1 completely determines , and moreover, we can very efficiently
18.2 Computing minimal polynomials: a special case  491

compute  given this initial segment. The following theorem provides the essential
ingredient.

Theorem 18.2. Let  = {zi}i=0 be a sequence of elements of F , and define the
reversed Laurent series

                                                    

                             z := ziX -(i+1)  F ((X -1)),

                                                    i=0

whose coefficients are the elements of the sequence . Then for every g  F [X ],
we have g  G() if and only if gz  F [X ]. In particular,  is linearly generated
if and only if z is a rational function, in which case, its minimal polynomial is the
denominator of z when expressed as a fraction in lowest terms.

Proof. Observe that for every polynomial g  F [X ] and every integer i  0,
the coefficient of X -(i+1) in the product gz is equal to X ig  -- just look at the
formulas defining these expressions! It follows that g is a generating polynomial
for  if and only if the coefficients of the negative powers of X in gz are all zero,
which is the same as saying that gz  F [X ]. Further, if g = 0 and h := gz  F [X ],
then deg(h) < deg(g) -- this follows simply from the fact that deg(z) < 0 (together
with the fact that deg(h) = deg(g) + deg(z)). All the statements in the theorem
follow immediately from these observations. 2

   By virtue of Theorem 18.2, we can compute the minimal polynomial  of 
using the algorithm in §17.5.1 for computing the numerator and denominator of a
rational function from its reversed Laurent series expansion. More precisely, we
can compute  given the bound M on its degree, along with the first 2M elements
z0, . . . , z2M-1 of , using O(M2) operations in F . Just for completeness, we write
down this algorithm:

     1. Run the extended Euclidean algorithm on inputs

                 f := X 2M and h := z0X 2M-1 + z1X 2M-2 + · · · + z2M-1,

         and apply Theorem 17.8 with f , h, r := M, and t := M, to obtain the
         polynomials r , s , t .
     2. Output  := t / lc(t ).

EXERCISE 18.6. Suppose F is a finite field and that  := {zi}i=0 is linearly gen-
erated, with minimal polynomial . Further, suppose X . Show that  is purely
periodic with period equal to the multiplicative order of [X ]  (F [X ]/()). Hint:

use Exercise 17.12 and Theorem 18.2.
492  Linearly generated sequences and applications

           18.3 Computing minimal polynomials: a more general case

Having dealt with the problem of finding the minimal polynomial of a linearly gen-
erated sequence , whose elements lie in F , we address the more general problem,
where the elements of  lie in a vector space V over F . We shall only deal with a
special case of this problem, but it is one which has useful applications:

     · First, we shall assume that V has finite dimension > 0 over F .
     · Second, we shall assume that the sequence  = {i}i=0 has full rank, by

         which we mean the following: if the minimal polynomial  of  over F
         has degree m, then {i}im=0-1 is linearly independent. This property implies
        that the minimal polynomial of  is the monic polynomial   F [X ] of
         least degree such that   = 0. The sequences considered in Examples
         18.2 and 18.3 are of this type.

     · Third, we shall assume that F is a finite field.

The dual space. Before presenting our algorithm for computing minimal polyno-

mials, we need to discuss the dual space DF (V ) of V (over F ), which consists
of all F -linear maps from V into F . Thus, DF (V ) = HomF (V , F ), and is a

vector space over F , with addition and scalar multiplication defined point-wise

(see Theorem 13.12). We shall call elements of DF (V ) projections.
   Now, fix a basis S = {i}i=1 for V . As was discussed in §14.2, every element

  V has a unique coordinate vector VecS () = (c1, . . . , c )  F 1× , where
 = i cii. Moreover, the map VecS : V  F 1× is a vector space isomorphism.

   To each projection   DF (V ) we may also associate the coordinate vector
((1), . . . , ( ))  F ×1. If U is the basis for F consisting of the single element

1F , then the coordinate vector of  is MatS,U (), that is, the matrix of  relative
to the bases S and U . By Theorem 14.4, the map MatS,U : DF (V )  F ×1 is a

vector space isomorphism.

   In working with algorithms that compute with elements of V and DF (V ), we

shall assume that such elements are represented using coordinate vectors rela-

tive to some convenient, fixed basis for V . If   V has coordinate vector
(c1, . . . , c )  F 1× , and   DF (V ) has coordinate vector (d1, . . . , d )  F ×1,

then () is easily computed, using O( ) operations in F , as i=1 cidi.

   We now return to the problem of computing the minimal polynomial  of the
linearly generated sequence  = {i}i=0. Assume we have a bound M > 0 on the
degree of . Since  has full rank and dimF (V ) = , we may assume that M  .

   For each   DF (V ), we may consider the projected sequence  := {(i)}i=0.
Observe that  is a generating polynomial for ; indeed, for every polynomial
g  F [X ], we have g  = (g ), and hence, for all i  0, we have
(X i)  = ((X i) ) = (0) = 0. Let   F [X ] denote the minimal
18.3 Computing minimal polynomials: a more general case             493

polynomial of . Since  divides every generating polynomial of , and since
 is a generating polynomial for , it follows that  divides .

   This suggests the following algorithm for efficiently computing the minimal
polynomial of , using the first 2M terms of :

Algorithm MP. Given the first 2M terms of the sequence  = {i}i=0, do the
following:

g  1  F [X ]

repeat

      choose   DF (V ) at random
      compute the first 2M terms of the projected sequence 

      use the algorithm in §18.2 to compute the minimal polynomial
              of 

      g  lcm(g, )
until g  = 0

output g

   A few remarks on the above procedure are in order:

     · in every iteration of the main loop, g is the least common multiple of a
         number of divisors of , and hence is itself a divisor of ; in particular,
         deg(g)  M;

     · under our assumption that  has full rank, and since g is a monic divisor of
         , if g  = 0, we may safely conclude that g = ;

     · under our assumption that F is finite, choosing a random element  of
         DF (V ) amounts to simply choosing at random the entries of the coordinate
         vector of , relative to some basis for V ;

     · we also assume that elements of V are represented as coordinate vectors,
         so that applying a projection   DF (V ) to an element of V takes O( )
         operations in F ; in particular, in each loop iteration, we can compute the
         first 2M terms of the projected sequence  using O(M ) operations in F ;

     · similarly, adding two elements of V , or multiplying an element of V by
         a scalar, takes O( ) operations in F ; in particular, in each loop iteration,
         we can compute g  using O(M ) operations in F (and using the first
         M + 1  2M terms of ).

   Based on the above observations, it follows that when the algorithm halts, its
output is correct, and that the cost of each loop iteration is O(M ) operations in
F . The remaining question to be answered is this: what is the expected number of
iterations of the main loop? The answer to this question is O(1), which leads to a
total expected cost of Algorithm MP of O(M ) operations in F .
494  Linearly generated sequences and applications

   The key to establishing that the expected number of iterations of the main loop
is constant is provided by the following theorem.

Theorem 18.3. Let  = {i}i=0 be a linearly generated sequence over the field
F , where the i's are elements of a vector space V of finite dimension > 0. Let
 be the minimal polynomial of  over F , let m := deg(), and assume that 
has full rank (i.e., {i}im=0-1 is linearly independent). Finally, let F [X ]<m denote the
vector space over F consisting of all polynomials in F [X ] of degree less than m.

   Under the above assumptions, there exists a surjective F -linear map

      : DF (V )  F [X ]<m

such that for all   DF (V ), the minimal polynomial  of the projected sequence
 := {(i)}i=0 satisfies

      =  
                  .
         gcd((), )

Proof. While the statement of this theorem looks a bit complicated, its proof is
quite straightforward, given our characterization of linearly generated sequences
in Theorem 18.2 in terms of rational functions. We build the linear map  as the
composition of two linear maps, 0 and 1.

   Let us define the map

     0 : DF (V )  F ((X -1))

                                  

                    (i)X -(i+1).

                                 i=0

We also define the map 1 to be the -multiplication map on F ((X -1)) -- that is, the

map that sends z  F ((X -1)) to  · z  F ((X -1)). The map  is just the composition

 = 1  0. It is clear that both 0 and 1 are F -linear maps, and hence, so is .
   First, observe that for   DF (V ), the series z := 0() is the series associ-

ated with the projected sequence , as in Theorem 18.2. Let  be the minimal
polynomial of . Since  is a generating polynomial for , it is also a generating
polynomial for . Therefore, Theorem 18.2 tells us that

     h := () =  · z  F [X ]<m,

and that  is the denominator of z when expressed as a fraction in lowest terms.
Now, we have z = h/, and it follows that  = / gcd(h, ) is this denominator.

   Second, the hypothesis that {i}im=0-1 is linearly independent implies that
dimF (Im 0)  m (see Exercise 13.21). Also, observe that 1 is an injective

map. Therefore, dimF (Im )  m. In the previous paragraph, we observed
18.3 Computing minimal polynomials: a more general case              495

that Im   F [X ]<m, and since dimF (F [X ]<m) = m, we may conclude that
Im  = F [X ]<m. That proves the theorem. 2

   Given the above theorem, we can analyze the expected number of iterations of

the main loop of Algorithm MP.

   First of all, we may as well assume that the degree m of  is greater than 0,

as otherwise, we are sure to get  in the very first iteration. Let 1, . . . , s be the
random projections chosen in the first s iterations of Algorithm MP. By Theo-
rem 18.3, each (i) is uniformly distributed over F [X ]<m, and we have g =  at
the end of loop iteration s if and only if gcd(, (1), . . . , (s)) = 1.

   Let us define F (s) to be the probability that gcd(, f1, . . . , fs) = 1, where
f1, . . . , fs are randomly chosen from F [X ]<m. Thus, the probability that we have
g =  at the end of loop iteration s is equal to F (s). While one can ana-
lyze the quantity F (s), it turns out to be easier, and sufficient for our purposes,
to analyze a different quantity. Let us define mF (s) to be the probability that
gcd(f1, . . . , fs) = 1, where f1, . . . , fs are randomly chosen from F [X ]<m. Clearly,
F (s)  Fm (s).

Theorem 18.4. If F is a finite field of cardinality q, and m and s are positive
integers, then we have

mF (s) = 1 - 1/qs-1 + (q - 1)/qsm.

Proof. For each positive integer n, let Un denote the set of all tuples of polyno-
mials (f1, . . . , fs)  F [X ]<n ×s with gcd(f1, . . . , fs) = 1, and let un := |Un|. Also,

for each monic polynomial h  F [X ] of degree less that n, let Un,h denote the

set of all s-tuples of polynomials of degree less than n whose gcd is h. Observe
that the set Un,h is in one-to-one correspondence with Un-k, where k := deg(h),

via the map that sends (f1, . . . , fs)  Un,h to (f1/h, . . . , fs/h)  Un-k. As
there are qk possible choices for h of degree k, if we define Vn,k to be the set
of tuples (f1, . . . , fs)  F [X ]<n ×s with deg(gcd(f1, . . . , fs)) = k, we see that
|Vn,k| = qkun-k. Every non-zero tuple in F [X ]<n ×s appears in exactly one of the
sets Vn,k, for k = 0, . . . , n - 1. Taking into account the zero tuple, it follows that

                 n-1                                                 (18.2)

qsn = 1 + qkun-k,

                k=0

which holds for all n  1. Replacing n by n - 1 in (18.2), we obtain

                      n-2                                            (18.3)

qs(n-1) = 1 + qkun-1-k,

                      k=0
496         Linearly generated sequences and applications

which holds for all n  2, and indeed, holds for n = 1 as well. Subtracting q times
(18.3) from (18.2), we deduce that for all n  1,

                                 qsn - qsn-s+1 = 1 + un - q,

and rearranging terms:

                        un = qsn - qsn-s+1 + q - 1.

Therefore,

            mF (s) = um/qsm = 1 - 1/qs-1 + (q - 1)/qsm. 2

   From the above theorem, it follows that for s  1, the probability Ps that Algo-
rithm MP runs for more than s loop iterations is at most 1/qs-1. If L is the total
number of loop iterations, then

        E[L] = P[L  i] = 1 + Ps  1 + 1/qs-1 = 1 + q  3.
                 i1 s1 s1 q - 1

Let us summarize all of the above analysis with the following:

Theorem 18.5. Let  be a sequence of elements of an F -vector space V of finite
dimension > 0 over F , where F is a finite field. Assume that  is linearly
generated over F with minimal polynomial   F [X ] of degree m, and that 
has full rank (i.e., the first m terms of  form a linearly independent family of

elements). Then given an upper bound M > 0 on m, along with the first 2M
elements of , Algorithm MP correctly computes  using an expected number of

O(M ) operations in F .

   We close this section with the following observation. Suppose the sequence 
is of the form {i()}i=0, where   V and  : V  V is an F -linear map.
Suppose that with respect to some basis S for V , elements of V are represented
by their coordinate vectors (which are elements of F 1× ), and elements of DF (V )
are represented by their coordinate vectors (which are elements of F ×1). The
linear map  also has a corresponding matrix A = MatS,S (V , V )  F × , so that
evaluating  at a point  in V corresponds to multiplying the coordinate vector of
 on the right by A. Now, suppose   V has coordinate vector v  F 1× and
that   DF (V ) has coordinate vector w  F ×1. Then if  is the sequence of
coordinate vectors of the elements of , we have

                           = {vA i i}=0 and  = {vA i iw}=0.

This more concrete, matrix-oriented point of view is sometimes useful; in partic-

ular, it makes quite transparent the symmetry of the roles played by  and  in

forming the projected sequence.
18.4 Solving sparse linear systems       497

EXERCISE 18.7. If |F | = q and   F [X ] is monic and factors into monic irre-
ducible polynomials in F [X ] as  = 1e1 · · · rer , show that

r            r

F (1) = (1 - q- deg(i))  1 - q- deg(i).

i=1          i=1

From this, conclude that the probability that Algorithm MP terminates after just one
loop iteration is 1 - O(m/q), where m = deg(). Thus, if q is very large relative

to m, it is highly likely that Algorithm MP terminates after just one iteration of the

main loop.

18.4 Solving sparse linear systems

Let V be a vector space of finite dimension > 0 over a finite field F , and let

 : V  V be an F -linear map. The goal of this section is to develop time- and
space-efficient algorithms for solving equations of the form

     () = ;                              (18.4)

that is, given  and   V , find   V satisfying (18.4). The algorithms we

develop will have the following properties: they will be probabilistic, and will
use an expected number of O( 2) operations in F , an expected number of O( )

evaluations of , and space for O( ) elements of F . By an "evaluation of ," we

mean the computation of () for a given   V .

   We shall assume that elements of V are represented as coordinate vectors with

respect to some fixed basis for V . This means that a single element of V is repre-

sented as a vector of elements of F . Now, if the matrix of  with respect to the
given basis is sparse, having, say, 1+o(1) non-zero entries, then the space required
to represent  is 1+o(1) elements of F , and the time required to evaluate  is 1+o(1)

operations in F . Under these assumptions, our algorithms to solve (18.4) use an
expected number of 2+o(1) operations in F , and space for 1+o(1) elements of F .

This is to be compared with standard Gaussian elimination: even if the original

matrix is sparse, during the execution of the algorithm, most of the entries in the

matrix may eventually be "filled in" with non-zero field elements, leading to a run-
ning time of ( 3) operations in F , and a space requirement of ( 2) elements of
F . Thus, the algorithms presented here will be much more efficient than Gaussian

elimination when the matrix of  is sparse.
   We hasten to point out that the algorithms presented here may be more efficient

than Gaussian elimination in other cases, as well. All that matters is that  can
be evaluated using o( 2) operations in F and/or represented using space for o( 2)

elements of F -- in either case, we obtain a time and/or space improvement over

Gaussian elimination. Indeed, there are applications where the matrix of the linear
498  Linearly generated sequences and applications

map  may not be sparse, but nevertheless has special structure that allows it to be
represented and evaluated in subquadratic time and/or space.

   We shall only present algorithms that work in two special, but important, cases:

     · the first case is where  is bijective,
     · the second case is where  is not bijective,  = 0, and a non-zero solution 

        to (18.4) is required (i.e., we are looking for a non-zero element of Ker ).

   In both cases, the key will be to use Algorithm MP in §18.3 to find the minimal
polynomial  of the linearly generated sequence

      := {i}i=0 (i := i(), i = 0, 1, . . .),        (18.5)

where  is a suitably chosen element of V . From the discussion in Example 18.3,
this sequence has full rank, and so we may use Algorithm MP. We may use M :=
as an upper bound on the degree of  (assuming we know nothing more about  and
 that would allow us to use a smaller upper bound). In using Algorithm MP in this
application, note that we do not want to store 0, . . . , 2 -1 -- if we did, we would
not satisfy our stated space bound. Instead of storing the i's in a "warehouse," we
use a "just in time" strategy for computing them, as follows:

     · In the body of the main loop of Algorithm MP, where we calculate the
        projections zi := (i), for i = 0 . . . 2 - 1, we perform the computation as

        follows:

     
     for i  0 to 2 - 1 do

           zi  (),   ()

     · In the test at the bottom of the main loop of Algorithm MP, if g =
          jk=0 ajX j, we compute  := g   V using the following Horner-like

        scheme:

     0

     for j  k down to 0 do
             () + aj · 

With this implementation, Algorithm MP uses an expected number of O( 2) oper-
ations in F , an expected number of O( ) evaluations of , and space for O( )
elements of F . Of course, the "warehouse" strategy is faster than the "just in time"
strategy by a constant factor, but it uses about times as much space; thus, for large
 , using the "just in time" strategy is a very good time/space trade-off.

The bijective case. Now consider the case where  is bijective, and we want
to solve (18.4) for a given   V . We may as well assume that  = 0, since
otherwise,  = 0 is the unique solution to (18.4). We proceed as follows. First,
                       18.4 Solving sparse linear systems                                  499

using Algorithm MP as discussed above, compute the minimal polynomial  of the

sequence  defined in (18.5), using  := . Let  =              m    cjX j,  where  cm  =  1  and
                                                             j=0

m > 0. Then we have

                       c0 + c1() + · · · + cmm() = 0.                                   (18.6)

   We claim that c0 = 0. To prove the claim, suppose that c0 = 0. Then applying
-1 to (18.6), we would obtain

                       c1 + · · · + cmm-1() = 0,

which would imply that /X is a generating polynomial for , contradicting the

minimality of . That proves the claim.
   Since c0 = 0, we can apply -1 to (18.6), and solve for  = -1() as follows:

                        = -c0-1(c1 + · · · + cmm-1()).

To actually compute , we use the same "just in time" strategy as was used in the
implementation of the computation of g  in Algorithm MP, which costs O( 2)

operations in F , O( ) evaluations of , and space for O( ) elements of F .

The non-bijective case. Now consider the case where  is not bijective, and we
want to find non-zero   V such that () = 0. The idea is this. Suppose we

choose an arbitrary, non-zero element  of V , and use Algorithm MP to compute

the minimal polynomial  of the sequence  defined in (18.5), using this value of

. Let  =  m    cjX j,  where  m  >  0  and  cm  =  1.  Then  we   have
          j=0

                       c0 + c1() + · · · + cmm() = 0.                                   (18.7)

Let

                                 := c1 + · · · + cmm-1().

We must have  = 0, since  = 0 would imply that /X is a non-zero generating
polynomial for , contradicting the minimality of . If it happens that c0 = 0,
then equation (18.7) implies that () = 0, and we are done. As before, to actually

compute , we use the same "just in time" strategy as was used in the implementa-
tion of the computation of g  in Algorithm MP, which costs O( 2) operations

in F , O( ) evaluations of , and space for O( ) elements of F .
   The above approach fails if c0 = 0. However, in this "bad" case, equation

(18.7) implies that  = -c0-1(); in particular,   Im . One way to avoid such
a "bad"  is to randomize: as  is not surjective, the image of  is a subspace

of V of dimension strictly less than , and therefore, a randomly chosen  lies

in the image of  with probability at most 1/|F |. So a simple technique is to

choose repeatedly  at random until we get a "good" . The overall complexity of
500  Linearly generated sequences and applications

the resulting algorithm will be as required: O( 2) expected operations in F , O( )

expected evaluations of , and space for O( ) elements of F .

   As a special case of this situation, consider the problem that arose in Chapter 15

in connection with algorithms for computing discrete logarithms and factoring. We

had to solve the following problem: given an × ( - 1) matrix A with entries in
a finite field F , containing 1+o(1) non-zero entries, find non-zero v  F 1× such
that vA = 0. To solve this problem, we can augment the matrix A, adding an extra
column of zeros, to get an × matrix A . Now, let V = F 1× and let  be the

F -linear map on V that sends   V to A . A non-zero solution  to the equation
() = 0 will provide us with the solution to our original problem; thus, we can
apply the above technique directly, solving this problem using 2+o(1) expected
operations in F , and space for 1+o(1) elements of F . As a side remark, in this

particular application, we can choose a "good"  in the above algorithm without
randomization: just choose  := (0, . . . , 0, 1), which is clearly not in the image of .

             18.5 Computing minimal polynomials in F [X ]/(f ) (II)

Let us return to the problem discussed in §17.2: F is a field, f  F [X ] is a monic
polynomial of degree > 0, and E := F [X ]/(f ); we are given an element   E,
and want to compute the minimal polynomial   F [X ] of  over F . As dis-
cussed in Example 18.2, this problem is equivalent to the problem of computing
the minimal polynomial of the sequence

                           := {i}i=0 (i := i, i = 0, 1, . . .),

and the sequence has full rank; therefore, we can use Algorithm MP in §18.3
directly to solve this problem, assuming F is a finite field.

   If we use the "just in time" strategy in the implementation of Algorithm MP,
as was used in §18.4, we get an algorithm that computes the minimal polynomial
of  using O( 3) expected operations in F , but space for just O( 2) elements of
F . Thus, in terms of space, this approach is far superior to the algorithm in §17.2,
based on Gaussian elimination. In terms of time complexity, the algorithm based
on linearly generated sequences is a bit slower than the one based on Gaussian
elimination (but only by a constant factor). However, if we use any subquadratic-
time algorithm for polynomial arithmetic (see §17.6 and §17.7), we immediately
get an algorithm that runs in subcubic time, while still using linear space. In the
exercises below, you are asked to develop an algorithm that computes the minimal
polynomial of  using just O( 2.5) operations in F , at the expense of requiring
space for O( 1.5) elements of F -- this algorithm does not rely on fast polynomial
arithmetic, and can be made even faster if such arithmetic is used.
18.6 The algebra of linear transformations ()  501

EXERCISE 18.8. Let f  F [X ] be a monic polynomial of degree > 0 over a field
F , and let E := F [X ]/(f ). Also, let  := [X ]f  E. For computational purposes,
we assume that elements of E and DF (E) are represented as coordinate vectors
with respect to the usual "polynomial" basis { i i-1} =1. For   E, let M denote
the -multiplication map on E that sends   E to   E, which is an F -linear

map from E into E.

    (a) Given as input the polynomial f defining E, along with a projection

           DF (E) and an element   E, show how to compute the projection
           M  DF (E), using O( 2) operations in F .

   (b) Given as input the polynomial f defining E, along with a projection

           DF (E), an element   E, and a parameter k > 0, show how to
         compute ((1), (), . . . , (k-1)) using just O(k + k1/2 2) operations in
         F , and space for O(k1/2 ) elements of F . Hint: use the same hint as in

         Exercise 17.3.

EXERCISE 18.9. Let f  F [X ] be a monic polynomial over a finite field F of
degree > 0, and let E := F [X ]/(f ). Show how to use the result of the previous

exercise, as well as Exercise 17.3, to get an algorithm that computes the minimal
polynomial of   E over F using O( 2.5) expected operations in F , and space for
O( 1.5) operations in F .

EXERCISE 18.10. Let f  F [X ] be a monic polynomial of degree > 0 over
a field F (not necessarily finite), and let E := F [X ]/(f ). Further, suppose that
f is irreducible, so that E is itself a field. Show how to compute the minimal
polynomial of   E over F deterministically, using algorithms that satisfy the
following complexity bounds:

    (a) O( 3) operations in F and space for O( ) elements of F ;

   (b) O( 2.5) operations in F and space for O( 1.5) elements of F .

                   18.6 The algebra of linear transformations ()

Throughout this chapter, one could hear the whispers of the algebra of linear trans-
formations. We develop some of the aspects of this theory here, leaving a number
of details as exercises. It will not play a role in any material that follows, but it
serves to provide the reader with a "bigger picture."

   Let F be a field and V be an F -vector space. We denote by LF (V ) the set
of all F -linear maps from V into V . Thus, LF (V ) = HomF (V , V ), and is a
vector space over F , with addition and scalar multiplication defined point-wise
(see Theorem 13.12). Elements of LF (V ) are called linear transformations.

   For ,   LF (V ), the composed map,    , which sends   V to ( ())
502  Linearly generated sequences and applications

is also an element of LF (V ). As always, function composition is associative (i.e.,
for ,  ,   LF (V ), we have   (   ) = (   )   ); however, function
composition is not in general commutative (i.e., we may have    =    for

some ,   LF (V )). The following theorem considers the interaction between

composition, addition, and scalar multiplication.

Theorem 18.6. For all ,  ,   LF (V ), and for all c  F , we have:
    (i)   ( +  ) =    +    ;
   (ii) ( +  )   =    +   ;

   (iii) (c)   = c(   ) =   (c ).

Proof. Exercise. 2

   Under the addition operation and scalar multiplication of the vector space
LF (V ), and defining multiplication on LF (V ) using the "" operation, the pre-
vious theorem implies that LF (V ) satisfies all the properties of an F -algebra (see
Definition 16.1), except for the fact that multiplication is not commutative (the
identity map acts as the multiplicative identity). Thus, we can think of LF (V ) as a
non-commutative F -algebra.

   Let   LF (V ) be a linear transformation. For each integer i  0, the map
i (i.e., the i-fold composition of ) is also an element of LF (V ). Note that 0
is by definition just the identity map on V . For each polynomial g  F [X ], with
g = i aiX i, we denote by g() the linear transformation

                                 g() := aii  LF (V ).

                                                              i

Thus, for   V , the value of g() at  is i aii().

Theorem 18.7. For all   LF (V ), for all c  F , and for all g, h  F [X ], we
have:

    (i) g() + h() = (g + h)();
   (ii) c · g() = (cg)();
   (iii) g()  h() = (gh)() = h()  g().

Proof. Exercise. 2

Let   LF (V ) be a linear transformation. We define
                           F [] := {g() : g  F [X ]},

which is a subset of LF (V ). By the previous theorem, it is clear that F [] is closed
under addition, multiplication (i.e., composition), and scalar multiplication, and
18.6 The algebra of linear transformations ()  503

that F [] is in fact an F -algebra in the usual sense (i.e., multiplication is commu-
tative). Moreover, the expressions F [] and g() (for g  F [X ]) have the same
meaning as in §16.1.

   Let  be the minimal polynomial of  over F , so that F [] is isomorphic as an
F -algebra to F [X ]/( ). We can also characterize  as follows:

        if there exists a non-zero polynomial g  F [X ] such that g() = 0,
         then  is the monic polynomial of least degree with this property;
         otherwise,  = 0.

Another way to characterize  is as follows:

          is the minimal polynomial of the sequence { i i}=0.

   If V has finite dimension > 0, then by Theorem 14.4, LF (V ) is isomorphic as
an F -vector space to F × , and so in particular, has dimension 2. Therefore, there
must be a linear dependence among 1, , . . . ,  2, which implies that the minimal
polynomial of  is non-zero with degree at most 2 (and at least 1). We shall show
below that in this case, the minimal polynomial of  actually has degree at most .

   For a fixed   LF (V ), we can define a "scalar multiplication" operation , that
maps g  F [X ] and   V to

                                    g  := g()()  V ;

that is, if g = i aiX i, then

                                     g  = aii().

                                                                    i

Theorem 18.8. The scalar multiplication , together with the usual addition oper-
ation on V , makes V into an F [X ]-module; that is, for all g, h  F [X ] and
,   V , we have

                g (h ) = (gh) , (g + h)  = g  + h ,
                         g ( + ) = g  + g , 1  = .

Proof. Exercise. 2

  Note that each choice of  gives rise to a different F [X ]-module structure, but all
of these structures are extensions of the usual vector space structure, in the sense
that for all c  F and   V , we have c  = c.

  Now, for fixed   LF (V ) and   V , consider the F [X ]-linear map , :
F [X ]  V that sends g  F [X ] to g  = g()(). The kernel of this map must be
a submodule, and hence an ideal, of F [X ]; since every ideal of F [X ] is principal,
it follows that Ker , is the ideal of F [X ] generated by some polynomial ,,
504  Linearly generated sequences and applications

which we can make unique by insisting that it is monic or zero. We call , the
minimal polynomial of  under . We can also characterize , as follows:

        if there exists a non-zero polynomial g  F [X ] such that g()() =
         0, then , the monic polynomial of least degree with this property;
         otherwise, , = 0.

Another way to characterize , is as follows:
         , is the minimal polynomial of the sequence {i()}i=0.

   Note that since  () is the zero map, we have

                                     =  ()() = 0,

and hence   Ker ,, which means that , | .
  Now consider the image of ,, which we shall denote by  . As an F [X ]-

module,   is isomorphic to F [X ]/(,). In particular, if , is non-zero and
has degree m, then   is a vector space of dimension m over F ; indeed, the
elements , (), . . . , m-1() form a basis for   over F ; moreover, m is the
smallest non-negative integer such that {i()}mi=0 is linearly dependent.

   Observe that for every    , we have ,  = 0; indeed, if  = g , then

             , (g ) = (,g)  = g (, ) = g 0 = 0.

   The following three theorems develop some simple facts; the proofs of these are
straightforward, and left as exercises. In each theorem,  is an element of LF (V ),
and is the associated scalar multiplication that makes V into an F [X ]-module.

Theorem 18.9. Let   V have minimal polynomial f  F [X ] under , and
let   V have minimal polynomial g  F [X ] under . If gcd(f, g) = 1, then
      = {0}, and  +  has minimal polynomial f · g under .

Theorem 18.10. Let   V . Let f  F [X ] be a monic irreducible polynomial
such that f e  = 0 but f e-1  = 0 for some integer e  1. Then f e is the

minimal polynomial of  under .

Theorem 18.11. Let   V , and suppose that  has minimal polynomial f  F [X ]
under , with f = 0. Let g  F [X ]. Then g  has minimal polynomial

f / gcd(f, g) under .

   We are now ready to state the main result of this section, whose statement and
proof are analogous to that of Theorem 6.41:

Theorem 18.12. Let   LF (V ), and suppose that  has non-zero minimal poly-
nomial . Then there exists   V such that the minimal polynomial of  under 
is .
                    18.6 The algebra of linear transformations ()             505

Proof. Let be the scalar multiplication associated with . Let  = 1e1 · · · rer be
the factorization of  into monic irreducible polynomials in F [X ].

   First, we claim that for each i = 1, . . . , r, there exists i  V such that
/i i = 0. Suppose the claim were false: then for some i, we would have
/i  = 0 for all   V ; however, this means that (/i)() = 0, contradicting

the minimality property in the definition of the minimal polynomial . That proves

the claim.
   Let 1, . . . , r be as in the above claim. Then by Theorem 18.10, each /iei i

has minimal polynomial iei under . Finally, by Theorem 18.9,

                            := /1e1 1 + · · · + /rer r

has minimal polynomial  under . 2

Theorem 18.12 says that if  has minimal polynomial  of degree m  0, then

there exists     V  such   that  { i ( )}m-1      is  linearly  independent.  From this, it

                                             i=0

immediately follows that:

Theorem 18.13. If V has finite dimension > 0, then for every   LF (V ), the
minimal polynomial of  is non-zero of degree at most .

  We close this section with a simple observation. Let V be an arbitrary F [X ]-
module with scalar multiplication . Restricting the scalar multiplication from
F [X ] to F , we can naturally view V as an F -vector space. Let  : V  V be the
map that sends   V to X . It is easy to see that   LF (V ), and that for all
polynomials g  F [X ], and all   V , we have g  = g()(). Thus, instead of
starting with a vector space and defining an F [X ]-module structure in terms of a
given linear map, we can go the other direction, starting from an F [X ]-module and
obtaining a corresponding linear map. Furthermore, using the language introduced
in Examples 13.19 and 13.20, we see that the F [X ]-exponent of V is the ideal
of F [X ] generated by the minimal polynomial of , and the F [X ]-order of any
element   V is the ideal of F [X ] generated by the minimal polynomial of 
under . Theorem 18.12 says that there exists an element in V whose F [X ]-order
is equal to the F [X ]-exponent of V , assuming the latter is non-zero.

   So depending on one's mood, one can place emphasis either on the linear map
, or just talk about F [X ]-modules without mentioning any linear maps.

EXERCISE 18.11. Let   LF (V ) have non-zero minimal polynomial  of degree
m, and let  = 1e1 · · · rer be the factorization of  into monic irreducible poly-
nomials in F [X ]. Let be the scalar multiplication associated with . Show
that   V has minimal polynomial  under  if and only if /i  = 0 for
i = 1, . . . , r.
506  Linearly generated sequences and applications

EXERCISE 18.12. Let   LF (V ) have non-zero minimal polynomial . Show
that  is bijective if and only if X .

EXERCISE 18.13. Let F be a finite field, and let V have finite dimension > 0
over F . Let   LF (V ) have minimal polynomial , with deg() = m (and of
course, by Theorem 18.13, we have m  ). Suppose that 1, . . . , s are randomly
chosen elements of V . Let gj be the minimal polynomial of j under , for j =
1, . . . , s. Let Q be the probability that lcm(g1, . . . , gs) = . The goal of this
exercise is to show that Q  F (s), where F (s) is as defined in §18.3.

    (a) Using Theorem 18.12 and Theorem 18.11, show that if m = , then Q =
         F (s).

   (b) Without the assumption that m = , things are a bit more challenging.

         Adopting the matrix-oriented point of view discussed at the end of §18.3,

         and transposing everything, show that

               - there exists   DF (V ) such that the sequence {   i i}=0 has
                  minimal polynomial , and

               - if, for j = 1, . . . , s, we define hj to be the minimal polynomial of the
                  sequence {(i(j))}i=0, then the probability that lcm(h1, . . . , hs) =
                   is equal to F (s).

    (c) Show that hj | gj, for j = 1, . . . , s, and conclude that Q  F (s).

EXERCISE 18.14. Let f , g  F [X ] with f = 0, and let h := f / gcd(f , g). Show

that g · F [X ]/(f ) and F [X ]/(h) are isomorphic as F [X ]-modules.

EXERCISE 18.15. In this exercise, you are to derive the fundamental theorem
of finite dimensional F [X ]-modules, which is completely analogous to the fun-
damental theorem of finite abelian groups. Both of these results are really special
cases of a more general decomposition theorem for modules over a principal ideal
domain. Let V be an F [X ]-module. Assume that as an F -vector space, V has
finite dimension > 0, and that the F [X ]-exponent of V is generated by the monic
polynomial   F [X ] (note that 1  deg()  ). Show that there exist monic,
non-constant polynomials 1, . . . , t  F [X ] such that

     · i | i+1 for i = 1, . . . , t - 1, and

     · V is isomorphic, as an F [X ]-module, to the direct product of F [X ]-modules

                             V := F [X ]/(1) × · · · × F [X ]/(t).

Moreover, show that the polynomials 1, . . . , t satisfying these conditions are
uniquely determined, and that t = . Hint: one can just mimic the proof of

Theorem 6.45, where the exponent of a group corresponds to the F [X ]-exponent of
18.6 The algebra of linear transformations ()                   507

an F [X ]-module, and the order of a group element corresponds to the F [X ]-order

of an element of an F [X ]-module -- everything translates rather directly, with just
a few minor, technical differences, and the previous exercise is useful in proving

the uniqueness part of the theorem.

EXERCISE 18.16. Let us adopt the same assumptions and notation as in Exer-
cise 18.15, and let   LF (V ) be the map that sends   V to X . Further,
let  : V  V be the isomorphism of that exercise, and let   LF (V ) be the
X -multiplication map on V .

    (a) Show that    =   .

(b) From part (a), derive the following: there exists a basis for V over F , with

respect to which the matrix of  is the "block diagonal" matrix

     C1                                         

         C2                                     
                                                
T =          .. ,
             .

             Ct

where each Ci is the companion matrix of i (see Example 14.1).

EXERCISE 18.17. Let us adopt the same assumptions and notation as in Exer-
cise 18.15.

   (a) Using the result of that exercise, show that V is isomorphic, as an F [X ]-
        module, to a direct product of F [X ]-modules

                               F [X ]/(f1e1 ) × · · · × F [X ]/(frer ),

where the fi's are monic irreducible polynomials (not necessarily distinct)
and the ei's are positive integers, and this direct product is unique up to the
order of the factors.

(b) Using part (a), show that there exists a basis for V over F , with respect to

which the matrix of  is the "block diagonal" matrix

     C1                                         

C                                               
         2                                      
T =          .. ,
             .

             Cr

where each Ci is the companion matrix of fiei.

EXERCISE 18.18. Let us adopt the same assumptions and notation as in Exer-
cise 18.15.
508  Linearly generated sequences and applications

(a) Suppose   V corresponds to ([g1]1, . . . , [gt]t )  V under the isomor-
    phism of that exercise. Show that the F [X ]-order of  is generated by the
     polynomial

     lcm(1/ gcd(g1, 1), . . . , t/ gcd(gt, t)).

(b) Using part (a), give a short and simple proof of the result of Exercise 18.13.

                                          18.7 Notes

Berlekamp [15] and Massey [64] discuss an algorithm for finding the minimal poly-
nomial of a linearly generated sequence that is closely related to the one presented
in §18.2, and which has a similar complexity. This connection between Euclid's
algorithm and finding minimal polynomials of linearly generated sequences has
been observed by many authors, including Mills [68], Welch and Scholtz [108],
and Dornstetter [36].

   The algorithm presented in §18.3 is due to Wiedemann [109], as are the algo-
rithms for solving sparse linear systems in §18.4, as well as the statement and proof
outline of the result in Exercise 18.13.

   Our proof of Theorem 18.4 is based on an exposition by Morrison [69].
   Using fast matrix and polynomial arithmetic, Shoup [96] shows how to imple-
ment the algorithms in §18.5 so as to use just O( (+1)/2) operations in F , where
 is the exponent for matrix multiplication (see §14.6), and so ( + 1)/2 < 1.7.

 The running times of these algorithms can be improved using faster algorithms for modular composition --
  see footnote on p. 485.
                            19

                          Finite fields

This chapter develops some of the basic theory of finite fields. As we already know
(see Theorem 7.7), every finite field must be of cardinality pw, for some prime p
and positive integer w. The main results of this chapter are:

     · for every prime p and positive integer w, there exists a finite field of cardi-
         nality pw, and

     · any two finite fields of the same cardinality are isomorphic.

                                     19.1 Preliminaries
We begin by stating some simple but useful divisibility criteria for polynomials
over an arbitrary field. These will play a crucial role in the development of the
theory.

  Let F be a field. A polynomial f  F [X ] is called square-free if it is not
divisible by the square of any polynomial of degree greater than zero. Using formal
derivatives (see §16.7), we obtain the following useful criterion for establishing
that a polynomial is square-free:
Theorem 19.1. If F is a field, and f  F [X ] with gcd(f, D(f )) = 1, then f is
square-free.
Proof. Suppose f is not square-free, and write f = g2h, for g, h  F [X ] with
deg(g) > 0. Taking formal derivatives, we have

                                D(f ) = 2gD(g)h + g2D(h),
and so clearly, g is a common divisor of f and D(f ). 2
Theorem 19.2. Let F be a field, and let k, be positive integers. Then X k - 1
divides X - 1 in F [X ] if and only if k divides .

                                               509
510  Finite fields

Proof. Let = kq + r, with 0  r < k. We have

     X  X kqX r  X r (mod X k - 1),

and X r  1 (mod X k - 1) if and only if r = 0. 2

Theorem 19.3. Let a  2 be an integer and let k, be positive integers. Then
ak - 1 divides a - 1 if and only if k divides .

Proof. The proof is analogous to that of Theorem 19.2. We leave the details to the
reader. 2

One may combine these last two theorems, obtaining:

Theorem 19.4. Let a  2 be an integer, k, be positive integers, and F a field.
Then X ak - X divides X a - X in F [X ] if and only if k divides .

Proof. Now, X ak - X divides X a - X if and only if X ak-1 - 1 divides X a -1 - 1. By
Theorem 19.2, this happens if and only if ak - 1 divides a - 1. By Theorem 19.3,
this happens if and only if k divides . 2

   We end this section by recalling some concepts discussed earlier, mainly in

§16.1, §16.5, and §16.6, that will play an important role in this chapter.

   Suppose F is a field, and E is an extension field of F ; that is, F is a subfield

of E (or, more generally, F is embedded in E via some canonical embedding, and

we identify elements of F with their images in E under this embedding). We may

view E as an F -algebra via inclusion, and in particular, as an F -vector space. If

E is also an extension field of F , and  : E  E is a ring homomorphism, then 
is an F -algebra homomorphism if and only if (a) = a for all a  F .

   Let us further assume that as an F -vector space, E has finite dimension . This

dimension is called the degree of E over F , and is denoted (E : F ), and E is

called a finite extension of F . Now consider an element   E. Then  is algebraic

over F , which means that there exists a non-zero polynomial g  F [X ] such that
g() = 0. The monic polynomial   F [X ] of least degree such that () = 0
is called the minimal polynomial of  over F . The polynomial  is irreducible
over F , and its degree m := deg() is called the degree of  over F . The ring
F [] = {g() : g  F [X ]}, which is the smallest subring of E containing F and
, is actually a field, and is isomorphic, as an F -algebra, to F [X ]/(), via the map
that sends g()  F [] to [g]  F [X ]/(). In particular, (F [] : F ) = m, and the
elements 1, , . . . , m-1 form a basis for F [] over F . Moreover, m divides .
19.2 The existence of finite fields                                   511

                            19.2 The existence of finite fields

Let F be a finite field. As we saw in Theorem 7.7, F must have cardinality pw,

where p is prime and w is a positive integer, and p is the characteristic of F . How-
ever, we can say a bit more than this. As discussed in Example 7.53, the field Zp
is embedded in F , and so we may simply view Zp as a subfield of F . Moreover, it
must be the case that w is equal to (F : Zp).

   We want to show that there exist finite fields of every prime-power cardinality.

Actually, we shall prove a more general result:

         If F is a finite field, then for every integer  1, there exists an
         extension field E of degree over F .

   For the remainder of this section, F denotes a finite field of cardinality q = pw,
where p is prime and w  1.

   Suppose for the moment that E is an extension of degree over F . Let us
derive some basic facts about E. First, observe that E has cardinality q . By Theo-
rem 7.29, E is cyclic, and the order of E is q -1. If   E is a generator for E,
then every non-zero element of E can be expressed as a power of ; in particular,
every element of E can be expressed as a polynomial in  with coefficients in F ;
that is, E = F []. Let   F [X ] be the minimal polynomial of  over F , which
is an irreducible polynomial of degree . It follows that E is isomorphic (as an
F -algebra) to F [X ]/().

   So we have shown that every extension of degree over F must be isomorphic,
as an F -algebra, to F [X ]/(f ) for some irreducible polynomial f  F [X ] of degree
 . Conversely, given any irreducible polynomial f over F of degree , we can
construct the finite field F [X ]/(f ), which has degree over F . Thus, the question
of the existence of a finite field of degree over F reduces to the question of the
existence of an irreducible polynomial over F of degree .

   We begin with a simple generalization of Fermat's little theorem:
Theorem 19.5. For every a  F , we have aq = a.

Proof. The multiplicative group of units F  of F has order q - 1, and hence, every
a  F  satisfies the equation aq-1 = 1. Multiplying this equation by a yields
aq = a for all a  F , and this latter equation obviously holds for a = 0 as well. 2

This simple fact has a number of consequences.

Theorem 19.6. We have
                                 X q - X = (X - a).

                                                                   aF

Proof. Since each a  F is a root of X q - X , by Theorem 7.13, the polynomial
512  Finite fields

  aF (X - a) divides the polynomial X q - X . Since the degrees and leading coeffi-
cients of these two polynomials are the same, the two polynomials must be equal. 2

Theorem 19.7. Let E be an F -algebra. Then the map  : E  E that sends
  E to q is an F -algebra homomorphism.

Proof. By Theorem 16.3, either E is trivial or contains an isomorphic copy of F as
a subring. In the former case, there is nothing to prove. So assume that E contains
an isomorphic copy of F as a subring. It follows that E must have characteristic p.

   Since q = pw, we see that  = w, where () := p. By the discussion in
Example 7.48, the map  is a ring homomorphism, and hence so is . Moreover,
by Theorem 19.5, we have

                              (c1E ) = (c1E )q = cq1Eq = c1E

for all c  F . Thus (see Theorem 16.5),  is an F -algebra homomorphism. 2

   The map  defined in Theorem 19.7 is called the Frobenius map on E over F .
In the case where E is a finite field, we can say more about it:

Theorem 19.8. Let E be a finite extension of F , and let  be the Frobenius map

on E over F . Then  is an F -algebra automorphism on E. Moreover, for all
  E, we have () =  if and only if   F .

Proof. The fact that  is an F -algebra homomorphism follows from the previous

theorem. Any ring homomorphism from a field into a field is injective (see Exer-

cise 7.47). Surjectivity follows from injectivity and finiteness.
   For the second statement, observe that () =  if and only if  is a root of

the polynomial X q - X , and since all q elements of F are already roots, by Theo-
rem 7.14, there can be no other roots. 2

   As the Frobenius map on finite fields plays a fundamental role in the study of

finite fields, let us develop a few simple properties right away. Suppose E is a finite

extension of F , and let  be the Frobenius map on E over F . Since the composition

of two F -algebra automorphisms is also an F -algebra automorphism, for every
i  0, the i-fold composition i, which sends   E to qi  E, is also an F -algebra
automorphism. Since  is an F -algebra automorphism, the inverse function -1 is
also an F -algebra automorphism. Hence, i is an F -algebra automorphism for all

i  Z. If E has degree over F , then applying Theorem 19.5 to the field E, we see
that  is the identity map. More generally, we have:

Theorem 19.9. Let E be a extension of degree over F , and let  be the Frobenius
map on E over F . Then for all integers i and j, we have i = j if and only if

i  j (mod ).
19.2 The existence of finite fields                                         513

Proof. We may assume i  j. We have
      i = j  i-j = 0  qi-j -  = 0 for all   E
                (X - ) | (X qi-j - X ) (by Theorem 7.13)

                                   E

                (X q - X ) | (X qi-j - X ) (by Theorem 19.6, applied to E)
                  | (i - j) (by Theorem 19.4)
                  i  j (mod ). 2

   From the above theorem, it follows that every power of the Frobenius map  can
be written uniquely as i for some i = 0, . . . , - 1.

The following theorem generalizes Theorem 19.6:

Theorem 19.10. For k  1, let Pk denote the product of all the monic irreducible
polynomials in F [X ] of degree k. For all positive integers , we have

                                   X q - X = Pk,

                                                                        k|

where the product is over all positive divisors k of .

Proof. First, we claim that the polynomial X q - X is square-free. This follows
immediately from Theorem 19.1, since D(X q - X ) = q X q -1 - 1 = -1.

   Thus, we have reduced the proof to showing that if f is a monic irreducible
polynomial of degree k, then f divides X q - X if and only if k divides .

  So let f be a monic irreducible polynomial of degree k. Let E := F [X ]/(f ) =
F [], where  := [X ]f  E. Observe that E is an extension field of degree k over
F . Let  be the Frobenius map on E over F .

  First, we claim that f divides X q - X if and only if  () = . Indeed, f is the
minimal polynomial of  over F , and so f divides X q - X if and only if  is a root
of X q - X , which is the same as saying q = , or equivalently,  () = .

   Second, we claim that  () =  if and only if  () =  for all   E. To
see this, first suppose that  () =  for all   E. Then in particular, this holds
for  = . Conversely, suppose that  () = . Every   E can be written as
 = g() for some g  F [X ], and since  is an F -algebra homomorphism, by

Theorem 16.7 we have

                         () =  (g()) = g( ()) = g() = .

   Finally, we see that  () =  for all   E if and only if  = 0, which by

Theorem 19.9 holds if and only if k | . 2
514                                        Finite fields

   For   1, let F ( ) denote the number of monic irreducible polynomials of
degree  in F [X ].

Theorem 19.11. For all   1, we have

                              q = kF (k).            (19.1)

                                          k|

Proof. Just equate the degrees of both sides of the identity in Theorem 19.10. 2

   From Theorem 19.11 it is easy to deduce that F ( ) > 0 for all , and in fact, one
can prove a density result -- essentially a "prime number theorem" for polynomials
over finite fields:

Theorem 19.12. For all   1, we have                  (19.2)
and                           q  F ( )  q ,          (19.3)
                              2

                          F ( ) = q + O q . /2

Proof. First, since all the terms in the sum on the right hand side of (19.1) are
non-negative, and F ( ) is one of these terms, we may deduce that F ( )  q ,

which proves the second inequality in (19.2). Since this holds for all , we have

                                                /2

            F ( ) = q - kF (k)  q - qk  q - qk.

                        k|  k|                  k=1

                        k<  k<

Let us set

                                /2

            S(q, ) := qk = q (q /2 - 1),
                         k=1 q - 1

so that F ( )  q - S(q, ). It is easy to see that S(q, ) = O(q /2), which proves
(19.3). For the first inequality of (19.2), it suffices to show that S(q, )  q /2.

One can verify this directly for  {1, 2, 3}, and for  4, we have

            S(q, )  q /2+1  q -1  q /2. 2

   We note that the inequalities in (19.2) are tight, in the sense that F ( ) = q /2
when q = 2 and = 2, and F ( ) = q when = 1. The first inequality in
(19.2) implies not only that F ( ) > 0, but that the fraction of all monic degree

polynomials that are irreducible is at least 1/2 , while (19.3) says that this fraction
gets arbitrarily close to 1/ as either q or are sufficiently large.
19.3 The subfield structure and uniqueness of finite fields             515

EXERCISE 19.1. Starting from Theorem 19.11, show that
                                 F ( ) = -1 µ(k)q /k,

                                                                    k|

where µ is the Möbius function (see §2.9).

EXERCISE 19.2. How many irreducible polynomials of degree 30 over Z2 are
there?

            19.3 The subfield structure and uniqueness of finite fields

Let E be an extension of degree over a field F . If K is an intermediate field,
that is, a subfield of E containing F , then Theorem 16.23 says that (E : F ) =
(E : K)(K : F ), and so in particular, the degree of K over F divides .

   In the case where F is a finite field, we can say much more about such inter-
mediate fields. Recall that if  : E  E be an F -algebra homomorphism, then
the subalgebra of E fixed by  is defined as K := {  E : () = } (see
Theorem 16.6). Not only is K a subalgebra of E, but it is also a field, and so K is
itself an intermediate field.

Theorem 19.13. Let E be an extension of degree over a finite field F . Let  be
the Frobenius map on E over F . Then the intermediate fields K, with F  K  E,
are in one-to-one correspondence with the divisors k of , where the divisor k
corresponds to the subalgebra of E fixed by k, which has degree k over F .

Proof. Let q be the cardinality of F .
   Suppose k is a divisor of . By Theorem 19.6 (applied to E), the polynomial

X q - X splits into distinct monic linear factors over E. By Theorem 19.4, the
polynomial X qk - X divides X q - X . Hence, X qk - X also splits into distinct
monic linear factors over E. This says that the subalgebra of E fixed by k, which
consists of the roots of X qk -X , has precisely qk elements, and hence is an extension
of degree k over F .

   Now let K be an arbitrary intermediate field, and let k be the degree of K over
F . As already mentioned, we must have k | . Also, by Theorem 19.8 (applied
with K in place of F ), K is the subalgebra of E fixed by k. 2

   The next theorem shows that up to isomorphism, there is only one finite field of
a given cardinality.

Theorem 19.14. Let E and E be finite extensions of the same degree over a finite
field F . Then E and E are isomorphic as F -algebras.

Proof. Let q be the cardinality of F , and let be the degree of the extensions.
516  Finite fields

As we have argued before, we have E = F [ ] for some   E , and so E is

isomorphic as an F -algebra to F [X ]/(), where  is the minimal polynomial of

 over F . As  is an irreducible polynomial of degree , by Theorem 19.10, 
divides X q - X , and by Theorem 19.6 (applied to E), X q - X = E (X - ),
from which it follows that  has a root   E. Since  is irreducible,  is the

minimal polynomial of  over F , and hence F [] is isomorphic as an F -algebra
to F [X ]/(). Since  has degree over F , we must have E = F []. Thus,
E = F [] = F [X ]/() = F [ ] = E . 2

EXERCISE 19.3. This exercise develops an alternative proof for the existence of
finite fields -- however, it does not yield a density result for irreducible polynomi-
als. Let F be a finite field of cardinality q, and let  1 be an integer. Let E be
a splitting field for the polynomial X q - X  F [X ] (see Theorem 16.25), and let
 be the Frobenius map on E over F . Let K be the subalgebra of E fixed by  .
Show that K is an extension of F of degree .

EXERCISE 19.4. Let E be an extension of degree over a finite field F of cardi-
nality q. Show that at least half the elements of E have degree over F , and that
the total number of elements of degree over F is q + O(q /2).

EXERCISE 19.5. Let E be a finite extension of a finite field F , and suppose
,   E, where  has degree a over F ,  has degree b over F , and gcd(a, b) = 1.
Show that  has degree b over F [], that  has degree a over F [], and that + has
degree ab over F . Hint: consider the subfields F [], F [], F [][] = F [, ] =
F [][], and F [ + ], and their degrees over F .

                           19.4 Conjugates, norms and traces

Throughout this section, F denotes a finite field of cardinality q, E denotes an
extension of degree over F , and  denotes the Frobenius map on E over F .

   Consider an element   E. We say that   E is conjugate to  (over F )
if  = i() for some i  Z. The reader may verify that the "conjugate to"
relation is an equivalence relation. We call the equivalence classes of this relation
conjugacy classes, and we call the elements of the conjugacy class containing 
the conjugates of .

   Starting with , we can start listing conjugates:

                                      , (), 2(), . . . .

As  is the identity map, this list will eventually start repeating. Let k be the
smallest positive integer such that k() = i() for some i = 0, . . . , k - 1. It must
     19.4 Conjugates, norms and traces       517

be the case that i = 0 -- otherwise, applying -1 to the equation k() = i()
would yield k-1() = i-1(), and since 0  i - 1 < k - 1, this would contradict

the minimality of k.
   Thus, , (), . . . , k-1() are all distinct, and k() = . Moreover, for every

integer i, we have i() = j(), where j = i mod k. Therefore, the k distinct
elements , (), . . . , k-1() are all the conjugates of . Also, i() =  if and
only if k divides i, and since  () = , it must be the case that k divides . In

addition, the conjugates of  are powers of , and in particular, they all belong to

F [].

   With  and k as above, consider the polynomial

                                                           k-1

                                   := (X - i()).

                                                            i=0

The coefficients of  obviously lie in E, but we claim that in fact, they lie in F .

This is easily seen as follows. Extend the domain of definition of  from E to E[X ]
by applying  coefficient-wise to polynomials; this yields a ring homomorphism

from E[X ] into E[X ], which we also denote by  (see Example 7.46). Applying 
to , we obtain

k-1                          k-1        k-1

() = (X - i()) = (X - i+1()) = (X - i()),

i=0                          i=0        i=0

since k() = . Thus we see that () = . Writing  = i ciX i, it follows
that (ci) = ci for all i, and hence by Theorem 19.8, ci  F for all i. Hence
  F [X ]. We further claim that  is the minimal polynomial of . To see this, let

f  F [X ] be any polynomial over F for which  is a root. Then for every integer

i, by Theorem 16.7, we have

     0 = i(0) = i(f ()) = f (i()).

Thus, all the conjugates of  are also roots of f, and so  divides f. That proves

that  is the minimal polynomial of . Since  is the minimal polynomial of  and
deg() = k, it follows that the number k is none other than the degree of  over F .

   Let us summarize the above discussion as follows:

Theorem 19.15. Let   E be of degree k over F , and let  be the minimal poly-
nomial of  over F . Then k is the smallest positive integer such that k() = ,
the distinct conjugates of  are , (), . . . , k-1(), and  factors over E (in fact,

over F []) as

                                                           k-1

                                   = (X - i()).

                                                           i=0
518  Finite fields

   Another useful way of reasoning about conjugates is as follows. First, if  = 0,

then the degree of  over F is 1, and there is nothing more to say, so let us assume
that   E. If r is the multiplicative order of , then note that every conjugate
i() also has multiplicative order r -- this follows from the fact that for every
positive integer s, s = 1 if and only if (i())s = 1. Also, note that we must have
r | |E| = q - 1, or equivalently, q  1 (mod r). Focusing now on the fact that

 is the q-power map, we see that the degree k of  is the smallest positive integer
such that qk = , which holds if and only if qk-1 = 1, which holds if and only if
qk  1 (mod r). Thus, the degree of  over F is simply the multiplicative order of

q modulo r. Again, we summarize these observations as a theorem:

Theorem 19.16. If   E has multiplicative order r, then the degree of  over
F is equal to the multiplicative order of q modulo r.

   For   E, define the polynomial

                                                             -1

                                   := (X - i()).

                                                            i=0

It is easy to see, using the same type of argument as was used to prove Theo-
rem 19.15, that   F [X ], and indeed, that

                                            =  /k,

where k is the degree of  over F . The polynomial  is called the characteristic
polynomial of  (from E to F ).

   Two functions that are often useful are the "norm" and "trace." The norm of 
(from E to F ) is defined as

                                                                        -1

                                    NE/F () := i(),

                                                                       i=0

while the trace of  (from E to F ) is defined as

                                                                        -1

                                   TrE/F () := i().

                                                                       i=0

It is easy to see that both the norm and trace of  are elements of F , as they are
fixed by ; alternatively, one can see this by observing that they appear, possibly
with a minus sign, as coefficients of the characteristic polynomial  -- indeed, the
constant term of  is equal to (-1) NE/F (), and the coefficient of X -1 in  is
-TrE/F ().

   The following two theorems summarize the most important facts about the norm
and trace functions.
                 19.4 Conjugates, norms and traces                     519

Theorem 19.17. The function NE/F , restricted to E, is a group homomorphism
from E onto F .

Proof. We have

                                      -1  i=-01 qi = (q -1)/(q-1).

                 NE/F () = qi = 

                                     i=0

Since E is a cyclic group of order q - 1, the image of the (q - 1)/(q - 1)-power
map on E is the unique subgroup of E of order q - 1 (see Theorem 6.32). Since
F  is a subgroup of E of order q - 1, it follows that the image of this power map
is F . 2

Theorem 19.18. The function TrE/F is an F -linear map from E onto F .

Proof. The fact that TrE/F is an F -linear map is a simple consequence of the fact
that  is an F -linear map. As discussed above, TrE/F maps into F . Since the
image of TrE/F is a subspace of F , the image is either {0} or F , and so it suffices
to show that TrE/F does not map all of E to zero. But an element   E is in the
kernel of TrE/F if and only if  is a root of the polynomial

                                 X + X q + · · · + X q -1 ,

which has degree q -1. Since E contains q elements, not all elements of E can lie
in the kernel of TrE/F . 2

Example 19.1. As an application of some of the above theory, let us investigate the
factorization of the polynomial X r - 1 over F , a finite field of cardinality q. Let us
assume that r > 0 and is relatively prime to q. Let E be a splitting field of X r - 1
(see Theorem 16.25), so that E is a finite extension of F in which X r - 1 splits into

linear factors:

                                   r

                 X r - 1 = (X - i).

                                 i=1

  We claim that the roots i of X r - 1 are distinct -- this follows from the Theo-
rem 19.1 and the fact that gcd(X r - 1, rX r-1) = 1.

  Next, observe that the r roots of X r - 1 in E actually form a subgroup of E,
and since E is cyclic, this subgroup must be cyclic as well. So the roots of X r - 1
form a cyclic subgroup of E of order r. Let  be a generator for this group. Then

all the roots of X r - 1 are contained in F [], and so we may as well assume that
E = F [].

Let us compute the degree of  over F . By Theorem 19.16, the degree of

 over F is the multiplicative order of q modulo r. Moreover, the (r) roots of
520            Finite fields

X r - 1 of multiplicative order r are partitioned into (r)/ conjugacy classes, each

of size (here,  is Euler's phi function); indeed, as the reader is urged to verify,

these conjugacy classes are in one-to-one correspondence with the cosets of the

subgroup of Zr generated by [q]r, where each such coset C  Zr corresponds to
the conjugacy class {a : [a]r  C}.

  More generally, for every s | r, every root of X r - 1 whose multiplicative

order is s has degree k over F , where k is the multiplicative order of q modulo

s. As above, the (s) roots of multiplicative order s are partitioned into (s)/k

conjugacy classes, which are in one-to-one correspondence with the cosets of the

subgroup of Zs generated by [q]s.
  This tells us exactly how X r - 1 splits into irreducible factors over F . Things

are a bit simpler when r is prime, in which case, from the above discussion, we see

that

                                  (r-1)/

               X r - 1 = (X - 1)          fi,

                                  i=1

where the fi's are distinct monic irreducible polynomials, each of degree , and
is the multiplicative order of q modulo r.

   In the above analysis, instead of constructing the field E using Theorem 16.25,

one could instead simply construct E as F [X ]/(f ), where f is any irreducible
polynomial of degree , and where is the multiplicative order of q modulo r.

We know that such a polynomial f exists by Theorem 19.12, and since E has
cardinality q , and r | (q -1) = |E|, and E is cyclic, we know that E contains an
element  of multiplicative order r, and each of the r distinct powers 1, , . . . , r-1
are roots of X r - 1, and so this E is a splitting field of X r - 1 over F . 2

EXERCISE 19.6. Let E be an extension of degree over a finite field F . Show
that for a  F , we have NE/F (a) = a and TrE/F (a) = a.

EXERCISE 19.7. Let E be a finite extension of a finite field F .  Let K be an
intermediate field, F  K  E. Show that for all   E

    (a) NE/F () = NK/F (NE/K ()), and
   (b) TrE/F () = TrK/F (TrE/K ()).

EXERCISE 19.8. Let F be a finite field, and let f  F [X ] be a monic irreducible
polynomial of degree . Let E = F [X ]/(f ) = F [], where  := [X ]f .

(a) Show that

               D(f ) 
               = TrE/F ( )X .
                                          j-1 -j

               f j=1
19.4 Conjugates, norms and traces                               521

(b) From part (a), deduce that the sequence of elements
                                 TrE/F (j-1) (j = 1, 2, . . .)

     is linearly generated over F with minimal polynomial f .

(c) Show that one can always choose a polynomial f so that sequence in part
     (b) is purely periodic with period q - 1.

EXERCISE 19.9. Let F be a finite field, and f  F [X ] a monic irreducible poly-
nomial of degree k over F . Let E be an extension of degree over F . Show that

over E, f factors as the product of d distinct monic irreducible polynomials, each
of degree k/d, where d := gcd(k, ).

EXERCISE 19.10. Let E be a finite extension of a finite field F of characteristic
p. Show that if   E and 0 = a  F , and if  and  + a are conjugate over F , then

p divides the degree of  over F .

EXERCISE 19.11. Let F be a finite field of characteristic p. For a  F , consider
the polynomial f := X p - X - a  F [X ].

    (a) Show that if F = Zp and a = 0, then f is irreducible.
   (b) More generally, show that if TrF/Zp (a) = 0, then f is irreducible, and

         otherwise, f splits into distinct monic linear factors over F .

EXERCISE 19.12. Let E be a finite extension of a finite field F . Show that every
F -algebra automorphism on E must be a power of the Frobenius map on E over F .

EXERCISE 19.13. Show that for all primes p, the polynomial X 4 + 1 is reducible
in Zp[X ]. (Contrast this to the fact that this polynomial is irreducible in Q[X ], as
discussed in Exercise 16.49.)

EXERCISE 19.14. This exercise depends on the concepts and results in §18.6. Let
E be an extension of degree over a finite field F . Let  be the Frobenius map on
E over F .

   (a) Show that the minimal polynomial of  over F is X - 1.

   (b) Show that there exists   E such that the minimal polynomial of  under
         is X - 1.

    (c) Conclude that , (), . . . ,  -1() form a basis for E over F . This type of
         basis is called a normal basis.
                            20

                 Algorithms for finite fields

This chapter discusses efficient algorithms for factoring polynomials over finite
fields, and related problems, such as testing if a given polynomial is irreducible,
and generating an irreducible polynomial of given degree.

         Throughout this chapter, F denotes a finite field of characteristic p
         and cardinality q = pw.
   In addition to performing the usual arithmetic and comparison operations in F ,
we assume that our algorithms have access to the numbers p, w, and q, and have the
ability to generate random elements of F . Generating such a random field element
will count as one "operation in F ," along with the usual arithmetic operations. Of
course, the "standard" ways of representing F as either Zp (if w = 1), or as the ring
of polynomials modulo an irreducible polynomial over Zp of degree w (if w > 1),
satisfy the above requirements, and also allow for the implementation of arithmetic
operations in F that take time O(len(q)2) on a RAM (using simple, quadratic-time
arithmetic for polynomials and integers).

             20.1 Tests for and constructing irreducible polynomials
Let f  F [X ] be a monic polynomial of degree > 0. We develop here an efficient
algorithm that determines if f is irreducible.

   The idea is a simple application of Theorem 19.10. That theorem says that for
every integer k  1, the polynomial X qk -X is the product of all monic irreducibles
whose degree divides k. Thus, gcd(X q -X , f ) is the product of all the distinct linear
factors of f . If f has no linear factors, then gcd(X q2 - X , f ) is the product of all the
distinct quadratic irreducible factors of f . And so on. Now, if f is not irreducible,
it must be divisible by some irreducible polynomial of degree at most /2, and if g
is an irreducible factor of f of minimal degree, say k, then we have k  /2 and
gcd(X qk - X , f ) = 1. Conversely, if f is irreducible, then gcd(X qk - X , f ) = 1 for

                                               522
20.1 Tests for and constructing irreducible polynomials          523

all positive integers k up to /2. So to test if f is irreducible, it suffices to check if
gcd(X qk - X , f ) = 1 for all positive integers k up to /2 -- if so, we may conclude

that f is irreducible, and otherwise, we may conclude that f is not irreducible.
To carry out the computation efficiently, we note that if h  X qk (mod f ), then
gcd(h - X , f ) = gcd(X qk - X , f ).

   The above observations suggest the following algorithm.

Algorithm IPT. On input f, where f  F [X ] is a monic polynomial of degree
  > 0, determine if f is irreducible as follows:

h  X mod f

for k  1 to /2 do
      h  hq mod f
      if gcd(h - X , f ) = 1 then return false

return true

   The correctness of Algorithm IPT follows immediately from the above discus-
sion. As for the running time, we have:

Theorem 20.1. Algorithm IPT uses O( 3 len(q)) operations in F .

Proof. Consider an execution of a single iteration of the main loop. The cost of
the qth-powering step (using a standard repeated-squaring algorithm) is O(len(q))
multiplications modulo f , and so O( 2 len(q)) operations in F . The cost of the
gcd computation is O( 2) operations in F . Thus, the cost of a single loop iteration
is O( 2 len(q)) operations in F , from which it follows that the cost of the entire
algorithm is O( 3 len(q)) operations in F . 2

   Using a standard representation for F , each operation in F takes time O(len(q)2)
on a RAM, and so the running time of Algorithm IPT on a RAM is O( 3 len(q)3),
which means that it is a polynomial-time algorithm.

   Let us now consider the related problem of constructing an irreducible poly-
nomial of specified degree > 0. To do this, we can simply use the result of
Theorem 19.12, which has the following probabilistic interpretation: if we choose
a random, monic polynomial f of degree over F , then the probability that f is
irreducible is at least 1/2 . This suggests the following probabilistic algorithm:

Algorithm RIP. On input , where is a positive integer, generate a monic irre-
ducible polynomial f  F [X ] of degree as follows:
524  Algorithms for finite fields

         repeat
               choose c0, . . . , c -1  F at random
              set f  X + i=0 -1 ciX i
               test if f is irreducible using Algorithm IPT

         until f is irreducible
         output f

Theorem 20.2. Algorithm RIP uses an expected number of O( 4 len(q)) opera-
tions in F , and its output is uniformly distributed over all monic irreducibles of
degree .

Proof. This is a simple application of the generate-and-test paradigm (see Theo-
rem 9.3, and Example 9.10 in particular). Because of Theorem 19.12, the expected
number of loop iterations of the above algorithm is O( ). Since Algorithm IPT
uses O( 3 len(q)) operations in F , the statement about the running time of Algo-
rithm RIP is immediate. The statement about its output distribution is clear. 2

   The expected running-time bound in Theorem 20.2 is actually a bit of an over-
estimate. The reason is that if we generate a random polynomial of degree , it
is likely to have a small irreducible factor, which will be discovered very quickly
by Algorithm IPT. In fact, it is known (see §20.7) that the expected value of
the degree of the least degree irreducible factor of a random monic polynomial of
degree over F is O(len( )), from which it follows that the expected number of
operations in F performed by Algorithm RIP is actually O( 3 len( ) len(q)).

EXERCISE 20.1. Let f  F [X ] be a monic polynomial of degree > 0. Also, let
 := [X ]f  E, where E is the F -algebra E := F [X ]/(f ).

    (a) Given as input   E and qm  E (for some integer m > 0), show how to
         compute the value qm  E, using just O( 2.5) operations in F , and space
         for O( 1.5) elements of F . Hint: see Theorems 16.7 and 19.7, as well as

         Exercise 17.3.
   (b) Given as input qm  E and qm  E, where m and m are positive integers,

         show how to compute the value qm+m  E, using O( 2.5) operations in F ,
         and space for O( 1.5) elements of F .

    (c) Given as input q  E and a positive integer m, show how to compute the
         value qm  E, using O( 2.5 len(m)) operations in F , and space for O( 1.5)

         elements of F . Hint: use a repeated-squaring-like algorithm.

EXERCISE 20.2. This exercise develops an alternative irreducibility test.

   (a) Show that a monic polynomial f  F [X ] of degree > 0 is irreducible if
        and only if X q  X (mod f ) and gcd(X q /s - X , f ) = 1 for all primes s | .
20.2 Computing minimal polynomials in F [X ]/(f ) (III)  525

(b) Using part (a) and the result of the previous exercise, show how to deter-
     mine if f is irreducible using O( 2.5 len( )( ) + 2 len(q)) operations in
     F , where ( ) is the number of distinct prime factors of .

(c) Show that the operation count in part (b) can be reduced to

                            O( 2.5 len( ) len(( )) + 2 len(q)).

Hint: see Exercise 3.39.

EXERCISE 20.3. Design and analyze a deterministic algorithm that takes as input

a list of irreducible polynomials f1, . . . , fr  F [X ], where i := deg(fi) for
i = 1, . . . , r, and assume that { i}ir=1 is pairwise relatively prime. Your algorithm
should output an irreducible polynomial f  F [X ] of degree := ir=1 i using
O( 3) operations in F . Hint: use Exercise 19.5.

EXERCISE 20.4. Design and analyze a probabilistic algorithm that, given a monic
irreducible polynomial f  F [X ] of degree as input, generates as output a random
monic irreducible polynomial g  F [X ] of degree (i.e., g should be uniformly
distributed over all such polynomials), using an expected number of O( 2.5) oper-
ations in F . Hint: use Exercise 18.9 (or alternatively, Exercise 18.10).

EXERCISE 20.5. Let f  F [X ] be a monic irreducible polynomial of degree , let
E := F [X ]/(f ), and let  := [X ]f  E. Design and analyze a deterministic algo-
rithm that takes as input the polynomial f defining the extension E, and outputs
the values

                         sj := TrE/F (j)  F (j = 0, . . . , - 1),

using O( 2) operations in F . Here, TrE/F is the trace from E to F (see §19.4).
Show that given an arbitrary   E, along with the values s0, . . . , s -1, one can
compute TrE/F () using just O( ) operations in F .

            20.2 Computing minimal polynomials in F [X ]/(f ) (III)

We consider, for the third and final time, the problem considered in §17.2 and
§18.5: f  F [X ] is a monic polynomial of degree > 0, and E := F [X ]/(f ) =
F [], where  := [X ]f ; we are given an element   E, and want to compute the
minimal polynomial   F [X ] of  over F . We develop an alternative algorithm,
based on the theory of finite fields. Unlike the algorithms in §17.2 and §18.5, this
algorithm only works when F is finite and the polynomial f is irreducible, so that
E is also a finite field.

   From Theorem 19.15, we know that the degree of  over F is the smallest pos-
itive integer k such that qk = . By successive qth powering, we can determine
526      Algorithms for finite fields

the degree k and compute the conjugates , q, . . . , qk-1 of , using O(k len(q))

operations in E, and hence O(k 2 len(q)) operations in F .

Now, we could simply compute the minimal polynomial  by directly using the

formula

                      k-1                                   (20.1)

         (Y ) = (Y - qi).

         i=0

This would involve computations with polynomials in the variable Y whose coef-

ficients lie in the extension field E, although at the end of the computation, we
would end up with a polynomial all of whose coefficients lie in F . The cost of this
approach would be O(k2) operations in E, and hence O(k2 2) operations in F .

  A more efficient approach is the following. Substituting  for Y in the identity

(20.1), we have

                                                                k-1

                                     () = ( - qi).

                                                                i=0

Using this formula, we can compute (given the conjugates of ) the value ()  E
using O(k) operations in E, and hence O(k 2) operations in F . Now, () is an

element of E, and for computational purposes, it is represented as [g]f for some
polynomial g  F [X ] of degree less than . Moreover, () = []f , and hence
  g (mod f ). In particular, if k < , then g = ; otherwise, if k = , then
g =  - f . In either case, we can recover  from g with an additional O( )

operations in F .
   Thus, given the conjugates of , we can compute  using O(k 2) operations in

F . Adding in the cost of computing the conjugates, this gives rise to an algorithm
that computes the minimal polynomial of  using O(k 2 len(q)) operations in F .

   In the worst case, then, this algorithm uses O( 3 len(q)) operations in F . A

reasonably careful implementation needs space for storing a constant number of

elements of E, and hence O( ) elements of F . For very small values of q, the
efficiency of this algorithm will be comparable to that of the algorithm in §18.5,
but for large q, it will be much less efficient. Thus, this approach does not really

yield a better algorithm, but it does serve to illustrate some of the ideas of the

theory of finite fields.

             20.3 Factoring polynomials: square-free decomposition

In the remaining sections of this chapter, we develop efficient algorithms for fac-
toring polynomials over the finite field F . We begin in this section with a simple
and efficient preprocessing step. Recall that a polynomial is called square-free if it
is not divisible by the square of any polynomial of degree greater than zero. This
             20.3 Factoring polynomials: square-free decomposition          527

preprocessing algorithm takes the polynomial to be factored, and partially factors

it into a product of square-free polynomials. Given this algorithm, we can focus

our attention on the problem of factoring square-free polynomials.

  Let f  F [X ] be a monic polynomial of degree > 0. Suppose that f is not
square-free. According to Theorem 19.1, d := gcd(f , D(f )) = 1, where D(f ) is

the formal derivative of f ; thus, we might hope to get a non-trivial factorization of
f by computing d. However, we have to consider the possibility that d = f. Can
this happen? The answer is "yes," but if it does happen that d = f , we can still get

a non-trivial factorization of f by other means:

Theorem 20.3. Suppose that f  F [X ] is a monic polynomial of degree > 0,
and that gcd(f , D(f )) = f . Then f = g(X p) for some g  F [X ]. Moreover, if
g = i aiX i, then f = hp, where

                              h = aip X . (w-1) i                           (20.2)

                                       i

Proof. Since deg(D(f )) < deg(f ) and gcd(f, D(f )) = f, we must have D(f ) = 0.
If f = i ciX i, then D(f ) = i iciX i-1. Since this derivative must be zero, it
follows that all the coefficients ci with i  0 (mod p) must be zero to begin with.
That proves that f = g(X p) for some g  F [X ]. Furthermore, if h is defined as

above, then

       hp =       p(w-1) i p     aip Xw ip =     ai(X p)i = g(X p) = f . 2

                ai X =

             i                i               i

Our goal now is to design an efficient algorithm that takes as input a monic poly-

nomial f  F [X ] of degree > 0, and outputs a list of pairs ((g1, s1), . . . , (gt, st)),

where

· each gi  F [X ] is monic, non-constant, and square-free,

     · each si is a positive integer,
     · the family of polynomials {gi}it=1 is pairwise relatively prime, and
     · f = it=1 gisi .
We call such a list a square-free decomposition of f. There are a number of ways
to do this. The algorithm we present is based on the following theorem, which
itself is a simple consequence of Theorem 20.3.

Theorem 20.4. Let f  F [X ] be a monic polynomial of degree > 0. Suppose
that the factorization of f into irreducibles is f = f1e1 · · · frer . Then

                              f = fi.
                gcd(f, D(f ))
                                              1ir

                                 ei0 (mod p)
528        Algorithms for finite fields

Proof. The theorem can be restated in terms of the following claim:  for each
i = 1, . . . , r, we have

     · fiei | D(f ) if ei  0 (mod p), and
     · fiei-1 | D(f ) but fiei D(f ) if ei  0 (mod p).

To prove the claim, we take formal derivatives using the usual rule for products,

obtaining

           D(f ) = ejfjej-1D(fj) fkek .                              (20.3)

           j  k=j

Consider a fixed index i. Clearly, fiei divides every term in the sum on the right-
hand side of (20.3), with the possible exception of the term with j = i. In the case
where ei  0 (mod p), the term with j = i vanishes, and that proves the claim in
this case. So assume that ei  0 (mod p). By the previous theorem, and the fact
that fi is irreducible, and in particular, not the pth power of any polynomial, we
see that D(fi) is non-zero, and (of course) has degree strictly less than that of fi.
From this, and (again) the fact that fi is irreducible, it follows that the term with
j = i is divisible by fiei-1, but not by fiei, from which the claim follows. 2

   This theorem provides the justification for the following square-free decompo-
sition algorithm.

Algorithm SFD. On input f, where f  F [X ] is a monic polynomial of degree
  > 0, compute a square-free decomposition of f as follows:

     initialize an empty list L

     s1

     repeat
           j  1, g  f / gcd(f, D(f ))
           while g = 1 do

                  f  f /g, h  gcd(f, g), m  g/h
                  if m = 1 then append (m, js) to L
                  g  h, j  j + 1

           if f = 1 then // f is a pth power

                  // compute a pth root as in (20.2)
                  f  f 1/p, s  ps
     until f = 1

     output L

Theorem 20.5. Algorithm SFD correctly computes a square-free decomposition
of f using O( 2 + (w - 1) len(p)/p) operations in F .

Proof. Let f = i fiei be the factorization of the input f into irreducibles. Let S
20.3 Factoring polynomials: square-free decomposition  529

be the set of indices i such that ei  0 (mod p), and let S be the set of indices
i such that ei  0 (mod p). Also, for j  1, let Sj := {i  S : ei  j} and
S=j := {i  S : ei = j}.

   Consider the first iteration of the main loop. By Theorem 20.4, the value first

assigned to g is iS fi. It is straightforward to prove by induction on j that at
the beginning of the jth iteration of the inner while loop, the value assigned to g is

   iSj fi, and the value assigned to f is iSj fiei-j+1 · iS fiei . Moreover, in
the jth loop iteration, the value assigned to m is iS=j fi. It follows that when the
while loop terminates, the value assigned to f is iS fiei, and the value assigned
to L is a square-free decomposition of iS fiei; if f does not equal 1 at this
point, then subsequent iterations of the main loop will append to L a square-free
decomposition of iS fiei.

   That proves the correctness of the algorithm. Now consider its running time.

Again, consider just the first iteration of the main loop. The cost of computing
f / gcd(f , D(f )) is at most C1 2 operations in F , for some constant C1. Now

consider the cost of the inner while loop. It is not hard to see that the cost of the

jth iteration of the inner while loop is at most

C2       deg(fi)

    iSj

operations in F , for some constant C2. This follows from the observation in the
previous paragraph that the value assigned to g is iSj fi, along with our usual
cost estimates for division and Euclid's algorithm. Therefore, the total cost of all

iterations of the inner while loop is at most

C2          deg(fi)

    j1 iSj

operations in F . In this double summation, for each i  S, the term deg(fi) is
counted exactly ei times, and so we can write this cost estimate as

C2 ei deg(fi)  C2 2.

       iS

Finally, it is easy to see that in the if-then statement at the end of the main loop
body, if the algorithm does in fact compute a pth root, then this takes at most

C3 (w - 1) len(p)/p

operations in F , for some constant C3. Thus, we have shown that the total cost of
the first iteration of the main loop is at most

                            (C1 + C2) 2 + C3 (w - 1) len(p)/p
530  Algorithms for finite fields

operations in F . If the main loop is executed a second time, the degree of f at the
start of the second iteration is at most /p, and hence the cost of the second loop
iteration is at most

                       (C1 + C2)( /p)2 + C3( /p)(w - 1) len(p)/p

operations in F . More generally, for t = 1, 2, . . . , the cost of loop iteration t is at
most

                   (C1 + C2)( /pt-1)2 + C3( /pt-1)(w - 1) len(p)/p,

operations in F , and summing over all t  1 yields the stated bound. 2

        20.4 Factoring polynomials: the Cantor-Zassenhaus algorithm

In this section, we present an algorithm due to Cantor and Zassenhaus for factoring
a given polynomial over the finite field F into irreducibles. We shall assume that
the input polynomial is square-free, using Algorithm SFD in §20.3 as a preprocess-
ing step, if necessary. The algorithm has two stages:

Distinct Degree Factorization: The input polynomial is decomposed into factors
         so that each factor is a product of distinct irreducibles of the same degree
          (and the degree of those irreducibles is also determined).

Equal Degree Factorization: Each of the factors produced in the distinct degree
         factorization stage are further factored into their irreducible factors.

   The algorithm we present for distinct degree factorization is a deterministic,
polynomial-time algorithm. The algorithm we present for equal degree factoriza-
tion is a probabilistic algorithm that runs in expected polynomial time (and whose
output is always correct).

                           20.4.1 Distinct degree factorization

The problem, more precisely stated, is this: given a monic, square-free polynomial
f  F [X ] of degree > 0, produce a list of pairs ((g1, k1), . . . , (gt, kt)) where

     · each gi is the product of monic irreducible polynomials of degree ki, and
     · f = it=1 gi.

   This problem can be easily solved using Theorem 19.10, using a simple variation
of the algorithm we discussed in §20.1 for irreducibility testing. The basic idea is
this. We can compute g := gcd(X q - X , f ), so that g is the product of all the
linear factors of f . After removing all linear factors from f , we next compute
gcd(X q2 -X , f ), which will be the product of all the quadratic irreducibles dividing
f , and we can remove these from f -- although X q2 - X is the product of all linear
20.4 Factoring polynomials: the Cantor-Zassenhaus algorithm      531

and quadratic irreducibles, since we have already removed the linear factors from
f , the gcd will give us just the quadratic factors of f. In general, for k = 1, . . . , ,

having removed all the irreducible factors of degree less than k from f, we compute
gcd(X qk - X , f ) to obtain the product of all the irreducible factors of f of degree

k, and then remove these from f.

   The above discussion leads to the following algorithm for distinct degree factor-
ization.

Algorithm DDF. On input f, where f  F [X ] is a monic square-free polynomial
of degree > 0, do the following:

         initialize an empty list L
        h  X mod f
         k0
         while f = 1 do

               h  hq mod f , k  k + 1
              g  gcd(h - X , f )
               if g = 1 then

                      append (g, k) to L
                      f  f /g
                      h  h mod f
         output L

   The correctness of Algorithm DDF follows from the discussion above. As for
the running time:

Theorem 20.6. Algorithm DDF uses O( 3 len(q)) operations in F .

Proof. Note that the body of the main loop is executed at most times, since after

  iterations, we will have removed all the factors of f . Thus, we perform at most
  qth-powering steps, each of which takes O( 2 len(q)) operations in F , and so the
total contribution to the running time of these is O( 3 len(q)) operations in F . We

also have to take into account the cost of the gcd and division computations. The
cost per loop iteration of these is O( 2) operations in F , contributing a term of
O( 3) to the total operation count. This term is dominated by the cost of the qth-
powering steps, and so the total cost of Algorithm DDF is O( 3 len(q)) operations

in F . 2
532  Algorithms for finite fields

                            20.4.2 Equal degree factorization
The problem, more precisely stated, is this: given a monic polynomial f  F [X ]
of degree > 0, and an integer k > 0, such that f is of the form

                                         f = f1 · · · fr

for distinct monic irreducible polynomials f1, . . . , fr, each of degree k, compute
these irreducible factors of f. Note that given f and k, the value of r is easily
determined, since r = /k.

   We begin by discussing the basic mathematical ideas that will allow us to effi-

ciently split f into two non-trivial factors, and then we present a somewhat more

elaborate algorithm that completely factors f .

   By the Chinese remainder theorem, we have an F -algebra isomorphism

                                : E  E1 × · · · × Er

                                     [g]f  ([g]f1, . . . , [g]fr ),

where E is the F -algebra F [X ]/(f ), and for i = 1, . . . , r, Ei is the extension field
F [X ]/(fi) of degree k over F .

   Recall that q = pw. We have to treat the cases p = 2 and p > 2 separately. We
first treat the case p = 2. Let us define the polynomial

                                                            wk-1             (20.4)

                                Mk := X 2j  F [X ].

                                                             j=0

(The algorithm in the case p > 2 will only differ in the definition of Mk.)
  For   E, if () = (1, . . . , r), then we have

     (Mk()) = Mk(()) = (Mk(1), . . . , Mk(r)).

Note that each Ei is an extension of Z2 of degree wk, and that

              wk-1

     Mk(i) =        i2j = TrEi/Z2 (i),

              j=0

where TrEi/Z2 : Ei  Z2 is the trace from Ei to Z2, which is a surjective, Z2-linear

map (see §19.4).

   Now, suppose we choose   E at random. Then if () = (1, . . . , r), the fam-
ily of random variables {i}ir=1 is mutually independent, with each i uniformly
distributed over Ei. It follows that the family of random variables {Mk(i)}ri=1 is
mutually independent, with each Mk(i) uniformly distributed over Z2. Thus, if
g := rep(Mk()) (i.e., g  F [X ] is the polynomial of degree less than such that
Mk() = [g]f ), then gcd(g, f ) will be the product of those factors fi of f such
that Mk(i) = 0. We will fail to get a non-trivial factorization only if the Mk(i)
20.4 Factoring polynomials: the Cantor-Zassenhaus algorithm  533

are either all 0 or all 1, which for r  2 happens with probability at most 1/2 (the
worst case being when r = 2).

   That is our basic splitting strategy. The algorithm for completely factoring f

works as follows. The algorithm proceeds in stages. At any stage, we have a partial
factorization f = hH h, where H is a set of non-constant, monic polynomials.
Initially, H = {f}. With each stage, we attempt to get a finer factorization of f

by trying to split each h  H using the above splitting strategy -- if we succeed in

splitting h into two non-trivial factors, then we replace h by these two factors. We
continue in this way until |H| = r.

Here is the full equal degree factorization algorithm.

Algorithm EDF. On input f, k, where f  F [X ] is a monic polynomial of degree
  > 0, and k is a positive integer, such that f is the product of r := /k distinct

monic irreducible polynomials, each of degree k, do the following, with Mk as
defined in (20.4):

         H  {f}
         while |H| < r do

               H 
               for each h  H do

                     choose   F [X ]/(h) at random
                      d  gcd(rep(Mk()), h)
                      if d = 1 or d = h

                             then H  H  {h}
                             else H  H  {d, h/d}
               H H
         output H

   The correctness of the algorithm is clear from the above discussion. As for its
expected running time, we can get a quick-and-dirty upper bound as follows:

· For a given h and   F [X ]/(h), the value Mk() can be computed using
   O(k deg(h)2 len(q)) operations in F , and so the number of operations in F

   performed in each iteration of the main loop is at most a constant times

k len(q) deg(h)2  k len(q)                   2

                                deg(h) = k 2 len(q).

hH                          hH

· The expected number of iterations of the main loop until we get some non-
   trivial split is O(1).

· The algorithm finishes after getting r - 1 non-trivial splits.
534                      Algorithms for finite fields

     · Therefore, the total expected cost is O(rk 2 len(q)), or O( 3 len(q)), oper-
        ations in F .

This analysis gives a bit of an over-estimate -- it does not take into account the
fact that we expect to get fairly "balanced" splits. For the purposes of analyzing
the overall running time of the Cantor-Zassenhaus algorithm, this bound suffices;
however, the following analysis gives a tight bound on the complexity of Algo-
rithm EDF.

Theorem 20.7. In the case p = 2, Algorithm EDF uses an expected number of
O(k 2 len(q)) operations in F .

Proof. We may assume r  2. Let L be the random variable that represents the
number of iterations of the main loop of the algorithm. For n  1, let Hn be
the random variable that represents the value of H at the beginning of the nth
loop iteration. For i, j = 1, . . . , r, we define Lij to be the largest value of n (with
1  n  L) such that fi | h and fj | h for some h  Hn.

   We first claim that E[L] = O(len(r)). To prove this claim, we make use of the
fact (see Theorem 8.17) that

                                     E[L] = P[L  n].

                                                               n1

Now, L  n if and only if for some i, j with 1  i < j  r, we have Lij  n.
Moreover, if fi and fj have not been separated at the beginning of one loop itera-
tion, then they will be separated at the beginning of the next with probability 1/2.
It follows that

                                     P[Lij  n] = 2-(n-1).

So we have

                P[L  n]  P[Lij  n]  r22-n.

                                    i<j

Therefore,

     E[L] = P[L  n] =                P[L  n] +              P[L  n]

            n1           n2 log2 r              n>2 log2 r

             2 log2 r +              r22-n  2 log2 r + 2-n = 2 log2 r + 2,

                         n>2 log2 r             n0

which proves the claim.

   As discussed in the paragraph above this theorem, the cost of each iteration of
the main loop is O(k 2 len(q)) operations in F . Combining this with the fact that
E[L] = O(len(r)), it follows that the expected number of operations in F for the
        20.4 Factoring polynomials: the Cantor-Zassenhaus algorithm     535

entire algorithm is O(len(r)k 2 len(q)). This is significantly better than the above
quick-and-dirty estimate, but is not quite the result we are after. For this, we have
to work a little harder.

   For each polynomial h dividing f, define (h) to be the number of irreducible
factors of h. Let us also define the random variable

                                       L

                                 S :=             (h)2.

                                       n=1 hHn

It is easy to see that the total number of operations performed by the algorithm is
O(Sk3 len(q)), and so it will suffice to show that E[S] = O(r2).

   We claim that
                                          S = Lij,

                                                                   i,j

where the sum is over all i, j = 1, . . . , r. To see this, define ij(h) to be 1 if both fi
and fj divide h, and 0 otherwise. Then we have

        S=                       ij (h) =                ij (h) = Lij,

                    n hHn i,j                i,j  n hHn    i,j

which proves the claim.

We can write

                    S = Lij + Lii = Lij + rL.

                            i=j           i       i=j

For i = j, we have

                    E[Lij] = P[Lij  n] = 2-(n-1) = 2,

                            n1                    i1

and so

        E[S] = E[Lij] + r E[L] = 2r(r - 1) + O(r len(r)) = O(r2).

                     i=j

That proves the theorem. 2

   That completes the discussion of Algorithm EDF in the case p = 2. Now assume
that p > 2, so that p, and hence also q, is odd. Algorithm EDF in this case is exactly
the same as above, except that in this case, we define the polynomial Mk as

                         Mk := X (qk-1)/2 - 1  F [X ].                  (20.5)

Just as before, for   E with () = (1, . . . , r), we have

              (Mk()) = Mk(()) = (Mk(1), . . . , Mk(r)).
536                               Algorithms for finite fields

Note that each group Ei is a cyclic group of order qk - 1, and therefore, the image
of the (qk - 1)/2-power map on Ei is {±1}.

   Now, suppose we choose   E at random. Then if () = (1, . . . , r), the fam-
ily of random variables {i}ir=1 is mutually independent, with each i uniformly
distributed over Ei. It follows that the family of random variables {Mk(i)}ri=1
is mutually independent. If i = 0, which happens with probability 1/qk, then
Mk(i) = -1; otherwise, i(q is uniformly distributed over {±1}, and so k-1)/2

Mk(i) is uniformly distributed over {0, -2}. That is to say,
                                kk
                                0 with probability (q - 1)/2q ,

                  Mk(i) = -1 with probability 1/qk,
                                -2 with probability (qk - 1)/2qk.

Thus, if g := rep(Mk()), then gcd(g, f ) will be the product of those factors fi of
f such that Mk(i) = 0. We will fail to get a non-trivial factorization only if the

Mk(i) are either all zero or all non-zero. Assume r  2. Consider the worst case,
namely, when r = 2. In this case, a simple calculation shows that the probability

that we fail to split these two factors is

               qk - 1 2 + qk + 1 2 = 1 (1 + 1/q2k).
                           2qk        2qk       2

The (very) worst case is when qk = 3, in which case the probability of failure is at
most 5/9.

   The same quick-and-dirty analysis given just above Theorem 20.7 applies here
as well, but just as before, we can do better:

Theorem 20.8. In the case p > 2, Algorithm EDF uses an expected number of
O(k 2 len(q)) operations in F .

Proof. The analysis is essentially the same as in the case p = 2, except that now
the probability that we fail to split a given pair of irreducible factors is at most 5/9,
rather than equal to 1/2. The details are left as an exercise for the reader. 2

               20.4.3 Analysis of the whole algorithm

Given an arbitrary monic square-free polynomial f  F [X ] of degree > 0, the
distinct degree factorization step takes O( 3 len(q)) operations in F . This step

produces a number of polynomials that must be further subjected to equal degree

factorization. If there are t such polynomials, where the ith polynomial has degree

i, for i = 1, . . . , t, then it=1 i = . Now, the equal degree factorization step
                                                                    3
for  the  ith  polynomial  takes  an  expected  number  of      O(  i  len(q  )  )  operations  in  F

(actually, our initial, "quick and dirty" estimate is good enough here), and so it
          20.4 Factoring polynomials: the Cantor-Zassenhaus algorithm  537

follows that the total expected cost of all the equal degree factorization steps is

O(  i  3  len(q  )  ),  which  is  O(  3 len(q)),  operations  in  F.  Putting this all together,
       i

we conclude:

Theorem 20.9. The Cantor-Zassenhaus factoring algorithm uses an expected
number of O( 3 len(q)) operations in F .

   This bound is tight, since in the worst case, when the input is irreducible, the
algorithm really does do this much work. Also, we have assumed the input to
the Cantor-Zassenhaus is a square-free polynomial. However, we may use Algo-
rithm SFD as a preprocessing step to ensure that this is the case. Even if we include
the cost of this preprocessing step, the running time estimate in Theorem 20.9
remains valid.

EXERCISE 20.6. Show how to modify Algorithm DDF so that the main loop halts
as soon as 2k > deg(f ).

EXERCISE 20.7. Suppose that in Algorithm EDF, we replace the two lines

               for each h  H do
                     choose   F [X ]/(h) at random

by the following:

               choose a0, . . . , a2k-1  F at random

                                 2k-1 j

              g  j=0 ajX  F [X ]
               for each h  H do

                       [g]h  F [X ]/(h)

Show that the expected running time bound of Theorem 20.6 still holds (you may
assume p = 2 for simplicity).

EXERCISE 20.8. This exercise extends the techniques developed in Exercise 20.1.
Let f  F [X ] be a monic polynomial of degree > 0, and let  := [X ]f  E,
where E := F [X ]/(f ). For each integer m > 0, define polynomials

   Tm := X + X q + · · · + X qm-1  F [X ] and Nm := X · X q · · · · · X qm-1  F [X ].

    (a) Given as input qm  E and qm  E, where m and m are positive integers,
         along with Tm() and Tm (), for some   E, show how to compute the
         values qm+m and Tm+m (), using O( 2.5) operations in F , and space for
         O( 1.5) elements of F .

   (b) Given as input q  E,   E, and a positive integer m, show how to
538  Algorithms for finite fields

     compute (using part (a)) the value Tm(), using O( 2.5 len(m)) operations
     in F , and space for O( 1.5) elements of F .

(c) Repeat parts (a) and (b), except with "N" in place of "T ."

EXERCISE 20.9. Using the result of the previous exercise, show how to implement
Algorithm EDF so that it uses an expected number of

                                  O(len(k) 2.5 + 2 len(q))

operations in F , and space for O( 1.5) elements of F .

EXERCISE 20.10. This exercise depends on the concepts and results in §18.6. Let

E be an extension field of degree over F , specified by an irreducible polynomial
of degree over F . Design and analyze an efficient probabilistic algorithm that

finds a normal basis for E over F (see Exercise 19.14). Hint: there are a number

of approaches to solving this problem; one way is to start by factoring X - 1
over F , and then turn the construction in Theorem 18.12 into an efficient proba-

bilistic procedure; if you mimic Exercise 11.2, your entire algorithm should use
O( 3 len( ) len(q)) operations in F (or O(len(r) 3 len(q)) operations, where r is

the number of distinct irreducible factors of X - 1 over F ).

               20.5 Factoring polynomials: Berlekamp's algorithm

We now develop an alternative algorithm, due to Berlekamp, for factoring a poly-
nomial over the finite field F into irreducibles. We shall assume that the input
polynomial is square-free, using Algorithm SFD in §20.3 as a preprocessing step,
if necessary.

  Let us now assume we have a monic square-free polynomial f  F [X ] of degree
  > 0 that we want to factor into irreducibles. We first present the mathematical
ideas underpinning the algorithm.
  Let E be the F -algebra F [X ]/(f ). Let  be the Frobenius map on E over F ,
which maps   E to q  E. We know that  is an F -algebra homomorphism (see
Theorem 19.7). Consider the subalgebra B of E fixed by  (see Theorem 16.6).
Thus,

                                   B = {  E : q = }.

   The subalgebra B is called the Berlekamp subalgebra of E. Let us take a closer
look at it. Suppose that f factors into irreducibles as

                                         f = f1 · · · fr,
                 20.5 Factoring polynomials: Berlekamp's algorithm                         539

and let

                     : E  E1 × · · · × Er

                    [g]f  ([g]f1, . . . , [g]fr )

be the F -algebra isomorphism from the Chinese remainder theorem, where Ei :=

F [X ]/(fi) is an extension field of F of finite degree for i = 1, . . . , r. Now, for
                                                                                  q
    E,   if  ()  =  (1, . . . , r),  then  we  have  q  =    if  and  only  if       =  i  for
                                                                                i
i = 1, . . . , r; moreover, by Theorem 19.8, we know that for all i  Ei, we have
iq = i if and only if i  F . Thus, we may characterize B as follows:

                    B = {-1(c1, . . . , cr) : c1, . . . , cr  F }.

   Since B is a subalgebra of E, then as F -vector spaces, B is a subspace of E.
Of course, E has dimension over F , with the natural basis { i i-1} =1, where
 := [X ]f . As for the Berlekamp subalgebra, from the above characterization of B,
it is evident that the elements

               -1(1, 0, . . . , 0), -1(0, 1, 0, . . . , 0), . . . , -1(0, . . . , 0, 1)

form a basis for B over F , and hence, B has dimension r over F .
   Now we come to the actual factoring algorithm.

                             Stage 1: Construct a basis for B

The first stage of Berlekamp's factoring algorithm constructs a basis for B over F .

We can easily do this using Gaussian elimination, as follows. Let  : E  E be
the map that sends   E to () -  = q - . Since  is an F -linear map, the

map  is also F -linear. Moreover, the kernel of  is none other than the Berlekamp

subalgebra B. So to find a basis for B, we simply need to find a basis for the kernel

of  using Gaussian elimination over F , as in §14.4.

   To perform the Gaussian elimination, we need to choose a basis S for E over
F , and construct the matrix Q := MatS,S ()  F × , that is, the matrix of  with
respect to this basis, as in §14.2, so that evaluation of  corresponds to multiplying

a row vector on the right by Q. We are free to choose a basis in any convenient
way, and the most convenient basis, of course, is S := { i i-1} =1, since for compu-
tational purposes, we already represent an element   E by its coordinate vector
VecS (). The matrix Q, then, is the × matrix whose ith row, for i = 1, . . . , , is
VecS ((i-1)). Note that if  = q, then (i-1) = (i-1)q - i-1 = (q)i-1 - i-1 =
i-1-i-1. This observation allows us to construct the rows of Q by first computing
q via repeated squaring, and then just computing successive powers of q.

   After we construct the matrix Q, we apply Gaussian elimination to get row vec-

tors v1, . . . , vr that form a basis for the row null space of Q. It is at this point that
540  Algorithms for finite fields

our algorithm actually discovers the number r of irreducible factors of f . Our basis
for B is {i}ri=1, where VecS (i) = vi for i = 1, . . . , r.

   Putting this all together, we have the following algorithm to compute a basis for
the Berlekamp subalgebra.

Algorithm B1. On input f, where f  F [X ] is a monic square-free polynomial
of degree > 0, do the following, where E := F [X ]/(f ),  := [X ]f  E, and
S := { i i-1} =1:

         let Q be an × matrix over F (initially with undefined entries)
         compute   q using repeated squaring
           1E
         for i  1 to do // invariant:  = i-1 = (i-1)q

               Rowi(Q)  VecS (), Q(i, i)  Q(i, i) - 1,   
         compute a basis {vi}ir=1 of the row null space of Q using

               Gaussian elimination
         for i = 1, . . . , r do i  VecS-1(vi)
         output {i}ir=1

   The correctness of Algorithm B1 is clear from the above discussion. As for the
running time:

Theorem 20.10. Algorithm B1 uses O( 3 + 2 len(q)) operations in F .

Proof. This is just a matter of counting. The computation of  takes O(len(q))
operations in E using repeated squaring, and hence O( 2 len(q)) operations in F .

To build the matrix Q, we have to perform an additional O( ) operations in E to
compute the successive powers of , which translates into O( 3) operations in F .
Finally, the cost of Gaussian elimination is an additional O( 3) operations in F . 2

     Stage 2: Splitting with a basis for B

The second stage of Berlekamp's factoring algorithm is a probabilistic procedure
that factors f using a basis {i}ir=1 for B. As we did with Algorithm EDF in
§20.4.2, we begin by discussing how to efficiently split f into two non-trivial fac-
tors, and then we present a somewhat more elaborate algorithm that completely
factors f.

  Let M1  F [X ] be the polynomial defined by (20.4) and (20.5); that is,

     M1 :=    jw=0-1 X 2j  if p = 2,
            X (q-1)/2 - 1  if p > 2.

Using our basis for B, we can easily generate a random element  of B by simply
                20.5 Factoring polynomials: Berlekamp's algorithm                                541

choosing c1, . . . , cr at random, and computing  := i cii. If () = (b1, . . . , br),
then the family of random variables {bi}ir=1 is mutually independent, with each bi
uniformly distributed over F . Just as in Algorithm EDF, gcd(rep(M1()), f ) will
be a non-trivial factor of f with probability at least 1/2, if p = 2, and probability

at least 4/9, if p > 2.

   That is the basic splitting strategy. We turn this into an algorithm to completely

factor f using the same technique of iterative refinement that was used in Algo-

rithm EDF. That is, at any stage of the algorithm, we have a partial factorization
f = hH h, which we try to refine by attempting to split each h  H using
the strategy outlined above. One technical difficulty is that to split such a poly-
nomial h, we need to efficiently generate a random element of the Berlekamp
subalgebra of F [X ]/(h). A particularly efficient way to do this is to use our

basis for the Berlekamp subalgebra of F [X ]/(f ) to generate a random element

of the Berlekamp subalgebra of F [X ]/(h) for all h  H simultaneously. Let
gi := rep(i) for i = 1, . . . , r. If we choose c1, . . . , cr  F at random, and set
g := c1g1 + · · · + crgr, then [g]f is a random element of the Berlekamp subalgebra
of F [X ]/(f ), and by the Chinese remainder theorem, it follows that the family

of random variables {[g]h}hH is mutually independent, with each [g]h uniformly

distributed over the Berlekamp subalgebra of F [X ]/(h).

   Here is the algorithm for completely factoring a polynomial, given a basis for
the corresponding Berlekamp subalgebra.

Algorithm  B2.  On  input  f  ,  {  i  }  r    ,  where  f    F [X ]  is  a  monic  square-free  poly-
                                          i=1
nomial of degree > 0, and {i}ir=1 is a basis for the Berlekamp subalgebra of
F [X ]/(f ), do the following, where gi := rep(i) for i = 1, . . . , r:

H  {f}

while |H| < r do
      choose c1, . . . , cr  F at random
      g  c1g1 + · · · + crgr  F [X ]
      H 

      for each h  H do
              [g]h  F [X ]/(h)
             d  gcd(rep(M1()), h)
             if d = 1 or d = h
                    then H  H  {h}
                    else H  H  {d, h/d}

      H H

output H
542                Algorithms for finite fields

   The correctness of the algorithm is clear. As for its expected running time, we
can get a quick-and-dirty upper bound as follows:

     · The cost of generating g in each loop iteration is O(r ) operations in F .
        For a given h, the cost of computing  := [g]h  F [X ]/(h) is O( deg(h))
         operations in F , and the cost of computing M1() is O(deg(h)2 len(q))
         operations in F . Therefore, the number of operations in F performed in
         each iteration of the main loop is at most a constant times

     r+            deg(h) + len(q) deg(h)2

         hH        hH

                    2 2 + len(q)                              2

                                                 deg(h) = O( 2 len(q)).

                                            hH

     · The expected number of iterations of the main loop until we get some non-
         trivial split is O(1).

     · The algorithm finishes after getting r - 1 non-trivial splits.
     · Therefore, the total expected cost is O(r 2 len(q)) operations in F .
A more careful analysis reveals:

Theorem 20.11. Algorithm B2 uses an expected number of
                                      O(len(r) 2 len(q))

operations in F .

Proof. The proof follows the same line of reasoning as the analysis of Algo-
rithm EDF. Indeed, using the same argument as was used there, the expected
number of iterations of the main loop is O(len(r)). As discussed in the paragraph
above this theorem, the cost per loop iteration is O( 2 len(q)) operations in F . The
theorem follows. 2

   The bound in the above theorem is tight (see Exercise 20.11 below): unlike
Algorithm EDF, we cannot make the multiplicative factor of len(r) go away.

   Putting together Algorithms B1 and B2, we get Berlekamp's complete factoring
algorithm. The running time bound is easily estimated from the results already
proved:

Theorem 20.12. Berlekamp's factoring algorithm uses an expected number of
O( 3 + 2 len( ) len(q)) operations in F .

   We have assumed the input to Berlekamp's algorithm is a square-free polyno-
mial. However, we may use Algorithm SFD as a preprocessing step to ensure that
20.5 Factoring polynomials: Berlekamp's algorithm  543

this is the case. Even if we include the cost of this preprocessing step, the running

time estimate in Theorem 20.12 remains valid.

   So we see that Berlekamp's algorithm is faster than the Cantor-Zassenhaus algo-
rithm, whose expected operation count is O( 3 len(q)). The speed advantage of

Berlekamp's algorithm grows as q gets large. The one disadvantage of Berlekamp's
algorithm is space: it requires space for ( 2) elements of F , while the Cantor-

Zassenhaus algorithm requires space for only O( ) elements of F . One can in fact
implement the Cantor-Zassenhaus algorithm so that it uses O( 3 + 2 len(q)) oper-
ations in F , while using space for only O( 1.5) elements of F -- see Exercise 20.13

below.

EXERCISE 20.11. Give an example of a family of input polynomials that cause
Algorithm B2 to use an expected number of at least ( 2 len( ) len(q)) operations
in F . Assume that computing M1() for   F [X ]/(h) takes (deg(h)2 len(q))
operations in F .

EXERCISE 20.12. Using the ideas behind Berlekamp's factoring algorithm, devise
a deterministic irreducibility test that, given a monic polynomial of degree over
F , uses O( 3 + 2 len(q)) operations in F .

EXERCISE 20.13. This exercise develops a variant of the Cantor-Zassenhaus
algorithm that uses O( 3 + 2 len(q)) operations in F , while using space for only
O( 1.5) elements of F . By making use the variant of Algorithm EDF discussed
in Exercise 20.9, our problem is reduced to that of implementing Algorithm DDF
within the stated time and space bounds, assuming that the input polynomial is
square-free.

    (a) Show that for all non-negative integers i, j, with i = j, the irreducible poly-
        nomials in F [X ] that divide X qi - X qj are precisely those whose degree
         divides i - j.

   (b) Let f  F [X ] be a monic polynomial of degree > 0, and let m = O( 1/2).
        Let  := [X ]f  E, where E := F [X ]/(f ). Show how to compute

                      q, q2 , . . . , qm-1  E and qm , q2m , . . . , q(m-1)m  E

         using O( 3 + 2 len(q)) operations in F , and space for O( 1.5) elements of
         F.

    (c) Combine the results of parts (a) and (b) to implement Algorithm DDF on
         square-free inputs of degree , so that it uses O( 3 + 2 len(q)) operations
         in F , and space for O( 1.5) elements of F .
544  Algorithms for finite fields

     20.6 Deterministic factorization algorithms ()

The algorithms of Cantor and Zassenhaus and of Berlekamp are probabilistic. The
exercises below develop a deterministic variant of the Cantor-Zassenhaus algo-
rithm. (One can also develop deterministic variants of Berlekamp's algorithm,
with similar complexity.)

   This algorithm is only practical for finite fields of small characteristic, and is
anyway mainly of theoretical interest, since from a practical perspective, there is
nothing wrong with the above probabilistic method. In all of these exercises, we
assume that we have access to a basis {i}iw=1 for F as a vector space over Zp.

   To make the Cantor-Zassenhaus algorithm deterministic, we only need to
develop a deterministic variant of Algorithm EDF, as Algorithm DDF is already
deterministic.

EXERCISE 20.14. Let f = f1 · · · fr, where the fi's are distinct monic irreducible
polynomials in F [X ]. Assume that r > 1, and let := deg(f ). For this exercise,

the degrees of the fi's need not be the same. For an intermediate field F , with
Zp  F  F , let us call a set S = {1, . . . , s}, where each u  F [X ] with
deg(u) < , a separating set for f over F if the following conditions hold:

     · for i = 1, . . . , r and u = 1, . . . , s, there exists cui  F such that u 
         cui (mod fi), and

     · for every pair of distinct indices i, j, with 1  i < j  r, there exists
         u = 1, . . . , s such that cui = cuj.

Show that if S is a separating set for f over Zp, then the following algorithm
completely factors f using O(p|S| 2) operations in F .

     H  {f}                                  {h}
     for each   S do                         {d, h/d}

           for each a  Zp do
                  H 
                  for each h  H do
                         d  gcd( - a, h)
                         if d = 1 or d = h
                                then H  H
                                else H  H
                  H H

     output H

EXERCISE 20.15. Let f be as in the previous exercise. Show that if S is a
20.6 Deterministic factorization algorithms ()              545

separating set for f over F , then the set

S :=  w-1

          (j)pi mod f : 1  j  w,   S

      i=0

is a separating set for f over Zp. Show how to compute this set using
O(|S| 2 len(p)w(w - 1)) operations in F .

EXERCISE 20.16. Let f be as in the previous two exercises, but further suppose
that each irreducible factor of f is of the same degree, say k. Let E := F [X ]/(f )
and  := [X ]f  E. Define the polynomial   E[Y ] as follows:

                                                             k-1

                                    := (Y - qi ).

                                                              i=0

If

                             = Y k + k-1Y k-1 + · · · + 0,

with 0, . . . , k-1  E, show that the set
                               S := {rep(i) : 0  i  k - 1}

is a separating set for f over F , and can be computed deterministically using
O(k2 + k len(q)) operations in E, and hence O(k2 2 + k 2 len(q)) operations in F .

EXERCISE 20.17. Put together all of the above pieces, together with Algo-
rithms SFD and DDF, so as to obtain a deterministic algorithm for factoring poly-
nomials over F that runs in time at most p times a polynomial in the size of the
input, and make a careful estimate of the running time of your algorithm.

EXERCISE 20.18. It is a fact that when our prime p is odd, then for all integers
a, b, with a  b (mod p), there exists a non-negative integer i  p1/2 log2 p such
that (a + i | p) = (b + i | p) (here, "(· | ·)" is the Legendre symbol). Using this

fact, design and analyze a deterministic algorithm for factoring polynomials over
F that runs in time at most p1/2 times a polynomial in the size of the input.

   The following two exercises show that the problem of factoring polynomials
over F reduces in deterministic polynomial time to the problem of finding roots of
polynomials over Zp.

EXERCISE 20.19. Let f be as in Exercise 20.14. Suppose that S = {1, . . . , s}
is a separating set for f over Zp, and u  F [X ] is the minimal polynomial over F
of [u]f  F [X ]/(f ) for u = 1, . . . , s. Show that each u is the product of linear
factors over Zp, and that given S, along with the roots of all the u's, we can deter-
ministically factor f using (|S| + )O(1) operations in F . Hint: see Exercise 16.9.
546  Algorithms for finite fields

EXERCISE 20.20. Using the previous exercise, show that the problem of factoring
a polynomial over F reduces in deterministic polynomial time to the problem of
finding roots of polynomials over Zp.

                                          20.7 Notes

The average-case analysis of Algorithm IPT, assuming its input is random, and
the application to the analysis of Algorithm RIP, is essentially due to Ben-Or [14].
If one implements Algorithm RIP using fast polynomial arithmetic, one gets an
expected cost of O( 2+o(1) len(q)) operations in F . Note that Ben-Or's analysis
is a bit incomplete -- see Exercise 32 in Chapter 7 of Bach and Shallit [11] for a
complete analysis of Ben-Or's claims.

   The asymptotically fastest probabilistic algorithm for constructing an irreducible
polynomial over F of given degree is due to Shoup [96]. That algorithm uses an
expected number of O( 2+o(1) + 1+o(1) len(q)) operations in F , and in fact does not
follow the "generate and test" paradigm of Algorithm RIP, but uses a completely
different approach.

   As far as deterministic algorithms for constructing irreducible polynomials of
given degree over F , the only known methods are efficient when the characteris-
tic p of F is small (see Chistov [26], Semaev [88], and Shoup [94]), or under a
generalization of the Riemann hypothesis (see Adleman and Lenstra [4]). Shoup
[94] in fact shows that the problem of constructing an irreducible polynomial of
given degree over F is deterministic, polynomial-time reducible to the problem of
factoring polynomials over F .

   The algorithm in §20.2 for computing minimal polynomials over finite fields is
due to Gordon [43].

   The square-free decomposition of a polynomial over a field of characteristic
zero can be computed using an algorithm of Yun [111] using O( 1+o(1)) field
operations. Yun's algorithm can be adapted to work over finite fields as well (see
Exercise 14.30 in von zur Gathen and Gerhard [39]).

   The Cantor-Zassenhaus algorithm was initially developed by Cantor and
Zassenhaus [24], although many of the basic ideas can be traced back quite a
ways. A straightforward implementation of this algorithm using fast polynomial
arithmetic uses an expected number of O( 2+o(1) len(q)) operations in F .

   Berlekamp's algorithm was initially developed by Berlekamp [15, 16], but again,
the basic ideas go back a long way. A straightforward implementation using fast
polynomial arithmetic uses an expected number of O( 3 + 1+o(1) len(q)) opera-
tions in F ; the term 3 may be replaced by , where  is the exponent of matrix
multiplication (see §14.6).

   There are no known efficient, deterministic algorithms for factoring polynomials
20.7 Notes  547

over F when the characteristic p of F is large (even under a generalization of the

Riemann hypothesis, except in certain special cases).

   The asymptotically fastest algorithms for factoring polynomials over F are due

to von zur Gathen, Kaltofen, and Shoup: the algorithm of von zur Gathen and
Shoup [40] uses an expected number of O( 2+o(1) + 1+o(1) len(q)) operations in

F ; the algorithm of Kaltofen and Shoup [53] has a cost that is subquadratic in the
degree -- it uses an expected number of O( 1.815 len(q)0.407) operations in F when
len(q) = O( 1.375). Exercises 20.1, 20.8, and 20.9 are based on [40]. Although

the "fast" algorithms in [40] and [53] are mainly of theoretical interest, a variant
in [53], which uses O( 2.5 + 1+o(1) len(q)) operations in F , and space for O( 1.5)

elements of F , has proven to be quite practical (Exercise 20.13 develops some of

these ideas; see also Shoup [97]).

 The running times of these algorithms can be improved using faster algorithms for modular composition --
  see footnote on p. 485.
                                 21

                  Deterministic primality testing

For many years, despite much research in the area, there was no known determinis-
tic, polynomial-time algorithm for testing whether a given integer n > 1 is a prime.
However, that is no longer the case -- the breakthrough algorithm of Agrawal,
Kayal, and Saxena, or Algorithm AKS for short, is just such an algorithm. Not only
is the result itself remarkable, but the algorithm is striking both in its simplicity,
and in the fact that the proof of its running time and correctness are completely
elementary (though ingenious).

   We should stress at the outset that although this result is an important theoretical
result, as of yet, it has no real practical significance: probabilistic tests, such as the
Miller-Rabin test discussed in Chapter 10, are much more efficient, and a practi-
cally minded person should not at all be bothered by the fact that such algorithms
may in theory make a mistake with an incredibly small probability.

                                     21.1 The basic idea
The algorithm is based on the following fact:

Theorem 21.1. Let n > 1 be an integer. If n is prime, then for all a  Zn, we have
the following identity in the ring Zn[X ]:

                  (X + a)n = X n + a.                                       (21.1)

Conversely, if n is composite, then for all a  Zn, the identity (21.1) does not hold.

Proof. Note that

                                                       n-1  n aiX n-i.
                                                            i
                  (X + a)n = X n + an +

                                                       i=1

If n is prime, then by Fermat's little theorem (Theorem 2.14), we have an = a,

and by Exercise 1.14, all of the binomial coefficients      n  ,  for i  =  1, . . . , n - 1,  are
                                                            i

                  548
                     21.2 The algorithm and its analysis              549

divisible by n, and hence their images in the ring Zn vanish. That proves that the

identity (21.1) holds when n is prime.

   Conversely, suppose that n is composite and that a  Zn. Consider any prime
factor p of n, and suppose n = pkm, where p m.

We claim that pk  n  .  To prove the claim, one simply observes that
                  p

                        n = n(n - 1) · · · (n - p + 1) ,
                        p               p!

and the numerator of this fraction is an integer divisible by pk, but no higher power

of p, and the denominator is divisible by p, but no higher power of p. That proves

the claim.
  From the claim, and the fact that a  Zn, it follows that the coefficient of X n-p

in (X + a)n is not zero, and hence the identity (21.1) does not hold. 2

   Of course, Theorem 21.1 does not immediately give rise to an efficient primality
test, since just evaluating the left-hand side of the identity (21.1) takes time (n) in

the worst case. The key observation of Agrawal, Kayal, and Saxena is that if (21.1)
holds modulo X r - 1 for a suitably chosen value of r, and for sufficiently many a,

then n must be prime. To make this idea work, one must show that a suitable r
exists that is bounded by a polynomial in len(n), and that the number of different

values of a that must be tested is also bounded by a polynomial in len(n).

                          21.2 The algorithm and its analysis

The algorithm is shown in Fig. 21.1. A few remarks on implementation are in
order:

     · In step 1, we can use the algorithm for perfect-power testing discussed in
         Exercise 3.31.

     · The search for r in step 2 can just be done by brute-force search; likewise,
         the determination of the multiplicative order of [n]r  Zr can be done by
         brute force: after verifying that gcd(n, r) = 1, compute successive powers
         of n modulo r until we get 1.

   We want to prove that Algorithm AKS runs in polynomial time and is correct.
To prove that it runs in polynomial time, it clearly suffices to prove that there exists
an integer r satisfying the condition in step 2 that is bounded by a polynomial in
len(n), since all other computations can be carried out in time (r + len(n))O(1).
Correctness means that it outputs true if and only if n is prime.
550  Deterministic primality testing

      On input n, where n is an integer and n > 1, do the following:

     1. if n is of the form ab for integers a > 1 and b > 1 then
                return false

     2. find the smallest integer r > 1 such that either
                gcd(n, r) > 1

          or
                gcd(n, r) = 1 and
                [n]r  Zr has multiplicative order > 4 len(n)2

     3. if r = n then return true
     4. if gcd(n, r) > 1 then return false
     5. for j  1 to 2 len(n) r1/2 + 1 do

                if (X + j)n  X n + j (mod X r - 1) in the ring Zn[X ] then
                       return false

     6. return true

     Fig. 21.1. Algorithm AKS

                               21.2.1 Running time analysis

The question of the running time of Algorithm AKS is settled by the following
fact:

Theorem 21.2. For integers n > 1 and m  1, the least prime r such that r n
and the multiplicative order of [n]r  Zr is greater than m is O(m2 len(n)).

Proof. Call a prime r "good" if r n and the multiplicative order of [n]r  Zr is
greater than m, and otherwise call r "bad." If r is bad, then either r | n or r | (nd -1)

for some d = 1, . . . , m. Thus, any bad prime r satisfies

                m

     r | n (nd - 1).

               d=1

If all primes r up to some given bound x  2 are bad, then the product of all primes
up to x divides n dm=1(nd - 1), and so in particular,

                    m

         r  n (nd - 1),

     rx  d=1
    21.2 The algorithm and its analysis                      551

where the first product is over all primes r up to x. Taking logarithms, we obtain

                             m              m

    log r  log n (nd - 1)  (log n)  1+ d

rx                           d=1          d=1

    = (log n)(1 + m(m + 1)/2).

But by Theorem 5.7, we have

                                             log r  cx

                                                        rx

for some constant c > 0, from which it follows that
                             x  c-1(log n)(1 + m(m + 1)/2),

and the theorem follows. 2

   From this theorem, it follows that the value of r found in step 2 -- which need
not be prime -- will be O(len(n)5). From this, we obtain:

Theorem 21.3. Algorithm AKS can be implemented so that its running time is
O(len(n)16.5).

Proof. As discussed above, the value of r determined in step 2 will be O(len(n)5).
It is fairly straightforward to see that the running time of the algorithm is dominated
by the running time of step 5. Here, we have to perform O(r1/2 len(n)) exponentia-
tions to the power n in the ring Zn[X ]/(X r -1). Each of these exponentiations takes
O(len(n)) operations in Zn[X ]/(X r - 1), each of which takes O(r2) operations in
Zn, each of which takes time O(len(n)2). This yields a running time bounded by a
constant times

                   r1/2 len(n) × len(n) × r2 × len(n)2 = r2.5 len(n)4.

Substituting the bound O(len(n)5) for r, we obtain the desired bound. 2

                                     21.2.2 Correctness
As for the correctness of Algorithm AKS, we first show:

Theorem 21.4. If the input to Algorithm AKS is prime, then the output is true.

Proof. Assume that the input n is prime. The test in step 1 will certainly fail. If the
algorithm does not return true in step 3, then certainly the test in step 4 will fail as
well. If the algorithm reaches step 5, then all of the tests in the loop in step 5 will
fail --this follows from Theorem 21.1. 2

   The interesting case is the following:
552                   Deterministic primality testing

Theorem 21.5. If the input to Algorithm AKS is composite, then the output is
false.

   The proof of this theorem is rather long, and is the subject of the remainder of
this section.

   Suppose the input n is composite. If n is a prime power, then this will be detected
in step 1, so we may assume that n is not a prime power. Assume that the algorithm
has found a suitable value of r in step 2. Clearly, the test in 3 will fail. If the test
in step 4 passes, we are done, so we may assume that this test fails; that is, we may
assume that all prime factors of n are greater than r. Our goal now is to show that
one of the tests in the loop in step 5 must pass. The proof will be by contradiction:
we shall assume that none of the tests pass, and derive a contradiction.

  The assumption that none of the tests in step 5 fail means that in the ring Zn[X ],
the following congruences hold:

     (X + j)n  X n + j (mod X r - 1) (j = 1, . . . , 2 len(n) r1/2 + 1). (21.2)

   For the rest of the proof, we fix a particular prime divisor p of n -- the choice

of p does not matter. Since p | n, we have a natural ring homomorphism from

Zn[X ] to Zp[X ] (see Examples 7.52 and 7.46), which implies that the congruences
(21.2) hold in the ring of polynomials over Zp as well. From now on, we shall work
exclusively with polynomials over Zp.

   Let us state in somewhat more abstract terms the precise assumptions we are

making in order to derive our contradiction:

(A0) n > 1, r > 1, and         1 are integers, p is a prime dividing n, and
       gcd(n, r) = 1;

(A1) n is not a prime power;

(A2) p > r;

(A3) the congruences

             (X + j)n  X n + j (mod X r - 1) (j = 1, . . . , )

        hold in the ring Zp[X ];
 (A4) the multiplicative order of [n]r  Zr is greater than 4 len(n)2;
 (A5) > 2 len(n) r1/2 .

   The rest of the proof will rely only on these assumptions, and not on any other
details of Algorithm AKS. From now on, only assumption (A0) will be implicitly
in force. The other assumptions will be explicitly invoked as necessary. Our goal
is to show that assumptions (A1), (A2), (A3), (A4), and (A5) cannot all be true
simultaneously.
21.2 The algorithm and its analysis              553

  Define the Zp-algebra E := Zp[X ]/(X r - 1), and let  := [X ]Xr-1  E, so that
E = Zp[]. Every element of E can be expressed uniquely as g() = [g]Xr-1, for
g  Zp[X ] of degree less than r, and for an arbitrary polynomial g  Zp[X ], we
have g() = 0 if and only if (X r - 1) | g. Note that   E and has multiplicative
order r: indeed, r = 1, and s - 1 cannot be zero for s < r, since X s - 1 has degree

less than r.

   Assumption (A3) implies that we have a number of interesting identities in the

Zp-algebra E:
                             ( + j)n = n + j (j = 1, . . . , ).

For the polynomials gj := X + j  Zp[X ], with j in the given range, these identities
say that gj()n = gj(n).

   In order to exploit these identities, we study more generally functions k, for
various integer values k, that send g()  E to g(k), for arbitrary g  Zp[X ], and

we investigate the implications of the assumption that such functions behave like

the k-power map on certain inputs. To this end, let Z(r) denote the set of all positive
integers k such that gcd(r, k) = 1. Note that the set Z(r) is multiplicative, by which
we mean 1  Z(r), and kk  Z(r) for all k, k  Z(r). Also note that because of our
assumption (A0), both n and p are in Z(r). For k  Z(r), let ^k : Zp[X ]  E be
the polynomial evaluation map that sends g  Zp[X ] to g(k). This is of course a
Zp-algebra homomorphism, and we have:

Lemma 21.6. For all k  Z(r), the kernel of ^k is (X r - 1), and the image of ^k
is E.

Proof. Let J := Ker ^k, which is an ideal of Zp[X ]. Let k be a positive integer
such that kk  1 (mod r), which exists because gcd(r, k) = 1.

  To show that J = (X r - 1), we first observe that

^k(X r - 1) = (k)r - 1 = (r)k - 1 = 1k - 1 = 0,

and hence (X r - 1)  J .
  Next, we show that J  (X r - 1). Let g  J . We want to show that (X r - 1) | g.

Now, g  J means that g(k) = 0. If we set h := g(X k), this implies that h() = 0,
which means that (X r - 1) | h. So let us write h = (X r - 1)f , for some f  Zp[X ].
Then

                     g() = g(kk ) = h(k ) = (k r - 1)f (k ) = 0,

which implies that (X r - 1) | g.
  That finishes the proof that J = (X r - 1).

   Finally, to show that ^k is surjective, suppose we are given an arbitrary element
554  Deterministic primality testing

of E, which we can express as g() for some g  Zp[X ]. Now set h := g(X k ), and
observe that

                            ^k(h) = h(k) = g(kk ) = g(). 2

   Because of Lemma 21.6, then by Theorem 7.26, the map k : E  E that sends
g()  E to g(k), for g  Zp[X ], is well defined, and is a ring automorphism --
indeed, a Zp-algebra automorphism -- on E. Note that for all k, k  Z(r), we have

     · k = k if and only if k = k if and only if k  k (mod r), and
     · k  k = k  k = kk .

So in fact, the set {k : k  Z(r)} under composition forms an abelian group that
is isomorphic to Zr .

         Remark. It is perhaps helpful (but not necessary for the proof) to examine
         the behavior of the map k in a bit more detail. Let   E, and let

                                                                           r-1

                                                = aii

                                                                           i=0

         be the canonical representation of . Since gcd(r, k) = 1, the map
          : {0, . . . , r-1}  {0, . . . , r-1} that sends i to ki mod r is a permutation
         whose inverse is the permutation  that sends i to k i mod r, where k is
         a multiplicative inverse of k modulo r. Then we have

     r-1  r-1  r-1

     k () = aiki = ai(i) = a (i)i.

     i=0  i=0  i=0

     Thus, the action of k is to permute the coordinate vector (a0, . . . , ar-1)
     of , sending  to the element in E whose coordinate vector is
     (a (0), . . . , a (r-1)). So we see that although we defined the maps k in
     a rather "highbrow" algebraic fashion, their behavior in concrete terms is
     actually quite simple.

   Recall that the p-power map on E is a Zp-algebra homomorphism (see Theo-
rem 19.7), and so for all   E, if  = g() for g  Zp[X ], then (by Theorem 16.7)
we have

                                p = g()p = g(p) = p().

Thus, p acts just like the p-power map on all elements of E.
   We can restate assumption (A3) as follows:

                           n( + j) = ( + j)n (j = 1, . . . , ).

That is to say, the map n acts just like the n-power map on the elements  + j for
j = 1, . . . , .

   Now, although the p map must act like the p-power map on all of E, there is
no good reason why the n map should act like the n-power map on any particular
21.2 The algorithm and its analysis                             555

element of E, and so the fact that it does so on all the elements  +j for j = 1, . . . ,
looks decidedly suspicious. To turn our suspicions into a contradiction, let us start
by defining some notation. For   E, let us define

C() := {k  Z(r) : k() = k},

and for k  Z(r), let us define
                              D(k) := {  E : k() = k}.

In words: C() is the set of all k for which k acts like the k-power map on ,
and D(k) is the set of all  for which k acts like the k-power map on . From the
discussion above, we have p  C() for all   E, and it is also clear that 1  C()

for all   E. Also, it is clear that   D(p) for all   E, and 1E  D(k) for all
k  Z(r).

   The following two simple lemmas say that the sets C() and D(k) are multi-

plicative.

Lemma 21.7. For every   E, if k  C() and k  C(), then kk  C().
Proof. If k() = k and k () = k , then

             kk () = k(k ()) = k(k ) = (k())k = (k)k = kk ,

where we have made use of the homomorphic property of k. 2
Lemma 21.8. For every k  Z(r), if   D(k) and   D(k), then   D(k).
Proof. If k() = k and k() = k, then

                         k() = k()k() = kk = ()k,

where again, we have made use of the homomorphic property of k. 2

   Let us define

     · s to be the multiplicative order of [p]r  Zr , and
     · t to be the order of the subgroup of Zr generated by [p]r and [n]r.
   Since r | (ps -1), if we take any extension field F of degree s over Zp (which we
know exists by Theorem 19.12), then since F  is cyclic (Theorem 7.29) and has
order ps - 1, we know that there exists an element   F  of multiplicative order

r (Theorem 6.32). Let us define the polynomial evaluation map ^ : Zp[X ]  F
that sends g  Zp[X ] to g()  F . Since X r - 1 is clearly in the kernel of ^, then
by Theorem 7.27, the map  : E  F that sends g() to g(), for g  Zp[X ], is a
well-defined ring homomorphism, and actually, it is a Zp-algebra homomorphism.

  For concreteness, one could think of F as Zp[X ]/(f ), where f is an irreducible
factor of X r - 1 of degree s. In this case, we could simply take  to be [X ]f (see
556  Deterministic primality testing

Example 19.1), and the map ^ above would be just the natural map from Zp[X ] to
Zp[X ]/(f ).

   The key to deriving our contradiction is to examine the set S := (D(n)), that

is, the image under  of the set D(n) of all elements   E for which n acts like
the n-power map.

Lemma 21.9. Under assumption (A1), we have
                                         |S|  n2 t1/2 .

Proof. Consider the set of integers
                             I := {nupv : u, v = 0, . . . , t1/2 }.

   We first claim that |I| > t. To prove this, we first show that each distinct pair
(u, v) gives rise to a distinct value nupv. To this end, we make use of our assumption

(A1) that n is not a prime power, and so is divisible by some prime q other than p.
Thus, if (u , v ) = (u, v), then either

     · u = u , in which case the power of q in the prime factorization of nupv is
         different from that in nu pv , or

     · u = u and v = v , in which case the power of p in the prime factorization
         of nupv is different from that in nu pv .

The claim now follows from the fact that both u and v range over a set of size
 t1/2 + 1 > t1/2, and so there are strictly more than t such pairs (u, v).

   Next, recall that t was defined to be the order of the subgroup of Zr generated
by [n]r and [p]r; equivalently, t is the number of distinct residue classes of the form
[nupv]r, where u and v range over all non-negative integers. Since each element of
I is of the form nupv, and |I| > t, we may conclude that there must be two distinct

elements of I, call them k and k , that are congruent modulo r. Furthermore, any
element of I is a product of two positive integers each of which is at most n t1/2 ,
and so both k and k lie in the range 1, . . . , n2 t1/2 .

   Now, let   D(n). This is equivalent to saying n  C(). We always have
1  C() and p  C(), and so by Lemma 21.7, we have nupv  C() for all

non-negative integers u, v, and so in particular, k, k  C().

   Since both k and k are in C(), we have

                               k() = k and k () = k .

Since k  k (mod r), we have k = k , and hence

                                            k = k .

Now apply the homomorphism , obtaining
                                        ()k = ()k .
           21.2 The algorithm and its analysis              557

Since this holds for all   D(n), we conclude that all elements of S are roots

of the polynomial X k - X k . Since k = k , we see that X k - X k is a non-zero
polynomial of degree at most max{k, k }  n2 t1/2 , and hence can have at most
n2 t1/2 roots in the field F (Theorem 7.14). 2

Lemma 21.10. Under assumptions (A2) and (A3), we have

           |S|  2min(t, ) - 1.

Proof. Let m := min(t, ). Under assumption (A3), we have  + j  D(n) for
j = 1, . . . , m. Under assumption (A2), we have p > r > t  m, and hence the
integers j = 1, . . . , m are distinct modulo p. Define

      m                                                m

P :=       (X + j)ej  Zp[X ] : ej  {0, 1} for j = 1, . . . , m, and ej < m .

      j=1                                              j=1

That is, we form P by taking products over all subsets S {X + j : j = 1, . . . , m}.
Clearly, |P | = 2m - 1.

   Define P () := {f ()  E : f  P } and P () := {f ()  F : f  P }. Note
that (P ()) = P (), and that by Lemma 21.8, P ()  D(n).

   Therefore, to prove the lemma, it suffices to show that |P ()| = 2m - 1. Suppose

that this is not the case. This would give rise to distinct polynomials g, h  Zp[X ],
both of degree at most t - 1, such that

           g()  D(n), h()  D(n), and (g()) = (h()).

So we have n  C(g()) and (as always) 1, p  C(g()). Likewise, we have
1, n, p  C(h()). By Lemma 21.7, for all integers k of the form nupv, where u and

v range over all non-negative integers, we have

                              k  C(g()) and k  C(h()).

For each such k, since (g()) = (h()), we have (g())k = (h())k, and hence

              0 = (g())k - (h())k
                 = (g()k) - (h()k) ( is a homomorphism)
                 = (g(k)) - (h(k)) (k  C(g()) and k  C(h()))
                 = g(k) - h(k) (definition of ).

Thus, the polynomial f := g - h  Zp[X ] is a non-zero polynomial of degree at
most t - 1, having roots k in the field F for all k of the form nupv. Now, t is by
definition the number of distinct residue classes of the form [nupv]r  Zr . Also,
since  has multiplicative order r, for all integers k, k , we have k = k if and
only if k  k (mod r). Therefore, as k ranges over all integers of the form nupv,
558  Deterministic primality testing

k ranges over precisely t distinct values in F . But since all of these values are
roots of the polynomial f, which is non-zero and of degree at most t - 1, this is
impossible (Theorem 7.14). 2

   We are now (finally!) in a position to complete the proof of Theorem 21.5.
Under assumptions (A1), (A2), and (A3), Lemmas 21.9 and 21.10 imply that

     2min(t, ) - 1  |S|  n2 t1/2 .               (21.3)

The contradiction is provided by the following:

Lemma 21.11. Under assumptions (A4) and (A5), we have

                                    2min(t, ) - 1 > n2 t1/2 .

Proof. Observe that log2 n  len(n), and so it suffices to show that
                                 2min(t, ) - 1 > 22 len(n) t1/2 ,

and for this, it suffices to show that

                                 min(t, ) > 2 len(n) t1/2 ,

since for all integers a, b with a > b  1, we have 2a > 2b + 1.
   To show that t > 2 len(n) t1/2 , it suffices to show that t > 2 len(n)t1/2, or

equivalently, that t > 4 len(n)2. But observe that by definition, t is the order
of the subgroup of Zr generated by [n]r and [p]r, which is at least as large as
the multiplicative order of [n]r in Zr , and by assumption (A4), this is larger than
4 len(n)2.

   Finally, directly by assumption (A5), we have > 2 len(n) t1/2 . 2

   That concludes the proof of Theorem 21.5.

EXERCISE 21.1. Show that if Conjecture 5.24 is true, then the value of r discov-
ered in step 2 of Algorithm AKS satisfies r = O(len(n)2).

                                          21.3 Notes

The algorithm presented here is due to Agrawal, Kayal, and Saxena [6].

   If fast algorithms for integer and polynomial arithmetic are used, then using

the analysis presented here, it is easy to see that the algorithm runs in time
O(len(n)10.5+o(1)). More generally, it is easy to see that the algorithm runs in
time O(r1.5+o(1) len(n)3+o(1)), where r is the value determined in step 2 of the

algorithm. In our analysis of the algorithm, we were able to obtain the bound
r = O(len(n)5), leading to the running-time bound O(len(n)10.5+o(1)). Using a
21.3 Notes                   559

result of Fouvry [37], one can show that r = O(len(n)3), leading to a running-
time bound of O(len(n)7.5+o(1)). Moreover, if Conjecture 5.24 on the density of
Sophie Germain primes were true, then one could show that r = O(len(n)2) (see
Exercise 21.1), which would lead to a running-time bound of O(len(n)6+o(1)). This
running-time bound can be achieved rigorously by a different algorithm, due to

Lenstra and Pomerance [62].

   Prior to this algorithm, the fastest deterministic, rigorously proved primality test

was one introduced by Adleman, Pomerance, and Rumely [5], called the Jacobi

sum test, which runs in time

O(len(n)c len(len(len(n))))

for some constant c. Note that for numbers n with less than 2256 bits, the value of
len(len(len(n))) is at most 8, and so this algorithm runs in time O(len(n)8c) for any
n that one could ever actually write down.

   We also mention the earlier work of Adleman and Huang [3], who gave a prob-
abilistic algorithm whose output is always correct, and which runs in expected
polynomial time (i.e., a Las Vegas algorithm, in the parlance of §9.7).
Appendix: Some useful facts

A1. Some handy inequalities. The following inequalities involving exponen-
     tials and logarithms are very handy.
          (i) For all real numbers x, we have
                                                1 + x  ex,

               or, taking logarithms, for x > -1, we have
                                              log(1 + x)  x.

(ii) For all real numbers x  0, we have
                                 e-x  1 - x + x2/2,

     or, taking logarithms,
                              -x  log(1 - x + x2/2).

(iii) For all real numbers x with 0  x  1/2, we have
                                1 - x  e-x-x2  e-2x,

      or, taking logarithms,
                            log(1 - x)  -x - x2  -2x.

(i) and (ii) follow easily from Taylor's formula with remainder, applied to

the function ex, while (iii) may be proved by expanding log(1 - x) as a

Taylor series, and making a simple calculation.

A2. Binomial coefficients. For integers n and k, with 0  k  n, one defines
     the binomial coefficient

n := n! = n(n - 1) · · · (n - k + 1) .
k k!(n - k)!                                     k!

561
562                   Appendix: Some useful facts

     We have the identities

                                   n = n = 1,
                                   n       0

     and for 0 < k < n, we have Pascal's identity

                             n = n-1 + n-1 ,
                             k        k-1          k

     which may be verified by direct calculation. From these identities, it fol-

     lows that  n  is an integer, and indeed, is equal to the number of subsets of
                k

     {1, . . . , n} of cardinality k. The usual binomial theorem also follows as an

     immediate consequence: for all numbers a, b, and for all positive integers

     n, we have the binomial expansion

                             (a + b)n =  n

                                              n n-k k
                                                   a b.
                                         k=0 k

     It is also easily verified, directly from the definition, that

                   n            n     for 0  k < (n - 1)/2,
                   k < k+1

                   n            n     for (n - 1)/2 < k < n, and
                   k > k+1

                   n= n               for 0  k  n.
                   k         n-k

     In other words, if we fix n, and view      n  as a function of k, then this
                                                k

     function is increasing on the interval [0, n/2], decreasing on the interval

     [n/2, n], and its graph is symmetric with respect to the line k = n/2.

A3. Countably infinite sets. Let Z+ := {1, 2, 3, . . .}, the set of positive inte-
     gers. A set S is called countably infinite if there is a bijection f : Z+  S;

     in this case, we can enumerate the elements of S as x1, x2, x3, . . . , where
     xi := f (i).

     A set S is called countable if it is either finite or countably infinite.

     For a set S, the following conditions are equivalent:

          · S is countable;
          · there is a surjective function g : Z+  S;
          · there is an injective function h : S  Z+.

     The following facts can be easily established:
           Appendix: Some useful facts        563

 (i) if S1, . . . , Sn are countable sets, then so are S1  · · ·  Sn and

      S1 × · · · × Sn;

(ii) if S1, S2, S3, . . . are countable sets, then so is i=1 Si;
(iii) if S is a countable set, then so is the set i=0 S×i of all finite

      sequences of elements in S.

Some examples of countably infinite sets: Z, Q, the set of all finite bit
strings. Some examples of uncountable sets: R, the set of all infinite bit
strings.

A4. Integrating piece-wise continuous functions. In discussing the Riemann

                        b

     integral a f (t) dt, many introductory calculus texts only discuss in any
     detail the case where the integrand f is continuous on the closed inter-

     val [a, b], in which case the integral is always well defined. However,

     the Riemann integral is well defined for much broader classes of func-
     tions. For our purposes in this text, it is convenient and sufficient to work

     with integrands that are piece-wise continuous on [a, b], which means that

     there exist real numbers x0, x1, . . . , xk and functions f1, . . . , fk, such that
     a = x0  x1  · · ·  xk = b, and for each i = 1, . . . , k, the function fi is
     continuous on the closed interval [xi-1, xi], and agrees with f on the open
     interval (xi-1, xi). In this case, f is integrable on [a, b], and indeed

           b              k    xi

              f (t) dt =           fi(t) dt.

           a              i=1  xi-1

It is not hard to prove this equality, using the basic definition of the Riemann
integral; however, for our purposes, we can also just take the value of the
expression on the right-hand side as the definition of the integral on the
left-hand side.

If f is piece-wise continuous on [a, b], then it is also bounded on [a, b],

meaning that there exists a positive number M such that |f (t)|  M for all

                                                               b

t  [a, b], from which it follows that | a f (t) dt|  M (b - a).

We also say that f is piece-wise continuous on [a, ) if for all b  a, f is

piece-wise continuous on [a, b]. In this case, we may define the improper

                                        b
integral a f (t) dt as the limit, as b  , of a f (t) dt, provided the limit

exists.

A5. Estimating sums by integrals. Using elementary calculus, it is easy to
     estimate a sum over a monotone sequence in terms of a definite integral, by
     interpreting the integral as the area under a curve. Let f be a real-valued
     function that is (at least piece-wise) continuous and monotone on the closed
564  Appendix: Some useful facts

     interval [a, b], where a and b are integers. Then we have

     b    b

     min(f (a), f (b))  f (i) - f (t) dt  max(f (a), f (b)).

     i=a  a

A6. Infinite series. Consider an infinite series i=1 xi. It is a basic fact from
     calculus that if the xi's are non-negative and i=1 xi converges to a value
     y, then any infinite series whose terms are a rearrangement of the xi's con-

     verges to the same value y.

     If we drop the requirement that the xi's are non-negative, but insist that
     the series i=1|xi| converges, then the series i=1 xi is called absolutely
     convergent. In this case, then not only does the series i=1 xi converge to
     some value y, but any infinite series whose terms are a rearrangement of

     the xi's also converges to the same value y.

A7. Double infinite series. The topic of double infinite series may not be

     discussed in a typical introductory calculus course; we summarize here the

     basic facts that we need.

     Suppose that {xij}i,j=1 is a family non-negative real numbers such that for
     each i, the series j xij converges to a value ri, and for each j the series

        i xij converges to a value cj. Then we can form the double infinite series
        i j xij = i ri and the double infinite series j i xij = j cj. If
      (i1, j1), (i2, j2), . . . is an enumeration of all pairs of indices (i, j), we can
     also form the single infinite series k xikjk . We then have i j xij =
        j i xij = k xikjk , where the three series either all converge to the same
     value, or all diverge. Thus, we can reverse the order of summation in a

     double infinite series of non-negative terms. If we drop the non-negativity

     requirement, the same result holds provided k|xikjk | < .

     Now suppose i ai is an infinite series of non-negative terms that converges
     to A, and that j bj is an infinite series of non-negative terms that converges
     to B. If (i1, j1), (i2, j2), . . . is an enumeration of all pairs of indices (i, j),
     then k aik bjk converges to AB. Thus, we can multiply term-wise infinite
     series with non-negative terms. If we drop the non-negativity requirement,

     the same result holds provided i ai and j bj converge absolutely.

A8. Convex functions. Let I be an interval of the real line (either open, closed,

     or half open, and either bounded or unbounded), and let f be a real-valued
     function defined on I. The function f is called convex on I if for all

     x0, x2  I, and for all t  [0, 1], we have

     f (tx0 + (1 - t)x2)  tf (x0) + (1 - t)f (x2).
Appendix: Some useful facts  565

Geometrically, convexity means that for every three points Pi = (xi, f (xi)),
i = 0, 1, 2, where each xi  I and x0 < x1 < x2, the point P1 lies on or
below the line through P0 and P2.

We state here the basic analytical facts concerning convex functions:

(i) if f is convex on I, then f is continuous on the interior of I (but
     not necessarily at the endpoints of I, if any);

(ii) if f is continuous on I and differentiable on the interior of I, then
     f is convex on I if and only if its derivative is non-decreasing on
     the interior of I.
                        Bibliography

 [1] L. M. Adleman. A subexponential algorithm for the discrete logarithm problem
      with applications to cryptography. In 20th Annual Symposium on Foundations of
      Computer Science, pages 55-60, 1979.

 [2] L. M. Adleman. The function field sieve. In Proc. 1st International Symposium on
      Algorithmic Number Theory (ANTS-I), pages 108-121, 1994.

 [3] L. M. Adleman and M.-D. Huang. Primality Testing and Two Dimensional Abelian
      Varieties over Finite Fields (Lecture Notes in Mathematics No. 1512). Springer-
      Verlag, 1992.

 [4] L. M. Adleman and H. W. Lenstra, Jr. Finding irreducible polynomials over finite
      fields. In 18th Annual ACM Symposium on Theory of Computing, pages 350-355,
      1986.

 [5] L. M. Adleman, C. Pomerance, and R. S. Rumely. On distinguishing prime numbers
      from composite numbers. Annals of Mathematics, 117:173-206, 1983.

 [6] M. Agrawal, N. Kayal, and N. Saxena. PRIMES is in P. Annals of Mathematics,
      160(2):781-793, 2004.

 [7] W. Alford, A. Granville, and C. Pomerance. There are infinitely many Carmichael
      numbers. Annals of Mathematics, 140:703-722, 1994.

 [8] T. M. Apostol. Introduction to Analytic Number Theory. Springer-Verlag, 1973.
 [9] E. Bach. How to generate factored random numbers. SIAM Journal on Computing,

      17:179-193, 1988.
[10] E. Bach. Explicit bounds for primality testing and related problems. Mathematics

      of Computation, 55:355-380, 1990.
[11] E. Bach and J. Shallit. Algorithmic Number Theory, volume 1. MIT Press, 1996.
[12] P. Bateman and R. Horn. A heuristic asymptotic formula concerning the distribution

      of prime numbers. Mathematics of Computation, 16:363-367, 1962.
[13] M. Bellare and P. Rogaway. Random oracles are practical: a paradigm for designing

      efficient protocols. In First ACM Conference on Computer and Communications
      Security, pages 62-73, 1993.
[14] M. Ben-Or. Probabilistic algorithms in finite fields. In 22nd Annual Symposium on
      Foundations of Computer Science, pages 394-398, 1981.
[15] E. R. Berlekamp. Algebraic Coding Theory. McGraw-Hill, 1968.

                                              566
Bibliography  567

[16] E. R. Berlekamp. Factoring polynomials over large finite fields. Mathematics of
      Computation, 24(111):713-735, 1970.

[17] L. Blum, M. Blum, and M. Shub. A simple unpredictable pseudo-random number
      generator. SIAM Journal on Computing, 15:364-383, 1986.

[18] D. Boneh. The Decision Diffie-Hellman Problem. In Proc. 3rd International Sym-
      posium on Algorithmic Number Theory (ANTS-III), pages 48-63, 1998. Springer
      LNCS 1423.

[19] D. Boneh and G. Durfee. Cryptanalysis of RSA with private key d less than N0.292.
      IEEE Transactions on Information Theory, IT-46:1339-1349, 2000.

[20] R. P. Brent and H. T. Kung. Fast algorithms for manipulating formal power series.
      Journal of the ACM, 25:581-595, 1978.

[21] J. P. Buhler, H. W. Lenstra, Jr., and C. Pomerance. Factoring integers with the
      number field sieve. In A. K. Lenstra and H. W. Lenstra, Jr., editors, The Development
      of the Number Field Sieve, pages 50-94. Springer-Verlag, 1993.

[22] D. A. Burgess. The distribution of quadratic residues and non-residues. Mathe-
      matika, 4:106-112, 1957.

[23] E. Canfield, P. Erds, and C. Pomerance. On a problem of Oppenheim concerning
      `Factorisatio Numerorum'. Journal of Number Theory, 17:1-28, 1983.

[24] D. G. Cantor and E. Kaltofen. On fast multiplication of polynomials over arbitrary
      rings. Acta Informatica, 28:693-701, 1991.

[25] J. L. Carter and M. N. Wegman. Universal classes of hash functions. Journal of
      Computer and System Sciences, 18:143-154, 1979.

[26] A. L. Chistov. Polynomial time construction of a finite field. In Abstracts of Lectures
      at 7th All-Union Conference in Mathematical Logic, Novosibirsk, page 196, 1984.
      In Russian.

[27] D. Coppersmith. Modifications to the number field sieve. Journal of Cryptology,
      6:169-180, 1993.

[28] D. Coppersmith and S. Winograd. Matrix multiplication via arithmetic progressions.
      Journal of Symbolic Computation, 9(3):23-52, 1990.

[29] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction to Algo-
      rithms. MIT Press, second edition, 2001.

[30] R. Crandall and C. Pomerance. Prime Numbers: A Computational Perspective.
      Springer, 2001.

[31] I. Damgård and G. Frandsen. Efficient algorithms for gcd and cubic residuosity in
      the ring of Eisenstein integers. In 14th International Symposium on Fundamentals
      of Computation Theory, Springer LNCS 2751, pages 109-117, 2003.

[32] I. Damgård, P. Landrock, and C. Pomerance. Average case error estimates for the
      strong probable prime test. Mathematics of Computation, 61:177-194, 1993.

[33] L. E. Dickson. A new extension of Dirichlet's theorem on prime numbers. Messen-
      ger of Mathematics, 33:151-161, 1904.

[34] W. Diffie and M. E. Hellman. New directions in cryptography. IEEE Transactions
      on Information Theory, IT-22:644-654, 1976.

[35] J. Dixon. Asymptotically fast factorization of integers. Mathematics of Computa-
      tion, 36:255-260, 1981.

[36] J. L. Dornstetter. On the equivalence between Berlekamp's and Euclid's algorithms.
      IEEE Transactions on Information Theory, IT-33:428-431, 1987.
568  Bibliography

[37] E. Fouvry. Théorème de Brun-Titchmarsh; application au théorème de Fermat.
      Inventiones Mathematicae, 79:383-407, 1985.

[38] M. Fürer. Faster integer multiplication. In 39th Annual ACM Symposium on Theory
      of Computing, pages 57-66, 2007.

[39] J. von zur Gathen and J. Gerhard. Modern Computer Algebra. Cambridge Univer-
      sity Press, second edition, 2003.

[40] J. von zur Gathen and V. Shoup. Computing Frobenius maps and factoring polyno-
      mials. Computational Complexity, 2:187-224, 1992.

[41] S. Goldwasser and S. Micali. Probabilistic encryption. Journal of Computer and
      System Sciences, 28:270-299, 1984.

[42] D. M. Gordon. Discrete logarithms in GF(p) using the number field sieve. SIAM
      Journal on Discrete Mathematics, 6:124-138, 1993.

[43] J. Gordon. Very simple method to find the minimal polynomial of an arbitrary non-
      zero element of a finite field. Electronic Letters, 12:663-664, 1976.

[44] H. Halberstam and H. Richert. Sieve Methods. Academic Press, 1974.

[45] G. H. Hardy and J. E. Littlewood. Some problems of partito numerorum. III. On the
      expression of a number as a sum of primes. Acta Mathematica, 44:1-70, 1923.

[46] G. H. Hardy and E. M. Wright. An Introduction to the Theory of Numbers. Oxford
      University Press, fifth edition, 1984.

[47] D. Heath-Brown. Zero-free regions for Dirichlet L-functions and the least prime
      in an arithmetic progression. Proceedings of the London Mathematical Society,
      64:265-338, 1992.

[48] R. Impagliazzo, L. Levin, and M. Luby. Pseudo-random number generation from
      any one-way function. In 21st Annual ACM Symposium on Theory of Computing,
      pages 12-24, 1989.

[49] R. Impagliazzo and D. Zuckermann. How to recycle random bits. In 30th Annual
      Symposium on Foundations of Computer Science, pages 248-253, 1989.

[50] H. Iwaniec. On the error term in the linear sieve. Acta Arithmetica, 19:1-30, 1971.

[51] H. Iwaniec. On the problem of Jacobsthal. Demonstratio Mathematica, 11:225-
      231, 1978.

[52] A. Kalai. Generating random factored numbers, easily. In Proc. 13th ACM-SIAM
      Symposium on Discrete Algorithms, page 412, 2002.

[53] E. Kaltofen and V. Shoup. Subquadratic-time factoring of polynomials over finite
      fields. In 27th Annual ACM Symposium on Theory of Computing, pages 398-406,
      1995.

[54] A. A. Karatsuba and Y. Ofman. Multiplication of multidigit numbers on automata.
      Soviet Physics Doklady, 7:595-596, 1963.

[55] S. H. Kim and C. Pomerance. The probability that a random probable prime is
      composite. Mathematics of Computation, 53(188):721-741, 1989.

[56] D. E. Knuth. The Art of Computer Programming, volume 2. Addison-Wesley,
      second edition, 1981.

[57] T. Krovetz and P. Rogaway. Variationally universal hashing. Information Processing
      Letters, 100:36-39, 1996.

[58] D. Lehmann. On primality tests. SIAM Journal on Computing, 11:374-375, 1982.

[59] D. Lehmer and R. Powers. On factoring large numbers. Bulletin of the AMS,
      37:770-776, 1931.
Bibliography  569

[60] H. W. Lenstra, Jr. Factoring integers with elliptic curves. Annals of Mathematics,
      126:649-673, 1987.

[61] H. W. Lenstra, Jr. and C. Pomerance. A rigorous time bound for factoring integers.
      Journal of the AMS, 4:483-516, 1992.

[62] H. W. Lenstra, Jr. and C. Pomerance. Primality testing with Gaussian periods.
      Manuscript, math.dartmouth.edu/~carlp, 2005.

[63] M. Luby. Pseudorandomness and Cryptographic Applications. Princeton Univer-
      sity Press, 1996.

[64] J. Massey. Shift-register synthesis and BCH coding. IEEE Transactions on Infor-
      mation Theory, IT-15:122-127, 1969.

[65] U. Maurer. Fast generation of prime numbers and secure public-key cryptographic
      parameters. Journal of Cryptology, 8:123-155, 1995.

[66] A. Menezes, P. van Oorschot, and S. Vanstone. Handbook of Applied Cryptography.
      CRC Press, 1997.

[67] G. L. Miller. Riemann's hypothesis and tests for primality. Journal of Computer
      and System Sciences, 13:300-317, 1976.

[68] W. Mills. Continued fractions and linear recurrences. Mathematics of Computation,
      29:173-180, 1975.

[69] K. Morrison. Random polynomials over finite fields. Manuscript, www.calpoly.
      edu/~kmorriso/Research/RPFF.pdf, 1999.

[70] M. Morrison and J. Brillhart. A method of factoring and the factorization of F7.
      Mathematics of Computation, 29:183-205, 1975.

[71] V. I. Nechaev. Complexity of a determinate algorithm for the discrete logarithm.
      Mathematical Notes, 55(2):165-172, 1994. Translated from Matematicheskie
      Zametki, 55(2):91-101, 1994.

[72] I. Niven and H. Zuckerman. An Introduction to the Theory of Numbers. John Wiley
      and Sons, Inc., second edition, 1966.

[73] J. Oesterlé. Versions effectives du théorème de Chebotarev sous l'hypothèse de
      Riemann généralisée. Astérisque, 61:165-167, 1979.

[74] P. van Oorschot and M. Wiener. On Diffie-Hellman key agreement with short expo-
      nents. In Advances in Cryptology-Eurocrypt '96, Springer LNCS 1070, pages 332-
      343, 1996.

[75] S. Pohlig and M. Hellman. An improved algorithm for computing logarithms over
      GF(p) and its cryptographic significance. IEEE Transactions on Information The-
      ory, IT-24:106-110, 1978.

[76] J. M. Pollard. Monte Carlo methods for index computation mod p. Mathematics of
      Computation, 32:918-924, 1978.

[77] J. M. Pollard. Factoring with cubic integers. In A. K. Lenstra and H. W. Lenstra, Jr.,
      editors, The Development of the Number Field Sieve, pages 4-10. Springer-Verlag,
      1993.

[78] C. Pomerance. Analysis and comparison of some integer factoring algorithms. In
      H. W. Lenstra, Jr. and R. Tijdeman, editors, Computational Methods in Number
      Theory, Part I, pages 89-139. Mathematisch Centrum, 1982.

[79] M. O. Rabin. Probabilistic algorithms. In Algorithms and Complexity, Recent
      Results and New Directions, pages 21-39. Academic Press, 1976.

[80] D. Redmond. Number Theory -- An Introduction. Marcel Dekker, 1996.
570    Bibliography

 [81]  I. Reed and G. Solomon. Polynomial codes over certain finite fields. SIAM Journal
 [82]  on Applied Mathematics, pages 300-304, 1960.

 [83]  R. L. Rivest, A. Shamir, and L. M. Adleman. A method for obtaining digital signa-
 [84]  tures and public-key cryptosystems. Communications of the ACM, 21(2):120-126,
       1978.
 [85]
 [86]  J. Rosser and L. Schoenfeld. Approximate formulas for some functions of prime
 [87]  numbers. Illinois Journal of Mathematics, 6:64-94, 1962.
 [88]  O. Schirokauer, D. Weber, and T. Denny. Discrete logarithms: the effectiveness of
       the index calculus method. In Proc. 2nd International Symposium on Algorithmic
 [89]  Number Theory (ANTS-II), pages 337-361, 1996.
 [90]
 [91]  A. Schönhage. Schnelle Berechnung von Kettenbruchentwicklungen. Acta Infor-
 [92]  matica, 1:139-144, 1971.

 [93]  A. Schönhage and V. Strassen. Schnelle Multiplikation grosser Zahlen. Computing,
 [94]  7:281-282, 1971.
 [95]
 [96]  R. Schoof. Elliptic curves over finite fields and the computation of square roots mod
 [97]  p. Mathematics of Computation, 44:483-494, 1985.
 [98]
 [99]  I. A. Semaev. Construction of irreducible polynomials over finite fields with lin-
[100]  early independent roots. Mat. Sbornik, 135:520-532, 1988. In Russian; English
[101]  translation in Math. USSR-Sbornik, 63(2):507-519, 1989.

       A. Shamir. Factoring numbers in O(log n) arithmetic steps. Information Processing
       Letters, 8:28-31, 1979.

       A. Shamir. How to share a secret. Communications of the ACM, 22:612-613, 1979.

       D. Shanks. Class number, a theory of factorization, and genera. In Proceedings of
       Symposia in Pure Mathematics, volume 20, pages 415-440, 1969.

       P. Shor. Algorithms for quantum computation: discrete logarithms and factoring.
       In 35th Annual Symposium on Foundations of Computer Science, pages 124-134,
       1994.

       P. Shor. Polynomial-time algorithms for prime factorization and discrete logarithms
       on a quantum computer. SIAM Review, 41:303-332, 1999.

       V. Shoup. New algorithms for finding irreducible polynomials over finite fields.
       Mathematics of Computation, 54(189):435-447, 1990.

       V. Shoup. Searching for primitive roots in finite fields. Mathematics of Computation,
       58:369-380, 1992.

       V. Shoup. Fast construction of irreducible polynomials over finite fields. Journal of
       Symbolic Computation, 17(5):371-391, 1994.

       V. Shoup. A new polynomial factorization algorithm and its implementation. Jour-
       nal of Symbolic Computation, 20(4):363-397, 1995.

       V. Shoup. Lower bounds for discrete logarithms and related problems. In Advances
       in Cryptology-Eurocrypt '97, pages 256-266, 1997.

       R. Solovay and V. Strassen. A fast Monte-Carlo test for primality. SIAM Journal on
       Computing, 6:84-85, 1977.

       J. Stein. Computational problems associated with Racah algebra. Journal of Com-
       putational Physics, 1:397-405, 1967.

       D. R. Stinson. Universal hashing and authentication codes. Designs, Codes, and
       Cryptography, 4:369-380, 1994.
       Bibliography  571

[102]  A. Walfisz. Weylsche Exponentialsummen in der neueren Zahlentheorie. VEB
[103]  Deutscher Verlag der Wissenschaften, 1963.
[104]
[105]  P. Wang, M. Guy, and J. Davenport. p-adic reconstruction of rational numbers.
[106]  SIGSAM Bulletin, 16:2-3, 1982.
[107]
[108]  Y. Wang. On the least primitive root of a prime. Scientia Sinica, 10(1):1-14, 1961.
[109]
[110]  M. N. Wegman and J. L. Carter. New hash functions and their use in authentication
[111]  and set equality. Journal of Computer and System Sciences, 22:265-279, 1981.
       A. Weilert. (1 + i)-ary GCD computation in Z[i] as an analogue to the binary GCD
       algorithm. Journal of Symbolic Computation, 30:605-617, 2000.

       A. Weilert. Asymptotically fast GCD computation in Z[i]. In Proc. 4th International
       Symposium on Algorithmic Number Theory (ANTS-IV), pages 595-613, 2000.

       L. Welch and R. Scholtz. Continued fractions and Berlekamp's algorithm. IEEE
       Transactions on Information Theory, IT-25:19-27, 1979.

       D. Wiedemann. Solving sparse linear systems over finite fields. IEEE Transactions
       on Information Theory, IT-32:54-62, 1986.

       M. Wiener. Cryptanalysis of short RSA secret exponents. IEEE Transactions on
       Information Theory, IT-44:553-558, 1990.

       D. Y. Y. Yun. On square-free decomposition algorithms. In Proc. ACM Symposium
       on Symbolic and Algebraic Computation, pages 26-35, 1976.
Index of notation

Entries are listed in order of appearance.

log: natural logarithm, xiv                          li: logarithmic integral, 117
                                                     (s): Riemann's zeta function, 118
exp: exponential function, xiv                       Map(I, G): group of functions f : I  G, 131
                                                     mG: the subgroup {ma : a  G}, 133
, , , , , , \, |·|: set notation, xiv                G{m}: the subgroup {a  G : ma = 0G}, 133
S1 × · · · × Sn, S×n: Cartesian product, xiv         Gm: multiplicative subgroup {am : a  G}, 133
{xi}iI : family, xv                                  H1 + H2: sum of subgroups, 136
{xi}ni=m, {xi}i=m: sequence, xv                      H1H2: product of subgroups, 136
Z: the integers, xv                                  a  b (mod H): a - b  H, 137
Q: the rationals, xv                                 [a]H : coset of H containing a, 138
R: the reals, xv                                     G/H: quotient group, 140
C: the complex numbers, xv                           [G : H]: index, 140
: arithmetic with infinity, xvi                      Ker : kernel, 143
                                                     Im : image, 143
[a, b], (a, b), etc.: interval notation, xvi         G = G : isomorphic groups, 146
                                                     Hom(G, G ): group homomorphisms G  G , 151
f (S): image of a set, xvi                            a : subgroup generated by a, 153
f -1: pre-image of a set/inverse function, xvi        a1, . . . , ak : subgroup generated by a1, . . . , ak, 153
                                                     : complex conjugate of , 167
f  g: function composition, xvi                      N (): norm of   C, 167
                                                     Map(I, R): ring of functions f : I  R, 168
a | b: a divides b, 1                                AB: ring-theoretic product, 169
                                                     a | b: a divides b, 170
 x : floor of x, 4                                   R: multiplicative group of units of R, 170
                                                     Z[i]: Gaussian integers, 174
 x : ceiling of x, 4                                 Q(m): {a/b : gcd(b, m) = 1}, 174
                                                    R[X ]: ring of polynomials, 176
a mod b: integer remainder, 4                        deg(g): degree of a polynomial, 177
aZ: ideal generated by a, 5                          lc(g): leading coefficient of a polynomial, 177
I1 + I2: sum of ideals, 6                            g mod h: polynomial remainder, 178
gcd: greatest common divisor, 7                      aR: ideal generated by a, 186
                                                     (a1, . . . , ak ): ideal generated by a1, . . . , ak, 186
p(n): largest power to which p divides n, 10         R/I: quotient ring, 187
lcm: least common multiple, 11                       a  b (mod d): a - b  dR, 187
                                                     [a]d: the residue class [a]dR, 187
a  b (mod n): a congruent to b modulo n, 16          R[]: smallest subring containing R and , 192
                                                     R[1, . . . , n]: smallest subring containing R and
b/a mod n: integer remainder, 22
a-1 mod n: integer modular inverse, 22                     1, . . . , n, 193

[a]n, [a]: residue class of a modulo n, 25      572
Zn: residue classes modulo n, 25
Zn: invertible residue classes, 28
(n): Euler's phi function, 31
(Zn )m: mth powers in Zn, 36
µ(n): Möbius function, 46
O, , , o, : asymptotic notation, 50

len(a): length (in bits) of an integer, 62
rep(): canonical representative of   Zn, 65
(x): number of primes up to x, 104

: Chebyshev's theta function, 107
                                                   Index of notation  573

R = R : isomorphic rings, 195

P: probability distribution, 207
P1 P2, P1n: product distribution, 211
P[A | B]: conditional probability of A given B, 214

E[X]: expected value of X, 233

Var[X]: variance of X, 235

E[X | B]: conditional expectation of X given B, 237
[X; Y]: statistical distance, 260

y   ¢  {0, 1},  y   ¢  {0, 1}×  :  assign  random  bit(s),  278

                   

    ¢

y  T : assign random element of T , 287

log : discrete logarithm, 327

(a | p): Legendre symbol, 342

(a | n): Jacobi symbol, 346

Jn: Jacobi map, 347

Map(I, M): R-module of functions f : I  M, 360

cM: submodule {c :   M}, 361
M{c}: submodule {  M : c = 0M }, 361

R: submodule {c : c  R}, 361

1, . . . , k R: submodule generated by 1, . . . , k,

       361

R[X ]< : polynomials of degree less than , 361

M/N: quotient module, 362
M = M : isomorphic modules, 365

HomR (M, M ): R-linear maps M  M , 366

dimF (V ): dimension, 372

A(i, j): (i, j) entry of A, 378

Rowi(A): ith row of A, 378

Colj (A): jth column of A, 378
Rm×n: m × n matrices over R, 378
0Rm×n: m × n zero matrix, 378
A : transpose of A, 380

VecS (): coordinate vector, 382

MatS,T (): matrix of linear map, 383
(y, x): number of y-smooth integers up to x, 399

Map(I, E): R-algebra of functions f : I  E, 423

R[]: subalgebra generated by , 426

gcd: greatest common divisor (polynomial), 432

lcm: least common multiple (polynomial), 433

h/g mod f : polynomial remainder, 435
g-1 mod f : polynomial modular inverse, 435

(E : F ): degree of an extension, 440

F (): smallest subfield containing F and , 441

R[[X ]]: formal power series, 446

R((X )): formal Laurent series, 447
R((X -1)): reversed Laurent series, 449
deg(g): degree of g  R((X -1)), 449
lc(g): leading coefficient of g  R((X -1)), 449
 g : floor of g  R((X -1)), 449

len(g): length of a polynomial, 466

rep(): canonical representative of   R[X ]/(f ),

       466

DF (V ): dual space, 492

LF (V ): space of linear transformations, 501

NE/F (): norm, 518

TrE/F (): trace, 518
                                             Index

Abel's identity, 112                              Bertrand's postulate, 109
abelian group, 126                                big-O, -Omega, -Theta, 50
additive identity, 27                             bijection, xvi
additive inverse, 27                              bijective, xvi
additive subgroup, 169                            binary gcd algorithm, 77
Adleman, L. M., 99, 103, 419, 420, 546, 559       binary operation, xvii
Agrawal, M., 548, 558                             binomial coefficient, 561
Alford, W., 325                                   binomial distribution, 223
algebra, 421                                      binomial expansion, 562
algebraic                                         binomial theorem, 169, 562
                                                  birthday paradox, 248
   element, 441                                   bivariate polynomial, 183
   extension, 441                                 Blum, L., 103
almost universal hash functions, 258              Blum, M., 103
Apostol, T. M., 125                               Boneh, D., 103, 341
approximately computes, 302                       Bonferroni's inequalities, 213
arithmetic function, 45                           Boole's equality, 210
arithmetic/geometric mean, 240                    Boole's inequality, 210
Artin's conjecture, 99                            Boolean circuits, 72
associate                                         Brent, R. P., 485
   elements of an integral domain, 451            Brillhart, J., 418
   polynomials, 430                               Buhler, J. P., 419
associative binary operation, xvii                Burgess, D. A., 357
asymptotic notation, 50
Atlantic City algorithm, 303                      C, xv
automorphism                                      cancellation law, 2, 21, 28, 129, 171, 435
   algebra, 424                                   Canfield, E., 418
   group, 146                                     canonical representative
   module, 365
   ring, 195                                         integer, 65
   vector space, 370                                 polynomial, 466
                                                  Cantor, D. G., 546
baby step/giant step method, 330                  Cantor-Zassenhaus algorithm, 530
Bach, E., 125, 305, 325, 340, 357, 546            cardinality, xiv
basis, 367                                        Carmichael number, 308
Bateman, P., 125                                  Carmichael, R. D., 325
Bayes' theorem, 215                               Carter, J. L., 275
Bellare, M., 420                                  Cartesian product, xiv
Ben-Or, M., 546                                   ceiling, 4
Berlekamp subalgebra, 538                         characteristic of a ring, 169
Berlekamp's algorithm, 538                        characteristic polynomial, 518
Berlekamp, E. R., 508, 546                        Chebyshev's inequality, 241
Bernoulli trial, 208                              Chebyshev's theorem, 105
Bernstein approximation, 240                      Chebyshev's theta function, 107

                                             574
                                             Index                                           575

Chernoff bound, 242                          deterministic poly-time reducible, 336
Chinese remainder theorem                    Dickson, L. E., 125
                                             Diffie, W., 341
   general, 202                              Diffie-Hellman key establishment protocol, 334
   integer, 23, 82                           Diffie-Hellman problem, 335
   polynomial, 435, 473                      dimension, 372
Chistov, A. L., 546                          direct product
classification of cyclic groups, 156
closed under, xvii                              of algebras, 422
column null space, 395                          of groups, 130
column rank, 394                                of modules, 360
column space, 394                               of rings, 168
column vector, 378                           Dirichlet inverse, 49
common divisor                               Dirichlet product, 45
   in an integral domain, 452                Dirichlet series, 120
   integer, 6                                Dirichlet's theorem, 121
   polynomial, 431                           Dirichlet, G., 125
common multiple                              discrete logarithm, 327
   in an integral domain, 452                   algorithm for computing, 329, 400
   integer, 11                               discrete probability distribution, 270
   polynomial, 433                           discriminant, 182
commutative binary operation, xvii           disjoint, xv
commutative ring with unity, 166             distinct degree factorization, 530, 543
companion matrix, 385                        distributive law
complex conjugation, 167, 198                   Boolean, 208
composite, 2                                 divides, 1, 170
composition, xvi                             divisible by, 1, 170
conditional distribution, 213, 224           division with remainder property
conditional expectation, 237                    integer, 3
conditional probability, 214                    polynomial, 178
congruence, 16, 137                          divisor, 1, 170
conjugacy class, 516                         Dixon, J., 418
conjugate, 516                               Dornstetter, J. L., 508
constant polynomial, 176                     dual space, 492
constant term, 177                           Durfee, G., 103
continued fraction method, 418
continued fractions, 103                     Eisenstein integers, 456
convex function, 564                         Eisenstein's criterion, 462
coordinate vector, 382                       elementary row operation, 389
   of a projection, 492                      elliptic curve method, 419
Coppersmith, D., 419                         equal degree factorization, 532, 537
Cormen, T. H., 340                           equivalence class, 15
coset, 138                                   equivalence relation, 15
countable, 562                               Eratosthenes
countably infinite, 562
covariance, 240                                 sieve of, 115
Crandall, R., 73, 125, 420                   Erds, P., 418
cyclic, 153                                  error correcting code, 97, 477
                                             error probability, 302
Damgård, I., 325, 464                        essentially equal
Davenport, J., 103
De Morgan's law, 208                            probability distributions, 212
decisional Diffie-Hellman problem, 338       Euclidean algorithm
degree
                                                extended
   of a polynomial, 177                            integer, 78
   of a reversed Laurent series, 449               polynomial, 470
   of an element in an extension field, 441
   of an extension, 440                         integer, 74
Denny, T., 420                                  polynomial, 469
derivative, 444                              Euclidean domain, 454
deterministic algorithm, 278                 Euler's criterion, 38, 165, 205
deterministic poly-time equivalent, 336      Euler's identity, 118
                                             Euler's phi function, 31
                                                and factoring, 320
                                             Euler's summation formula, 114
576                                          Index

Euler's theorem, 34, 157                           gcd
Euler's totient function, 31                          integer, 7
Euler, L., 125                                        polynomial, 432
event, 208
eventually positive, 50                            generating polynomial, 487
exp, xiv                                           generator, 153
expectation, 233
expected polynomial time, 283                         algorithm for finding, 327
expected running time, 283                         geometric distribution, 271, 274
expected value, 233                                Gerhard, J., 485, 546
exponent, 160                                      Goldwasser, S., 357
                                                   Gordon, D. M., 420
   module, 363                                     Gordon, J., 546
extended Euclidean algorithm                       Granville, A., 325
                                                   greatest common divisor
   integer, 78
   polynomial, 470                                    in an integral domain, 452
extended Gaussian elimination, 391                    integer, 6
extension, xvi                                        polynomial, 431
extension field, 175, 440                          group, 126
extension ring, 174                                Guy, M., 103

factoring                                          Hadamard, J., 124
   and Euler's phi function, 320                   Halberstam, H., 325
                                                   Hardy, G. H., 103, 124, 125
factoring algorithm                                hash function, 252
   integer, 407, 414                               Heath-Brown, D., 125
      deterministic, 484                           Hellman, M., 340, 341
   polynomial, 530, 538                            Hensel lifting, 351
      deterministic, 544                           homomorphism

family, xv                                            algebra, 424
fast Fourier transform, 480                           group, 142
Fermat's little theorem, 34, 35                       module, 363
FFT, 480                                              ring, 192
field, 170                                            vector space, 370
field of fractions, 427                            Horn, R., 125
finite dimensional, 372                            Horner's rule, 467
finite expectation, 272                            Huang, M.-D., 559
finite extension, 440                              hybrid argument, 264
finite fields
                                                   ideal, 5, 185
   existence, 511                                     generated by, 5, 186
   subfield structure, 515                            maximal, 190
   uniqueness, 515                                    prime, 189
finitely generated                                    principal, 5, 186
   abelian group, 153
   module, 361                                     identity element, 126
floor, 4                                           identity map, xvi
   of reversed Laurent series, 449                 identity matrix, 379
formal derivative, 444                             image, xvi
formal Laurent series, 447                         image of a random variable, 221
formal power series, 446                           Impagliazzo, R., 276
Fouvry, E., 559                                    inclusion map, xvi
Frandsen, G., 464                                  inclusion/exclusion principle, 210
Frobenius map, 512                                 independent, 214, 224
fundamental theorem
   of arithmetic, 2                                   k-wise, 218, 225
   of finite abelian groups, 163                      mutually, 218, 225
  of finite dimensional F [X ]-modules, 506        indeterminate, 176
Fürer, M., 72                                      index, 140
                                                   index calculus method, 420
von zur Gathen, J., 485, 546, 547                  index set, xv
Gauss' lemma, 344                                  indicator variable, 222
Gaussian elimination, 389                          infinite extension, 440
Gaussian integers, 174, 199, 454, 457              infinite order, 130
                                                   injective, xvi
                                      Index  577

integral domain, 171                  Leiserson, C. E., 340
internal direct product, 148          len, 62, 466
inverse                               length

   multiplicative, 170                   of a polynomial, 466
   of a group element, 126               of an integer, 62
   of a matrix, 386                   Lenstra, Jr., H. W., 419, 546, 559
inverse function, xvi                 Levin, L., 276
invertible matrix, 386                li, 117
irreducible element, 451              linear combination, 361
irreducible polynomial, 430           linear map, 363
   algorithm for generating, 523      linear transformation, 501
   algorithm for testing, 522         linearly dependent, 367
   number of, 514                     linearly generated sequence, 486
isomorphism                              minimal polynomial of, 487
   algebra, 424                          of full rank, 492
   group, 146                         linearly independent, 367
   module, 365                        little-o, 50
   ring, 195                          Littlewood, J. E., 125
   vector space, 370                  log, xiv
Iwaniec, H., 340                      logarithmic integral, 117
                                      lowest terms, 12
Jacobi map, 347                       Luby, M., 276, 305
Jacobi sum test, 559
Jacobi symbol, 346                    map, xvi
                                      Markov's inequality, 241
   algorithm for computing, 348       Massey, J., 508
Jensen's inequality, 239, 275         matrix, 377
                                      matrix of a linear map, 383
Kalai, A., 305                        Maurer, U., 325
Kaltofen, E., 547                     maximal ideal, 190
Karatsuba, A. A., 71                  memory cells, 53
Kayal, N., 548, 558                   Menezes, A., 103
Kedlaya, K., 485                      Mertens' theorem, 113
kernel, 143                           Micali, S., 357
kills, 160                            Miller, G. L., 324, 325
Kim, S. H., 325                       Miller-Rabin test, 307
Knuth, D. E., 72, 73, 103             Mills, W., 484, 508
von Koch, H., 125                     min entropy, 266
Kronecker substitution, 479           minimal polynomial, 438
Krovetz, T., 276
Kung, H. T., 485                         algorithm for computing, 468, 500, 525
                                         of a linear transformation, 503
Lagrange interpolation formula, 436      of a linearly generated sequence, 487
Las Vegas algorithm, 303                 of an element under a linear transformation, 504
Latin square, 131                     Möbius function (µ), 46
law of large numbers, 242             Möbius inversion formula, 47
law of quadratic reciprocity, 343     mod, 4, 16, 22, 178, 435
law of total expectation, 237         modular composition, 467, 485
law of total probability, 215         modular square root
lcm                                      algorithm for computing, 350
                                      module, 358
   integer, 11                        modulus, 16
   polynomial, 433                    monic associate, 430
leading coefficient, 177              monic polynomial, 177
   of a reversed Laurent series, 449  monomial, 183
least common multiple                 Monte Carlo algorithm, 303
   in an integral domain, 452         Morrison, K., 508
   integer, 11                        Morrison, M., 418
   polynomial, 433                    multi-variate polynomial, 184
leftover hash lemma, 267              multiple, 1, 170
Legendre symbol, 342                  multiple root, 182
Lehmann, D., 325                      multiplication map, 143, 166, 363
Lehmer, D., 418
578                                        Index

multiplicative function, 46                      de la Vallée Poussin, C.-J., 124, 125
multiplicative group of units, 170               power map, 143
multiplicative identity, 27                      pre-image, xvi
multiplicative inverse                           pre-period, 98
                                                 primality test
   in a ring, 170
   modulo integers, 21                              deterministic, 548
   modulo polynomials, 435                          probabilistic, 306
multiplicative order, 33, 153                    prime
multiplicative order modulo n, 33                   ideal, 189
multiplicity, 182                                   number, 2
mutually independent, 218, 225                   prime number theorem, 116
                                                    irreducible polynomials over a finite field, 514
natural map, 143, 192                            primitive polynomial, 459
Newton interpolation, 474                        principal ideal, 5, 186
Newton's identities, 450                         principal ideal domain, 455
Niven, I., 357                                   probabilistic algorithm, 278
norm, 167, 518                                   probability distribution
normal basis, 521                                   conditional, 213
number field sieve, 419                             discrete, 270
                                                    finite, 207
Oesterlé, J., 125                                product distribution, 211
one-sided error, 304                             program, 53
one-time pad, 229                                projection, 492
one-to-one correspondence, xvi                   public key cryptography, 341
van Oorschot, P., 103, 341                       public key cryptosystem, 99
order                                            purely periodic, 98

   in a module, 364                              Q, xv
   of a group element, 153                       quadratic formula, 182
   of an abelian group, 130                      quadratic reciprocity, 343
                                                 quadratic residue, 36
pairwise disjoint, xv                            quadratic residuosity
pairwise independent
                                                    algorithm for testing, 349
   events, 218                                      assumption, 355
   hash functions, 252                           quadratic sieve, 415
   random variables, 225                         quantum computer, 420
pairwise relatively prime                        quotient algebra, 423
   integers, 11                                  quotient group, 140
   polynomials, 434                              quotient module, 362
parity check matrix, 385                         quotient ring, 187
partition, xv                                    quotient space, 370
Pascal's identity, 562
Penk, M., 103                                    R, xv
perfect power, 64                                Rabin, M. O., 324
period, 98                                       Rackoff, C., 418
periodic sequence, 98                            RAM, 53
PID, 455                                         random access machine, 53
pivot element, 389                               random self-reduction, 337
pivot sequence, 388                              random variable, 221
Pohlig, S., 340
Pollard, J. M., 340, 419                            conditional distribution of, 224
polynomial                                          conditional expectation, 237
   associate, 430                                   distribution of, 222
   irreducible, 430                                 expected value, 233
   monic, 177                                       image, 221
   primitive, 459                                   independent, 224
   reducible, 430                                   real valued, 221
polynomial evaluation map, 192, 426                 variance, 235
polynomial time, 55                              random walk, 244
   expected, 283                                 randomized algorithm, 278
   strict, 283                                   rank, 395
Pomerance, C., 73, 125, 325, 418-420, 559        rational function field, 429
                                                   Index                                     579

rational function reconstruction, 474              Shallit, J., 125, 357, 546
rational reconstruction problem, 90                Shamir, A., 71, 99, 103, 275
recursion tree, 332, 340                           Shanks, D., 340
Redmond, D., 125                                   shift register sequence, 488
reduced row echelon form, 388                      Shor, P., 420
reducible polynomial, 430                          Shoup, V., 340, 508, 546, 547
Reed, I., 484                                      Shub, M., 103
Reed-Solomon code, 97, 477                         sieve of Eratosthenes, 115
regular function, 223                              simple root, 182
relatively prime                                   smooth number, 399, 414
                                                   Solomon, G., 484
   in an integral domain, 452                      Solovay, R., 325, 357
   integers, 7                                     Sophie Germain prime, 123
   polynomials, 432                                spans, 367
Renyi entropy, 266                                 splitting field, 443
rep, 65, 466                                       square root (modular)
repeated-squaring algorithm, 65
representation, 337                                   algorithm for computing, 350
representative                                     square-free
   of a coset, 138
   of a residue class, 25                             integer, 9
   of an equivalence class, 16                        polynomial, 509
residue class, 25, 187                             square-free decomposition, 527
residue class ring, 187                            square-free decomposition algorithm, 527
restriction, xvi                                   standard basis, 368
reversed Laurent series, 448                       statistical distance, 260
Richert, H., 325                                   Stein, C., 340
Riemann hypothesis, 118, 120, 122, 125, 324, 326,  Stein, J., 103
                                                   Stinson, D. R., 276
      340, 357, 482, 546, 547                      Stirling's approximation, 114
Riemann's zeta function, 118                       Strassen, V., 72, 325, 357
Riemann, B., 125                                   strict polynomial time, 283
ring, 166                                          subalgebra, 423
ring of polynomials, 176                              fixed by, 425
ring-theoretic product, 169                           generated by, 426
Rivest, R. L., 99, 103, 340                        subfield, 175
Rogaway, P., 276, 420                              subgroup, 132
root of a polynomial, 179, 425                        generated by, 153
Rosser, J., 124                                    submodule, 360
row echelon form, 397                              subring, 173
row null space, 393                                subspace, 370
row rank, 394                                      sufficiently large, 50
row space, 392                                     surjective, xvi
row vector, 378
RSA cryptosystem, 99                               theta function of Chebyshev, 107
Rumely, R. S., 559                                 total degree, 183
running time                                       total expectation

   expected, 283                                      law of, 237
                                                   total probability
sample mean, 241
sample space, 207                                     law of, 215
Saxena, N., 548, 558                               trace, 518
scalar, 358                                        transcendental element, 441
scalar multiplication map, 358                     transpose, 380
Schirokauer, O., 420                               trial division, 306
Schoenfeld, L., 124                                trivial
Schönhage, A., 72, 102
Scholtz, R., 508                                      group, 130
Schoof, R., 103                                       module, 359
secret sharing, 230                                   ring, 168
Semaev, I. A., 546                                 twin primes conjecture, 124
separating set, 544                                two-sided error, 304
sequence, xv
                                                   UFD, 451
                                                   ultimately periodic sequence, 98
580                                   Index

Umans, C., 485
union bound, 210
unique factorization

   in a Euclidean domain, 454
   in a PID, 455
  in D[X ], 459
  in F [X ], 430
   in Z, 2
unique factorization domain, 451
unit, 170
universal hash functions, 252

Vandermonde matrix, 385
Vanstone, S., 103
variance, 235
vector space, 370

Walfisz, A., 124
Wang, P., 103
Wang, Y., 340
Weber, D., 420
Wegman, N. M., 275
Weilert, A., 464
Welch, L., 508
well-behaved complexity function, 69
Wiedemann, D., 508
Wiener, M., 103, 341
Wright, E. M., 103, 124, 125

Yun, D. Y. Y., 546

Z, xv
Zassenhaus, H., 546
zero divisor, 171
zero element, 129
zero matrix, 378
zero-sided error, 304
zeta function of Riemann, 118
Zuckerman, H., 357
Zuckermann, D., 276
