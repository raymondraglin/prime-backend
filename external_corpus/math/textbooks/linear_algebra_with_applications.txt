Linear Algebra
with
Applications

W. Keith Nicholson

2023-A-D
WITH OPEN TEXTS

             Open Texts and Editorial                        Support

Digital access to our high-quality      Access to our in-house support team
texts is entirely FREE! All content is  is available 7 days/week to provide
reviewed and updated regularly by       prompt resolution to both student and
subject matter experts, and the texts   instructor inquiries. In addition, we
are adaptable; custom editions can      work one-on-one with instructors to
be produced by the Vretta-Lyryx         provide a comprehensive system, cus-
editorial team for those adopting       tomized for their course. This can
Vretta-Lyryx assessment. Access to      include managing multiple sections,
the original source files is also open  assistance with preparing online
to anyone!                              examinations, help with LMS integra-
                                        tion, and much more!

Online Assessment                       Instructor Supplements

Vretta-Lyryx has been developing        Additional instructor resources are
online formative and summative          also available to users. Product depen-
assessment (homework and exams)         dent, these include: full sets of adapt-
for more than 20 years combined, and    able slides, solutions manuals, and
as with the textbook content,           multiple choice question banks with an
questions and problems are regularly    exam building tool.
reviewed and updated. Students
receive immediate personalized                  Contact Vretta-Lyryx Today!
feedback on their mistakes to guide                                   info@lyryx.com
their learning. Student grade reports
and performance statistics are also
provided, and full LMS integration,
including gradebook synchronization,
is available.
Linear Algebra with Applications

                                                   Open Edition

Distribution Version                               -- Revision A-D

AUTHOR

        W. Keith Nicholson, University of Calgary

CONTRIBUTIONS

        Tim Alderson, University of New Brunswick
        Daniel Brinkman, San Jose State University
        Karl-Dieer Crisman, Gordon College
        Benjamin E. Dozier, Cornell University
        Jérôme Fortier, McGill University
        David S. Gerstl, Farmingdale State College
        Robert Gross, Boston College
        Mark Fels, Utah State University
        Chris Sangwin, University of Edinburgh
        Alistair Savage, University of Ottawa
        Csilla Tamas, Langara College
        Vretta-Lyryx

Be a champion of OER!

Contribute suggestions for improvements, new content, or errata:

    · A new topic

    · A new example

    · An interesting new question

    · Any other suggestions to improve the material

Contact Vretta-Lyryx at info@lyryx.com with your ideas.

Creative Commons License (CC BY-NC-SA): This text, including the art and illustrations, are available under
the Creative Commons license (CC BY-NC-SA), allowing anyone to reuse, revise, remix and redistribute the
text.

To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-sa/ . /
             Linear Algebra with Applications

             Open Edition

             by W. Keith Nicholson -- Distribution Version                             -- Revision A-D

Attribution

To redistribute all of this book in its original form, please follow the guide below:

The front matter of the text should include a "License" page that includes the following statement.

   This text is Linear Algebra with Applications by W. Keith Nicholson and Vretta-Lyryx. View the text for free at
   https://lyryx.com/linear-algebra-applications/

To redistribute part of this book in its original form, please follow the guide below. Clearly indicate which content has been
redistributed

The front matter of the text should include a "License" page that includes the following statement.

    This text includes the following content from Linear Algebra with Applications by W. Keith Nicholson and
    Vretta-Lyryx Inc. View the entire text for free at https://lyryx.com/linear-algebra-applications/.

        <List of content from the original text>

The following must also be included at the beginning of the applicable content. Please clearly indicate which content has
been redistributed from the Vretta-Lyryx text.

    This chapter is redistributed from the original Linear Algebra with Applications by W. Keith Nicholson and
    Vretta-Lyryx Inc. View the original text for free at https://lyryx.com/linear-algebra-applications/.

To adapt and redistribute all or part of this book in its original form, please follow the guide below. Clearly indicate which
content has been adapted/redistributed and summarize the changes made.

The front matter of the text should include a "License" page that includes the following statement.

    This text contains content adapted from the original Linear Algebra with Applications by W. Keith Nicholson and
    Vretta-Lyryx Inc. View the original text for free at https://lyryx.com/linear-algebra-applications/.

        <List of content and summary of changes>

The following must also be included at the beginning of the applicable content. Please clearly indicate which content has
been adapted from the Vretta-Lyryx text.

    This chapter was adapted from the original Linear Algebra with Applications by W. Keith Nicholson and Vretta-
    Lyryx Inc. View the original text for free at https://lyryx.com/linear-algebra-applications/.

Citation

Use the information below to create a citation:
        Author: W. Keith Nicholson
        Contributing Author: Claude Laflamme
        Publisher: Vretta-Lyryx Inc.
        Book title: Linear Algebra with Applications
        Book version: 2023-A-D
        Publication date: July 1, 2023
        Location: Toronto, Ontario, Canada
        Book URL: https://lyryx.com/linear-algebra-applications

For questions or comments please contact editorial@lyryx.com
          Linear Algebra with Applications

                                    Open Edition

          Revision History: Distribution Version  -- Revision A-D

Revision  Contributor    Significant Changes

2023 A    T. Alderson:   · An example of standard basis vectors in R3 added to Section 2.6.
2021 A    D. Brinkman:   · Section 3.3 has been split into three sections 3.4, 3.5 and 3.6.
          K-D. Crisman:  · The various definitions of standard basis have been clarified and consolidated.
                         · The notation for points and vectors has been clarified.
          D.E. Dozier:   · Theorem 6.3.2 has been clarified.
                         · The proof of Lemma 6.6.1 has been expanded.
          J. Fortier:    · The statement of Theorem 10.5.2 has been corrected.
          D. Gerstl:     · The statement of Theorem 5.2.67 has been corrected and clarified.
                         · The diagram for Exercise 1.5.2 has been corrected.
          R. Gross:      · Various typos have been corrected.
          D. Morris:     · Various typos have been corrected.
          C. Sangwin:    · Various typos have been corrected.
                         · Theorem 5.4.2 on rank and nullity has been expanded.
                         · Various typos have been corrected.

          · Front matter has been updated including cover, Vretta-Lyryx with Open Texts, copyright, and
            revision pages.

          · An attribution page has been added.

          · Typo and other minor fixes have been implemented throughout.

2019 A    · New Section on Singular Value Decomposition (8.6) is included.

          · New Example 2.3.2 and Theorem 2.2.4. Please note that this will impact the numbering of subse-
            quent examples and theorems in the relevant sections.

          · Section 2.2 is renamed as Matrix-Vector Multiplication.

          · Minor revisions made throughout, including fixing typos, adding exercises, expanding explana-
            tions, and other small edits.

2018 B    · Images have been converted to LaTeX tikz throughout.
          · Text has been converted to LaTeX with minor fixes throughout.
          · Full index has been implemented.

2018 A    · Text has been released with a Creative Commons license.
                                         Table of Contents

Table of Contents                                           iii

Foreward                                                    vii

Preface                                                     ix

1 Systems of Linear Equations                               1

1.1 Solutions and Elementary Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1

1.2 Gaussian Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8

1.3 Homogeneous Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

1.4 An Application to Network Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22

1.5 An Application to Electrical Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

1.6 An Application to Chemical Reactions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26

2 Matrix Algebra                                            29

2.1 Matrix Addition, Scalar Multiplication, and Transposition . . . . . . . . . . . . . . . . . . 30

2.2 Matrix-Vector Multiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

2.3 Matrix Multiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55

2.4 Matrix Inverses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67

2.5 Elementary Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79

2.6 Linear Transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86

2.7 LU-Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99

2.8 An Application to Input-Output Economic Models . . . . . . . . . . . . . . . . . . . . . 109

2.9 An Application to Markov Chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114

3 Determinants and Diagonalization                          123

3.1 The Cofactor Expansion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124

3.2 Determinants and Matrix Inverses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134

3.3 Eigenvalues and Eigenvectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146

3.4 Diagonalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153

3.5 Linear Dynamical Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158

3.6 An Application to Linear Recurrences . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166

3.7 An Application to Systems of Differential Equations . . . . . . . . . . . . . . . . . . . . 170

3.8 Proof of the Cofactor Expansion Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . 176

4 Vector Geometry                                           181

                                    iii
iv Table of Contents

4.1 Vectors and Lines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
4.2 Projections and Planes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
4.3 More on the Cross Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
4.4 Linear Operators on R3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212
4.5 An Application to Computer Graphics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219

5 Vector Space Rn         223

5.1 Subspaces and Spanning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223

5.2 Independence and Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230

5.3 Orthogonality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239

5.4 Rank of a Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246

5.5 Similarity and Diagonalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254

5.6 Best Approximation and Least Squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264

5.7 An Application to Correlation and Variance . . . . . . . . . . . . . . . . . . . . . . . . . 275

6 Vector Spaces           281

6.1 Examples and Basic Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282

6.2 Subspaces and Spanning Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290

6.3 Linear Independence and Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295

6.4 Finite Dimensional Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302

6.5 An Application to Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308

6.6 An Application to Differential Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . 313

7 Linear Transformations  319

7.1 Examples and Elementary Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320

7.2 Kernel and Image of a Linear Transformation . . . . . . . . . . . . . . . . . . . . . . . . 325

7.3 Isomorphisms and Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333

7.4 A Theorem about Differential Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . 341

7.5 More on Linear Recurrences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345

8 Orthogonality           355

8.1 Orthogonal Complements and Projections . . . . . . . . . . . . . . . . . . . . . . . . . . 355

8.2 Orthogonal Diagonalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363

8.3 Positive Definite Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371

8.4 QR-Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375

8.5 Computing Eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 379

8.6 The Singular Value Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383

8.6.1 Singular Value Decompositions . . . . . . . . . . . . . . . . . . . . . . . . . . . 384

8.6.2 Fundamental Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390

8.6.3 The Polar Decomposition of a Real Square Matrix . . . . . . . . . . . . . . . . . 393
                           v

      8.6.4 The Pseudoinverse of a Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395
8.7 Complex Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 398
8.8 An Application to Linear Codes over Finite Fields . . . . . . . . . . . . . . . . . . . . . . 408
8.9 An Application to Quadratic Forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 422
8.10 An Application to Constrained Optimization . . . . . . . . . . . . . . . . . . . . . . . . . 432
8.11 An Application to Statistical Principal Component Analysis . . . . . . . . . . . . . . . . . 436

9 Change of Basis          439

9.1 The Matrix of a Linear Transformation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 440

9.2 Operators and Similarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447

9.3 Invariant Subspaces and Direct Sums . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 456

10 Inner Product Spaces    469

10.1 Inner Products and Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 469

10.2 Orthogonal Sets of Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 477

10.3 Orthogonal Diagonalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 484

10.4 Isometries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 490

10.5 An Application to Fourier Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . 501

11 Canonical Forms         507

11.1 Block Triangular Form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 508

11.2 The Jordan Canonical Form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 515

A Complex Numbers          521

B Proofs                   535

C Mathematical Induction   541

D Polynomials              547

Selected Exercise Answers  551

Index                      553
                                                                      Foreward

Mathematics education at the beginning university level is closely tied to the traditional publishers. In my
opinion, it gives them too much control of both cost and content. The main goal of most publishers is
profit, and the result has been a sales-driven business model as opposed to a pedagogical one. This results
in frequent new "editions" of textbooks motivated largely to reduce the sale of used books rather than to
update content quality. It also introduces copyright restrictions which stifle the creation and use of new
pedagogical methods and materials. The overall result is high cost textbooks which may not meet the
evolving educational needs of instructors and students.

    To be fair, publishers do try to produce material that reflects new trends. But their goal is to sell books
and not necessarily to create tools for student success in mathematics education. Sadly, this has led to
a model where the primary choice for adapting to (or initiating) curriculum change is to find a different
commercial textbook. My editor once said that the text that is adopted is often everyone's third choice.

    Of course instructors can produce and maintain their own lecture notes, and have done so for years,
but this remains an onerous task and difficult for others to benefit. The publishing industry arose from the
need to provide authors with copy-editing, editorial, and marketing services, as well as extensive reviews
of prospective customers to ascertain market trends and content updates. These are necessary skills and
services that the industry continues to offer.

    Authors of open educational resources (OER) including (but not limited to) textbooks and lecture
notes, cannot afford this on their own. But they do have two great advantages: The cost to students is
significantly lower, and open licenses return content control to instructors. Through editable file formats
and open licenses, OER can be developed, maintained, reviewed, edited, and improved by a variety of
contributors. Instructors can now respond to curriculum change by revising and reordering material to
create content that meets the needs of their students. While editorial and quality control remain daunting
tasks, great strides have been made in addressing the issues of accessibility, affordability and adaptability
of the material.

    For all the above reasons I have decided to release my text under an open license, even though it was
published for many years through a traditional publisher.

    However supporting students and instructors in a typical first year College or University classroom
requires much more than a textbook. Thus, while anyone is welcome to use the distributed text at no
cost, I also decided to work closely with colleagues at the University of Calgary and help create Lyryx
Learning almost 20 years ago. The original idea was to develop quality but affordable formative online
assessment and other educational software to assist students. Revenues are then used to sustain the project
including editorial for the open textbook. Lyryx is now part of Vretta-Lyryx, and they continue working
with authors, contributors, and reviewers to ensure instructors need not sacrifice quality and rigour when
adopting an open text.

    I believe this is the right direction for mathematical publishing going forward, and I look forward to
being a part of how this new approach develops.

W. Keith Nicholson, Author
University of Calgary

                                                              vii
                                                                         Preface

This textbook is an introduction to the ideas and techniques of linear algebra for first- or second-year
students with a working knowledge of high school algebra. The contents have enough flexibility to present
a traditional introduction to the subject, or to allow for a more applied course. Chapters 1-4 contain a one-
semester course for beginners whereas Chapters 5-9 contain a second semester course (see the Suggested
Course Outlines below). The text is primarily about real linear algebra with complex numbers being
mentioned when appropriate (reviewed in Appendix A). Overall, the aim of the text is to achieve a balance
among computational skills, theory, and applications of linear algebra. Calculus is not a prerequisite;
places where it is mentioned may be omitted.

    As a rule, students of linear algebra learn by studying examples and solving problems. Accordingly,
the book contains a variety of exercises (over 1200, many with multiple parts), ordered as to their difficulty.
In addition, more than 375 solved examples are included in the text, many of which are computational in
nature. The examples are also used to motivate (and illustrate) concepts and theorems, carrying the student
from concrete to abstract. While the treatment is rigorous, proofs are presented at a level appropriate to
the student and may be omitted with no loss of continuity. As a result, the book can be used to give a
course that emphasizes computation and examples, or to give a more theoretical treatment (some longer
proofs are deferred to the end of the Section).

    Linear Algebra has application to the natural sciences, engineering, management, and the social sci-
ences as well as mathematics. Consequently, 18 optional "applications" sections are included in the text
introducing topics as diverse as electrical networks, economic models, Markov chains, linear recurrences,
systems of differential equations, and linear codes over finite fields. Additionally some applications (for
example linear dynamical systems, and directed graphs) are introduced in context. The applications sec-
tions appear at the end of the relevant chapters to encourage students to browse.

SUGGESTED COURSE OUTLINES

This text includes the basis for a two-semester course in linear algebra.

    · Chapters 1-4 provide a standard one-semester course of 35 lectures, including linear equations, ma-
       trix algebra, determinants, diagonalization, and geometric vectors, with applications as time permits.
       At Calgary, we cover Sections 1.1-1.3, 2.1-2.6, 3.1-3.3, and 4.1-4.4 and the course is taken by all
       science and engineering students in their first semester. Prerequisites include a working knowledge
       of high school algebra (algebraic manipulations and some familiarity with polynomials); calculus is
       not required.

    · Chapters 5-9 contain a second semester course including Rn, abstract vector spaces, linear trans-
       formations (and their matrices), orthogonality, complex matrices (up to the spectral theorem) and
       applications. There is more material here than can be covered in one semester, and at Calgary we
       cover Sections 5.1-5.5, 6.1-6.4, 7.1-7.3, 8.1-8.7, and 9.1-9.3 with a couple of applications as time
       permits.

    · Chapter 5 is a "bridging" chapter that introduces concepts like spanning, independence, and basis
       in the concrete setting of Rn, before venturing into the abstract in Chapter 6. The duplication is

                                                              ix
x Preface

balanced by the value of reviewing these notions, and it enables the student to focus in Chapter 6
on the new idea of an abstract system. Moreover, Chapter 5 completes the discussion of rank and
diagonalization from earlier chapters, and includes a brief introduction to orthogonality in Rn, which
creates the possibility of a one-semester, matrix-oriented course covering Chapter 1-5 for students
not wanting to study the abstract theory.

CHAPTER DEPENDENCIES

The following chart suggests how the material introduced in each chapter draws on concepts covered in
certain earlier chapters. A solid arrow means that ready assimilation of ideas and techniques presented
in the later chapter depends on familiarity with the earlier chapter. A broken arrow indicates that some
reference to the earlier chapter is made but the chapter need not be covered.

                        Chapter 1: Systems of Linear Equations

                        Chapter 2: Matrix Algebra

           Chapter 3: Determinants and Diagonalization  Chapter 4: Vector Geometry

                        Chapter 5: The Vector Space Rn

                        Chapter 6: Vector Spaces

           Chapter 7: Linear Transformations            Chapter 8: Orthogonality

                        Chapter 9: Change of Basis

           Chapter 10: Inner Product Spaces             Chapter 11: Canonical Forms

HIGHLIGHTS OF THE TEXT

· Two-stage definition of matrix multiplication. First, in Section 2.2 matrix-vector products are
  introduced naturally by viewing the left side of a system of linear equations as a product. Second,
  matrix-matrix products are defined in Section 2.3 by taking the columns of a product AB to be A
  times the corresponding columns of B. This is motivated by viewing the matrix product as compo-
  sition of maps (see next item). This works well pedagogically and the usual dot-product definition
  follows easily. As a bonus, the proof of associativity of matrix multiplication now takes four lines.

· Matrices as transformations. Matrix-column multiplications are viewed (in Section 2.2) as trans-
  formations Rn  Rm. These maps are then used to describe simple geometric reflections and rota-
  tions in R2 as well as systems of linear equations.

· Early linear transformations. It has been said that vector spaces exist so that linear transformations
  can act on them--consequently these maps are a recurring theme in the text. Motivated by the matrix
  transformations introduced earlier, linear transformations Rn  Rm are defined in Section 2.6, their
  standard matrices are derived, and they are then used to describe rotations, reflections, projections,
  and other operators on R2.
                                                                                                                        xi

· Early diagonalization. As requested by engineers and scientists, this important technique is pre-
  sented in the first term using only determinants and matrix inverses (before defining independence
  and dimension). Applications to population growth and linear recurrences are given.

· Early dynamical systems. These are introduced in Chapter 3, and lead (via diagonalization) to
  applications like the possible extinction of species. Beginning students in science and engineering
  can relate to this because they can see (often for the first time) the relevance of the subject to the real
  world.

· Bridging chapter. Chapter 5 lets students deal with tough concepts (like independence, spanning,
  and basis) in the concrete setting of Rn before having to cope with abstract vector spaces in Chap-
  ter 6.

· Examples. The text contains over 375 worked examples, which present the main techniques of the
  subject, illustrate the central ideas, and are keyed to the exercises in each section.

· Exercises. The text contains a variety of exercises (nearly 1175, many with multiple parts), starting
  with computational problems and gradually progressing to more theoretical exercises. Select solu-
  tions are available at the end of the book or in the Student Solution Manual. There is a complete
  Solution Manual is available for instructors.

· Applications. There are optional applications at the end of most chapters (see the list below).
  While some are presented in the course of the text, most appear at the end of the relevant chapter to
  encourage students to browse.

· Appendices. Because complex numbers are needed in the text, they are described in Appendix A,
  which includes the polar form and roots of unity. Methods of proofs are discussed in Appendix B,
  followed by mathematical induction in Appendix C. A brief discussion of polynomials is included
  in Appendix D. All these topics are presented at the high-school level.

· Self-Study. This text is self-contained and therefore is suitable for self-study.

· Rigour. Proofs are presented as clearly as possible (some at the end of the section), but they are
  optional and the instructor can choose how much he or she wants to prove. However the proofs are
  there, so this text is more rigorous than most. Linear algebra provides one of the better venues where
  students begin to think logically and argue concisely. To this end, there are exercises that ask the
  student to "show" some simple implication, and others that ask her or him to either prove a given
  statement or give a counterexample. I personally present a few proofs in the first semester course
  and more in the second (see the Suggested Course Outlines).

· Major Theorems. Several major results are presented in the book. Examples: Uniqueness of the
  reduced row-echelon form; the cofactor expansion for determinants; the Cayley-Hamilton theorem;
  the Jordan canonical form; Schur's theorem on block triangular form; the principal axes and spectral
  theorems; and others. Proofs are included because the stronger students should at least be aware of
  what is involved.
xii Preface

CHAPTER SUMMARIES

Chapter : Systems of Linear Equations.

A standard treatment of gaussian elimination is given. The rank of a matrix is introduced via the row-
echelon form, and solutions to a homogeneous system are presented as linear combinations of basic solu-
tions. Applications to network flows, electrical networks, and chemical reactions are provided.

Chapter : Matrix Algebra.

After a traditional look at matrix addition, scalar multiplication, and transposition in Section 2.1, matrix-

vector multiplication is introduced in Section 2.2 by viewing the left side of a system of linear equations

as the product Ax of the coefficient matrix A with the column x of variables. The usual dot-product

definition of a matrix-vector multiplication follows. Section 2.2 ends by viewing an m × n matrix A as a
transformation Rn  Rm. This is illustrated for R2  R2 by describing reflection in the x axis, rotation of
R2           ,
    through     shears,  and  so  on.
             2

    In Section 2.3, the product of matrices A and B is defined by AB = Ab1 Ab2 · · · Abn , where

the bi are the columns of B. A routine computation shows that this is the matrix of the transformation B

followed by A. This observation is used frequently throughout the book, and leads to simple, conceptual

proofs of the basic axioms of matrix algebra. Note that linearity is not required--all that is needed is some

basic properties of matrix-vector multiplication developed in Section 2.2. Thus the usual arcane definition

of matrix multiplication is split into two well motivated parts, each an important aspect of matrix algebra.

Of course, this has the pedagogical advantage that the conceptual power of geometry can be invoked to

illuminate and clarify algebraic techniques and definitions.

    In Section 2.4 and 2.5 matrix inverses are characterized, their geometrical meaning is explored, and
block multiplication is introduced, emphasizing those cases needed later in the book. Elementary ma-
trices are discussed, and the Smith normal form is derived. Then in Section 2.6, linear transformations
Rn  Rm are defined and shown to be matrix transformations. The matrices of reflections, rotations, and
projections in the plane are determined. Finally, matrix multiplication is related to directed graphs, matrix
LU-factorization is introduced, and applications to economic models and Markov chains are presented.

Chapter : Determinants and Diagonalization.

The cofactor expansion is stated (proved by induction later) and used to define determinants inductively
and to deduce the basic rules. The product and adjugate theorems are proved. Then the diagonalization
algorithm is presented (motivated by an example about the possible extinction of a species of birds). As
requested by our Engineering Faculty, this is done earlier than in most texts because it requires only deter-
minants and matrix inverses, avoiding any need for subspaces, independence and dimension. Eigenvectors
of a 2 × 2 matrix A are described geometrically (using the A-invariance of lines through the origin). Di-
agonalization is then used to study discrete linear dynamical systems and to discuss applications to linear
recurrences and systems of differential equations. A brief discussion of Google PageRank is included.
                                                                                                                            xiii

Chapter : Vector Geometry.

Vectors are presented intrinsically in terms of length and direction, and are related to matrices via coordi-
nates. Then vector operations are defined using matrices and shown to be the same as the corresponding
intrinsic definitions. Next, dot products and projections are introduced to solve problems about lines and
planes. This leads to the cross product. Then matrix transformations are introduced in R3, matrices of pro-
jections and reflections are derived, and areas and volumes are computed using determinants. The chapter
closes with an application to computer graphics.

Chapter : The Vector Space Rn.

Subspaces, spanning, independence, and dimensions are introduced in the context of Rn in the first two
sections. Orthogonal bases are introduced and used to derive the expansion theorem. The basic properties
of rank are presented and used to justify the definition given in Section 1.2. Then, after a rigorous study of
diagonalization, best approximation and least squares are discussed. The chapter closes with an application
to correlation and variance.

    This is a "bridging" chapter, easing the transition to abstract spaces. Concern about duplication with
Chapter 6 is mitigated by the fact that this is the most difficult part of the course and many students
welcome a repeat discussion of concepts like independence and spanning, albeit in the abstract setting.
In a different direction, Chapter 1-5 could serve as a solid introduction to linear algebra for students not
requiring abstract theory.

Chapter 6: Vector Spaces.

Building on the work on Rn in Chapter 5, the basic theory of abstract finite dimensional vector spaces is
developed emphasizing new examples like matrices, polynomials and functions. This is the first acquain-
tance most students have had with an abstract system, so not having to deal with spanning, independence
and dimension in the general context eases the transition to abstract thinking. Applications to polynomials
and to differential equations are included.

Chapter : Linear Transformations.

General linear transformations are introduced, motivated by many examples from geometry, matrix theory,
and calculus. Then kernels and images are defined, the dimension theorem is proved, and isomorphisms
are discussed. The chapter ends with an application to linear recurrences. A proof is included that the
order of a differential equation (with constant coefficients) equals the dimension of the space of solutions.

Chapter 8: Orthogonality.

The study of orthogonality in Rn, begun in Chapter 5, is continued. Orthogonal complements and pro-
jections are defined and used to study orthogonal diagonalization. This leads to the principal axes theo-
rem, the Cholesky factorization of a positive definite matrix, QR-factorization, and to a discussion of the
singular value decomposition, the polar form, and the pseudoinverse. The theory is extended to Cn in
Section 8.7 where hermitian and unitary matrices are discussed, culminating in Schur's theorem and the
spectral theorem. A short proof of the Cayley-Hamilton theorem is also presented. In Section 8.8 the field
Zp of integers modulo p is constructed informally for any prime p, and codes are discussed over any finite
field. The chapter concludes with applications to quadratic forms, constrained optimization, and statistical
principal component analysis.
xiv Preface

Chapter : Change of Basis.

The matrix of general linear transformation is defined and studied. In the case of an operator, the rela-
tionship between basis changes and similarity is revealed. This is illustrated by computing the matrix of a
rotation about a line through the origin in R3. Finally, invariant subspaces and direct sums are introduced,
related to similarity, and (as an example) used to show that every involution is similar to a diagonal matrix
with diagonal entries ±1.

Chapter : Inner Product Spaces.

General inner products are introduced and distance, norms, and the Cauchy-Schwarz inequality are dis-
cussed. The Gram-Schmidt algorithm is presented, projections are defined and the approximation theorem
is proved (with an application to Fourier approximation). Finally, isometries are characterized, and dis-
tance preserving operators are shown to be composites of a translations and isometries.

Chapter : Canonical Forms.

The work in Chapter 9 is continued. Invariant subspaces and direct sums are used to derive the block
triangular form. That, in turn, is used to give a compact proof of the Jordan canonical form. Of course the
level is higher.

Appendices

In Appendix A, complex arithmetic is developed far enough to find nth roots. In Appendix B, methods of
proof are discussed, while Appendix C presents mathematical induction. Finally, Appendix D describes
the properties of polynomials in elementary terms.

LIST OF APPLICATIONS

    · Network Flow (Section 1.4)

    · Electrical Networks (Section 1.5)

    · Chemical Reactions (Section 1.6)

    · Directed Graphs (in Section 2.3)

    · Input-Output Economic Models (Section 2.8)

    · Markov Chains (Section 2.9)

    · Polynomial Interpolation (in Section 3.2)

    · Population Growth (Examples 3.3.1 and 3.5.1, Section 3.3)

    · Google PageRank (in Section 3.3)

    · Linear Recurrences (Section 3.4; see also Section 7.5)

    · Systems of Differential Equations (Section 3.5)

    · Computer Graphics (Section 4.5)
                                                                                                                             xv

    · Least Squares Approximation (in Section 5.6)
    · Correlation and Variance (Section 5.7)
    · Polynomials (Section 6.5)
    · Differential Equations (Section 6.6)
    · Linear Recurrences (Section 7.5)
    · Error Correcting Codes (Section 8.8)
    · Quadratic Forms (Section 8.9)
    · Constrained Optimization (Section 8.10)
    · Statistical Principal Component Analysis (Section 8.11)
    · Fourier Approximation (Section 10.5)

ACKNOWLEDGMENTS

It is also a pleasure to recognize the contributions of several people over the many years all the way since
the early days of this text. Now that the text has an open license, we have a much more fluid and powerful
mechanism to incorporate comments and suggestions.

    The editorial group at Vretta-Lyryx invites instructors and students to contribute to the text, and we
will post revisions and credits in a separate revision page.

    W. Keith Nicholson
         University of Calgary
Chapter 1

              Systems of Linear Equations

 . Solutions and Elementary Operations

Practical problems in many fields of study--such as biology, business, chemistry, computer science, eco-
nomics, electronics, engineering, physics and the social sciences--can often be reduced to solving a sys-
tem of linear equations. Linear algebra arose from attempts to find systematic methods for solving these
systems, so it is natural to begin this book by studying linear equations.

    If a, b, and c are real numbers, the graph of an equation of the form

                                                        ax + by = c

is a straight line (if a and b are not both zero), so such an equation is called a linear equation in the
variables x and y. However, it is often convenient to write the variables as x1, x2, . . . , xn, particularly
when more than two variables are involved. An equation of the form

                                              a1x1 + a2x2 + · · · + anxn = b

is called a linear equation in the n variables x1, x2, . . . , xn. Here a1, a2, . . . , an denote real numbers
(called the coefficients of x1, x2, . . . , xn, respectively) and b is also a number (called the constant term
of the equation). A finite collection of linear equations in the variables x1, x2, . . . , xn is called a system of
linear equations in these variables. Hence,

                                                   2x1 - 3x2 + 5x3 = 7

is a linear equation; the coefficients of x1, x2, and x3 are 2, -3, and 5, and the constant term is 7. Note that
each variable in a linear equation occurs to the first power only.

    Given a linear equation a1x1 + a2x2 + · · · + anxn = b, a sequence s1, s2, . . . , sn of n numbers is called
a solution to the equation if

                                               a1s1 + a2s2 + · · · + ansn = b
that is, if the equation is satisfied when the substitutions x1 = s1, x2 = s2, . . . , xn = sn are made. A
sequence of numbers is called a solution to a system of equations if it is a solution to every equation in
the system.

    For example, x = -2, y = 5, z = 0 and x = 0, y = 4, z = -1 are both solutions to the system

                                                        x+y+ z=3
                                                      2x + y + 3z = 1

A system may have no solution at all, or it may have a unique solution, or it may have an infinite family of
solutions. For instance, the system x + y = 2, x + y = 3 has no solution because the sum of two numbers
cannot be 2 and 3 simultaneously. A system that has no solution is called inconsistent; a system with at
least one solution is called consistent. The system in the following example has infinitely many solutions.

           1
Systems of Linear Equations

Example 1.1.1
Show that, for arbitrary values of s and t,

                                                    x1 = t - s + 1
                                                    x2 = t + s + 2
                                                    x3 = s
                                                    x4 = t

is a solution to the system

                                           x1 - 2x2 +3x3 +x4 = -3
                                          2x1 - x2 +3x3 -x4 = 0

Solution. Simply substitute these values of x1, x2, x3, and x4 in each equation.

                      x1 - 2x2 + 3x3 + x4 = (t - s + 1) - 2(t + s + 2) + 3s + t = -3
                      2x1 - x2 + 3x3 - x4 = 2(t - s + 1) - (t + s + 2) + 3s - t = 0

Because both equations are satisfied, it is a solution for all choices of s and t.

    The quantities s and t in Example 1.1.1 are called parameters, and the set of solutions, described in

this way, is said to be given in parametric form and is called the general solution to the system. It turns

out that the solutions to every system of equations (if there are solutions) can be given in parametric form

(that is, the variables x1, x2, . . . are given in terms of new independent variables s, t, etc.). The following
example shows how this happens in the simplest systems where only one equation is present.

Example 1.1.2
Describe all solutions to 3x - y + 2z = 6 in parametric form.

Solution. Solving the equation for y in terms of x and z, we get y = 3x + 2z - 6. If s and t are
arbitrary then, setting x = s, z = t, we get solutions

                                     x=s                     s and t arbitrary
                                     y = 3s + 2t - 6
                                     z=t

Of  course  we  could  have  solved  for  x:  x  =  1 (y  -  2z  +  6).  Then,  if  we  take  y  =  p,  z  =  q,  the

solutions are represented as follows:               3

                             x = 13 (p - 2q + 6)                 p and q arbitrary
                             y=p

                             z=q

The same family of solutions can "look" quite different!
                                                             . . Solutions and Elementary Operations

y                                  When only two variables are involved, the solutions to systems of lin-
                               ear equations can be described geometrically because the graph of a lin-
            x-y = 1            ear equation ax + by = c is a straight line if a and b are not both zero.
                               Moreover, a point P(s, t) with coordinates s and t lies on the line if and
   x+y = 3                     only if as + bt = c--that is when x = s, y = t is a solution to the equa-
                               tion. Hence the solutions to a system of linear equations correspond to the
   P(2, 1)                     points P(s, t) that lie on all the lines in question.

               x                   In particular, if the system consists of just one equation, there must
                               be infinitely many solutions because there are infinitely many points on a
 (a) Unique Solution           line. If the system has two equations, there are three possibilities for the
     (x = 2, y = 1)            corresponding straight lines:

y

          x+y = 4              1. The lines intersect at a single point. Then the system has a unique
                                  solution corresponding to that point.
          x+y = 2
                               2. The lines are parallel (and distinct) and so do not intersect. Then
                        x         the system has no solution.

       (b) No Solution         3. The lines are identical. Then the system has infinitely many
                                  solutions--one for each point on the (common) line.
  y
                                   These three situations are illustrated in Figure 1.1.1. In each case the
         -6x + 2y = -8         graphs of two specific lines are plotted and the corresponding equations are
              3x - y = 4       indicated. In the last case, the equations are 3x-y = 4 and -6x+2y = -8,
                               which have identical graphs.
                        x
                                   With three variables, the graph of an equation ax + by + cz = d can be
(c) Infinitely many solutions  shown to be a plane (see Section 4.2) and so again provides a "picture"
      (x = t, y = 3t - 4)      of the set of solutions. However, this graphical method has its limitations:
                               When more than three variables are involved, no physical image of the
      Figure 1.1.1             graphs (called hyperplanes) is possible. It is necessary to turn to a more
                               "algebraic" method of solution.

                                   Before describing the method, we introduce a concept that simplifies
                               the computations involved. Consider the following system

                               3x1 + 2x2 - x3 + x4 = -1

                               2x1  - x3 + 2x4 = 0

                               3x1 + x2 + 2x3 + 5x4 = 2

of three equations in four variables. The array of numbers1

                                3 2 -1 1 -1 
                                2 0 -1 2 0 

                                  31 25 2

occurring in the system is called the augmented matrix of the system. Each row of the matrix consists
of the coefficients of the variables (in order) from the corresponding equation, together with the constant

    1A rectangular array of numbers is called a matrix. Matrices will be discussed in more detail in Chapter 2.
     Systems of Linear Equations

term. For clarity, the constants are separated by a vertical line. The augmented matrix is just a different
way of describing the system of equations. The array of coefficients of the variables

                                                   3 2 -1 1 
                                                   2 0 -1 2 

                                                     31 25

                                                             -1 
is called the coefficient matrix of the system and  0  is called the constant matrix of the system.

                                                                  2

Elementary Operations

The algebraic method for solving systems of linear equations is described as follows. Two such systems
are said to be equivalent if they have the same set of solutions. A system is solved by writing a series of
systems, one after the other, each equivalent to the previous system. Each of these systems has the same
set of solutions as the original one; the aim is to end up with a system that is easy to solve. Each system
in the series is obtained from the preceding system by a simple manipulation chosen so that it does not
change the set of solutions.

    As an illustration, we solve the system x + 2y = -2, 2x + y = 7 in this manner. At each stage, the
corresponding augmented matrix is displayed. The original system is

                                           x + 2y = -2                  1 2 -2
                                          2x + y = 7                    21 7

First, subtract twice the first equation from the second. The resulting system is

                                          x + 2y = -2            1 2 -2
                                           - 3y = 11             0 -3 11

which  is  equivalent  to  the  original  (see  Theorem        1.1.1).  At  this  stage  we  obtain  y  =  - 11   by  multiplying

the  second  equation  by  -1.   The  result  is  the  equivalent  system                                      3

                              3

                                          x + 2y = -2              1 2 -2
                                                y = - 11           0 1 - 11

                                                            3                     3

Finally, we subtract twice the second equation from the first to get another equivalent system.

                                          x = 16                            16 
                                                                  10
                                                    3                       3

                                          y = - 11               0 1 - 11            

                                                    3                             3

Now this system is easy to solve! And because it is equivalent to the original system, it provides the
solution to that system.

    Observe that, at each stage, a certain operation is performed on the system (and thus on the augmented
matrix) to produce an equivalent system.
                                                                          . . Solutions and Elementary Operations

   Definition 1.1 Elementary Operations
   The following operations, called elementary operations, can routinely be performed on systems
   of linear equations to produce equivalent systems.

        I. Interchange two equations.

       II. Multiply one equation by a nonzero number.

      III. Add a multiple of one equation to a different equation.

   Theorem 1.1.1
   Suppose that a sequence of elementary operations is performed on a system of linear equations.
   Then the resulting system has the same set of solutions as the original, so the two systems are
   equivalent.

The proof is given at the end of this section.
    Elementary operations performed on a system of equations produce corresponding manipulations of

the rows of the augmented matrix. Thus, multiplying a row of a matrix by a number k means multiplying
every entry of the row by k. Adding one row to another row means adding each entry of that row to the
corresponding entry of the other row. Subtracting two rows is done similarly. Note that we regard two
rows as equal when corresponding entries are the same.

    In hand calculations (and in computer programs) we manipulate the rows of the augmented matrix
rather than the equations. For this reason we restate these elementary operations for matrices.

   Definition 1.2 Elementary Row Operations
   The following are called elementary row operations on a matrix.

        I. Interchange two rows.

       II. Multiply one row by a nonzero number.

      III. Add a multiple of one row to a different row.

    In the illustration above, a series of such operations led to a matrix of the form
                                                           10
                                                           01

where the asterisks represent arbitrary numbers. In the case of three equations in three variables, the goal
is to produce a matrix of the form

                                                     1 0 0 
                                                     0 1 0 

                                                         001
This does not always happen, as we will see in the next section. Here is an example in which it does
happen.
6 Systems of Linear Equations

Example 1.1.3

Find all solutions to the following system of equations.

                                                 3x + 4y + z = 1
                                                 2x + 3y = 0
                                                 4x + 3y - z = -2

Solution. The augmented matrix of the original system is

                                                   3 4 1 1
                                                   2 3 0 0

                                                      4 3 -1 -2

To  create  a  1  in  the  upper  left  corner  we  could  multiply  row  1  through  by  1.  However,  the  1  can  be

                                                                                          3
obtained without introducing fractions by subtracting row 2 from row 1. The result is

                                                   1 1 1 1
                                                   2 3 0 0

                                                      4 3 -1 -2

The upper left 1 is now used to "clean up" the first column, that is create zeros in the other
positions in that column. First subtract 2 times row 1 from row 2 to obtain

                                                   1 1 1 1
                                                    0 1 -2 -2 

                                                      4 3 -1 -2

Next subtract 4 times row 1 from row 3. The result is

                                              1 1 1 1
                                               0 1 -2 -2 

                                                 0 -1 -5 -6

This completes the work on column 1. We now use the 1 in the second position of the second row
to clean up the second column by subtracting row 2 from row 1 and then adding row 2 to row 3.
For convenience, both row operations are done in one step. The result is

                                                   1 0 3 3
                                                    0 1 -2 -2 

                                                      0 0 -7 -8

Note that the last two manipulations did not affect the first column (the second row has a zero

there), so our previous effort there has not been undermined. Finally we clean up the third column.
                                         -1
Begin  by   multiplying    row    3  by        to  obtain
                                            7

                                                   1 0 3 3

                                                    0 1 -2 -2 
                                                                     8
                                                    00     1         7
                                                                     . . Solutions and Elementary Operations

Now subtract 3 times row 3 from row 1, and then add 2 times row 3 to row 2 to get

                                                               -3    

                                                1     0   0       7

                                                                     
                                                                  2
                                           0 1 0 7 

                                                                  8  
                                                                  7
                                                001

The  corresponding  equations  are  x  =  -3,   y  =  2,  and  z  =  8,  which  give  the  (unique)  solution.

                                             7        7              7

    Every elementary row operation can be reversed by another elementary row operation of the same
type (called its inverse). To see how, we look at types I, II, and III separately:

Type I Interchanging two rows is reversed by interchanging them again.

Type II Multiplying a row by a nonzero number k is reversed by multiplying by 1/k.

Type III Adding k times row p to a different row q is reversed by adding -k times row p to row q
            (in the new matrix). Note that p = q is essential here.

    To illustrate the Type III situation, suppose there are four rows in the original matrix, denoted R1, R2,
R3, and R4, and that k times R2 is added to R3. Then the reverse operation adds -k times R2, to R3. The
following diagram illustrates the effect of doing the operation first and then the reverse:

                                                                                 
                    R1                 R1                            R1                    R1

      R3   R2    R3 + kR2   R2    (R3 + kR2) - kR2   R2  =  R3   R2 

                    R4                 R4                            R4                    R4

The existence of inverses for elementary row operations and hence for elementary operations on a system
of equations, gives:

Proof of Theorem 1.1.1. Suppose that a system of linear equations is transformed into a new system
by a sequence of elementary operations. Then every solution of the original system is automatically a
solution of the new system because adding equations, or multiplying an equation by a nonzero number,
always results in a valid equation. In the same way, each solution of the new system must be a solution
to the original system because the original system can be obtained from the new one by another series of
elementary operations (the inverses of the originals). It follows that the original and new systems have the
same solutions. This proves Theorem 1.1.1.
8 Systems of Linear Equations

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                        Engage Active Learning App!

                  Vretta-Lyryx Engage is an active learning app designed to increase
                 student engagement in reading linear algebra material. The content is
               "chunked" into small blocks, each with an interactive assessment activity

                                          to promote comprehension.

                Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

. Gaussian Elimination

The algebraic method introduced in the preceding section can be summarized as follows: Given a system

of linear equations, use a sequence of elementary row operations to carry the augmented matrix to a "nice"

matrix (meaning that the corresponding equations are easy to solve). In Example 1.1.3, this nice matrix

took the form                  1 0 0 

                               0 1 0 

                               001

The following definitions identify the nice matrices that arise in this process.

Definition 1.3 Row-Echelon Form (Reduced)
A matrix is said to be in row-echelon form (and will be called a row-echelon matrix) if it
satisfies the following three conditions:

   1. All zero rows (consisting entirely of zeros) are at the bottom.

   2. The first nonzero entry from the left in each nonzero row is a 1, called the leading 1 for that
       row.

   3. Each leading 1 is to the right of all leading 1s in the rows above it.
                                                                    . . Gaussian Elimination

A row-echelon matrix is said to be in reduced row-echelon form (and will be called a reduced
row-echelon matrix) if, in addition, it satisfies the following condition:

   4. Each leading 1 is the only nonzero entry in its column.

The row-echelon matrices have a "staircase" form, as indicated by the following example (the asterisks
indicate arbitrary numbers).

                                               0 1     
                                               0 0 0 1   
                                                0 0 0 0 1   
                                                0 0 0 0 0 0 1 

                                                  0000000

The leading 1s proceed "down and to the right" through the matrix. Entries above and to the right of the
leading 1s are arbitrary, but all entries below and to the left of them are zero. Hence, a matrix in row-
echelon form is in reduced form if, in addition, the entries directly above each leading 1 are all zero. Note
that a matrix in row-echelon form can, with a few more row operations, be carried to reduced form (use
row operations to create zeros above each leading one in succession, beginning from the right).

Example 1.2.1
The following matrices are in row-echelon form (for any choice of numbers in -positions).

1     0 1    1     1   
001   0 0 1   0 1    0 1  

        0000 0001 001

The following, on the other hand, are in reduced row-echelon form.

10    0 1 0   1 0  0  1 0 0 
001   0 0 1   0 1  0  0 1 0 

        0000 0001 001

The choice of the positions for the leading 1s determines the (reduced) row-echelon form (apart
from the numbers in -positions).

The importance of row-echelon matrices comes from the following theorem.

Theorem 1.2.1
Every matrix can be brought to (reduced) row-echelon form by a sequence of elementary row
operations.

    In fact we can give a step-by-step procedure for actually finding a row-echelon matrix. Observe that
while there are many sequences of row operations that will bring a matrix to row-echelon form, the one
we use is systematic and is easy to program on a computer. Note that the algorithm deals with matrices in
general, possibly with columns of zeros.
       Systems of Linear Equations

   Theorem: Gaussian2Algorithm3
           Step 1. If the matrix consists entirely of zeros, stop--it is already in row-echelon form.

           Step 2. Otherwise, find the first column from the left containing a nonzero entry (call it a),
           and move the row containing that entry to the top position.

           Step 3. Now multiply the new top row by 1/a to create a leading 1.

           Step 4. By subtracting multiples of that row from rows below it, make each entry below the
           leading 1 zero.

   This completes the first row, and all further row operations are carried out on the remaining rows.

           Step 5. Repeat steps 1-4 on the matrix consisting of the remaining rows.

   The process stops when either no rows remain at step 5 or the remaining rows consist entirely of
   zeros.

    Observe that the gaussian algorithm is recursive: When the first leading 1 has been obtained, the
procedure is repeated on the remaining rows of the matrix. This makes the algorithm easy to use on a
computer. Note that the solution to Example 1.1.3 did not use the gaussian algorithm as written because
the first leading 1 was not created by dividing row 1 by 3. The reason for this is that it avoids fractions.
However, the general pattern is clear: Create the leading 1s from left to right, using each of them in turn
to create zeros below it. Here are two more examples.

   Example 1.2.2
   Solve the following system of equations.

                                                    3x + y - 4z = -1
                                                      x + 10z = 5

                                                    4x + y + 6z = 1

   Solution. The corresponding augmented matrix is
                                                    3 1 -4 -1 
                                                    1 0 10 5 
                                                      41 6 1

   Create the first leading one by interchanging rows 1 and 2
                                                    1 0 10 5 
                                                    3 1 -4 -1 
                                                      41 6 1

    2Carl Friedrich Gauss (1777-1855) ranks with Archimedes and Newton as one of the three greatest mathematicians of all
time. He was a child prodigy and, at the age of 21, he gave the first proof that every polynomial has a complex root. In
1801 he published a timeless masterpiece, Disquisitiones Arithmeticae, in which he founded modern number theory. He went
on to make ground-breaking contributions to nearly every branch of mathematics, often well before others rediscovered and
published the results.

    3The algorithm was known to the ancient Chinese.
                                                                                       . . Gaussian Elimination

Now subtract 3 times row 1 from row 2, and subtract 4 times row 1 from row 3. The result is

                                              1 0 10 5 
                                              0 1 -34 -16 

                                                 0 1 -34 -19

Now subtract row 2 from row 3 to obtain

                                              1 0 10 5 
                                              0 1 -34 -16 

                                                 0 0 0 -3

This means that the following reduced system of equations

                                                 x + 10z = 5
                                                    y - 34z = -16
                                                           0 = -3

is equivalent to the original system. In other words, the two have the same solutions. But this last
system clearly has no solution (the last equation requires that x, y and z satisfy 0x + 0y + 0z = -3,
and no such numbers exist). Hence the original system has no solution.

Example 1.2.3
Solve the following system of equations.

                                          x1 - 2x2 - x3 + 3x4 = 1
                                          2x1 - 4x2 + x3           =5

                                          x1 - 2x2 + 2x3 - 3x4 = 4

Solution. The augmented matrix is

                                           1 -2 -1 3 1 
                                           2 -4 1 0 5 

                                             1 -2 2 -3 4

Subtracting twice row 1 from row 2 and subtracting row 1 from row 3 gives

                                           1 -2 -1 3 1 
                                           0 0 3 -6 3 

                                             0 0 3 -6 3

Now  subtract  row  2  from  row  3  and  multiply  row  2  by  1  to  get
                                                                3

                                           1 -2 -1 3 1 
                                           0 0 1 -2 1 

                                             0 0 0 00
       Systems of Linear Equations

   This is in row-echelon form, and we take it to reduced form by adding row 2 to row 1:

                                                  1 -2 0 1 2 
                                                  0 0 1 -2 1 

                                                    0 00 00

   The corresponding reduced system of equations is

                                                  x1 - 2x2 + x4 = 2
                                                             x3 - 2x4 = 1
                                                                      0=0

   The leading ones are in columns 1 and 3 here, so the corresponding variables x1 and x3 are called
   leading variables. Because the matrix is in reduced row-echelon form, these equations can be used
   to solve for the leading variables in terms of the nonleading variables x2 and x4. More precisely, in
   the present example we set x2 = s and x4 = t where s and t are arbitrary, so these equations become

                                          x1 - 2s + t = 2 and x3 - 2t = 1

   Finally the solutions are given by

                                                      x1 = 2 + 2s - t
                                                      x2 = s
                                                      x3 = 1 + 2t
                                                      x4 = t

   where s and t are arbitrary.

    The solution of Example 1.2.3 is typical of the general case. To solve a linear system, the augmented
matrix is carried to reduced row-echelon form, and the variables corresponding to the leading ones are
called leading variables. Because the matrix is in reduced form, each leading variable occurs in exactly
one equation, so that equation can be solved to give a formula for the leading variable in terms of the
nonleading variables. It is customary to call the nonleading variables "free" variables, and to label them
by new variables s, t, . . . , called parameters. Hence, as in Example 1.2.3, every variable xi is given by a
formula in terms of the parameters s and t. Moreover, every choice of these parameters leads to a solution
to the system, and every solution arises in this way. This procedure works in general, and has come to be
called

   Theorem: Gaussian Elimination
   To solve a system of linear equations proceed as follows:

       1. Carry the augmented matrix to a reduced row-echelon matrix using elementary row
           operations.

       2. If a row 0 0 0 · · · 0 1 occurs, the system is inconsistent.

       3. Otherwise, assign the nonleading variables (if any) as parameters, and use the equations
                                                     . . Gaussian Elimination

corresponding to the reduced row-echelon matrix to solve for the leading variables in terms
of the parameters.

    There is a variant of this procedure, wherein the augmented matrix is carried only to row-echelon form.
The nonleading variables are assigned as parameters as before. Then the last equation (corresponding to
the row-echelon form) is used to solve for the last leading variable in terms of the parameters. This last
leading variable is then substituted into all the preceding equations. Then, the second last equation yields
the second last leading variable, which is also substituted back. The process continues to give the general
solution. This procedure is called back-substitution. This procedure can be shown to be numerically
more efficient and so is important when solving very large systems.4

   Example 1.2.4

   Find a condition on the numbers a, b, and c such that the following system of equations is
   consistent. When that condition is satisfied, find all solutions (in terms of a, b, and c).

                                                      x1 + 3x2 + x3 = a
                                                    -x1 - 2x2 + x3 = b
                                                     3x1 + 7x2 - x3 = c

Solution. We use gaussian elimination except that now the augmented matrix

                              1 3 1 a
                              -1 -2 1 b 

                                  3 7 -1 c

has entries a, b, and c as well as known numbers. The first leading one is in place, so we create

zeros below it in column 1:  1 3 1 a 

                              0 1 2 a+b 

                             0 -2 -4 c - 3a

The second leading 1 has appeared, so use it to create zeros in the rest of column 2:

                              1 0 -5 -2a - 3b 
                              0 1 2 a+b 

                                0 0 0 c - a + 2b

Now the whole solution depends on the number c - a + 2b = c - (a - 2b). The last row
corresponds to an equation 0 = c - (a - 2b). If c = a - 2b, there is no solution (just as in Example
1.2.2). Hence:

The system is consistent if and only if c = a - 2b.

    4With n equations where n is large, gaussian elimination requires roughly n3/2 multiplications and divisions, whereas this
number is roughly n3/3 if back substitution is used.
Systems of Linear Equations

In this case the last matrix becomes

                                       1 0 -5 -2a - 3b 

                                       0 1 2 a+b 

                                      00 0           0

Thus, if c = a - 2b, taking x3 = t where t is a parameter gives the solutions

x1 = 5t - (2a + 3b) x2 = (a + b) - 2t x3 = t.

Rank

It can be proven that the reduced row-echelon form of a matrix A is uniquely determined by A. That is,
no matter which series of row operations is used to carry A to a reduced row-echelon matrix, the result
will always be the same matrix. (A proof is given at the end of Section 2.5.) By contrast, this is not
true for row-echelon matrices: Different series of row operations can carry the same matrix A to different
row-echelon matrices. Indeed, the matrix A = 1 -1 4 2 -1 2 can be carried (by one row operation) to
the row-echelon matrix 1 -1 4 0 1 -6 , and then by another row operation to the (reduced) row-echelon
matrix 1 0 -2 0 1 -6 . However, it is true that the number r of leading 1s must be the same in each of
these row-echelon matrices (this will be proved in Chapter 5). Hence, the number r depends only on A
and not on the way in which A is carried to row-echelon form.

   Definition 1.4 Rank of a Matrix

   The rank of matrix A is the number of leading 1s in any row-echelon matrix to which A can be
   carried by row operations.

Example 1.2.5

                                1 1 -1 4 
Compute the rank of A =  2 1 3 0 .

                                  0 1 -5 8

Solution. The reduction of A to row-echelon form is

 1 1 -1 4   1 1 -1 4   1 1 -1 4 

A =  2 1 3 0    0 -1 5 -8    0 1 -5 8 

0 1 -5 8                              0 1 -5 8                   00 00

Because this row-echelon matrix has two leading 1s, rank A = 2.

    Suppose that rank A = r, where A is a matrix with m rows and n columns. Then r  m because the
leading 1s lie in different rows, and r  n because the leading 1s lie in different columns. Moreover, the
                                                                                           . . Gaussian Elimination

rank has a useful application to equations. Recall that a system of linear equations is called consistent if it
has at least one solution.

   Theorem 1.2.2
   Suppose a system of m equations in n variables is consistent, and that the rank of the augmented
   matrix is r.

       1. The set of solutions involves exactly n - r parameters.
       2. If r < n, the system has infinitely many solutions.
       3. If r = n, the system has a unique solution.

Proof. The fact that the rank of the augmented matrix is r means there are exactly r leading variables, and
hence exactly n - r nonleading variables. These nonleading variables are all assigned as parameters in the
gaussian algorithm, so the set of solutions involves exactly n - r parameters. Hence if r < n, there is at
least one parameter, and so infinitely many solutions. If r = n, there are no parameters and so a unique
solution.

    Theorem 1.2.2 shows that, for any system of linear equations, exactly three possibilities exist:

   1. No solution. This occurs when a row 0 0 · · · 0 1 occurs in the row-echelon form. This is
       the case where the system is inconsistent.

   2. Unique solution. This occurs when every variable is a leading variable.

   3. Infinitely many solutions. This occurs when the system is consistent and there is at least one
       nonleading variable, so at least one parameter is involved.

   Example 1.2.6
   Suppose the matrix A in Example 1.2.5 is the augmented matrix of a system of m = 3 linear
   equations in n = 3 variables. As rank A = r = 2, the set of solutions will have n - r = 1 parameter.
   The reader can verify this fact directly.

    Many important problems involve linear inequalities rather than linear equations. For example, a
condition on the variables x and y might take the form of an inequality 2x - 5y  4 rather than an equality
2x - 5y = 4. There is a technique (called the simplex algorithm) for finding solutions to a system of such
inequalities that maximizes a function of the form p = ax + by where a and b are fixed constants.
6 Systems of Linear Equations

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

         Engage Active Learning App!

   Vretta-Lyryx Engage is an active learning app designed to increase
  student engagement in reading linear algebra material. The content is
"chunked" into small blocks, each with an interactive assessment activity

                           to promote comprehension.

 Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

 . Homogeneous Equations

A system of equations in the variables x1, x2, . . . , xn is called homogeneous if all the constant terms are
zero--that is, if each equation of the system has the form

                                              a1x1 + a2x2 + · · · + anxn = 0

Clearly x1 = 0, x2 = 0, . . . , xn = 0 is a solution to such a system; it is called the trivial solution. Any
solution in which at least one variable has a nonzero value is called a nontrivial solution. Our chief goal
in this section is to give a useful condition for a homogeneous system to have nontrivial solutions. The
following example is instructive.

Example 1.3.1
Show that the following homogeneous system has nontrivial solutions.

                               x1 - x2 + 2x3 - x4 = 0
                               2x1 + 2x2  + x4 = 0

                               3x1 + x2 + 2x3 - x4 = 0
                                                              . . Homogeneous Equations

Solution. The reduction of the augmented matrix to reduced row-echelon form is outlined below.

 1 -1 2 -1 0   1 -1 2 -1 0   1 0 1 0 0 

 2 2 0 1 0    0 4 -4 3 0    0 1 -1 0 0 

3 1 2 -1 0  0 4 -4 2 0                                        00 010

The leading variables are x1, x2, and x4, so x3 is assigned as a parameter--say x3 = t. Then the
general solution is x1 = -t, x2 = t, x3 = t, x4 = 0. Hence, taking t = 1 (say), we get a nontrivial
solution: x1 = -1, x2 = 1, x3 = 1, x4 = 0.

The existence of a nontrivial solution in Example 1.3.1 is ensured by the presence of a parameter in the
solution. This is due to the fact that there is a nonleading variable (x3 in this case). But there must be
a nonleading variable here because there are four variables and only three equations (and hence at most
three leading variables). This discussion generalizes to a proof of the following fundamental theorem.

   Theorem 1.3.1
   If a homogeneous system of linear equations has more variables than equations, then it has a
   nontrivial solution (in fact, infinitely many).

Proof. Suppose there are m equations in n variables where n > m, and let R denote the reduced row-echelon
form of the augmented matrix. If there are r leading variables, there are n - r nonleading variables, and so
n - r parameters. Hence, it suffices to show that r < n. But r  m because R has r leading 1s and m rows,
and m < n by hypothesis. So r  m < n, which gives r < n.

    Note that the converse of Theorem 1.3.1 is not true: if a homogeneous system has nontrivial solutions,
it need not have more variables than equations (the system x1 + x2 = 0, 2x1 + 2x2 = 0 has nontrivial
solutions but m = 2 = n.)

    Theorem 1.3.1 is very useful in applications. The next example provides an illustration from geometry.

Example 1.3.2

We call the graph of an equation ax2 + bxy + cy2 + dx + ey + f = 0 a conic if the numbers a, b, and
c are not all zero. Show that there is at least one conic through any five points in the plane that are
not all on a line.

Solution. Let the coordinates of the five points be (p1, q1), (p2, q2), (p3, q3), (p4, q4), and
(p5, q5). The graph of ax2 + bxy + cy2 + dx + ey + f = 0 passes through (pi, qi) if

a p2i       + bpiqi  +  cq  2  +  d  p  i  + eqi  +  f  =  0
                            i

This gives five equations, one for each i, linear in the six variables a, b, c, d, e, and f . Hence, there
is a nontrivial solution by Theorem 1.3.1. If a = b = c = 0, the five points all lie on the line with
equation dx + ey + f = 0, contrary to assumption. Hence, one of a, b, c is nonzero.
8 Systems of Linear Equations

Linear Combinations and Basic Solutions

As for rows, two columns are regarded as equal if they have the same number of entries and corresponding
entries are the same. Let x and y be columns with the same number of entries. As for elementary row
operations, their sum x + y is obtained by adding corresponding entries and, if k is a number, the scalar
product kx is defined by multiplying each entry of x by k. More precisely:

                                                    x1 + y1            
                  x1              y1                                      kx1

                 x2               y2                x2 + y2              kx2  
                                                                              
If x =  ..  and y =  ..  then x + y =  ..  and kx =  ..  .
               .               .                 .                     .

                 xn               yn                xn + yn              kxn

A sum of scalar multiples of several columns is called a linear combination of these columns. For
example, sx + ty is a linear combination of x and y for any choice of numbers s and t.

Example 1.3.3
If x = 3 -2 and y = -11 then 2x + 5y = 6 -4 + -55 = 11 .

Example 1.3.4

1 2                               3               0            1

Let x =  0  , y =  1  and z =  1 . If v =  -1  and w =  1 , determine whether v

1                    0                1          2                  1

and w are linear combinations of x, y and z.

Solution. For v, we must determine whether numbers r, s, and t exist such that v = rx + sy + tz,
that is, whether

                0   1   2   3   r + 2s + 3t 

                -1  = r  0  + s  1  + t  1  =  s + t 

                     2         1              0  1             r+t

Equating corresponding entries gives a system of linear equations r + 2s + 3t = 0, s + t = -1, and
r + t = 2 for r, s, and t. By gaussian elimination, the solution is r = 2 - k, s = -1 - k, and t = k
where k is a parameter. Taking k = 0, we see that v = 2x - y is a linear combination of x, y, and z.
Turning to w, we again look for r, s, and t such that w = rx + sy + tz; that is,

                  1   1   2   3   r + 2s + 3t 

                  1  = r 0 +s 1 +t 1  =  s+t 

                     1         1              0  1             r+t

leading to equations r + 2s + 3t = 1, s + t = 1, and r + t = 1 for real numbers r, s, and t. But this
time there is no solution as the reader can verify, so w is not a linear combination of x, y, and z.

    Our interest in linear combinations comes from the fact that they provide one of the best ways to
describe the general solution of a homogeneous system of linear equations. When solving such a system
                                                                                          . . Homogeneous Equations

                                                                                                      
                                                                                                         x1

                                                                          5                              x2    
                                                                                                               
with n variables x1, x2, . . . , xn, write the variables as a column matrix: x =  .. . The trivial solution
                                                                                                      .

                                                                                                         xn

               0

                  0
                  
is denoted 0 =  .. . As an illustration, the general solution in Example 1.3.1 is x1 = -t, x2 = t, x3 = t,
                  .

               0

and  x4  =   0, where  t  is  a  parameter,  and  we  would   now  express      this  by  saying         that  the  general  solution  is
              
         -t

         t  t , where t is arbitrary.
x=       0

     

    Now let x and y be two solutions to a homogeneous system with n variables. Then any linear combi-
nation sx + ty of these solutions turns out to be again a solution to the system. More generally:

               Any linear combination of solutions to a homogeneous system is again a solution.                              (1.1)

     In  fact, suppose    that a  typical  equation  in  the  system  is  a1x1  +  a2x2   +  ·  ·  ·  +  anxn  =  0,  and  suppose  that
                             
         x1               y1

         x2               y2  
                              
x =  .. , y =  ..  are solutions. Then a1x1 + a2x2 + · · · + anxn = 0 and a1y1 + a2y2 + · · · + anyn = 0.
     . .

         xn               yn

                          sx1 + ty1  

                          sx2 + ty2  
                                     
Hence sx + ty =  ..  is also a solution because
               .

                          sxn + tyn

           a1(sx1 + ty1) + a2(sx2 + ty2) + · · · + an(sxn + tyn)
                            = [a1(sx1) + a2(sx2) + · · · + an(sxn)] + [a1(ty1) + a2(ty2) + · · · + an(tyn)]
                            = s(a1x1 + a2x2 + · · · + anxn) + t(a1y1 + a2y2 + · · · + anyn)
                            = s(0) + t(0)
                            =0

A similar argument shows that Statement 1.1 is true for linear combinations of more than two solutions.
    The remarkable thing is that every solution to a homogeneous system is a linear combination of certain

particular solutions and, in fact, these solutions are easily computed using the gaussian algorithm. Here is
an example.

5The reason for using columns will be apparent later.
    Systems of Linear Equations

Example 1.3.5

Solve the homogeneous system with coefficient matrix

                                                  1 -2 3 -2 
                                           A =  -3 6 1 0 

                                                    -2 4 4 -2

Solution. The reduction of the augmented matrix to reduced form is

                                                                                           10   

                              1 -2 3 -2 0    1 -2 0 -                                      5

                              -3          61           0   0            0         0 1 -3 0 5
                                                                   

                                -2 4 4 -2 0                                                     

                                                                        0 00 00

so  the  solutions  are  x1  =  2s  +  1t,  x2  =  s,  x3  =  3t,  and  x4  =  t  by  gaussian  elimination.  Hence  we

                                       5                      5
can write the general solution x in the matrix form

                                                       1    1      2
                                                2s + 5t
                                x1
                                                                                      5
                              x2   s   1   0 
                      x=            =              3        = s         +t            3   = sx1 + tx2.
                                x3                 5t              0                  5  

                                x4                 t               0                  1

               2                    1

         1                             5
                                     0
Here x1 =            and x2 =          3     are particular solutions determined by the gaussian algorithm.
               0                            
                                       5

               0                       1

The solutions x1 and x2 in Example 1.3.5 are denoted as follows:

   Definition 1.5 Basic Solutions
   The gaussian algorithm systematically produces solutions to any homogeneous linear system,
   called basic solutions, one for every parameter.

Moreover, the algorithm gives a routine way to express every solution as a linear combination of basic
solutions as in Example 1.3.5, where the general solution x becomes

                                            1    2                          2              1
                                                              5
                                          1 0                           1                  0
                                                                               + 5t   1       
                                x=s         0     +t          3    =s       0              3  
                                                                 
                                                              5                               

                                            0              1                0              5

Hence by introducing a new parameter r = t/5 we can multiply the original basic solution x2 by 5 and so
eliminate fractions. For this reason:
                                                                                      . . Homogeneous Equations

       Convention:
       Any nonzero scalar multiple of a basic solution will still be called a basic solution.

    In the same way, the gaussian algorithm produces basic solutions to every homogeneous system, one
for each parameter (there are no basic solutions if the system has only the trivial solution). Moreover every
solution is given by the algorithm as a linear combination of these basic solutions (as in Example 1.3.5).
If A has rank r, Theorem 1.2.2 shows that there are exactly n - r parameters, and so n - r basic solutions.
This proves:

   Theorem 1.3.2
   Let A be an m × n matrix of rank r, and consider the homogeneous system in n variables with A as
   coefficient matrix. Then:

       1. The system has exactly n - r basic solutions, one for each parameter.

       2. Every solution is a linear combination of these basic solutions.

Example 1.3.6

Find basic solutions of the homogeneous system with coefficient matrix A, and express every
solution as a linear combination of the basic solutions, where

                                              1 -3 0 2 2 
                                        A =  3 -9 -1 0 7   -2 6 1 2 -5 

                                                 -3 9 2 6 -8

Solution. The reduction of the augmented matrix to reduced row-echelon form is

 1 -3 0 2 2 0   1 -3 0 2 2 0 

 3 -9 -1 0 7 0   -2 6 1 2 -5 0    0  0                        0 1 6 -1 0 
                                                              0 0 0 0 0 

-3 9 2 6 -8 0                                          0 000 00

so the general solution is x1 = 3r - 2s - 2t, x2 = r, x3 = -6s + t, x4 = s, and x5 = t where r, s, and
t are parameters. In matrix form this is

 x1   3r - 2s - 2t   3   -2   -2 

 x2                                    r       1  0  0

                                    -6s + t        
x=     x3                  =                   = r  0  + s  -6  + t    1
                                                                         
 x4                                    s       0  1  0

       x5                              t               0           0   1

Hence basic solutions are

                                  3            -2                 -2 

                                  1            0                  0

                                                                 
                           x1  =    0  ,  x2  =    -6  ,  x3  =    1
                                                                     
                                               1                 
                                  0                               0

                                    0              0               1
       Systems of Linear Equations

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

 . An Application to Network Flow

There are many types of problems that concern a network of conductors along which some sort of flow
is observed. Examples of these include an irrigation network and a network of streets or freeways. There
are often points in the system at which a net flow either enters or leaves the system. The basic principle
behind the analysis of such systems is that the total flow into the system must equal the total flow out. In
fact, we apply this principle at every junction in the system.

   Theorem: Junction Rule
   At each of the junctions in the network, the total flow into that junction must equal the total flow
   out.

This requirement gives a linear equation relating the flows in conductors emanating from the junction.

   Example 1.4.1
   A network of one-way streets is shown in the accompanying diagram. The rate of flow of cars into
   intersection A is 500 cars per hour, and 400 and 100 cars per hour emerge from B and C,
   respectively. Find the possible flows along each street.
   Solution. Suppose the flows along the streets are f1, f2, f3, f4, f5, and f6 cars per hour in the
   directions shown.
                                                                    . . An Application to Network Flow

Then, equating the flow in with the flow out at each intersection, we get

Intersection A                                     500 = f1 + f2 + f3
Intersection B                            f1 + f4 + f6 = 400
Intersection C
Intersection D                                 f3 + f5 = f6 + 100
                                                     f2 = f4 + f5

These give four equations in the six variables f1, f2, . . . , f6.

f1 + f2 + f3                              = 500

f1                                        + f4 + f6 = 400
                                          f3 + f5 - f6 = 100
    f2 - f4 - f5 = 0

The reduction of the augmented matrix is

 1 1 1 0 0 0 500   1 0 0 1 0 1 400 

 0 0 1 0 1 -1 100   1 0 0 1 0 1 400    0 0 1 0 1 -1 100   0 1 0 -1 -1 0 0 

0 1 0 -1 -1 0 0                           000 0 0 0 0

Hence, when we use f4, f5, and f6 as parameters, the general solution is

f1 = 400 - f4 - f6 f2 = f4 + f5 f3 = 100 - f5 + f6

This gives all solutions to the system of equations and hence all the possible flows.
Of course, not all these solutions may be acceptable in the real situation. For example, the flows
f1, f2, . . . , f6 are all positive in the present context (if one came out negative, it would mean traffic
flowed in the opposite direction). This imposes constraints on the flows: f1  0 and f3  0 become

f4 + f6  400 f5 - f6  100

Further constraints might be imposed by insisting on maximum values on the flow in each street.
       Systems of Linear Equations

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.
                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.
                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

 . An Application to Electrical Networks6

In an electrical network it is often necessary to find the current in amperes (A) flowing in various parts of
the network. These networks usually contain resistors that retard the current. The resistors are indicated
by a symbol ( ), and the resistance is measured in ohms (). Also, the current is increased at various
points by voltage sources (for example, a battery). The voltage of these sources is measured in volts (V),
and they are represented by the symbol ( ). We assume these voltage sources have no resistance. The
flow of current is governed by the following principles.

   Theorem: Ohm's Law
   The current I and the voltage drop V across a resistance R are related by the equation V = RI.

   Theorem: Kirchhoff's Laws
       1. (Junction Rule) The current flow into a junction equals the current flow out of that junction.
       2. (Circuit Rule) The algebraic sum of the voltage drops (due to resistances) around any closed
           circuit of the network must equal the sum of the voltage increases around the circuit.

    6This section is independent of Section 1.4
                                                 . . An Application to Electrical Networks

When applying rule 2, select a direction (clockwise or counterclockwise) around the closed circuit and
then consider all voltages and currents positive when in this direction and negative when in the opposite
direction. This is why the term algebraic sum is used in rule 2. Here is an example.

Example 1.5.1
Find the various currents in the circuit shown.

Solution.

                A 10           First apply the junction rule at junctions A, B, C, and D to obtain

                    I3                           Junction A       I1 = I2 + I3
    10 V                                         Junction B       I6 = I1 + I5
                                                 Junction C  I2 + I4 = I6
20 5 V                   20 V                    Junction D  I3 + I5 = I4

            I2           5 D
    B

I1         I6 C          I4 Note that these equations are not independent

5                              (in fact, the third is an easy consequence of the other three).
                   10 V        Next, the circuit rule insists that the sum of the voltage increases

                                    (due to the sources) around a closed circuit must equal the sum of
                         I5 the voltage drops (due to resistances). By Ohm's law, the voltage

loss across a resistance R (in the direction of the current I) is RI. Going counterclockwise around

three closed circuits yields

                               Upper left   10 + 5 = 20I1
                               Upper right  -5 + 20 = 10I3 + 5I4
                               Lower
                                                   -10 = -5I5 - 5I4

Hence, disregarding the redundant equation obtained at junction C, we have six equations in the
six unknowns I1, . . . , I6. The solution is

                               I1 = 15           I4 = 28

                                       20                20

                               I2 = -1           I5 = 12

                                       20                20

                               I3 = 16           I6 = 27

                                       20                20

The fact that I2 is negative means, of course, that this current is in the opposite direction, with a
                1
magnitude  of   20  amperes.
 6 Systems of Linear Equations

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

 .6 An Application to Chemical Reactions

When a chemical reaction takes place a number of molecules combine to produce new molecules. Hence,
when hydrogen H2 and oxygen O2 molecules combine, the result is water H2O. We express this as

                                                     H2 + O2  H2O
Individual atoms are neither created nor destroyed, so the number of hydrogen and oxygen atoms going
into the reaction must equal the number coming out (in the form of water). In this case the reaction is
said to be balanced. Note that each hydrogen molecule H2 consists of two atoms as does each oxygen
molecule O2, while a water molecule H2O consists of two hydrogen atoms and one oxygen atom. In the
above reaction, this requires that twice as many hydrogen molecules enter the reaction; we express this as
follows:

                                                    2H2 + O2  2H2O
This is now balanced because there are 4 hydrogen atoms and 2 oxygen atoms on each side of the reaction.

   Example 1.6.1
   Balance the following reaction for burning octane C8H18 in oxygen O2:

                                               C8H18 + O2  CO2 + H2O
                                                                         .6. An Application to Chemical Reactions

where CO2 represents carbon dioxide. We must find positive integers x, y, z, and w such that

                                        xC8H18 + yO2  zCO2 + wH2O

Equating the number of carbon, hydrogen, and oxygen atoms on each side gives 8x = z, 18x = 2w
and 2y = 2z + w, respectively. These can be written as a homogeneous linear system

                                               8x - z = 0

                                               18x                - 2w = 0

                                                    2y - 2z - w = 0

which can be solved by gaussian elimination. In larger systems this is necessary but, in such a
                                                                                         1t,        8t,         16 t         25 t .
simple  situation,  it  is  easier  to  solve  directly.  Set  w  =  t,  so  that  x  =       z  =       2y  =        +t  =
                                                                                         9          9           9            9
But x, y, z, and w must be positive integers, so the smallest value of t that eliminates fractions is 18.

Hence, x = 2, y = 25, z = 16, and w = 18, and the balanced reaction is

                                        2C8H18 + 25O2  16CO2 + 18H2O

The reader can verify that this is indeed balanced.

    It is worth noting that this problem introduces a new element into the theory of linear equations: the
insistence that the solution must consist of positive integers.

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!
Chapter 2

           Matrix Algebra

In the study of systems of linear equations in Chapter 1, we found it convenient to manipulate the aug-
mented matrix of the system. Our aim was to reduce it to row-echelon form (using elementary row oper-
ations) and hence to write down all solutions to the system. In the present chapter we consider matrices
for their own sake. While some of the motivation comes from linear equations, it turns out that matrices
can be multiplied and added and so form an algebraic system somewhat analogous to the real numbers.
This "matrix algebra" is useful in ways that are quite different from the study of linear equations. For
example, the geometrical transformations obtained by rotating the euclidean plane about the origin can be
viewed as multiplications by certain 2 × 2 matrices. These "matrix transformations" are an important tool
in geometry and, in turn, the geometry provides a "picture" of the matrices. Furthermore, matrix algebra
has many other applications, some of which will be explored in this chapter. This subject is quite old and
was first studied systematically in 1858 by Arthur Cayley.1

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

    1Arthur Cayley (1821-1895) showed his mathematical talent early and graduated from Cambridge in 1842 as senior wran-
gler. With no employment in mathematics in view, he took legal training and worked as a lawyer while continuing to do
mathematics, publishing nearly 300 papers in fourteen years. Finally, in 1863, he accepted the Sadlerian professorship in Cam-
bridge and remained there for the rest of his life, valued for his administrative and teaching skills as well as for his scholarship.
His mathematical achievements were of the first rank. In addition to originating matrix theory and the theory of determinants,
he did fundamental work in group theory, in higher-dimensional geometry, and in the theory of invariants. He was one of the
most prolific mathematicians of all time and produced 966 papers.

                                                              29
Matrix Algebra

. Matrix Addition, Scalar Multiplication, and
     Transposition

A rectangular array of numbers is called a matrix (the plural is matrices), and the numbers are called the
entries of the matrix. Matrices are usually denoted by uppercase letters: A, B, C, and so on. Hence,

                A = 1 2 -1 0 5 6  B = 1 -1 0 2               1

                                                        C= 3 
                                                                 2

are matrices. Clearly matrices come in various shapes depending on the number of rows and columns.
For example, the matrix A shown has 2 rows and 3 columns. In general, a matrix with m rows and n
columns is referred to as an m × n matrix or as having size m × n. Thus matrices A, B, and C above have
sizes 2 × 3, 2 × 2, and 3 × 1, respectively. A matrix of size 1 × n is called a row matrix, whereas one of
size m × 1 is called a column matrix. Matrices of size n × n for some n are called square matrices.

    Each entry of a matrix is identified by the row and column in which it lies. The rows are numbered
from the top down, and the columns are numbered from left to right. Then the (i, j)-entry of a matrix is
the number lying simultaneously in row i and column j. For example,

                The (1, 2)-entry of    1 -1        is - 1.
                The (2, 3)-entry of    01

                                       1 2 -1 0 5 6 is 6.

    A special notation is commonly used for the entries of a matrix. If A is an m × n matrix, and if the
(i, j)-entry of A is denoted as ai j, then A is displayed as follows:

                  a11 a12 a13 · · · a1n                 

                  a21             a22  a23  ···  a2n    
                                                        
                A =  ..           ..   ..           ..
                                  ...              .    
                                                        

                  am1 am2 am3 · · · amn

This is usually denoted simply as A = ai j . Thus ai j is the entry in row i and column j of A. For example,
a 3 × 4 matrix in this notation is written

                                  a11 a12 a13 a14  

                A =  a21 a22 a23 a24 

                                  a31 a32 a33 a34

It is worth pointing out a convention regarding rows and columns: Rows are mentioned before columns.
For example:

· If a matrix has size m × n, it has m rows and n columns.

· If we speak of the (i, j)-entry of a matrix, it lies in row i and column j.

· If an entry is denoted ai j, the first subscript i refers to the row and the second subscript j to the
  column in which ai j lies.
                                                  . . Matrix Addition, Scalar Multiplication, and Transposition

    Two points (x1, y1) and (x2, y2) in the plane are equal if and only if2 they have the same coordinates,
that is x1 = x2 and y1 = y2. Similarly, two matrices A and B are called equal (written A = B) if and only if:

   1. They have the same size.
   2. Corresponding entries are equal.

If the entries of A and B are written in the form A = ai j , B = bi j , described earlier, then the second
condition takes the following form:

                                   A = ai j = bi j means ai j = bi j for all i and j

Example 2.1.1        1 2 -1   and C =    10                              discuss the possibility that A = B,
                     30 1              -1 2
Given A = a b , B =
                cd

B = C, A = C.

Solution. A = B is impossible because A and B are of different sizes: A is 2 × 2 whereas B is 2 × 3.

Similarly, B = C is impossible. But A = C is possible provided that corresponding entries are
equal: a b c d = 1 0 -1 2 means a = 1, b = 0, c = -1, and d = 2.

Matrix Addition

   Definition 2.1 Matrix Addition
   If A and B are matrices of the same size, their sum A + B is the matrix formed by adding
   corresponding entries.

If A = ai j and B = bi j , this takes the form
                                                    A + B = ai j + bi j

Note that addition is not defined for matrices of different sizes.

Example 2.1.2        and B =  1 1 -1 2 0 6 , compute A + B.
If A = 2 1 3 -1 2 0

Solution.

                     A + B = 2 + 1 1 + 1 3 - 1 -1 + 2 2 + 0 0 + 6 = 3 2 2 1 2 6

   2If p and q are statements, we say that p implies q if q is true whenever p is true. Then "p if and only if q" means that both
p implies q and q implies p. See Appendix B for more on this.
Matrix Algebra

Example 2.1.3
Find a, b, and c if a b c + c a b = 3 2 -1 .

Solution. Add the matrices on the left side to obtain

                                     a + c b + a c + b = 3 2 -1

Because corresponding entries must be equal, this gives three equations: a + c = 3, b + a = 2, and
c + b = -1. Solving these yields a = 3, b = -1, c = 0.

If A, B, and C are any matrices of the same size, then

                                    A+B = B+A                     (commutative law)
                             A + (B +C) = (A + B) +C                (associative law)

In fact, if A = ai j and B = bi j , then the (i, j)-entries of A + B and B + A are, respectively, ai j + bi j and
bi j + ai j. Since these are equal for all i and j, we get

                A + B = ai j + bi j = bi j + ai j = B + A

The associative law is verified similarly.

    The m × n matrix in which every entry is zero is called the m × n zero matrix and is denoted as 0 (or
0mn if it is important to emphasize the size). Hence,

                             0+X =X

holds for all m × n matrices X . The negative of an m × n matrix A (written -A) is defined to be the m × n
matrix obtained by multiplying each entry of A by -1. If A = ai j , this becomes -A = -ai j . Hence,

                             A + (-A) = 0

holds for all matrices A where, of course, 0 is the zero matrix of the same size as A.
    A closely related notion is that of subtracting matrices. If A and B are two m × n matrices, their

difference A - B is defined by
                                                    A - B = A + (-B)

Note that if A = ai j and B = bi j , then

                             A - B = ai j + -bi j = ai j - bi j

is the m × n matrix formed by subtracting corresponding entries.

Example 2.1.4                1 -1 1 -2 0 6 , C =        1 0 -2 3 1 1 . Compute -A, A - B, and
Let A = 3 -1 0 1 2 -4 , B =
A + B -C.
                                    . . Matrix Addition, Scalar Multiplication, and Transposition

Solution.

                  -A =        -3 1 0
                A-B =         -1 -2 4
           A+B-C =
                              3 - 1 -1 - (-1) 0 - 1 1 - (-2) 2 - 0 -4 - 6 = 2 0 -1 3 2 -10

                              3 + 1 - 1 -1 - 1 - 0 0 + 1 - (-2) 1 - 2 - 3 2 + 0 - 1 -4 + 6 - 1 = 3 -2 3 -4 1 1

Example 2.1.5   +X =            10  where X is a matrix.
Solve 3 2 -1 1                -1 2

Solution. We solve a numerical equation a + x = b by subtracting the number a from both sides to
obtain x = b - a. This also works for matrices. To solve 3 2 -1 1 + X = 1 0 -1 2 simply

subtract the matrix 3 2 -1 1 from both sides to get

           X = 1 0 -1 2 - 3 2 -1 1 = 1 - 3 0 - 2 -1 - (-1) 2 - 1 = -2 -2 0 1

The reader should verify that this matrix X does indeed satisfy the original equation.

    The solution in Example 2.1.5 solves the single matrix equation A + X = B directly via matrix subtrac-
tion: X = B - A. This ability to work with matrices as entities lies at the heart of matrix algebra.

It is important to note that the sizes of matrices involved in some calculations are often determined by

the context. For example, if

                                    A +C = 1 3 -1 2 0 1

then A and C must be the same size (so that A +C makes sense), and that size must be 2 × 3 (so that the
sum is 2 × 3). For simplicity we shall often omit reference to such facts when they are clear from the
context.

Scalar Multiplication

In gaussian elimination, multiplying a row of a matrix by a number k means multiplying every entry of
that row by k.

Definition 2.2 Matrix Scalar Multiplication

More generally, if A is any matrix and k is any number, the scalar multiple kA is the matrix
obtained from A by multiplying each entry of A by k.
Matrix Algebra

If A = ai j , this is

                                           kA = kai j

Thus 1A = A and (-1)A = -A for any matrix A.

    The term scalar arises here because the set of numbers from which the entries are drawn is usually
referred to as the set of scalars. We have been using real numbers as scalars, but we could equally well
have been using complex numbers.

Example 2.1.6          and B =     1 2 -1     compute  5A,   1 B,  and  3A - 2B.
If A = 3 -1 4 2 0 6                03 2
                                                             2

Solution.

                             5A =  15 -5 20 10 0 30 ,  1B =  21 1 -1    2
                       3A - 2B =   9 -3 12 6 0 18 -          03 1
                                                       2        2

                                                       2 4 -2 0 6 4 = 7 -7 14 6 -6 14

    If A is any matrix, note that kA is the same size as A for all scalars k. We also have
                                                 0A = 0 and k0 = 0

because the zero matrix has every entry zero. In other words, kA = 0 if either k = 0 or A = 0. The converse
of this statement is also true, as Example 2.1.7 shows.

   Example 2.1.7
   If kA = 0, show that either k = 0 or A = 0.

   Solution. Write A = ai j so that kA = 0 means kai j = 0 for all i and j. If k = 0, there is nothing to
   do. If k = 0, then kai j = 0 implies that ai j = 0 for all i and j; that is, A = 0.

    For future reference, the basic properties of matrix addition and scalar multiplication are listed in
Theorem 2.1.1.

   Theorem 2.1.1
   Let A, B, and C denote arbitrary m × n matrices where m and n are fixed. Let k and p denote
   arbitrary real numbers. Then

       1. A + B = B + A.

       2. A + (B +C) = (A + B) +C.
       3. There is an m × n matrix 0, such that 0 + A = A for each A.

       4. For each A there is an m × n matrix, -A, such that A + (-A) = 0.
                                                 . . Matrix Addition, Scalar Multiplication, and Transposition

       5. k(A + B) = kA + kB.
       6. (k + p)A = kA + pA.
       7. (kp)A = k(pA).
       8. 1A = A.

Proof. Properties 1-4 were given previously. To check Property 5, let A = ai j and B = bi j denote
matrices of the same size. Then A + B = ai j + bi j , as before, so the (i, j)-entry of k(A + B) is

                                                k(ai j + bi j) = kai j + kbi j

But this is just the (i, j)-entry of kA + kB, and it follows that k(A + B) = kA + kB. The other Properties
can be similarly verified; the details are left to the reader.

    The Properties in Theorem 2.1.1 enable us to do calculations with matrices in much the same way that
numerical calculations are carried out. To begin, Property 2 implies that the sum

                                               (A + B) +C = A + (B +C)

is the same no matter how it is formed and so is written as A + B +C. Similarly, the sum

                                                       A+B+C+D

is independent of how it is formed; for example, it equals both (A + B) + (C + D) and A + [B + (C + D)].
Furthermore, property 1 ensures that, for example,

                                            B+D+A+C = A+B+C+D

In other words, the order in which the matrices are added does not matter. A similar remark applies to
sums of five (or more) matrices.

    Properties 5 and 6 in Theorem 2.1.1 are called distributive laws for scalar multiplication, and they
extend to sums of more than two terms. For example,

                                              k(A + B -C) = kA + kB - kC

                                             (k + p - m)A = kA + pA - mA
Similar observations hold for more than three summands. These facts, together with properties 7 and
8, enable us to simplify expressions by collecting like terms, expanding, and taking common factors in
exactly the same way that algebraic expressions involving variables and real numbers are manipulated.
The following example illustrates these techniques.

   Example 2.1.8
   Simplify 2(A + 3C) - 3(2C - B) - 3 [2(2A + B - 4C) - 4(A - 2C)] where A, B, and C are all
   matrices of the same size.
6 Matrix Algebra

  Solution. The reduction proceeds as though A, B, and C were variables.

                           2(A + 3C) - 3(2C - B) - 3 [2(2A + B - 4C) - 4(A - 2C)]
                                 = 2A + 6C - 6C + 3B - 3 [4A + 2B - 8C - 4A + 8C]
                                 = 2A + 3B - 3 [2B]
                                 = 2A - 3B

Transpose of a Matrix

Many results about a matrix A involve the rows of A, and the corresponding result for columns is derived
in an analogous way, essentially by replacing the word row by the word column throughout. The following
definition is made with such applications in mind.

   Definition 2.3 Transpose of a Matrix

   If A is an m × n matrix, the transpose of A, written AT , is the n × m matrix whose rows are just the
   columns of A in the same order.

In other words, the first row of AT is the first column of A (that is it consists of the entries of column 1 in
order). Similarly the second row of AT is the second column of A, and so on.

Example 2.1.9
Write down the transpose of each of the following matrices.

                1               1 2                                3 1 -1 
           A= 3  B= 5 2 6  C= 3 4                            D= 1 3 2 

                   2                56                                -1 2 1

Solution.

           AT =                        5     135                        , and DT = D.
                 1 3 2 , BT =  2  , CT =     246

                                          6

   If A = ai j is a matrix, write AT = bi j . Then bi j is the jth element of the ith row of AT and so is the
jth element of the ith column of A. This means bi j = a ji, so the definition of AT can be stated as follows:

                 If A = ai j , then AT = a ji .                                        (2.1)

This is useful in verifying the following properties of transposition.
                                                  . . Matrix Addition, Scalar Multiplication, and Transposition

   Theorem 2.1.2
   Let A and B denote matrices of the same size, and let k denote a scalar.

       1. If A is an m × n matrix, then AT is an n × m matrix.
       2. (AT )T = A.
       3. (kA)T = kAT .
       4. (A + B)T = AT + BT .

Proof. Property 1 is part of the definition of AT , and Property 2 follows from (2.1). As to Property 3: If
A = ai j , then kA = kai j , so (2.1) gives

                                            (kA)T = ka ji = k a ji = kAT
Finally, if B = bi j , then A + B = ci j where ci j = ai j + bi j Then (2.1) gives Property 4:

                       (A + B)T = ci j T = c ji = a ji + b ji = a ji + b ji = AT + BT
8 Matrix Algebra

    There is another useful way to think of transposition. If A = ai j is an m × n matrix, the elements
a11, a22, a33, . . . are called the main diagonal of A. Hence the main diagonal extends down and to the
right from the upper left corner of the matrix A; it is outlined in the following examples:

                    a11 a12                         a11 a12 a13  

                   a21 a22  a11 a12 a13  a21 a22 a23  a11
                    a31 a32 a21 a22 a23 a31 a32 a33 a21

 Thus forming the transpose of a matrix A can be viewed as "flipping" A about its main diagonal, or
as "rotating" A through 180 about the line containing the main diagonal. This makes Property 2 in

Theorem 2.1.2 transparent.

Example 2.1.10            12      T= 2 3 .
Solve for A if 2AT - 3  -1 1              -1 2

Solution. Using Theorem 2.1.2, the left side of the equation is

           2AT - 3 1 2 -1 1       T                 1 2 T = 2A - 3 1 -1
                                                    -1 1             21
                                    = 2 AT T - 3

Hence the equation becomes

                               2A - 3 1 -1 2 1 = 2 3 -1 2

Thus 2A =  23       +3      1 -1  =  50  , so finally A = 2      1 50 5 10   .
                                                                     =2
           -1 2             21       55                          55      11

Note that Example 2.1.10 can also be solved by first transposing both sides, then solving for AT , and so
obtaining A = (AT )T . The reader should do this.

    The matrix D = 1 2 2 5 in Example 2.1.9 has the property that D = DT . Such matrices are important;
a matrix A is called symmetric if A = AT . A symmetric matrix A is necessarily square (if A is m × n, then
AT is n×m, so A = AT forces n = m). The name comes from the fact that these matrices exhibit a symmetry
about the main diagonal. That is, entries that are directly across the main diagonal from each other are
equal.

                     a b c
    For example,  b d e  is symmetric when b = b, c = c, and e = e.

                        c e f

   Example 2.1.11

   If A and B are symmetric n × n matrices, show that A + B is symmetric.

   Solution. We have AT = A and BT = B, so, by Theorem 2.1.2, we have
                                                 . . Matrix Addition, Scalar Multiplication, and Transposition

   (A + B)T = AT + BT = A + B. Hence A + B is symmetric.

   Example 2.1.12
   Suppose a square matrix A satisfies A = 2AT . Show that necessarily A = 0.
   Solution. If we iterate the given equation, Theorem 2.1.2 gives

                                       A = 2AT = 2 2AT T = 2 2(AT )T = 4A
   Subtracting A from both sides gives 3A = 0, so A = 13(0) = 0.

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.
                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.
                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!
Matrix Algebra

 . Matrix-Vector Multiplication

Up to now we have used matrices to solve systems of linear equations by manipulating the rows of the
augmented matrix. In this section we introduce a different way of describing linear systems that makes
more use of the coefficient matrix of the system and leads to a useful way of "multiplying" matrices.

Vectors

It is a well-known fact in analytic geometry that two points in the plane with coordinates (a1, a2) and
(b1, b2) are equal if and only if a1 = b1 and a2 = b2. Moreover, a similar condition applies to points
(a1, a2, a3) in space. We extend this idea as follows.

    An ordered sequence (a1, a2, . . . , an) of real numbers is called an ordered n-tuple. The word "or-
dered" here reflects our insistence that two ordered n-tuples are equal if and only if corresponding entries
are the same. In other words,

         (a1, a2, . . . , an) = (b1, b2, . . . , bn) if and only if a1 = b1, a2 = b2, . . . , and an = bn.

Thus the ordered 2-tuples and 3-tuples are just the ordered pairs and triples familiar from geometry.

   Definition 2.4 The set Rn of ordered n-tuples of real numbers
   Let R denote the set of all real numbers. The set of all ordered n-tuples from R has a special
   notation:

                            Rn denotes the set of all ordered n-tuples of real numbers.

While elements in Rn can be written as rows (r1, r2, . . . , rn), we will most often write them as n × 1
                 
                    r1

  r2            
                
column matrices  ..  and use matrix algebra as previously seen in Section 2.1 on elements of R .n

.

                        rn
These are called vectors or n-vectors and will be denoted using bold type such as x or v. This is indeed

very convenient and powerful as we will see. For example, an m × n matrix A will be written as a row of
n-vectors (its columns):

A = a1 a2 · · · an where a j denotes column j of A for each j.

If x and y are two n-vectors in Rn, it is clear that their matrix sum x + y is also in Rn as is the scalar
multiple kx for any real number k. We express this observation by saying that Rn is closed under addition
and scalar multiplication. In particular, all the basic properties in Theorem 2.1.1 are true of these n-vectors.
These properties are fundamental and will be used frequently below without comment. As for matrices in
general, the n × 1 zero matrix is called the zero n-vector in Rn and, if x is an n-vector, the n-vector -x is
called the negative x.

    Of course, we have already encountered these n-vectors in Section 1.3 as the solutions to systems of
linear equations with n variables. In particular we defined the notion of a linear combination of vectors
and showed that a linear combination of solutions to a homogeneous system is again a solution. Clearly, a
linear combination of n-vectors in Rn is again in Rn, a fact that we will be using.
                                                      . . Matrix-Vector Multiplication

    There is also a geometric interpretation that will be revisited in Chapter 4 and 5: elements in Rn can
be viewed as points, such as the point P(2, 3) in the plane R2, or as a vector v = 23 (as in an arrow)

from the origin to the point P(2, 3) and hence in anticipation the reason for introducing the name vector.

Matrix-Vector Multiplication

Given a system of linear equations, the left sides of the equations depend only on the coefficient matrix A
and the column x of variables, and not on the constants. This observation leads to a fundamental idea in
linear algebra: We view the left sides of the equations as the "product" Ax of the matrix A and the vector
x. This simple change of perspective leads to a completely new way of viewing linear systems--one that
is very useful and will occupy our attention throughout this book.

To motivate the definition of the "product" Ax, consider first the following system of two equations in

three variables:              ax1 + bx2 + cx3 = b1
                              ax1 + bx2 + cx3 = b1
                                                                                        (2.2)

                                      
                 a b c , x =  x2 , b = x1 b1 denote the coefficient matrix, the variable matrix, and
and let A = a b c b2

                                         x3
the constant matrix, respectively. The system (2.2) can be expressed as a single vector equation

                                               ax1 + bx2 + cx3 ax1 + bx2 + cx3 = b2 b1
which in turn can be written as follows:

                                      x1 aa + x2 bb + x3 cc = b1 b2
Now observe that the vectors appearing on the left side are just the columns

                                     a1 = aa , a2 = bb , and a3 = cc
of the coefficient matrix A. Hence the system (2.2) takes the form

                              x1a1 + x2a2 + x3a3 = b                                    (2.3)

This shows that the system (2.2) has a solution if and only if the constant matrix b is a linear combination3
of the columns of A, and that in this case the entries of the solution are the coefficients x1, x2, and x3 in
this linear combination.

    Moreover, this holds in general. If A is any m × n matrix, it is often convenient to view A as a row of
columns. That is, if a1, a2, . . . , an are the columns of A, we write

                                                 A = a1 a2 · · · an

and say that A = a1 a2 · · · an is given in terms of its columns.

    3Linear combinations were introduced in Section 1.3 to describe the solutions of homogeneous systems of linear equations.
They will be used extensively in what follows.
Matrix Algebra

Now consider any system of linear equations with m × n coefficient matrix A. If b is the constant
                                   
                                      x1

                   x2                          
                                               
matrix of the system, and if x =  ..  is the matrix of variables then, exactly as above, the system can
                .

                                           xn
be written as a single vector equation

                                               x1a1 + x2a2 + · · · + xnan = b       (2.4)

Example 2.2.1

                     3x1 + 2x2 - 4x3 = 0
Write the system x1 - 3x2 + x3 = 3 in the form given in (2.4).

                      x2 - 5x3 = -1

Solution.

                 3   2   -4   0 

                x1  1  + x2  -3  + x3  1  =  3 
                0                                1  -5                          -1

    As mentioned above, we view the left side of (2.4) as the product of the matrix A and the vector x.
This basic idea is formalized in the following definition:

Definition 2.5 Matrix-Vector Multiplication

Let A = a1 a2 · · · an be an m × n matrix, written in terms of its columns a1, a2, . . . , an. If
     
        x1

  x2       
           
x =  ..  is any n-vector, the product Ax is defined to be the m-vector given by:
.

  xn

                                               Ax = x1a1 + x2a2 + · · · + xnan

In other words, if A is m × n and x is an n-vector, the product Ax is the linear combination of the columns
of A where the coefficients are the entries of x (in order).

    Note that if A is an m × n matrix, the product Ax is only defined if x is an n-vector and then the vector
Ax is an m-vector because this is true of each column a j of A. But in this case the system of linear equations
with coefficient matrix A and constant vector b takes the form of a single matrix equation

                                                           Ax = b

The following theorem combines Definition 2.5 and equation (2.4) and summarizes the above discussion.
Recall that a system of linear equations is said to be consistent if it has at least one solution.
                                                    . . Matrix-Vector Multiplication

Theorem 2.2.1

1. Every system of linear equations has the form Ax = b where A is the coefficient matrix, b is
   the constant matrix, and x is the matrix of variables.

2. The system Ax = b is consistent if and only if b is a linear combination of the columns of A.

                          
                             x1

                            x2                    
                                                  
3. If a1, a2, . . . , an are the columns of A and if x =  .. , then x is a solution to the linear
                          .

                                                               xn
system Ax = b if and only if x1, x2, . . . , xn are a solution of the vector equation

               x1a1 + x2a2 + · · · + xnan = b

A system of linear equations in the form Ax = b as in (1) of Theorem 2.2.1 is said to be written in matrix
form. This is a useful way to view linear systems as we shall see.

    Theorem 2.2.1 transforms the problem of solving the linear system Ax = b into the problem of ex-
pressing the constant matrix B as a linear combination of the columns of the coefficient matrix A. Such
a change in perspective is very useful because one approach or the other may be better in a particular
situation; the importance of the theorem is that there is a choice.

Example 2.2.2

 2 -1 3 5       2
                    1
If A =  0 2 -3 1  and x =    , compute Ax.
                                               0
-3 4 1 2 -2

                2   -1   3   5   -7 

Solution. By Definition 2.5: Ax = 2  0  + 1  2  + 0  -3  - 2  1  =  0 .

               -3      4                            1  2                               -6

Example 2.2.3

Given columns a1, a2, a3, and a4 in R3, write 2a1 - 3a2 + 5a3 + a4 in the form Ax where A is a
matrix and x is a vector.

                                                            2
Solution. Here the column of coefficients is x =  5   -3  . Hence Definition 2.5 gives

                                                                1

                                           Ax = 2a1 - 3a2 + 5a3 + a4

where A = a1 a2 a3 a4 is the matrix with a1, a2, a3, and a4 as its columns.
    Matrix Algebra

Example 2.2.4

                                                                                                        2

Let A = a1 a2 a3 a4 be the 3 × 4 matrix given in terms of its columns a1 =  0 ,
                                                                                                        -1

       1  3                                           3

a2 =  1 , a3 =  -1 , and a4 =  1 . In each case below, either express b as a linear
          1                  -3                       0

combination of a1, a2, a3, and a4, or show that it is not such a linear combination. Explain what
your answer means for the corresponding system Ax = b of linear equations.

            1                                                                 4
    a. b =  2                                                        b. b =  2 

                3                                                                1

Solution. By Theorem 2.2.1, b is a linear combination of a1, a2, a3, and a4 if and only if the
system Ax = b is consistent (that is, it has a solution). So in each case we carry the augmented

matrix [A|b] of the system Ax = b to reduced form.

                  2 1 3 3 1 1 0 2 1 0

    a. Here  0 1 -1 1 2    0 1 -1 1 0 , so the system Ax = b has no

                 -1 1 -3 0 3                               00 001

       solution in this case. Hence b is not a linear combination of a1, a2, a3, and a4.

                  2 1 3 3 4 1 0 2 1 1

    b. Now  0 1 -1 1 2    0 1 -1 1 2 , so the system Ax = b is consistent.

                 -1 1 -3 0 1                               00 000

Thus b is a linear combination of a1, a2, a3, and a4 in this case. In fact the general solution is
       1     2s  -t,         2        -t,         s,  and            where     and     are  arbitrary  parameters.  Hence
x1  =     -           x2  =     +  s       x3  =           x4  =  t         s       t
                                               
                                                  4

x1a1 + x2a2 + x3a3 + x4a4 = b =  2  for any choice of s and t. If we take s = 0 and t = 0, this
                                           1

becomes a1 + 2a2 = b, whereas taking s = 1 = t gives -2a1 + 2a2 + a3 + a4 = b.

Example 2.2.5

Taking A to be the zero matrix, we have 0x = 0 for all vectors x by Definition 2.5 because every
column of the zero matrix is zero. Similarly, A0 = 0 for all matrices A because every entry of the
zero vector is zero.
                                                             . . Matrix-Vector Multiplication

Example 2.2.6

       1 0 0
If I =  0 1 0 , show that Ix = x for any vector x in R3.

          001

                    
                       x1

Solution. If x =  x2  then Definition 2.5 gives

                       x3

   1   0   0   x1   0   0   x1 
Ix = x1  0  + x2  1  + x3  0  =  0  +  x2  +  0  =  x2  = x
      0  0                                 1     0        0
                                                             x3  x3

    The matrix I in Example 2.2.6 is called the 3 × 3 identity matrix, and we will encounter such matrices
again in Example 2.2.11 below. Before proceeding, we develop some algebraic properties of matrix-vector
multiplication that are used extensively throughout linear algebra.

   Theorem 2.2.2
   Let A and B be m × n matrices, and let x and y be n-vectors in Rn. Then:

       1. A(x + y) = Ax + Ay.

       2. A(ax) = a(Ax) = (aA)x for all scalars a.

       3. (A + B)x = Ax + Bx.

Proof. We prove (3); the other verifications are similar and are left as exercises. Let A = a1 a2 · · · an
and B = b1 b2 · · · bn be given in terms of their columns. Since adding two matrices is the same

as adding their columns, we have

         A + B = a1 + b1 a2 + b2 · · · an + bn


   x1

  x2  
      
If we write x =  ..  Definition 2.5 gives
.

  xn

  (A + B)x = x1(a1 + b1) + x2(a2 + b2) + · · · + xn(an + bn)
             = (x1a1 + x2a2 + · · · + xnan) + (x1b1 + x2b2 + · · · + xnbn)
             = Ax + Bx

Theorem 2.2.2 allows matrix-vector computations to be carried out much as in ordinary arithmetic. For
example, for any m × n matrices A and B and any n-vectors x and y, we have:

                           A(2x - 5y) = 2Ax - 5Ay and (3A - 7B)x = 3Ax - 7Bx

We will use such manipulations throughout the book, often without mention.
6 Matrix Algebra

Linear Equations

Theorem 2.2.2 also gives a useful way to describe the solutions to a system

                                                           Ax = b
of linear equations. There is a related system

                                                           Ax = 0
called the associated homogeneous system, obtained from the original system Ax = b by replacing all
the constants by zeros. Suppose x1 is a solution to Ax = b and x0 is a solution to Ax = 0 (that is Ax1 = b
and Ax0 = 0). Then x1 + x0 is another solution to Ax = b. Indeed, Theorem 2.2.2 gives

                                         A(x1 + x0) = Ax1 + Ax0 = b + 0 = b
This observation has a useful converse.

   Theorem 2.2.3
   Suppose x1 is any particular solution to the system Ax = b of linear equations. Then every solution
   x2 to Ax = b has the form

                                                       x2 = x0 + x1
   for some solution x0 of the associated homogeneous system Ax = 0.

Proof. Suppose x2 is also a solution to Ax = b, so that Ax2 = b. Write x0 = x2 - x1. Then x2 = x0 + x1
and, using Theorem 2.2.2, we compute

                                     Ax0 = A(x2 - x1) = Ax2 - Ax1 = b - b = 0

Hence x0 is a solution to the associated homogeneous system Ax = 0.
Note that gaussian elimination provides one such representation.

Example 2.2.7

Express every solution to the following system as the sum of a specific solution plus a solution to
the associated homogeneous system.

                                              x1 - x2 - x3 + 3x4 = 2
                                            2x1 - x2 - 3x3 + 4x4 = 6

                                              x1 - 2x3 + x4 = 4

Solution. Gaussian elimination gives x1 = 4 + 2s - t, x2 = 2 + s + 2t, x3 = s, and x4 = t where s
and t are arbitrary parameters. Hence the general solution can be written

                   x1   4 + 2s - t   4    2   -1 

x =  x3   x2  =  s   2 + s + 2t  =  0   2  + s  1   1  + t  0   2 

                  x4  t  0  0  1
                                                                  . . Matrix-Vector Multiplication

  4                                                                     2   -1 

Thus x1 =  0   2  is a particular solution (where s = 0 = t), and x0 = s  1   1  + t  0   2  gives all

  0                                                                    0       1

solutions to the associated homogeneous system. (To see why this is so, carry out the gaussian

elimination again but with all the constants set equal to zero.)

The following useful result is included with no proof.

Theorem 2.2.4
Let Ax = b be a system of equations with augmented matrix A b . Write rank A = r.

   1. rank A b is either r or r + 1.
   2. The system is consistent if and only if rank A b = r.
   3. The system is inconsistent if and only if rank A b = r + 1.

The Dot Product

Definition 2.5 is not always the easiest way to compute a matrix-vector product Ax because it requires
that the columns of A be explicitly identified. There is another way to find such a product which uses the
matrix A as a whole with no reference to its columns, and hence is useful in practice. The method depends
on the following notion.

   Definition 2.6 Dot Product in Rn
   If (a1, a2, . . . , an) and (b1, b2, . . . , bn) are two ordered n-tuples, their dot product is defined to
   be the number

                                                 a1b1 + a2b2 + · · · + anbn
   obtained by multiplying corresponding entries and adding the results.

To see how this relates to matrix products, let A denote a 3 × 4 matrix and let x be a 4-vector. Writing

     
     x1
                                                                       
     x =  x2  and A =  a21 a22 a23 a24  a11 a12 a13 a14
      x3 
                                                 a31 a32 a33 a34
     x4

in the notation of Section 2.1, we compute

         
           x1
                                                
  a11 a12 a13 a14                           a11                   a12     a13      a14
Ax =  a21 a22 a23 a24   x2  = x1  a21  + x2  a22  + x3  a23  + x4  a24 
  a31 a32 a33 a34  x3  a31 a32 a33 a34
           x4
8 Matrix Algebra

                        a11x1 + a12x2 + a13x3 + a14x4                      

                      =  a21x1 + a22x2 + a23x3 + a24x4 

                        a31x1 + a32x2 + a33x3 + a34x4

From this we see that each entry of Ax is the dot product of the corresponding row of A with x. This
computation goes through in general, and we record the result in Theorem 2.2.5.

Theorem 2.2.5: Dot Product Rule

Let A be an m × n matrix and let x be an n-vector. Then each entry of the vector Ax is the dot
product of the corresponding row of A with x.

This result is used extensively throughout linear algebra.

    If A is m × n and x is an n-vector, the computation of Ax by the dot product rule is simpler than
using Definition 2.5 because the computation can be carried out directly with no explicit reference to the
columns of A (as in Definition 2.5). The first entry of Ax is the dot product of row 1 of A with x. In
hand calculations this is computed by going across row one of A, going down the column x, multiplying
corresponding entries, and adding the results. The other entries of Ax are computed in the same way using
the other rows of A with the column x.

                      In general, compute entry i of Ax as follows (see the diagram):

  A      x        Ax

                      Go across row i of A and down column x, multiply corre-
                      sponding entries, and add the results.
           = 

  row i           entry i As an illustration, we rework Example 2.2.2 using the dot product rule
                      instead of Definition 2.5.

Example 2.2.8

          2 -1 3 5     2

If A =  0 2 -3 1  and x =  0   1 , compute Ax.
         -3 4 1 2 -2

Solution. The entries of Ax are the dot products of the rows of A with x:

       2 -1 3 5   2   2 · 2 + (-1)1 + 3 · 0 + 5(-2)   -7 
Ax =  0 2 -3 1   0   1  =  0 · 2 + 2 · 1 + (-3)0 + 1(-2)  =  0 

          -3 4 1 2 -2 (-3)2 + 4 · 1 + 1 · 0 + 2(-2) -6

Of course, this agrees with the outcome in Example 2.2.2.
                                                               . . Matrix-Vector Multiplication

Example 2.2.9
Write the following system of linear equations in the form Ax = b.

                                        5x1 - x2 + 2x3 + x4 - 3x5 = 8
                                         x1 + x2 + 3x3 - 5x4 + 2x5 = -2

                                       -x1 + x2 - 2x3 + - 3x5 = 0

                                                                         
                                                                            x1

  5 -1 2 1 -3                                             8                   x2

Solution. Write A =  1               1  3 -5  2           -2                         Then the dot
                                                                                  
                                                 , b =         , and x =      x3  .
                                                                                  
  -1 1 -2 0 -3                                            0               x4 

                                                                              x5

   5x1 - x2 + 2x3 + x4 - 3x5 
product rule gives Ax =  x1 + x2 + 3x3 - 5x4 + 2x5 , so the entries of Ax are the left sides of
  -x1 + x2 - 2x3                                 - 3x5
the equations in the linear system. Hence the system becomes Ax = b because matrices are equal if

and only corresponding entries are equal.

Example 2.2.10
If A is the zero m × n matrix, then Ax = 0 for each n-vector x.

Solution. For each k, entry k of Ax is the dot product of row k of A with x, and this is zero because
row k of A consists of zeros.

Definition 2.7 The Identity Matrix

For each n > 2, the identity matrix In is the n × n matrix with 1s on the main diagonal (upper left
to lower right), and zeros elsewhere.

The first few identity matrices are

I2 = 1 0 ,                              1 0 0             1 0 0 0
                                                             0 1 0 0
                                        I3 =  0 1 0  ,                   , ...
                                                        I4 = 
01                                      001               0 0 1 0

                                                               0001

In Example 2.2.6 we showed that I3x = x for each 3-vector x using Definition 2.5. The following result
shows that this holds in general, and is the reason for the name.

Example 2.2.11
For each n  2 we have Inx = x for each n-vector x in Rn.
Matrix Algebra

                                                                        
                                                                           x1

Solution. We verify the case n = 4. Given the 4-vector x =  x3   x2  the dot product rule gives
                                                                           x4

                 1 0 0 0   x1   x1 + 0 + 0 + 0   x1 

I4x =  0 0 1 0   x3   0 1 0 0   x2  =  0 + 0 + x3 + 0   0 + x2 + 0 + 0  =  x3   x2  = x

                  0 0 0 1 x4  0 + 0 + 0 + x4                            x4

In general, Inx = x because entry k of Inx is the dot product of row k of In with x, and row k of In
has 1 in position k and zeros elsewhere.

Example 2.2.12

Let A = a1 a2 · · · an be any m × n matrix with columns a1, a2, . . . , an. If e j denotes
column j of the n × n identity matrix In, then Ae j = a j for each j = 1, 2, . . . , n.

                
                   t1

                  t2  
                      
Solution. Write e j =  ..  where t j = 1, but ti = 0 for all i = j. Then Theorem 2.2.5 gives
                .

                  tn

                Ae j = t1a1 + · · · + t ja j + · · · + tnan = 0 + · · · + a j + · · · + 0 = a j

Example 2.2.12 will be referred to later; for now we use it to prove:

Theorem 2.2.6
Let A and B be m × n matrices. If Ax = Bx for all x in Rn, then A = B.

Proof. Write A = a1 a2 · · · an and B = b1 b2 · · · bn and in terms of their columns. It is
enough to show that ak = bk holds for all k. But we are assuming that Aek = Bek, which gives ak = bk by
Example 2.2.12.

    We have introduced matrix-vector multiplication as a new way to think about systems of linear equa-
tions. But it has several other uses as well. It turns out that many geometric operations can be described
using matrix multiplication, and we now investigate how this happens. As a bonus, this description pro-
vides a geometric "picture" of a matrix by revealing the effect on a vector when it is multiplied by A. This
"geometric view" of matrices is a fundamental tool in understanding them.
                                                   . . Matrix-Vector Multiplication

Transformations

The set R2 has a geometrical interpretation as the euclidean plane where a vector a1 in R2 represents
                                                                                                     a2

the point (a1, a2) in the plane (see Figure 2.2.1). In this way we regard R2 as the set of all points in
the plane. Accordingly, we will refer to vectors in R2 as points, and denote their coordinates as a column
rather than a row. To enhance this geometrical interpretation of the vector a1 , it is denoted graphically

                                                                                           a2
by an arrow from the origin 00 to the vector as in Figure 2.2.1.

x2                                                        x3

a2                                             a1           a3                               
                                                                                                a1
                                               a2             0
                                                      a1                                      a2 
    0 = 00  a1 x1                                                                               a3
                                                   x1
                                                                                                 a2

                                                                                                     x2

    Figure 2.2.1                                   Figure 2.2.2

                                                                                                                      
                                                                                                                         a1

    Similarly we identify R3 with 3-dimensional space by writing a point (a1, a2, a3) as the vector  a2 

                                                                                                                         a3
in R3, again represented by an arrow4 from the origin to the point as in Figure 2.2.2. In this way the terms
"point" and "vector" mean the same thing in the plane or in space.

    We begin by describing a particular geometrical transformation of the plane R2.

Example 2.2.13                                     Consider the transformation of R2 given by reflection in the
                                                   x axis. This operation carries the vector a1 to its reflection
       y
                                                                                                      a2
                                         a1             a1 as in Figure 2.2.3. Now observe that
                                         a2          -a2

                                x                                           a1 -a2 = 1 0 a1 0 -1 a2

      0                                            so reflecting a1 in the x axis can be achieved by multiplying
                                                                     a2
                                           a1
                                         -a2       by the matrix 1 0 0 -1 .

          Figure 2.2.3

4This "arrow" representation of vectors in R2 and R3 will be used extensively in Chapter 4.
Matrix Algebra

    If we write A = 1 0 0 -1 , Example 2.2.13 shows that reflection in the x axis carries each vector x in
R2 to the vector Ax in R2. It is thus an example of a function

                       T : R2  R2 where T (x) = Ax for all x in R2

As such it is a generalization of the familiar functions f : R  R that carry a number x to another real
number f (x).

                             More generally, functions T : Rn  Rm are called transformations
                         from Rn to Rm. Such a transformation T is a rule that assigns to every

    T                    vector x in Rn a uniquely determined vector T (x) in Rm called the image
                         of x under T . We denote this state of affairs by writing
x               T (x)

Rn              Rm                  T : Rn  Rm                   or    nT m

                                                                     R - R

    Figure 2.2.4         The transformation T can be visualized as in Figure 2.2.4.

                         To describe a transformation T : Rn  Rm we must specify the vector

T (x) in Rm for every x in Rn. This is referred to as defining T , or as specifying the action of T . Saying

that the action defines the transformation means that we regard two transformations S : Rn  Rm and

T : Rn  Rm as equal if they have the same action; more formally

                       S = T if and only if S(x) = T (x) for all x in Rn.

Again, this what we mean by f = g where f , g : R  R are ordinary functions.

    Functions f : R  R are often described by a formula, examples being f (x) = x2 + 1 and f (x) = sin x.
The same is true of transformations; here is an example.

Example 2.2.14

       
                x1
                                  
The formula T  x2  =  x2 + x3  defines a transformation R x1 + x2 4  R3.
        x3 
                         x3 + x4
                x4

    Example 2.2.13 suggests that matrix multiplication is an important way of defining transformations
Rn  Rm. If A is any m × n matrix, multiplication by A gives a transformation

                            TA : Rn  Rm defined by TA(x) = Ax for every x in Rn

   Definition 2.8 Matrix Transformation TA
   TA is called the matrix transformation induced by A.

    Thus Example 2.2.13 shows that reflection in the x axis is the matrix transformation R2  R2 in-
duced by the matrix 1 0 0 -1 . Also, the transformation R : R4  R3 in Example 2.2.13 is the matrix
                                                                                   . . Matrix-Vector Multiplication

transformation induced by the matrix

                                                                               
                                                                                   x1
                           1 1 0 0                   1 1 0 0                                    x1 + x2       

                    A= 0      1          1  0  because  0       1     1  0         x2           x2 + x3
                                                                                       =
                                                          0 0 1 1  x3  x3 + x4                                

                              0011                                                 x4

Example 2.2.15

Let  R     : R2        R2  denote  counterclockwise  rotation      about    the    origin  through      radians    (that  is,
        2                                                                                            2

90)5. Show that R  is induced by the matrix 0 -1 .
                           2                                 10

Solution.

              y                             The effect of R  is to rotate the vector x = a
                                                                                                           b
                                                             2
                                            counterclockwise       through         to  produce  the  vector             shown
     R  (x) = -b                                                               2                                R  (x)
                                                                                                                2
     2              a                       in Figure 2.2.5. Since triangles 0px and 0qR  (x) are identical,

           b     q                                                                                      2

              a               x= a          we obtain R  (x) = -b . But -b = 0 -1 a ,
                                      b                                                                    10 b
                      a                                   2                 a              a
                            b
           0                                so we obtain R  (x) = Ax for all x in R2 where A = 0 -1 .
                           px                                                                                         10

                                                             2
                                            In other words, R  is the matrix transformation induced by A.
              Figure 2.2.5                                         2

    If A is the m × n zero matrix, then A induces the transformation

                             T : Rn  Rm given by T (x) = Ax = 0 for all x in Rn

This is called the zero transformation, and is denoted T = 0.
    Another important example is the identity transformation

                              1Rn : Rn  Rn given by 1Rn(x) = x for all x in Rn

That is, the action of 1Rn on x is to do nothing to it. If In denotes the n × n identity matrix, we showed in
Example 2.2.11 that Inx = x for all x in Rn. Hence 1Rn(x) = Inx for all x in Rn; that is, the identity matrix
In induces the identity transformation.

    Here are two more examples of matrix transformations with a clear geometric description.

5Radian measure for angles is based on the fact that 360  equals 2       radians.  Hence   radians  = 180     and     radians  = 90.
                                                                                                                   2
Matrix Algebra

Example 2.2.16

If a > 0, the matrix transformation T xy = axy induced by the matrix A = a 0 0 1 is called
an x-expansion of R2 if a > 1, and an x-compression if 0 < a < 1. The reason for the names is
clear in the diagram below. Similarly, if b > 0 the matrix A = 1 0 0 b gives rise to y-expansions
and y-compressions.

                y                               y     x-compression              y     x-expansion

                                  x                      21x                                           23x

                                  y                       y                                            y

             0                       x       0                          x     0                           x

                                                      a= 1   2                            a= 32

Example 2.2.17

If a is a number, the matrix transformation T x = x + ay induced by the matrix
                                                      y                 y

A = 1 a 0 1 is called an x-shear of R2 (positive if a > 0 and negative if a < 0). Its effect is

illustrated  below    when  a  =  1  and  a  =  -1.
                                  4
                                                   4

             y                                    y                                    y  Negative x-shear

                                                      Positive x-shear

                               x                                     x+ 1y                             x- 1y

                               y                                           4                                 4
                                                                        y                              y

   0                                 x                                  x           0                           x
                                               0
                                                      a= 1                                a = -1    4

                                                             4

y                                     We hasten to note that there are important geometric transformations
                                  that are not matrix transformations. For example, if w is a fixed column in
             Tw(x) =  x+2         Rn, define the transformation Tw : Rn  Rn by
                      y+1
                                                           Tw(x) = x + w for all x in Rn
              x= x
                       y          Then Tw is called translation by w. In particular, if w = 21 in R2, the

                      x
0

    Figure 2.2.6
                                                                 . . Matrix Multiplication

                        effect of Tw on x is to translate it two units to the right and one unit
                                              y

up (see Figure 2.2.6).

    The translation Tw is not a matrix transformation unless w = 0. Indeed, if Tw were induced by a matrix
A, then Ax = Tw(x) = x + w would hold for every x in Rn. In particular, taking x = 0 gives w = A0 = 0.

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                 Engage Active Learning App!

                           Vretta-Lyryx Engage is an active learning app designed to increase
                          student engagement in reading linear algebra material. The content is
                        "chunked" into small blocks, each with an interactive assessment activity

                                                   to promote comprehension.

                         Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

. Matrix Multiplication

In Section 2.2 matrix-vector products were introduced. If A is an m × n matrix, the product Ax was defined
for any n-column x in Rn as follows: If A = a1 a2 · · · an where the a j are the columns of A, and if


   x1

  x2  
      
x =  .. , Definition 2.5 reads
.

  xn

                                Ax = x1a1 + x2a2 + · · · + xnan                                     (2.5)

This was motivated as a way of describing systems of linear equations with coefficient matrix A. Indeed
every such system has the form Ax = b where b is the column of constants.

    In this section we extend this matrix-vector multiplication to a way of multiplying matrices in gen-
eral, and then investigate matrix algebra for its own sake. While it shares several properties of ordinary
arithmetic, it will soon become clear that matrix arithmetic is different in a number of ways.

    Matrix multiplication is closely related to composition of transformations.
6 Matrix Algebra

Composition and Matrix Multiplication

Sometimes two transformations "link" together as follows:

                                                 kT nS m
                                                 R - R - R
              ST

         T             S                In this case we can apply T first and then apply S, and the result is a
                                    new transformation

Rk               Rn          Rm                             S  T : Rk  Rm

                                    called the composite of S and T , defined by

                                        (S  T )(x) = S [T (x)] for all x in Rk

The action of S  T can be described as "first T then S " (note the order!)6. This new transformation

is described in the diagram. The reader will have encountered composition of ordinary functions: For

example, consider R - R - R where f (x) = x and g(x) = x + 1 for all x in R. Thengf2

                                    ( f  g)(x) = f [g(x)] = f (x + 1) = (x + 1)2
                                    (g  f )(x) = g [ f (x)] = g(x2) = x2 + 1

for all x in R.

Our concern here is with matrix transformations. Suppose that A is an m × n matrix and B is an n × k

matrix,  and  let  Rk  TB  Rn  TA   Rm  be  the  matrix  transformations  induced  by  B  and  A  respectively,  that  is:

                       -       -

                          TB(x) = Bx for all x in Rk and TA(y) = Ay for all y in Rn

Write B = b1 b2 · · · bk where b j denotes column j of B for each j. Hence each b j is an n-vector
(B is n × k) so we can form the matrix-vector product Ab j. In particular, we obtain an m × k matrix

                                                 Ab1 Ab2 · · · Abk

                                                                                   
                                                                                      x1

                                                                                          x2  
                                                                                              
with columns Ab1, Ab2, · · · , Abk. Now compute (TA  TB)(x) for any x =  ..  in R :               k

                                                                                   .

                                                                                          xk

              (TA  TB)(x) = TA [TB(x)]                                    Definition of TA  TB
                               = A(Bx)                                    A and B induce TA and TB
                                                                          Equation 2.5 above
                               = A(x1b1 + x2b2 + · · · + xkbk)            Theorem 2.2.2
                               = A(x1b1) + A(x2b2) + · · · + A(xkbk)      Theorem 2.2.2
                               = x1(Ab1) + x2(Ab2) + · · · + xk(Abk)      Equation 2.5 above
                               = Ab1 Ab2 · · · Abk x

Because x was an arbitrary vector in Rn, this shows that TA  TB is the matrix transformation induced by
the matrix Ab1 Ab2 · · · Abn . This motivates the following definition.

    6When reading the notation S  T , we read S first and then T even though the action is "first T then S ". This annoying state
of affairs results because we write T (x) for the effect of the transformation T on x, with T on the left. If we wrote this instead
as (x)T , the confusion would not occur. However the notation T (x) is well established.
                                                                . . Matrix Multiplication

Definition 2.9 Matrix Multiplication
Let A be an m × n matrix, let B be an n × k matrix, and write B = b1 b2 · · · bk where b j is
column j of B for each j. The product matrix AB is the m × k matrix defined as follows:

                         AB = A b1 b2 · · · bk = Ab1 Ab2 · · · Abk

Thus the product matrix AB is given in terms of its columns Ab1, Ab2, . . . , Abn: Column j of AB is the
matrix-vector product Ab j of A and the corresponding column b j of B. Note that each such product Ab j
makes sense by Definition 2.5 because A is m × n and each b j is in Rn (since B has n rows). Note also that
if B is a column matrix, this definition reduces to Definition 2.5 for matrix-vector multiplication.

    Given matrices A and B, Definition 2.9 and the above computation give

                                    A(Bx) = Ab1 Ab2 · · · Abn x = (AB)x

for all x in Rk. We record this for reference.

   Theorem 2.3.1
   Let A be an m × n matrix and let B be an n × k matrix. Then the product matrix AB is m × k and
   satisfies

                                           A(Bx) = (AB)x for all x in Rk

Here is an example of how to compute the product AB of two matrices using Definition 2.9.

Example 2.3.1

               2 3 5                      8 9

Compute AB if A =  1 4 7  and B =  7 2 .

               018                         61

                      8                            9

Solution. The columns of B are b1 =  7  and b2 =  2 , so Definition 2.5 gives
                                       6           1

                2 3 5   8   67                         2 3 5   9   29 

Ab1 =  1 4 7   7  =  78  and Ab2 =  1 4 7   2  =  24 
               018 6                   55             018 1     10

Hence Definition 2.9 above gives AB =     Ab1 Ab2      67 29 
                                                   =  78 24 .

                                                         55 10
8 Matrix Algebra

Example 2.3.2

If A is m × n and B is n × k, Theorem 2.3.1 gives a simple formula for the composite of the matrix
transformations TA and TB:

                                                   TA  TB = TAB

Solution. Given any x in Rk,

                                (TA  TB)(x) = TA[TB(x)]
                                                 = A[Bx]
                                                 = (AB)x
                                                 = TAB(x)

    While Definition 2.9 is important, there is another way to compute the matrix product AB that gives
a way to calculate each individual entry. In Section 2.2 we defined the dot product of two n-tuples to be
the sum of the products of corresponding entries. We went on to show (Theorem 2.2.5) that if A is an
m × n matrix and x is an n-vector, then entry j of the product Ax is the dot product of row j of A with x.
This observation was called the "dot product rule" for matrix-vector multiplication, and the next theorem
shows that it extends to matrix multiplication in general.

   Theorem 2.3.2: Dot Product Rule

   Let A and B be matrices of sizes m × n and n × k, respectively. Then the (i, j)-entry of AB is the
   dot product of row i of A with column j of B.

Proof. Write B = b1 b2 · · · bn in terms of its columns. Then Ab j is column j of AB for each j.
Hence the (i, j)-entry of AB is entry i of Ab j, which is the dot product of row i of A with b j. This proves
the theorem.

Thus to compute the (i, j)-entry of AB, proceed as follows (see the diagram):

   Go across row i of A, and down column j of B, multiply corresponding entries, and add the results.

                               A   B   AB 

                                  =                        

                                row i column j (i, j)-entry

Note that this requires that the rows of A must be the same length as the columns of B. The following rule
is useful for remembering this and for deciding the size of the product matrix AB.

Compatibility Rule
                                                                          . . Matrix Multiplication

A  B                   Let A and B denote matrices. If A is m × n and B is n × k, the product AB
                       can be formed if and only if n = n. In this case the size of the product
m × n n × k            matrix AB is m × k, and we say that AB is defined, or that A and B are

                       compatible for multiplication.

The diagram provides a useful mnemonic for remembering this. We adopt the following convention:

Convention

Whenever a product of matrices is written, it is tacitly assumed that the sizes of the factors are such that
the product is defined.

    To illustrate the dot product rule, we recompute the matrix product in Example 2.3.1.

Example 2.3.3

                  2 3 5                     8 9

Compute AB if A =  1 4 7  and B =  7 2 .

                  018                       61

Solution. Here A is 3 × 3 and B is 3 × 2, so the product matrix AB is defined and will be of size
3 × 2. Theorem 2.3.2 gives each entry of AB as the dot product of the corresponding row of A with
the corresponding column of B j that is,

    2 3 5  8 9   2·8+3·7+5·6                            2 · 9 + 3 · 2 + 5 · 1   67 29 

AB =  1 4 7   7 2  =  1 · 8 + 4 · 7 + 7 · 6             1 · 9 + 4 · 2 + 7 · 1  =  78 24 

   018 61                0·8+1·7+8·6                    0·9+1·2+8·1       55 10

Of course, this agrees with Example 2.3.1.

Example 2.3.4

Compute the (1, 3)- and (2, 4)-entries of AB where

                       3 -1 2                           2 1 6 0
                       A= 0 1 4             and B =     0 2 3 4 .

                                                        -1 0 5 8

Then compute AB.

Solution. The (1, 3)-entry of AB is the dot product of row 1 of A and column 3 of B (highlighted
in the following display), computed by multiplying corresponding entries and adding the results.

   3 -1 2               2 1 6 0
   0 14                 0 2 3 4  (1, 3)-entry = 3 · 6 + (-1) · 3 + 2 · 5 = 25

                          -1 0 5 8

Similarly, the (2, 4)-entry of AB involves row 2 of A and column 4 of B.

               3 -1 2   2 1 6 0
               0 14     0 2 3 4  (2, 4)-entry = 0 · 0 + 1 · 4 + 4 · 8 = 36

                          -1 0 5 8
6 Matrix Algebra

Since A is 2 × 3 and B is 3 × 4, the product is 2 × 4.

                  AB =  3 -1 2   2 1 6 0                  4 1 25 12
                        0 14     0 2 3 4 =              -4 2 23 36

                                   -1 0 5 8

Example 2.3.5               5
If A = 1 3 2      and B =  6 , compute A2, AB, BA, and B2 when they are defined.7

                               4

Solution. Here, A is a 1 × 3 matrix and B is a 3 × 1 matrix, so A2 and B2 are not defined. However,
the compatibility rule reads

                        A B 1 × 3 3 × 1 and B A 3 × 1 1 × 3

so both AB and BA can be formed and these are 1 × 1 and 3 × 3 matrices, respectively.

                  AB =  132     5     1·5+3·6+2·4            = 31

                                 6 =
                                   4

       5                         5 · 1 5 · 3 5 · 2   5 15 10 
BA =  6 
                        132     =  6 · 1 6 · 3 6 · 2  =  6 18 12 
          4
                                4·1 4·3 4·2                  4 12 8

    Unlike numerical multiplication, matrix products AB and BA need not be equal. In fact they need not
even be the same size, as Example 2.3.5 shows. It turns out to be rare that AB = BA (although it is by no
means impossible), and A and B are said to commute when this happens.

   Example 2.3.6
   Let A = 6 9 -4 -6 and B = 1 2 -1 0 . Compute A2, AB, BA.

   Solution. A2 = 6 9 -4 -6 6 9 -4 -6 = 0 0 0 0 , so A2 = 0 can occur even if A = 0. Next,
                                   AB = 6 9 -4 -6 1 2 -1 0 = -3 12 2 -8
                                   BA = 1 2 -1 0 6 9 -4 -6 = -2 -3 -6 -9

   Hence AB = BA, even though AB and BA are the same size.

    7As for numbers, we write A2 = A · A, A3 = A · A · A, etc. Note that A2 is defined if and only if A is of size n × n for some n.
                                                             . . Matrix Multiplication 6

Example 2.3.7
If A is any matrix, then IA = A and AI = A, and where I denotes an identity matrix of a size so that
the multiplications are defined.

Solution. These both follow from the dot product rule as the reader should verify. For a more
formal proof, write A = a1 a2 · · · an where a j is column j of A. Then Definition 2.9 and
Example 2.2.11 give

                          IA = Ia1 Ia2 · · · Ian = a1 a2 · · · an = A

If e j denotes column j of I, then Ae j = a j for each j by Example 2.2.12. Hence Definition 2.9
gives:

          AI = A e1 e2 · · · en = Ae1 Ae2 · · · Aen = a1 a2 · · · an = A

    The following theorem collects several results about matrix multiplication that are used everywhere in
linear algebra.

Theorem 2.3.3

Assume that a is any scalar, and that A, B, and C are matrices of sizes such that the indicated
matrix products are defined. Then:

1. IA = A and AI = A where I denotes an  4. (B +C)A = BA +CA.
   identity matrix.                      5. a(AB) = (aA)B = A(aB).
                                         6. (AB)T = BT AT .
2. A(BC) = (AB)C.

3. A(B +C) = AB + AC.

Proof. Condition (1) is Example 2.3.7; we prove (2), (4), and (6) and leave (3) and (5) as exercises.

2. If C = c1 c2 · · · ck in terms of its columns, then BC =  Bc1 Bc2 · · · Bck  by Defini-
   tion 2.9, so                                                Definition 2.9

                       A(BC) = A(Bc1) A(Bc2) · · · A(Bck)

= (AB)c1 (AB)c2 · · · (AB)ck)                                Theorem 2.3.1

= (AB)C                                                      Definition 2.9

4. We know (Theorem 2.2.2) that (B +C)x = Bx +Cx holds for every column x. If we write
6 Matrix Algebra

    A = a1 a2 · · · an in terms of its columns, we get                                                                          Definition 2.9
          (B +C)A = (B +C)a1 (B +C)a2 · · · (B +C)an                                                                            Theorem 2.2.2
                        = Ba1 +Ca1 Ba2 +Ca2 · · · Ban +Can                                                                      Adding Columns
                        = Ba1 Ba2 · · · Ban + Ca1 Ca2 · · · Can                                                                 Definition 2.9
                        = BA +CA

6.  As in Section 2.1, write A = [ai j] and B = [bi j], so that AT                        = [ai j] and BT              =     [bi  j]  where  a    j  =  a  ji  and

                                                                                                                                               i

    b     =  bi  j  for  all  i  and    j.  If ci j  denotes the (i,         j)-entry of BT AT , then ci j       is the dot product of row i of

      ji

    BT with column j of AT . Hence

                              ci  j  =           +            +  ·  ·  ·  +         j  =  b1ia  j1  +  b2ia  j2  +  ·  ·  ·  +  bmia  jm

                                        bi1a1 j      bi2a2 j                 bimam

                                                                                       = a j1b1i + a j2b2i + · · · + a jmbmi

    But this is the dot product of row j of A with column i of B; that is, the ( j, i)-entry of AB; that is,
    the (i, j)-entry of (AB)T . This proves (6).

    Property 2 in Theorem 2.3.3 is called the associative law of matrix multiplication. It asserts that the
equation A(BC) = (AB)C holds for all matrices (if the products are defined). Hence this product is the
same no matter how it is formed, and so is written simply as ABC. This extends: The product ABCD of
four matrices can be formed several ways--for example, (AB)(CD), [A(BC)]D, and A[B(CD)]--but the
associative law implies that they are all equal and so are written as ABCD. A similar remark applies in
general: Matrix products can be written unambiguously with no parentheses.

    However, a note of caution about matrix multiplication must be taken: The fact that AB and BA need
not be equal means that the order of the factors is important in a product of matrices. For example ABCD
and ADCB may not be equal.

Warning

    If the order of the factors in a product of matrices is changed, the product matrix may change
    (or may not be defined). Ignoring this warning is a source of many errors by students of linear
    algebra!

    Properties 3 and 4 in Theorem 2.3.3 are called distributive laws. They assert that A(B +C) = AB + AC
and (B +C)A = BA +CA hold whenever the sums and products are defined. These rules extend to more
than two terms and, together with Property 5, ensure that many manipulations familiar from ordinary
algebra extend to matrices. For example

                                        A(2B - 3C + D - 5E) = 2AB - 3AC + AD - 5AE
                                              (A + 3C - 2D)B = AB + 3CB - 2DB

Note again that the warning is in effect: For example A(B -C) need not equal AB -CA. These rules make
possible a lot of simplification of matrix expressions.
                                                                                           . . Matrix Multiplication 6

   Example 2.3.8
   Simplify the expression A(BC -CD) + A(C - B)D - AB(C - D).

   Solution.

       A(BC -CD) + A(C - B)D - AB(C - D) = A(BC) - A(CD) + (AC - AB)D - (AB)C + (AB)D
                                                        = ABC - ACD + ACD - ABD - ABC + ABD
                                                        =0

    Example 2.3.9 and Example 2.3.10 below show how we can use the properties in Theorem 2.3.2 to
deduce other facts about matrix multiplication. Matrices A and B are said to commute if AB = BA.

   Example 2.3.9
   Suppose that A, B, and C are n × n matrices and that both A and B commute with C; that is,
   AC = CA and BC = CB. Show that AB commutes with C.

   Solution. Showing that AB commutes with C means verifying that (AB)C = C(AB). The
   computation uses the associative law several times, as well as the given facts that AC = CA and
   BC = CB.

                              (AB)C = A(BC) = A(CB) = (AC)B = (CA)B = C(AB)

Example 2.3.10
Show that AB = BA if and only if (A - B)(A + B) = A2 - B2.

Solution. The following always holds:

(A - B)(A + B) = A(A + B) - B(A + B) = A2 + AB - BA - B2    (2.6)

Hence if AB = BA, then (A - B)(A + B) = A2 - B2 follows. Conversely, if this last equation holds,

then equation (2.6) becomes  A2 - B2 = A2 + AB - BA - B2

This gives 0 = AB - BA, and AB = BA follows.

    In Section 2.2 we saw (in Theorem 2.2.1) that every system of linear equations has the form

                                                           Ax = b

where A is the coefficient matrix, x is the column of variables, and b is the constant matrix. Thus the
system of linear equations becomes a single matrix equation. Matrix multiplication can yield information
about such a system.
6 Matrix Algebra

   Example 2.3.11
   Consider a system Ax = b of linear equations where A is an m × n matrix. Assume that a matrix C
   exists such that CA = In. If the system Ax = b has a solution, show that this solution must be Cb.
   Give a condition guaranteeing that Cb is in fact a solution.

   Solution. Suppose that x is any solution to the system, so that Ax = b. Multiply both sides of this
   matrix equation by C to obtain, successively,

                               C(Ax) = Cb, (CA)x = Cb, Inx = Cb, x = Cb

   This shows that if the system has a solution x, then that solution must be x = Cb, as required. But
   it does not guarantee that the system has a solution. However, if we write x1 = Cb, then

                                                  Ax1 = A(Cb) = (AC)b

   Thus x1 = Cb will be a solution if the condition AC = Im is satisfied.

    The ideas in Example 2.3.11 lead to important information about matrices; this will be pursued in the
next section.

Block Multiplication

   Definition 2.10 Block Partition of a Matrix
   It is often useful to consider matrices whose entries are themselves matrices (called blocks). A
   matrix viewed in this way is said to be partitioned into blocks.

For example, writing a matrix B in the form

                  B = b1 b2 · · · bk where the b j are the columns of B

is such a block partition of B. Here is another example.
    Consider the matrices

     1 0 0 0 0                                             4 -2 
A =  2 -1  0 1 4 2 1  0 0 0  =
                                             I2 023                    5    3  = 6  X
        3 1 -1 7 5                           PQ                             0  Y
                                                          and B =  7
                                                                       
                                                                        -1

                                                          16

where the blocks have been labelled as indicated. This is a natural way to partition A into blocks in view of

the blocks I2 and 023 that occur. This notation is particularly useful when we are multiplying the matrices
A and B because the product AB can be computed in block form as follows:

AB = I 0                                                             4 -2 
          PQ      YX = PX + QY IX + 0Y = PX + QY X =  30 8   5 6 

                                                                        8 27
                                                                        . . Matrix Multiplication 6

This is easily checked to be the product AB, computed in the conventional manner.
    In other words, we can compute the product AB by ordinary matrix multiplication, using blocks as

entries. The only requirement is that the blocks be compatible. That is, the sizes of the blocks must be
such that all (matrix) products of blocks that occur make sense. This means that the number of columns
in each block of A must equal the number of rows in the corresponding block of B.

   Theorem 2.3.4: Block Multiplication

   If matrices A and B are partitioned compatibly into blocks, the product AB can be computed by
   matrix multiplication using blocks as entries.

We omit the proof.                                                                   is a matrix where
    We have been using two cases of block multiplication. If B = b1 b2 · · · bk

the b j are the columns of B, and if the matrix product AB is defined, then we have

                AB = A b1 b2 · · · bk = Ab1 Ab2 · · · Abk

This is Definition 2.9 and is a block multiplication where A = [A] has only one block. As another illustra-

tion,
                                                          
                                                              x1

                                       x2  
                                           
                Bx = b1 b2 · · · bk  ..  = x1b1 + x2b2 + · · · + xkbk
                                    .

                                       xk

where x is any k × 1 column matrix (this is Definition 2.5).

    It is not our intention to pursue block multiplication in detail here. However, we give one more example
because it will be used below.

Theorem 2.3.5
Suppose matrices A = B X 0 C and A1 = B1 X1 0 C1 are partitioned as shown where B and B1
are square matrices of the same size, and C and C1 are also square of the same size. These are
compatible partitionings and block multiplication gives

                AA1 = B X 0 C B1 X1 0 C1 = BB1 BX1 + XC1 0 CC1

Example 2.3.12

Obtain a formula for Ak where A = I X 0 0 is square and I is an identity matrix.

Solution. We have A2 = I X 0 0  IX  =      I2 IX +X0                 =  IX  = A. Hence
                                                                  2
                                00         00                           00

A3 = AA2 = AA = A2 = A. Continuing in this way, we see that Ak = A for every k  1.
66 Matrix Algebra

    Block multiplication has theoretical uses as we shall see. However, it is also useful in computing
products of matrices in a computer with limited memory capacity. The matrices are partitioned into blocks
in such a way that each product of blocks can be handled. Then the blocks are stored in auxiliary memory
and their products are computed one by one.

Directed Graphs

The study of directed graphs illustrates how matrix multiplication arises in ways other than the study of
linear equations or matrix transformations.

    A directed graph consists of a set of points (called vertices) connected by arrows (called edges). For

example, the vertices could represent cities and the edges available flights. If the graph has n vertices
v1, v2, . . . , vn, the adjacency matrix A = ai j is the n × n matrix whose (i, j)-entry ai j is 1 if there is an
edge from v j to vi (note the order), and zero otherwise. For example, the adjacency matrix of the directed

                        1 1 0
graph shown is A =  1 0 1 .

                           100

                   A path of length r (or an r-path) from vertex j to vertex i is a sequence

v1                 v2 of r edges leading from v j to vi. Thus v1  v2  v1  v1  v3 is a 4-path
                   from v1 to v3 in the given graph. The edges are just the paths of length 1,
                   so the (i, j)-entry ai j of the adjacency matrix A is the number of 1-paths

    v3             from v j to vi. This observation has an important extension:

Theorem 2.3.6

If A is the adjacency matrix of a directed graph with n vertices, then the (i, j)-entry of Ar is the
number of r-paths v j  vi.

As an illustration, consider the adjacency matrix A in the graph shown. Then

         1 1 0           2 1 1                   4 2 1
                   A2 =  2 1 0  ,  and A3 =  3 2 1 
    A =  1 0 1 ,
            100              110                     211

Hence, since the (2, 1)-entry of A2 is 2, there are two 2-paths v1  v2 (in fact they are v1  v1  v2 and
v1  v3  v2). Similarly, the (2, 3)-entry of A2 is zero, so there are no 2-paths v3  v2, as the reader
can verify. The fact that no entry of A3 is zero shows that it is possible to go from any vertex to any other
vertex in exactly three steps.

    To see why Theorem 2.3.6 is true, observe that it asserts that

                   the (i, j)-entry of Ar equals the number of r-paths v j  vi                        (2.7)

holds for each r  1. We proceed by induction on r (see Appendix C). The case r = 1 is the definition of
the adjacency matrix. So assume inductively that (2.7) is true for some r  1; we must prove that (2.7)
also holds for r + 1. But every (r + 1)-path v j  vi is the result of an r-path v j  vk for some k, followed
by a 1-path vk  vi. Writing A = ai j and Ar = bi j , there are bk j paths of the former type (by induction)
and aik of the latter type, and so there are aikbk j such paths in all. Summing over k, this shows that there
are

                                ai1b1 j + ai2b2 j + · · · + ainbn j (r + 1)-paths v j  vi
                                                                                                  . . Matrix Inverses 6

But this sum is the dot product of the ith row ai1 ai2 · · · ain of A with the jth column b1 j b2 j · · · bn j T
of Ar. As such, it is the (i, j)-entry of the matrix product ArA = Ar+1. This shows that (2.7) holds for
r + 1, as required.

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                            Engage Active Learning App!

                                      Vretta-Lyryx Engage is an active learning app designed to increase
                                     student engagement in reading linear algebra material. The content is
                                   "chunked" into small blocks, each with an interactive assessment activity

                                                              to promote comprehension.

                                    Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

   . Matrix Inverses

Three basic operations on matrices, addition, multiplication, and subtraction, are analogs for matrices of
the same operations for numbers. In this section we introduce the matrix analog of numerical division.

   To begin, consider how a numerical equation ax = b is solved when a and b are known numbers. If
                                                                                                                            a-1     1
a  =  0,  there  is  no  solution  (unless  b  =  0).  But  if  a  =  0,  we  can  multiply  both  sides  by  the  inverse       =  a

to obtain the solution x = a-1b. Of course multiplying by a-1 is just dividing by a, and the property of
a-1 that makes this work is that a-1a = 1. Moreover, we saw in Section 2.2 that the role that 1 plays in

arithmetic is played in matrix algebra by the identity matrix I. This suggests the following definition.

   Definition 2.11 Matrix Inverses
   If A is a square matrix, a matrix B is called an inverse of A if and only if

                                               AB = I and BA = I
   A matrix A that has an inverse is called an invertible matrix.8
68 Matrix Algebra

Example 2.4.1      -1 1  is an inverse of A =  0 1 1 1 .
Show that B =        10

Solution. Compute AB and BA.

AB = 0 1 1 1 -1 1 1 0 = 1 0 0 1 BA = -1 1 1 0                 0 1 1 1 = 1 0 0 1

Hence AB = I = BA, so B is indeed an inverse of A.

Example 2.4.2            has no inverse.
Show that A = 0 0 1 3

Solution. Let B = a b denote an arbitrary 2 × 2 matrix. Then
                         cd

                                  AB = 0 0 1 3 a b c d = 0 0 a + 3c b + 3d

   so AB has a row of zeros. Hence AB cannot equal I for any B.

    The argument in Example 2.4.2 shows that no zero matrix has an inverse. But Example 2.4.2 also
shows that, unlike arithmetic, it is possible for a nonzero matrix to have no inverse. However, if a matrix
does have an inverse, it has only one.

   Theorem 2.4.1
   If B and C are both inverses of A, then B = C.

Proof. Since B and C are both inverses of A, we have CA = I = AB. Hence
                                         B = IB = (CA)B = C(AB) = CI = C

    If A is an invertible matrix, the (unique) inverse of A is denoted A-1. Hence A-1 (when it exists) is a
square matrix of the same size as A with the property that

                                              AA-1 = I and A-1A = I

These equations characterize A-1 in the following sense:

       Inverse Criterion: If somehow a matrix B can be found such that AB = I and BA = I, then A
       is invertible and B is the inverse of A; in symbols, B = A-1.

    8Only square matrices have inverses. Even though it is plausible that nonsquare matrices A and B could exist such that
AB = Im and BA = In, where A is m × n and B is n × m, we claim that this forces n = m. Indeed, if m < n there exists a nonzero
column x such that Ax = 0 (by Theorem 1.3.1), so x = Inx = (BA)x = B(Ax) = B(0) = 0, a contradiction. Hence m  n.
Similarly, the condition AB = Im implies that n  m. Hence m = n so A is square.
                                                                                                  . . Matrix Inverses 6

This is a way to verify that the inverse of a matrix exists. Example 2.4.3 and Example 2.4.4 offer illustra-
tions.

Example 2.4.3         , show that A3 = I and so find A-1.
If A = 0 -1 1 -1

Solution. We have A2 = 0 -1 1 -1 0 -1 1 -1 = -1 1 -1 0 , and so

                            A3 = A2A = -1 1 -1 0 0 -1 1 -1 = 1 0 0 1 = I

Hence A3 = I, as asserted. This can be written as A2A = I = AA2, so it shows that A2 is the inverse
of A. That is, A-1 = A2 = -1 1 -1 0 .

    The next example presents a useful formula for the inverse of a 2 × 2 matrix A = a b when it
                                                                                                             cd

exists. To state it, we define the determinant det A and the adjugate adj A of the matrix A as follows:

                        det a b c d = ad - bc, and adj a b c d = d -b -c a

Example 2.4.4
If A = a b , show that A has an inverse if and only if det A = 0, and in this case

           cd
                                                 A-1 = 1 adj A

                                                                             det A

Solution. For convenience, write e = det A = ad - bc and B = adj A = d -b                                . Then
                                                                                          -c a

AB  =  eI  =  BA  as  the  reader  can  verify.  So  if  e  =  0,  scalar  multiplication  by  1  gives
                                                                                               e

                                                 A(  1  B)  =  I  =  (  1  B)A
                                                     e                  e

Hence A is invertible and A-1 = 1e B. Thus it remains only to show that if A-1 exists, then e = 0.
We prove this by showing that assuming e = 0 leads to a contradiction. In fact, if e = 0, then
AB = eI = 0, so left multiplication by A-1 gives A-1AB = A-10; that is, IB = 0, so B = 0. But this
implies that a, b, c, and d are all zero, so A = 0, contrary to the assumption that A-1 exists.

As an illustration, if A =    24        then det A = 2 · 8 - 4 · (-3) = 28 = 0. Hence A is invertible and
                            -3 8

-1     1              1 8 -4       , as the reader is invited to verify.
A = det A adj A = 28       32
       Matrix Algebra

    The determinant and adjugate will be defined in Chapter 3 for any square matrix, and the conclusions
in Example 2.4.4 will be proved in full generality.

Inverses and Linear Systems

Matrix inverses can be used to solve certain systems of linear equations. Recall that a system of linear
equations can be written as a single matrix equation

                                                           Ax = b

where A and b are known and x is to be determined. If A is invertible, we multiply each side of the equation
on the left by A-1 to get

                                                      A-1Ax = A-1b
                                                            Ix = A-1b
                                                             x = A-1b

This gives the solution to the system of equations (the reader should verify that x = A-1b really does
satisfy Ax = b). Furthermore, the argument shows that if x is any solution, then necessarily x = A-1b, so
the solution is unique. Of course the technique works only when the coefficient matrix A has an inverse.
This proves Theorem 2.4.2.

   Theorem 2.4.2
   Suppose a system of n equations in n variables is written in matrix form as

                                                          Ax = b

   If the n × n coefficient matrix A is invertible, the system has the unique solution
                                                         x = A-1b

Example 2.4.5                                              5x7 1 - 3x2 = -4 x1 + 4x2 = 8 .
Use Example 2.4.4 to solve the system

Solution. In matrix form this is Ax = b where A = 5 -3 7 4                                       ,x=      x1 x2 , and b = -48  . Then
                                                                                                    4   35 by Example 2.4.4.   Thus
det  A  =  5  ·  4  -  (-3)    ·  7  =   41,  so  A  is  invertible  and  A-1  =            1
                                                                                            41    -7     8
                                                                                                        68
Theorem 2.4.2 gives                                                                             =1

                                                -1 1 4 3 -4                                         41
                                         x = A b = 41
                                                           -7 5                8

so  the  solution      is  x1  =     8   and  x2  =  68 .
                                     41
                                                     41
                                                                                                  . . Matrix Inverses

An Inversion Method

If a matrix A is n × n and invertible, it is desirable to have an efficient technique for finding the inverse.
The following procedure will be justified in Section 2.5.

   Theorem: Matrix Inversion Algorithm
   If A is an invertible (square) matrix, there exists a sequence of elementary row operations that carry
   A to the identity matrix I of the same size, written A  I. This same series of row operations
   carries I to A-1; that is, I  A-1. The algorithm can be summarized as follows:

                                                    A I  I A-1

   where the row operations on A and I are carried out simultaneously.

Example 2.4.6

Use the inversion algorithm to find the inverse of the matrix

                                                     2 7 1
                                               A =  1 4 -1 

                                                        13 0

Solution. Apply elementary row operations to the double matrix

           2 7 1 1 0 0

A I =  1 4 -1 0 1 0 
               13 0001

so as to carry A to I. First interchange rows 1 and 2.

 1 4 -1 0 1 0 
2 7 1 1 0 0

   13 0001

Next subtract 2 times row 1 from row 2, and subtract row 1 from row 3.

                                           1 4 -1 0 1 0 
                                           0 -1 3 1 -2 0 

                                             0 -1 1 0 -1 1

Continue to reduced row-echelon form.

                                           1 0 11 4 -7 0 
                                           0 1 -3 -1 2 0 

                                             0 0 -2 -1 1 1

  1 0 0 -3 -3 11                                               

        2 2 2
        1                                               1 -3 
 0 1 0                                                          
        2 22

                      
   001
        1 -1 -1
        22 2
Matrix Algebra

 -3 -3 11 
Hence A-1 = 1  1
                          1 -3 , as is readily verified.
                       2
                1 -1 -1

    Given any n × n matrix A, Theorem 1.2.1 shows that A can be carried by elementary row operations to
a matrix R in reduced row-echelon form. If R = I, the matrix A is invertible (this will be proved in the next
section), so the algorithm produces A-1. If R = I, then R has a row of zeros (it is square), so no system of
linear equations Ax = b can have a unique solution. But then A is not invertible by Theorem 2.4.2. Hence,
the algorithm is effective in the sense conveyed in Theorem 2.4.3.

   Theorem 2.4.3
   If A is an n × n matrix, either A can be reduced to I by elementary row operations or it cannot. In
   the first case, the algorithm produces A-1; in the second case, A-1 does not exist.

Properties of Inverses

The following properties of an invertible matrix are used everywhere.

   Example 2.4.7: Cancellation Laws
   Let A be an invertible matrix. Show that:

       1. If AB = AC, then B = C.
       2. If BA = CA, then B = C.

   Solution. Given the equation AB = AC, left multiply both sides by A-1 to obtain A-1AB = A-1AC.
   Thus IB = IC, that is B = C. This proves (1) and the proof of (2) is left to the reader.

Properties (1) and (2) in Example 2.4.7 are described by saying that an invertible matrix can be "left
cancelled" and "right cancelled", respectively. Note however that "mixed" cancellation does not hold in
general: If A is invertible and AB = CA, then B and C may not be equal, even if both are 2 × 2. Here is a
specific example:

                                    A = 1 1 0 1 , B = 0 0 1 2 , C = 1 1 1 1
Sometimes the inverse of a matrix is given by a formula. Example 2.4.4 is one illustration; Example 2.4.8
and Example 2.4.9 provide two more. The idea is the Inverse Criterion: If a matrix B can be found such
that AB = I = BA, then A is invertible and A-1 = B.

   Example 2.4.8
   If A is an invertible matrix, show that the transpose AT is also invertible. Show further that the
   inverse of AT is just the transpose of A-1; in symbols, (AT )-1 = (A-1)T .
                                                                                              . . Matrix Inverses

Solution. A-1 exists (by assumption). Its transpose (A-1)T is the candidate proposed for the
inverse of AT . Using the inverse criterion, we test it as follows:

                                         AT (A-1)T = (A-1A)T = IT = I
                                         (A-1)T AT = (AA-1)T = IT = I
Hence (A-1)T is indeed the inverse of AT ; that is, (AT )-1 = (A-1)T .

Example 2.4.9
If A and B are invertible n × n matrices, show that their product AB is also invertible and
(AB)-1 = B-1A-1.
Solution. We are given a candidate for the inverse of AB, namely B-1A-1. We test it as follows:

                           (B-1A-1)(AB) = B-1(A-1A)B = B-1IB = B-1B = I
                           (AB)(B-1A-1) = A(BB-1)A-1 = AIA-1 = AA-1 = I

Hence B-1A-1 is the inverse of AB; in symbols, (AB)-1 = B-1A-1.

We now collect several basic properties of matrix inverses for reference.

Theorem 2.4.4
All the following matrices are square matrices of the same size.

1. I is invertible and I-1 = I.

2. If A is invertible, so is A-1, and (A-1)-1 = A.

3. If A and B are invertible, so is AB, and (AB)-1 = B-1A-1.

4. If A1, A2, . . . , Ak are all invertible, so is their product A1A2 · · · Ak, and

                                 (A1A2  ·  ·  ·  Ak)-1  =  Ak-1  ·  ·  ·  A-1A-1.

                                                                            21

5. If A is invertible, so is Ak for any k  1, and (Ak)-1 = (A-1)k.
6. If A is invertible and a = 0 is a number, then aA is invertible and (aA)-1 = 1aA-1.
7. If A is invertible, so is its transpose AT , and (AT )-1 = (A-1)T .

Proof.
   1. This is an immediate consequence of the fact that I2 = I.

   2. The equations AA-1 = I = A-1A show that A is the inverse of A-1; in symbols, (A-1)-1 = A.
Matrix Algebra

3. This is Example 2.4.9.

4. Use induction on k. If k = 1, there is nothing to prove, and if k = 2, the result is property 3. If
                                       (A1A2 · · · Ak-1)-1     A-1 · · · A-1A-1.
k  >  2,  assume  inductively    that                       =  k-1  21             We apply this fact together

with property 3 as follows:

                                 [A1A2 · · · Ak-1Ak]-1 = [(A1A2 · · · Ak-1) Ak]-1

                                        = Ak-1 (A1A2 · · · Ak-1)-1

                                        = A-1 A-1 · · · A-1A-1
                                                            k  k-1  21

So the proof by induction is complete.

5. This is property 4 with A1 = A2 = · · · = Ak = A.

6. This is left as Exercise ??.

   7. This is Example 2.4.8.

    The reversal of the order of the inverses in properties 3 and 4 of Theorem 2.4.4 is a consequence of
the fact that matrix multiplication is not commutative. Another manifestation of this comes when matrix
equations are dealt with. If a matrix equation B = C is given, it can be left-multiplied by a matrix A to yield
AB = AC. Similarly, right-multiplication gives BA = CA. However, we cannot mix the two: If B = C, it
need not be the case that AB = CA even if A is invertible, for example, A = 1 1 0 1 , B = 0 0 1 0 = C.

    Part 7 of Theorem 2.4.4 together with the fact that (AT )T = A gives

Corollary 2.4.1
A square matrix A is invertible if and only if AT is invertible.

Example 2.4.10             2 1 -1 0 .
Find A if (AT - 2I)-1 =

Solution. By Theorem 2.4.4(2) and Example 2.4.4, we have

                  (AT - 2I) =    AT - 2I -1 -1 =              21    -1    0 -1
                                                            -1 0          12
                                                                       =

Hence AT = 2I + 0 -1 1 2 = 2 -1 1 4 , so A = 2 1 -1 4 by Theorem 2.4.4(7).

    The following important theorem collects a number of conditions all equivalent9 to invertibility. It will
be referred to frequently below.

    9If p and q are statements, we say that p implies q (written p  q) if q is true whenever p is true. The statements are called
equivalent if both p  q and q  p (written p  q, spoken "p if and only if q"). See Appendix B.
                                                                                                  . . Matrix Inverses

    Theorem 2.4.5: Inverse Theorem
    The following conditions are equivalent for an n × n matrix A:

       1. A is invertible.

       2. The homogeneous system Ax = 0 has only the trivial solution x = 0.

       3. A can be carried to the identity matrix In by elementary row operations.

       4. The system Ax = b has at least one solution x for every choice of column b.

       5. There exists an n × n matrix C such that AC = In.

Proof. We show that each of these conditions implies the next, and that (5) implies (1).
    (1)  (2). If A-1 exists, then Ax = 0 gives x = Inx = A-1Ax = A-10 = 0.
    (2)  (3). Assume that (2) is true. Certainly A  R by row operations where R is a reduced, row-

echelon matrix. It suffices to show that R = In. Suppose that this is not the case. Then R has a row
of zeros (being square). Now consider the augmented matrix A 0 of the system Ax = 0. Then

  A 0  R 0 is the reduced form, and R 0 also has a row of zeros. Since R is square there
must be at least one nonleading variable, and hence at least one parameter. Hence the system Ax = 0 has
infinitely many solutions, contrary to (2). So R = In after all.

    (3)  (4). Consider the augmented matrix A b of the system Ax = b. Using (3), let A  In by a
sequence of row operations. Then these same operations carry A b  In c for some column c.
Hence the system Ax = b has a solution (in fact unique) by gaussian elimination. This proves (4).

    (4)  (5). Write In = e1 e2 · · · en where e1, e2, . . . , en are the columns of In. For each
j = 1, 2, . . . , n, the system Ax = e j has a solution c j by (4), so Ac j = e j. Now let C = c1 c2 · · · cn
be the n × n matrix with these matrices c j as its columns. Then Definition 2.9 gives (5):

              AC = A c1 c2 · · · cn = Ac1 Ac2 · · · Acn = e1 e2 · · · en = In

(5)  (1). Assume that (5) is true so that AC = In for some matrix C. Then Cx = 0 implies x = 0 (because
x = Inx = ACx = A0 = 0). Thus condition (2) holds for the matrix C rather than A. Hence the argument
above that (2)  (3)  (4)  (5) (with A replaced by C) shows that a matrix C exists such that CC = In.
But then

                                       A = AIn = A(CC) = (AC)C = InC = C
Thus CA = CC = In which, together with AC = In, shows that C is the inverse of A. This proves (1).

    The proof of (5)  (1) in Theorem 2.4.5 shows that if AC = I for square matrices, then necessarily
CA = I, and hence that C and A are inverses of each other. We record this important fact for reference.

    Corollary 2.4.2
    If A and C are square matrices such that AC = I, then also CA = I. In particular, both A and C are
    invertible, C = A-1, and A = C-1.

    Here is a quick way to remember Corollary 2.4.2. If A is a square matrix, then
6 Matrix Algebra

1. If AC = I then C = A-1.
2. If CA = I then C = A-1.

Observe that Corollary 2.4.2 is false if A and C are not square matrices. For example, we have

                121    -1 1                           -1 1 
                            1 -1  = I2 but  1 -1  1 2 1 = I3
                111  0 1                                    0 1 111

In fact, it is verified in the footnote on page 68 that if AB = Im and BA = In, where A is m × n and B is
n × m, then m = n and A and B are (square) inverses of each other.

    An n × n matrix A has rank n if and only if (3) of Theorem 2.4.5 holds. Hence

Corollary 2.4.3
An n × n matrix A is invertible if and only if rank A = n.

Here is a useful fact about inverses of block matrices.

Example 2.4.11

Let P =  AX       and Q =   A0  be block matrices where A is m × m and B is n × n (possibly
m = n).  0B                 YB

a. Show that P is invertible if and only if A and B are both invertible. In this case, show that

                                P-1 =    A-1 -A-1X B-1
                                                            B-1
                                            0

b. Show that Q is invertible if and only if A and B are both invertible. In this case, show that

                                Q-1 =                A-1         0
                                         -B-1YA-1 B-1

Solution. We do (a.) and leave (b.) for the reader.

a. If A-1 and B-1 both exist, write R =  A-1 -A-1X B-1              . Using block multiplication, one
                                                          B-1
                                         0

verifies that PR = Im+n = RP, so P is invertible, and P-1 = R. Conversely, suppose that P is

invertible, and write P-1 = C V in block form, where C is m × m and D is n × n.
                                    WD

Then the equation PP-1 = In+m becomes

                  AX        C V W D = AC + XW AV + X D BW BD = Im+n = Im 0 0 In
                  0B
                                                                                      . . Matrix Inverses

using block notation. Equating corresponding blocks, we find
                             AC + XW = Im, BW = 0, and BD = In

Hence B is invertible because BD = In (by Corollary 2.4.1), then W = 0 because BW = 0,
and finally, AC = Im (so A is invertible, again by Corollary 2.4.1).

Inverses of Matrix Transformations

Let T = TA : Rn  Rn denote the matrix transformation induced by the n × n matrix A. Since A is square,
it may very well be invertible, and this leads to the question:

What does it mean geometrically for T that A is invertible?

To answer this, let T  = TA-1 : Rn  Rn denote the transformation induced by A-1. Then

T  [T (x)] = A-1 [Ax] = Ix = x

T [T (x)] = A A-1x = Ix = x     for all x in Rn                                                            (2.8)

The first of these equations asserts that, if T carries x to a vector T (x), then T  carries T (x) right back to
x; that is T  "reverses" the action of T . Similarly T "reverses" the action of T . Conditions (2.8) can be

stated compactly in terms of composition:

T   T = 1Rn and T  T  = 1Rn                                                                                (2.9)

When these conditions hold, we say that the matrix transformation T  is an inverse of T , and we have
shown that if the matrix A of T is invertible, then T has an inverse (induced by A-1).

    The converse is also true: If T has an inverse, then its matrix A must be invertible. Indeed, suppose
S : Rn  Rn is any inverse of T , so that S  T = 1Rn and T  S = 1Rn. It can be shown that S is also a matrix
transformation. If B is the matrix of S, we have

BAx = S [T (x)] = (S  T )(x) = 1Rn(x) = x = Inx for all x in Rn

It follows by Theorem 2.2.6 that BA = In, and a similar argument shows that AB = In. Hence A is invertible
with A-1 = B. Furthermore, the inverse transformation S has matrix A-1, so S = T  using the earlier
notation. This proves the following important theorem.

Theorem 2.4.6
Let T : Rn  Rn denote the matrix transformation induced by an n × n matrix A. Then

                                A is invertible if and only if T has an inverse.
In this case, T has exactly one inverse (which we denote as T -1), and T -1 : Rn  Rn is the
transformation induced by the matrix A-1. In other words

                                                   (TA)-1 = TA-1
8 Matrix Algebra

The geometrical relationship between T and T -1 is embodied in equations (2.8) above:
                            T -1 [T (x)] = x and T T -1(x) = x for all x in Rn

These equations are called the fundamental identities relating T and T -1. Loosely speaking, they assert
that each of T and T -1 "reverses" or "undoes" the action of the other.

    This geometric view of the inverse of a linear transformation provides a new way to find the inverse of
a matrix A. More precisely, if A is an invertible matrix, we proceed as follows:

   1. Let T be the linear transformation induced by A.
   2. Obtain the linear transformation T -1 which "reverses" the action of T .
   3. Then A-1 is the matrix of T -1.

Here is an example.

Example 2.4.12

                                       Find the inverse of A = 0 1 1 0      by viewing it as a linear
                                       transformation R2  R2.
y Q1 x = y
                y    x

                     y = x Solution. If x = xy the vector Ax = 0 1 x 1 0 y = yx

                        x              is the result of reflecting x in the line y = x (see the diagram).
                                       Hence, if Q1 : R2  R2 denotes reflection in the line y = x, then
                        y

       0                     x         A is the matrix of Q1. Now observe that Q1 reverses itself because
                                                                                                           -1
                                       reflecting a vector x twice results in x. Consequently Q1 = Q1.
Since  A-1  is  the  matrix  of    -1  and     is  the  matrix  of  Q,  it  follows  that  A-1     A.  Of  course  this
                                            A                                                   =
                                 Q1
conclusion is clear by simply observing directly that A2 = I, but the geometric method can often

work where these other methods may be less straightforward.
       . . Elementary Matrices

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                Engage Active Learning App!

          Vretta-Lyryx Engage is an active learning app designed to increase
         student engagement in reading linear algebra material. The content is
       "chunked" into small blocks, each with an interactive assessment activity

                                  to promote comprehension.

        Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

 . Elementary Matrices

It is now clear that elementary row operations are important in linear algebra: They are essential in solving
linear systems (using the gaussian algorithm) and in inverting a matrix (using the matrix inversion algo-
rithm). It turns out that they can be performed by left multiplying by certain invertible matrices. These
matrices are the subject of this section.

   Definition 2.1 Elementary Matrices

   An n × n matrix E is called an elementary matrix if it can be obtained from the identity matrix In
   by a single elementary row operation (called the operation corresponding to E). We say that E is
   of type I, II, or III if the operation is of that type (see Definition 1.2).

Hence  E1 = 0 1 1 0 , E2 = 1 0 0 9 , and E3 = 1 5 0 1

are elementary of types I, II, and III, respectively, obtained from the 2 × 2 identity matrix by interchanging
rows 1 and 2, multiplying row 2 by 9, and adding 5 times row 2 to row 1.

Suppose now that the matrix A = a b c p q r is left multiplied by the above elementary matrices E1,
8 Matrix Algebra

E2, and E3. The results are:

                  E1A = 0 1 1 0  abc = pqr
                                 pqr                       abc

                  E2A = 1 0 0 9 a b c p q r = a b c 9p 9q 9r

                  E3A = 1 5 0 1  a b c = a + 5p b + 5q c + 5r
                                 pqr                       p            q  r

In each case, left multiplying A by the elementary matrix has the same effect as doing the corresponding
row operation to A. This works in general.

Lemma 2.5.1: 10

If an elementary row operation is performed on an m × n matrix A, the result is EA where E is the
elementary matrix obtained by performing the same operation on the m × m identity matrix.

Proof. We prove it for operations of type III; the proofs for types I and II are left as exercises. Let E be the
elementary matrix corresponding to the operation that adds k times row p to row q = p. The proof depends
on the fact that each row of EA is equal to the corresponding row of E times A. Let K1, K2, . . . , Km denote
the rows of Im. Then row i of E is Ki if i = q, while row q of E is Kq + kKp. Hence:

                        If i = q then row i of EA = KiA = (row i of A).
                        Row q of EA = (Kq + kKp)A = KqA + k(KpA)

                                                           = (row q of A) plus k (row p of A).

Thus EA is the result of adding k times row p of A to row q, as required.

    The effect of an elementary row operation can be reversed by another such operation (called its inverse)
which is also elementary of the same type (see the discussion following (Example 1.1.3). It follows that
each elementary matrix E is invertible. In fact, if a row operation on I produces E, then the inverse
operation carries E back to I. If F is the elementary matrix corresponding to the inverse operation, this
means FE = I (by Lemma 2.5.1). Thus F = E-1 and we have proved

   Lemma 2.5.2

   Every elementary matrix E is invertible, and E-1 is also a elementary matrix (of the same type).
   Moreover, E-1 corresponds to the inverse of the row operation that produces E.

The following table gives the inverse of each type of elementary row operation:

Type              Operation                        Inverse Operation

  I       Interchange rows p and q             Interchange rows p and q
  II      Multiply row p by k = 0            Multiply row p by 1/k, k = 0
 III  Add k times row p to row q = p  Subtract k times row p from row q, q = p

Note that elementary matrices of type I are self-inverse.

10A lemma is an auxiliary theorem used in the proof of other theorems.
                                                               . . Elementary Matrices 8

Example 2.5.1
Find the inverse of each of the elementary matrices

                       0 1 0           1 0 0                            1 0 5
                 E1 =  1 0 0  ,  E2 =  0 1 0  ,           and E3 =  0 1 0  .

                           001             009                             001

Solution. E1, E2, and E3 are of type I, II, and III respectively, so the table gives

                 0 1 0           1 0 0                          1 0 -5 
E-1 =  1                         E-1 =  0                      E-1 =  0
                    0  0  = E1,            1         0 ,  and                         1  0 .
  1                                2                             3
                 001             00 1                          00 1
                                                     9

Inverses and Elementary Matrices

Suppose that an m × n matrix A is carried to a matrix B (written A  B) by a series of k elementary row
operations. Let E1, E2, . . . , Ek denote the corresponding elementary matrices. By Lemma 2.5.1, the
reduction becomes

                    A  E1A  E2E1A  E3E2E1A  · · ·  EkEk-1 · · · E2E1A = B

In other words,

                        A  UA = B where U = EkEk-1 · · · E2E1

 The matrix U = EkEk-1 · · · E2E1 is invertible, being a product of invertible matrices by Lemma 2.5.2.
Moreover, U can be computed without finding the Ei as follows: If the above series of operations carrying
A  B is performed on Im in place of A, the result is Im  U Im = U . Hence this series of operations carries
the block matrix A Im  B U . This, together with the above discussion, proves

Theorem 2.5.1
Suppose A is m × n and A  B by elementary row operations.

   1. B = UA where U is an m × m invertible matrix.
   2. U can be computed by A Im  B U using the operations carrying A  B.
   3. U = EkEk-1 · · · E2E1 where E1, E2, . . . , Ek are the elementary matrices corresponding (in

       order) to the elementary row operations carrying A to B.

Example 2.5.2       , express the reduced row-echelon form R of A as R = UA where U is invertible.
If A = 2 3 1 1 2 1
8 Matrix Algebra

Solution. Reduce the double matrix A I  R U as follows:

            A I = 2 3 1 1 0 1 2 1 0 1  1 2 1 0 1 2 3 1 1 0  1 2 1 0 1 0 -1 -1 1 -2
                                                                            1 0 -1 2 -3 0 1 1 -1 2

Hence R = 1 0 -1 0 1 1 and U = 2 -3 -1 2 .

    Now suppose that A is invertible. We know that A  I by Theorem 2.4.5, so taking B = I in Theo-
rem 2.5.1 gives A I  I U where I = UA. Thus U = A-1, so we have A I  I A-1 .
This is the matrix inversion algorithm in Section 2.4. However, more is true: Theorem 2.5.1 gives
A-1 = U = EkEk-1 · · · E2E1 where E1, E2, . . . , Ek are the elementary matrices corresponding (in order) to
the row operations carrying A  I. Hence

                        -1 -1                         -1   -1 -1   -1 -1                            (2.10)
                  A= A         = (EkEk-1 · · · E2E1) = E1 E2 · · · Ek-1Ek

By Lemma 2.5.2, this shows that every invertible matrix A is a product of elementary matrices. Since
elementary matrices are invertible (again by Lemma 2.5.2), this proves the following important character-
ization of invertible matrices.

Theorem 2.5.2
A square matrix is invertible if and only if it is a product of elementary matrices.

    It follows from Theorem 2.5.1 that A  B by row operations if and only if B = UA for some invertible
matrix U . In this case we say that A and B are row-equivalent. (See Exercise ??.)

Example 2.5.3

Express A = -2 3 1 0 as a product of elementary matrices.

Solution. Using Lemma 2.5.1, the reduction of A  I is as follows:

A = -2 3 1 0  E1A = 1 0 -2 3  E2E1A = 1 0 0 3  E3E2E1A = 1 0 0 1

where the corresponding elementary matrices are

                           E1 = 0 1 1 0 ,  E2 = 1 0 2 1 ,  E3 =    10
Hence (E3 E2 E1)A = I, so:                                         01

                                                                        3

               A = (E3E2E1)-1 = E-1E-1E-1 =           01     10            10
                                                      10   -2 1            03
                                                 123
                                                                        . . Elementary Matrices 8

Smith Normal Form

Let A be an m × n matrix of rank r, and let R be the reduced row-echelon form of A. Theorem 2.5.1 shows

that R = UA where U is invertible, and that U can be found from A Im  R U .

The matrix R has r leading ones (since rank A = r) so, as R is reduced, the n × m matrix RT con-

tains each row of Ir in the first r columns.  Thus row operations will carry RT  Ir0       0 . Hence
                                                                                           0 n×m
Theorem 2.5.1 (again) shows that     Ir 0          = U1RT where U1 is an n × n invertible  matrix. Writing
V = U1T , we obtain                  00
                                              n×m

                                  T           TT         Ir 0 T =       Ir 0
              UAV = RV = RU1 = U1R =                     0 0 n×m        0 0 m×n

Moreover, the matrix U1 = V T can be computed by RT In            Ir 0     V T . This proves

                                                                  0 0 n×m

Theorem 2.5.3

Let A be an m × n matrix of rank r. There exist invertible matrices U and V of size m × m and

n × n, respectively, such that

                                     UAV =         Ir 0
                                                   0 0 m×n

Moreover, if R is the reduced row-echelon form of A, then:

1. U can be computed by           A Im  R U ;
2. V can be computed by
                                  RT In            Ir 0     VT .

                                                   0 0 n×m

If A is an m × n matrix of rank r, the matrix      Ir 0     is called the Smith normal form11 of A.
                                                   00

Whereas the reduced row-echelon form of A is the "nicest" matrix to which A can be carried by row

operations, the Smith canonical form is the "nicest" matrix to which A can be carried by row and column

operations. This is because doing row operations to RT amounts to doing column operations to R and then

transposing.

Example 2.5.4                                                                              Ir 0 0 0 ,

              1 -1 1 2 
Given A =  2 -2 1 -1 , find invertible matrices U and V such that UAV =

                -1 1 0 3
where r = rank A.

Solution. The matrix U and the reduced row-echelon form R of A are computed by the row

11Named after Henry John Stephen Smith (1826-83).
8 Matrix Algebra

reduction A I3  R U :

             1 -1 1 2 1 0 0   1 -1 0 -3 -1 1 0 

             2 -2 1 -1 0 1 0    0 0 1 5 2 -1 0 

            -1 1 0 3 0 0 1                   0 0 0 0 -1 1 1

Hence              1 -1 0 -3                    -1 1 0 

                  R =  0 0 1 5  and U =  2 -1 0 

                  0 00 0                              -1 1 1

In particular, r = rank R = 2. Now row-reduce RT I4   Ir 0                 VT :
                                                      00

                   1 0 0 1 0 0 0 1 0 0 1 0 0 0

                   0 1 0 0 0 1 0   -1 0 0 0 1 0 0    0 0 0 1 1  0 1 0 0 0  1 0
                                                                           0 0 

                  -3 5 0 0 0 0 1             0 0 0 3 0 -5 1

whence            1 0 0 0                    1 0 1 3
Then UAV =
                  V T =  1 1 0 0   0 0 1 0  so V =  0 1 0 -5   0 0 1 0 

                  3 0 -5 -1                           000 1

            I2 0 0 0 as is easily verified.

Uniqueness of the Reduced Row-echelon Form

In this short subsection, Theorem 2.5.1 is used to prove the following important theorem.

   Theorem 2.5.4
   If a matrix A is carried to reduced row-echelon matrices R and S by row operations, then R = S.

Proof. Observe first that U R = S for some invertible matrix U (by Theorem 2.5.1 there exist invertible
matrices P and Q such that R = PA and S = QA; take U = QP-1). We show that R = S by induction on

the number m of rows of R and S. The case m = 1 is left to the reader. If R j and S j denote column j in R
and S respectively, the fact that U R = S gives

                       U R j = S j for each j                                                       (2.11)

Since U is invertible, this shows that R and S have the same zero columns. Hence, by passing to the
matrices obtained by deleting the zero columns from R and S, we may assume that R and S have no zero
columns.
                                                                                           . . Elementary Matrices 8

    But then the first column of R and S is the first column of Im because R and S are row-echelon, so
(2.11) shows that the first column of U is column 1 of Im. Now write U , R, and S in block form as follows.

                           U = 1 X 0 V , R = 0 R 1 Y , and S = 0 S 1 Z

Since U R = S, block multiplication gives V R = S so, since V is invertible (U is invertible) and both R
and S are reduced row-echelon, we obtain R = S by induction. Hence R and S have the same number
(say r) of leading 1s, and so both have m-r zero rows.

    In fact, R and S have leading ones in the same columns, say r of them. Applying (2.11) to these
columns shows that the first r columns of U are the first r columns of Im. Hence we can write U , R, and S
in block form as follows:

                         U = Ir M 0 W , R = R1 R2 0 0 , and S = S1 S2 0 0
where R1 and S1 are r × r. Then using U R = S block multiplication gives R1 = S1 and R2 = S2; that is,
S = R. This completes the proof.

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!
86 Matrix Algebra

 .6 Linear Transformations

If A is an m × n matrix, recall that the transformation TA : Rn  Rm defined by
                                              TA(x) = Ax for all x in Rn

is called the matrix transformation induced by A. In Section 2.2, we saw that many important geometric
transformations were in fact matrix transformations. These transformations can be characterized in a
different way. The new idea is that of a linear transformation, one of the basic notions in linear algebra. We
define these transformations in this section, and show that they are really just the matrix transformations
looked at in another way. Having these two ways to view them turns out to be useful because, in a given
situation, one perspective or the other may be preferable.

Linear Transformations

   Definition 2.2 Linear Transformations Rn  Rm
   A transformation T : Rn  Rm is called a linear transformation if it satisfies the following two
   conditions for all vectors x and y in Rn and all scalars a:

      T1 T (x + y) = T (x) + T (y)

      T2 T (ax) = aT (x)

Of course, x + y and ax here are computed in Rn, while T (x) + T (y) and aT (x) are in Rm. We say that T
preserves addition if T1 holds, and that T preserves scalar multiplication if T2 holds. Moreover, taking
a = 0 and a = -1 in T2 gives

                                   T (0) = 0 and T (-x) = -T (x) for all x
Hence T preserves the zero vector and the negative of a vector. Even more is true.

    Recall that a vector y in Rn is called a linear combination of vectors x1, x2, . . . , xk if y has the form
                                              y = a1x1 + a2x2 + · · · + akxk

for some scalars a1, a2, . . . , ak. Conditions T1 and T2 combine to show that every linear transformation
T preserves linear combinations in the sense of the following theorem. This result is used repeatedly in
linear algebra.

   Theorem 2.6.1: Linearity Theorem
   If T : Rn  Rm is a linear transformation, then for each k = 1, 2, . . .

                       T (a1x1 + a2x2 + · · · + akxk) = a1T (x1) + a2T (x2) + · · · + akT (xk)

   for all scalars ai and all vectors xi in Rn.

Proof. If k = 1, it reads T (a1x1) = a1T (x1) which is Condition T1. If k = 2, we have

                   T (a1x1 + a2x2) = T (a1x1) + T (a2x2)       by Condition T1
                                        = a1T (x1) + a2T (x2)  by Condition T2
                                                                                   .6. Linear Transformations 8

If k = 3, we use the case k = 2 to obtain

T (a1x1 + a2x2 + a3x3) = T [(a1x1 + a2x2) + a3x3]                                      collect terms
                              = T (a1x1 + a2x2) + T (a3x3)                             by Condition T1
                              = [a1T (x1) + a2T (x2)] + T (a3x3)                       by the case k = 2
                              = [a1T (x1) + a2T (x2)] + a3T (x3)                       by Condition T2

The proof for any k is similar, using the previous case k - 1 and Conditions T1 and T2.

The method of proof in Theorem 2.6.1 is called mathematical induction (Appendix C).
    Theorem 2.6.1 shows that if T is a linear transformation and T (x1), T (x2), . . . , T (xk) are all known,

then T (y) can be easily computed for any linear combination y of x1, x2, . . . , xk. This is a very useful
property of linear transformations, and is illustrated in the next two examples.

Example 2.6.1

                                                     1                     0                  1 -2 , and
If T : R3  R2 is a linear transformation, T  0  =                  2 ,T 1 =
                                                                 -1 0
                                               0

  0                               3
                 -4 , find T  -4 .
T 0 =
     1             42

                       3 1 0                                                    0

Solution. Write w =  -4 , e1 =  0 , e2 =  1 , and e3 =  0 . Then we know T (e1),
                              2            0                  0                    1

T (e2), and T (e3) and we want T (w), so it is enough by Theorem 2.6.1 to express w as a linear

combination of e1, e2 and e3 . But clearly w = 3e1 - 4e2 + 2e3. Thus Theorem 2.6.1 gives

T (w) = 3T (e1) - 4T (e2) + 2T (e3) = 3 2 -1 - 4 1 -2 + 2 -44 = -613

    The vectors {e1, e2, e3} form what is called the standard basis of R3, more on this below in Definition
2.3. Here is possibly a more subtle example.

Example 2.6.2
If T : R2  R2 is a linear transformation, T 11 = 2 -3 and T 1 -2 = 51 , find T 43 .

Solution. Write z = 43 , x = 11 , and y = 1 -2 for convenience. Then we know T (x) and

T (y) and we want T (z), so it is enough by Theorem 2.6.1 to express z as a linear combination of x

and y. That is, we want to find numbers a and b such that z = ax + by. Equating entries gives two
                                                                    11             1,            11 x +  1 y.
equations  4  =  a+b  and  3  =  a - 2b.  The  solution  is,  a  =  3   and  b  =      so  z  =                Thus
                                                                                   3             3       3
88 Matrix Algebra

Theorem 2.6.1 gives

                         11     1    11 2                 +3 15     1 27
                   T (z) = 3 T (x) + 3 T (y) = 3     -3       1  =3   -32

This is what we wanted.

We now show that any matrix transformation is a linear transformation.

Example 2.6.3
If A is m × n, the matrix transformation TA : Rn  Rm, is a linear transformation.
Solution. We have TA(x) = Ax for all x in Rn, so Theorem 2.2.2 gives

                             TA(x + y) = A(x + y) = Ax + Ay = TA(x) + TA(y)
and

                                      TA(ax) = A(ax) = a(Ax) = aTA(x)
hold for all x and y in Rn and all scalars a. Hence TA satisfies T1 and T2, and so is linear.

    The remarkable thing is that the converse of Example 2.6.3 is true: Every linear transformation
T : Rn  Rm is actually a matrix transformation. To see why, we define the standard basis of Rn.

Definition 2.3 Standard Basis of Rn
The standard basis of Rn is the set of columns {e1, e2, . . . , en} of the identity matrix In. That is:

                             1       0                           0

                             0       1                           0

                                                                 
                             0       0                           0
                                                                 
                         e1 =  ..  , e2 =  ..  , · · · , en =  ..  .
                             .       .                           .

                              0       0                           0 

                             0                    0              1

                                   
                                      x1

                                     x2           
                                                  
Then each ei is in R and every vector x =  ..  in R is a linear combination of the ei. In fact:nn

                                   .

                                     xn

                             x = x1e1 + x2e2 + · · · + xnen

as the reader can verify. Hence Theorem 2.6.1 shows that

T (x) = T (x1e1 + x2e2 + · · · + xnen) = x1T (e1) + x2T (e2) + · · · + xnT (en)
                                                                .6. Linear Transformations 8

Now observe that each T (ei) is a column in Rm, so

                          A = T (e1) T (e2) · · · T (en)

is an m × n matrix. Hence we can apply Definition 2.5 to get

                                                                                
                                                                                   x1

T (x) = x1T (e1) + x2T (e2) + · · · + xnT (en) =    T (e1) T (e2) · · · T (en)    x2  
                                                                                      
                                                                                 ..  = Ax
                                                                                .

                                                                                  xn

Since this holds for every x in Rn, it shows that T is the matrix transformation induced by A, and so proves
most of the following theorem.

Theorem 2.6.2
Let T : Rn  Rm be a transformation.

   1. T is linear if and only if it is a matrix transformation.
   2. In this case T = TA is the matrix transformation induced by a unique m × n matrix A, given

       in terms of its columns by

                                         A = T (e1) T (e2) · · · T (en)
       where {e1, e2, . . . , en} is the standard basis of Rn.

Proof. It remains to verify that the matrix A is unique. Suppose that T is induced by another matrix B.
Then T (x) = Bx for all x in Rn. But T (x) = Ax for each x, so Bx = Ax for every x. Hence A = B by
Theorem 2.2.6.

    Hence we can speak of the matrix of a linear transformation. Because of Theorem 2.6.2 we may (and
shall) use the phrases "linear transformation" and "matrix transformation" interchangeably.

Example 2.6.4

                                                    
                      x1                               x1
Define T : R3  R2 by T  x2  = x1 x2 for all  x2  in R3. Show that T is a linear
                      x3                            x3
transformation and use Theorem 2.6.2 to find its matrix.

                                                                x1 + y1  
                  x1         y1

Solution. Write x =  x2  and y =  y2 , so that x + y =  x2 + y2 . Hence

               x3         y3                                    x3 + y3

                        T (x + y) = x1 + y1 x2 + y2 = x1 x2 + y1 y2 = T (x) + T (y)
Similarly, the reader can verify that T (ax) = aT (x) for all a in R, so T is a linear transformation.
Matrix Algebra

Now the standard basis of R3 is                          0                            0
                                                   e2 =  1  ,           and e3 =  0 
                                  1
                            e1 =  0  ,                      0                            1

                                      0

so, by Theorem 2.6.2, the matrix of T is

                                  A = T (e1) T (e2) T (e3) = 1 0 0 0 1 0

                                                                         
                                     x1                                     x1
Of course, the fact that T  x2  = x1 x2 = 1 0 0 0 1 0  x2  shows directly that T is a
                                  x3                                          x3
matrix transformation (hence linear) and reveals the matrix.

    To illustrate how Theorem 2.6.2 is used, we rederive the matrices of the transformations in Exam-
ples 2.2.13 and 2.2.15.

Example 2.6.5

Let Q0 : R2  R2 denote reflection in the x axis (as in Example 2.2.13) and let R  : R2  R2
                                                   2
denote counterclockwise rotation through 2 about the origin (as in Example 2.2.15). Use
Theorem 2.6.2 to find the matrices of Q0 and R  .
                                                           2
y
                                           Solution. Observe that Q0 and R  are linear by Example 2.6.3
                                                                                   2
                                           (they are matrix transformations), so Theorem 2.6.2 applies
            0

e2 1                                       to them. The standard basis of R2 is {e1, e2} where e1 = 10

                           1

                   0                       points along the positive x axis, and e2 = 01 points along

0              e1             x

         Figure 2.6.1                      the positive y axis (see Figure 2.6.1).

The reflection of e1 in the x axis is e1 itself because e1 points along the x axis, and the reflection
of e2 in the x axis is -e2 because e2 is perpendicular to the x axis. In other words, Q0(e1) = e1 and

Q0(e2) = -e2. Hence Theorem 2.6.2 shows that the matrix of Q0 is

                                  Q0(e1) Q0(e2) = e1 -e2 = 1 0 0 -1

which agrees with Example 2.2.13.

Similarly,     rotating       e1  through     counterclockwise    about  the  origin     produces  e2,  and  rotating  e2
                                           2
through        counterclockwise   about       the  origin  gives  -e1.  That  is,  R  (e1)  =      and  R  (e2)  =  -e2.
         2                                                                            2        e2       2

Hence, again by Theorem 2.6.2, the matrix of R  is
                                                           2

                                  R  (e1) R  (e2) = e2 -e1 = 0 -1
                                  2           2                                       10

agreeing with Example 2.2.15.
                                                                                      .6. Linear Transformations

Example 2.6.6

      T y x = y                     Let Q1 : R2  R2 denote reflection in the line y = x. Show that
            y       x               Q1 is a matrix transformation, find its matrix, and use it to illustrate
                                    Theorem 2.6.2.
                       y=x

      e2 xy Solution. Figure 2.6.2 shows that Q1 xy = yx . Hence

         0 e1           x           Q1 xy = 0 1 1 0 yx , so Q1 is the matrix transformation

         Figure 2.6.2               induced by the matrix A = 0 1 1 0 . Hence Q1 is linear (by

Example 2.6.3) and so Theorem 2.6.2 applies. If e1 = 10 and e2 = 01 are the standard basis

of R2, then it is clear geometrically that Q1(e1) = e2 and Q1(e2) = e1. Thus (by Theorem 2.6.2)

the matrix of Q1 is Q1(e1) Q1(e2) = e2 e1 = A as before.

    Recall that, given two "linked" transformations

                                                                          kT nS m

                                                     R - R - R
we can apply T first and then apply S, and so obtain a new transformation

                                                     S  T : Rk  Rm
called the composite of S and T , defined by

                                          (S  T )(x) = S [T (x)] for all x in Rk
If S and T are linear, the action of S  T can be computed by multiplying their matrices.

Theorem 2.6.3

Let  Rk  T  Rn   S  Rm  be  linear  transformations,    and  let  A  and  B  be  the  matrices  of  S    and   T

         -      -

respectively. Then S  T is linear with matrix AB.

Proof. (S  T )(x) = S [T (x)] = A [Bx] = (AB)x for all x in Rk.

    Theorem 2.6.3 shows that the action of the composite S  T is determined by the matrices of S and
T . But it also provides a very useful interpretation of matrix multiplication. If A and B are matrices, the
product matrix AB induces the transformation resulting from first applying B and then applying A. Thus
the study of matrices can cast light on geometrical transformations and vice-versa. Here is an example.

Example 2.6.7

Show  that  reflection  in  the  x  axis  followed  by  rotation  through       is  reflection  in  the  line  y  =  x.
                                                                             2
Matrix Algebra

Solution. The composite in question is R   Q0 where Q0 is reflection in the x axis and R  is
                                                    2                                                                                                2
                                                                                   0 -1
rotation through 2 . By Example 2.6.5, R  has matrix A =                                        and Q0 has matrix
                                                                                   10
                                                    2

B = 1 0 . Hence Theorem 2.6.3 shows that the matrix of R   Q0 is
0 -1                                                                                     2

AB = 0 -1 1 0        1 0 0 -1 = 0 1 1 0 , which is the matrix of reflection in the line y = x by
Example 2.6.4.

This conclusion can also be seen geometrically. Let x be a typical point in R2, and assume that x

makes an angle  with the positive x axis. The effect of first applying Q0 and then applying R  is shown
                                                                                                                                                         2
in Figure 2.6.3. The fact that R  [Q0(x)] makes the angle  with the positive y axis shows that R  [Q0(x)]
                                 2                                                                                                                           2
is the reflection of x in the line y = x.

                     yyy

                                                                                                        R  [Q0(x)] y = x

                                                                                                                                         2

                                    x                            x                                   x

                                        x           0                x             0                     x
                     0
                                                                 Q0(x)                               Q0(x)

                                                       Figure 2.6.3

    In Theorem 2.6.3, we saw that the matrix of the composite of two linear transformations is the product
of their matrices (in fact, matrix products were defined so that this is the case). We are going to apply
this fact to rotations, reflections, and projections in the plane. Before proceeding, we pause to present
useful geometrical descriptions of vector addition and scalar multiplication in the plane, and to give a
short review of angles and the trigonometric functions.

Some Geometry                              As we have seen, it is convenient to view a vector x in R2 as an arrow

x2                                         from the origin to the point x (see Section 2.2). This enables us to

                 2x = 24                   visualize what sums and scalar multiples mean geometrically. For
                                                                                                                                                                      1
                                           example consider x =                1      in R . Then 2x =2                                          21                   2
                                                                                                                                                     , 2x =
       x = 12                                                                  2                                                                 4                    1

                1                          and  -   1  x  =    -1
                                                    2            2 , and these are shown as arrows in Figure 2.6.4.
    1x = 2
                                                               -1

   2            1                               Observe that the arrow for 2x is twice as long as the arrow for x

0                    x1                    and  in  the   same   direction,        and   that   the  arrows                                 for  1x  is  also     in  the

1      -12                                                                                                                                       2
-2x =                                      same direction as the arrow for x, but only half as long. On the other
       -1
                                                                         1
                                           hand,  the     arrow  for  -  2  x  is  half  as  long    as  the                                arrow   for  x,  but  in  the

Figure 2.6.4                               opposite direction.

More generally, we have the following geometrical description of scalar multiplication in R2:
                                                                                           .6. Linear Transformations

    Theorem: Scalar Multiple Law

    Let x be a vector in R2. The arrow for kx is |k| times12as long as the arrow for x, and is in the same
    direction as the arrow for x if k > 0, and in the opposite direction if k < 0.

                         x + y = 34                Now consider two vectors x = 21 and y = 13 in R2. They

        x2                                   are plotted in Figure 2.6.5 along with their sum x + y = 34 . It is

              y = 13

                                             a routine matter to verify that the four points 0, x, y, and x + y form

                                             the vertices of a parallelogram-that is opposite sides are parallel

                         x = 21              and of the same length. (The reader should verify that the side from
                                                                       1,
                                 x1          0  to  x  has  slope  of      as   does  the  side  from  y  to  x + y,  so  these  sides
                                                                       2
        0                                    are parallel.) We state this as follows:

             Figure 2.6.5
    Theorem: Parallelogram Law

    Consider vectors x and y in R2. If the arrows for x and y are drawn (see Figure 2.6.6), the arrow
    for x + y corresponds to the fourth vertex of the parallelogram determined by the points x, y, and 0.

                                             We will have more to say about this in Chapter 4.

        x2               x+y                       Before proceeding we turn to a brief review of angles and the

                                             trigonometric functions. Recall that an angle  is said to be in stan-

              y                              dard position if it is measured counterclockwise from the positive

                                             x axis (as in Figure 2.6.7). Then  uniquely determines a point p on

                      x                      the unit circle (radius 1, centre at the origin). The radian measure

           0                     x1          of  is the length of the arc on the unit circle from the positive x axis

to  p.  Thus  360     =  2    radians,  180  =  ,  90  =       ,  and  so  on.
                                                            2
              Figure 2.6.6
                                                   The point p in Figure 2.6.7 is also closely linked to the trigono-

                    y    Radian              metric functions cosine and sine, written cos  and sin  respec-
                         measure
              p                              tively. In fact these functions are defined to be the x and y coor-
                 1         of                dinates of p; that is p = cos  sin  . This defines cos  and sin  for
                    0                        the arbitrary angle  (possibly negative), and agrees with the usual
                               x

                                             values when  is an acute angle                0                  as the reader should
                                                                                                       2
                                             verify. For more discussion of this, see Appendix A.

    12        Figure 2.6.7
        If k is a real number, |k| denotes the absolute value of k; that is, |k| = k if k  0 and |k| = -k if k < 0.
Matrix Algebra

Rotations

We can now describe rotations in the plane. Given an angle  , let

                                                       R : R2  R2
denote counterclockwise rotation of R2 about the origin through the angle  . The action of R is depicted
in Figure 2.6.8. We have already looked at R  (in Example 2.2.15) and found it to be a matrix trans-

                                                                            2

formation. It turns out that R is a matrix transformation for every angle  (with a simple formula for
the matrix), but it is not clear how to find the matrix. Our approach is to first establish the (somewhat
surprising) fact that R is linear, and then obtain the matrix from Theorem 2.6.2.

y                                                 y                                x+y

   R (x)                        R (x + y)                                          x
                                             R (x)
                                                          y                             x

                                                                   R (y)         

                     x                                                    0
0
                             x

                                                                   Figure 2.6.9

        Figure 2.6.8

    Let x and y be two vectors in R2. Then x + y is the diagonal of the parallelogram determined by x and
y as in Figure 2.6.9.

The effect of R is to rotate the entire parallelogram to obtain the new parallelogram determined by R (x)
and R (y), with diagonal R (x + y). But this diagonal is R (x) + R (y) by the parallelogram law (applied
to the new parallelogram). It follows that

        y                       R (x + y) = R (x) + R (y)

sin        e2 cos                         A similar argument shows that R (ax) = aR (x) for any
R (e2)                               scalar a, so R : R2  R2 is indeed a linear transformation.

        1 1          R (e1)               With linearity established we can find the matrix of R .
                                     Let e1 = 10 and e2 = 01 denote the standard basis of
                      sin x          R2. By Figure 2.6.10 we see that

         0 cos        e1                   R (e1) = cos  sin  and R (e2) = - sin  cos 

Figure 2.6.10

Hence Theorem 2.6.2 shows that R is induced by the matrix          - sin 
                                        R (e1) R (e2) = cos  sin     cos 

We record this as
                                                                                  .6. Linear Transformations

Theorem 2.6.4
The rotation R : R2  R2 is the linear transformation with matrix cos  - sin  sin  cos  .

For example, R  and R have matrices 0 -1 and -1 0 , respectively, by Theorem 2.6.4.
               2            10  0 -1

The first of these confirms the result in Example 2.2.15. The second shows that rotating a vector x = x
                                                                                                                           y

through the angle  results in R (x) = -1 0 0 -1 xy = -x -y = -x. Thus applying R is the same
as negating x, a fact that is evident without Theorem 2.6.4.

Example 2.6.8               Let  and  be angles. By finding the matrix of the composite
     y                      R  R , obtain expressions for cos( +  ) and sin( +  ).

  R R (x)                                                                                      2 R 2 R 2

         R (x)              Solution. Consider the transformations R - R - R . Their
           x                composite R  R is the transformation that first rotates the
                            plane through  and then rotates it through  , and so is the rotation
                         x  through the angle  +  (see Figure 2.6.11).
                            In other words
0
                                                          R + = R  R
     Figure 2.6.11
                            Theorem 2.6.3 shows that the corresponding equation holds
                            for the matrices of these transformations, so Theorem 2.6.4 gives:

               cos( +  ) - sin( +  ) sin( +  ) cos( +  ) = cos  - sin  sin  cos   cos  - sin 
                                                                                  sin  cos 

If we perform the matrix multiplication on the right, and then compare first column entries, we
obtain

                            cos( +  ) = cos  cos  - sin  sin 
                            sin( +  ) = sin  cos  + cos  sin 

These are the two basic identities from which most of trigonometry can be derived.
6 Matrix Algebra

Reflections

                               The line through the origin with slope m has equation y = mx, and we let
                               Qm : R2  R2 denote reflection in the line y = mx.
y

                                  This transformation is described geometrically in Figure 2.6.12. In

           Qm(x) y = mx        words, Qm(x) is the "mirror image" of x in the line y = mx. If m = 0 then
                               Q0 is reflection in the x axis, so we already know Q0 is linear. While we
                   x           could show directly that Qm is linear (with an argument like that for R ),
                               we prefer to do it another way that is instructive and derives the matrix of
                         x
                               Qm directly without using Theorem 2.6.2.
   0

      Figure 2.6.12                Let  denote the angle between the positive x axis and the line y = mx.
                               The key observation is that the transformation Qm can be accomplished in

three steps: First rotate through - (so our line coincides with the x axis), then reflect in the x axis, and

finally rotate back through  . In other words:

                                        Qm = R  Q0  R-

Since R- , Q0, and R are all linear, this (with Theorem 2.6.3) shows that Qm is linear and that its matrix
is the product of the matrices of R , Q0, and R- . If we write c = cos  and s = sin  for simplicity, then
the matrices of R , R- , and Q0 are

                     c -s ,         c s , and                         10      respectively.13
                     sc           -s c                                0 -1

Hence, by Theorem 2.6.3, the matrix of Qm = R  Q0  R- is

                            c -s  10             c s = c2 - s2                  2sc
                            sc    0 -1          -s c                     2sc  s2 - c2

                                  We can obtain this matrix in terms of m alone. Figure 2.6.13

      y                           shows that     cos  =  1 1+m2 and sin  =  m 1+m2

                1                                c2 - s2 2sc                                     1   1 - m2 2m
                                                   2sc s2 - c2                                 1+m2
                m                 so the matrix                               of  Qm  becomes        2m m - 1         2  .

      1 + m2         y = mx
                   m

                            x

   0         1
Theorem 2.6.5

Let Qm Fdiegnuorte r2e.fl6.e1c3tion in the line y = mx. Then Qm is a
                                  1 1 - m2 2m
linear transformation with matrix 1+m2          2m m - 12             .

                                                  Note that if m = 0, the matrix in Theorem 2.6.5 becomes
                                                1 0 0 -1 , as expected. Of course this analysis fails for reflection

   13The matrix of R- comes from the matrix of R using the fact that, for all angles  , cos(- ) = cos  and
sin(- ) = - sin( ).
                                                                                     .6. Linear Transformations

                                             in the y axis because vertical lines have no slope. However it is an
easy exercise to verify directly that reflection in the y axis is indeed linear with matrix -1 0 0 1 .14

Example 2.6.9

Let  T  :  R2    R2  be  rotation  through  -     followed  by  reflection  in  the  y  axis.  Show  that  T  is  a

                                               2
reflection in a line through the origin and find the line.

                                                                

Solution. The matrix of R-  is           cos(- 2 )   - sin(- 2 )              01        and the matrix of
                                                                   =        -1 0
                                 2       sin(- 2 )
                                                       cos(- 2 )

reflection in the y axis is -1 0 0 1 . Hence the matrix of T is

-1 0             0 1 -1 0 = 0 -1 -1 0 and this is reflection in the line y = -x (take m = -1 in
  01

Theorem 2.6.5).

Projections

     y                                   The method in the proof of Theorem 2.6.5 works more generally.
                                         Let Pm : R2  R2 denote projection on the line y = mx. This trans-
        Pm(x)    y = mx                  formation is described geometrically in Figure 2.6.14.
                  x
                                             If m = 0, then P0 xy = x0 for all xy in R2, so P0 is
                              x          linear with matrix 1 0 0 0 . Hence the argument above for Qm goes
                                         through for Pm. First observe that
        0
                                                                Pm = R  P0  R-

           Figure 2.6.14                 as before. So, Pm is linear with matrix

                                                     c -s 1 0                        cs        c2 sc
                                                     s c 0 0 -s c = sc s2

where c = cos  =  1 1+m2 and s = sin  =  m 1+m2 .
    This gives:

Theorem 2.6.6
Let Pm : R2  R2 be projection on the line y = mx. Then Pm is a linear transformation with matrix

  1 1m
1+m2 m m2 .

14Note that    -1 0  = lim       1     1 - m2 2m     .
                 01                               2
                                    2  2m m - 1
                         m 1+m
8 Matrix Algebra

    Again, if m = 0, then the matrix in Theorem 2.6.6 reduces to 1 0 0 0 as expected. As the y axis has
no slope, the analysis fails for projection on the y axis, but this transformation is indeed linear with matrix

  0 0 0 1 as is easily verified directly.

    Note that the formula for the matrix of Qm in Theorem 2.6.5 can be derived from the above formula
for the matrix of Pm. Using Figure 2.6.12, observe that Qm(x) = x + 2[Pm(x) - x] so Qm(x) = 2Pm(x) - x.
Substituting the matrices for Pm(x) and 1R2(x) gives the desired formula.

Example 2.6.10

Given x in R2, write y = Pm(x). The fact that y lies on the line y = mx means that Pm(y) = y. But
then

                (Pm  Pm)(x) = Pm(y) = y = Pm(x) for all x in R2, that is, Pm  Pm = Pm.

In  particular,  if  we  write  the  matrix  of  Pm  as  A  =    1   1m  2 , then A = A. The reader should2
                                                               1+m2
                                                                     mm

verify this directly.

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!
                                                                       . . LU-Factorization

 . LU-Factorization15

The solution to a system Ax = b of linear equations can be solved quickly if A can be factored as A = LU
where L and U are of a particularly nice form. In this section we show that gaussian elimination can be
used to find such factorizations.

Triangular Matrices

As for square matrices, if A = ai j is an m × n matrix, the elements a11, a22, a33, . . . form the main
diagonal of A. Then A is called upper triangular if every entry below and to the left of the main diagonal

is zero. Every row-echelon matrix is upper triangular, as are the matrices

                   1 -1 0 3    0 2 1 0 5                         1 1 1
                  0 2 1 1      0 0 0 3 1                          0 -1 1 
                                                                  0 0 0 
                     0 0 -3 0     00101
                                                                    0 00

By analogy, a matrix A is called lower triangular if its transpose is upper triangular, that is if each entry
above and to the right of the main diagonal is zero. A matrix is called triangular if it is upper or lower
triangular.

Example 2.7.1

Solve the system

                     x1 + 2x2 - 3x3 - x4 + 5x5 = 3
                                  5x3 + x4 + x5 = 8
                                               2x5 = 6

where the coefficient matrix is upper triangular.

Solution. As in gaussian elimination, let the "non-leading" variables be parameters: x2 = s and
x4 = t. Then solve for x5, x3, and x1 in that order as follows. The last equation gives

                                   x5  =           6  =   3
                                                   2

Substitution into the second last equation gives

                                   x3  =           1  -  1t

                                                         5

Finally, substitution of both x5 and x3 into the first equation gives

                               x1  =  -9           -  2s  +  2t

                                                             5

    The method used in Example 2.7.1 is called back substitution because later variables are substituted
into earlier equations. It works because the coefficient matrix is upper triangular. Similarly, if the coeffi-
cient matrix is lower triangular the system can be solved by forward substitution where earlier variables
are substituted into later equations. As observed in Section 1.2, these procedures are more numerically
efficient than gaussian elimination.

   15This section is not used later and so may be omitted with no loss of continuity.
        Matrix Algebra

    Now consider a system Ax = b where A can be factored as A = LU where L is lower triangular and U
is upper triangular. Then the system Ax = b can be solved in two stages as follows:

   1. First solve Ly = b for y by forward substitution.
   2. Then solve U x = y for x by back substitution.

Then x is a solution to Ax = b because Ax = LU x = Ly = b. Moreover, every solution x arises this way
(take y = U x). Furthermore the method adapts easily for use in a computer.

    This focuses attention on efficiently obtaining such factorizations A = LU . The following result will
be needed; the proof is straightforward and is left as Exercises ?? and ??.

   Lemma 2.7.1
   Let A and B denote matrices.

       1. If A and B are both lower (upper) triangular, the same is true of AB.
       2. If A is n × n and lower (upper) triangular, then A is invertible if and only if every main

           diagonal entry is nonzero. In this case A-1 is also lower (upper) triangular.

LU-Factorization

Let A be an m × n matrix. Then A can be carried to a row-echelon matrix U (that is, upper triangular). As
in Section 2.5, the reduction is

                        A  E1A  E2E1A  E3E2E1A  · · ·  EkEk-1 · · · E2E1A = U
where E1, E2, . . . , Ek are elementary matrices corresponding to the row operations used. Hence

                                           A = LU

where  L  =  (EkEk-1 · · · E2E1)-1  =  E-1E-1 · · · E-1 E-1.  If  we  do  not  insist  that  U  is  reduced  then,  except
                                       12  k-1 k
for row interchanges, none of these row operations involve adding a row to a row above it. Thus, if no

row interchanges are used, all the Ei are lower triangular, and so L is lower triangular (and invertible) by

Lemma 2.7.1. This proves the following theorem. For convenience, let us say that A can be lower reduced

if it can be carried to row-echelon form using no row interchanges.

Theorem 2.7.1
If A can be lower reduced to a row-echelon matrix U, then

                                                       A = LU
where L is lower triangular and invertible and U is upper triangular and row-echelon.
                            . . LU-Factorization

   Definition 2.4 LU-factorization
   A factorization A = LU as in Theorem 2.7.1 is called an LU-factorization of A.

    Such a factorization may not exist (Exercise ??) because A cannot be carried to row-echelon form
using no row interchange. A procedure for dealing with this situation will be outlined later. However, if
an LU-factorization A = LU does exist, then the gaussian algorithm gives U and also leads to a procedure
for finding L. Example 2.7.2 provides an illustration. For convenience, the first nonzero column from the
left in a matrix A is called the leading column of A.

   Example 2.7.2

                                            0 2 -6 -2 4 
   Find an LU-factorization of A =  0 -1 3 3 2 .

                                               0 -1 3 7 10
   Solution. We lower reduce A to row-echelon form as follows:

 0 2 -6 -2 4   0 1 -3 -1 2   0 1 -3 -1 2 

A =  0 -1 3 3 2    0 0 0 2 4    0 0 0 1 2  = U

0 -1 3 7 10   0 0 0 6 12    00 0 00

The circled columns are determined as follows: The first is the leading column of A, and is used

(by lower reduction) to create the first leading 1 and create zeros below it. This completes the work

on row 1, and we repeat the procedure on the matrix consisting of the remaining rows. Thus the

second circled column is the leading column of this smaller matrix, which we use to create the

second leading 1 and the zeros below it. As the remaining row is zero here, we are finished. Then

A = LU where   2 0 0

              L =  -1 2 0 

              -1 6 1

This matrix L is obtained from I3 by replacing the bottom of the first two columns by the circled
columns in the reduction. Note that the rank of A is 2 here, and this is the number of circled
columns.

    The calculation in Example 2.7.2 works in general. There is no need to calculate the elementary
matrices Ei, and the method is suitable for use in a computer because the circled columns can be stored in
memory as they are created. The procedure can be formally stated as follows:

   Theorem: LU-Algorithm
   Let A be an m × n matrix of rank r, and suppose that A can be lower reduced to a row-echelon
   matrix U. Then A = LU where the lower triangular, invertible matrix L is constructed as follows:

       1. If A = 0, take L = Im and U = 0.
        Matrix Algebra

       2. If A = 0, write A1 = A and let c1 be the leading column of A1. Use c1 to create the first
           leading 1 and create zeros below it (using lower reduction). When this is completed, let A2
           denote the matrix consisting of rows 2 to m of the matrix just created.

       3. If A2 = 0, let c2 be the leading column of A2 and repeat Step 2 on A2 to create A3.
       4. Continue in this way until U is reached, where all rows below the last leading 1 consist of

           zeros. This will happen after r steps.
       5. Create L by placing c1, c2, . . . , cr at the bottom of the first r columns of Im.

A proof of the LU-algorithm is given at the end of this section.
                                                                                               . . LU-Factorization

    LU-factorization is particularly important if, as often happens in business and industry, a series of
equations Ax = B1, Ax = B2, . . . , Ax = Bk, must be solved, each with the same coefficient matrix A. It is
very efficient to solve the first system by gaussian elimination, simultaneously creating an LU-factorization
of A, and then using the factorization to solve the remaining systems by forward and back substitution.

   Example 2.7.3
                                             5 -5 10 0 5 

   Find an LU-factorization for A =  -3 -2 2 0 -1 0  3 2 2 1 .
                                                  1 -1 10 2 5

   Solution. The reduction to row-echelon form is

 5 -5 10 0 5   1 -1 2 0 1 

 -3  2 0 -1 0  3 2 2 1    0  0  0 8 2 4
 -2                             0 4 -1 2 

1 -1 10 2 5    0 08 24

                                                                               

               1 -1 2 0 1

                                01                                               
                                                                           1 1
                 0
                                                                                 
                                                                           4 2
                
                0               0 0 -2 0 

                                                                               

               0 00 00

                                                                           

               1 -1 2 0 1

                                                                        1  
                                                                           1
                 0              01                                           
                                                                        4 2 =U
                                0 0 1 0 
                
                0

                                                                           

               0 0000

If U denotes this row-echelon matrix, then A = LU , where

                                                   5 0 0 0
                                            L =  -2 4 -2 0   -3 8 0 0 

                                                       18 01
Matrix Algebra

The next example deals with a case where no row of zeros is present in U (in fact, A is invertible).

Example 2.7.4

                                         2 4 2
Find an LU-factorization for A =  1 1 2 .

                                            -1 0 2

Solution. The reduction to row-echelon form is

 2 4 2 1 2 1 1 2 1 1 2 1

 1 1 2    0 -1 1    0 1 -1    0 1 -1  = U

-1 0 2          0 23                                00 5  00 1

                                     2 0 0
   Hence A = LU where L =  1 -1 0 .

                                       -1 2 5

    There are matrices (for example 0 1 1 0 ) that have no LU-factorization and so require at least one
row interchange when being carried to row-echelon form via the gaussian algorithm. However, it turns
out that, if all the row interchanges encountered in the algorithm are carried out first, the resulting matrix
requires no interchanges and so has an LU-factorization. Here is the precise result.

   Theorem 2.7.2
   Suppose an m × n matrix A is carried to a row-echelon matrix U via the gaussian algorithm. Let
   P1, P2, . . . , Ps be the elementary matrices corresponding (in order) to the row interchanges used,
   and write P = Ps · · · P2P1. (If no interchanges are used take P = Im.) Then:

       1. PA is the matrix obtained from A by doing these interchanges (in order) to A.

       2. PA has an LU-factorization.

The proof is given at the end of this section.
    A matrix P that is the product of elementary matrices corresponding to row interchanges is called

a permutation matrix. Such a matrix is obtained from the identity matrix by arranging the rows in a
different order, so it has exactly one 1 in each row and each column, and has zeros elsewhere. We regard
the identity matrix as a permutation matrix. The elementary permutation matrices are those obtained from
I by a single row interchange, and every permutation matrix is a product of elementary ones.
                                                                              . . LU-Factorization

Example 2.7.5

         0 0 -1 2 
If A =  2 1 -3 6   -1 -1 1 2 , find a permutation matrix P such that PA has an LU-factorization,

             0 1 -1 4
and then find the factorization.

Solution. Apply the gaussian algorithm to A:

 -1 -1 1 2   1 1 -1 -2   1 1 -1 -2 

    0    0 -1 2   0 0 -1 2    0 -1 -1 10 
                                                                -
                                                                   
A -                                            0                                             
      2  1  -3                       6            -1  -1  10         0        0 -1           2
                                                              

0 1 -1 4                                       0 1 -1 4              0 1 -1 4

                                           1 1 -1 -2   1 1 -1 -2 

                                            0 0 -1 2   0 1 1 -10    0 0  0 1  1 -10 
                                                                              1 -2 

                                               0 0 -2 14             0 0 0 10

Two row interchanges were needed (marked with ), first rows 1 and 2 and then rows 2 and 3.
Hence, as in Theorem 2.7.2,

          1 0 0 0  0 1 0 0   0 1 0 0 

P =  0 1 0 0   0 0 1 0   0 0 1 0   1 0 0 0  =  1 0 0 0   0 0 1 0 

            0001 0001                                           0001

If we do these interchanges (in order) to A, the result is PA. Now apply the LU-algorithm to PA:

 -1 -1 1 2   1 1 -1 -2   1 1 -1 -2 

PA =  0  2 0 -1 2  1 -3 6    0 0 -1 2   0 -1 -1 10    0 0 -1 2   0 1 1 -10 

0 1 -1 4                                  0 1 -1 4              0 0 -2 14

                                           1 1 -1 -2   1 1 -1 -2 

                                          0 0  0 1 1 -2  1 -10    0 0  0 1 1 -2  1 -10  = U

                                          0 0 0 10              00 0 1

             -1 0 0 0                                          1 1 -1 -2 

Hence, PA = LU , where L =           2 -1 0       0  0  and U =  0 0  0 1     1 -2  1 -10 .
                                     0 0 -1

                                     0 1 -2 10                  00 0 1

Theorem 2.7.2 provides an important general factorization theorem for matrices. If A is any m × n
6 Matrix Algebra

matrix, it asserts that there exists a permutation matrix P and an LU-factorization PA = LU . Moreover,

it shows that either P = I or P = Ps · · · P2P1, where P1, P2, . . . , Ps are the elementary permutation matri-
ces arising in the reduction of A to row-echelon form.  Now  observe  that  P-1         for each i (they are
                                                                                 =  Pi
                                                                             i
elementary row interchanges). Thus, P-1 = P1P2 · · · Ps, so the matrix A can be factored as

                           A = P-1LU

where P-1 is a permutation matrix, L is lower triangular and invertible, and U is a row-echelon matrix.
This is called a PLU-factorization of A.

    The LU-factorization in Theorem 2.7.1 is not unique. For example,

                  1 0 1 -2 3 3 2 0 0 0 = 1 0 1 -2 3 3 1 0 0 0

However, it is necessary here that the row-echelon matrix has a row of zeros. Recall that the rank of a
matrix A is the number of nonzero rows in any row-echelon matrix U to which A can be carried by row
operations. Thus, if A is m × n, the matrix U has no row of zeros if and only if A has rank m.

Theorem 2.7.3
Let A be an m × n matrix that has an LU-factorization

                                                       A = LU
If A has rank m (that is, U has no row of zeros), then L and U are uniquely determined by A.

Proof. Suppose A = MV is another LU-factorization of A, so M is lower triangular and invertible and V is
row-echelon. Hence LU = MV , and we must show that L = M and U = V . We write N = M-1L. Then N

is lower triangular and invertible (Lemma 2.7.1) and NU = V , so it suffices to prove that N = I. If N is

m × m, we use induction on m. The case m = 1 is left to the reader. If m > 1, observe first that column 1

of V is N times column 1 of U . Thus if either column is zero, so is the other (N is invertible). Hence, we

can assume (by deleting zero columns) that the (1, 1)-entry is 1 in both U and V .

Now we write N =  a 0 ,U=  1 Y 0 U1 , and V =                1Z             in block form. Then NU = V
                  X N1                                       0 V1

becomes a aY X XY + N1U1 = 1 Z 0 V1 . Hence a = 1, Y = Z, X = 0, and N1U1 = V1. But N1U1 = V1
implies N1 = I by induction, whence N = I.

    If A is an m × m invertible matrix, then A has rank m by Theorem 2.4.5. Hence, we get the following
important special case of Theorem 2.7.3.

Corollary 2.7.1

If an invertible matrix A has an LU-factorization A = LU, then L and U are uniquely determined by
A.

Of course, in this case U is an upper triangular matrix with 1s along the main diagonal.
                                                                                                              . . LU-Factorization

Proofs of Theorems

Proof of the LU-Algorithm. If c1, c2, . . . , cr are columns of lengths m, m-1, . . . , m-r +1, respectively,

write L(m) [c1, c2, . . . , cr] for the lower triangular m × m matrix obtained from Im by placing c1, c2, . . . , cr
at the bottom of the first r columns of Im.

    Proceed by induction on n. If A = 0 or n = 1, it is left to the reader. If n > 1, let c1 denote the leading
column of A and let k1 denote the first column of the m × m identity matrix. There exist elementary
matrices E1, . . . , Ek such that, in block form,

                          (Ek · · · E2E1)A = 0 k1 X1 A1                          where (Ek · · · E2E1)c1 = k1

Moreover, each E j can be taken to be lower triangular (by assumption). Write

                                      G   =    (Ek  ·  ·  ·  E2E1)-1  =    -1 -1             ·  ·  ·  Ek-1

                                                                         E1 E2

Then G is lower triangular, and Gk1 = c1. Also, each E j (and so each E-1 j ) is the result of either multiply-
ing row 1 of Im by a constant or adding a multiple of row 1 to another row. Hence,

                                      G   =      -1 -1       ·  ·  ·  E-1 k )Im  =     c1 0
                                                                                            Im-1
                                             (E1 E2

in block form. Now, by induction, let A1 = L1U1 be an LU-factorization of A1, where L1 = L(m-1) [c2, . . . , cr]
and U1 is row-echelon. Then block multiplication gives

                          G-1A = 0 k1 X1 L1U1 = 1 0 0 L1                                              0 1 X1
                                                                                                      0 0 U1

Hence A = LU , where U =          0 1 X1               is row-echelon and
                                  0 0 U1

                 L = c1 0                      1 0 0 L1 = c1 0L1 = L(m) [c1, c2, . . . , cr]
                              Im-1

This completes the proof.

Proof of Theorem 2.7.2. Let A be a nonzero m × n matrix and let k j denote column j of Im. There is a
permutation matrix P1 (where either P1 is elementary or P1 = Im) such that the first nonzero column c1 of
P1A has a nonzero entry on top. Hence, as in the LU-algorithm,

                                          L(m) [c1]-1 · P1 · A =                 0 1 X1
                                                                                 0 0 A1

in block form. Then let P2 be a permutation matrix (either elementary or Im) such that

                                      P2 · L(m) [c1]-1 · P1 · A =                   0 1 X1
                                                                                                   A
                                                                                    0  0
                                                                                                     1

and  the  first  nonzero  column  c2  of  A    has  a     nonzero     entry      on    top.     Thus,

                                            1

                                                                          0 1                               X1 
                          L(m) [k1, c2]-1 · P2 · L(m) [c1]-1 · P1 · A =  0 0                            0 1 X2 
                                                                                                        0 0 A2
    8 Matrix Algebra

in block form. Continue to obtain elementary permutation matrices P1, P2, . . . , Pr and columns c1, c2, . . . , cr
of lengths m, m - 1, . . . , such that

                                                        (LrPrLr-1Pr-1 · · · L2P2L1P1)A = U

where U is a row-echelon matrix and L j = L(m) k1, . . . , k j-1, c j -1 for each j, where the notation
means the first j - 1 columns are those of Im. It is not hard to verify that each L j has the form L j =

L(m)    k1,  ...,  k j-1,    c          where     c    is  a  column           of   length    m-         j + 1.  We    now     claim    that  each        permutation

                               j                    j

matrix Pk can be "moved past" each matrix L j to the right of it, in the sense that

                                                                               PkL j = LjPk

where   L    =  L(m)     k1,  ...,         k j-1,  c       for  some           column     c     of       length  m-       j + 1.  Given   that      this  is    true,  we

          j                                          j                                      j

obtain a factorization of the form

                                                   (Lr           ·  ·  ·  L    L    )(Pr  Pr-1  ·  ·  ·  P2P1)A  =  U

                                                           Lr-1             2    1

If  we  write  P   =  PrPr-1      ·  ·  ·  P2P1,  this  shows       that  PA        has   an  LU-factorization            because     Lr  L      ·  ·  ·  L L   is  lower

                                                                                                                                            r-1             21
triangular and invertible. All that remains is to prove the following rather technical result.

    Lemma 2.7.2

    Let Pk result from interchanging row k of Im with a row below it. If j < k, let c j be a column of
    length               1.  Then          there   is  another         column       c     of  length                   1  such    that
             m  -  j  +                                                                                    m  -  j  +
                                                                                      j

                                        Pk · L(m)      k1, . . . , k j-1, c j       = L(m)            k1,  ...,  k j-1,   c     · Pk

                                                                                                                            j

The proof is left as Exercise ??.
             .8. An Application to Input-Output Economic Models

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

         Engage Active Learning App!

   Vretta-Lyryx Engage is an active learning app designed to increase
  student engagement in reading linear algebra material. The content is
"chunked" into small blocks, each with an interactive assessment activity

                           to promote comprehension.

 Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

 .8 An Application to Input-Output Economic Models16

In 1973 Wassily Leontief was awarded the Nobel prize in economics for his work on mathematical mod-
els.17 Roughly speaking, an economic system in this model consists of several industries, each of which
produces a product and each of which uses some of the production of the other industries. The following
example is typical.

Example 2.8.1

A primitive society has three basic needs: food, shelter, and clothing. There are thus three
industries in the society--the farming, housing, and garment industries--that produce these
commodities. Each of these industries consumes a certain proportion of the total output of each
commodity according to the following table.

CONSUMPTION  Farming  Farming  OUTPUT   Garment
             Housing     0.4                0.3
             Garment     0.2   Housing      0.4
                         0.4       0.2      0.3
                                   0.6
                                   0.2

Find the annual prices that each industry must charge for its income to equal its expenditures.

16The applications in this section and the next are independent and may be taken in any order.
17See W. W. Leontief, "The world economy of the year 2000," Scientific American, Sept. 1980.
Matrix Algebra

Solution. Let p1, p2, and p3 be the prices charged per year by the farming, housing, and garment

industries, respectively, for their total output. To see how these prices are determined, consider the

farming industry. It receives p1 for its production in any year. But it consumes products from all
these industries in the following amounts (from row 1 of the table): 40% of the food, 20% of the

housing, and 30% of the clothing. Hence, the expenditures of the farming industry are

0.4p1 + 0.2p2 + 0.3p3, so           0.4p1 + 0.2p2 + 0.3p3 = p1

A similar analysis of the other two industries leads to the following system of equations.

                                    0.4p1 + 0.2p2 + 0.3p3 = p1
                                    0.2p1 + 0.6p2 + 0.4p3 = p2
                                    0.4p1 + 0.2p2 + 0.3p3 = p3

This has the matrix form Ep = p, where

                                     0.4 0.2 0.3                
                                                                   p1

                             E =  0.2 0.6 0.4  and p =  p2 

                                    0.4 0.2 0.3                                p3

The equations can be written as the homogeneous system

                                           (I - E)p = 0

where I is the 3 × 3 identity matrix, and the solutions are

                                                 2t 
                                           p =  3t 

                                                   2t

where t is a parameter. Thus, the pricing must be such that the total output of the farming industry

has the same value as the total output of the garment industry, whereas the total value of the
                             3
housing  industry  must  be  2  as  much.

In general, suppose an economy has n industries, each of which uses some (possibly none) of the

production of every industry. We assume first that the economy is closed (that is, no product is exported

or imported) and that all product is used. Given two industries i and j, let ei j denote the proportion of the

total annual output of industry j that is consumed by industry i. Then E = ei j is called the input-output

matrix for the economy. Clearly,

                                    0  ei j  1 for all i and j                                  (2.12)

Moreover, all the output from industry j is used by some industry (the model is closed), so

                                    e1 j + e2 j + · · · + ei j = 1 for each j                   (2.13)

This condition asserts that each column of E sums to 1. Matrices satisfying conditions (2.12) and (2.13)
are called stochastic matrices.
        .8. An Application to Input-Output Economic Models

    As in Example 2.8.1, let pi denote the price of the total annual production of industry i. Then pi is the
annual revenue of industry i. On the other hand, industry i spends ei1 p1 + ei2 p2 + · · · + ein pn annually for
the product it uses (ei j p j is the cost for product from industry j). The closed economic system is said to

be in equilibrium if the annual expenditure equals the annual revenue for each industry--that is, if

        ei1 p1 + ei2 p2 + · · · + ein pn = pi for each i = 1, 2, . . . , n


   p1

  p2  
      
If we write p =  .. , these equations can be written as the matrix equation
.

  pn

        Ep = p

This is called the equilibrium condition, and the solutions p are called equilibrium price structures.
The equilibrium condition can be written as

        (I - E)p = 0

which is a system of homogeneous equations for p. Moreover, there is always a nontrivial solution p.
Indeed, the column sums of I - E are all 0 (because E is stochastic), so the row-echelon form of I - E has
a row of zeros. In fact, more is true:

Theorem 2.8.1

Let E be any n × n stochastic matrix. Then there is a nonzero n × 1 vector p with nonnegative
entries such that Ep = p. If all the entries of E are positive, the matrix p can be chosen with all
entries positive.

    Theorem 2.8.1 guarantees the existence of an equilibrium price structure for any closed input-output
system of the type discussed here. The proof is beyond the scope of this book.18

   Example 2.8.2
   Find the equilibrium price structures for four industries if the input-output matrix is

                                                     0.6 0.2 0.1 0.1 
                                              E =  0.1 0.3 0.5 0.2   0.3 0.4 0.2 0 

                                                         0 0.1 0.2 0.7
   Find the prices if the total value of business is $1000.

                        
                           p1

   Solution. If p =  p3   p2  is the equilibrium price structure, then the equilibrium condition reads
                           p4

   18The interested reader is referred to P. Lancaster's Theory of Matrices (New York: Academic Press, 1969) or to E. Seneta's
Non-negative Matrices (New York: Wiley, 1973).
    Matrix Algebra

    Ep = p. When we write this as (I - E)p = 0, the methods of Chapter 1 yield the following family

    of solutions:          44t 

                          p =  51t   39t 

                          47t

    where t is a parameter. If we insist that p1 + p2 + p3 + p4 = 1000, then t = 5.525. Hence

                                243.09 
                          p =  281.76   215.47 

                                  259.67

    to five figures.

The Open Model

We now assume that there is a demand for products in the open sector of the economy, which is the part of
the economy other than the producing industries (for example, consumers). Let di denote the total value of
the demand for product i in the open sector. If pi and ei j are as before, the value of the annual demand for
product i by the producing industries themselves is ei1 p1 + ei2 p2 + · · · + ein pn, so the total annual revenue
pi of industry i breaks down as follows:

                      pi = (ei1 p1 + ei2 p2 + · · · + ein pn) + di for each i = 1, 2, . . . , n

                   
                      d1
                      ..
The column d =        .    is called the demand matrix, and this gives a matrix equation

                      dn

                          p = Ep+d

or

                          (I - E)p = d                                                           (2.14)

This is a system of linear equations for p, and we ask for a solution p with every entry nonnegative. Note
that every entry of E is between 0 and 1, but the column sums of E need not equal 1 as in the closed model.

    Before proceeding, it is convenient to introduce a useful notation. If A = ai j and B = bi j are
matrices of the same size, we write A > B if ai j > bi j for all i and j, and we write A  B if ai j  bi j for all
i and j. Thus P  0 means that every entry of P is nonnegative. Note that A  0 and B  0 implies that
AB  0.

    Now, given a demand matrix d  0, we look for a production matrix p  0 satisfying equation (2.14).
This certainly exists if I - E is invertible and (I - E)-1  0. On the other hand, the fact that d  0 means
any solution p to equation (2.14) satisfies p  Ep. Hence, the following theorem is not too surprising.
                         .8. An Application to Input-Output Economic Models

Theorem 2.8.2

Let E  0 be a square matrix. Then I - E is invertible and (I - E)-1  0 if and only if there exists
a column p > 0 such that p > Ep.

Heuristic Proof.

If (I - E)-1  0, the existence of p > 0 with p > Ep is left as Exercise ??. Conversely, suppose such a
column p exists. Observe that

                         (I - E)(I + E + E2 + · · · + Ek-1) = I - Ek

holds for all k  2. If we can show that every entry of Ek approaches 0 as k becomes large then, intuitively,

the infinite matrix sum  U = I +E +E2 +···

exists and (I - E)U = I. Since U  0, this does it. To show that Ek approaches 0, it suffices to show that
EP < µP for some number µ with 0 < µ < 1 (then EkP < µkP for all k  1 by induction). The existence
of µ is left as Exercise ??.

    The condition p > Ep in Theorem 2.8.2 has a simple economic interpretation. If p is a production
matrix, entry i of Ep is the total value of all product used by industry i in a year. Hence, the condition

p > Ep means that, for each i, the value of product produced by industry i exceeds the value of the product
it uses. In other words, each industry runs at a profit.

Example 2.8.3            0.3 
                         0.2 , show that I - E is invertible and (I - E)-1  0.
         0.6 0.2         0.1
If E =  0.1 0.4

           0.2 0.5

Solution. Use p = (3, 2, 2)T in Theorem 2.8.2.

    If p0 = (1, 1, 1)T , the entries of Ep0 are the row sums of E. Hence p0 > Ep0 holds if the row sums of
E are all less than 1. This proves the first of the following useful facts (the second is Exercise ??).

   Corollary 2.8.1
   Let E  0 be a square matrix. In each case, I - E is invertible and (I - E)-1  0:

       1. All row sums of E are less than 1.
       2. All column sums of E are less than 1.
        Matrix Algebra

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

 . An Application to Markov Chains

Many natural phenomena progress through various stages and can be in a variety of states at each stage.
For example, the weather in a given city progresses day by day and, on any given day, may be sunny or
rainy. Here the states are "sun" and "rain," and the weather progresses from one state to another in daily
stages. Another example might be a football team: The stages of its evolution are the games it plays, and
the possible states are "win," "draw," and "loss."

    The general setup is as follows: A real conceptual "system" is run generating a sequence of outcomes.
The system evolves through a series of "stages," and at any stage it can be in any one of a finite number of
"states." At any given stage, the state to which it will go at the next stage depends on the past and present
history of the system--that is, on the sequence of states it has occupied to date.

   Definition 2.5 Markov Chain
   A Markov chain is such an evolving system wherein the state to which it will go next depends
   only on its present state and does not depend on the earlier history of the system.19

    Even in the case of a Markov chain, the state the system will occupy at any stage is determined only
in terms of probabilities. In other words, chance plays a role. For example, if a football team wins a

   19The name honours Andrei Andreyevich Markov (1856-1922) who was a professor at the university in St. Petersburg,
Russia.
                                                                                        . . An Application to Markov Chains

particular game, we do not know whether it will win, draw, or lose the next game. On the other hand, we

may know that the team tends to persist in winning streaks; for example, if it wins one game it may win
                 1                              4                                 1
the  next  game  2  of  the  time,      lose    10  of  the    time,  and  draw   10  of  the  time.    These  fractions   are  called    the

probabilities of these various possibilities. Similarly, if the team loses, it may lose the next game with
             1                                                                1,                                   1.
probability  2  (that  is,   half  the  time),  win     with   probability        and   draw   with  probability        The  probabilities
                                                                              4                                    4
of the various outcomes after a drawn game will also be known.

    We shall treat probabilities informally here: The probability that a given event will occur is the long-
run proportion of the time that the event does indeed occur. Hence, all probabilities are numbers between
0 and 1. A probability of 0 means the event is impossible and never occurs; events with probability 1 are
certain to occur.

    If a Markov chain is in a particular state, the probabilities that it goes to the various states at the next
stage of its evolution are called the transition probabilities for the chain, and they are assumed to be
known quantities. To motivate the general conditions that follow, consider the following simple example.
Here the system is a man, the stages are his successive lunches, and the states are the two restaurants he
chooses.

     Example 2.9.1

     A man always eats lunch at one of two restaurants, A and B. He never eats at A twice in a row.
     However, if he eats at B, he is three times as likely to eat at B next time as at A. Initially, he is
     equally likely to eat at either restaurant.

        a. What is the probability that he eats at A on the third day after the initial one?

        b. What proportion of his lunches does he eat at A?

     Solution. The table of transition probabilities follows. The A column indicates that if he eats at A
     on one day, he never eats there again on the next day and so is certain to go to B.

                                                                                  Present Lunch

                                                                                  A            B

                                               Next                   A           0            0.25
                                              Lunch
                                                                      B           1            0.75

     The  B  column  shows   that,      if  he  eats    at  B  on  one  day,  he  will  eat  there  on  the  next  day  3  of  the  time
                                                                                                                        4
     and  switches  to       only  1  of  the   time.
                        A          4

     The restaurant he visits on a given day is not determined. The most that we can expect is to know

     the probability that he will visit A or B on that day.

                 (m) 
     Let sm =  s1(m)  denote the state vector for day m. Here s(1m) denotes the probability that he

                   s2

                                            (m)

     eats at A on day m, and s2 is the probability that he eats at B on day m. It is convenient to let s0
     correspond to the initial day. Because he is equally likely to eat at A or B on that initial day,

     (0)                (0)                           0.5 0.5 . Now let
     s1 = 0.5 and s2 = 0.5, so s0 =

                                                               P = 0 0.25 1 0.75
6 Matrix Algebra

denote the transition matrix. We claim that the relationship

                                      sm+1 = Psm

holds for all integers m  0. This will be derived later; for now, we use it as follows to successively
compute s1, s2, s3, . . . .

                  s1 = Ps0 =  0 0.25        0.5 0.5 = 0.125 0.875
                  s2 = Ps1 =  1 0.75        0.125 0.875 = 0.21875 0.78125
                  s3 = Ps2 =                0.21875 0.78125 = 0.1953125 0.8046875
                              0 0.25
                              1 0.75

                              0 0.25
                              1 0.75

Hence, the probability that his third lunch (after the initial one) is at A is approximately 0.195,
whereas the probability that it is at B is 0.805. If we carry these calculations on, the next state
vectors are (to five figures):

                      s4 = 0.20117 0.79883  s5 = 0.19971 0.80029
                      s6 = 0.20007 0.79993  s7 = 0.19998 0.80002

Moreover, as m increases the entries of sm get closer and closer to the corresponding entries of
  0.2 0.8 . Hence, in the long run, he eats 20% of his lunches at A and 80% at B.

Present        Next       Example 2.9.1 incorporates most of the essential features of all Markov
 State         State  chains. The general model is as follows: The system evolves through
               state  various stages and at each stage can be in exactly one of n distinct states. It
        p1 j          progresses through a sequence of states as time goes on. If a Markov chain
 state p2 j      1    is in state j at a particular stage of its development, the probability pi j that
                      it goes to state i at the next stage is called the transition probability. The
    j          state  n × n matrix P = pi j is called the transition matrix for the Markov
         pn j    2    chain. The situation is depicted graphically in the diagram.

               state      We make one important assumption about the transition matrix P =
                 n     pi j : It does not depend on which stage the process is in. This assumption
                      means that the transition probabilities are independent of time--that is,
                      they do not change as time goes on. It is this assumption that distinguishes
                      Markov chains in the literature of this subject.
                                                       . . An Application to Markov Chains

Example 2.9.2
Suppose the transition matrix of a three-state Markov chain is

                                                     Present state

                                                     12 3

  p11 p12 p13        0.3 0.1 0.6  1

P =  p21 p22 p23  =  0.5 0.9 0.2  2 Next state
                                                     0.2 0.0 0.2 3
  p31 p32 p33

If, for example, the system is in state 2, then column 2 lists the probabilities of where it goes next.

Thus, the probability is p12 = 0.1 that it goes from state 2 to state 1, and the probability is
p22 = 0.9 that it goes from state 2 to state 2. The fact that p32 = 0 means that it is impossible for it
to go from state 2 to state 3 at the next stage.

Consider the jth column of the transition matrix P.

                  
                     p1 j

                    p2 j                             
                                                     
                   .. 
                  .

                    pn j

If the system is in state j at some stage of its evolution, the transition probabilities p1 j, p2 j, . . . , pn j
represent the fraction of the time that the system will move to state 1, state 2, . . . , state n, respectively, at
the next stage. We assume that it has to go to some state at each transition, so the sum of these probabilities
is 1:

                                         p1 j + p2 j + · · · + pn j = 1 for each j

Thus, the columns of P all sum to 1 and the entries of P lie between 0 and 1. Hence P is called a stochastic
matrix.

    As in Example 2.9.1, we introduce the following notation: Let si(m) denote the probability that the
system is in state i after m transitions. The n × 1 matrices

          (m) 
            s1

         
             (m) 
         
  sm  =   s2        m = 0, 1, 2, . . .
               .
          ..      
                  

          (m) 
            sn

are called the state vectors for the Markov chain. Note that the sum of the entries of sm must equal 1
because the system must be in some state after m transitions. The matrix s0 is called the initial state
vector for the Markov chain and is given as part of the data of the particular chain. For example, if the
chain has only two states, then an initial vector s0 = 10 means that it started in state 1. If it started in

state 2, the initial vector would be s0 = 01 . If s0 = 0.5 0.5 , it is equally likely that the system started
in state 1 or in state 2.
8 Matrix Algebra

Theorem 2.9.1
Let P be the transition matrix for an n-state Markov chain. If sm is the state vector at stage m, then

                                                    sm+1 = Psm
for each m = 0, 1, 2, . . . .

Heuristic Proof. Suppose that the Markov chain has been run N times, each time starting with the same
initial state vector. Recall that pi j is the proportion of the time the system goes from state j at some stage
to state i at the next stage, whereas si(m) is the proportion of the time it is in state i at stage m. Hence

                                                      sm+1 i N

is (approximately) the number of times the system is in state i at stage m + 1. We are going to calculate
this number another way. The system got to state i at stage m + 1 through some other state (say state j)
at stage m. The number of times it was in state j at that stage is (approximately) s j(m)N, so the number of

times it got to state i via state j is pi j(s j(m)N). Summing over j gives the number of times the system is in
state i (at stage m + 1). This is the number we calculated before, so

                             (m+1)          (m)             (m)             (m)
                            si N = pi1s1 N + pi2s2 N + · · · + pinsn N

                     (m+1)       (m)        (m)                  (m)
Dividing by N gives si = pi1s1 + pi2s2 + · · · + pinsn for each i, and this can be expressed as the
matrix equation sm+1 = Psm.

    If the initial probability vector s0 and the transition matrix P are given, Theorem 2.9.1 gives s1, s2, s3, . . . ,
one after the other, as follows:

                                                          s1 = Ps0
                                                          s2 = Ps1
                                                          s3 = Ps2

                                                               ..
                                                               .

Hence, the state vector sm is completely determined for each m = 0, 1, 2, . . . by P and s0.

Example 2.9.3
A wolf pack always hunts in one of three regions R1, R2, and R3. Its hunting habits are as follows:

1. If it hunts in some region one day, it is as likely as not to hunt there again the next day.

2. If it hunts in R1, it never hunts in R2 the next day.

3. If it hunts in R2 or R3, it is equally likely to hunt in each of the other regions the next day.

If the pack hunts in R1 on Monday, find the probability that it hunts there on Thursday.

Solution. The stages of this process are the successive days; the states are the three regions. The

transition matrix P is determined as follows (see the table): The first habit asserts that
                        1.
p11  =  p22  =  p33  =      Now  column  1  displays  what  happens   when  the  pack  starts  in  R1:  It  never
                        2
                                                                               . . An Application to Markov Chains

goes  to  state  2,  so  p21  =  0   and,  because    the  column    must  sum  to  1,  p31    =  1.  Column       2  describes

                                                   1            and            equal  (by         2       so                  1
what  happens    if  it  starts  in  R2:           2  and                 are              habit     3),                      2
                                           p22  =          p12       p32                                      p12  =  p32  =

because the column sum must equal 1. Column 3 is filled in a similar way.

                                                           R1 R2 R3

                                                              111

                                                      R1 2 4 4
                                                      R2 0 1 1

                                                                      24
                                                              111

                                                      R3 2 4 2

                                                          1

Now let Monday be the initial stage. Then s0 =  0  because the pack hunts in R1 on that day.
                                                             0

Then s1, s2, and s3 describe Tuesday, Wednesday, and Thursday, respectively, and we compute
them using Theorem 2.9.1.

                                       1                           3                          11 
                                       2                           8                          32 
                         s1 = Ps0 =  0                               1                       6
                                                      s2 = Ps1 =     8         s3 = Ps2 =  32 
                                                                                             

                                                1                                                     15
                                                2                                                     32
                                                                       4
                                                                       8

Hence,    the  probability    that   the   pack  hunts  in  Region   R1   on  Thursday     is  11

                                                                                               32 .

Steady State Vector

Another phenomenon that was observed in Example 2.9.1 can be expressed in general terms. The state
vectors s0, s1, s2, . . . were calculated in that example and were found to "approach" s = 0.2 0.8 . This
means that the first component of sm becomes and remains very close to 0.2 as m becomes large, whereas
the second component gets close to 0.8 as m increases. When this is the case, we say that sm converges to
s. For large m, then, there is very little error in taking sm = s, so the long-term probability that the system
is in state 1 is 0.2, whereas the probability that it is in state 2 is 0.8. In Example 2.9.1, enough state vectors
were computed for the limiting vector s to be apparent. However, there is a better way to do this that works
in most cases.

    Suppose P is the transition matrix of a Markov chain, and assume that the state vectors sm converge to
a limiting vector s. Then sm is very close to s for sufficiently large m, so sm+1 is also very close to s. Thus,
the equation sm+1 = Psm from Theorem 2.9.1 is closely approximated by

                                                           s = Ps

so it is not surprising that s should be a solution to this matrix equation. Moreover, it is easily solved
because it can be written as a system of homogeneous linear equations

                                                        (I - P)s = 0

with the entries of s as variables.
        Matrix Algebra

    In Example 2.9.1, where P = 0 0.25 1 0.75 , the general solution to (I - P)s = 0 is s = t4t , where t
is a parameter. But if we insist that the entries of S sum to 1 (as must be true of all state vectors), we find
t = 0.2 and so s = 0.2 0.8 as before.

    All this is predicated on the existence of a limiting vector for the sequence of state vectors of the
Markov chain, and such a vector may not always exist. However, it does exist in one commonly occurring
situation. A stochastic matrix P is called regular if some power Pm of P has every entry greater than zero.
The matrix P = 0 0.25 1 0.75 of Example 2.9.1 is regular (in this case, each entry of P2 is positive), and
the general theorem is as follows:

    Theorem 2.9.2
   Let P be the transition matrix of a Markov chain and assume that P is regular. Then there is a
   unique column matrix s satisfying the following conditions:

       1. Ps = s.

       2. The entries of s are positive and sum to 1.

   Moreover, condition 1 can be written as

                                                       (I - P)s = 0

   and so gives a homogeneous system of linear equations for s. Finally, the sequence of state vectors
   s0, s1, s2, . . . converges to s in the sense that if m is large enough, each entry of sm is closely
   approximated by the corresponding entry of s.

This theorem will not be proved here.20
    If P is the regular transition matrix of a Markov chain, the column s satisfying conditions 1 and 2 of

Theorem 2.9.2 is called the steady-state vector for the Markov chain. The entries of s are the long-term
probabilities that the chain will be in each of the various states.

    Example 2.9.4
    A man eats one of three soups--beef, chicken, and vegetable--each day. He never eats the same
    soup two days in a row. If he eats beef soup on a certain day, he is equally likely to eat each of the
    others the next day; if he does not eat beef soup, he is twice as likely to eat it the next day as the
    alternative.

       a. If he has beef soup one day, what is the probability that he has it again two days later?

       b. What are the long-run probabilities that he eats each of the three soups?

   20The interested reader can find an elementary proof in J. Kemeny, H. Mirkil, J. Snell, and G. Thompson, Finite Mathematical
Structures (Englewood Cliffs, N.J.: Prentice-Hall, 1958).
                                                                         . . An Application to Markov Chains

Solution. The states here are B, C, and V , the three soups. The transition matrix P is given in the
table. (Recall that, for each state, the corresponding column lists the probabilities for the next
state.)

                                                        BCV

                                                     B0 2 2

                                                                 33

                                                     C1012         3

                                                     V110

                                                           23

If he has beef soup initially, then the initial state vector is

                                                           1
                                                     s0 =  0 

                                                              0

Then two days later the state vector is s2. If P is the transition matrix, then

                                                  0                          4
                                               1                             1
                               s1    = Ps0  =        1  ,      s2  = Ps1  =     1  
                                               2                             6
                                                     1                          1

so  he  eats  beef  soup  two  days  later  with  probability  2.  This  answers  (a.)  and  also  shows  that  he

               and  vegetable  soup                            3   1.
eats  chicken                        each   with     probability
                                                                   6
To find the long-run probabilities, we must find the steady-state vector s. Theorem 2.9.2 applies
because P is regular (P2 has positive entries), so s satisfies Ps = s. That is, (I - P)s = 0 where

                                                         6 -4 -4 
                                                     1
                                            I-P   =     -3         6 -2 
                                                     6
                                                        -3 -2 6

                     4t                                                          0.4 

The solution is s =  3t , where t is a parameter, and we use s =  0.3  because the entries of

                          3t                                                       0.3

s must sum to 1. Hence, in the long run, he eats beef soup 40% of the time and eats chicken soup

and vegetable soup each 30% of the time.
        Matrix Algebra

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.
                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.
                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!
Chapter 3

           Determinants and Diagonalization

With each square matrix we can calculate a number, called the determinant of the matrix, which tells us
whether or not the matrix is invertible. In fact, determinants can be used to give a formula for the inverse
of a matrix. They also arise in calculating certain numbers (called eigenvalues) associated with the matrix.
These eigenvalues are essential to a technique called diagonalization that is used in many applications
where it is desired to predict the future behaviour of a system. For example, we use it to predict whether a
species will become extinct.

    Determinants were first studied by Leibnitz in 1696, and the term "determinant" was first used in
1801 by Gauss is his Disquisitiones Arithmeticae. Determinants are much older than matrices (which
were introduced by Cayley in 1878) and were used extensively in the eighteenth and nineteenth centuries,
primarily because of their significance in geometry (see Section 4.4). Although they are somewhat less
important today, determinants still play a role in the theory and application of matrix algebra.

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

           123
Determinants and Diagonalization

 . The Cofactor Expansion

In Section 2.4 we defined the determinant of a 2 × 2 matrix A = a b as follows:1
                                                                                cd

                   det A = a b = ad - bc
                               cd

and showed (in Example 2.4.4) that A has an inverse if and only if det A = 0. One objective of this chapter
is to do this for any square matrix A. There is no difficulty for 1 × 1 matrices: If A = [a], we define
det A = det [a] = a and note that A is invertible if and only if a = 0.

    If A is 3 × 3 and invertible, we look for a suitable definition of det A by trying to carry A to the identity
matrix by row operations. The first column is not zero (A is invertible); suppose the (1, 1)-entry a is not
zero. Then row operations give

a b c  a b c  a b                                  c  a b c 

A =  d e f    ad ae a f    0 ae - bd a f - cd  =  0 u a f - cd 

gh i  ag ah ai                    0 ah - bg ai - cg          0 v ai - cg

where u = ae - bd and v = ah - bg. Since A is invertible, one of u and v is nonzero (by Example 2.4.11);
suppose that u = 0. Then the reduction proceeds

a b c  a b                            c  a b c 

A   0 u a f - cd    0 u a f - cd    0 u a f - cd 

      0 v ai - cg                 0 uv u(ai - cg)    00 w

where w = u(ai - cg) - v(a f - cd) = a(aei + b f g + cdh - ceg - a f h - bdi). We define

      det A = aei + b f g + cdh - ceg - a f h - bdi                                              (3.1)

and observe that det A = 0 because a det A = w = 0 (is invertible).

    To motivate the definition below, collect the terms in Equation 3.1 involving the entries a, b, and c in
row 1 of A:

                  abc
      det A = d e f = aei + b f g + cdh - ceg - a f h - bdi

                  gh i

                                  = a(ei - f h) - b(di - f g) + c(dh - eg)

                                  =a e f -b d f +c d e
                                  hi  gi             gh

This last expression can be described as follows: To compute the determinant of a 3 × 3 matrix A, multiply
each entry in row 1 by a sign times the determinant of the 2 × 2 matrix obtained by deleting the row and
column of that entry, and add the results. The signs alternate down row 1, starting with +. It is this
observation that we generalize below.

1Determinants are commonly written |A| = det A using vertical bars. We will use both notations.
                                               . . The Cofactor Expansion

Example 3.1.1

                2 3 7 0 6              -4 6    -4 0
               det  -4 0 6  = 2 5 0 - 3 1 0 + 7 1 5
               150

                    = 2(-30) - 3(-6) + 7(-20)

                    = -182

    This suggests an inductive method of defining the determinant of any square matrix in terms of de-
terminants of matrices one size smaller. The idea is to define determinants of 3 × 3 matrices in terms of
determinants of 2 × 2 matrices, then we do 4 × 4 matrices in terms of 3 × 3 matrices, and so on.

    To describe this, we need some terminology.

   Definition 3.1 Cofactors of a Matrix
   Assume that determinants of (n - 1) × (n - 1) matrices have been defined. Given the n × n matrix
   A, let

          Ai j denote the (n - 1) × (n - 1) matrix obtained from A by deleting row i and column j.

   Then the (i, j)-cofactor ci j(A) is the scalar defined by

                                                ci j(A) = (-1)i+ j det (Ai j)

   Here (-1)i+ j is called the sign of the (i, j)-position.

The sign of a position is clearly 1 or -1, and the following diagram is useful for remembering it:

                     + - + - ··· 

                      -  +  -  +       ···  
                                            
                      +  -  +  -       ···
                                            
                                            
                     - + - + ··· 
                      ... ... ... ...
                                            

Note that the signs alternate along each row and column with + in the upper left corner.

Example 3.1.2

Find the cofactors of positions (1, 2), (3, 1), and (2, 3) in the following matrix.

                                                      3 -1 6 
                                               A= 5 2 7 

                                                        8 94

Solution. Here A12 is the matrix 5 7 8 4 that remains when row 1 and column 2 are deleted. The
  6 Determinants and Diagonalization

   sign of position (1, 2) is (-1)1+2 = -1 (this is also the (1, 2)-entry in the sign diagram), so the
   (1, 2)-cofactor is

                      c12(A) = (-1)1+2 5 7 8 4 = (-1)(5 · 4 - 7 · 8) = (-1)(-36) = 36
   Turning to position (3, 1), we find

                     c31(A) = (-1)3+1A31 = (-1)3+1 -1 6 2 7 = (+1)(-7 - 12) = -19
   Finally, the (2, 3)-cofactor is

                      c23(A) = (-1)2+3A23 = (-1)2+3 3 -1 8 9 = (-1)(27 + 8) = -35
   Clearly other cofactors can be found--there are nine in all, one for each position in the matrix.

    We can now define det A for any square matrix A

   Definition 3.2 Cofactor expansion of a Matrix
   Assume that determinants of (n - 1) × (n - 1) matrices have been defined. If A = ai j is n × n
   define

                                  det A = a11c11(A) + a12c12(A) + · · · + a1nc1n(A)
   This is called the cofactor expansion of det A along row 1.

    It asserts that det A can be computed by multiplying the entries of row 1 by the corresponding cofac-
tors, and adding the results. The astonishing thing is that det A can be computed by taking the cofactor
expansion along any row or column: Simply multiply each entry of that row or column by the correspond-
ing cofactor and add.

   Theorem 3.1.1: Cofactor Expansion Theorem2
   The determinant of an n × n matrix A can be computed by using the cofactor expansion along any
   row or column of A. That is det A can be computed by multiplying each entry of the row or
   column by the corresponding cofactor and adding the results.

    The proof will be given in Section 3.6.

    2The cofactor expansion is due to Pierre Simon de Laplace (1749-1827), who discovered it in 1772 as part of a study of
linear differential equations. Laplace is primarily remembered for his work in astronomy and applied mathematics.
                                                                                       . . The Cofactor Expansion

   Example 3.1.3

                                            3 4 5
   Compute the determinant of A =  1 7 2 .

                                               9 8 -6

   Solution. The cofactor expansion along the first row is as follows:

                                   det A = 3c11(A) + 4c12(A) + 5c13(A)
                                          = 3 7 2 8 -6 - 4 1 2 9 -6 + 5 1 7 9 8
                                          = 3(-58) - 4(-24) + 5(-55)
                                          = -353

   Note that the signs alternate along the row (indeed along any row or column). Now we compute
   det A by expanding along the first column.

                                    det A = 3c11(A) + 1c21(A) + 9c31(A)
                                           = 3 7 2 8 -6 - 4 5 8 -6 + 9 4 5 7 2
                                           = 3(-58) - (-64) + 9(-27)
                                           = -353

   The reader is invited to verify that det A can be computed by expanding along any other row or
   column.

    The fact that the cofactor expansion along any row or column of a matrix A always gives the same
result (the determinant of A) is remarkable, to say the least. The choice of a particular row or column can
simplify the calculation.

   Example 3.1.4

                                     3 0 0 0
   Compute det A where A =  2 6 0 -1   5 1 2 0 .

                                        -6 3 1 0

   Solution. The first choice we must make is which row or column to use in the cofactor expansion.
   The expansion involves multiplying entries by cofactors, so the work is minimized when the row
   or column contains as many zero entries as possible. Row 1 is a best choice in this matrix (column
   4 would do as well), and the expansion is

                                   det A = 3c11(A) + 0c12(A) + 0c13(A) + 0c14(A)
                                                 12 0

                                          = 3 6 0 -1
                                                 31 0
8 Determinants and Diagonalization

This is the first stage of the calculation, and we have succeeded in expressing the determinant of
the 4 × 4 matrix A in terms of the determinant of a 3 × 3 matrix. The next stage involves this 3 × 3
matrix. Again, we can use any row or column for the cofactor expansion. The third column is
preferred (with two zeros), so

                            det A = 3 0 6 0 3 1 - (-1) 1 2 3 1 + 0 1 2 6 0
                                   = 3[0 + 1(-5) + 0]
                                   = -15

This completes the calculation.

    Computing the determinant of a matrix A can be tedious. For example, if A is a 4 × 4 matrix, the
cofactor expansion along any row or column involves calculating four cofactors, each of which involves
the determinant of a 3 × 3 matrix. And if A is 5 × 5, the expansion involves five determinants of 4 × 4
matrices! There is a clear need for some techniques to cut down the work.3

    The motivation for the method is the observation (see Example 3.1.4) that calculating a determinant
is simplified a great deal when a row or column consists mostly of zeros. (In fact, when a row or column
consists entirely of zeros, the determinant is zero--simply expand along that row or column.)

    Recall next that one method of creating zeros in a matrix is to apply elementary row operations to it.
Hence, a natural question to ask is what effect such a row operation has on the determinant of the matrix.
It turns out that the effect is easy to determine and that elementary column operations can be used in the
same way. These observations lead to a technique for evaluating determinants that greatly reduces the
labour involved. The necessary information is given in Theorem 3.1.2.

Theorem 3.1.2
Let A denote an n × n matrix.

   1. If A has a row or column of zeros, det A = 0.

   2. If two distinct rows (or columns) of A are interchanged, the determinant of the resulting
       matrix is - det A.

   3. If a row (or column) of A is multiplied by a constant u, the determinant of the resulting
       matrix is u( det A).

   4. If two distinct rows (or columns) of A are identical, det A = 0.

   5. If a multiple of one row of A is added to a different row (or if a multiple of a column is added
       to a different column), the determinant of the resulting matrix is det A.

a b c                               a b c a b
3If A =  d e f  we can calculate det A by considering  d e f d e  obtained from A by adjoining columns

gh i                                gh igh

1 and 2 on the right. Then det A = aei + b f g + cdh - ceg - a f h - bdi, where the positive terms aei, b f g, and cdh are the

products down and to the right starting at a, b, and c, and the negative terms ceg, a f h, and bdi are the products down and to the

left starting at c, a, and b. Warning: This rule does not apply to n × n matrices where n > 3 or n = 2.
                                                                   . . The Cofactor Expansion

Proof. We prove properties 2, 4, and 5 and leave the rest as exercises.

    Property 2. If A is n × n, this follows by induction on n. If n = 2, the verification is left to the reader.
If n > 2 and two rows are interchanged, let B denote the resulting matrix. Expand det A and det B along a
row other than the two that were interchanged. The entries in this row are the same for both A and B, but
the cofactors in B are the negatives of those in A (by induction) because the corresponding (n -1) ×(n -1)
matrices have two rows interchanged. Hence, det B = - det A, as required. A similar argument works if
two columns are interchanged.

    Property 4. If two rows of A are equal, let B be the matrix obtained by interchanging them. Then
B = A, so det B = detA. But det B = - det A by property 2, so det A = det B = 0. Again, the same
argument works for columns.

    Property 5. Let B be obtained from A = ai j by adding u times row p to row q. Then row q of B is

          (aq1 + uap1, aq2 + uap2, . . . , aqn + uapn)

The cofactors of these elements in B are the same as in A (they do not involve row q): in symbols,
cq j(B) = cq j(A) for each j. Hence, expanding B along row q gives

det B = (aq1 + uap1)cq1(A) + (aq2 + uap2)cq2(A) + · · · + (aqn + uapn)cqn(A)
      = [aq1cq1(A) + aq2cq2(A) + · · · + aqncqn(A)] + u[ap1cq1(A) + ap2cq2(A) + · · · + apncqn(A)]
      = det A + u det C

where C is the matrix obtained from A by replacing row q by row p (and both expansions are along row
q). Because rows p and q of C are equal, det C = 0 by property 4. Hence, det B = det A, as required. As
before, a similar proof holds for columns.

To illustrate Theorem 3.1.2, consider the following determinants.

3 -1 2

2 5 1 =0          (because the last row consists of zeros)

0 00

3 -1 5    5 -1 3

2 8 7 =- 7 8 2    (because two columns are interchanged)

1 2 -1    -1 2 1

81 2      81 2

3 0 9 =3 1 0 3    (because the second row of the matrix on the left is 3 times
                  the second row of the matrix on the right)
1 2 -1    1 2 -1

212               (because two columns are identical)
4 0 4 =0
131

252       0 9 20

-1 2 9 = -1 2 9   (because twice the second row of the matrix on the left was
                  added to the first row)
311       31 1

The following four examples illustrate how Theorem 3.1.2 is used to evaluate determinants.
    Determinants and Diagonalization

Example 3.1.5
                                1 -1 3 

Evaluate det A when A =  1 0 -1 .
                                  216

Solution. The matrix does have zero entries, so expansion along (say) the second row would
involve somewhat less work. However, a column operation can be used to get a zero in position
(2, 3)--namely, add column 1 to column 3. Because this does not change the value of the
determinant, we obtain

                      1 -1 3 1 -1 4 det A = 1 0 -1 = 1 0 0 = - -1 4 = 12
                                   2 1 6 2 18 18

where we expanded the second 3 × 3 matrix along row 2.

Example 3.1.6

a b c                  a+x b+y c+z 

If det  p q r  = 6, evaluate det A where A =  3x 3y 3z .

xyz                   -p -q -r

Solution. First take common factors out of rows 2 and 3.

                                      a+x b+y c+z 
               det A = 3(-1) det  x y z 

                                           pqr

Now subtract the second row from the first and interchange the last two rows.

               a b c  a b c

               det A = -3 det  x y z  = 3 det  p q r  = 3 · 6 = 18

               pqr                                        xyz

    The determinant of a matrix is a sum of products of its entries. In particular, if these entries are
polynomials in x, then the determinant itself is a polynomial in x. It is often of interest to determine which
values of x make the determinant zero, so it is very useful if the determinant is given in factored form.
Theorem 3.1.2 can help.

   Example 3.1.7

                                                                    1 x x
   Find the values of x for which det A = 0, where A =  x 1 x .

                                                                       xx1
                                                                                       . . The Cofactor Expansion

Solution. To evaluate det A, first subtract x times row 1 from rows 2 and 3.

                   1 x x 1 x det A = x 1 x = 0 1 - x2 x x - x2 = 1 - x22 x - x22
                               xx1               0 x - x2 1 - x2                     x-x 1-x

At this stage we could simply evaluate the determinant (the result is 2x3 - 3x2 + 1). But then we
would have to factor this polynomial to find the values of x that make it zero. However, this
factorization can be obtained directly by first factoring each entry in the determinant and taking a
common factor of (1 - x) from each row.

                det A =      (1 - x)(1 + x)         x(1 - x)            = (1 - x)2 1 + x x x 1 + x
                                x(1 - x)         (1 - x)(1 + x)         = (1 - x)2(2x + 1)

Hence,  det  A  =  0  means  (1 - x)2(2x + 1)    =  0,  that  is  x  =  1  or  x  =  -1.

                                                                                        2

Example 3.1.8                              a1 2

If a1, a2, and a3 are given show that      a2     = (a3 - a1)(a3 - a2)(a2 - a1)

                                   1 a1      2
                              det  1 a2    a2
                                              3
                                     1 a3

Solution. Begin by subtracting row 1 from rows 2 and 3, and then expand along column 1:

                   1 a1 a1   2             1 a1                   a1 2

        det  1        a2  a2   =    det    0     a2 - a1      a2 - a2      =      det      a2 - a1  a2 - a2
                                                                                           a3 - a1
                            2                                   21                                    21
                   1      a2               0                  a2 - a2
                      a3                         a3 - a1                                            a2 - a2
                            3                                   31
                                                                                                      31

Now (a2 - a1) and (a3 - a1) are common factors in rows 1 and 2, respectively, so

                             1 a1 a12

                      det  1    a2  a2      = (a2 - a1)(a3 - a1) det                 1 a2 + a1
                                                                                     1 a3 + a1
                                      2
                             1      a2
                                a3
                                      3

                                             = (a2 - a1)(a3 - a1)(a3 - a2)

The matrix in Example 3.1.8 is called a Vandermonde matrix, and the formula for its determinant can be
generalized to the n × n case (see Theorem 3.2.7).

    If A is an n × n matrix, forming uA means multiplying every row of A by u. Applying property 3 of
Theorem 3.1.2, we can take the common factor u out of each row and so obtain the following useful result.
Determinants and Diagonalization

Theorem 3.1.3
If A is an n × n matrix, then det (uA) = un det A for any number u.

The next example displays a type of matrix whose determinant is easy to compute.

Example 3.1.9

                          a 0 0 0
Evaluate det A if A =  v w c 0   u b 0 0 .

                              x y zd

                                                            b00
Solution. Expand along row 1 to get det A = a w c 0 . Now expand this along the top row to

                                                            y zd
get det A = ab c 0 = abcd, the product of the main diagonal entries.

                    zd

    A square matrix is called a lower triangular matrix if all entries above the main diagonal are zero
(as in Example 3.1.9). Similarly, an upper triangular matrix is one for which all entries below the main
diagonal are zero. A triangular matrix is one that is either upper or lower triangular. Theorem 3.1.4
gives an easy rule for calculating the determinant of any triangular matrix. The proof is like the solution
to Example 3.1.9.

   Theorem 3.1.4
   If A is a square triangular matrix, then det A is the product of the entries on the main diagonal.

Theorem 3.1.4 is useful in computer calculations because it is a routine matter to carry a matrix to trian-
gular form using row operations.

    Block matrices such as those in the next theorem arise frequently in practice, and the theorem gives an
easy method for computing their determinants. This dovetails with Example 2.4.11.

Theorem 3.1.5

Consider matrices A X 0 B and A 0 Y B in block form, where A and B are square matrices.

Then           det A X 0 B = det A det B and det A 0 Y B = det A det B

Proof. Write T = det A X 0 B and proceed by induction on k where A is k × k. If k = 1, it is the cofactor
expansion along column 1. In general let Si(T ) denote the matrix obtained from T by deleting row i and
                                                              . . The Cofactor Expansion

column 1. Then the cofactor expansion of det T along the first column is

                det T = a11 det (S1(T )) - a21 det (S2(T )) + · · · ± ak1 det (Sk(T ))                (3.2)

where a11, a21, · · · , ak1 are the entries in the first column of A. But Si(T ) =  Si(A) Xi          for each
                                                                                      0B

i = 1, 2, · · · , k, so det (Si(T )) = det (Si(A)) · det B by induction. Hence, Equation 3.2 becomes

det T = {a11 det (S1(T )) - a21 det (S2(T )) + · · · ± ak1 det (Sk(T ))} det B
       = { det A} det B

as required. The lower triangular case is similar.

Example 3.1.10

2 3 1 3         2 1 33

det  0 1 0 1   1 -2 -1 1  = - 1 -1 -2 1 0 0 1 1 = - 2 1 1 -1  1 1 4 1 = -(-3)(-3) = -9

0 4 01          0 0 41

    The next result shows that det A is a linear transformation when regarded as a function of a fixed
column of A. The proof is Exercise ??.

   Theorem 3.1.6
   Given columns c1, · · · , c j-1, c j+1, · · · , cn in Rn, define T : Rn  R by

                         T (x) = det c1 · · · c j-1 x c j+1 · · · cn for all x in Rn
   Then, for all x and y in Rn and all a in R,

                                 T (x + y) = T (x) + T (y) and T (ax) = aT (x)
Determinants and Diagonalization

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                          Engage Active Learning App!

                    Vretta-Lyryx Engage is an active learning app designed to increase
                   student engagement in reading linear algebra material. The content is
                 "chunked" into small blocks, each with an interactive assessment activity

                                            to promote comprehension.

                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

 . Determinants and Matrix Inverses

In this section, several theorems about determinants are derived. One consequence of these theorems is
that a square matrix A is invertible if and only if det A = 0. Moreover, determinants are used to give a
formula for A-1 which, in turn, yields a formula (called Cramer's rule) for the solution of any system of
linear equations with an invertible coefficient matrix.

    We begin with a remarkable theorem (due to Cauchy in 1812) about the determinant of a product of
matrices. The proof is given at the end of this section.

   Theorem 3.2.1: Product Theorem
   If A and B are n × n matrices, then det (AB) = det A det B.

    The complexity of matrix multiplication makes the product theorem quite unexpected. Here is an
example where it reveals an important numerical identity.

Example 3.2.1    and B =    cd    then AB =    ac - bd ad + bc .
If A = a b -b a           -d c               -(ad + bc) ac - bd
                                                                             . . Determinants and Matrix Inverses

Hence det A det B = det (AB) gives the identity
                                (a2 + b2)(c2 + d2) = (ac - bd)2 + (ad + bc)2

    Theorem 3.2.1 extends easily to det (ABC) = det A det B det C. In fact, induction gives
                             det (A1A2 · · · Ak-1Ak) = det A1 det A2 · · · det Ak-1 det Ak

for any square matrices A1, . . . , Ak of the same size. In particular, if each Ai = A, we obtain
                                           det (Ak) = (detA)k, for any k  1

We can now give the invertibility condition.

Theorem 3.2.2

An  n×n      matrix   A    is  invertible  if  and  only  if  det  A  =  0.  When  this  is  the  case,  det (A-1)  =    1
                                                                                                                       det A

Proof. If A is invertible, then AA-1 = I; so the product theorem gives

                                      1 = det I = det (AA-1) = det A det A-1

Hence,  det  A  =  0  and  also  det  A-1  =    1.

                                               det A

Conversely, if det A = 0, we show that A can be carried to I by elementary row operations (and invoke

Theorem 2.4.5). Certainly, A can be carried to its reduced row-echelon form R, so R = Ek · · · E2E1A where
the Ei are elementary matrices (Theorem 2.5.1). Hence the product theorem gives

                                           det R = det Ek · · · det E2 det E1 det A

Since det E = 0 for all elementary matrices E, this shows det R = 0. In particular, R has no row of zeros,
so R = I because R is square and reduced row-echelon. This is what we wanted.

Example 3.2.2

                                       1 0 -c 
For which values of c does A =  -1 3 1  have an inverse?

                                            0 2c -4

Solution. Compute det A by first adding c times column 1 to column 3 and then expanding along

row 1.                            1 0 -c                            10 0 

                det A = det  -1 3 1  = det  -1 3 1 - c  = 2(c + 2)(c - 3)

                                      0 2c -4                            0 2c -4

Hence, det A = 0 if c = -2 or c = 3, and A has an inverse if c = -2 and c = 3.
6 Determinants and Diagonalization

 Example 3.2.3
 If a product A1A2 · · · Ak of square matrices is invertible, show that each Ai is invertible.
 Solution. We have det A1 det A2 · · · det Ak = det (A1A2 · · · Ak) by the product theorem, and
 det (A1A2 · · · Ak) = 0 by Theorem 3.2.2 because A1A2 · · · Ak is invertible. Hence

                                             det A1 det A2 · · · det Ak = 0
 so det Ai = 0 for each i. This shows that each Ai is invertible, again by Theorem 3.2.2.

Theorem 3.2.3
If A is any square matrix, det AT = det A.

Proof. Consider first the case of an elementary matrix E. If E is of type I or II, then ET = E; so certainly
det ET = det E. If E is of type III, then ET is also of type III; so det ET = 1 = det E by Theorem 3.1.2.
Hence, det ET = det E for every elementary matrix E.

Now let A be any square matrix. If A is not invertible, then neither is AT ; so det AT = 0 = det A by

Theorem 3.2.2. On the other hand, if A is invertible, then A = Ek · · · E2E1, where the Ei are elementary
matrices  (Theorem  2.5.2).  Hence,                                so  the  product  theorem        gives
                                     AT  =    ET ET     · · · EkT

                                                12

                    det      AT  =  det  ET   det  ET   ···  det   E Tk  =  det  E1  det  E2 · · ·  det  Ek

                                           1         2

                                                                         = det Ek · · · det E2 det E1

                                                                         = det A

This completes the proof.

Example 3.2.4
If det A = 2 and det B = 5, calculate det (A3B-1AT B2).

Solution. We use several of the facts just derived.

                    det (A3B-1AT B2) = det (A3) det (B-1) det (AT ) det (B2)

                                              =    ( det     A)3     1   B  det  A( det  B)2
                                                                   det

                                              = 23 · 1 · 2 · 52

                                                        5

                                              = 80
                                                                          . . Determinants and Matrix Inverses

    Example 3.2.5
    A square matrix is called orthogonal if A-1 = AT . What are the possible values of det A if A is
    orthogonal?

    Solution. If A is orthogonal, we have I = AAT . Take determinants to obtain
                                 1 = det I = det (AAT ) = det A det AT = ( det A)2

    Since det A is a number, this means det A = ±1.

    Hence Theorems 2.6.4 and 2.6.5 imply that rotation about the origin and reflection about a line through
the origin in R2 have orthogonal matrices with determinants 1 and -1 respectively. In fact they are the
only such transformations of R2. We have more to say about this in Section 8.2.

Adjugates

In Section 2.4 we defined the adjugate of a 2 × 2 matrix A = a b c d to be adj (A) = d -b -c a . Then

we  verified  that  A( adj  A)  =  ( det  A)I  =  ( adj  A)A  and  hence  that,  if  det  A  =  0,  A-1  =    1    adj  A.  We  are
                                                                                                            det A
now able to define the adjugate of an arbitrary square matrix and to show that this formula for the inverse

remains valid (when the inverse exists).

    Recall that the (i, j)-cofactor ci j(A) of a square matrix A is a number defined for each position (i, j)

in the matrix. If A is a square matrix, the cofactor matrix of A is defined to be the matrix ci j(A) whose
(i, j)-entry is the (i, j)-cofactor of A.

    Definition 3.3 Adjugate of a Matrix
    The adjugate4of A, denoted adj (A), is the transpose of this cofactor matrix; in symbols,

                                                    adj (A) = ci j(A) T

This agrees with the earlier definition for a 2 × 2 matrix A as the reader can verify.

Example 3.2.6
                                     1 3 -2 

Compute the adjugate of A =  0 1 5  and calculate A( adj A) and ( adj A)A.
                                       -2 -6 7

Solution. We first find the cofactor matrix.

4This is also called the classical adjoint of A, but the term "adjoint" has another meaning.
8 Determinants and Diagonalization

                                     15             - 0 5 -2 7            01
                                     -6 7                               -2 -6 
                                    
                                                                                       
           c12(A)  c13(A)           
   c11(A)  c22(A)                                                                      
           c32(A)  c23(A)  =  - 3 -2  -6 7            1 -2
 c21(A)                                             -2 7                    1 3 
   c31(A)                                                            - -2 -6 
                   c33(A)           
                                                                                       

                                                    - 1 -2 0 5                         
                                     3 -2
                                                                                       
                                            15                            13 
                                                                          01

                                        37 -10 2 
                                    =  -9 3 0 

                                           17 -5 1

Then the adjugate of A is the transpose of this cofactor matrix.

                    37 -10 2 T  37 -9 17 

           adj A =  -9 3 0  =  -10 3 -5 

                                    17 -5 1         201

The computation of A( adj A) gives

            1 3 -2   37 -9 17   3 0 0 

A( adj A) =  0 1 5   -10 3 -5  =  0 3 0  = 3I

                   -2 -6 7                       201                 003

and the reader can verify that also ( adj A)A = 3I. Hence, analogy with the 2 × 2 case would
indicate that det A = 3; this is, in fact, the case.

    The relationship A( adj A) = ( det A)I holds for any square matrix A. To see why this is so, consider
the general 3 × 3 case. Writing ci j(A) = ci j for short, we have

                      c11 c12 c13 T  c11 c21 c31 
           adj A =  c21 c22 c23  =  c12 c22 c32 

                                    c31 c32 c33     c13 c23 c33

If A = ai j in the usual notation, we are to verify that A( adj A) = ( det A)I. That is,

            a11 a12 a13   c11 c21 c31   det A 0                                           0

A( adj A) =  a21 a22 a23   c12 c22 c32  =  0 det A 0 

           a31 a32 a33                c13 c23 c33                 0  0 det A

Consider the (1, 1)-entry in the product. It is given by a11c11 + a12c12 + a13c13, and this is just the cofactor
expansion of det A along the first row of A. Similarly, the (2, 2)-entry and the (3, 3)-entry are the cofactor
expansions of det A along rows 2 and 3, respectively.

    So it remains to be seen why the off-diagonal elements in the matrix product A( adj A) are all zero.
Consider the (1, 2)-entry of the product. It is given by a11c21 + a12c22 + a13c23. This looks like the
                                                                      . . Determinants and Matrix Inverses

cofactor expansion of the determinant of some matrix. To see which, observe that c21, c22, and c23 are
all computed by deleting row 2 of A (and one of the columns), so they remain the same if row 2 of A is

changed. In particular, if row 2 of A is replaced by row 1, we obtain

                                                                      a11 a12 a13   

                                  a11c21 + a12c22 + a13c23 = det  a11 a12 a13  = 0

                                                                      a31 a32 a33

where the expansion is along row 2 and where the determinant is zero because two rows are identical. A
similar argument shows that the other off-diagonal entries are zero.

This argument works in general and yields the first part of Theorem 3.2.4. The second assertion follows
                                                              1.
from  the  first  by  multiplying through by    the  scalar
                                                             det A

Theorem 3.2.4: Adjugate Formula
If A is any square matrix, then

                                        A( adj A) = ( det A)I = ( adj A)A
In particular, if det A = 0, the inverse of A is given by

                                                 A-1 = 1 adj A

                                                                             det A

It is important to note that this theorem is not an efficient way to find the inverse of the matrix A. For
example, if A were 10 × 10, the calculation of adj A would require computing 102 = 100 determinants of
9 × 9 matrices! On the other hand, the matrix inversion algorithm would find A-1 with about the same
effort as finding det A. Clearly, Theorem 3.2.4 is not a practical result: its virtue is that it gives a formula
for A-1 that is useful for theoretical purposes.

Example 3.2.7

                                           2 1 3
Find the (2, 3)-entry of A-1 if A =  5 -7 1 .

                                              3 0 -6

Solution. First compute

                                  21            3       2 17                        1 7 = 180
                      det A = 5 -7
                                                1 = 5 -7 11 = 3       -7 11

                                        3 0 -6          3 00

Since      A-1    =     1    adj  A  =   1   ci j(A) T , the (2, 3)-entry of A-1 is the (3, 2)-entry of the matrix
                      det A             180
                                                             23
180 ci j(A) ; that is, it equals 180 c32(A) = 180 -111                = 13 .
                                                             51
                                                                          180
    Determinants and Diagonalization

Example 3.2.8
If A is n × n, n  2, show that det ( adj A) = ( det A)n-1.

Solution. Write d = det A; we must show that det ( adj A) = dn-1. We have A( adj A) = dI by
Theorem 3.2.4, so taking determinants gives d det ( adj A) = dn. Hence we are done if d = 0.
Assume d = 0; we must show that det ( adj A) = 0, that is, adj A is not invertible. If A = 0, this
follows from A( adj A) = dI = 0; if A = 0, it follows because then adj A = 0.

Cramer's Rule

Theorem 3.2.4 has a nice application to linear equations. Suppose

                                                      Ax = b

is a system of n equations in n variables x1, x2, . . . , xn. Here A is the n × n coefficient matrix, and x and b
are the columns
                                                                     
                                            x1                            b1

                                            x2                            b2       
                                                                                   
                                  x =  ..  and b =  .. 
                                       .                             .

                                            xn                            bn

of variables and constants, respectively. If det A = 0, we left multiply by A-1 to obtain the solution
x = A-1b. When we use the adjugate formula, this becomes

                 
                    x1

                   x2          1
                       
                  ..  = det A ( adj A)b
                 .

                   xn

                                                                                             
                                       c11(A) c21(A) · · · cn1(A) b1

                               1       c12(A)            c22(A)      ···        cn2(A)           b2       
                                                                                                          
                         = det A             ..          ..                           ..      .. 
                                            .              .                            .              .

                                       c1n(A) c2n(A) · · · cnn(A) bn

Hence, the variables x1, x2, . . . , xn are given by

                   x1  =    1  A  [b1  c11  (A)       +  b2c21  (A)  +    ·  ·  ·  +    bn  cn1  (A)]
                          det

                   x2  =    1  A  [b1  c12  (A)       +  b2c22  (A)  +    ·  ·  ·  +    bn  cn2  (A)]
                          det
                                        ..                      ..
                                       .                          .

                   xn  =    1  A  [b1  c1n  (A)       +  b2c2n  (A)  +    ·  ·  ·  +    bn  cnn  (A)]
                          det

    Now the quantity b1c11(A) + b2c21(A) + · · · + bncn1(A) occurring in the formula for x1 looks like the
cofactor expansion of the determinant of a matrix. The cofactors involved are c11(A), c21(A), . . . , cn1(A),
corresponding to the first column of A. If A1 is obtained from A by replacing the first column of A by b,
                                                                         . . Determinants and Matrix Inverses

then ci1(A1) = ci1(A) for each i because column 1 is deleted when computing them. Hence, expanding
det (A1) by the first column gives

                                     det A1 = b1c11(A1) + b2c21(A1) + · · · + bncn1(A1)
                                             = b1c11(A) + b2c21(A) + · · · + bncn1(A)
                                             = ( det A)x1

Hence,  x1  =   det A1  and  similar  results  hold    for  the  other   variables.
                det A

Theorem 3.2.5: Cramer's Rule5
If A is an invertible n × n matrix, the solution to the system

                                                            Ax = b

of n equations in the variables x1, x2, . . . , xn is given by

                                               det A1            det A2              det An
                                         x1 = det A , x2 = det A , · · · , xn = det A

where, for each k, Ak is the matrix obtained from A by replacing column k by b.

Example 3.2.9
Find x1, given the following system of equations.

                                                5x1 + x2 - x3 = 4
                                                9x1 + x2 - x3 = 1

                                                  x1 - x2 + 5x3 = 2

Solution. Compute the determinants of the coefficient matrix A and the matrix A1 obtained from it
by replacing the first column by the column of constants.

                                                          5 1 -1 
                                          det A = det  9 1 -1  = -16

                                                            1 -1 5

                                                          4 1 -1 
                                         det A1 = det  1 1 -1  = 12

                                                            2 -1 5

Hence,      x1  =  det A1  = -3      by  Cramer's  rule.
                   det A
                                  4

    Cramer's rule is not an efficient way to solve linear systems or invert matrices. True, it enabled us to
calculate x1 here without computing x2 or x3. Although this might seem an advantage, the truth of the

    5Gabriel Cramer (1704-1752) was a Swiss mathematician who wrote an introductory work on algebraic curves. He popu-
larized the rule that bears his name, but the idea was known earlier.
   Determinants and Diagonalization

matter is that, for large systems of equations, the number of computations needed to find all the variables
by the gaussian algorithm is comparable to the number required to find one of the determinants involved in
Cramer's rule. Furthermore, the algorithm works when the matrix of the system is not invertible and even
when the coefficient matrix is not square. Like the adjugate formula, then, Cramer's rule is not a practical
numerical technique; its virtue is theoretical.

Polynomial Interpolation

Given a set of data, it is often the case that one is interested to understand a trend so to forecast other
values. One such method is that of modeling the trend with a polynomial, here is an example.

Example 3.2.10

A forester wants to estimate the age (in years) of a tree by measuring the diameter of the trunk (in
cm). She obtains the following data:

                                    Trunk Diameter  Tree 1        Tree 2  Tree 3
                                    Age                5            10      15
                                                       3             5       6

6  Age  (10, 5)  (15, 6)            Use this
                                    date to estimate the age of a tree with a trunk diameter of 12 cm.
4 (5, 3)
                                    Solution. The forester decides to "fit" a quadratic polynomial
2
                          Diameter                             p(x) = r0 + r1x + r2x2

   0 5 10 12 15                     to the data, that is choose the coefficients r0,
                                    r1, and r2 so that p(5) = 3, p(10) = 5, and p(15) = 6, and then use
                                    p(12) as the estimate. These conditions give three linear equations:

                                          r0 + 5r1 + 25r2 = 3
                                          r0 + 10r1 + 100r2 = 5
                                          r0 + 15r1 + 225r2 = 6

The (unique) solution is r0 = 0,    r1 =  7,  and   r2  =  -  1   , so
                                                              50
                                          10

                                    p(x)  =   7 x-  12     =  510 x(35 - x)

                                              10    50 x

Hence the estimate is p(12) = 5.52.

    As in Example 3.2.10, it often happens that two variables x and y are related but the actual functional

form y = f (x) of the relationship is unknown. Suppose that for certain values x1, x2, . . . , xn of x the
corresponding values y1, y2, . . . , yn are known (say from experimental measurements). One way to
                                                                                           . . Determinants and Matrix Inverses

estimate the value of y corresponding to some other value a of x is to find a polynomial6

                                  p(x) = r0 + r1x + r2x2 + · · · + rn-1xn-1

that "fits" the data, that is p(xi) = yi holds for each i = 1, 2, . . . , n. Then the estimate for y is p(a). As we
will see, such a polynomial always exists if the xi are distinct.

    The conditions that p(xi) = yi are

                                  r0  +        r1x1   +    r2x21  +     ·   ·  ·  +  rn-1   n-1   =  y1

                                                                                           x1
                                                           r2x22                            n-1
                                  r0  +        r1x2   +           +     ·   ·  ·  +  rn-1         =  y2
                                                                                           x2
                                           ..          ..           ..                     .. ..
                                          .           .            .                       ..

                                  r0  +        r1xn   +    r2x2n  +     ·   ·  ·  +  rn-1  xn-1   =  yn

                                                                                            n

In matrix form, this is   1 x1 x21 · · · xn1-1   r0   y1 

                              1   x2           x2     ···  xn-1                        r1             y2                         (3.3)
                                                            2                                               
                              ..      ..       ..2                 ..   ..  =  .. 

                                .. .                              .                    .                 .
                         

                              1   xn           x2     ···  xn-1                      rn-1             yn

                                                n           n

It can be shown (see Theorem 3.2.7) that the determinant of the coefficient matrix equals the product of

all terms (xi - x j) with i > j and so is nonzero (because the xi are distinct). Hence the equations have a

unique solution r0, r1, . . . , rn-1. This proves

Theorem 3.2.6
Let n data pairs (x1, y1), (x2, y2), . . . , (xn, yn) be given, and assume that the xi are distinct. Then
there exists a unique polynomial

                                    p(x) = r0 + r1x + r2x2 + · · · + rn-1xn-1
such that p(xi) = yi for each i = 1, 2, . . . , n.

The polynomial in Theorem 3.2.6 is called the interpolating polynomial for the data.

We conclude by evaluating the determinant of the coefficient matrix in Equation 3.3. If a1, a2, . . . , an

are numbers, the determinant                         2 n-1 

                                                       1 a1 a1 · · · a1
                                                       1                a2             an-1 
                                                           a2                     ···   2
                                                                          2            an-1 
                                                       1                a2              3
                                               det     ..  ..a3         ..        ···      ..
                                                                          3

                                                         .. .                                .    
                                                                                                  

                                                       1   an           a2        ···  an-1

                                                                          n              n

is called a Vandermonde determinant.7 There is a simple formula for this determinant. If n = 2, it equals

(a2 - a1); if n = 3, it is (a3 - a2)(a3 - a1)(a2 - a1) by Example 3.1.8. The general result is the product

                                                             (ai - a j)

                                                           1 j<in

    6A polynomial is an expression of the form a0 + a1x + a2x2 + · · · + anxn where the ai are numbers and x is a variable. If
an = 0, the integer n is called the degree of the polynomial, and an is called the leading coefficient. See Appendix D.

    7Alexandre Théophile Vandermonde (1735-1796) was a French mathematician who made contributions to the theory of
equations.
Determinants and Diagonalization

of all factors (ai - a j) where 1  j < i  n. For example, if n = 4, it is
                              (a4 - a3)(a4 - a2)(a4 - a1)(a3 - a2)(a3 - a1)(a2 - a1)

Theorem 3.2.7

Let a1, a2, . . . , an be numbers where n  2. Then the corresponding Vandermonde determinant is
given by
                                2 n-1 
                                  1 a1 a1 · · · a1
                                 1       a2            an-1 
                                     a2           ···   2
                                           2           an-1 
                                 1       a2             3 =
                          det    ..  a3....       ···       ..               (ai - a j)
                                           3

                                 .. .                      .          1 j<in
                                                                 

                                 1   an  a2       ···  an-1

                                           n             n

Proof. We may assume that the ai are distinct; otherwise both sides are zero. We proceed by induction on

n  2; we have it for n = 2, 3. So assume it holds for n - 1. The trick is to replace an by a variable x, and

consider the determinant                  2 n-1 
                                              1 a1 a1 · · · a1
                                         1                       a2 · · · an-1 
                                              ..       ..a2      ..2             2..
                                              .        .         .               .
                               p(x) = det                                             
                                                                                      

                                          1 an-1 a2 · · · an-1 
                                                                 n-1             n-1
                                              1 x x2 · · · xn-1

Then p(x) is a polynomial of degree at most n - 1 (expand along the last row), and p(ai) = 0 for each
i = 1, 2, . . . , n - 1 because in each case there are two identical rows in the determinant. In particular,

p(a1) = 0, so we have p(x) = (x - a1)p1(x) by the factor theorem (see Appendix D). Since a2 = a1, we
obtain p1(a2) = 0, and so p1(x) = (x - a2)p2(x). Thus p(x) = (x - a1)(x - a2)p2(x). As the ai are distinct,
this process continues to obtain

                                 p(x) = (x - a1)(x - a2) · · · (x - an-1)d                  (3.4)

where d is the coefficient of xn-1 in p(x). By the cofactor expansion of p(x) along the last row we get

                                               2 n-2 
                                                  1 a1 a1 · · · a1
                                     n+n  1 a2 a22 · · · an2-2 
                          d = (-1) det  .. .. ..                                      .. 
                                              . . .                                   .
                                                                                       n-2
                                                  1        an-1      a2     ···
                                                                                     an-1
                                                                       n-1

Because (-1)n+n = 1 the induction hypothesis shows that d is the product of all factors (ai - a j) where
1  j < i  n - 1. The result now follows from Equation 3.4 by substituting an for x in p(x).

Proof of Theorem 3.2.1. If A and B are n × n matrices we must show that

                                         det (AB) = det A det B                             (3.5)

Recall that if E is an elementary matrix obtained by doing one row operation to In, then doing that operation

to a matrix C (Lemma 2.5.1) results in EC. By looking at the three types of elementary matrices separately,

Theorem 3.1.2 shows that

                               det (EC) = det E det C for any matrix C                      (3.6)
                                          . . Determinants and Matrix Inverses

Thus if E1, E2, . . . , Ek are all elementary matrices, it follows by induction that

det (Ek · · · E2E1C) = det Ek · · · det E2 det E1 det C for any matrix C              (3.7)

Lemma. If A has no inverse, then det A = 0.

    Proof. Let A  R where R is reduced row-echelon, say En · · · E2E1A = R. Then R has a row of zeros by
Part (4) of Theorem 2.4.5, and hence det R = 0. But then Equation 3.7 gives det A = 0 because det E = 0
for any elementary matrix E. This proves the Lemma.

    Now we can prove Equation 3.5 by considering two cases.
Case 1. A has no inverse. Then AB also has no inverse (otherwise A[B(AB)-1] = I so A is invertible by
Corollary 2.4.2 to Theorem 2.4.5). Hence the above Lemma (twice) gives

det (AB) = 0 = 0 det B = det A det B

proving Equation 3.5 in this case.

Case 2. A has an inverse. Then A is a product of elementary matrices by Theorem 2.5.2, say A =
E1E2 · · · Ek. Then Equation 3.7 with C = I gives

det A = det (E1E2 · · · Ek) = det E1 det E2 · · · det Ek

But then Equation 3.7 with C = B gives

det (AB) = det [(E1E2 · · · Ek)B] = det E1 det E2 · · · det Ek det B = det A det B

and Equation 3.5 holds in this case too.

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!
6 Determinants and Diagonalization

 . Eigenvalues and Eigenvectors

The world is filled with examples of systems that evolve in time--the weather in a region, the economy
of a nation, the diversity of an ecosystem, etc. Describing such systems is difficult in general and various
methods have been developed in special cases. In this section we describe one such method, called diag-
onalization, which is one of the most important techniques in linear algebra. A very fertile example of
this procedure is in modelling the growth of the population of an animal species. This has attracted more
attention in recent years with the ever increasing awareness that many species are endangered. To motivate
the technique, we begin by setting up a simple model of a bird population in which we make assumptions
about survival and reproduction rates.

Example 3.3.1

Consider the evolution of the population of a species of birds. Because the number of males and
females are nearly equal, we count only females. We assume that each female remains a juvenile
for one year and then becomes an adult, and that only adults have offspring. We make three
assumptions about reproduction and survival rates:

1. The number of juvenile females hatched in any year is twice the number of adult females
   alive the year before (we say the reproduction rate is 2).

2.  Half of the adult females in any year survive to the next year (the adult survival rate is                    1 ).

                                                                                                                  2

3. One quarter of the juvenile females in any year survive into adulthood (the juvenile survival
              1 ).
    rate  is
              4

If there were 100 adult females and 40 juvenile females alive initially, compute the population of
females k years later.

Solution. Let ak and jk denote, respectively, the number of adult and juvenile females after k years,
so that the total female population is the sum ak + jk. Assumption 1 shows that jk+1 = 2ak, while
assumptions   2  and  3  show  that           1         1  jk.  Hence  the  numbers      and      in  successive  years
                                     ak+1  =  2  ak  +  4                            ak       jk

are related by the following equations:

                                                 ak+1   =  1 ak  +  1  jk
                                                                    4
                                                           2

                                                 jk+1 = 2ak

                      ak 1 1
If we write vk =          and A = 2 4 these equations take the matrix form
                      jk                   20

                                     vk+1 = Avk, for each k = 0, 1, 2, . . .

Taking k = 0 gives v1 = Av0, then taking k = 1 gives v2 = Av1 = A2v0, and taking k = 2 gives
v3 = Av2 = A3v0. Continuing in this way, we get

                                     vk = Akv0, for each k = 0, 1, 2, . . .

Since v0 = a0j0 = 100 40 is known, finding the population profile vk amounts to computing Ak
for all k  0. We will complete this calculation in Example 3.5.1 after some new techniques have
been developed.
                                                               . . Eigenvalues and Eigenvectors

    Let A be a fixed n × n matrix. A sequence v0, v1, v2, . . . of column vectors in Rn is called a linear
dynamical system8 if v0 is known and the other vk are determined (as in Example 3.3.1) by the conditions

                               vk+1 = Avk for each k = 0, 1, 2, . . .

These conditions are called a matrix recurrence for the vectors vk. As in Example 3.3.1, they imply that

                               vk = Akv0 for all k  0

so finding the columns vk amounts to calculating Ak for k  0.

Direct computation of the powers Ak of a square matrix A can be time-consuming, so we adopt an

indirect method that is commonly used. The idea is to first diagonalize the matrix A, that is, to find an

invertible matrix P such that  P-1AP = D is a diagonal matrix

                                                                                                 (3.8)

This works because the powers Dk of the diagonal matrix D are easy to compute, and Equation 3.8 enables
us to compute powers Ak of the matrix A in terms of powers Dk of D. Indeed, we can solve Equation 3.8
for A to get A = PDP-1. Squaring this gives

                               A2 = (PDP-1)(PDP-1) = PD2P-1

Using this we can compute A3 as follows:

                               A3 = AA2 = (PDP-1)(PD2P-1) = PD3P-1

Continuing in this way we obtain Theorem 3.3.1 (even if D is not diagonal).

Theorem 3.3.1
If A = PDP-1 then Ak = PDkP-1 for each k = 1, 2, . . . .

    Hence computing Ak comes down to finding an invertible matrix P as in equation Equation 3.8. To do
this it is necessary to first compute certain numbers (called eigenvalues) associated with the matrix A.

Eigenvalues and Eigenvectors

   Definition 3.4 Eigenvalues and Eigenvectors of a Matrix
   If A is an n × n matrix, a number  is called an eigenvalue of A if

                                           Ax =  x for some column x = 0.

   In this case, x is called an eigenvector of A corresponding to the eigenvalue  , or a  -eigenvector
   for short.

    8More precisely, this is a linear discrete dynamical system. Many models regard vt as a continuous function of the time t,
and replace our condition between bk+1 and Avk with a differential relationship viewed as functions of time.
  8 Determinants and Diagonalization

   Example 3.3.2
   If A = 3 5 1 -1 and x = 51 then Ax = 4x so  = 4 is an eigenvalue of A with corresponding
   eigenvector x.

    The matrix A in Example 3.3.2 has another eigenvalue in addition to  = 4. To find it, we develop a
general procedure for any n × n matrix A.

    By definition a number  is an eigenvalue of the n ×n matrix A if and only if Ax =  x for some column
x = 0. This is equivalent to asking that the homogeneous system

                                                      ( I - A)x = 0
of linear equations has a nontrivial solution x = 0. By Theorem 2.4.5 this happens if and only if the matrix
 I - A is not invertible and this, in turn, holds if and only if the determinant of the coefficient matrix is
zero:

                                                     det ( I - A) = 0
This last condition prompts the following definition:

   Definition 3.5 Characteristic Polynomial of a Matrix
   If A is an n × n matrix, the characteristic polynomial cA(x) of A is defined by

                                                   cA(x) = det (xI - A)

Note that cA(x) is indeed a polynomial in the variable x, and it has degree n when A is an n × n matrix (this
is illustrated in the examples below). The above discussion shows that a number  is an eigenvalue of A if
and only if cA( ) = 0, that is if and only if  is a root of the characteristic polynomial cA(x). We record
these observations in

   Theorem 3.3.2
   Let A be an n × n matrix.

       1. The eigenvalues  of A are the roots of the characteristic polynomial cA(x) of A.
       2. The  -eigenvectors x are the nonzero solutions to the homogeneous system

                                                          ( I - A)x = 0

           of linear equations with  I - A as coefficient matrix.

In practice, solving the equations in part 2 of Theorem 3.3.2 is a routine application of gaussian elimina-
tion, but finding the eigenvalues can be difficult, often requiring computers (see Section 8.5). For now,
the examples and exercises will be constructed so that the roots of the characteristic polynomials are rela-
tively easy to find (usually integers). However, the reader should not be misled by this into thinking that
eigenvalues are so easily obtained for the matrices that occur in practical applications!
                                                             . . Eigenvalues and Eigenvectors

Example 3.3.3                                          35    discussed in Example 3.3.2, and
                                                       1 -1
Find the characteristic polynomial of the matrix A =
then find all the eigenvalues and their eigenvectors.

Solution. Since xI - A = x 0 0 x - 3 5 1 -1 = x - 3 -5 -1 x + 1 we get

cA(x) = det x - 3 -5 -1 x + 1 = x2 - 2x - 8 = (x - 4)(x + 2)

Hence, the roots of cA(x) are 1 = 4 and 2 = -2, so these are the eigenvalues of A. Note that
1 = 4 was the eigenvalue mentioned in Example 3.3.2, but we have found a new one: 2 = -2.
To find the eigenvectors corresponding to 2 = -2, observe that in this case

                             (2I - A)x = 2 - 3 -5 -1 2 + 1 = -5 -5 -1 -1

so the general solution to (2I - A)x = 0 is x = t -11 where t is an arbitrary real number.

Hence, the eigenvectors x corresponding to  2 are x = t -11 where t = 0 is arbitrary. Similarly,

1 = 4 gives rise to the eigenvectors x = t 51 , t = 0 which includes the observation in
Example 3.3.2.

    Note that a square matrix A has many eigenvectors associated with any given eigenvalue  . In fact
every nonzero solution x of ( I - A)x = 0 is an eigenvector. Recall that these solutions are all linear com-
binations of certain basic solutions determined by the gaussian algorithm (see Theorem 1.3.2). Observe
that any nonzero multiple of an eigenvector is again an eigenvector,9 and such multiples are often more
convenient.10 Any set of nonzero multiples of the basic solutions of ( I - A)x = 0 will be called a set of
basic eigenvectors corresponding to  .

   Example 3.3.4

   Find the characteristic polynomial, eigenvalues, and basic eigenvectors for

                                                         2 0 0
                                                   A =  1 2 -1 

                                                            1 3 -2

 9In fact, any nonzero linear combination of  -eigenvectors is again a  -eigenvector.
10Allowing nonzero multiples helps eliminate round-off error when the eigenvectors involve fractions.
Determinants and Diagonalization

Solution. Here the characteristic polynomial is given by

                                   x-2       0    0
                   cA(x) = det  -1         x-2    1  = (x - 2)(x - 1)(x + 1)
                                            -3  x+2
                                      -1

so the eigenvalues are 1 = 2, 2 = 1, and 3 = -1. To find all eigenvectors for 1 = 2, compute

                                 1 - 2 0                  0   0 0 0

                   1I - A =  -1 1 - 2 1  =  -1 0 1 
                                      -1 -3 1 + 2                         -1 -3 4

We want the (nonzero) solutions to (1I - A)x = 0. The augmented matrix becomes

                          0 0 0 0   1 0 -1 0 

                          -1 0 1 0    0 1 -1 0 

                                -1 -3 4 0                 00 00

                                                                                     1

using row operations. Hence, the general solution x to (1I - A)x = 0 is x = t  1  where t is
                                                                                                 1

                                1

arbitrary, so we can use x1 =  1  as the basic eigenvector corresponding to 1 = 2. As the
                                       1

                                                                          0                           0
                                                                                                     1
reader can verify, the gaussian algorithm gives basic eigenvectors x2 =   1           and x3 =       3  

                                                                          1                          1

corresponding  to  2  =  1 and  3  =  -1,  respectively.  Note  that  to  eliminate  fractions,  we  could
                         
                      0

instead use 3x3 =  1  as the basic 3-eigenvector.
                         3

Example 3.3.5
If A is a square matrix, show that A and AT have the same characteristic polynomial, and hence the
same eigenvalues.
Solution. We use the fact that xI - AT = (xI - A)T . Then

                    cAT (x) = det xI - AT = det (xI - A)T = det (xI - A) = cA(x)
by Theorem 3.2.3. Hence cAT (x) and cA(x) have the same roots, and so AT and A have the same
eigenvalues (by Theorem 3.3.2).

The eigenvalues of a matrix need not be distinct. For example, if A = 1 1 0 1 the characteristic poly-
                                                . . Eigenvalues and Eigenvectors

nomial is (x - 1)2 so the eigenvalue 1 occurs twice. Furthermore, eigenvalues are usually not computed
as the roots of the characteristic polynomial. There are iterative, numerical methods (for example the
QR-algorithm in Section 8.5) that are much more efficient for large matrices.

A-Invariance

If A is a 2 × 2 matrix, we can describe the eigenvectors of A geometrically using the following concept. A
line L through the origin in R2 is called A-invariant if Ax is in L whenever x is in L. If we think of A as a
linear transformation R2  R2, this asks that A carries L into itself, that is the image Ax of each vector x
in L is again in L.

Example 3.3.6   x0 | x in R is A-invariant for any matrix of the form
The x axis L =

      A = a b 0 c because a b 0 c x0 = ax0 is L for all x = x0 in L

                   To see the connection with eigenvectors, let x = 0 be any nonzero vec-
                   tor in R2 and let Lx denote the unique line through the origin containing x
y

                   (see the diagram). By the definition of scalar multiplication in Section 2.6,

                   we see that Lx consists of all scalar multiples of x, that is

                Lx

                                                             Lx = Rx = {tx | t in R}

      x

                        Now suppose that x is an eigenvector of A, say Ax =  x for some  in R.

   0            x  Then if tx is in Lx then

                   A(tx) = t (Ax) = t( x) = (t )x is again in Lx

That is, Lx is A-invariant. On the other hand, if Lx is A-invariant then Ax is in Lx (since x is in Lx). Hence
Ax = tx for some t in R, so x is an eigenvector for A (with eigenvalue t). This proves:

Theorem 3.3.3
Let A be a 2 × 2 matrix, let x = 0 be a vector in R2, and let Lx be the line through the origin in R2
containing x. Then

                       x is an eigenvector of A if and only if Lx is A-invariant

Example 3.3.7                                   cos  - sin   has no real eigenvalue.
   1. If  is not a multiple of , show that A =  sin  cos 
    Determinants and Diagonalization

    2.  If m is real show that B =       1   1 - m2 2m    has a 1 as an eigenvalue.
                                       1+m2    2m m2 - 1

Solution.

   1. A induces rotation about the origin through the angle  (Theorem 2.6.4). Since  is not a
       multiple of , this shows that no line through the origin is A-invariant. Hence A has no
       eigenvector by Theorem 3.3.3, and so has no eigenvalue.

   2. B induces reflection Qm in the line through the origin with slope m by Theorem 2.6.5. If x is
       any nonzero point on this line then it is clear that Qmx = x, that is Qmx = 1x. Hence 1 is an
       eigenvalue (with eigenvector x).

If      =     in  Example  3.3.7,  then  A=  0 -1  so cA(x) = x2 + 1. This polynomial has no root
           2                                 10

in R, so A has no (real) eigenvalue, and hence no eigenvector. In fact its eigenvalues are the complex

numbers i and -i, with corresponding eigenvectors 1 and 1 In other words, A has eigenvalues
                                                   -i     i

and eigenvectors, just not real ones.

Note that every polynomial has complex roots,11 so every matrix has complex eigenvalues. While

these eigenvalues may very well be real, this suggests that we really should be doing linear algebra over the

complex numbers. Indeed, everything we have done (gaussian elimination, matrix algebra, determinants,

etc.) works if all the scalars are complex.

11This is called the Fundamental Theorem of Algebra and was first proved by Gauss in his doctoral dissertation.
                                                             . . Diagonalization

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                       Engage Active Learning App!

                 Vretta-Lyryx Engage is an active learning app designed to increase
                student engagement in reading linear algebra material. The content is
              "chunked" into small blocks, each with an interactive assessment activity

                                         to promote comprehension.

               Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

. Diagonalization

An n × n matrix D is called a diagonal matrix if all its entries off the main diagonal are zero, that is if D

has the form         1 0 · · · 0  

                   0  2  ···  0
                                 
              D =  .. .. . . ..  = diag (1, 2, · · · , n)
                   . . . .

                     0 0 · · · n

where 1, 2, . . . , n are numbers. Calculations with diagonal matrices are very easy. Indeed, if
D = diag (1, 2, . . . , n) and E = diag (µ1, µ2, . . . , µn) are two diagonal matrices, their product DE and
sum D + E are again diagonal, and are obtained by doing the same operations to corresponding diagonal

elements:

                 DE = diag (1µ1, 2µ2, . . . , nµn)
              D + E = diag (1 + µ1, 2 + µ2, . . . , n + µn)

Because of the simplicity of these formulas, and with an eye on Theorem 3.3.1 and the discussion preced-
ing it, we make another definition:
Determinants and Diagonalization

Definition 3.6 Diagonalizable Matrices
An n × n matrix A is called diagonalizable if

                           P-1AP is diagonal for some invertible n × n matrix P
Here the invertible matrix P is called a diagonalizing matrix for A.

    To discover when such a matrix P exists, we let x1, x2, . . . , xn denote the columns of P and look
for ways to determine when such xi exist and how to compute them. To this end, write P in terms of its
columns as follows:

                                                   P = [x1, x2, · · · , xn]

Observe that P-1AP = D for some diagonal matrix D holds if and only if

                                  AP = PD

If we write D = diag (1, 2, . . . , n), where the i are numbers to be determined, the equation AP = PD

becomes                                      1 0 · · · 0                         

                                           0  2 · · ·                        0
                                                                                
         A [x1, x2, · · · , xn] = [x1, x2, · · · , xn]  .. .. . . .. 
                                           . . . .

                                             0 0 · · · n

By the definition of matrix multiplication, each side simplifies as follows

                                Ax1 Ax2 · · · Axn = 1x1 2x2 · · · nxn

Comparing columns shows that Axi = ixi for each i, so
                                 P-1AP = D if and only if Axi = ixi for each i

In other words, P-1AP = D holds if and only if the diagonal entries of D are eigenvalues of A and the
columns of P are corresponding eigenvectors. This proves the following fundamental result.

Theorem 3.4.1
Let A be an n × n matrix.

   1. A is diagonalizable if and only if it has eigenvectors x1, x2, . . . , xn such that the matrix
       P = x1 x2 . . . xn is invertible.

   2. When this is the case, P-1AP = diag (1, 2, . . . , n) where, for each i, i is the eigenvalue
       of A corresponding to xi.
                                                                                       . . Diagonalization

Example 3.4.1

                                  2 0 0
Diagonalize the matrix A =  1 2 -1  in Example 3.3.4.

                                     1 3 -2

Solution.  By  Example  3.3.4,  the   eigenvalues of  A  are1  = 2,  2    =  1,  and  3 =  -1,  with
                                                                                 
                                        1                     0                     0

corresponding basic eigenvectors x1 =  1  , x2 =  1 , and x3 =  1  respectively. Since
                                        1                     1                     3

                                      1 0 0

the matrix P = x1 x2 x3 =  1 1 1  is invertible, Theorem 3.4.1 guarantees that
                                            113

                                       1 0 0   2 0 0 
                        P-1AP =  0 2 0  =  0 1 0  = D
                                      0 0 3                      0 0 -1

The reader can verify this directly--easier to check AP = PD.

    In Example 3.4.1, suppose we let Q = x2 x1 x3 be the matrix formed from the eigenvectors x1,
x2, and x3 of A, but in a different order than that used to form P. Then Q-1AQ = diag (2, 1, 3) is diag-
onal by Theorem 3.4.1, but the eigenvalues are in the new order. Hence we can choose the diagonalizing
matrix P so that the eigenvalues i appear in any order we want along the main diagonal of D.

    In every example above each eigenvalue has had only one basic eigenvector. Here is a diagonalizable
matrix where this is not the case.

   Example 3.4.2

                                      0 1 1
   Diagonalize the matrix A =  1 0 1 

                                         110

   Solution. To compute the characteristic polynomial of A first add rows 2 and 3 of xI - A to row 1:

                                   x -1 -1                      x-2 x-2 x-2 

                     cA(x) = det  -1 x -1  = det  -1 x -1 
                                      -1 -1 x                        -1 -1 x

                                x-2       0      0
                        = det  -1       x+1      0  = (x - 2)(x + 1)2
                                               x+1
                                    -1    0

Hence the eigenvalues are 1 = 2 and 2 = -1, with 2 repeated twice (we say that 2 has

multiplicity  two).  However,  A  is  diagonalizable. For  1  =  2,  the  system  of   equations
                                                 
                                           1

(1I - A)x = 0 has general solution x = t  1  as the reader can verify, so a basic 1-eigenvector
                                                     1
6 Determinants and Diagonalization

1

is x1 =  1 .
            1

Turning to the repeated eigenvalue 2 = -1, we must solve (2I - A)x = 0. By gaussian
                                                   
                                    -1      -1

elimination, the general solution is x = s  1  + t  0  where s and t are arbitrary.  Hence
                                                                                     
                                    0       1                                         If we

                                                                    -1    -1

the gaussian algorithm produces two basic 2-eigenvectors x2 =  1  and y2 =  0
                                                                   0     1

                1 -1 -1 

take P = x1 x2 y2 =  1 1 0  we find that P is invertible. Hence
                                    101

P-1AP = diag (2, -1, -1) by Theorem 3.4.1.

    Example 3.4.2 typifies every diagonalizable matrix. To describe the general case, we need some ter-
minology.

   Definition 3.7 Multiplicity of an Eigenvalue
   An eigenvalue  of a square matrix A is said to have multiplicity m if it occurs m times as a root of
   the characteristic polynomial cA(x).

For example, the eigenvalue 2 = -1 in Example 3.4.2 has multiplicity 2. In that example the gaussian
algorithm yields two basic 2-eigenvectors, the same number as the multiplicity. This works in general.

   Theorem 3.4.2
   A square matrix A is diagonalizable if and only if every eigenvalue  of multiplicity m yields
   exactly m basic eigenvectors; that is, if and only if the general solution of the system ( I - A)x = 0
   has exactly m parameters.

One case of Theorem 3.4.2 deserves mention.

   Theorem 3.4.3
   An n × n matrix with n distinct eigenvalues is diagonalizable.

The proofs of Theorem 3.4.2 and Theorem 3.4.3 require more advanced techniques and are given in Chap-
ter 5. The following procedure summarizes the method.

   Theorem: Diagonalization Algorithm
   To diagonalize an n × n matrix A:

      Step 1. Find the distinct eigenvalues  of A.
                                                                                         . . Diagonalization

Step 2. Compute a set of basic eigenvectors corresponding to each of these eigenvalues  as
basic solutions of the homogeneous system ( I - A)x = 0.

Step 3. The matrix A is diagonalizable if and only if there are n basic eigenvectors in all.

Step 4. If A is diagonalizable, the n × n matrix P with these basic eigenvectors as its columns is
a diagonalizing matrix for A, that is, P is invertible and P-1AP is diagonal.

The diagonalization algorithm is valid even if the eigenvalues are nonreal complex numbers. In this case
the eigenvectors will also have complex entries, but we will not pursue this here.

Example 3.4.3             is not diagonalizable.
Show that A = 1 1 0 1

Solution 1. The characteristic polynomial is cA(x) = (x - 1)2, so A has only one eigenvalue 1 = 1
of multiplicity 2. But the system of equations (1I - A)x = 0 has general solution t 10 , so there
is only one parameter, and so only one basic eigenvector 12 . Hence A is not diagonalizable.

Solution 2. We have cA(x) = (x - 1)2 so the only eigenvalue of A is  = 1. Hence, if A were
diagonalizable, Theorem 3.4.1 would give P-1AP = 1 0 0 1 = I for some invertible matrix P.
But then A = PIP-1 = I, which is not the case. So A cannot be diagonalizable.

    Diagonalizable matrices share many properties of their eigenvalues. The following example illustrates
why.

Example 3.4.4
If  3 = 5 for every eigenvalue of the diagonalizable matrix A, show that A3 = 5A.

Solution.  Let  P-1AP  =  D  =  diag (1,  ...,  n).  Because  3    =  5i  for  each  i,  we  obtain

                                                                i

                       D3 = diag (13, . . . , n3) = diag (51, . . . , 5n) = 5D

Hence A3 = (PDP-1)3 = PD3P-1 = P(5D)P-1 = 5(PDP-1) = 5A using Theorem 3.3.1. This is
what we wanted.

    If p(x) is any polynomial and p( ) = 0 for every eigenvalue of the diagonalizable matrix A, an ar-
gument similar to that in Example 3.4.4 shows that p(A) = 0. Thus Example 3.4.4 deals with the case
p(x) = x3 - 5x. In general, p(A) is called the evaluation of the polynomial p(x) at the matrix A. For
example, if p(x) = 2x3 - 3x + 5, then p(A) = 2A3 - 3A + 5I--note the use of the identity matrix.
  8 Determinants and Diagonalization

    In particular, if cA(x) denotes the characteristic polynomial of A, we certainly have cA( ) = 0 for each
eigenvalue  of A (Theorem 3.3.2). Hence cA(A) = 0 for every diagonalizable matrix A. This is, in fact,
true for any square matrix, diagonalizable or not, and the general result is called the Cayley-Hamilton
theorem. It is proved in Section 8.7 and again in Section 11.1.

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

         Engage Active Learning App!

   Vretta-Lyryx Engage is an active learning app designed to increase
  student engagement in reading linear algebra material. The content is
"chunked" into small blocks, each with an interactive assessment activity

                           to promote comprehension.

 Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

. Linear Dynamical Systems

We began Section 3.3 with an example from ecology which models the evolution of the population of a
species of birds as time goes on. As promised, we now complete the example--Example 3.5.1 below.

    The bird population was described by computing the female population profile vk = akj of the k
species, where ak and jk represent the number of adult and juvenile females present k years after the initial
values a0 and j0 were observed. The model assumes that these numbers are related by the following
equations:

ak+1                        =  1 ak  +  1  jk
                                        4
                               2

jk+1 = 2ak

                              11

If we write A = 2 4 the columns vk satisfy vk+1 = Avk for each k = 0, 1, 2, . . . .
                      20

Hence vk = Akv0 for each k = 1, 2, . . . . We can now use our diagonalization techniques to determine the
population profile vk for all values of k in terms of the initial values.
                                                                                            . . Linear Dynamical Systems

Example 3.5.1

Assuming that the initial values were a0 = 100 adult females and j0 = 40 juvenile females,
compute ak and jk for k = 1, 2, . . . .

                                                                                                   11

Solution. The characteristic polynomial of the matrix A = 2 4 is
                                                                          20

cA(x)  =  x2  -  1x-  1  =   (x - 1)(x +  1 ),  so   the  eigenvalues         are    1   =  1  and     2  =  -1    and  gaussian
                      2
                 2                        2                             1                -1                     2

elimination gives corresponding basic eigenvectors 2 and 4 . For convenience, we can
                                                                       1                    1

use multiples x1 = 12 and x2 = -14 respectively. Hence a diagonalizing matrix is

P = 1 -1 2 4 and we obtain

                                      P-1AP = D where D =                     10
                                                                              0 -1

                                                                                      2

This gives A = PDP-1 so, for each k  0, we can compute Ak explicitly:

                         Ak = PDkP-1 =              1 -1           1 0 1 41
                                                    24             0 (- 1 )k 6 -2 1

                                                                             2

                                             =6 1       4 + 2(- 21 )k         1 - (-1 )k

                                                                                         2
                                                                   1k                    1k
                                                        8 - 8(-2) 2 + 4(-2)

Hence we obtain

                         ak               k          1    4 + 2(- 12 )k          1 - (-1 )k               100
                         j = vk = A v0 = 6
                                                                                            2             40
                                                                      1k                    1k
                         k                                8 - 8(-2) 2 + 4(-2)

                                                =6   1 440 + 160(- 12)k
                                                                              1k
                                                          880 - 640(-2)

Equating top and bottom entries, we obtain exact formulas for ak and jk:

                 ak = 220 + 80 - 1 k and jk = 440 + 320 - 1 k for k = 1, 2, · · ·3   2
                             3 32                                          3

In practice, the exact values of ak and jk are not usually required. What is needed is a measure of
how  these    numbers    behave  for  large  values     of  k.  This   is  easy      to  obtain  here.    Since    (- 1 )k  is  nearly

zero for large k, we have the following approximate values                                                              2

                                      ak        220  and    jk     440  if    k  is  large
                                                 3                  3

Hence, in the long term, the female population stabilizes with approximately twice as many
juveniles as adults.
6 Determinants and Diagonalization

Definition 3.8 Linear Dynamical System
If A is an n × n matrix, a sequence v0, v1, v2, . . . of columns in Rn is called a linear dynamical
system if v0 is specified and v1, v2, . . . are given by the matrix recurrence vk+1 = Avk for each
k  0. We call A the migration matrix of the system.

We have v1 = Av0, then v2 = Av1 = A2v0, and continuing we find

                                  vk = Akv0 for each k = 1, 2, · · ·                                 (3.9)

Hence the columns vk are determined by the powers Ak of the matrix A and, as we have seen, these powers
can be efficiently computed if A is diagonalizable. In fact Equation 3.9 can be used to give a nice "formula"
for the columns vk in this case.

Assume that A is diagonalizable with eigenvalues 1, 2, . . . , n and corresponding basic eigenvectors

x1, x2, . . . , xn. If P =  x1 x2 . . . xn is a diagonalizing matrix with the xi as columns, then P is
invertible and                       P-1AP = D = diag (1, 2, · · · , n)

by Theorem 3.4.1. Hence A = PDP-1 so Equation 3.9 and Theorem 3.3.1 give

                            vk = Akv0 = (PDP-1)kv0 = (PDkP-1)v0 = PDk(P-1v0)

for each k = 1, 2, . . . . For convenience, we denote the column P-1v0 arising here as follows:

                                                      
                                                         b1

                                             -1         b2   
                                                             
                                    b = P v0 =  .. 
                                                      .

                                                        bn

Then matrix multiplication gives

                            vk = PDk(P-1v0)

                                                 k      0       ···  0     
                                                                            b1
                                                   1
                                                 0      k            0
                                                                ···         b2   
                                                          2                      
                            = x1 x2 · · · xn  .. .. . .              ...    ...
                                             . . .                               
                                                                                 

                                                 0 0 ···             k      bn

                                                                       n

                                                 b11k   

                                                 b22k   
                                                        
                            = x1 x2 · · · xn  .. 
                                             .

                                                 b3nk

                            = b11kx1 + b22kx2 + · · · + bnnkxn                                   (3.10)

for each k  0. This is a useful exact formula for the columns vk. Note that, in particular,

                                  v0 = b1x1 + b2x2 + · · · + bnxn
                                                                            . . Linear Dynamical Systems 6

    However, such an exact formula for vk is often not required in practice; all that is needed is to estimate
vk for large values of k (as was done in Example 3.5.1). This can be easily done if A has a largest
eigenvalue. An eigenvalue  of a matrix A is called a dominant eigenvalue of A if it has multiplicity 1
and

                                         | | > |µ| for all eigenvalues µ = 

where | | denotes the absolute value of the number  . For example, 1 = 1 is dominant in Example 3.5.1.

    Returning to the above discussion, suppose that A has a dominant eigenvalue. By choosing the order
in which the columns xi are placed in P, we may assume that 1 is dominant among the eigenvalues
1, 2, . . . , n of A (see the discussion following Example 3.4.1). Now recall the exact expression for vk
in Equation 3.10 above:

                                        vk = b11kx1 + b22kx2 + · · · + bnnkxn

Take  k    out  as  a  common  factor  in  this  equation  to  get

        1

                                           k 2 k                            n k
                               vk = 1 b1x1 + b2 1 x2 + · · · + bn 1 xn

for each k  0. Since 1 is dominant, we have |i| < |1| for each i  2, so each of the numbers (i/1)k
become small in absolute value as k increases. Hence vk is approximately equal to the first term 1kb1x1,
and we write this as vk  1kb1x1. These observations are summarized in the following theorem (together
with the above exact formula for vk).

Theorem 3.5.1
Consider the dynamical system v0, v1, v2, . . . with matrix recurrence

                                                 vk+1 = Avk for k  0

where A and v0 are given. Assume that A is a diagonalizable n × n matrix with eigenvalues
1, 2, . . . , n and corresponding basic eigenvectors x1, x2, . . . , xn, and let
P = x1 x2 . . . xn be the diagonalizing matrix. Then an exact formula for vk is

                           vk = b11kx1 + b22kx2 + · · · + bnnkxn for each k  0
where the coefficients bi come from

                                                                    
                                                                       b1

                                                 -1                   b2  
                                                                          
                                                 b = P v0 =  .. 
                                                                    .

                                                                      bn

Moreover, if A has dominant12eigenvalue 1, then vk is approximated by

                                           vk = b11kx1 for sufficiently large k.

   12Similar results can be found in other situations. If for example, eigenvalues 1 and 2 (possibly equal) satisfy |1| = |2| >
|i| for all i > 2, then we obtain vk  b11kx1 + b22kx2 for large k.
6 Determinants and Diagonalization

Example 3.5.2

Returning to Example 3.5.1, we see that 1 = 1 is the dominant eigenvalue, with eigenvector
       1                     1 -1                       100                      220
                                                                   -1       1              220
x1 =   2      . Here P =     24            and v0 =     40      so P v0 = 3      -80    . Hence b1 = 3 in

the notation of Theorem 3.5.1, so

                                        ak                  k      220 k 1
                                             = vk  b11 x1 = 3 1          2
                                        jk

where  k  is  large.  Hence  ak    220  and  jk    440  as  in  Example  3.5.1.
                                    3               3

This next example uses Theorem 3.5.1 to solve a "linear recurrence." See also Section 3.4.

Example 3.5.3
Suppose a sequence x0, x1, x2, . . . is determined by insisting that

                         x0 = 1, x1 = -1, and xk+2 = 2xk - xk+1 for every k  0
Find a formula for xk in terms of k.

Solution. Using the linear recurrence xk+2 = 2xk - xk+1 repeatedly gives
                x2 = 2x0 - x1 = 3, x3 = 2x1 - x2 = -5, x4 = 11, x5 = -21, . . .

so the xi are determined but no pattern is apparent. The idea is to find vk = xk x for each k k+1
instead, and then retrieve xk as the top component of vk. The reason this works is that the linear
recurrence guarantees that these vk are a dynamical system:

                    vk+1 = xk+1 x = xk+1 = Av k+2 2xk - xk+1 k where A = 0 1 2 -1

The eigenvalues of A are 1 = -2 and 2 = 1 with eigenvectors x1 = 1 -2 and x2 = 11 , so
the diagonalizing matrix is P = 1 1 -2 1 .
Moreover, b = P0-1v0 = 13 21 so the exact formula for vk is

                      xk     = vk = b11 x1 + b22 x2 = 3 (-2)kk  2      k 1 1k 1
                                                                         -2      + 31   1
                      xk+1

Equating top entries gives the desired formula for xk:

                                 xk  =  1   2(-2)k + 1      for all k = 0, 1, 2, . . .
                                        3

The reader should check this for the first few values of k.
                                                 . . Linear Dynamical Systems 6

Graphical Description of Dynamical Systems

If a dynamical system vk+1 = Avk is given, the sequence v0, v1, v2, . . . is called the trajectory of the
system starting at v0. It is instructive to obtain a graphical plot of the system by writing vk = xk y and k
plotting the successive values as points in the plane, identifying vk with the point (xk, yk) in the plane. We
give several examples which illustrate properties of dynamical systems. For ease of calculation we assume
that the matrix A is simple, usually diagonal.

Example 3.5.4        Let A =  101 Then the eigenvalues are 2 and 3, with11
              y
                              2
               O              03
                     corresponding eigenvectors x1 = 10 and x2 = 01 .

                     The exact formula is

                  x               vk = b1 21k 1  + b2 31k 0
                                             0              1

                     for k = 0, 1, 2, . . . by Theorem 3.5.1, where the coefficients
                     b1 and b2 depend on the initial point v0. Several trajectories are
                     plotted in the diagram and, for each choice of v0, the trajectories
                     converge toward the origin because both eigenvalues are less

                     than 1 in absolute value. For this reason, the origin is called

                     an attractor for the system.

Example 3.5.5        Let A =  304 . Here the eigenvalues are 2 and 3 , with34
              y
                              2
              O               03
                     corresponding eigenvectors x1 = 10 and x2 = 01 as before.

                     The exact formula is

                  x               vk = b1 23k 1  + b2 34k 0
                                             0              1

                     for k = 0, 1, 2, . . . . Since both eigenvalues are greater than
                     1 in absolute value, the trajectories diverge away from the origin

                     for every choice of initial point V0. For this reason, the origin
                     is called a repellor for the system.
6 Determinants and Diagonalization

Example 3.5.6                             Let A = 1          1 -1       . Now the eigenvalues are 2 and 2, with3         1
              y
                                                                     2
              O                                          -2 1
                                          corresponding eigenvectors x1 = -11 and x2 = 11 The

                                          exact formula is

                           x                                 vk = b1 2      3 k -1           + b2 2       1k 1
                                                                                          1               1

                                          for  k  =  0,  1,  2,  ....   In  this    case  3  is  the  dominant  eigenvalue
                                                                                          2
                                                                                         3 k -1
                                          so, if b1 = 0, we have vk  b1 2                        1        for large k and vk

                                          is approaching the line y = -x.

                                          However, if b1 = 0, then vk = b2 2                 1k 1         and so approaches
                                                                                                      1

                                          the origin along the line y = x. In general the trajectories appear

                                          as in the diagram, and the origin is called a saddle point for the

dynamical system in this case.

Example 3.5.7

               01                                                                                21

                    2
Let A = 1 . Now the characteristic polynomial is cA(x) = x + 4, so the eigenvalues are
            -2 0
the  complex   numbers  i  and      -i    where      i2     -1.  Hence          is  not  diagonalizable   as    a  real  matrix.
                        2                                =                  A
                                       2
However, the trajectories are not difficult to describe. If we start with v0 = 11 then the

trajectory begins as

            1              -1                        -1                     1                         1            -1
v1 = 2 , v2 = 4 , v3 = 8 , v4 = 16 , v5 = 32 , v6 = 64 , . . .
     -1                    -1                            1                  1                    -1                -1
            2                    4                       8                  16                        32                    64

     y                                    The first five of these points are plotted in the diagram. Here
                                          each trajectory spirals in toward the origin, so the origin is an
        1             v0                  attractor. Note that the two (complex) eigenvalues have absolute
                                          value less than 1 here. If they had absolute value greater than
      v3                      x           1, the trajectories would spiral out from the origin.
         O          1
                 v1
     v2
                                                                                   . . Linear Dynamical Systems 6

Google PageRank

Dominant eigenvalues are useful to the Google search engine for finding information on the Web. If an
information query comes in from a client, Google has a sophisticated method of establishing the "rele-
vance" of each site to that query. When the relevant sites have been determined, they are placed in order of
importance using a ranking of all sites called the PageRank. The relevant sites with the highest PageRank
are the ones presented to the client. It is the construction of the PageRank that is our interest here.

    The Web contains many links from one site to another. Google interprets a link from site j to site
i as a "vote" for the importance of site i. Hence if site i has more links to it than does site j, then i is
regarded as more "important" and assigned a higher PageRank. One way to look at this is to view the sites
as vertices in a huge directed graph (see Section 2.2). Then if site j links to site i there is an edge from j
to i, and hence the (i, j)-entry is a 1 in the associated adjacency matrix (called the connectivity matrix in
this context). Thus a large number of 1s in row i of this matrix is a measure of the PageRank of site i.13

    However this does not take into account the PageRank of the sites that link to i. Intuitively, the higher
the rank of these sites, the higher the rank of site i. One approach is to compute a dominant eigenvector x
for the connectivity matrix. In most cases the entries of x can be chosen to be positive with sum 1. Each
site corresponds to an entry of x, so the sum of the entries of sites linking to a given site i is a measure of
the rank of site i. In fact, Google chooses the PageRank of a site so that it is proportional to this sum.14

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

   13For more on PageRank, visit https://en.wikipedia.org/wiki/PageRank.
   14See the articles "Searching the web with eigenvectors" by Herbert S. Wilf, UMAP Journal 23(2), 2002, pages 101-103,
and "The worlds largest matrix computation: Google's PageRank is an eigenvector of a matrix of order 2.7 billion" by Cleve
Moler, Matlab News and Notes, October 2002, pages 12-13.
66 Determinants and Diagonalization

 .6 An Application to Linear Recurrences

It often happens that a problem can be solved by finding a sequence of numbers x0, x1, x2, . . . where the
first few are known, and subsequent numbers are given in terms of earlier ones. Here is a combinatorial
example where the object is to count the number of ways to do something.

Example 3.6.1
An urban planner wants to determine the number xk of ways that a row of k parking spaces can be
filled with cars and trucks if trucks take up two spaces each. Find the first few values of xk.

Solution. Clearly, x0 = 1 and x1 = 1, while x2 = 2 since there can be two cars or one truck. We
have x3 = 3 (the 3 configurations are ccc, cT, and Tc) and x4 = 5 (cccc, ccT, cTc, Tcc, and TT). The

key to this method is to find a way to express each subsequent xk in terms of earlier values. In this
case we claim that

xk+2 = xk + xk+1 for every k  0                        (3.11)

Indeed, every way to fill k + 2 spaces falls into one of two categories: Either a car is parked in the

first space (and the remaining k + 1 spaces are filled in xk+1 ways), or a truck is parked in the first
two spaces (with the other k spaces filled in xk ways). Hence, there are xk+1 + xk ways to fill the
k + 2 spaces. This is Equation 3.11.

The recurrence in Equation 3.11 determines xk for every k  2 since x0 and x1 are given. In fact,
the first few values are

                                              x0 = 1

                                     x1 = 1
                                     x2 = x0 + x1 = 2

                                     x3 = x1 + x2 = 3

                                     x4 = x2 + x3 = 5
                                     x5 = x3 + x4 = 8
                                      ..   ..   ..
                                     .    .    .

Clearly, we can find xk for any value of k, but one wishes for a "formula" for xk as a function of k.
It turns out that such a formula can be found using diagonalization. We will return to this example
later.

    A sequence x0, x1, x2, . . . of numbers is said to be given recursively if each number in the sequence is
completely determined by those that come before it. Such sequences arise frequently in mathematics and
computer science, and also occur in other parts of science. The formula xk+2 = xk+1 + xk in Example 3.6.1
is an example of a linear recurrence relation of length 2 because xk+2 is the sum of the two preceding
terms xk+1 and xk; in general, the length is m if xk+m is a sum of multiples of xk, xk+1, . . . , xk+m-1.

    The simplest linear recursive sequences are of length 1, that is xk+1 is a fixed multiple of xk for each k,
say xk+1 = axk. If x0 is specified, then x1 = ax0, x2 = ax1 = a2x0, and x3 = ax2 = a3x0, . . . . Continuing,
we obtain xk = akx0 for each k  0, which is an explicit formula for xk as a function of k (when x0 is given).

    Such formulas are not always so easy to find for all choices of the initial values. Here is an example
where diagonalization helps.
                                                   .6. An Application to Linear Recurrences 6

Example 3.6.2
Suppose the numbers x0, x1, x2, . . . are given by the linear recurrence relation

                                xk+2 = xk+1 + 6xk for k  0

where x0 and x1 are specified. Find a formula for xk when x0 = 1 and x1 = 3, and also when x0 = 1
and x1 = 1.

Solution. If x0 = 1 and x1 = 3, then

x2 = x1 + 6x0 = 9, x3 = x2 + 6x1 = 27, x4 = x3 + 6x2 = 81

and it is apparent that

                                xk = 3k for k = 0, 1, 2, 3, and 4

This formula holds for all k because it is true for k = 0 and k = 1, and it satisfies the recurrence
xk+2 = xk+1 + 6xk for each k as is readily checked.
However, if we begin instead with x0 = 1 and x1 = 1, the sequence continues

                         x2 = 7, x3 = 13, x4 = 55, x5 = 133, . . .

In this case, the sequence is uniquely determined but no formula is apparent. Nonetheless, a simple
device transforms the recurrence into a matrix recurrence to which our diagonalization techniques
apply.
The idea is to compute the sequence v0, v1, v2, . . . of columns instead of the numbers
x0, x1, x2, . . . , where

                                          vk = xk x for each k  0 k+1

Then v0 = x0 x1 = 11 is specified, and the numerical recurrence xk+2 = xk+1 + 6xk transforms
into a matrix recurrence as follows:

                     vk+1 = xk+1 x = xk+1 = 0 1 k+2 6xk + xk+1 6 1 xk x = Av k+1 k

where A = 0 1 6 1 . Thus these columns vk are a linear dynamical system, so Theorem 3.5.1
applies provided the matrix A is diagonalizable.

We have cA(x) = (x - 3)(x + 2) so the eigenvalues are 1 = 3 and 2 = -2 with corresponding
eigenvectors x1 = 13 and x2 = -12 as the reader can check. Since

P = x1 x2 = 1 -1 3 2 is invertible, it is a diagonalizing matrix for A. The coefficients bi in

                            b1  = P-1v0 =     3
                                                5
Theorem 3.5.1 are given by  b2 5              -2   , so that the theorem gives

xk                                    k    k       3k 1            -2  k -1
                         = vk = b11 x1 + b22 x2 = 5 3  3    + 5 (-2)               2
xk+1
68 Determinants and Diagonalization

Equating top entries yields

                             xk = 1 3k+1 - (-2)k+1 for k  0

                                     5

This gives x0 = 1 = x1, and it satisfies the recurrence xk+2 = xk+1 + 6xk as is easily verified.
Hence, it is the desired formula for the xk.

Returning to Example 3.6.1, these methods give an exact formula and a good approximation for the num-
bers xk in that problem.

Example 3.6.3

In Example 3.6.1, an urban planner wants to determine xk, the number of ways that a row of k
parking spaces can be filled with cars and trucks if trucks take up two spaces each. Find a formula
for xk and estimate it for large k.

Solution. We saw in Example 3.6.1 that the numbers xk satisfy a linear recurrence

                             xk+2 = xk + xk+1 for every k  0

If we write vk = xk x as before, this recurrence becomes a matrix recurrence for the v k+1 k:

             vk+1 = xk+1 x = xk+1 = 0 1 k+2 xk + xk+1 1 1 xk x = Av k+1 k

for all k  0 where A = 0 1 1 1 . Moreover, A is diagonalizable here. The characteristic
                            2 1

polynomial is cA(x) = x - x - 1 with roots 2 1 ± 5 by the quadratic formula, so A has

eigenvalues                  1 = 12 1 + 5 and 2 = 12 1 - 5

Corresponding eigenvectors are x1 = 11 and x2 = 12 respectively as the reader can verify.
As the matrix P = x1 x2 = 1 1 1 2 is invertible, it is a diagonalizing matrix for A. We
compute the coefficients b1 and b2 (in Theorem 3.5.1) as follows:

             b1 b2 = P-1v0 = -1 2 -1 5 -1 1 11 = 1 1 5 -2

where we used the fact that 1 + 2 = 1. Thus Theorem 3.5.1 gives

                      xk x = v k+1 k = b11kx1 + b22kx2 = 15 1k 11 - 25 2k 12
Comparing top entries gives an exact formula for the numbers xk:

                             xk = 1  k+1 -  k+1 for k  02
                                     51
                                                               .6. An Application to Linear Recurrences 6

Finally, observe that 1 is dominant here (in fact, 1 = 1.618 and 2 = -0.618 to three decimal
places)  so    k+1  is  negligible  compared  with    k+1  is  large.  Thus,

             2                                      1

                                    xk  15 1k+1 for each k  0.

This is a good approximation, even for as small a value as k = 12. Indeed, repeated use of the
recurrence xk+2 = xk + xk+1 gives the exact value x12 = 233, while the approximation is
x12  (1.618)13 = 232.94.

                5

    The sequence x0, x1, x2, . . . in Example 3.6.3 was first discussed in 1202 by Leonardo Pisano of Pisa,
also known as Fibonacci,15 and is now called the Fibonacci sequence. It is completely determined by
the conditions x0 = 1, x1 = 1 and the recurrence xk+2 = xk + xk+1 for each k  0. These numbers have
been studied for centuries and have many interesting properties (there is even a journal, the Fibonacci
Quarterly, devoted exclusively to them). For example, biologists have discovered that the arrangement of
leaves around the stems of some plants follow a Fibonacci pattern. The formula xk = 15 1k+1 - 2k+1
in Example 3.6.3 is called the Binet formula. It is remarkable in that the xk are integers but 1 and 2 are
not. This phenomenon can occur even if the eigenvalues i are nonreal complex numbers.

    We conclude with an example showing that nonlinear recurrences can be very complicated.

Example 3.6.4

Suppose a sequence x0, x1, x2, . . . satisfies the following recurrence:

                                    xk+1 =    1 xk             if xk is even

                                              2
                                              3xk + 1 if xk is odd

If x0 = 1, the sequence is 1, 4, 2, 1, 4, 2, 1, . . . and so continues to cycle indefinitely. The same
thing happens if x0 = 7. Then the sequence is

                    7, 22, 11, 34, 17, 52, 26, 13, 40, 20, 10, 5, 16, 8, 4, 2, 1, . . .

and it again cycles. However, it is not known whether every choice of x0 will lead eventually to 1.
It is quite possible that, for some x0, the sequence will continue to produce different values
indefinitely, or will repeat a value and cycle without reaching 1. No one knows for sure.

   15Fibonacci was born in Italy. As a young man he travelled to India where he encountered the "Fibonacci" sequence. He
returned to Italy and published this in his book Liber Abaci in 1202. In the book he is the first to bring the Hindu decimal
system for representing numbers to Europe.
Determinants and Diagonalization

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

         Engage Active Learning App!

   Vretta-Lyryx Engage is an active learning app designed to increase
  student engagement in reading linear algebra material. The content is
"chunked" into small blocks, each with an interactive assessment activity

                           to promote comprehension.

 Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

. An Application to Systems of Differential Equations

A function f of a real variable is said to be differentiable if its derivative exists and, in this case, we let f 
denote the derivative. If f and g are differentiable functions, a system

                                  f  = 3 f + 5g
                                  g = - f + 2g

is called a system of first order differential equations, or a differential system for short. Solving many
practical problems often comes down to finding sets of functions that satisfy such a system (often in-
volving more than two functions). In this section we show how diagonalization can help. Of course an
acquaintance with calculus is required.

The Exponential Function

The simplest differential system is the following single equation:

f  = a f where a is constant                                        (3.12)

It is easily verified that f (x) = eax is one solution; in fact, Equation 3.12 is simple enough for us to find
all solutions. Suppose that f is any solution, so that f (x) = a f (x) for all x. Consider the new function g
given by g(x) = f (x)e-ax. Then the product rule of differentiation gives

                                          g(x) = f (x) -ae-ax + f (x)e-ax
                                                      . . An Application to Systems of Differential Equations

                                                 = -a f (x)e-ax + [a f (x)] e-ax
                                                 =0

for all x. Hence the function g(x) has zero derivative and so must be a constant, say g(x) = c. Thus
c = g(x) = f (x)e-ax, that is

                                                         f (x) = ceax
In other words, every solution f (x) of Equation 3.12 is just a scalar multiple of eax. Since every such
scalar multiple is easily seen to be a solution of Equation 3.12, we have proved

   Theorem 3.7.1
   The set of solutions to f  = a f is {ceax | c any constant} = Reax.

Remarkably, this result together with diagonalization enables us to solve a wide variety of differential
systems.

   Example 3.7.1
   Assume that the number n(t) of bacteria in a culture at time t has the property that the rate of
   change of n is proportional to n itself. If there are n0 bacteria present when t = 0, find the number
   at time t.

   Solution. Let k denote the proportionality constant. The rate of change of n(t) is its time-derivative
   n(t), so the given relationship is n(t) = kn(t). Thus Theorem 3.7.1 shows that all solutions n are
   given by n(t) = cekt , where c is a constant. In this case, the constant c is determined by the
   requirement that there be n0 bacteria present when t = 0. Hence n0 = n(0) = cek0 = c, so

                                                        n(t) = n0ekt

   gives the number at time t. Of course the constant k depends on the strain of bacteria.

    The condition that n(0) = n0 in Example 3.7.1 is called an initial condition or a boundary condition
and serves to select one solution from the available solutions.

General Differential Systems

Solving a variety of problems, particularly in science and engineering, comes down to solving a system
of linear differential equations. Diagonalization enters into this as follows. The general problem is to find
differentiable functions f1, f2, . . . , fn that satisfy a system of equations of the form

f   =  a11  f1  +  a12  f2  +  ·  ·  ·  +  a1n  fn

 1
f   =           +           +  ·  ·  ·  +
       a21  f1     a22..f2                 a2n..fn
 2
.. ..
..                 .                       .

f   =  an1  f1  +  an2  f2  +  ·  ·  ·  +  ann  fn

 n
Determinants and Diagonalization

where the ai j are constants. This is called a linear system of differential equations or simply a differen-
tial system. The first step is to put it in matrix form. Write

                                                   f                   a11 a12 · · · a1n                   
                              f1
                                                          1
                                f2                                     a21  a22  ···       a2n             
                                                   f                                                       
                      f =  ..                   2              A =  .. ..                  .. 
                           .                 f =  ..                 . .                   .

                                fn                .                    an1 an2 · · · ann
                                                      f

                                                          n

Then the system can be written compactly using matrix multiplication:

                                                         f = Af

Hence, given the matrix A, the problem is to find a column f of differentiable functions that satisfies this
condition. This can be done if A is diagonalizable. Here is an example.

Example 3.7.2                                        f   =     f1 + 3 f2
Find a solution to the system
                                                      1
that satisfies f1(0) = 0, f2(0) = 5.                 f   =     2 f1 + 2 f2

                                                      2

Solution. This is f = Af, where f = f1f2 and A = 1 3 2 2 . The reader can verify that
cA(x) = (x - 4)(x + 1), and that x1 = 11 and x2 = 3-2 are eigenvectors corresponding to
the eigenvalues 4 and -1, respectively. Hence the diagonalization algorithm gives
P-1AP = 4 0 0 -1 , where P = x1 x2 = 1 3 1 -2 . Now consider new functions g1 and g2

given by f = Pg (equivalently, g = P-1f ), where g = g1 Then
                                                                   g2

                           f1f2 = 1 3 g1 1 -2 g2                     that is, f1 = g1 + 3g2 f2 = g1 - 2g2

Hence  f   =  g    +  3g2  and  f   =   g    -  2g2  so  that

        1       1                2        1

                                                f              13      g1 = Pg
                                                                       g2
                                                 1
                                    f=  =                      1 -2
                                                f2

If this is substituted in f = Af, the result is Pg = APg, whence

                                                         g = P-1APg

But this means that                 g1 4 0  =                               so g1 = 4g  1
                                                                                g2 = -g2
                                    g2          0 -1           g1 ,
                                                               g2
                               . . An Application to Systems of Differential Equations

Hence Theorem 3.7.1 gives g1(x) = ce4x, g2(x) = de-x, where c and d are constants. Finally, then,

               f1(x) f2(x) = P g1(x) g2(x) = 1 3 1 -2      ce4x   ce4x + 3de-x
                                                           de-x = ce4x - 2de-x

so the general solution is

                               f1(x) = ce4x + 3de-x        c and d constants
                               f2(x) = ce4x - 2de-x

It is worth observing that this can be written in matrix form as

                               f1(x) f2(x) = c 11 e4x + d 3 -2 e-x

That is,                       f(x) = cx1e4x + dx2e-x

This form of the solution works more generally, as will be shown.
Finally, the requirement that f1(0) = 0 and f2(0) = 5 in this example determines the constants c
and d:

                               0 = f1(0) = ce0 + 3de0 = c + 3d
                               5 = f2(0) = ce0 - 2de0 = c - 2d

These equations give c = 3 and d = -1, so

                               f1(x) = 3e4x - 3e-x
                               f2(x) = 3e4x + 2e-x

satisfy all the requirements.

The technique in this example works in general.

Theorem 3.7.2

Consider a linear system

                                                   f = Af

of differential equations, where A is an n × n diagonalizable matrix. Let P-1AP be diagonal, where
P is given in terms of its columns

                                              P = [x1, x2, · · · , xn]

and {x1, x2, . . . , xn} are eigenvectors of A. If xi corresponds to the eigenvalue i for each i, then
every solution f of f = Af has the form

                               f(x) = c1x1e1x + c2x2e2x + · · · + cnxnenx

where c1, c2, . . . , cn are arbitrary constants.
       Determinants and Diagonalization

Proof. By Theorem 3.4.1, the matrix P = x1 x2 . . . xn is invertible and

                                                                1 0 · · · 0              

                                                 -1     0       2 · · ·               0
                                                                                         
                                               P AP =  .. ..                          .. 
                                                        . .                           .

                                                                0 0 · · · n

                                                                          
                                             f1                              g1

                                            f2                                  g2                  -1
                                                                                      
As in Example 3.7.2, write f =  ..  and define g =  ..  by g = P f; equivalently, f = Pg. If
                                          .                               .

                                            fn                                  gn

P = pi j , this gives

                                                 fi = pi1g1 + pi2g2 + · · · + pingn

Since the pi j are constants, differentiation preserves this relationship:

                                               f   =  pi1g1  +  pi2g2  +  ·  ·  ·  +  pingn

                                                i

so f = Pg. Substituting this into f = Af gives Pg = APg. But then left multiplication by P-1 gives
g = P-1APg, so the original system of equations f = Af for f becomes much simpler in terms of g:

                                            g         1 0 · · ·                 0      
                                                                                         g1
                                              1
                                           g          0         2 · · ·         0
                                           2                                             g2   
                                           ..  =  .. ..                                       
                                                                                   ..   .. 
                                          . . .                                    .  . 

                                            g           0 0 · · · n gn

                                              n

Hence  g    =  igi  holds  for  each  i,  and  Theorem  3.7.1   implies            that  the  only  solutions  are

         i

                                            gi(x) = cieix ci some constant

Then the relationship f = Pg gives the functions f1, f2, . . . , fn as follows:

                                                    c1e1x 

                                                      c2e2x  
                    f(x) = [x1, x2, · · · , xn]  .  = c1x1e1x + c2x2e2x + · · · + cnxnenx
                                                    .. 

                                                      cnenx

This is what we wanted.

The theorem shows that every solution to f = Af is a linear combination

                                      f(x) = c1x1e1x + c2x2e2x + · · · + cnxnenx

where the coefficients ci are arbitrary. Hence this is called the general solution to the system of differential

equations. In most cases the solution functions fi(x) are required to satisfy boundary conditions, often of

the form fi(a) = bi, where a, b1, . . . , bn are prescribed numbers. These conditions determine the constants
ci. The following example illustrates this and displays a situation where one eigenvalue has multiplicity

greater than 1.
                                         . . An Application to Systems of Differential Equations

Example 3.7.3
Find the general solution to the system

         f                               =  5 f1 + 8 f2 + 16 f3

          1
         f                               =  4 f1 +          f2 + 8 f3

          2
                                         =  -4        -  4        -  11
         f  3                                   f  1        f  2         f  3

Then find a solution satisfying the boundary conditions f1(0) = f2(0) = f3(0) = 1.

                                                                   5 8 16 

Solution. The system has the form f = Af, where A =  4 1 8 . In this case

                                                                     -4 -4 -11

cA(x) = (x + 3)2(x - 1) and eigenvectors corresponding to the eigenvalues -3, -3, and 1 are,
respectively,

       -1                                              -2                       2

x1 =  1  x2 =  0  x3 =  1 
      0                                                     1                      -1

Hence, by Theorem 3.7.2, the general solution is

 -1                                       -2                          2
f(x) = c1  1  e-3x + c2  0  e-3x + c3  1  ex,
                                                                                       ci constants.

   0                                        1                                  -1

The boundary conditions f1(0) = f2(0) = f3(0) = 1 determine the constants ci.

1                                         -1   -2   2 

 1  = f(0) = c1  1  + c2  0  + c3  1 
1                                               0                        1             -1

          -1 -2 2   c1 
      =  1 0 1   c2 

              0 1 -1 c3

The solution is c1 = -3, c2 = 5, c3 = 4, so the required specific solution is
                                               f1(x) = -7e-3x + 8ex
                                               f2(x) = -3e-3x + 4ex
                                               f3(x) = 5e-3x - 4ex
6 Determinants and Diagonalization

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                              Engage Active Learning App!

                                        Vretta-Lyryx Engage is an active learning app designed to increase
                                       student engagement in reading linear algebra material. The content is
                                     "chunked" into small blocks, each with an interactive assessment activity

                                                                to promote comprehension.

                                      Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

.8 Proof of the Cofactor Expansion Theorem

Recall that our definition of the term determinant is inductive: The determinant of any 1 × 1 matrix is
defined first; then it is used to define the determinants of 2 × 2 matrices. Then that is used for the 3 × 3
case, and so on. The case of a 1 × 1 matrix [a] poses no problem. We simply define

                                                                    det [a] = a

as in Section 3.1. Given an n × n matrix A, define Ai j to be the (n - 1) × (n - 1) matrix obtained from A by
deleting row i and column j. Now assume that the determinant of any (n - 1) × (n - 1) matrix has been

defined. Then the determinant of A is defined to be

                           det A = a11 det A11 - a21 det A21 + · · · + (-1)n+1an1 det An1

                                                   n

                       = (-1)i+1ai1 det Ai1

                                                  i=1

where summation notation has been introduced for convenience.16 Observe that, in the terminology of
Section 3.1, this is just the cofactor expansion of det A along the first column, and that (-1)i+ j det Ai j is
the (i, j)-cofactor (previously denoted as ci j(A)).17 To illustrate the definition, consider the 2 × 2 matrix

16Summation notation is a convenient shorthand way to write sums of similar expressions. For example a1 + a2 + a3 + a4 =
  4  ai,                                      k8=5  akbk,  and  12     22     32     42     52      j5=1  j2.
          a5b5  +  a6b6  +  a7b7  +  a8b8  =                        +      +      +      +      =
i=1
17Note that we used the expansion along row 1 at the beginning of Section 3.1. The column 1 expansion definition is more

convenient here.
                                                                 .8. Proof of the Cofactor Expansion Theorem

A = a11 a12 . Then the definition gives
        a21 a22

                        det a11 a12 a21 a22 = a11 det [a22] - a21 det [a12] = a11a22 - a21a12

and this is the same as the definition in Section 3.1.
    Of course, the task now is to use this definition to prove that the cofactor expansion along any row

or column yields det A (this is Theorem 3.1.1). The proof proceeds by first establishing the properties of
determinants stated in Theorem 3.1.2 but for rows only (see Lemma 3.8.2). This being done, the full proof
of Theorem 3.1.1 is not difficult. The proof of Lemma 3.8.2 requires the following preliminary result.

   Lemma 3.8.1
   Let A, B, and C be n × n matrices that are identical except that the pth row of A is the sum of the
    pth rows of B and C. Then

                                                  det A = det B + det C

Proof. We proceed by induction on n, the cases n = 1 and n = 2 being easily checked. Consider ai1 and
Ai1:

    Case 1: If i = p,
                                ai1 = bi1 = ci1 and det Ai1 = det Bi1 = det Ci1

by induction because Ai1, Bi1, Ci1 are identical except that one row of Ai1 is the sum of the corresponding
rows of Bi1 and Ci1.

    Case 2: If i = p,
                                      ap1 = bp1 + cp1 and Ap1 = Bp1 = Cp1

Now write out the defining sum for det A, splitting off the pth term for special attention.

              det A =  ai1(-1)i+1 det Ai1 + ap1(-1)p+1 det Ap1

                                         i= p

                   =  ai1(-1)i+1 [ det Bi1 + det Bi1] + (bp1 + cp1)(-1)p+1 det Ap1

                                         i= p

where det Ai1 = det Bi1 + det Ci1 by induction. But the terms here involving Bi1 and bp1 add up to det B
because ai1 = bi1 if i = p and Ap1 = Bp1. Similarly, the terms involving Ci1 and cp1 add up to det C. Hence
det A = det B + det C, as required.

   Lemma 3.8.2
   Let A = ai j denote an n × n matrix.

       1. If B = bi j is formed from A by multiplying a row of A by a number u, then det B = u det A.

       2. If A contains a row of zeros, then det A = 0.

       3. If B = bi j is formed by interchanging two rows of A, then det B = - det A.

       4. If A contains two identical rows, then det A = 0.
8 Determinants and Diagonalization

5. If B = bi j is formed by adding a multiple of one row of A to a different row, then
   det B = det A.

Proof. For later reference the defining sums for det A and det B are as follows:

              n                                                                         (3.13)
                                                                                        (3.14)
det A =  ai1(-1)i+1 det Ai1

             i=1
              n

det B =  bi1(-1)i+1 det Bi1

             i=1

    Property 1. The proof is by induction on n, the cases n = 1 and n = 2 being easily verified. Consider
the ith term in the sum 3.14 for det B where B is the result of multiplying row p of A by u.

a. If i = p, then bi1 = ai1 and det Bi1 = u det Ai1 by induction because Bi1 comes from Ai1 by multi-
   plying a row by u.

b. If i = p, then bp1 = uap1 and Bp1 = Ap1.

In either case, each term in Equation 3.14 is u times the corresponding term in Equation 3.13, so it is clear
that det B = u det A.

    Property 2. This is clear by property 1 because the row of zeros has a common factor u = 0.

    Property 3. Observe first that it suffices to prove property 3 for interchanges of adjacent rows. (Rows
p and q (q > p) can be interchanged by carrying out 2(q - p) - 1 adjacent changes, which results in an
odd number of sign changes in the determinant.) So suppose that rows p and p + 1 of A are interchanged
to obtain B. Again consider the ith term in Equation 3.14.

a. If i = p and i = p + 1, then bi1 = ai1 and det Bi1 = - det Ai1 by induction because Bi1 results from
   interchanging adjacent rows in Ai1. Hence the ith term in Equation 3.14 is the negative of the ith
   term in Equation 3.13. Hence det B = - det A in this case.

b. If i = p or i = p + 1, then bp1 = ap+1, 1 and Bp1 = Ap+1, 1, whereas bp+1, 1 = ap1 and Bp+1, 1 = Ap1.
   Hence terms p and p + 1 in Equation 3.14 are

                             bp1(-1)p+1 det Bp1 = -ap+1, 1(-1)(p+1)+1 det (Ap+1, 1)

                             bp+1, 1(-1)(p+1)+1 det Bp+1, 1 = -ap1(-1)p+1 det (Ap1)

    This means that terms p and p + 1 in Equation 3.14 are the same as these terms in Equation 3.13,
except that the order is reversed and the signs are changed. Thus the sum 3.14 is the negative of the sum
3.13; that is, det B = - det A.

    Property 4. If rows p and q in A are identical, let B be obtained from A by interchanging these rows.
Then B = A so det A = det B. But det B = - det A by property 3 so det A = - det A. This implies that
det A = 0.

    Property 5. Suppose B results from adding u times row q of A to row p. Then Lemma 3.8.1 applies to
B to show that det B = det A + det C, where C is obtained from A by replacing row p by u times row q. It
now follows from properties 1 and 4 that det C = 0 so det B = det A, as asserted.
                                                                 .8. Proof of the Cofactor Expansion Theorem

    These facts are enough to enable us to prove Theorem 3.1.1. For convenience, it is restated here in the
notation of the foregoing lemmas. The only difference between the notations is that the (i, j)-cofactor of
an n × n matrix A was denoted earlier by

                                                 ci j(A) = (-1)i+ j det Ai j

Theorem 3.8.1
If A = ai j is an n × n matrix, then

1. det A = in=1 ai j(-1)i+ j det Ai j   (cofactor expansion along column j).
2. det A =  jn=1 ai j(-1)i+ j det Ai j  (cofactor expansion along row i).

Here Ai j denotes the matrix obtained from A by deleting row i and column j.

Proof. Lemma 3.8.2 establishes the truth of Theorem 3.1.2 for rows. With this information, the arguments
in Section 3.2 proceed exactly as written to establish that det A = det AT holds for any n × n matrix A.
Now suppose B is obtained from A by interchanging two columns. Then BT is obtained from AT by
interchanging two rows so, by property 3 of Lemma 3.8.2,

det B = det BT = - det AT = - det A

Hence property 3 of Lemma 3.8.2 holds for columns too.

    This enables us to prove the cofactor expansion for columns. Given an n × n matrix A = ai j , let
B = bi j be obtained by moving column j to the left side, using j - 1 interchanges of adjacent columns.
Then det B = (-1) j-1 det A and, because Bi1 = Ai j and bi1 = ai j for all i, we obtain

                                                                                                 n

                    det A = (-1) j-1 det B = (-1) j-1  bi1(-1)i+1 det Bi1

                                                                                               i=1
                                                       n

                         =  ai j(-1)i+ j det Ai j

                                                     i=1

This is the cofactor expansion of det A along column j.
    Finally, to prove the row expansion, write B = AT . Then Bi j = (ATi j) and bi j = a ji for all i and j.

Expanding det B along column j gives

                                            n

det A = det AT = det B =  bi j(-1)i+ j det Bi j

                                           i=1

n                                                     n
=  a ji(-1) j+i det (ATji) =  a ji(-1) j+i det A ji
i=1                                                   i=1

This is the required expansion of det A along row j.
 8 Determinants and Diagonalization

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.
                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.
                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!
Chapter 4

                                        Vector Geometry

. Vectors and Lines

In this chapter we study the geometry of 3-dimensional space. We view a point in 3-space as an arrow from
the origin to that point. Doing so provides a "picture" of the point that is truly worth a thousand words.
We used this idea earlier, in Section 2.6, to describe rotations, reflections, and projections of the plane R2.
We now apply the same techniques to 3-space to examine similar transformations of R3. Moreover, the
method enables us to completely describe all lines and planes in space.

Vectors in R3

Introduce a coordinate system in 3-dimensional space in the usual way. First choose a point O called the
origin, then choose three mutually perpendicular lines through O, called the x, y, and z axes, and establish
a number scale on each axis with zero at the origin. Given a point P in 3-space we associate three numbers
x, y, and z with P, as described in Figure 4.1.1. These numbers are called the coordinates of P, and we
denote the point as (x, y, z), or P(x, y, z) to emphasize the label P. The result is called a cartesian1
coordinate system for 3-space, and the resulting description of 3-space is called cartesian geometry.

                                As in the plane, we introduce vectors by identifying each point

                                     
                                     x
              z                 P(x, y, z) with the vector v =  y  in R3, represented by the arrow

           O     P(x, y, z)                                               z
                         x      from the origin to P as in Figure 4.1.1. Informally, we say that the point P
x
                    v= y        has vector v, and that vector v has point P. In this way 3-space is identi-
                             z  fied with R3, and this identification will be made throughout this chapter,

                            y   often without comment. In particular, the terms "vector" and "point" are
                                interchangeable.2 The resulting description of 3-space is called vector
                 P0(x, y, 0)
                                                                              0
Figure 4.1.1
                                geometry. Note that the origin is 0 =  0 .
                                                                                 0

Length and Direction

We are going to discuss two fundamental geometric properties of vectors in R3: length and direction. First,
if v is a vector with point P, the length v of vector v is defined to be the distance from the origin to P,
that is the length of the arrow representing v. The following properties of length will be used frequently.

    1Named after René Descartes who introduced the idea in 1637.
    2Recall that we defined Rn as the set of all ordered n-tuples of real numbers, and reserved the right to denote them as rows
or as columns.

                                181
8 Vector Geometry

Theorem 4.1.1
          x

Let v =  y  be a vector.
             z

   1. v = x2 + y2 + z2. 3

   2. v = 0 if and only if v = 0
   3. av = |a| v for all scalars a. 4

   z                 Proof. Let v have point P(x, y, z).

                P       1. In Figure 4.1.2, v is the hypotenuse of the right triangle OQP, and
                            so v 2 = h2 + z2 by Pythagoras' theorem.5 But h is the hypotenuse
       v                    of the right triangle ORQ, so h2 = x2 + y2. Now (1) follows by
                            eliminating h2 and taking positive square roots.
                  z
   O                    2. If v = 0, then x2 + y2 + z2 = 0 by (1). Because squares of real
                            numbers are nonnegative, it follows that x = y = z = 0, and hence
   Rx h y                   that v = 0. The converse is because 0 = 0.
   iy
x                       3. We have av = ax ay az T so (1) gives
             Q

   Figure 4.1.2

                                                           av 2 = (ax)2 + (ay)2 + (az)2 = a2 v 2
   Hence av = a2 v , and we are done because a2 = |a| for any real number a.

Of course the R2-version of Theorem 4.1.1 also holds.

Example 4.1.1

          2

                                                       3
If v =  -1  then v = 4 + 1 + 9 = 14. Similarly if v = -4 in 2-space then
   3
   v = 9 + 16 = 5.

    When we view two nonzero vectors as arrows emanating from the origin, it is clear geometrically
what we mean by saying that they have the same or opposite direction. This leads to a fundamental new
description of vectors.

    3When we write p we mean the positive square root of p.
    4Recall that the absolute value |a| of a real number is defined by |a| = a if a  0 -a if a < 0 .
    5Pythagoras' theorem states that if a and b are sides of right triangle with hypotenuse c, then a2 + b2 = c2. A proof is given
at the end of this section.
                                                                                     . . Vectors and Lines 8

Theorem 4.1.2
Let v = 0 and w = 0 be vectors in R3. Then v = w as matrices if and only if v and w have the same
direction and the same length.6

         z                       Proof. If v = w, they clearly have the same direction and length. Conversely,

                    P            let v and w be vectors with points P(x, y, z) and Q(x1, y1, z1) respectively. If
              v
                                 v and w have the same length and direction then, geometrically, P and Q must
                           Q
                  w              be  the  same  point  (see  Figure  4.1.3).  Hence  x = x1,  y = y1,  and  z = z1,  that  is
         Oy                                             
x                                         x     x1

  Figure 4.1.3                   v =  y  =  y1  = w.

                                          z     z1

    A characterization of a vector in terms of its length and direction only is called an intrinsic description
of the vector. The point to note is that such a description does not depend on the choice of coordinate
system in R3. Such descriptions are important in applications because physical laws are often stated in
terms of vectors, and these laws cannot depend on the particular coordinate system used to describe the
situation.

Geometric Vectors

If A and B are distinct points in space, the arrow from A to B has length and direction.

                                                             z

                                                                     B

                                                                -
                                                       A        AB

                                                             O          y

                                                x

                                                          Figure 4.1.4

Hence:

Definition 4.1 Geometric Vectors

Suppose that A and B are any two points in R3. In Figure 4.1.4 the line segment from A to B is
        -                                                                                                   -
denoted AB and is called the geometric vector from A to B. Point A is called the tail of AB, B is
                              -                 -                    -
called the tip of AB, and the length of AB is denoted AB .

                                             3                          -
Note that if v is any vector in R with point P then v = OP is itself a geometric vector where O is the
                              -
origin. Referring to AB as a "vector" seems justified by Theorem 4.1.2 because it has a direction (from A
                              -
to B) and a length AB .

    6It is Theorem 4.1.2 that gives vectors their power in science and engineering because many physical quantities are deter-
mined by their length and magnitude (and are called vector quantities). For example, saying that an airplane is flying at 200
km/h does not describe where it is going; the direction must also be specified. The speed and direction comprise the velocity
of the airplane, a vector quantity.
8 Vector Geometry

        y                                         However there appears to be a problem because two geometric vectors

                                              can have the same length and direction even if the tips and tails are differ-
                                                                       - -                                                                 
                    B(2, 3)                   ent. For example AB and PQ in Figure 4.1.5 have the same length 5 and

   Q(0, 2)                                    the same direction (1 unit left and 2 units up) so, by Theorem 4.1.2, they

          P(1, 0)                             are the same vector! The best way to understand this apparent paradox
O                        A(3, 1)                     - -
                                              is to see AB and PQ as different representations of the same underlying             7
                                x
                                              vector -12 . Once it is clarified, this phenomenon is a great benefit
           Figure 4.1.5
                                              because, thanks to Theorem 4.1.2, it means that the same geometric vec-

                                              tor can be positioned anywhere in space; what is important is the length

                                              and direction, not the location of the tip and tail. This ability to move

geometric vectors about is very useful as we shall soon see.

The Parallelogram Law

   vP                  P             We now give an intrinsic description of the sum of two vectors v and w in R3,
Aw                      v+w          that is a description that depends only on the lengths and directions of v and w
                                     and not on the choice of coordinate system. Using Theorem 4.1.2 we can think
                   Q                 of these vectors as having a common tail A. If their tips are P and Q respectively,
                                     then they both lie in a plane P containing A, P, and Q, as shown in Figure 4.1.6.
Figure 4.1.6                         The vectors v and w create a parallelogram8 in P , shaded in Figure 4.1.6, called
                                     the parallelogram determined by v and w.

    If we now choose a coordinate system in the plane P with A as origin, then the parallelogram law in
the plane (Section 2.6) shows that their sum v + w is the diagonal of the parallelogram they determine with
tail A. This is an intrinsic description of the sum v + w because it makes no reference to coordinates. This
discussion proves:

Theorem: The Parallelogram Law

In the parallelogram determined by two vectors v and w, the vector v + w is the diagonal with the
same tail as v and w.

                                                  w                                                         Because a vector can be po-
                                                                                                        sitioned with its tail at any point,
                v   v+w                    v                           w+v v                            the parallelogram law leads to
                                                                                                        another way to view vector addi-
            P       w                (b) v + w (c) w                                                    tion. In Figure 4.1.7(a) the sum
           (a)                                                                                          v + w of two vectors v and w is
                                                                                                        shown as given by the parallelo-
                                           Figure 4.1.7                                                 gram law. If w is moved so its
                                                                                                        tail coincides with the tip of v
(Figure 4.1.7(b)) then         the sum v + w is seen as "first v and then w.                         Similarly, moving the tail of v to
the tip of w shows in          Figure 4.1.7(c) that v + w is "first w and then                       v." This will be referred to as the

7Fractions         provide  another  example  of  quantities  that  can    be  the  same  but  look  different.  For  example  6  and  14  certainly
                                                                                                                               9       21
appear  different,  but  they  are  equal  fractions--both  equal   2  in  "lowest  terms".
                                                                    3
8Recall that a parallelogram is a four-sided figure whose opposite sides are parallel and of equal length.
                                                                                  . . Vectors and Lines 8

tip-to-tail rule, and it gives a graphic illustration of why v + w = w + v.
           -

    Since AB denotes the vector from a point A to a point B, the tip-to-tail rule takes the easily remembered
form - - -

                                                      AB + BC = AC

for any points A, B, and C. The next example uses this to derive a theorem in geometry without using
coordinates.

Example 4.1.2
Show that the diagonals of a parallelogram bisect each other.

                      B             Solution. Let the parallelogram have vertices A, B, C, and D,

                                    as shown; let E denote the intersection of the two diagonals;

A                                   and let M denote the midpoint of diagonal AC. We must show
           ME                       that M = E and that this is the midpoint of diagonal BD. This

                                                                            - --
                                    is accomplished by showing that BM = MD. (Then the fact

                         C          that these vectors have the same direction means that M = E,

                                    and the fact that they have the same length means that M = E
                   D                                           - -
                                    is the midpoint of BD.) Now AM = MC because M is the midpoint
                                              - -
                                    of AC, and BA = CD because the figure is a parallelogram. Hence

                            - - - - - - - --
                            BM = BA + AM = CD + MC = MC +CD = MD

where the first and last equalities use the tip-to-tail rule of vector addition.

                            u+v+w                 One reason for the importance of the tip-to-
                                              tail rule is that it means two or more vectors can
     w                      u       w         be added by placing them tip-to-tail in sequence.
   u                                          This gives a useful "picture" of the sum of several
                               v              vectors, and is illustrated for three vectors in Fig-
                v                             ure 4.1.8 where u + v + w is viewed as first u, then
                                              v, then w.
                      Figure 4.1.8

                                                   There is also a simple geometrical way to visu-

                B                    v-w v+w  alize the (matrix) difference v - w of two vectors.
                         v
v                  -                          If v and w are positioned so that they have a com-
                   CB           w
                                              mon tail A (see Figure 4.1.9), and if B and C are
A
          wC                                  their respective tips, then the tip-to-tail rule gives
                                                   -                              -
                                              w +CB = v. Hence v - w = CB is the vector from
                      Figure 4.1.9
                                              the tip of w to the tip of v. Thus both v - w and

v + w appear as diagonals in the parallelogram determined by v and w (see Figure 4.1.9). We record this

for reference.

Theorem 4.1.3
If v and w have a common tail, then v - w is the vector from the tip of w to the tip of v.
86 Vector Geometry

    One of the most useful applications of vector subtraction is that it gives a simple formula for the vector
from one point to another, and for the distance between the points.

Theorem 4.1.4
Let P1(x1, y1, z1) and P2(x2, y2, z2) be two points. Then:

                  x2 - x1  

   1.  -          y2 - y1  .
                

       P1P2 =

                  z2 - z1

   2. The distance between P1 and P2 is (x2 - x1)2 + (y2 - y1)2 + (z2 - z1)2.

                               Proof. If O is the origin, write

       v1         P1                                                           
                                                   - x1               - x2
O                 --                     v1 = OP1 =  y1  and v2 = OP2 =  y2 
                  P1P2

             v2 P2                                          z1                 z2

   Figure 4.1.10               as in Figure 4.1.10.
                                                                     -

                                   Then Theorem 4.1.3 gives P1P2 = v2 - v1, and (1) follows. But the
                                                                     -

                               distance between P1 and P2 is P1P2 , so (2) follows from (1) and Theo-
                               rem 4.1.1.

Of course the R2-version of Theorem 4.1.4 is also valid: If P1(x1, y1) and P2(x2, y2) are points in R2,

     -       x2 - x1 y2 - y1 , and the distance between P1 and P2 is  (x2 - x1)2 + (y2 - y1)2.
then P1P2 =

Example 4.1.3

The distance between P1(2, -1, 3) and P2(1, 1, 4) is  (-1)2 + (2)2 + (1)2 = 6, and the vector

                            -1 
                   -
                               2 .

from P1 to P2 is P1P2 =        1

    As for the parallelogram law, the intrinsic rule for finding the length and direction of a scalar multiple
of a vector in R3 follows easily from the same situation in R2.

Theorem: Scalar Multiple Law
If a is a real number and v = 0 is a vector then:

   1. The length of av is av = |a| v .

   2. If9av = 0, the direction of av is  the same as v if a > 0,
                                         opposite to v if a < 0.
                                                                                          . . Vectors and Lines 8

Proof.

1. This is part of Theorem 4.1.1.

2. Let O denote the origin in R3, let v have point P, and choose any plane containing O and P. If we
                                                                                      -

   set up a coordinate system in this plane with O as origin, then v = OP so the result in (2) follows
   from the scalar multiple law in the plane (Section 2.6).

Figure 4.1.11 gives several examples of scalar multiples of a vector v.

            2v              (-2)v      Consider a line L through the origin, let P be any point on L other than
v                    (- 21 )v                                     -

          1v                       the origin O, and let p = OP. If t = 0, then tp is a point on L because it
                                   has direction the same or opposite as that of p. Moreover t > 0 or t < 0
             2                     according as the point tp lies on the same or opposite side of the origin as
                                   P. This is illustrated in Figure 4.1.12.
        Figure 4.1.11

                                                                                          1

                         L         A vector u is called a unit vector if u = 1. Then i =  0 ,

                     P 3p                                                                 0

           O     1p  p2            0                      0

-1p             2                  j =  1 , and k =  0  are unit vectors, called the coordinate vectors.

        2                          0                         1

        Figure 4.1.12              We discuss them in more detail in Section 4.2.

Example 4.1.4

If v = 0 show that         1  v is the unique unit vector in the same direction as v.
                           v

Solution. The vectors in the same direction as v are the scalar multiples av where a > 0. But
 av = |a| v = a v when a > 0, so av is a unit vector if and only if a = 1v .

    The next example shows how to find the coordinates of a point on the line segment between two given
points. The technique is important and will be used again below.

Example 4.1.5
Let p1 and p2 be the vectors of two points P1 and P2. If M is the point one third the way from P1 to
P2, show that the vector m of M is given by

                                               m  =  2p      +   1p

                                                     31          32

Conclude that if P1 = P1(x1, y1, z1) and P2 = P2(x2, y2, z2), then M has coordinates

                                   M=M  2  x1  +  1  x2,  2  y1  +  1  y2,  32 z1 + 1 z23
                                        3         3       3         3

9Since the zero vector has no direction, we deal only with the case av = 0.
88 Vector Geometry

                                   Solution. The vectors p1, p2, and m are shown in the diagram. We
                                         -- 1 -                       --                                    -
                   P1              have P1M = 3 P1P2 because P1M is in the same direction as P1P2 and
            p1                     1                                                 -
                                    3 as long. By Theorem 4.1.3 we have P1P2 = p2 - p1, so tip-to-tail
                         M         addition gives

          m                                                    --                1                2   1
O                                                    m = p1 + P1M = p1 + 3 (p2 - p1) = 3 p1 + 3 p2

              p2               P2                                                                                 
                                                                                                  x1              x2

                                   as required. For the coordinates, we have p1 =  y1  and p2 =  y2 ,

                                   so z1 z2

                                                                              2 x1 + 1 x2   

                                                                         3 3 
                                                 x1            x2
                                      m= 2           + 1                       2 y1 + 1 y2
                                                 y1            y2     =       3      3      
                                              3             3                               

                                                 z1 z2  2 1 
                                                                               3 z1 + 3 z2

by matrix addition. The last statement follows.

Note  that  in  Example  4.1.5  m     =  2p  +   1p  is  a  "weighted  average"      of  p1  and  p2  with  more  weight  on  p1

because m is closer to p1.               31      32

The point M halfway between points P1 and P2 is called the midpoint between these points. In the
same way, the vector m of M is
                                                     1p        1p     12 (p1
                                             m   =          +      =          + p2)
                                                     21        22

as the reader can verify, so m is the "average" of p1 and p2 in this case.

Example 4.1.6

Show that the midpoints of the four sides of any quadrilateral are the vertices of a parallelogram.
Here a quadrilateral is any figure with four vertices and straight sides.

                   F           C         Solution. Suppose that the vertices of the quadrilateral are A, B,
                                         C, and D (in that order) and that E, F, G, and H are the midpoints
            B
                                                                                                             - -
      E                     G            of the sides as shown in the diagram. It suffices to show EF = HG
                                         (because then sides EF and HG are parallel and of equal length).

                                                                                                          - 1 -
                                         Now the fact that E is the midpoint of AB means that EB = 2 AB.

                                                     - 1 -
                                         Similarly, BF = 2BC, so

      A                                          - - - 1 - 1 - 1 - - 1 -
                                                 EF = EB + BF = 2 AB + 2 BC = 2 (AB + BC) = 2 AC
                H           D

                                                                                    - 1 -             - -
                                         A similar argument shows that HG = 2AC too, so EF = HG
                                         as required.
                                                                          . . Vectors and Lines 8

Definition 4.2 Parallel Vectors in R3
Two nonzero vectors are called parallel if they have the same or opposite direction.

    Many geometrical propositions involve this notion, so the following theorem will be referred to repeat-
edly.

   Theorem 4.1.5
   Two nonzero vectors v and w are parallel if and only if one is a scalar multiple of the other.

Proof. If one of them is a scalar multiple of the other, they are parallel by the scalar multiple law.

Conversely, assume that v and w are parallel and write d =  v  for convenience. Then v and w have
                                                            w
the same or opposite direction. If they have the same direction we show that v = dw by showing that v

and dw have the same length and direction. In fact, dw = |d| w = v by Theorem 4.1.1; as to the

direction, dw and w have the same direction because d > 0, and this is the direction of v by assumption.

Hence v = dw in this case by Theorem 4.1.2. In the other case, v and w have opposite direction and a

similar argument shows that v = -dw. We leave the details to the reader.

Example 4.1.7

                                                                                                 - -
Given points P(2, -1, 4), Q(3, -1, 3), A(0, 2, 1), and B(1, 3, 0), determine if PQ and AB are
parallel.

                                  -  -                                    - -
Solution. By Theorem 4.1.3, PQ = (1, 0, -1) and AB = (1, 1, -1). If PQ = tAB then
                                                                                      -
(1, 0, -1) = (t, t, -t), so 1 = t and 0 = t, which is impossible. Hence PQ is not a scalar multiple
-
of AB, so these vectors are not parallel by Theorem 4.1.5.

Lines in Space

These vector techniques can be used to give a very simple way of describing straight lines in space. In
order to do this, we first need a way to specify the orientation of such a line, much as the slope does in the
plane.

Definition 4.3 Direction Vector of a Line
With this in mind, we call a nonzero vector d = 0 a direction vector for the line if it is parallel to
-
AB for some pair of distinct points A and B on the line.

       P0 d                                                              -
                               P     Of course it is then parallel to CD for any distinct points C and
                                     D on the line. In particular, any nonzero scalar multiple of d
                 P0P                 will also serve as a direction vector of the line.
   p0 p
                                         We use the fact that there is exactly one line that passes
      Origin                         through a particular point P0(x0, y0, z0) and has a given direc-

        Figure 4.1.13                                  a
                                     tion vector d =  b . We want to describe this line by giving

                                                           c
Vector Geometry

                                a condition on x, y, and z that the point P(x, y, z) lies on this

                          x
   x0

line. Let p0 =  y0  and p =  y  denote the vectors of P0 and P, respectively (see Figure 4.1.13).

                      z0  z
Then                                              -

                                      p = p0 + P0P

                          -                                        -
Hence P lies on the line if and only if P0P is parallel to d--that is, if and only if P0P = td for some scalar

t by Theorem 4.1.5. Thus p is the vector of a point on the line if and only if p = p0 + td for some scalar t.
This discussion is summed up as follows.

Theorem: Vector Equation of a Line
The line parallel to d = 0 through the point with vector p0 is given by

                                           p = p0 + td t any scalar

In other words, the point P with vector p is on this line if and only if a real number t exists such
that p = p0 + td.

In component form the vector equation becomes

                             
                             x                 x0               a

                           y  =  y0  + t  b 

                             z                 z0               c

Equating components gives a different description of the line.

Theorem: Parametric Equations of a Line

                                                                     a
The line through P0(x0, y0, z0) with direction vector d =  b  = 0 is given by

                                                                        c

                                             x = x0 + ta
                                            y = y0 + tb t any scalar
                                             z = z0 + tc

In other words, the point P(x, y, z) is on this line if and only if a real number t exists such that
x = x0 + ta, y = y0 + tb, and z = z0 + tc.

Example 4.1.8
Find the equations of the line through the points P0(2, 0, 1) and P1(4, -1, 1).
                                                                                   . . Vectors and Lines

                    -        2
                    P0P1
Solution.  Let  d=        =    -1    denote  the  vector  from  P0  to  P1.  Then  d  is  parallel  to  the  line

                               0

(P0 and P1 are on the line), so d serves as a direction vector for the line. Using P0 as the point on

the line leads to the parametric equations

                                     x = 2 + 2t   t a parameter

                                     y = -t
                                     z=1

Note that if P1 is used (rather than P0), the equations are

                                     x = 4 + 2s   s a parameter
                                     y = -1 - s
                                     z=1

These are different from the preceding equations, but this is merely the result of a change of
parameter. In fact, s = t - 1.

Example 4.1.9

Find the equations of the line through P0(3, -1, 2) parallel to the line with equations

                                                    x = -1 + 2t
                                                    y = 1+t
                                                    z = -3 + 4t

                                                                      2
Solution. The coefficients of t give a direction vector d =  1  of the given line. Because the

                                                                         4
line we seek is parallel to this line, d also serves as a direction vector for the new line. It passes
through P0, so the parametric equations are

                                                     x = 3 + 2t
                                                     y = -1 + t
                                                     z = 2 + 4t

Example 4.1.10
Determine whether the following lines intersect and, if so, find the point of intersection.

                                     x = 1 - 3t   x = -1 + s
                                     y = 2 + 5t   y = 3 - 4s
                                     z = 1+t      z = 1-s
Vector Geometry

Solution. Suppose P(x, y, z) with vector p lies on both lines. Then

                           1 - 3t   x   -1 + s 

                           2 + 5t  =  y  =  3 - 4s  for some t and s,

                          1+t     z                 1-s

where the first (second) equation is because P lies on the first (second) line. Hence the lines
intersect if and only if the three equations

                                     1 - 3t = -1 + s
                                     2 + 5t = 3 - 4s

                                      1+t = 1-s

have a solution. In this case, t = 1 and s = -1 satisfy all three equations, so the lines do intersect

and the point of intersection is

                                   1 - 3t   -2 

                                  p =  2 + 5t  =  7 

                                               1+t       2

                                                                           -1 + s 
using t = 1. Of course, this point can also be found from p =  3 - 4s  using s = -1.

                                                                               1-s

Example 4.1.11

Show that the line through P0(x0, y0) with slope m has direction vector d = 1 and equation
                                                                                              m

y - y0 = m(x - x0). This equation is called the point-slope formula.

                                  Solution. Let P1(x1, y1) be the point on the line one unit
                                  to the right of P0 (see the diagram). Hence x1 = x0 + 1.
y                                    -
                                  Then d = P0P1 serves as direction vector of the line, and
              P1(x1, y1)          d = x1 - x0 = 1 . But the slope m can be computed
 P0(x0, y0)                       y1 - y0           y1 - y0
                                  as follows:
                                                    m = y1-y0 = y1-y0 = y1 - y0
                                                         x1 -x0  1

O x0 x1 = x0 + 1 x                Hence d = 1 and the parametric equations are x = x0 + t,
                                                  m

y = y0 + mt. Eliminating t gives y - y0 = mt = m(x - x0), as asserted.

 Note that the vertical line through P0(x0, y0) has a direction vector d = 01 that is not of the form
1 for any m. This result confirms that the notion of slope makes no sense in this case. However, the
m
                                                                                             . . Vectors and Lines

vector method gives parametric equations for the line:

                                                           x = x0
                                                           y = y0 + t

Because y is arbitrary here (t is arbitrary), this is usually written simply as x = x0.

Pythagoras' Theorem

     B p c D                           The Pythagorean theorem was known earlier, but Pythagoras (c. 550 B.C.)
     a                                 is credited with giving the first rigorous, logical, deductive proof of the
                                       result. The proof we give depends on a basic property of similar triangles:
                    q                  ratios of corresponding sides are equal.

     C         b        A

        Figure 4.1.14

Theorem 4.1.6: Pythagoras' Theorem
Given a right-angled triangle with hypotenuse c and sides a and b, then a2 + b2 = c2.

Proof. Let A, B, and C be the vertices of the triangle as in Figure 4.1.14. Draw a perpendicular line from

C to the point D on the hypotenuse, and let p and q be the lengths of BD and DA respectively. Then DBC
                                                                  a2
and  CBA  are  similar  triangles   so  p   =  ac .  This  means      =  pc.  In  the  same  way,  the  similarity  of  DCA  and
                                        a
     gives  q     bc ,  whence  b2     qc.  But      then
CBA         b  =                    =

                                            a2 + b2 = pc + qc = (p + q)c = c2

because p + q = c. This proves Pythagoras' theorem10.

   10There is an intuitive geometrical proof of Pythagoras' theorem in Example B.3.
Vector Geometry

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                              Engage Active Learning App!

                        Vretta-Lyryx Engage is an active learning app designed to increase
                       student engagement in reading linear algebra material. The content is
                     "chunked" into small blocks, each with an interactive assessment activity

                                                to promote comprehension.

                      Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

. Projections and Planes

     P               Any student of geometry soon realizes that the notion of perpendicular
         Q           lines is fundamental. As an illustration, suppose a point P and a plane
                     are given and it is desired to find the point Q that lies in the plane and is
Figure 4.2.1         closest to P, as shown in Figure 4.2.1. Clearly, what is required is to find
                     the line through P that is perpendicular to the plane and then to obtain Q
                     as the point of intersection of this line with the plane. Finding the line
                     perpendicular to the plane requires a way to determine when two vectors
                     are perpendicular. This can be done using the idea of the dot product of
                     two vectors.

The Dot Product and Angles

Definition 4.4 Dot Product in R3

                                  
                 x1               x2
Given vectors v =  y1  and w =  y2 , their dot product v · w is a number defined

                 z1               z2

                            v · w = x1x2 + y1y2 + z1z2 = vT w
                                                                          . . Projections and Planes

Because v · w is a number, it is sometimes called the scalar product of v and w.11

Example 4.2.1

 2              1

If v =  -1  and w =  4 , then v · w = 2 · 1 + (-1) · 4 + 3 · (-1) = -5.

3              -1

The next theorem lists several basic properties of the dot product.

Theorem 4.2.1
Let u, v, and w denote vectors in R3 (or R2).

   1. v · w is a real number.
   2. v · w = w · v.
   3. v · 0 = 0 = 0 · v.
   4. v · v = v 2.
   5. (kv) · w = k(w · v) = v · (kw) for all scalars k.
   6. u · (v ± w) = u · v ± u · w

Proof. (1), (2), and (3) are easily verified, and (4) comes from Theorem 4.1.1. The rest are properties of
matrix arithmetic (because w · v = vT w), and are left to the reader.

    The properties in Theorem 4.2.1 enable us to do calculations like

                                 3u · (2v - 3w + 4z) = 6(u · v) - 9(u · w) + 12(u · z)

and such computations will be used without comment below. Here is an example.

Example 4.2.2
Verify that v - 3w 2 = 1 when v = 2, w = 1, and v · w = 2.

Solution. We apply Theorem 4.2.1 several times:

               v - 3w  2 = (v - 3w) · (v - 3w)
                        = v · (v - 3w) - 3w · (v - 3w)
                        = v · v - 3(v · w) - 3(w · v) + 9(w · w)
                        = v 2 - 6(v · w) + 9 w 2
                        = 4 - 12 + 9 = 1

11Similarly, if v = x1 y1 and w = x2 y2 in R2, then v · w = x1x2 + y1y2.
  6 Vector Geometry

    There is an intrinsic description of the dot product of two nonzero vectors in R3. To understand it we
require the following result from trigonometry.

   Theorem: Law of Cosines
   If a triangle has sides a, b, and c, and if  is the interior angle opposite c then

                                                c2 = a2 + b2 - 2ab cos 

                  Proof.  We              prove  it  when  is    acute,    that  is  0      <  ;    the   obtuse  case

                                                                                               2
                  is similar. In Figure 4.2.2 we have p = a sin  and q = a cos  . Hence

a pc              Pythagoras' theorem gives
 q b-q
                          c2 = p2 + (b - q)2 = a2 sin2  + (b - a cos  )2
         b                                      = a2(sin2  + cos2  ) + b2 - 2ab cos 

  Figure 4.2.2    The law of cosines follows because sin2  + cos2  = 1 for any angle  .

                  Note that the law of cosines reduces to Pythagoras' theorem if  is a right

       obtuse     angle  (because         cos        =  0).
v                                                2

              w   Now let v and w be nonzero vectors positioned with a common tail as

     v  acute     in Figure 4.2.3. Then they determine a unique angle  in the range
         
              w                                                  0 

    Figure 4.2.3  This angle  will be called the angle between v and w. Figure 4.2.3 il-

                  lustrates  when           is   acute  (less  than     )  and   obtuse   (greater  than     ).  Clearly
                                                                     2                                    2
                  v and w are parallel if  is either 0 or . Note that we do not define the

                  angle between v and w if one of these vectors is 0.

                      The next result gives an easy way to compute the angle between two
                  nonzero vectors using the dot product.

Theorem 4.2.2
Let v and w be nonzero vectors. If  is the angle between v and w, then

                                              v · w = v w cos 

v         v-w     Proof. We calculate v - w 2 in two ways. First apply the law of cosines
                  to the triangle in Figure 4.2.4 to obtain:

       w                              v - w 2 = v 2 + w 2 - 2 v w cos 

          Figure 4.2.4
On the other hand, we use Theorem 4.2.1:

                  v - w 2 = (v - w) · (v - w)
                                                                                             . . Projections and Planes

                                                   = v·v-v·w-w·v+w·w
                                                   = v 2 - 2(v · w) + w 2

Comparing these we see that -2 v w cos  = -2(v · w), and the result follows.

    If v and w are nonzero vectors, Theorem 4.2.2 gives an intrinsic description of v · w because v , w ,
and the angle  between v and w do not depend on the choice of coordinate system. Moreover, since v
and w are nonzero (v and w are nonzero vectors), it gives a formula for the cosine of the angle  :

                                                    cos  = v·w v w                                                                    (4.1)

Since 0    , this can be used to find  .

Example 4.2.3

                                         -1                       2

Compute the angle between u =  1  and v =  1 .

                                                2                   -1

          y                             Solution. Compute cos  = uu·vv = -2+61-2 6 = - 12 . Now recall
                                        that cos  and sin  are defined so that (cos  , sin  ) is the point on
            
     -1 , 3                             the unit circle determined by the angle  (drawn counterclockwise,
      22
                                        starting from the positive x axis). In the present case, we know
                        2                              -1                                                           1,
                         3              that  cos   =        and    that    0            .   Because     cos     =      it  follows
                                                          2                                                   3     2
      -1 O x

         2

                                        that    =  2   (see  the    diagram).
                                                   3

If v and w are nonzero, equation (4.1) shows that cos  has the same sign as v · w, so

                            v · w > 0 if and only if                is  acute     (0         <    )
                            v · w < 0 if and only if
                            v · w = 0 if and only if                              (               2
                                                                    is  obtuse           <        0)
                                                                                    2
                                                                 =

                                                                         2

In this last case, the (nonzero) vectors are perpendicular. The following terminology is used in linear
algebra:

Definition 4.5 Orthogonal Vectors in R3

Two  vectors  v  and  w  are  said  to  be  orthogonal       if  v  =  0  or   w  =   0  or  the  angle  between    them    is     .
                                                                                                                                2

Since v · w = 0 if either v = 0 or w = 0, we have the following theorem:

   Theorem 4.2.3
   Two vectors v and w are orthogonal if and only if v · w = 0.
8 Vector Geometry

Example 4.2.4
Show that the points P(3, -1, 1), Q(4, 1, 4), and R(6, 0, 4) are the vertices of a right triangle.

Solution. The vectors along the sides of the triangle are

                   1                     3                            2
                   -              -                   -
                            2  ,  PR  =    1  ,                         

                   PQ =                          and QR = -1

                            3              3                         0

   - -                            - -
Evidently PQ · QR = 2 - 2 + 0 = 0, so PQ and QR are orthogonal vectors. This means sides PQ

and QR are perpendicular--that is, the angle at Q is a right angle.

    Example 4.2.5 demonstrates how the dot product can be used to verify geometrical theorems involving
perpendicular lines.

Example 4.2.5

A parallelogram with sides of equal length is called a rhombus. Show that the diagonals of a
rhombus are perpendicular.

      u-v          Solution. Let u and v denote vectors along two adjacent sides
                   of a rhombus, as shown in the diagram. Then the diagonals are
u     u+v          u - v and u + v, and we compute

   v                             (u - v) · (u + v) = u · (u + v) - v · (u + v)
                                                     = u·u+u·v-v·u-v·v
                                                     = u 2- v 2
                                                     =0

because u = v (it is a rhombus). Hence u - v and u + v are orthogonal.

Projections

In applications of vectors, it is frequently useful to write a vector as the sum of two orthogonal vectors.
Here is an example.

   Example 4.2.6
   Suppose a ten-kilogram block is placed on a flat surface inclined 30 to the horizontal as in the
   diagram. Neglecting friction, how much force is required to keep the block from sliding down the
   surface?

   Solution. Let w denote the weight (force due to gravity) exerted on the block. Then w = 10
   kilograms and the direction of w is vertically down as in the diagram.
                                                                                       . . Projections and Planes

                                                          The idea is to write w as a sum w = w1 + w2

                            w1 where w1 is parallel to the inclined surface and w2 is perpendicular to the surface. Since there
                                                          is no friction, the force required is -w1 because the
                w2 30 w
                                30                        force w2 has no effect parallel to the surface. As the

                        1.                  =1            angle between w and w2 is 30 in the diagram, we have
                                                             1 10
w1      =   sin 30   =  2   Hence   w1          2     w   =        =  5.  Thus    the  required  force  has  a  magnitude        of  5
w                                                            2
kilograms weight directed up the surface.

       u        P                       If a nonzero vector d is specified, the key idea in Example 4.2.6 is to
    Q              u - u1           be able to write an arbitrary vector u as a sum of two vectors,
(a)                          d
                                                                         u = u1 + u2
                        P1
                u1                  where u1 is parallel to d and u2 = u - u1 is orthogonal to d. Suppose that
                                    u and d = 0 emanate from a common tail Q (see Figure 4.2.5). Let P be
Pd                                  the tip of u, and let P1 denote the foot of the perpendicular from P to the
        u                           line through Q parallel to d.

u - u1             Q                                  -
              u1                        Then u1 = QP1 has the required properties:
            P1
                                       1. u1 is parallel to d.
(b)
                                       2. u2 = u - u1 is orthogonal to d.
    Figure 4.2.5
                                       3. u = u1 + u2.

Definition 4.6 Projection in R3
                    -

The vector u1 = QP1 in Figure 4.2.5 is called the projection of u on d. It is denoted

                                                   u1 = projd u

In Figure 4.2.5(a) the vector u1 = projd u has the same direction as d; however, u1 and d have opposite
directions  if  the  angle      between     and       is  greater  than      (Figure   4.2.5(b)).  Note that the projection
                                         u         d                      2

u1 = projd u is zero if and only if u and d are orthogonal.

Calculating the projection of u on d = 0 is remarkably easy.

Theorem 4.2.4
Let u and d = 0 be vectors.

1.      The projection of u on d is given by projd u =                    u·d d.

                                                                          d2

2. The vector u - projd u is orthogonal to d.
    Vector Geometry

Proof. The vector u1 = projd u is parallel to d and so has the form u1 = td for some scalar t. The
requirement that u - u1 and d are orthogonal determines t. In fact, it means that (u - u1) · d = 0 by
Theorem 4.2.3. If u1 = td is substituted here, the condition is

                             0 = (u - td) · d = u · d - t(d · d) = u · d - t d 2

It follows that t =  u·d  ,  where  the  assumption    that   d  =  0  guarantees    that  d 2 = 0.
                     d2

Example 4.2.7

                                     2                  1

Find the projection of u =  -3  on d =  -1  and express u = u1 + u2 where u1 is parallel
                                         1                    3

to d and u2 is orthogonal to d.

Solution. The projection u1 of u on d is

                                                                        1  1
                                                              2+3+3    2  -1  = 8  -1 
                          u1 = projd u =       u·d d =    21 +(-1) +32                 11
                                               d                              3               3
                                                   2

                              14 
                             1
Hence  u2  =  u - u1      =         -25     ,  and  this  is  orthogonal  to  d  by  Theorem  4.2.4  (alternatively,
                             11
                                    -13
observe that d · u2 = 0). Since u = u1 + u2, we are done.

Example 4.2.8

             P(1, 3, -2)     Find the shortest distance (see diagram) from the point P(1, 3, -2)
                u - u1                                                                               1

        u u1 d               to the line through P0(2, 0, -1) with direction vector d =  -1 .
                 Q                                                                                        0

     P0(2, 0, -1)  1         Also find the point Q that lies on the line and is closest to P.
Solution. Let u =  3
                               2   -1 
                         -2
the projection of u on d.     -  0  =  3  denote the vector from P0 to P, and let u1 denote
                                         -1               -1
                             Thus

                                                       -1-3+0                     -2 

                             u1 =        u·d d =d   21 +(-1) +02 2 d = -2d =           2

                                             2                                         0

by Theorem 4.2.4. We see geometrically that the point Q on the line is closest to P, so the distance

is                                                                     1

                                         -                                           
                                         QP = u - u1 =  1  = 3
                                                                       -1
                                                                                   . . Projections and Planes

To  find  the  coordinates   of  Q,  let  p0 and   q denote  the  vectors  of  P0  and  Q,  respectively.  Then
                                                    
          2                                     0

p0 =  0  and q = p0 + u1 =  2 . Hence Q(0, 2, -1) is the required point. It can be
         -1 -1 

checked that the distance from Q to P is 3, as expected.

Planes

It is evident geometrically that among all planes that are perpendicular to a given straight line there is
exactly one containing any given point. This fact can be used to give a very simple description of a plane.
To do this, it is necessary to introduce the following notion:

   Definition 4.7 Normal Vector in a Plane
   A nonzero vector n is called a normal for a plane if it is orthogonal to every vector in the plane.

                                 For example, the coordinate vector k is a normal for the x-y plane.

                          P          Given a point P0 = P0(x0, y0, z0) and a nonzero vector n, there is a
     n
P0                               unique plane through P0 with normal n, shaded in Figure 4.2.6. A point
                                                                                                     -
    Figure 4.2.6
                                 P = P(x, y, z) lies on this plane if and only if the vector P0P is orthogonal

                                                                                                           x - x0  

                                 to n--that is,                        -           Because  -              y - y0   this
                                                   if and only if n · P0P = 0.                      

                                                                                            P0P =

                                 gives the following result:                                               z - z0

Theorem: Scalar Equation of a Plane

                                                             a
The plane through P0(x0, y0, z0) with normal n =  b  = 0 as a normal vector is given by

                                                                c

                                     a(x - x0) + b(y - y0) + c(z - z0) = 0

In other words, a point P(x, y, z) is on this plane if and only if x, y, and z satisfy this equation.

Example 4.2.9
                                                                           3

Find an equation of the plane through P0(1, -1, 3) with n =  -1  as normal.
                                                                               2

Solution. Here the general scalar equation becomes

                                        3(x - 1) - (y + 1) + 2(z - 3) = 0
       Vector Geometry

This simplifies to 3x - y + 2z = 10.

                                                                                             a

If we write d = ax0 + by0 + cz0, the scalar equation shows that every plane with normal n =  b  has

                                                                                                    c

a linear equation of the form

                                      ax + by + cz = d                                                 (4.2)

                                                                                                a
for some constant d. Conversely, the graph of this equation is a plane with n =  b  as a normal vector

                                                                                                    c
(assuming that a, b, and c are not all zero).

Example 4.2.10

Find an equation of the plane through P0(3, -1, 2) that is parallel to the plane with equation
2x - 3y = 6.

                                                                             2
Solution. The plane with equation 2x - 3y = 6 has normal n =  -3 . Because the two planes

                                                                                  0
are parallel, n serves as a normal for the plane we seek, so the equation is 2x - 3y = d for some d
by Equation 4.2. Insisting that P0(3, -1, 2) lies on the plane determines d; that is,
d = 2 · 3 - 3(-1) = 9. Hence, the equation is 2x - 3y = 9.

                                                               x
                                                           x0

Consider points P0(x0, y0, z0) and P(x, y, z) with vectors p0 =  y0  and p =  y . Given a nonzero

                                                        z0     z

                                                               a

vector n, the scalar equation of the plane through P0(x0, y0, z0) with normal n =  b  takes the vector

                                                                  c

form:

Theorem: Vector Equation of a Plane
The plane with normal n = 0 through the point with vector p0 is given by

                                                 n · (p - p0) = 0
In other words, the point with vector p is on the plane if and only if p satisfies this condition.

Moreover, Equation 4.2 translates as follows:
                 Every plane with normal n has vector equation n · p = d for some number d.

This is useful in the second solution of Example 4.2.11.
                                                                                          . . Projections and Planes

Example 4.2.11

Find the shortest distance from the point P(2, 1, -3) to the plane with equation 3x - y + 4z = 1.
Also find the point Q on this plane closest to P.

      n                P(2, 1, -3)                                                                   3
    u1                 Q(x, y, z)      Solution 1. The plane in question has normal n =  -1 .

                u                                                                                         4
                                       Choose any point P0 on the plane--say P0(0, -1, 0)--and let
P0(0, -1, 0)                           Q(x, y, z) be the point on the plane closest to P (see the diagram).

                                                                              2
                                       The vector from P0 to P is u =  2 . Now erect n with its

                                                                                -3
                                                          -
                                       tail at P0. Then QP = u1 and u1 is the projection of u on n:

                                                              3  3
                                                 n·u n = -8           = -4 
                                      u1 =                       -1                -1
                                                     2
                                                 n 26 4 13 4                              

                                                                                                          
                                                                                                            x
                              -                      
Hence the distance is                                4 26    To  calculate  the    point  Q,  let                and
                              QP    =     u1     =                                                 q  =     y  
                                                      13 .

                                                                                                            z

       0

p0 =  -1  be the vectors of Q and P0. Then
            0

                                                                                                  38 
                                                  0   2   3   13 
                                                                            4                         9
                       q = p0 + u - u1 =  -1  +                  2   +      13     -1         =          
                                                                                                      13

                                                     0           -3                4 -  23 

                                                                                                      13

This  gives  the   coordinates    of  Q(  38  ,  9,  -23 ).
                                          13
                                                 13   13

                          x                       2

Solution 2. Let q =  y  and p =  1  be the vectors of Q and P. Then Q is on the line

                              z                      -3

through P with direction vector n, so q = p + tn for some scalar t. In addition, Q lies on the plane,

so n · q = 1. This determines t:

                              1 = n · q = n · (p + tn) = n · p + t n 2 = -7 + t(26)

This  gives  t  =  8   =  4,  so
                   26
                          13

                       x                                 2   3   38 
                                                             1  + 4  -1  = 1 
                        y  = q = p+tn =                              13                   13          9

                          z                                 -3                  4                -23

as before. This determines Q (in the diagram), and the reader can verify that the required distance
    - 4

is QP = 13 26, as before.
Vector Geometry

The Cross Product

If P, Q, and R are three distinct points in R3 that are not all on some line, it is clear geometrically that
                                                                      - -

there is a unique plane containing all three. The vectors PQ and PR both lie in this plane, so finding a
                                                                             - -

normal amounts to finding a nonzero vector orthogonal to both PQ and PR. The cross product provides a
systematic way to do this.

Definition 4.8 Cross Product

                              
                 x1                    x2
Given vectors v1 =  y1  and v2 =  y2 , define the cross product v1 × v2 by

                 z1                    z2

                                             y1z2 - z1y2   

                              v1 × v2 =  -(x1z2 - z1x2) 

                                             x1y2 - y1x2

         z           (Because it is a vector, v1 × v2 is often called the vector product.) There
                     is an easy way to remember this definition using the coordinate vectors:
            k
        ij                                   1 0                  0

         Oy                                i =  0  , j =  1  , and k =  0 
x
                                             0             0             1
       Figure 4.2.7
                     They are vectors of length 1 pointing along the positive x, y, and z axes,
                     respectively, as in Figure 4.2.7. The reason for the name is that any vector
                     can be written as

                              x
                               y  = xi + yj + zk

                                 z

With this, the cross product can be described as follows:

Theorem: Determinant Form of the Cross Product

                     
x1                   x2
If v1 =  y1  and v2 =  y2  are two vectors, then

z1                   z2

                              i x1 x2  

    v1 × v2 = det  j y1 y2  = y1 y2 i - x1 x2 j + x1 x2 k
                                             z1 z2         z1 z2  y1 y2
                              k z1 z2

where the determinant is expanded along the first column.
                                                                        . . Projections and Planes

Example 4.2.12

 2                       1

If v =  -1  and w =  3 , then

           4             7

                                  i 2 1  -1 3 2 1 2 1
                v1 × v2 = det  j -1 3  = 4 7 i - 4 7 j + -1 3 k

                                    k 47

                                                    = -19i - 10j + 7k

                                                        -19 
                                                    =  -10 

                                                              7

    Observe that v × w is orthogonal to both v and w in Example 4.2.12. This holds in general as can be
verified directly by computing v · (v × w) and w · (v × w), and is recorded as the first part of the following
theorem. It will follow from a more general result which, together with the second part, will be proved in
Section 4.3 where a more detailed study of the cross product will be undertaken.

   Theorem 4.2.5
   Let v and w be vectors in R3.

       1. v × w is a vector orthogonal to both v and w.
       2. If v and w are nonzero, then v × w = 0 if and only if v and w are parallel.

It is interesting to contrast Theorem 4.2.5(2) with the assertion (in Theorem 4.2.3) that
                                  v · w = 0 if and only if v and w are orthogonal.

Example 4.2.13
Find the equation of the plane through P(1, 3, -2), Q(1, 1, 5), and R(2, -2, 3).

                         -      0           -      1
                         PQ =               PR =
Solution.  The  vectors          -2    and          -5    lie  in  the  plane,  so

                                 7                  5

                                      i 0 1                              25 
                - -
                                       -2   -5  = 25i + 7j + 2k =               7

                PQ × PR = det j

                                       k75                                      2

                                                             - -
is a normal for the plane (being orthogonal to both PQ and PR). Hence the plane has equation

                               25x + 7y + 2z = d for some number d.
6 Vector Geometry
Since P(1, 3, -2) lies in the plane we have 25 · 1 + 7 · 3 + 2(-2) = d. Hence d = 42 and the

                                                                                                        - -
equation is 25x + 7y + 2z = 42. Incidentally, the same equation is obtained (verify) if QP and QR,

    - -
or RP and RQ, are used as the vectors in the plane.

Example 4.2.14
Find the shortest distance between the nonparallel lines

                      x  1 2                                             x 3  1

                       y  =  0  + t  0  and  y  =  1  + s  1 

                         z             -1               1                      z         0            -1

Then find the points A and B on the lines that are closest together.

                                                                         2                      1

Solution. Direction vectors for the two lines are d1 =  0  and d2 =  1 , so
                                                                               1                 -1

                                                                  i 2 1   -1 

                                   n = d1 × d2 = det  j 0 1  =  3 
                                                                     k 1 -1                 2
                      n P2

                 B                            is perpendicular to both lines. Consider the plane shaded in

                      u                       the diagram containing the first line with n as normal. This plane

                 A                            contains P1(1, 0, -1) and is parallel to the second line. Because
                     P1
                                              P2(3, 1, 0) is on the second line, the distance in question is just the
                                              shortest distance between P2(3, 1, 0) and this plane. The vector
                                                                                  
                                                                                      2
                                                                              -
                                              u   from  P1  to  P2   is  u = P1P2  =  1     and  so,  as  in  Example  4.2.11,

                                                                                      1

the distance is the length of the projection of u on n.

                                                                                         
                                       distance =           u·n          |u·n| 3        3 14
                                                            n 2 n = n = 14 = 14

Note that it is necessary that n = d1 × d2 be nonzero for this calculation to be possible. As is
shown later (Theorem 4.3.4), this is guaranteed by the fact that d1 and d2 are not parallel.

The points A and B have coordinates A(1 + 2t, 0, t - 1) and B(3 + s, 1 + s, -s) for some s

                       2 + s - 2t 
           -
                              1+s      . This vector is orthogonal to both d1 and d2, and the conditions

and t, so AB =

-                        1-s-t
                      -
                                                                                                              -5
AB · d1 = 0 and AB · d2 = 0 give equations 5t - s = 5 and t - 3s = 2. The solution is s = 14 and
      13  ,  so  the  points  are      40  ,  0,  -1 ) and B( 37 ,   9,  5 ).  We have   -        3 14    as  before.
t  =  14                           A(  14         14             14                      AB    =
                                                                     14  14                        14 ,
                                                                 . . More on the Cross Product

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                           Engage Active Learning App!

                     Vretta-Lyryx Engage is an active learning app designed to increase
                    student engagement in reading linear algebra material. The content is
                  "chunked" into small blocks, each with an interactive assessment activity

                                             to promote comprehension.

                   Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

. More on the Cross Product

                                                           
                                                    x1           x2
The cross product v × w of two R3-vectors v =  y1  and w =  y2  was defined in Section 4.2 where

                                                    z1           z2
we observed that it can be best remembered using a determinant:

                        i x1 x2  

    v × w = det  j y1 y2  = y1 y2 i - x1 x2 j + x1 x2 k                                         (4.3)
                                                    z1 z2  z1 z2       y1 y2
                        k z1 z2

1 0                     1

Here i =  0 , j =  1 , and k =  0  are the coordinate vectors, and the determinant is expanded

0              0        0

along the first column. We observed (but did not prove) in Theorem 4.2.5 that v × w is orthogonal to both

v and w. This follows easily from the next result.

Theorem 4.3.1

                                                                                 
x0                x1             x2                                    x0 x1 x2
If u =  y0 , v =  y1 , and w =  y2 , then u · (v × w) = det  y0 y1 y2 .

z0                z1             z2                                    z0 z1 z2
8 Vector Geometry

Proof. Recall that u · (v × w) is computed by multiplying corresponding components of u and v × w and
then adding. Using equation (4.3), the result is:

                                                          x0 x1 x2  

u · (v × w) = x0 y1 y2 + y0 - x1 x2 + z0 x1 x2 = det  y0 y1 y2 
                   z1 z2  z1 z2                  y1 y2
                                                          z0 z1 z2

where the last determinant is expanded along column 1.

    The result in Theorem 4.3.1 can be succinctly stated as follows: If u, v, and w are three vectors in R3,
then

                                             u · (v × w) = det u v w

where u v w denotes the matrix with u, v, and w as its columns. Now it is clear that v × w is
orthogonal to both v and w because the determinant of a matrix is zero if two columns are identical.

    Because of (4.3) and Theorem 4.3.1, several of the following properties of the cross product follow
from properties of determinants (they can also be verified directly).

Theorem 4.3.2                                    6. (ku) × v = k(u × v) = u × (kv) for any
Let u, v, and w denote arbitrary vectors in R3.     scalar k.

   1. u × v is a vector.                         7. u × (v + w) = (u × v) + (u × w).
   2. u × v is orthogonal to both u and v.
   3. u × 0 = 0 = 0 × u.                         8. (v + w) × u = (v × u) + (w × u).
   4. u × u = 0.
   5. u × v = -(v × u).

Proof. (1) is clear; (2) follows from Theorem 4.3.1; and (3) and (4) follow because the determinant of a
matrix is zero if one column is zero or if two columns are identical. If two columns are interchanged, the
determinant changes sign, and this proves (5). The proofs of (6), (7), and (8) are left as Exercise ??.
We now come to a fundamental relationship between the dot and cross products.

   Theorem 4.3.3: Lagrange Identity12
   If u and v are any two vectors in R3, then

                                             u × v 2 = u 2 v 2 - (u · v)2

   12Joseph Louis Lagrange (1736-1813) was born in Italy and spent his early years in Turin. At the age of 19 he solved a
famous problem by inventing an entirely new method, known today as the calculus of variations, and went on to become one
of the greatest mathematicians of all time. His work brought a new level of rigour to analysis and his Mécanique Analytique
is a masterpiece in which he introduced methods still in use. In 1766 he was appointed to the Berlin Academy by Frederik the
Great who asserted that the "greatest mathematician in Europe" should be at the court of the "greatest king in Europe." After
the death of Frederick, Lagrange went to Paris at the invitation of Louis XVI. He remained there throughout the revolution and
was made a count by Napoleon.
                                                                 . . More on the Cross Product

                                                                                
                                                                    x1             x2

Proof. Given u and v, introduce a coordinate system and write u =  y1  and v =  y2  in component

                                                                   z1               z2
form. Then all the terms in the identity can be computed in terms of the components. The detailed proof

is left as Exercise ??.

    An expression for the magnitude of the vector u × v can be easily obtained from the Lagrange identity.
If  is the angle between u and v, substituting u · v = u v cos  into the Lagrange identity gives

                         u × v 2 = u 2 v 2 - u 2 v 2 cos2  = u 2 v 2 sin2 

using the fact that 1 - cos2  = sin2  . But sin  is nonnegative on the range 0    , so taking the
positive square root of both sides gives

                            u × v = u v sin 

u  u sin                        This expression for u × v makes no reference to a coordinate
                            system and, moreover, it has a nice geometrical interpretation. The
                            parallelogram determined by the vectors u and v has base length
            v
                             v and altitude u sin  (see Figure 4.3.1). Hence the area of the
   Figure 4.3.1             parallelogram formed by u and v is

                                                     ( u sin  ) v = u × v

This proves the first part of Theorem 4.3.4.

Theorem 4.3.4
If u and v are two nonzero vectors and  is the angle between u and v, then

   1. u × v = u v sin  = the area of the parallelogram determined by u and v.
   2. u and v are parallel if and only if u × v = 0.

Proof of (2). By (1), u × v = 0 if and only if the area of the parallelogram is zero. By Figure 4.3.1 the area
vanishes if and only if u and v have the same or opposite direction--that is, if and only if they are parallel.

Example 4.3.1

                            Find the area of the triangle with vertices P(2, 1, 0), Q(3, -1, 1),

                            and R(1, 0, 1).

P

                                                    -       1           -      2
                                                    RP                  RQ =
                         Q  Solution.  We     have      =    1     and          -1  .   The  area  of

                                                             -1                 0

R                           the triangle is half the area of the parallelogram (see the diagram),
   Vector Geometry

                 1 - -
and so equals 2 RP × RQ . We have

                                                i  1 2   -1 
                             - -                   1 -1  =  -2 

                                                
                             RP × RQ = det j

                                   k -1 0                        -3

                             1 - - 1                        1
so the area of the triangle is 2 RP × RQ = 2 1 + 4 + 9 = 2 14.

u×v                              If three vectors u, v, and w are given, they determine a "squashed"
                             rectangular solid called a parallelepiped (Figure 4.3.2), and it is often
   w                         useful to be able to find the volume of such a solid. The base of the solid
         h                   is the parallelogram determined by u and v, so it has area A = u × v by
                             Theorem 4.3.4. The height of the solid is the length h of the projection of
u           v                w on u × v. Hence

     Figure 4.3.2                                  w·(u×v)          |w·(u×v)| |w·(u×v)|
                                   h = u×v 2 u × v = u×v = A

   Thus the volume of the parallelepiped is hA = |w · (u × v)|. This proves

Theorem 4.3.5

The volume of the parallelepiped determined by three vectors w, u, and v (Figure 4.3.2) is given
by |w · (u × v)|.

Example 4.3.2
Find the volume of the parallelepiped determined by the vectors

                              1   1   -2 

                             w =  2 , u =  1 , v =  0 

                             -1                    0             1

                                                         1 1 -2 
Solution. By Theorem 4.3.1, w · (u × v) = det  2 1 0  = -3. Hence the volume is

                                                            -1 0 1
|w · (u × v)| = | - 3| = 3 by Theorem 4.3.5.

         Oy                      We can now give an intrinsic description of the cross product u × v.
                             Its magnitude u × v = u v sin  is coordinate-free. If u × v = 0, its
   x                         direction is very nearly determined by the fact that it is orthogonal to both
          z                  u and v and so points along the line normal to the plane determined by u
                             and v. It remains only to decide which of the two possible directions is
   Left-hand system          correct.
         z

                          y
            O
                                 . . More on the Cross Product

                                          Before this can be done, the basic issue of how coordinates are as-
                                      signed must be clarified. When coordinate axes are chosen in space, the
                                      procedure is as follows: An origin is selected, two perpendicular lines (the
                                      x and y axes) are chosen through the origin, and a positive direction on
                                      each of these axes is selected quite arbitrarily. Then the line through the
                                      origin normal to this x-y plane is called the z axis, but there is a choice of
                                      which direction on this axis is the positive one. The two possibilities are
                                      shown in Figure 4.3.3, and it is a standard convention that cartesian coor-
                                      dinates are always right-hand coordinate systems. The reason for this
terminology is that, in such a system, if the z axis is grasped in the right hand with the thumb pointing in
the positive z direction, then the fingers curl around from the positive x axis to the positive y axis (through
a right angle).

    Suppose now that u and v are given and that  is the angle between them (so 0    ). Then the
direction of u × v is given by the right-hand rule.

   Theorem: Right-hand Rule

   If the vector u × v is grasped in the right hand and the fingers curl around from u to v through the
   angle  , the thumb points in the direction for u × v.

                      To indicate why this is true, introduce coordinates in R3 as follows: Let

z                     u and v have a common tail O, choose the origin at O, choose the x axis

                      so that u points in the positive x direction, and then choose the y axis

                      so that v is in the x-y plane and the positive y axis is on the same side

                      of the x axis as v. Then, in this system, u and v have component form

          O           a  b
   a u v
 b           c        u =  0  and v =  c  where a > 0 and c > 0. The situation is depicted

x                  y  0  0

                      in Figure 4.3.4. The right-hand rule asserts that u × v should point in the

                      positive z direction. But our definition of u × v gives

Figure 4.3.4                 i a b  0 

                         u × v = det  j 0 c  =  0  = (ac)k

                            k00  ac

                      and (ac)k has the positive z direction because ac > 0.
Vector Geometry

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                          Engage Active Learning App!

                    Vretta-Lyryx Engage is an active learning app designed to increase
                   student engagement in reading linear algebra material. The content is
                 "chunked" into small blocks, each with an interactive assessment activity

                                            to promote comprehension.

                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

 . Linear Operators on R3

Recall that a transformation T : Rn  Rm is called linear if T (x + y) = T (x) + T (y) and T (ax) = aT (x)
holds for all x and y in Rn and all scalars a. In this case we showed (in Theorem 2.6.2) that there exists
an m × n matrix A such that T (x) = Ax for all x in Rn, and we say that T is the matrix transformation
induced by A.

Definition 4.9 Linear Operator on Rn  T : Rn  Rn
A linear transformation

is called a linear operator on Rn.

In Section 2.6 we investigated three important linear operators on R2: rotations about the origin, reflections
in a line through the origin, and projections on this line.

    In this section we investigate the analogous operators on R3: Rotations about a line through the origin,
reflections in a plane through the origin, and projections onto a plane or line through the origin in R3. In
every case we show that the operator is linear, and we find the matrices of all the reflections and projections.

    To do this we must prove that these reflections, projections, and rotations are actually linear operators
on R3. In the case of reflections and rotations, it is convenient to examine a more general situation. A
transformation T : R3  R3 is said to be distance preserving if the distance between T (v) and T (w) is
                                                                                     . . Linear Operators on R3

the same as the distance between v and w for all v and w in R3; that is,

                         T (v) - T (w) = v - w for all v and w in R3                                             (4.4)

Clearly reflections and rotations are distance preserving, and both carry 0 to 0, so the following theorem
shows that they are both linear.

Theorem 4.4.1
If T : R3  R3 is distance preserving, and if T (0) = 0, then T is linear.

Proof. Since T (0) = 0, taking w = 0 in (4.4) shows that T (v) = v for all v in R3, that is T preserves
length. Also, T (v) - T (w) 2 = v - w 2 by (4.4). Since v - w 2 = v 2 - 2v · w + w 2 always holds,
it follows that T (v) · T (w) = v · w for all v and w. Hence (by Theorem 4.2.2) the angle between T (v) and
T (w) is the same as the angle between v and w for all (nonzero) vectors v and w in R3.

            z                         With this we can show that T is linear. Given nonzero
                                  vectors v and w in R3, the vector v + w is the diagonal of
   T (v + w)                      the parallelogram determined by v and w. By the preceding

               T (v)              paragraph, the effect of T is to carry this entire parallelogram
                                  to the parallelogram determined by T (v) and T (w), with di-
   T (w)       w                  agonal T (v + w). But this diagonal is T (v) + T (w) by the
                                  parallelogram law (see Figure 4.4.1).
                               y
                                      In other words, T (v + w) = T (v) + T (w). A similar argu-
   x           v         v+w      ment shows that T (av) = aT (v) for all scalars a, proving that
                                  T is indeed linear.
          Figure 4.4.1

Distance-preserving linear operators are called isometries, and we return to them in Section 10.4.

Reflections and Projections

In Section 2.6 we studied the reflection Qm : R2  R2 in the line y = mx and projection Pm : R2  R2 on
the same line. We found (in Theorems 2.6.5 and 2.6.6) that they are both linear and

          has  matrix      1      1 - m2 2m                                          1 1m2.
      Qm                 1+m2       2m m2 - 1  and Pm has matrix 1+m2
                                                                                     mm

      v                            We now look at the analogues in R3.
                      L
                                   Let L denote a line through the origin in R3. Given a vector v in R3,
               PL(v)           the reflection QL(v) of v in L and the projection PL(v) of v on L are defined
                               in Figure 4.4.2. In the same figure, we see that

0         QL(v)                   PL(v) = v + 12 [QL(v) - v] = 12 [QL(v) + v]                                    (4.5)

   Figure 4.4.2                so the fact that QL is linear (by Theorem 4.4.1) shows that PL is also lin-
                               ear.13

13Note that Theorem 4.4.1 does not apply to PL since it does not preserve distance.
Vector Geometry

                                                                                                 a
    However, Theorem 4.2.4 gives us the matrix of PL directly. In fact, if d =  b  = 0 is a direction

                                                                                                    c

                                     x
vector for L, and we write v =  y , then

                                        z

                                                       a                     a2 ab ac   x 
PL(v) = v·d d 2 d = aa2x+b +b2y+c +c2z  b  = a2 1 +b2+c2  ab b2 bc   y 
                                                          c                     ac bc c2 z

as the reader can verify. Note that this shows directly that PL is a matrix transformation and so gives
another proof that it is linear.

Theorem 4.4.2

                                                                                       a
Let L denote the line through the origin in R3 with direction vector d =  b  = 0. Then PL and

                                                                                      c

QL are both linear and                                                 a2 ab ac 

                                         PL  has  matrix       1        ab  b2  bc 
                                                          a2 +b2 +c2
                                                                        ac bc c2

                                              1       a2 - b2 - c2          2ab            2ac 
                                         a2 +b2 +c2                     b2 - a2 - c2       2bc 
QL             has               matrix                      2ab                      c2 - a2 - b2
                                                                             2bc
                                                             2ac

Proof. It remains to find the matrix of QL. But (4.5) implies that QL(v) = 2PL(v) - v for each v in R3, so
       x

if v =  y  we obtain (with some matrix arithmetic):

           z

                                   a2 ab ac   1 0                               0    x 
QL(v) = a2 2 +b2+c2  ab b2 bc  -  0 1                                           0 y
                                                  ac bc c2              00      1 z

                                               a2 - b2 - c2           2ab             2ac   x 
                                 = a2 1 +b2+c2  2ab               b2 - a2 - c2
                                                                                      2bc   y 
                                                       2ac             2bc      c2 - a2 - b2
                                                                                              z

as required.                           In R3 we can reflect in planes as well as lines. Let M denote a plane
                                   through the origin in R3. Given a vector v in R3, the reflection QM(v) of
                      v            v in M and the projection PM(v) of v on M are defined in Figure 4.4.3. As
                                   above, we have
                              M
          O PM(v)                                    PM(v) = v + 21 [QM(v) - v] = 12 [QM(v) + v]

              QM (v)

Figure 4.4.3
                                                                                . . Linear Operators on R3

                                  so the fact that QM is linear (again by Theorem 4.4.1) shows that PM is
                                  also linear.

Again we can obtain the matrix directly. If n is a normal for the plane M, then Figure 4.4.3 shows that

                               PM(v) = v - projn v = v - v·n n 2 n for all vectors v.

a                     x

If n =  b  = 0 and v =  y , a computation like the above gives

c                              z

                                     1 0 0  x                      a
                 PM(v) =  0 1 0   y  - aa2x+b +b2y+c +c2z  b 
                                    001 z                                  c

                                                b2 + c2    -ab     -ac   x 
                                  = a2 1 +b2+c2  -ab      a2 + c2
                                                                   -bc   y 
                                                    -ac    -bc     b2 + c2
                                                                                   z

This proves the first part of

Theorem 4.4.3

                                                                               a
Let M denote the plane through the origin in R3 with normal n =  b  = 0. Then PM and QM are

                                                                                c

both linear and                                          b2 + c2   -ab         -ac 
                                                                  a2 + c2      -bc 
                      PM       has  matrix       1        -ab                 a2 + b2
                                            a2 +b2 +c2             -bc
                                                          -ac

                                         1       b2 + c2 - a2        -2ab             -2ac 
                                    a2 +b2 +c2                    a2 + c2 - b2        -2bc 
   QM            has  matrix                      -2ab                             a2 + b2 - c2
                                                                     -2bc
                                                  -2ac

Proof. It remains to compute the matrix of QM. Since QM(v) = 2PM(v) - v for each v in R3, the compu-
tation is similar to the above and is left as an exercise for the reader.

Rotations

In Section 2.6 we studied the rotation R : R2  R2 counterclockwise about the origin through the angle
 . Moreover, we showed in Theorem 2.6.4 that R is linear and has matrix cos  - sin  sin  cos  . One
extension of this is given in the following example.

   Example 4.4.1
   Let Rz,  : R3  R3 denote rotation of R3 about the z axis through an angle  from the positive x
   axis toward the positive y axis. Show that Rz,  is linear and find its matrix.
6 Vector Geometry

                             Solution.

     z                       First R is distance preserving and so is linear by Theorem 4.4.1.

    k Rz(j)                  Hence we apply Theorem 2.6.2 to obtain the matrix of Rz,  .
                                        1  0                        0
                                                    

 i j y                       Let i =  0 , j =  1 , and k =  0  denote the standard
x Rz(i)
                                        0  0                        1

                             basis of R3; we must find Rz,  (i), Rz,  (j), and Rz,  (k). Clearly
                             Rz,  (k) = k. The effect of Rz,  on the x-y plane is to rotate
                             it counterclockwise through the angle  . Hence Figure 4.4.4 gives

   Figure 4.4.4                             cos                         - sin  

                                        Rz,  (i) =  sin   , Rz,  (j) =  cos  
                                              0                        0

                             so, by Theorem 2.6.2, Rz,  has matrix

                                            cos  - sin  0 

                   Rz,  (i) Rz,  (j) Rz,  (k) =  sin cos  0 
                                                 0  01

    Example 4.4.1 begs to be generalized. Given a line L through the origin in R3, every rotation about L
through a fixed angle is clearly distance preserving, and so is a linear operator by Theorem 4.4.1. However,
giving a precise description of the matrix of this rotation is not easy and will have to wait until more
techniques are available.

Transformations of Areas and Volumes

                          v  Let v be a nonzero vector in R3. Each vector in the same
               sv            direction as v whose length is a fraction s of the length of v
   Origin                    has the form sv (see Figure 4.4.5).

      Figure 4.4.5               With this, scrutiny of Figure 4.4.6 shows that a vector u is
                             in the parallelogram determined by v and w if and only if it
                             has the form u = sv + tw where 0  s  1 and 0  t  1. But
                             then, if T : R3  R3 is a linear transformation, we have

                                           T (sv + tw) = T (sv) + T (tw) = sT (v) + tT (w)

     v                       Hence T (sv + tw) is in the parallelogram determined by T (v)
   sv                        and T (w). Conversely, every vector in this parallelogram has
                             the form T (sv + tw) where sv + tw is in the parallelogram de-
     sv + tw                 termined by v and w. For this reason, the parallelogram de-
                             termined by T (v) and T (w) is called the image of the paral-
O  tw w                      lelogram determined by v and w. We record this discussion

   Figure 4.4.6              as:
                                                            . . Linear Operators on R3

Theorem 4.4.4

If T : R3  R3 (or R2  R2) is a linear operator, the image of the parallelogram determined by
vectors v and w is the parallelogram determined by T (v) and T (w).

                               T (w) This result is illustrated in Figure 4.4.7, and was used

                                                  in Examples 2.2.15 and 2.2.16 to reveal the effect of ex-

                               T (u)              pansion and shear transformations.

        v                   O                     We now describe the effect of a linear transformation

           u                                      T : R3  R3 on the parallelepiped determined by three

Ow                                    T (v)       vectors u, v, and w in R3 (see the discussion preced-

                                                  ing Theorem 4.3.5). If T has matrix A, Theorem 4.4.4

              Figure 4.4.7                        shows that this parallelepiped is carried to the paral-
                                                  lelepiped determined by T (u) = Au, T (v) = Av, and

                                                            T (w) = Aw. In particular, we want to discover how the
volume changes, and it turns out to be closely related to the determinant of the matrix A.

Theorem 4.4.5
Let vol (u, v, w) denote the volume of the parallelepiped determined by three vectors u, v, and w
in R3, and let area (p, q) denote the area of the parallelogram determined by two vectors p and q
in R2. Then:

   1. If A is a 3 × 3 matrix, then vol (Au, Av, Aw) = | det (A)| · vol (u, v, w).
   2. If A is a 2 × 2 matrix, then area (Ap, Aq) = | det (A)| · area (p, q).

Proof.

1. Let u v w denote the 3 × 3 matrix with columns u, v, and w. Then

                                      vol (Au, Av, Aw) = |Au · (Av × Aw)|

by Theorem 4.3.5. Now apply Theorem 4.3.1 twice to get

                            Au · (Av × Aw) = det  Au Av Aw  = det (A u v w )
                                                            = det (A) det u v w
                                                            = det (A)(u · (v × w))

where we used Definition 2.9 and the product theorem for determinants. Finally (1) follows from
Theorem 4.3.5 by taking absolute values.

        k                                                                    x
             q1                       2. Given p = xy in R2, p1 =  y  in R3.
                        p1
                                                                                 0
                                         By the diagram, area (p, q) = vol (p1, q1, k) where k is the
                                         (length 1) coordinate vector along the z axis. If A is a 2 × 2
8 Vector Geometry            matrix, write A1 = A 0 0 1 in block form, and observe that
    as required.             (Av)1 = (A1v1) for all v in R2 and A1k = k.
                             Hence part (1) of this theorem shows

                   area (Ap, Aq) = vol (A1p1, A1q1, A1k)
                                    = | det (A1)| vol (p1, q1, k)
                                    = | det (A)| area (p, q)

    Define the unit square and unit cube to be the square and cube corresponding to the coordinate
vectors in R2 and R3, respectively. Then Theorem 4.4.5 gives a geometrical meaning to the determinant
of a matrix A:

    · If A is a 2 × 2 matrix, then | det (A)| is the area of the image of the unit square under multiplication
       by A;

    · If A is a 3 × 3 matrix, then | det (A)| is the volume of the image of the unit cube under multiplication
       by A.

These results, together with the importance of areas and volumes in geometry, were among the reasons for
the initial development of determinants.

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!
                                                                       . . An Application to Computer Graphics

 . An Application to Computer Graphics

Computer graphics deals with images displayed on a computer screen, and so arises in a variety of appli-
cations, ranging from word processors, to Star Wars animations, to video games, to wire-frame images of
an airplane. These images consist of a number of points on the screen, together with instructions on how
to fill in areas bounded by lines and curves. Often curves are approximated by a set of short straight-line
segments, so that the curve is specified by a series of points on the screen at the end of these segments.
Matrix transformations are important here because matrix images of straight line segments are again line
segments.14 Note that a colour image requires that three images are sent, one to each of the red, green,
and blue phosphorus dots on the screen, in varying intensities.

                                                5

                            4    3

                         1          2
                         Origin

Figure 4.5.1             Figure 4.5.2     Figure 4.5.3          Figure 4.5.4  Figure 4.5.5

    Consider displaying the letter A. In reality, it is depicted on the screen, as in Figure 4.5.1, by specifying
the coordinates of the 11 corners and filling in the interior. For simplicity, we will disregard the thickness
of the letter, so we require only five coordinates as in Figure 4.5.2.

    This simplified letter can then be stored as a data matrix

                                    Vertex 1 2 3 4 5

                                       D = 0 6 5 1 3 0 0 3 3 9

where the columns are the coordinates of the vertices in order. Then if we want to transform the letter by

a 2 × 2 matrix A, we left-multiply this data matrix by A (the effect is to multiply each column by A and so

transform each vertex).

    For example, we can slant the letter to the right by multiplying by an x-shear matrix A =  1 0.2
--see Section 2.2. The result is the letter with data matrix                                   01

              A = 1 0.2 0 1 0 6 5 1 3 0 0 3 3 9 = 0 6 5.6 1.6 4.8 0 0 3 3 9

which is shown in Figure 4.5.3.                                                                0.8 0
    If we want to make this slanted matrix narrower, we can now apply an x-scale matrix B =    01

that shrinks the x-coordinate by 0.8. The result is the composite transformation

                         BAD = 0.8 0 0 1  1 0.2         06513
                                          01            00339

   14If v0 and v1 are vectors, the vector from v0 to v1 is d = v1 - v0. So a vector v lies on the line segment between v0 and
v1 if and only if v = v0 + td for some number t in the range 0  t  1. Thus the image of this segment is the set of vectors
Av = Av0 + tAd with 0  t  1, that is the image is the segment between Av0 and Av1.
    Vector Geometry

                                    = 0 4.8 4.48 1.28 3.84 0 0 3 3 9

which is drawn in Figure 4.5.4.

On  the  other  hand,  we  can  rotate  the  letter  about  the  origin  through     (or  30)  by  multiplying  by  the
                                                                                  6
                                    
              cos( 6 ) - sin( 6 )
matrix R  =                         =        0.866 -0.5          . This gives

    2           6sin(  ) cos(  ) 6           0.5 0.866

                           R =          0.866 -0.5          065          13
                                        0.5 0.866           003          39
                              2
                                        0 5.196 2.83        -0.634       -1.902
                               =                              3.098        9.294
                                        03           5.098

and is plotted in Figure 4.5.5.

    This poses a problem: How do we rotate at a point other than the origin? It turns out that we can do this
when we have solved another more basic problem. It is clearly important to be able to translate a screen
image by a fixed vector w, that is apply the transformation Tw : R2  R2 given by Tw(v) = v + w for all v
in R2. The problem is that these translations are not matrix transformations R2  R2 because they do not
carry 0 to 0 (unless w = 0). However, there is a clever way around this.

                                                                              x
    The idea is to represent a point v = x as a 3 × 1 column  y , called the homogeneous coordi-

                                                  y1

nates of v. Then translation by w = p can be achieved by multiplying by a 3 × 3 matrix:
                                               q

                            1 0 p  x   x+p                                  Tw(v)
                                                                              1
                            0 1 q  y  =  y+q  =

                                 001 1                           1

Thus, by using homogeneous coordinates we can implement the translation Tw in the top two coordinates.
On the other hand, the matrix transformation induced by A = a b is also given by a 3 × 3 matrix:

                                                                             cd

                            a b 0   x   ax + by  Av
                            c d 0   y  =  cx + dy  = 0 0 1 1 1 1

So everything can be accomplished at the expense of using 3 × 3 matrices and homogeneous coordinates.

Example 4.5.1

Rotate the letter A in Figure 4.5.2 through 6 about the point            4     .

                                                                         5

Solution.
Using homogeneous coordinates for the vertices of the letter results in a data matrix with three
                                                  . . An Application to Computer Graphics

rows:                                    0     6 5 1 3
                                  Kd =  0      0 3 3 9
                                               1111
                                            1
                                                If we write w = 45 ,
       Origin                                   the idea is to use a composite of transformations:
                                                First translate the letter by -w so that
          Figure 4.5.6                          the point w moves to the origin, then rotate this
                                                translated letter, and then translate it by w back
                                                to its original position. The matrix arithmetic is
                                                as follows (remember the order of composition!):

        1 0 4   0.866 -0.5 0   1 0 -4   0 6 5 1 3 

        0 1 5   0.5 0.866 0   0 1 -5   0 0 3 3 9 

       001 0                         01 00 1 11111

        3.036 8.232 5.866 2.402 1.134 

       =  -1.33 1.67 3.768 1.768 7.964 

       1  1                       1  1         1

This is plotted in Figure 4.5.6.

    This discussion merely touches the surface of computer graphics, and the reader is referred to special-
ized books on the subject. Realistic graphic rendering requires an enormous number of matrix calcula-
tions. In fact, matrix multiplication algorithms are now embedded in microchip circuits, and can perform
over 100 million matrix multiplications per second. This is particularly important in the field of three-
dimensional graphics where the homogeneous coordinates have four components and 4 × 4 matrices are
required.
         Vector Geometry

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.
                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.
                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!
Chapter 5

                                                  Vector Space Rn

 . Subspaces and Spanning

In Section 2.2 we introduced the set Rn of all n-tuples (called vectors), and began our investigation of the
matrix transformations Rn  Rm given by matrix multiplication by an m × n matrix. Particular attention
was paid to the euclidean plane R2 where certain simple geometric transformations were seen to be ma-
trix transformations. Then in Section 2.6 we introduced linear transformations, showed that they are all
matrix transformations, and found the matrices of rotations and reflections in R2. We returned to this in
Section 4.4 where we showed that projections, reflections, and rotations of R2 and R3 were all linear, and
where we related areas and volumes to determinants.

    In this chapter we investigate Rn in full generality, and introduce some of the most important concepts
and methods in linear algebra. The n-tuples in Rn will continue to be denoted x, y, and so on, and will be
written as rows or columns depending on the context.

Subspaces of Rn

   Definition 5.1 Subspace of Rn
   A set1U of vectors in Rn is called a subspace of Rn if it satisfies the following properties:

     S1. The zero vector 0  U.

     S2. If x  U and y  U , then x + y  U .

     S3. If x  U, then ax  U for every real number a.

We say that the subset U is closed under addition if S2 holds, and that U is closed under scalar multi-
plication if S3 holds.

Clearly Rn is a subspace of itself, and this chapter is about these subspaces and their properties. The

set U = {0}, consisting of only the zero vector, is also a subspace because 0 + 0 = 0 and a0 = 0 for each a

in R; it is called the zero subspace. Any subspace of Rn other than {0} or Rn is called a proper subspace.
             z We saw in Section 4.2 that every plane M through the origin in R3

   n       has equation ax + by + cz = 0 where a, b, and c are not all zero. Here

           a

         y n =  b  is a normal for the plane and

           c

x     M    M = {v in R3 | n · v = 0}

    1We use the language of sets. Informally, a set X is a collection of objects, called the elements of the set. The fact that x is

an element of X is denoted x  X. Two sets X and Y are called equal (written X = Y ) if they have the same elements. If every
element of X is in the set Y , we say that X is a subset of Y , and write X  Y . Hence X  Y and Y  X both hold if and only if
X =Y.

              223
Vector Space Rn

             x
where v =  y  and n · v denotes the dot product introduced in Section 2.2 (see the diagram).2 Then M

                z
is a subspace of R3. Indeed we show that M satisfies S1, S2, and S3 as follows:

  S1. 0  M because n · 0 = 0;
  S2. If v  M and v1  M , then n · (v + v1) = n · v + n · v1 = 0 + 0 = 0 , so v + v1  M;
  S3. If v  M , then n · (av) = a(n · v) = a(0) = 0 , so av  M.

This proves the first part of

Example 5.1.1                     Planes and lines through the origin in R3 are all subspaces of R3.

               z                  Solution. We dealt with planes above. If L is a line through
                       L          the origin with direction vector d, then L = {td | t  R} (see
                                  the diagram). We leave it as an exercise to verify that L satisfies
                     d            S1, S2, and S3.
                               y

x

Example 5.1.1 shows that lines through the origin in R2 are subspaces; in fact, they are the only proper
subspaces of R2 (Exercise ??). Indeed, we shall see in Example 5.2.14 that lines and planes through the
origin in R3 are the only proper subspaces of R3. Thus the geometry of lines and planes through the origin
is captured by the subspace concept. (Note that every line or plane is just a translation of one of these.)

    Subspaces can also be used to describe important features of an m × n matrix A. The null space of A,
denoted null A, and the image space of A, denoted im A, are defined by

                           null A = {x  Rn | Ax = 0} and im A = {Ax | x  Rn}

In the language of Chapter 2, null A consists of all solutions x in Rn of the homogeneous system Ax = 0,
and im A is the set of all vectors y in Rm such that Ax = y has a solution x. Note that x is in null A if it
satisfies the condition Ax = 0, while im A consists of vectors of the form Ax for some x in Rn. These two
ways to describe subsets occur frequently.

   Example 5.1.2
   If A is an m × n matrix, then:

       1. null A is a subspace of Rn.

       2. im A is a subspace of Rm.

    2We are using set notation here. In general {q | p} means the set of all objects q with property p.
      . . Subspaces and Spanning

Solution.
   1. The zero vector 0  Rn lies in null A because A0 = 0.3 If x and x1 are in null A, then x + x1
       and ax are in null A because they satisfy the required condition:

                      A(x + x1) = Ax + Ax1 = 0 + 0 = 0 and A(ax) = a(Ax) = a0 = 0
       Hence null A satisfies S1, S2, and S3, and so is a subspace of Rn.
   2. The zero vector 0  Rm lies in im A because 0 = A0. Suppose that y and y1 are in im A, say
       y = Ax and y1 = Ax1 where x and x1 are in Rn. Then

                           y + y1 = Ax + Ax1 = A(x + x1) and ay = a(Ax) = A(ax)
       show that y + y1 and ay are both in im A (they have the required form). Hence im A is a
       subspace of Rm.

    There are other important subspaces associated with a matrix A that clarify basic properties of A. If A
is an n × n matrix and  is any number, let

                                             E (A) = {x  Rn | Ax =  x}
A vector x is in E (A) if and only if ( I - A)x = 0, so Example 5.1.2 gives:

   Example 5.1.3
   E (A) = null ( I - A) is a subspace of Rn for each n × n matrix A and number  .

E (A) is called the eigenspace of A corresponding to  . The reason for the name is that, in the terminology
of Section 3.3,  is an eigenvalue of A if E (A) = {0}. In this case the nonzero vectors in E (A) are called
the eigenvectors of A corresponding to  .

    The reader should not get the impression that every subset of Rn is a subspace. For example:

U1 =  x x  0 satisfies S1 and S2, but not S3;
U2 =  y
      x x2 = y2 satisfies S1 and S3, but not S2;
      y

Hence neither U1 nor U2 is a subspace of R2. (However, see Exercise ??.)

    3We are using 0 to represent the zero vector in both Rm and Rn. This abuse of notation is common and causes no confusion
once everybody knows what is going on.
6 Vector Space Rn

Spanning Sets

Let v and w be two nonzero, nonparallel vectors in R3 with their tails at the origin. The plane M through
the origin containing these vectors is described in Section 4.2 by saying that n = v × w is a normal for M,
and that M consists of all vectors p such that n · p = 0.4 While this is a very useful way to look at planes,
there is another approach that is at least as useful in R3 and, more importantly, works for all subspaces of
Rn for any n  1.

                            The idea is as follows: Observe that, by the diagram, a vector p is in
                        M if and only if it has the form

av

               p                                 p = av + bw

   v                    M for certain real numbers a and b (we say that p is a linear combination of
                                v and w). Hence we can describe M as
0
       w bw

                        M = {av + bw | a, b  R}.5

and we say that {v, w} is a spanning set for M. It is this notion of a spanning set that provides a way to
describe all subspaces of Rn.

    As in Section 1.3, given vectors x1, x2, . . . , xk in Rn, a vector of the form

                                  t1x1 + t2x2 + · · · + tkxk where the ti are scalars

is called a linear combination of the xi, and ti is called the coefficient of xi in the linear combination.

Definition 5.2 Linear Combinations and Span in Rn
The set of all such linear combinations is called the span of the xi and is denoted

                        span {x1, x2, . . . , xk} = {t1x1 + t2x2 + · · · + tkxk | ti in R}

If V = span {x1, x2, . . . , xk}, we say that V is spanned by the vectors x1, x2, . . . , xk, and that the
vectors x1, x2, . . . , xk span the space V .

Here are two examples:

                        span {x} = {tx | t  R}

which we write as span {x} = Rx for simplicity.

                        span {x, y} = {rx + sy | r, s  R}

In particular, the above discussion shows that, if v and w are two nonzero, nonparallel vectors in R3, then

                        M = span {v, w}

is the plane in R3 containing v and w. Moreover, if d is any nonzero vector in R3 (or R2), then

                        L = span {v} = {td | t  R} = Rd

is the line with direction vector d. Hence lines and planes can both be described in terms of spanning sets.

    4The vector n = v × w is nonzero because v and w are not parallel.
    5In particular, this implies that any vector p orthogonal to v × w must be a linear combination p = av + bw of v and w for
some a and b. Can you prove this directly?
                                                                                     . . Subspaces and Spanning

   Example 5.1.4
   Let x = (2, -1, 2, 1) and y = (3, 4, -1, 1) in R4. Determine whether p = (0, -11, 8, 1) or
   q = (2, 3, 1, 2) are in U = span {x, y}.
   Solution. The vector p is in U if and only if p = sx + ty for scalars s and t. Equating components
   gives equations

                         2s + 3t = 0, -s + 4t = -11, 2s - t = 8, and s + t = 1
   This linear system has solution s = 3 and t = -2, so p is in U . On the other hand, asking that
   q = sx + ty leads to equations

                           2s + 3t = 2, -s + 4t = 3, 2s - t = 1, and s + t = 2
   and this system has no solution. So q does not lie in U .

   Theorem 5.1.1: Span Theorem
   Let U = span {x1, x2, . . . , xk} in Rn. Then:

       1. U is a subspace of Rn containing each xi.
       2. If W is a subspace of Rn and each xi  W , then U  W .

Proof.
   1. The zero vector 0 is in U because 0 = 0x1 + 0x2 + · · · + 0xk is a linear combination of the xi. If
       x = t1x1 + t2x2 + · · · + tkxk and y = s1x1 + s2x2 + · · · + skxk are in U , then x + y and ax are in U
       because
                               x + y = (t1 + s1)x1 + (t2 + s2)x2 + · · · + (tk + sk)xk, and
                                   ax = (at1)x1 + (at2)x2 + · · · + (atk)xk
       Finally each xi is in U (for example, x2 = 0x1 + 1x2 + · · · + 0xk) so S1, S2, and S3 are satisfied for
       U , proving (1).
   2. Let x = t1x1 + t2x2 + · · · + tkxk where the ti are scalars and each xi  W . Then each tixi  W because
       W satisfies S3. But then x  W because W satisfies S2 (verify). This proves (2).
    Condition (2) in Theorem 5.1.1 can be expressed by saying that span {x1, x2, . . . , xk} is the smallest

subspace of Rn that contains each xi. This is useful for showing that two subspaces U and W are equal,
since this amounts to showing that both U  W and W  U . Here is an example of how it is used.

   Example 5.1.5
   If x and y are in Rn, show that span {x, y} = span {x + y, x - y}.
8 Vector Space Rn

Solution. Since both x + y and x - y are in span {x, y}, Theorem 5.1.1 gives
                                       span {x + y, x - y}  span {x, y}

But x = 12 (x + y) + 12(x - y) and y = 12 (x + y) - 12(x - y) are both in span {x + y, x - y}, so
                                       span {x, y}  span {x + y, x - y}

again by Theorem 5.1.1. Thus span {x, y} = span {x + y, x - y}, as desired.

    It turns out that many important subspaces are best described by giving a spanning set. Here are three
examples, beginning with an important spanning set for Rn itself.

Recall  from  Definition  2.3 the     standard  basis  {e1,  e2,  ...,  en}  of  Rn  as  the  set  of  columns  of  the  n×n
                             
                          x1

                          x2        
                                    
identity matrix. If x =  ..  is any vector in R , then x = x1e1 + x2e2 + · · · + xnen, as the reader cann

                    .

                                xn
verify. This proves:

Example 5.1.6
Rn = span {e1, e2, . . . , en} where e1, e2, . . . , en are the columns of In.

    If A is an m × n matrix A, the next two examples show that it is a routine matter to find spanning sets
for null A and im A.

   Example 5.1.7
   Given an m × n matrix A, let x1, x2, . . . , xk denote the basic solutions to the system Ax = 0 given
   by the gaussian algorithm. Then

                                             null A = span {x1, x2, . . . , xk}

   Solution. If x  null A, then Ax = 0 so Theorem 1.3.2 shows that x is a linear combination of the
   basic solutions; that is, null A  span {x1, x2, . . . , xk}. On the other hand, if x is in
   span {x1, x2, . . . , xk}, then x = t1x1 + t2x2 + · · · + tkxk for scalars ti, so

                           Ax = t1Ax1 + t2Ax2 + · · · + tkAxk = t10 + t20 + · · · + tk0 = 0
   This shows that x  null A, and hence that span {x1, x2, . . . , xk}  null A. Thus we have equality.
                                                                                  . . Subspaces and Spanning

Example 5.1.8
Let c1, c2, . . . , cn denote the columns of the m × n matrix A. Then

                                          im A = span {c1, c2, . . . , cn}

Solution. If {e1, e2, . . ., en} is the standard basis of Rn, observe that

           Ae1 Ae2 · · · Aen = A e1 e2 · · · en = AIn = A = c1 c2 · · · cn .

Hence  ci  =  Aei  is  in  im  A  for  each  i,  so  span {c1,  c2,  ...,  cn}    im  A.
                                                                                  
                                                                                      x1  

                                                                                      x2  
                                                                                          , then Definition 2.5 gives
Conversely, let y be in im A, say y = Ax for some x in R . If x =  ..n                    

                                                                                  .

                                                                                      xn

                       y = Ax = x1c1 + x2c2 + · · · + xncn is in span {c1, c2, . . . , cn}

This shows that im A  span {c1, c2, . . . , cn}, and the result follows.

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!
         Vector Space Rn

 . Independence and Dimension

Some spanning sets are better than others. If U = span {x1, x2, . . . , xk} is a subspace of Rn, then every
vector in U can be written as a linear combination of the xi in at least one way. Our interest here is
in spanning sets where each vector in U has exactly one representation as a linear combination of these
vectors.

Linear Independence

Given x1, x2, . . . , xk in Rn, suppose that two linear combinations are equal:
                                 r1x1 + r2x2 + · · · + rkxk = s1x1 + s2x2 + · · · + skxk

We are looking for a condition on the set {x1, x2, . . . , xk} of vectors that guarantees that this representation
is unique; that is, ri = si for each i. Taking all terms to the left side gives

                                   (r1 - s1)x1 + (r2 - s2)x2 + · · · + (rk - sk)xk = 0
so the required condition is that this equation forces all the coefficients ri - si to be zero.

   Definition 5.3 Linear Independence in Rn
   With this in mind, we call a set {x1, x2, . . . , xk} of vectors linearly independent (or simply
   independent) if it satisfies the following condition:

                             If t1x1 + t2x2 + · · · + tkxk = 0 then t1 = t2 = · · · = tk = 0

We record the result of the above discussion for reference.

   Theorem 5.2.1
   If {x1, x2, . . . , xk} is an independent set of vectors in Rn, then every vector in
   span {x1, x2, . . . , xk} has a unique representation as a linear combination of the xi.

    It is useful to state the definition of independence in different language. Let us say that a linear
combination vanishes if it equals the zero vector, and call a linear combination trivial if every coefficient
is zero. Then the definition of independence can be compactly stated as follows:

       A set of vectors is independent if and only if the only linear combination that vanishes is the
       trivial one.

Hence we have a procedure for checking that a set of vectors is independent:

   Theorem: Independence Test
   To verify that a set {x1, x2, . . . , xk} of vectors in Rn is independent, proceed as follows:

       1. Set a linear combination equal to zero: t1x1 + t2x2 + · · · + tkxk = 0.
                                                                            . . Independence and Dimension

   2. Show that ti = 0 for each i (that is, the linear combination is trivial).
Of course, if some nontrivial linear combination vanishes, the vectors are not independent.

Example 5.2.1
Determine whether {(1, 0, -2, 5), (2, 1, 0, -1), (1, 1, 2, 1)} is independent in R4.
Solution. Suppose a linear combination vanishes:

                      r(1, 0, -2, 5) + s(2, 1, 0, -1) + t(1, 1, 2, 1) = (0, 0, 0, 0)
Equating corresponding entries gives a system of four equations:

                        r + 2s + t = 0, s + t = 0, -2r + 2t = 0, and 5r - s + t = 0
The only solution is the trivial one r = s = t = 0 (verify), so these vectors are independent by the
independence test.

Recall from Definition 2.3 that the standard basis of Rn is the set of columns of the identity matrix In.
Example 5.2.2
Show that the standard basis {e1, e2, . . . , en} of Rn is independent.
Solution. The components of t1e1 + t2e2 + · · · + tnen are t1, t2, . . . , tn (see the discussion preceding
Example 5.1.6) So the linear combination vanishes if and only if each ti = 0. Hence the
independence test applies.

Example 5.2.3
If {x, y} is independent, show that {2x + 3y, x - 5y} is also independent.
Solution. If s(2x + 3y) + t(x - 5y) = 0, collect terms to get (2s + t)x + (3s - 5t)y = 0. Since
{x, y} is independent this combination must be trivial; that is, 2s + t = 0 and 3s - 5t = 0. These
equations have only the trivial solution s = t = 0, as required.

Example 5.2.4
Show that the zero vector in Rn does not belong to any independent set.
Solution. No set {0, x1, x2, . . . , xk} of vectors is independent because we have a vanishing,
nontrivial linear combination 1 · 0 + 0x1 + 0x2 + · · · + 0xk = 0.
         Vector Space Rn

   Example 5.2.5
   Given x in Rn, show that {x} is independent if and only if x = 0.

   Solution. A vanishing linear combination from {x} takes the form tx = 0, t in R. This implies that
   t = 0 because x = 0.

The next example will be needed later.

   Example 5.2.6
   Show that the nonzero rows of a row-echelon matrix R are independent.

   Solution. We illustrate the case with 3 leading 1s; the general case is analogous. Suppose R has the
                0 1    

   form R =  0 0 0 0 1    0 0 0 1    where  indicates a nonspecified number. Let R1, R2, and R3
                   000000

   denote the nonzero rows of R. If t1R1 + t2R2 + t3R3 = 0 we show that t1 = 0, then t2 = 0, and
   finally t3 = 0. The condition t1R1 + t2R2 + t3R3 = 0 becomes

                (0, t1, , , , ) + (0, 0, 0, t2, , ) + (0, 0, 0, 0, t3, ) = (0, 0, 0, 0, 0, 0)

   Equating second entries show that t1 = 0, so the condition becomes t2R2 + t3R3 = 0. Now the same
   argument shows that t2 = 0. Finally, this gives t3R3 = 0 and we obtain t3 = 0.

    A set of vectors in Rn is called linearly dependent (or simply dependent) if it is not linearly indepen-
dent, equivalently if some nontrivial linear combination vanishes.

   Example 5.2.7
   If v and w are nonzero vectors in R3, show that {v, w} is dependent if and only if v and w are
   parallel.

   Solution. If v and w are parallel, then one is a scalar multiple of the other (Theorem 4.1.5), say
   v = aw for some scalar a. Then the nontrivial linear combination v - aw = 0 vanishes, so {v, w}
   is dependent.
   Conversely, if {v, w} is dependent, let sv + tw = 0 be nontrivial, say s = 0. Then v = - ts w so v
   and w are parallel (by Theorem 4.1.5). A similar argument works if t = 0.

    With this we can give a geometric description of what it means for a set {u, v, w} in R3 to be in-
dependent. Note that this requirement means that {v, w} is also independent (av + bw = 0 means that
0u + av + bw = 0), so M = span {v, w} is the plane containing v, w, and 0 (see the discussion preceding
Example 5.1.4). So we assume that {v, w} is independent in the following example.
                                                                                . . Independence and Dimension

Example 5.2.8                          Let u, v, and w be nonzero vectors in R3 where {v, w}
                                       independent. Show that {u, v, w} is independent if and only
                  u                    if u is not in the plane M = span {v, w}. This is illustrated in
                                       the diagrams.
                        v
                         w             Solution. If {u, v, w} is independent, suppose u is in the plane
                          M
                                       M = span {v, w}, say u = av + bw, where a and b are in R. Then
               {u, v, w} independent
                                       1u - av - bw = 0, contradicting the independence of {u, v, w}.
                        v
                         u             On the other hand, suppose that u is not in M; we must show
                         w
                         M             that {u, v, w} is independent. If ru + sv + tw = 0 where r, s,
                                                        R3,
            {u, v, w} not independent  and  t  are  in       then  r  =  0  since    otherwise  u   =    - sr v  +  -t  w  is
                                                                                                                     r
                                       in M. But then sv + tw = 0, so s = t = 0 by our assumption.

                                       This shows that {u, v, w} is independent, as required.

By the inverse theorem, the following conditions are equivalent for an n × n matrix A:

1. A is invertible.
2. If Ax = 0 where x is in Rn, then x = 0.
3. Ax = b has a solution x for every vector b in Rn.

While condition 1 makes no sense if A is not square, conditions 2 and 3 are meaningful for any matrix A

and,  in  fact,  are related to  independence  and  spanning.  Indeed,      if  c1,  c2,  ...,  cn  are  the  columns      of  A,  and
                     
                   x1

                   x2  
                       
if we write x =  .. , then
                  .

                   xn                  Ax = x1c1 + x2c2 + · · · + xncn

by Definition 2.5. Hence the definitions of independence and spanning show, respectively, that condition

2 is equivalent to the independence of {c1, c2, . . . , cn} and condition 3 is equivalent to the requirement
that span {c1, c2, . . . , cn} = Rm. This discussion is summarized in the following theorem:

Theorem 5.2.2
If A is an m × n matrix, let {c1, c2, . . . , cn} denote the columns of A.

   1. {c1, c2, . . . , cn} is independent in Rm if and only if Ax = 0, x in Rn, implies x = 0.
   2. Rm = span {c1, c2, . . . , cn} if and only if Ax = b has a solution x for every vector b in Rm.

    For a square matrix A, Theorem 5.2.2 characterizes the invertibility of A in terms of the spanning and
independence of its columns (see the discussion preceding Theorem 5.2.2). It is important to be able to
discuss these notions for rows. If x1, x2, . . . , xk are 1 × n rows, we define span {x1, x2, . . . , xk} to be
         Vector Space Rn

the set of all linear combinations of the xi (as matrices), and we say that {x1, x2, . . . , xk} is linearly
independent if the only vanishing linear combination is the trivial one (that is, if {xT1 , xT2 , . . . , xTk } is
independent in Rn, as the reader can verify).6

   Theorem 5.2.3
   The following are equivalent for an n × n matrix A:

       1. A is invertible.
       2. The columns of A are linearly independent.
       3. The columns of A span Rn.
       4. The rows of A are linearly independent.
       5. The rows of A span the set of all 1 × n rows.

Proof. Let c1, c2, . . . , cn denote the columns of A.
    (1)  (2). By Theorem 2.4.5, A is invertible if and only if Ax = 0 implies x = 0; this holds if and only

if {c1, c2, . . . , cn} is independent by Theorem 5.2.2.
    (1)  (3). Again by Theorem 2.4.5, A is invertible if and only if Ax = b has a solution for every

column B in Rn; this holds if and only if span {c1, c2, . . . , cn} = Rn by Theorem 5.2.2.
    (1)  (4). The matrix A is invertible if and only if AT is invertible (by Corollary 2.4.1 to Theorem

2.4.4); this in turn holds if and only if AT has independent columns (by (1)  (2)); finally, this last
statement holds if and only if A has independent rows (because the rows of A are the transposes of the
columns of AT ).

    (1)  (5). The proof is similar to (1)  (4).

   Example 5.2.9
   Show that S = {(2, -2, 5), (-3, 1, 1), (2, 7, -4)} is independent in R3.

                                               2 -2 5 
   Solution. Consider the matrix A =  -3 1 1  with the vectors in S as its rows. A routine

                                                    2 7 -4
   computation shows that det A = -117 = 0, so A is invertible. Hence S is independent by
   Theorem 5.2.3. Note that Theorem 5.2.3 also shows that R3 = span S.

    6It is best to view columns and rows as just two different notations for ordered n-tuples. This discussion will become
redundant in Chapter 6 where we define the general notion of a vector space.
                                                                               . . Independence and Dimension

Dimension

It is common geometrical language to say that R3 is 3-dimensional, that planes are 2-dimensional and
that lines are 1-dimensional. The next theorem is a basic tool for clarifying this idea of "dimension". Its
importance is difficult to exaggerate.

   Theorem 5.2.4: Fundamental Theorem
   Let U be a subspace of Rn. If U is spanned by m vectors, and if U contains k linearly independent
   vectors, then k  m.

This proof is given in Theorem 6.3.2 in much greater generality.

   Definition 5.4 Basis of a Subspace of Rn
   If U is a subspace of Rn, a set {x1, x2, . . . , xm} of vectors in U is called a basis of U if it satisfies
   the following two conditions:

       1. {x1, x2, . . . , xm} is linearly independent.
       2. U = span {x1, x2, . . . , xm}.

The most remarkable result about bases7 is:

   Theorem 5.2.5: Invariance Theorem
   If {x1, x2, . . . , xm} and {y1, y2, . . . , yk} are bases of a subspace U of Rn, then m = k.

Proof. We have k  m by the fundamental theorem because {x1, x2, . . . , xm} spans U , and {y1, y2, . . . , yk}
is independent. Similarly, by interchanging x's and y's we get m  k. Hence m = k.

    The invariance theorem guarantees that there is no ambiguity in the following definition:

   Definition 5.5 Dimension of a Subspace of Rn
   If U is a subspace of Rn and {x1, x2, . . . , xm} is any basis of U , the number, m, of vectors in the
   basis is called the dimension of U, denoted

                                                         dim U = m

The importance of the invariance theorem is that the dimension of U can be determined by counting the
number of vectors in any basis.8

    Recall from Definition 2.3 the standard basis of Rn {e1, e2, . . . , en}, that is the set of columns of the
identity matrix. Then Rn = span {e1, e2, . . . , en} by Example 5.1.6, and {e1, e2, . . . , en} is independent
by Example 5.2.2. Hence it is indeed a basis of Rn in the present terminology, and we have

    7The plural of "basis" is "bases".
    8We will show in Theorem 5.2.6 that every subspace of Rn does indeed have a basis.
6 Vector Space Rn

Example 5.2.10
dim (Rn) = n and {e1, e2, . . . , en} is a basis.

    This agrees with our geometric sense that R2 is two-dimensional and R3 is three-dimensional. It also
says that R1 = R is one-dimensional, and {1} is a basis. Returning to subspaces of Rn, we define

                                                        dim {0} = 0

This amounts to saying {0} has a basis containing no vectors. This makes sense because 0 cannot belong
to any independent set (Example 5.2.4).

Example 5.2.11

                      
   r
                      
Let U =  s  r, s in R . Show that U is a subspace of R3, find a basis, and calculate dim U .

   r 

                r                                  1  0

Solution. Clearly,  s  = ru + sv where u =  0  and v =  1 . It follows that

                   r                               1  0

U = span {u, v}, and hence that U is a subspace of R3. Moreover, if ru + sv = 0, then

r 0

 s  =  0  so r = s = 0. Hence {u, v} is independent, and so a basis of U . This means

r           0

dim U = 2.

   Example 5.2.12

   Let B = {x1, x2, . . . , xn} be a basis of Rn. If A is an invertible n × n matrix, then
   D = {Ax1, Ax2, . . . , Axn} is also a basis of Rn.

   Solution. Let x be a vector in Rn. Then A-1x is in Rn so, since B is a basis, we have
   A-1x = t1x1 + t2x2 + · · · + tnxn for ti in R. Left multiplication by A gives
   x = t1(Ax1) + t2(Ax2) + · · · + tn(Axn), and it follows that D spans Rn. To show independence, let
   s1(Ax1) + s2(Ax2) + · · · + sn(Axn) = 0, where the si are in R. Then A(s1x1 + s2x2 + · · · + snxn) = 0
   so left multiplication by A-1 gives s1x1 + s2x2 + · · · + snxn = 0. Now the independence of B shows
   that each si = 0, and so proves the independence of D. Hence D is a basis of Rn.

    While we have found bases in many subspaces of Rn, we have not yet shown that every subspace has
a basis. This is part of the next theorem, the proof of which is deferred to Section 6.4 (Theorem 6.4.1)
where it will be proved in more generality.
                                                                               . . Independence and Dimension

   Theorem 5.2.6
   Let U = {0} be a subspace of Rn. Then:

       1. U has a basis and dim U  n.
       2. Any independent set in U can be enlarged (by adding vectors from any fixed basis of U) to a

           basis of U, if not already so.
       3. Any spanning set for U can be cut down (by deleting vectors) to a basis of U, if not already

           so.

   Example 5.2.13
   Find a basis of R4 containing S = {u, v} where u = (0, 1, 2, 3) and v = (2, -1, 0, 1).
   Solution. By Theorem 5.2.6 we can find such a basis by adding vectors from the standard basis of
   R4 to S. If we try e1 = (1, 0, 0, 0), we find easily that {e1, u, v} is independent.
   Now add another vector from the standard basis, say e2. Again we find that B = {e1, e2, u, v} is
   independent. Since B has 4 = dim R4 vectors, then B must span R4 by Theorem 5.2.7 below (or
   simply verify it directly). Hence B is a basis of R4.

Theorem 5.2.6 has a number of useful consequences. Here is the first.

   Theorem 5.2.7
   Let U be a subspace of Rn where dim U = m and let B = {x1, x2, . . . , xm} be a set of m vectors in
   U. Then B is independent if and only if B spans U.

Proof. Suppose B is independent. If B does not span U then, by Theorem 5.2.6, B can be enlarged to a
basis of U containing more than m vectors. This contradicts the invariance theorem because dim U = m,
so B spans U . Conversely, if B spans U but is not independent, then B can be cut down to a basis of U
containing fewer than m vectors, again a contradiction. So B is independent, as required.

    As we saw in Example 5.2.13, Theorem 5.2.7 is a "labour-saving" result. It asserts that, given a
subspace U of dimension m and a set B of exactly m vectors in U , to prove that B is a basis of U it suffices
to show either that B spans U or that B is independent. It is not necessary to verify both properties.

   Theorem 5.2.8
   Let U  W be subspaces of Rn. Then:

       1. dim U  dim W .
       2. If dim U = dim W , then U = W .

Proof. Write dim W = k, and let B be a basis of U .
8 Vector Space Rn

1. If dim U > k, then B is an independent set in W containing more than k vectors, contradicting the
   fundamental theorem. So dim U  k = dim W .

2. If dim U = k, then B is an independent set in W containing k = dim W vectors, so B spans W by
   Theorem 5.2.7. Hence W = span B = U , proving (2).

It follows from Theorem 5.2.8 that if U is a subspace of Rn, then dim U is one of the integers 0, 1, 2, . . . , n,

and that:          dim U = 0 if and only if U = {0},

                   dim U = n if and only if U = Rn

The other subspaces of Rn are called proper. The following example uses Theorem 5.2.8 to show that the
proper subspaces of R2 are the lines through the origin, while the proper subspaces of R3 are the lines and
planes through the origin.

Example 5.2.14
   1. If U is a subspace of R2 or R3, then dim U = 1 if and only if U is a line through the origin.
   2. If U is a subspace of R3, then dim U = 2 if and only if U is a plane through the origin.

Proof.

   1. Since dim U = 1, let {u} be a basis of U . Then U = span {u} = {tu | t in R}, so U is the line
       through the origin with direction vector u. Conversely each line L with direction vector d = 0 has
       the form L = {td | t in R}. Hence {d} is a basis of U , so U has dimension 1.

   2. If U  R3 has dimension 2, let {v, w} be a basis of U . Then v and w are not parallel (by Exam-
       ple 5.2.7) so n = v × w = 0. Let P = {x in R3 | n · x = 0} denote the plane through the origin with
       normal n. Then P is a subspace of R3 (Example 5.1.1) and both v and w lie in P (they are orthogonal
       to n), so U = span {v, w}  P by Theorem 5.1.1. Hence

                                                           U  P  R3

       Since dim U = 2 and dim (R3) = 3, it follows from Theorem 5.2.8 that dim P = 2 or 3, whence
       P = U or R3. But P = R3 (for example, n is not in P) and so U = P is a plane through the origin.
       Conversely, if U is a plane through the origin, then dim U = 0, 1, 2, or 3 by Theorem 5.2.8. But
       dim U = 0 or 3 because U = {0} and U = R3, and dim U = 1 by (1). So dim U = 2.

Note that this proof shows that if v and w are nonzero, nonparallel vectors in R3, then span {v, w} is the
plane with normal n = v × w. We gave a geometrical verification of this fact in Section 5.1.
                                                                               . . Orthogonality

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

         Engage Active Learning App!

   Vretta-Lyryx Engage is an active learning app designed to increase
  student engagement in reading linear algebra material. The content is
"chunked" into small blocks, each with an interactive assessment activity

                           to promote comprehension.

 Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

 . Orthogonality

Length and orthogonality are basic concepts in geometry and, in R2 and R3, they both can be defined
using the dot product. In this section we extend the dot product to vectors in Rn, and so endow Rn with
euclidean geometry. We then introduce the idea of an orthogonal basis--one of the most useful concepts
in linear algebra, and begin exploring some of its applications.

Dot Product, Length, and Distance

If x = (x1, x2, . . . , xn) and y = (y1, y2, . . . , yn) are two n-tuples in Rn, recall that their dot product was
defined in Section 2.2 as follows:

                                             x · y = x1y1 + x2y2 + · · · + xnyn
Observe that if x and y are written as columns then x · y = xT y is a matrix product (and x · y = xyT if they
are written as rows). Here x · y is a 1 × 1 matrix, which we take to be a number.

Definition 5.6 Length in Rn

As in R3, the length x of the vector is defined by

                                               x2   +  x2  +  ·  ·  ·  +  x2n
                             x = x·x=
                                                1       2

Where ( ) indicates the positive square root.
       Vector Space Rn

A vector x of length 1 is called a unit vector. If x = 0, then                    x      = 0 and it follows easily that      1     x  is  a
                                                                                                                             x
unit vector (see Theorem 5.3.6 below), a fact that we shall use later.

    Example 5.3.1
    If x = (1, -1, -3, 1) and y = (2,1, 1, 0) in R4, then x · y = 2 - 1 - 3 + 0 = -2 and

     x = 1 + 1 + 9 + 1 = 12 = 2 3. Hence 21 3x is a unit vector; similarly 16y is a unit vector.

These definitions agree with those in R2 and R3, and many properties carry over to Rn:

   Theorem 5.3.1
   Let x, y, and z denote vectors in Rn. Then:

       1. x · y = y · x.
       2. x · (y + z) = x · y + x · z.
       3. (ax) · y = a(x · y) = x · (ay) for all scalars a.
       4. x 2 = x · x.
       5. x  0, and x = 0 if and only if x = 0.
       6. ax = |a| x for all scalars a.

Proof. (1), (2), and (3) follow from matrix arithmetic because x · y = xT y; (4) is clear from the definition;
                                                          
and (6) is a routine verification since |a| =                a2. If x = (x1, x2, . . . , xn), then x =       x2  +    x2  +  ·  ·  ·  +  x2n

                                                                                                              1        2

so  x  =  0  if  and  only  if  x2  + x22  + · · · + x2n  =  0.  Since  each  xi  is  a  real  number  this  happens  if  and   only      if

                                 1
xi = 0 for each i; that is, if and only if x = 0. This proves (5).

    Because of Theorem 5.3.1, computations with dot products in Rn are similar to those in R3. In partic-
ular, the dot product

                                       (x1 + x2 + · · · + xm) · (y1 + y2 + · · · + yk)

equals the sum of mk terms, xi · y j, one for each choice of i and j. For example:

                      (3x - 4y) · (7x + 2y) = 21(x · x) + 6(x · y) - 28(y · x) - 8(y · y)
                                               = 21 x 2 - 22(x · y) - 8 y 2

holds for all vectors x and y.

    Example 5.3.2
    Show that x + y 2 = x 2 + 2(x · y) + y 2 for any x and y in Rn.

    Solution. Using Theorem 5.3.1 several times:
                                x + y 2 = (x + y) · (x + y) = x · x + x · y + y · x + y · y
                                                                                                     . . Orthogonality

                                   = x 2 + 2(x · y) + y 2

Example 5.3.3

Suppose that Rn = span {f1, f2, . . . , fk} for some vectors fi. If x · fi = 0 for each i where x is in Rn,
show that x = 0.

Solution. We show x = 0 by showing that x = 0 and using (5) of Theorem 5.3.1. Since the fi
span Rn, write x = t1f1 + t2f2 + · · · + tkfk where the ti are in R. Then

                                   x 2 = x · x = x · (t1f1 + t2f2 + · · · + tkfk)
                                        = t1(x · f1) + t2(x · f2) + · · · + tk(x · fk)
                                        = t1(0) + t2(0) + · · · + tk(0)
                                        =0

We saw in Section 4.2 that if u and v are nonzero vectors in R3, then                u·v      = cos  where  is the angle
                                                                                    uv
between u and v. Since | cos  |  1 for any angle  , this shows that |u · v|  u v . In this form the result

holds in Rn.

Theorem 5.3.2: Cauchy Inequality9

If x and y are vectors in Rn, then

                                            |x · y|  x y

Moreover |x · y| = x y if and only if one of x and y is a multiple of the other.

Proof. The inequality holds if x = 0 or y = 0 (in fact it is equality). Otherwise, write x = a > 0 and
 y = b > 0 for convenience. A computation like that preceding Example 5.3.2 gives

                       bx - ay 2 = 2ab(ab - x · y) and bx + ay 2 = 2ab(ab + x · y)                                          (5.1)

It follows that ab-x·y  0 and ab+x·y  0, and hence that -ab  x·y  ab. Hence |x·y|  ab = x y ,
proving the Cauchy inequality.

    If equality holds, then |x · y| = ab, so x · y = ab or x · y = -ab. Hence Equation 5.1 shows that
bx - ay = 0 or bx + ay = 0, so one of x and y is a multiple of the other (even if a = 0 or b = 0).

The Cauchy inequality is equivalent to (x · y)2  x 2 y 2. In R5 this becomes

(x1y1         +  x2y2  +  x3y3  +  x4y4  +  x5y5)2    (x21  +  x2  +  x2  +  x2  +  x25)(y21  +  y2  +  y2  +  y2  +  y25)

                                                                2      3      4                   2      3      4

for all xi and yi in R.

    9Augustin Louis Cauchy (1789-1857) was born in Paris and became a professor at the École Polytechnique at the age of
26. He was one of the great mathematicians, producing more than 700 papers, and is best remembered for his work in analysis
in which he established new standards of rigour and founded the theory of functions of a complex variable. He was a devout
Catholic with a long-term interest in charitable work, and he was a royalist, following King Charles X into exile in Prague after
he was deposed in 1830. Theorem 5.3.2 first appeared in his 1812 memoir on determinants.
   Vector Space Rn

    There is an important consequence of the Cauchy inequality. Given x and y in Rn, use Example 5.3.2
and the fact that x · y  x y to compute

                   x + y 2 = x 2 + 2(x · y) + y 2  x 2 + 2 x y + y 2 = ( x + y )2
Taking positive square roots gives:

   Corollary 5.3.1: Triangle Inequality
   If x and y are vectors in Rn, then x + y  x + y .

v       w                 The reason for the name comes from the observation that in R3 the
                      inequality asserts that the sum of the lengths of two sides of a triangle is
                      not less than the length of the third side. This is illustrated in the diagram.

   v+w

Definition 5.7 Distance in Rn
If x and y are two vectors in Rn, we define the distance d(x, y) between x and y by

                                                d(x, y) = x - y

                 v-w      The motivation again comes from R3 as is clear in the diagram. This
w                     distance function has all the intuitive properties of distance in R3, includ-
                      ing another version of the triangle inequality.
          v

Theorem 5.3.3                                         Triangle inequality.
If x, y, and z are three vectors in Rn we have:

   1. d(x, y)  0 for all x and y.
   2. d(x, y) = 0 if and only if x = y.
   3. d(x, y) = d(y, x) for all x and y .
   4. d(x, z)  d(x, y) + d(y, z)for all x, y, and z.

Proof. (1) and (2) restate part (5) of Theorem 5.3.1 because d(x, y) = x - y , and (3) follows because
 u = - u for every vector u in Rn. To prove (4) use the Corollary to Theorem 5.3.2:

                         d(x, z) = x - z = (x - y) + (y - z)
                                                 (x - y) + (y - z) = d(x, y) + d(y, z)
                                                                                                  . . Orthogonality

Orthogonal Sets and the Expansion Theorem

   Definition 5.8 Orthogonal and Orthonormal Sets
   We say that two vectors x and y in Rn are orthogonal if x · y = 0, extending the terminology in R3
   (See Theorem 4.2.3). More generally, a set {x1, x2, . . . , xk} of vectors in Rn is called an
   orthogonal set if

                                  xi · x j = 0 for all i = j and xi = 0 for all i10
   Note that {x} is an orthogonal set if x = 0. A set {x1, x2, . . . , xk} of vectors in Rn is called
   orthonormal if it is orthogonal and, in addition, each xi is a unit vector:

                                                    xi = 1 for each i.

Example 5.3.4
The standard basis {e1, e2, . . . , en} is an orthonormal set in Rn.

The routine verification is left to the reader, as is the proof of:

Example 5.3.5
If {x1, x2, . . . , xk} is orthogonal, so also is {a1x1, a2x2, . . . , akxk} for any nonzero scalars ai.

If x = 0, it follows from item (6) of Theorem 5.3.1 that  1  x is a unit vector, that is it has length 1.
                                                          x

Definition 5.9 Normalizing an Orthogonal Set
Hence if {x1, x2, . . . , xk} is an orthogonal set, then { x11 x1, x12 x2, · · · , x1k xk} is an
orthonormal set, and we say that it is the result of normalizing the orthogonal set {x1, x2, · · · , xk}.

Example 5.3.6

 1   1   -1           -1 

If f1 =  1   1 , f2 =  1   0 , f3 =  1   0 , and f4 =  -1   3  then {f1, f2, f3, f4} is an orthogonal

-1             2  0                                          1

set in R4 as is easily verified. After normalizing, the corresponding orthonormal set is
{ 12 f1, 16 f2, 12 f3, 21 3 f4}

   10The reason for insisting that orthogonal sets consist of nonzero vectors is that we will be primarily concerned with orthog-
onal bases.
Vector Space Rn

v+w     w            The most important result about orthogonality is Pythagoras' theorem.
                 Given orthogonal vectors v and w in R3, it asserts that
     v
                                                 v+w 2 = v 2+ w 2
                 as in the diagram. In this form the result holds for any orthogonal set in Rn.

Theorem 5.3.4: Pythagoras' Theorem
If {x1, x2, . . . , xk} is an orthogonal set in Rn, then

                             x1 + x2 + · · · + xk 2 = x1 2 + x2 2 + · · · + xk 2.

Proof. The fact that xi · x j = 0 whenever i = j gives

        x1 + x2 + · · · + xk  2 = (x1 + x2 + · · · + xk) · (x1 + x2 + · · · + xk)

                               = (x1 · x1 + x2 · x2 + · · · + xk · xk) +  xi · x j

                                                                                         i= j

                               = x1 2 + x2 2 + · · · + xk 2 + 0

This is what we wanted.

    If v and w are orthogonal, nonzero vectors in R3, then they are certainly not parallel, and so are linearly
independent by Example 5.2.7. The next theorem gives a far-reaching extension of this observation.

Theorem 5.3.5
Every orthogonal set in Rn is linearly independent.

Proof. Let {x1, x2, . . . , xk} be an orthogonal set in Rn and suppose a linear combination vanishes, say:
t1x1 + t2x2 + · · · + tkxk = 0. Then

                                0 = x1 · 0 = x1 · (t1x1 + t2x2 + · · · + tkxk)
                                            = t1(x1 · x1) + t2(x1 · x2) + · · · + tk(x1 · xk)
                                            = t1 x1 2 + t2(0) + · · · + tk(0)
                                            = t1 x1 2

Since x1 2 = 0, this implies that t1 = 0. Similarly ti = 0 for each i.

    Theorem 5.3.5 suggests considering orthogonal bases for Rn, that is orthogonal sets that span Rn.
These turn out to be the best bases in the sense that, when expanding a vector as a linear combination of
the basis vectors, there are explicit formulas for the coefficients.
                                                                              . . Orthogonality

Theorem 5.3.6: Expansion Theorem
Let {f1, f2, . . . , fm} be an orthogonal basis of a subspace U of Rn. If x is any vector in U , we have

              x = x·f1 f 2 f1 + x·f2 f 2 f2 + · · · + x·fm f 2 fm12  m

Proof. Since {f1, f2, . . . , fm} spans U , we have x = t1f1 +t2f2 + · · · +tmfm where the ti are scalars. To find
t1 we take the dot product of both sides with f1:

                                    x · f1 = (t1f1 + t2f2 + · · · + tmfm) · f1
                                          = t1(f1 · f1) + t2(f2 · f1) + · · · + tm(fm · f1)
                                          = t1 f1 2 + t2(0) + · · · + tm(0)
                                          = t1 f1 2

Since f1 = 0, this gives t1 = x·f1 f1 2 . Similarly, ti = x·fi fi 2 for each i.
The expansion in Theorem 5.3.6 of x as a linear combination of the orthogonal basis {f1, f2, . . . , fm} is
called the Fourier expansion of x, and the coefficients t1 = xf·fi i 2 are called the Fourier coefficients. Note
that if {f1, f2, . . . , fm} is actually orthonormal, then ti = x · fi for each i. We will have a great deal more to
say about this in Section 10.5.

Example 5.3.7

Expand x = (a, b, c, d) as a linear combination of the orthogonal basis {f1, f2, f3, f4} of R4 given
in Example 5.3.6.

Solution. We have f1 = (1, 1, 1, -1), f2 = (1, 0, 1, 2), f3 = (-1, 0, 1, 0), and
f4 = (-1, 3, -1, 1) so the Fourier coefficients are

t1 =  x·f1    =  41(a + b + c - d)     t3 =  x·f3    =            1 (-a + c)2

           2                                      2
      f1                                     f3

t2 =  x·f2    =  61(a + c + 2d)        t4 = x·f4 f4 2 = 112 (-a + 3b - c + d)

           2
      f2

The reader can verify that indeed x = t1f1 + t2f2 + t3f3 + t4f4.

    A natural question arises here: Does every subspace U of Rn have an orthogonal basis? The answer is
"yes"; in fact, there is a systematic procedure, called the Gram-Schmidt algorithm, for turning any basis
of U into an orthogonal one. This leads to a definition of the projection onto a subspace U that generalizes
the projection along a vector used in R2 and R3. All this is discussed in Section 8.1.
  6 Vector Space Rn

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.
                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.
                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

 . Rank of a Matrix

In this section we use the concept of dimension to clarify the definition of the rank of a matrix given in
Section 1.2, and to study its properties. This requires that we deal with rows and columns in the same way.
While it has been our custom to write the n-tuples in Rn as columns, in this section we will frequently
write them as rows. Subspaces, independence, spanning, and dimension are defined for rows using matrix
operations, just as for columns. If A is an m × n matrix, we define:

   Definition 5.10 Column and Row Space of a Matrix
   The column space, col A, of A is the subspace of Rm spanned by the columns of A.
   The row space, row A, of A is the subspace of Rn spanned by the rows of A.

Much of what we do in this section involves these subspaces. We begin with:

   Lemma 5.4.1
   Let A and B denote m × n matrices.

       1. If A  B by elementary row operations, then row A = row B.
       2. If A  B by elementary column operations, then col A = col B.
                                                                                               . . Rank of a Matrix

Proof. We prove (1); the proof of (2) is analogous. It is enough to do it in the case when A  B by a single
row operation. Let R1, R2, . . . , Rm denote the rows of A. The row operation A  B either interchanges
two rows, multiplies a row by a nonzero constant, or adds a multiple of a row to a different row. We leave
the first two cases to the reader. In the last case, suppose that a times row p is added to row q where p < q.
Then the rows of B are R1, . . . , Rp, . . . , Rq + aRp, . . . , Rm, and Theorem 5.1.1 shows that

              span {R1, . . . , Rp, . . . , Rq, . . . , Rm} = span {R1, . . . , Rp, . . . , Rq + aRp, . . . , Rm}

That is, row A = row B.
    If A is any matrix, we can carry A  R by elementary row operations where R is a row-echelon matrix.

Hence row A = row R by Lemma 5.4.1; so the first part of the following result is of interest.

   Lemma 5.4.2
   If R is a row-echelon matrix, then

       1. The nonzero rows of R are a basis of row R.

       2. The columns of R containing leading ones are a basis of col R.

Proof. The rows of R are independent by Example 5.2.6, and they span row R by definition. This proves
(1).

    Let c j1, c j2, . . . , c jr denote the columns of R containing leading 1s. Then {c j1, c j2, . . . , c jr} is
independent because the leading 1s are in different rows (and have zeros below and to the left of them).
Let U denote the subspace of all columns in Rm in which the last m -r entries are zero. Then dim U = r (it
is just Rr with extra zeros). Hence the independent set {c j1, c j2, . . . , c jr} is a basis of U by Theorem 5.2.7.
Since each c ji is in col R, it follows that col R = U , proving (2).

    With Lemma 5.4.2 we can fill a gap in the definition of the rank of a matrix given in Chapter 1. Let A
be any matrix and suppose A is carried to some row-echelon matrix R by row operations. Note that R is
not unique. In Section 1.2 we defined the rank of A, denoted rank A, to be the number of leading 1s in R,
that is the number of nonzero rows of R. The fact that this number does not depend on the choice of R was
not proved in Section 1.2. However part 1 of Lemma 5.4.2 shows that

                                                  rank A = dim ( row A)

and hence that rank A is independent of R.
    Lemma 5.4.2 can be used to find bases of subspaces of Rn (written as rows). Here is an example.

   Example 5.4.1
   Find a basis of U = span {(1, 1, 2, 3), (2, 4, 1, 0), (1, 5, -4, -9)}.

                                          1 1 2 3
   Solution. U is the row space of  2 4 1 0 . This matrix has row-echelon form

                                              1 5 -4 -9
8 Vector Space Rn

1 1 2 3                                            -3,
      -3
0  1        -3  ,  so  {(1,  1,  2,  3),  (0,  1,     2  -3)}  is  basis  of U  by  Lemma  5.4.2.
         2
00 0 0

Note that {(1, 1, 2, 3), (0, 2, -3, -6)} is another basis that avoids fractions.

    Lemmas 5.4.1 and 5.4.2 are enough to prove the following fundamental theorem.

   Theorem 5.4.1: Rank Theorem
   Let A denote any m × n matrix of rank r. Then

                                            dim ( col A) = dim ( row A) = r

   Moreover, if A is carried to a row-echelon matrix R by row operations, then

       1. The r nonzero rows of R are a basis of row A.
       2. If the leading 1s lie in columns j1, j2, . . . , jr of R, then columns j1, j2, . . . , jr of A are a

           basis of col A.

Proof. We have row A = row R by Lemma 5.4.1, so (1) follows from Lemma 5.4.2. Moreover, R = UA
for some invertible matrix U by Theorem 2.5.1. Now write A = c1 c2 . . . cn where c1, c2, . . . , cn
are the columns of A. Then

                          R = UA = U c1 c2 · · · cn = U c1 U c2 · · · U cn
Thus, in the notation of (2), the set B = {U c j1, U c j2, . . . , U c jr} is a basis of col R by Lemma 5.4.2. So, to
prove (2) and the fact that dim ( col A) = r, it is enough to show that D = {c j1, c j2, . . . , c jr } is a basis of
col A. First, D is linearly independent because U is invertible (verify), so we show that, for each j, column
c j is a linear combination of the c ji. But U c j is column j of R, and so is a linear combination of the U c ji,
say U c j = a1U c j1 + a2U c j2 + · · · + arU c jr where each ai is a real number.

    Since U is invertible, it follows that c j = a1c j1 + a2c j2 + · · · + arc jr and the proof is complete.

Example 5.4.2

                                1 2 2 -1 
Compute the rank of A =  3 6 5 0  and find bases for row A and col A.

                                  121 2

Solution. The reduction of A to row-echelon form is as follows:

             1 2 2 -1   1 2 2 -1   1 2 2 -1 

             3 6 5 0    0 0 -1 3    0 0 -1 3 

                121 2                     0 0 -1 3                        00 0 0

Hence rank A = 2, and { 1 2 2 -1 , 0 0 1 -3 } is a basis of row A by Lemma 5.4.2.
Since the leading 1s are in columns 1 and 3 of the row-echelon matrix, Theorem 5.4.1 shows that
                                                      . . Rank of a Matrix

                                        1   2  
columns 1 and 3 of A are a basis  3  ,  5  of col A.

1  1

    Theorem 5.4.1 has several important consequences. The first, Corollary 5.4.1 below, follows because
the rows of A are independent (respectively span row A) if and only if their transposes are independent
(respectively span col A).

   Corollary 5.4.1
   If A is any matrix, then rank A = rank (AT ).

    If A is an m × n matrix, we have col A  Rm and row A  Rn. Hence Theorem 5.2.8 shows that
dim ( col A)  dim (Rm) = m and dim ( row A)  dim (Rn) = n. Thus Theorem 5.4.1 gives:

   Corollary 5.4.2
   If A is an m × n matrix, then rank A  m and rank A  n.

    Corollary 5.4.3
    rank A = rank (UA) = rank (AV ) whenever U and V are invertible.

Proof. Lemma 5.4.1 gives rank A = rank (UA). Using this and Corollary 5.4.1 we get
                         rank (AV ) = rank (AV )T = rank (V T AT ) = rank (AT ) = rank A

    The next corollary requires a preliminary lemma.

    Lemma 5.4.3
    Let A, U, and V be matrices of sizes m × n, p × m, and n × q respectively.

       1. col (AV )  col A, with equality if VV  = In for some V .
       2. row (UA)  row A, with equality if U U = Im for some U .

Proof. For (1), write V = v1, v2, . . . , vq where v j is column j of V . Then we have
AV = Av1, Av2, . . . , Avq , and each Av j is in col A by Definition 2.4. It follows that col (AV )  col A.
If VV  = In, we obtain col A = col [(AV )V ]  col (AV ) in the same way. This proves (1).

    As to (2), we have col (UA)T = col (ATU T )  col (AT ) by (1), from which row (UA)  row A. If
U U = Im, this is equality as in the proof of (1).
         Vector Space Rn

    Corollary 5.4.4
   If A is m × n and B is n × m, then rank AB  rank A and rank AB  rank B.

Proof. By Lemma 5.4.3, col (AB)  col A and row (BA)  row A, so Theorem 5.4.1 applies.
    In Section 5.1 we discussed two other subspaces associated with an m × n matrix A: the null space

null (A) and the image space im (A)
                            null (A) = {x in Rn | Ax = 0} and im (A) = {Ax | x in Rn}

Before we proceed to an important theorem, we first define what is meant by the nullity of a matrix.

    Definition 5.11 Nullity
   The dimension of the null space of a matrix is called the nullity, denoted by dim [ null (A)].

    We will see shortly that the rank and the nullity of an m × n matrix A add up to n (no matter m!), this
is part (1) of the following theorem.

    Recall that im (A) = col (A) by Example 5.1.8. So if A has rank r, we have dim [ im (A)] = dim [ col (A)] =
r. Hence Theorem 5.4.1 provides a method of finding a basis of im (A) and this is recorded as part (3) of
the following theorem.

    Theorem 5.4.2: Rank and Nullity
   Let A denote an m × n matrix of rank r. Then

       1. rank (A) + dim [ null (A)] = n.
       2. The n - r basic solutions to the system Ax = 0 provided by the gaussian algorithm are a

           basis of null (A), so dim [ null (A)] = n - r.
       3. Theorem 5.4.1 provides a basis of im (A) = col (A), and dim [ im (A)] = r.

Proof. Part (1) follows from part (2), which only remains to be proved. We already know (Theorem 2.2.1)
that null (A) is spanned by the n - r basic solutions of Ax = 0. Hence using Theorem 5.2.7, it suffices
to show that dim [ null (A)] = n - r. So let {x1, . . . , xk} be a basis of null (A), and extend it to a basis
{x1, . . . , xk, xk+1, . . . , xn} of Rn (by Theorem 5.2.6). It is enough to show that {Axk+1, . . . , Axn} is a
basis of im (A); then n - k = r by the above and so k = n - r as required.

    Spanning. Choose Ax in im (A), x in Rn, and write x = a1x1 + · · ·+ akxk + ak+1xk+1 + · · ·+ anxn where
the ai are in R. Then Ax = ak+1Axk+1 + · · · + anAxn because {x1, . . . , xk}  null (A).

    Independence. Let tk+1Axk+1 + · · · + tnAxn = 0, ti in R. Then tk+1xk+1 + · · · + tnxn is in null A, so
tk+1xk+1 + · · · + tnxn = t1x1 + · · · + tkxk for some t1, . . . , tk in R. But then the independence of the xi
shows that ti = 0 for every i.

    We can now see this result in practice.
                                                            . . Rank of a Matrix

Example 5.4.3

         1 -2 1 1 
If A =  -1 2 0 1 , find bases of null (A) and im (A), and so find their dimensions.

             2 -4 1 0

Solution. If x is in null (A), then Ax = 0, so x is given by solving the system Ax = 0. The
reduction of the augmented matrix to reduced form is

                     1 -2 1 1 0   1 -2 0 -1 0 

                     -1 2 0 1 0    0 0 1 2 0 

                    2 -4 1 0 0  0 00 00

                                                                       1   1  
Hence r = rank (A) = 2. Here, im (A) = col (A) has basis  -1  ,  0  by Theorem 5.4.1

                                                         2  1

because the leading 1s are in columns 1 and 3. In particular, dim [ im (A)] = 2 = r as in

Theorem 5.4.2.

Turning to null (A), we use gaussian elimination. The leading variables are x1 and x3, so the

nonleading variables become parameters: x2 = s and x4 = t. It follows from the reduced matrix
that x1 = 2s + t and x3 = -2t, so the general solution is

 x1   2s + t                                             2   1

x =  x  3   x2  =  -2t   s  = sx1 + tx2 where x1 =  0   1  , and x2 =  -2   0  .
                                                         0     1
                x4  t

Hence null (A). But x1 and x2 are solutions (basic), so

                       null (A) = span {x1, x2}

However Theorem 5.4.2 asserts that {x1, x2} is a basis of null (A). (In fact it is easy to verify
directly that {x1, x2} is independent in this case.) In particular, dim [ null (A)] = 2 = n - r, as
Theorem 5.4.2 asserts.

    Let A be an m×n matrix. Corollary 5.4.2 of Theorem 5.4.1 asserts that rank A  m and rank A  n, and
it is natural to ask when these extreme cases arise. If c1, c2, . . . , cn are the columns of A, Theorem 5.2.2
shows that {c1, c2, . . . , cn} spans Rm if and only if the system Ax = b is consistent for every b in Rm, and
that {c1, c2, . . . , cn} is independent if and only if Ax = 0, x in Rn, implies x = 0. The next two useful
theorems improve on both these results, and relate them to when the rank of A is n or m.

   Theorem 5.4.3
   The following are equivalent for an m × n matrix A:

       1. rank A = n.

       2. The rows of A span Rn.
Vector Space Rn

3. The columns of A are linearly independent in Rm.
4. The n × n matrix AT A is invertible.
5. CA = In for some n × m matrix C.
6. If Ax = 0, x in Rn, then x = 0.

Proof. (1)  (2). We have row A  Rn, and dim ( row A) = n by (1), so row A = Rn by Theorem 5.2.8.
This is (2).

    (2)  (3). By (2), row A = Rn, so rank A = n. This means dim ( col A) = n. Since the n columns of
A span col A, they are independent by Theorem 5.2.7.

    (3)  (4). If (AT A)x = 0, x in Rn, we show that x = 0 (Theorem 2.4.5). We have

                                        Ax 2 = (Ax)T Ax = xT AT Ax = xT 0 = 0

Hence Ax = 0, so x = 0 by (3) and Theorem 5.2.2.                                Hence  dim ( col A) = n,
    (4)  (5). Given (4), take C = (AT A)-1AT .

    (5)  (6). If Ax = 0, then left multiplication by C (from (5)) gives x = 0.

    (6)  (1). Given (6), the columns of A are independent by Theorem 5.2.2.
and (1) follows.

Theorem 5.4.4
The following are equivalent for an m × n matrix A:

   1. rank A = m.
   2. The columns of A span Rm.
   3. The rows of A are linearly independent in Rn.
   4. The m × m matrix AAT is invertible.
   5. AC = Im for some n × m matrix C.
   6. The system Ax = b is consistent for every b in Rm.

Proof. (1)  (2). By (1), dim ( col A = m, so col A = Rm by Theorem 5.2.8.

    (2)  (3). By (2), col A = Rm, so rank A = m. This means dim ( row A) = m. Since the m rows of A
span row A, they are independent by Theorem 5.2.7.

    (3)  (4). We have rank A = m by (3), so the n × m matrix AT has rank m. Hence applying Theo-
rem 5.4.3 to AT in place of A shows that (AT )T AT is invertible, proving (4).

    (4)  (5). Given (4), take C = AT (AAT )-1 in (5).

    (5)  (6). Comparing columns in AC = Im gives Ac j = e j for each j, where c j and e j denote column j
of C and Im respectively. Given b in Rm, write b =  jm=1 r je j, r j in R. Then Ax = b holds with x =  jm=1 r jc j
as the reader can verify.
                                                                                          . . Rank of a Matrix
(6)  (1). Given (6), the columns of A span Rm by Theorem 5.2.2. Thus col A = Rm and (1) follows.

Example 5.4.4          x+y+z        is invertible if x, y, and z are not all equal.
Show that 3           x2 + y2 + z2

               x+y+z

                                                                     1 x

Solution. The given matrix has the form AT A where A =  1 y  has independent columns
                                                                        1z

because x, y, and z are not all equal (verify). Hence Theorem 5.4.3 applies.

    Theorem 5.4.3 and Theorem 5.4.4 relate several important properties of an m × n matrix A to the
invertibility of the square, symmetric matrices AT A and AAT . In fact, even if the columns of A are not
independent or do not span Rm, the matrices AT A and AAT are both symmetric and, as such, have real
eigenvalues as we shall see. We return to this in Chapter 7.

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!
Vector Space Rn

 . Similarity and Diagonalization

In Section 3.3 we studied diagonalization of a square matrix A, and found important applications (for
example to linear dynamical systems). We can now utilize the concepts of subspace, basis, and dimension
to clarify the diagonalization process, reveal some new results, and prove some theorems which could not
be demonstrated in Section 3.3.

    Before proceeding, we introduce a notion that simplifies the discussion of diagonalization, and is used
throughout the book.

Similar Matrices

Definition 5.12 Similar Matrices

If A and B are n × n matrices, we say that A and B are similar, and write A  B, if B = P-1AP for
some invertible matrix P.

Note that A  B if and only if B = QAQ-1 where Q is invertible (write P-1 = Q). The language of
similarity is used throughout linear algebra. For example, a matrix A is diagonalizable if and only if it is
similar to a diagonal matrix.

    If A  B, then necessarily B  A. To see why, suppose that B = P-1AP. Then A = PBP-1 = Q-1BQ
where Q = P-1 is invertible. This proves the second of the following properties of similarity (the others
are left as an exercise):

                 1. A  A for all square matrices A.

                 2. If A  B, then B  A.              (5.2)

                 3. If A  B and B  C, then A  C.

These properties are often expressed by saying that the similarity relation  is an equivalence relation on
the set of n × n matrices. Here is an example showing how these properties are used.

Example 5.5.1

If A is similar to B and either A or B is diagonalizable, show that the other is also diagonalizable.

Solution. We have A  B. Suppose that A is diagonalizable, say A  D where D is diagonal. Since
B  A by (2) of (5.2), we have B  A and A  D. Hence B  D by (3) of (5.2), so B is
diagonalizable too. An analogous argument works if we assume instead that B is diagonalizable.

    Similarity is compatible with inverses, transposes, and powers:

              If A  B then A-1  B-1, AT  BT , and Ak  Bk for all integers k  1.

The proofs are routine matrix computations using Theorem 3.3.1. Thus, for example, if A is diagonaliz-
able, so also are AT , A-1 (if it exists), and Ak (for each k  1). Indeed, if A  D where D is a diagonal
matrix, we obtain AT  DT , A-1  D-1, and Ak  Dk, and each of the matrices DT , D-1, and Dk is
diagonal.

    We pause to introduce a simple matrix function that will be referred to later.
                                                        . . Similarity and Diagonalization

Definition 5.13 Trace of a Matrix
The trace tr A of an n × n matrix A is defined to be the sum of the main diagonal elements of A.

In other words:

                 If A = ai j , then tr A = a11 + a22 + · · · + ann.

It is evident that tr (A + B) = tr A + tr B and that tr (cA) = c tr A holds for all n × n matrices A and B and
all scalars c. The following fact is more surprising.

Lemma 5.5.1
Let A and B be n × n matrices. Then tr (AB) = tr (BA).

Proof. Write A = ai j and B = bi j . For each i, the (i, i)-entry di of the matrix AB is given as follows:
di = ai1b1i + ai2b2i + · · · + ainbni =  j ai jb ji. Hence

                 tr (AB) = d1 + d2 + · · · + dn =  di =   ai jb ji
                 i                                      ij

Similarly we have tr (BA) = i( j bi ja ji). Since these two double sums are the same, Lemma 5.5.1 is
proved.

    As the name indicates, similar matrices share many properties, some of which are collected in the next
theorem for reference.

Theorem 5.5.1

If A and B are similar n × n matrices, then A and B have the same determinant, rank, trace,
characteristic polynomial, and eigenvalues.

Proof. Let B = P-1AP for some invertible matrix P. Then we have
                      det B = det (P-1) det A det P = det A because det (P-1) = 1/ det P

Similarly, rank B = rank (P-1AP) = rank A by Corollary 5.4.3. Next Lemma 5.5.1 gives
                                 tr (P-1AP) = tr P-1(AP) = tr (AP)P-1 = tr A

As to the characteristic polynomial,

                                  cB(x) = det (xI - B) = det {x(P-1IP) - P-1AP}
                                                           = det {P-1(xI - A)P}
                                                           = det (xI - A)
                                                           = cA(x)

Finally, this shows that A and B have the same eigenvalues because the eigenvalues of a matrix are the
roots of its characteristic polynomial.
  6 Vector Space Rn

   Example 5.5.2
   Sharing the five properties in Theorem 5.5.1 does not guarantee that two matrices are similar. The
   matrices A = 1 1 0 1 and I = 1 0 0 1 have the same determinant, rank, trace, characteristic
   polynomial, and eigenvalues, but they are not similar because P-1IP = I for any invertible matrix
   P.

Diagonalization Revisited

Recall that a square matrix A is diagonalizable if there exists an invertible matrix P such that P-1AP = D
is a diagonal matrix, that is if A is similar to a diagonal matrix D. Unfortunately, not all matrices are
diagonalizable, for example 1 1 0 1 (see Example 3.4.3). Determining whether A is diagonalizable is
closely related to the eigenvalues and eigenvectors of A. Recall that a number  is called an eigenvalue of
A if Ax =  x for some nonzero column x in Rn, and any such nonzero vector x is called an eigenvector of
A corresponding to  (or simply a  -eigenvector of A). The eigenvalues and eigenvectors of A are closely
related to the characteristic polynomial cA(x) of A, defined by

                                                   cA(x) = det (xI - A)

If A is n ×n this is a polynomial of degree n, and its relationship to the eigenvalues is given in the following
theorem (a repeat of Theorem 3.3.2).

   Theorem 5.5.2
   Let A be an n × n matrix.

       1. The eigenvalues  of A are the roots of the characteristic polynomial cA(x) of A.
       2. The  -eigenvectors x are the nonzero solutions to the homogeneous system

                                                          ( I - A)x = 0

           of linear equations with  I - A as coefficient matrix.

   Example 5.5.3
   Show that the eigenvalues of a triangular matrix are the main diagonal entries.

   Solution. Assume that A is triangular. Then the matrix xI - A is also triangular and has diagonal
   entries (x - a11), (x - a22), . . . , (x - ann) where A = ai j . Hence Theorem 3.1.4 gives

                                        cA(x) = (x - a11)(x - a22) · · · (x - ann)
   and the result follows because the eigenvalues are the roots of cA(x).
. . Similarity and Diagonalization

    Theorem 3.4.1 asserts (in part) that an n × n matrix A is diagonalizable if and only if it has n eigen-
vectors x1, . . . , xn such that the matrix P = x1 · · · xn with the xi as columns is invertible. This is
equivalent to requiring that {x1, . . . , xn} is a basis of Rn consisting of eigenvectors of A. Hence we can
restate Theorem 3.4.1 as follows:

   Theorem 5.5.3
   Let A be an n × n matrix.

       1. A is diagonalizable if and only if Rn has a basis {x1, x2, . . . , xn} consisting of eigenvectors
           of A.

       2. When this is the case, the matrix P = x1 x2 · · · xn is invertible and
           P-1AP = diag (1, 2, . . . , n) where, for each i, i is the eigenvalue of A corresponding to
           xi.

    The next result is a basic tool for determining when a matrix is diagonalizable. It reveals an important
connection between eigenvalues and linear independence: Eigenvectors corresponding to distinct eigen-
values are necessarily linearly independent.

   Theorem 5.5.4
   Let x1, x2, . . . , xk be eigenvectors corresponding to distinct eigenvalues 1, 2, . . . , k of an n × n
   matrix A. Then {x1, x2, . . . , xk} is a linearly independent set.

Proof. We use induction on k. If k = 1, then {x1} is independent because x1 = 0. In general, suppose

the theorem is true for some k  1. Given eigenvectors {x1, x2, . . . , xk+1}, suppose a linear combination
vanishes:

t1x1 + t2x2 + · · · + tk+1xk+1 = 0                                                                  (5.3)

We must show that each ti = 0. Left multiply (5.3) by A and use the fact that Axi = ixi to get

t11x1 + t22x2 + · · · + tk+1k+1xk+1 = 0                                                             (5.4)

If we multiply (5.3) by 1 and subtract the result from (5.4), the first terms cancel and we obtain

t2(2 - 1)x2 + t3(3 - 1)x3 + · · · + tk+1(k+1 - 1)xk+1 = 0

Since x2, x3, . . . , xk+1 correspond to distinct eigenvalues 2, 3, . . . , k+1, the set {x2, x3, . . . , xk+1} is
independent by the induction hypothesis. Hence,

t2(2 - 1) = 0, t3(3 - 1) = 0, . . . , tk+1(k+1 - 1) = 0

and so t2 = t3 = · · · = tk+1 = 0 because the i are distinct. Hence (5.3) becomes t1x1 = 0, which implies
that t1 = 0 because x1 = 0. This is what we wanted.

    Theorem 5.5.4 will be applied several times; we begin by using it to give a useful condition for when
a matrix is diagonalizable.
8 Vector Space Rn

Theorem 5.5.5
If A is an n × n matrix with n distinct eigenvalues, then A is diagonalizable.

Proof. Choose one eigenvector for each of the n distinct eigenvalues. Then these eigenvectors are inde-
pendent by Theorem 5.5.4, and so are a basis of Rn by Theorem 5.2.7. Now use Theorem 5.5.3.

Example 5.5.4

                   1 0 0
Show that A =  1 2 3  is diagonalizable.

                     -1 1 0

Solution. A routine computation shows that cA(x) = (x - 1)(x - 3)(x + 1) and so has distinct
eigenvalues 1, 3, and -1. Hence Theorem 5.5.5 applies.

    However, a matrix can have multiple eigenvalues as we saw in Section 3.3. To deal with this situation,
we prove an important lemma which formalizes a technique that is basic to diagonalization, and which
will be used three times below.

Lemma 5.5.2
Let {x1, x2, . . . , xk} be a linearly independent set of eigenvectors of an n × n matrix A, extend it to
a basis {x1, x2, . . . , xk, . . . , xn} of Rn, and let

                   P = x1 x2 · · · xn

be the (invertible) n × n matrix with the xi as its columns. If 1, 2, . . . , k are the (not necessarily
distinct) eigenvalues of A corresponding to x1, x2, . . ., xk respectively, then P-1AP has block form

                   P-1AP =               diag (1, 2, . . . , k) B
                                         0
                                                                  A1

where B has size k × (n - k) and A1 has size (n - k) × (n - k).

Proof. If {e1, e2, . . . , en} is the standard basis of Rn, then

e1 e2 . . . en     = In = P-1P = P-1 x1 x2 · · · xn
                                   = P-1x1 P-1x2 · · · P-1xn

Comparing columns, we have P-1xi = ei for each 1  i  n. On the other hand, observe that

P-1AP = P-1A x1 x2 · · · xn = (P-1A)x1 (P-1A)x2 · · · (P-1A)xn

Hence, if 1  i  k, column i of P-1AP is

                   (P-1A)xi = P-1(ixi) = i(P-1xi) = iei
                                                            . . Similarity and Diagonalization

This describes the first k columns of P-1AP, and Lemma 5.5.2 follows.
Note that Lemma 5.5.2 (with k = n) shows that an n × n matrix A is diagonalizable if Rn has a basis of
eigenvectors of A, as in (1) of Theorem 5.5.3.

   Definition 5.14 Eigenspace of a Matrix
   If  is an eigenvalue of an n × n matrix A, define the eigenspace of A corresponding to  by

                                             E (A) = {x in Rn | Ax =  x}

This is a subspace of Rn and the eigenvectors corresponding to  are just the nonzero vectors in E (A). In
fact E (A) is the null space of the matrix ( I - A):

E (A) = {x | ( I - A)x = 0} = null ( I - A)

Hence, by Theorem 5.4.2, the basic solutions of the homogeneous system ( I - A)x = 0 given by the
gaussian algorithm form a basis for E (A). In particular

dim E (A) is the number of basic solutions x of ( I - A)x = 0                                   (5.5)

Now recall (Definition 3.7) that the multiplicity11 of an eigenvalue  of A is the number of times  occurs
as a root of the characteristic polynomial cA(x) of A. In other words, the multiplicity of  is the largest
integer m  1 such that

                                                  cA(x) = (x -  )mg(x)

for some polynomial g(x). Because of (5.5), the assertion (without proof) in Theorem 3.4.2 can be stated
as follows: A square matrix is diagonalizable if and only if the multiplicity of each eigenvalue  equals
dim [E (A)]. We are going to prove this, and the proof requires the following result which is valid for any
square matrix, diagonalizable or not.

Lemma 5.5.3
Let  be an eigenvalue of multiplicity m of a square matrix A. Then dim [E (A)]  m.

Proof. Write dim [E (A)] = d. It suffices to show that cA(x) = (x -  )dg(x) for some polynomial g(x),
because m is the highest power of (x -  ) that divides cA(x). To this end, let {x1, x2, . . . , xd} be a basis
of E (A). Then Lemma 5.5.2 shows that an invertible n × n matrix P exists such that

P-1AP =   Id B
          0 A1

in block form, where Id denotes the d × d identity matrix. Now write A = P-1AP and observe that
cA(x) = cA(x) by Theorem 5.5.1. But Theorem 3.1.5 gives

                    cA(x) = cA(x) = det (xIn - A) = det  (x -  )Id -B
                                                         0
11This is often called the algebraic multiplicity of  .     xIn-d - A1
6 Vector Space Rn

                                                            = det [(x -  )Id] det [(xIn-d - A1)]
                                                            = (x -  )dg(x)

where g(x) = cA1(x). This is what we wanted.
    It is impossible to ignore the question when equality holds in Lemma 5.5.3 for each eigenvalue  . It

turns out that this characterizes the diagonalizable n × n matrices A for which cA(x) factors completely
over R. By this we mean that cA(x) = (x - 1)(x - 2) · · · (x - n), where the i are real numbers (not
necessarily distinct); in other words, every eigenvalue of A is real. This need not happen (consider A =

  0 -1 1 0 ), and we investigate the general case below.

   Theorem 5.5.6
   The following are equivalent for a square matrix A for which cA(x) factors completely.

       1. A is diagonalizable.

       2. dim [E (A)] equals the multiplicity of  for every eigenvalue  of the matrix A.

Proof. Let A be n × n and let 1, 2, . . . , k be the distinct eigenvalues of A. For each i, let mi denote the
multiplicity of i and write di = dim Ei(A) . Then

                                     cA(x) = (x - 1)m1(x - 2)m2 . . . (x - k)mk

so m1 + · · · + mk = n because cA(x) has degree n. Moreover, di  mi for each i by Lemma 5.5.3.
    (1)  (2). By (1), Rn has a basis of n eigenvectors of A, so let ti of them lie in Ei(A) for each i. Since

the subspace spanned by these ti eigenvectors has dimension ti, we have ti  di for each i by Theorem 5.2.4.
Hence

                                n = t1 + · · · + tk  d1 + · · · + dk  m1 + · · · + mk = n

It follows that d1 + · · · + dk = m1 + · · · + mk so, since di  mi for each i, we must have di = mi. This is (2).
    (2)  (1). Let Bi denote a basis of Ei(A) for each i, and let B = B1  · · ·  Bk. Since each Bi contains

mi vectors by (2), and since the Bi are pairwise disjoint (the i are distinct), it follows that B contains n
vectors. So it suffices to show that B is linearly independent (then B is a basis of Rn). Suppose a linear
combination of the vectors in B vanishes, and let yi denote the sum of all terms that come from Bi. Then yi
lies in Ei(A), so the nonzero yi are independent by Theorem 5.5.4 (as the i are distinct). Since the sum
of the yi is zero, it follows that yi = 0 for each i. Hence all coefficients of terms in yi are zero (because Bi
is independent). Since this holds for each i, it shows that B is independent.

Example 5.5.5

 5 8 16             2 1 1

If A =  4 1 8  and B =  2 1 -2  show that A is diagonalizable but B is not.

-4 -4 -11          -1 0 -2

Solution. We have cA(x) = (x + 3)2(x - 1) so the eigenvalues are 1 = -3 and 2 = 1. The
                   . . Similarity and Diagonalization 6

corresponding eigenspaces are E1(A) = span {x1, x2} and E2(A) = span {x3} where

 -1       -2        2

x1 =  1  , x2 =  0  , x3 =  1 
0        1           -1

as the reader can verify. Since {x1, x2} is independent, we have dim (E1(A)) = 2 which is the
multiplicity of 1. Similarly, dim (E2(A)) = 1 equals the multiplicity of 2. Hence A is
diagonalizable by Theorem 5.5.6, and a diagonalizing matrix is P = x1 x2 x3 .
Turning to B, cB(x) = (x + 1)2(x - 3) so the eigenvalues are 1 = -1 and 2 = 3. The
corresponding eigenspaces are E1(B) = span {y1} and E2(B) = span {y2} where

    -1       5

y1 =  2  , y2 =  6 
      1        -1

Here dim (E1(B)) = 1 is smaller than the multiplicity of 1, so the matrix B is not diagonalizable,
again by Theorem 5.5.6. The fact that dim (E1(B)) = 1 means that there is no possibility of
finding three linearly independent eigenvectors.

Complex Eigenvalues

All the matrices we have considered have had real eigenvalues. But this need not be the case: The matrix
A = 0 -1 1 0 has characteristic polynomial cA(x) = x2 + 1 which has no real roots. Nonetheless, this
matrix is diagonalizable; the only difference is that we must use a larger set of scalars, the complex
numbers. The basic properties of these numbers are outlined in Appendix A.

    Indeed, nearly everything we have done for real matrices can be done for complex matrices. The
methods are the same; the only difference is that the arithmetic is carried out with complex numbers rather
than real ones. For example, the gaussian algorithm works in exactly the same way to solve systems of
linear equations with complex coefficients, matrix multiplication is defined the same way, and the matrix
inversion algorithm works in the same way.

    But the complex numbers are better than the real numbers in one respect: While there are polynomials
like x2 + 1 with real coefficients that have no real root, this problem does not arise with the complex
numbers: Every nonconstant polynomial with complex coefficients has a complex root, and hence factors
completely as a product of linear factors. This fact is known as the fundamental theorem of algebra.12

   Example 5.5.6

   Diagonalize the matrix A = 0 -1 1 0 .

   Solution. The characteristic polynomial of A is

                                    cA(x) = det (xI - A) = x2 + 1 = (x - i)(x + i)

   12This was a famous open problem in 1799 when Gauss solved it at the age of 22 in his Ph.D. dissertation.
6 Vector Space Rn

where i2 = -1. Hence the eigenvalues are 1 = i and 2 = -i, with corresponding eigenvectors
x1 = 1 -i and x2 = 1i . Hence A is diagonalizable by the complex version of Theorem 5.5.5,

and the complex version of Theorem 5.5.3 shows that P = x1 x2 = 1 1 -i i is invertible

and P-1AP = 1 0 = i 0 . Of course, this can be checked directly.
              0 2           0 -i

We shall return to complex linear algebra in Section 8.7.

Symmetric Matrices13

On the other hand, many of the applications of linear algebra involve a real matrix A and, while A will
have complex eigenvalues by the fundamental theorem of algebra, it is always of interest to know when
the eigenvalues are, in fact, real. While this can happen in a variety of ways, it turns out to hold whenever
A is symmetric. This important theorem will be used extensively later. Surprisingly, the theory of complex
eigenvalues can be used to prove this useful result about real eigenvalues.

Let z denote the conjugate of a complex number z. If A is a complex matrix, the conjugate matrix A

is defined to be the matrix obtained from A by conjugating every entry. Thus, if A = zi j , then A = zi j .

For example,

                    If A =  -i + 2 5     then A =          i+2 5
                               i 3 + 4i                     -i 3 - 4i

Recall that z + w = z + w and zw = z w hold for all complex numbers z and w. It follows that if A and B
are two complex matrices, then

                      A + B = A + B, AB = A B and  A =  A

hold for all complex scalars  . These facts are used in the proof of the following theorem.

Theorem 5.5.7
Let A be a symmetric real matrix. If  is any complex eigenvalue of A, then  is real.14

Proof. Observe that A = A because A is real. If  is an eigenvalue of A, we show that  is real by showing
that  =  . Let x be a (possibly complex) eigenvector corresponding to  , so that x = 0 and Ax =  x.
Define c = xT x.

                       
                           z1

                z2  
                    
If we write x =  ..  where the zi are complex numbers, we have
              .

                zn

                        c = xT x = z1z1 + z2z2 + · · · + znzn = |z1|2 + |z2|2 + · · · + |zn|2

13This discussion uses complex conjugation and absolute value. These topics are discussed in Appendix A.
14This theorem was first proved in 1829 by the great French mathematician Augustin Louis Cauchy (1789-1857).
                                                                               . . Similarity and Diagonalization 6

Thus c is a real number, and c > 0 because at least one of the zi = 0 (as x = 0). We show that  =  by
verifying that  c =  c. We have

                                     c =  (xT x) = ( x)T x = (Ax)T x = xT AT x

At this point we use the hypothesis that A is symmetric and real. This means AT = A = A so we continue
the calculation:

                                    c = xT AT x = xT (A x) = xT (Ax) = xT ( x)
                                                                             = xT ( x)
                                                                             =  xT x
                                                                             = c

as required.
The technique in the proof of Theorem 5.5.7 will be used again when we return to complex linear algebra
in Section 8.7.

   Example 5.5.7
   Verify Theorem 5.5.7 for every real, symmetric 2 × 2 matrix A.

   Solution. If A = a b b c we have cA(x) = x2 - (a + c)x + (ac - b2), so the eigenvalues are given
   by  = 12[(a + c) ± (a + c)2 - 4(ac - b2)]. But here

                                      (a + c)2 - 4(ac - b2) = (a - c)2 + 4b2  0

   for any choice of a, b, and c. Hence, the eigenvalues are real numbers.
 6 Vector Space Rn

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

 .6 Best Approximation and Least Squares

Often an exact solution to a problem in applied mathematics is difficult to obtain. However, it is usually
just as useful to find arbitrarily close approximations to a solution. In particular, finding "linear approx-
imations" is a potent technique in applied mathematics. One basic case is the situation where a system
of linear equations has no solution, and it is desirable to find a "best approximation" to a solution to the
system. In this section best approximations are defined and a method for finding them is described. The
result is then applied to "least squares" approximation of data.

    Suppose A is an m × n matrix and b is a column in Rm, and consider the system

                                                           Ax = b
of m linear equations in n variables. This need not have a solution. However, given any column z  Rn,
the distance b - Az is a measure of how far Az is from b. Hence it is natural to ask whether there is a
column z in Rn that is as close as possible to a solution in the sense that

                                                           b - Az
is the minimum value of b - Ax as x ranges over all columns in Rn.

    The answer is "yes", and to describe it define
                                                 U = {Ax | x lies in Rn}
                        .6. Best Approximation and Least Squares 6

    b b - Az            This is a subspace of Rm (verify) and we want a vector Az in U as close as
                     U  possible to b. That there is such a vector is clear geometrically if m = 3 by
                        the diagram. In general such a vector Az exists by a general result called
0 Az                    the projection theorem that will be proved in Chapter 8 (Theorem 8.1.3).
                        Moreover, the projection theorem gives a simple way to compute z because
                        it also shows that the vector b - Az is orthogonal to every vector Ax in U .
                        Thus, for all x in Rn,

                        0 = (Ax) · (b - Az) = (Ax)T (b - Az) = xT AT (b - Az)
                                                                     = x · [AT (b - Az)]

In other words, the vector AT (b - Az) in Rn is orthogonal to every vector in Rn and so must be zero (being
orthogonal to itself). Hence z satisfies

                                                      (AT A)z = AT b

Definition 5.15 Normal Equations
This is a system of linear equations called the normal equations for z.

Note that this system can have more than one solution (see Exercise ??). However, the n × n matrix AT A
is invertible if (and only if) the columns of A are linearly independent (Theorem 5.4.3); so, in this case,
z is uniquely determined and is given explicitly by z = (AT A)-1AT b. However, the most efficient way to
find z is to apply gaussian elimination to the normal equations.

    This discussion is summarized in the following theorem.

   Theorem 5.6.1: Best Approximation Theorem
   Let A be an m × n matrix, let b be any column in Rm, and consider the system

                                                          Ax = b

   of m equations in n variables.

       1. Any solution z to the normal equations

                                                         (AT A)z = AT b

           is a best approximation to a solution to Ax = b in the sense that b - Az is the minimum
           value of b - Ax as x ranges over all columns in Rn.
       2. If the columns of A are linearly independent, then AT A is invertible and z is given uniquely
           by z = (AT A)-1AT b.

We note in passing that if A is n × n and invertible, then
                                                z = (AT A)-1AT b = A-1b
66 Vector Space Rn

is the solution to the system of equations, and b - Az = 0. Hence if A has independent columns, then
(AT A)-1AT is playing the role of the inverse of the nonsquare matrix A. The matrix AT (AAT )-1 plays a
similar role when the rows of A are linearly independent. These are both special cases of the generalized
inverse of a matrix A (see Exercise ??). However, we shall not pursue this topic here.

Example 5.6.1
The system of linear equations

                                                    3x - y = 4
                                                     x + 2y = 0

                                                    2x + y = 1

has no solution. Find the vector z = x0 that best approximates a solution.
                                               y0

Solution. In this case,

                      3      -1                       312                 3 -1                 14 1
                 A= 1          2  , so AT A =       -1 2 1                1 2 =                 16
                               1
                         2                                                  21

is invertible. The normal equations (AT A)z = AT b are

                                    14 1  z=        14          , so z = 831 87

                                    16              -3                         -56

Thus  x0  =  87  and  y0  =  -56 .  With  these  values  of  x  and  y,  the  left  sides  of  the  equations  are,
             83
approximately,                83

                                          3x0 -     y0 = 317 =           3.82

                                                             83

                                          x0     +  2y0  =   -25  =  -0.30
                                                              83

                                          2x0 +     y0 = 118 =           1.42

                                                             83

This is as close as possible to a solution.

Example 5.6.2

The average number g of goals per game scored by a hockey player seems to be related linearly to
two factors: the number x1 of years of experience and the number x2 of goals in the preceding 10
games. The data on the following page were collected on four players. Find the linear function
g = a0 + a1x1 + a2x2 that best fits these data.

                                                     g x1 x2
                                                    0.8 5 3
                                                    0.8 3 4
                                                    0.6 1 5
                                                    0.4 2 1
                                                                .6. Best Approximation and Least Squares 6

Solution. If the relationship is given by g = r0 + r1x1 + r2x2, then the data can be described as

follows:                                  1 5 3     0.8 
                                                            r0  0.8 
                                         1  3  4     r          =
                                                  
                                            1 1 5  1  0.6                  

                                                                      0.4
                                                            r2
                                            121

Using the notation in Theorem 5.6.1, we get

              z = (AT A)-1AT b

                                          119 -17 -19   1 1 1 1   0.8  0.14  
              = 1  -17                                                     0.8
                                         5     1  5     3       1  2              =  0.09
              42                                                                
                                                                       0.6                    

                                         -19 1 5 3 4 5 1 0.4 0.08

Hence the best-fitting function is g = 0.14 + 0.09x1 + 0.08x2. The amount of computation would
have been reduced if the normal equations had been constructed and then solved by gaussian
elimination.

Least Squares Approximation

In many scientific investigations, data are collected that relate two variables. For example, if x is the

number of dollars spent on advertising by a manufacturer and y is the value of sales in the region in

question, the manufacturer could generate data by spending x1, x2, . . . , xn dollars at different times and
measuring the corresponding sales values y1, y2, . . . , yn.

                                         Suppose it is known that a linear relationship exists between the vari-

   y               Line 2 Line 1         ables x and y--in other words, that y = a + bx for some constants a and
                               (x5, y5)  b. If the data are plotted, the points (x1, y1), (x2, y2), . . . , (xn, yn) may
    (x1, y1)                             appear to lie on a straight line and estimating a and b requires finding
                         (x4, y4)
                   (x3, y3)              the "best-fitting" line through these data points. For example, if five data

              (x2, y2)                   points occur as shown in the diagram, line 1 is clearly a better fit than line

                                         2. In general, the problem is to find the values of the constants a and b

              x such that the line y = a + bx best approximates the data in question. Note
0                                        that an exact fit would be obtained if a and b were such that yi = a + bxi

                                         were true for each data point (xi, yi). But this is too much to expect. Ex-

perimental errors in measurement are bound to occur, so the choice of a and b should be made in such a

way that the errors between the observed values yi and the corresponding fitted values a + bxi are in some

sense minimized. Least squares approximation is a way to do this.

    The first thing we must do is explain exactly what we mean by the best fit of a line y = a + bx to an
observed set of data points (x1, y1), (x2, y2), . . . , (xn, yn). For convenience, write the linear function
r0 + r1x as

                                               f (x) = r0 + r1x

so that the fitted points (on the line) have coordinates (x1, f (x1)), . . . , (xn, f (xn)).
68 Vector Space Rn

y                                                    The second diagram is a sketch of what the line y = f (x) might look

                      dn                         like. For each i the observed data point (xi, yi) and the fitted point
              (xi, yi) (xn, f (xn))              (xi, f (xi)) need not be the same, and the distance di between them mea-
                                                 sures how far the line misses the observed point. For this reason di is often
              di         (xn, yn)                called the error at xi, and a natural measure of how close the line y = f (x)
              f (x) (xi, f (xi))                 is to the observed data points is the sum d1 + d2 + · · · + dn of all these
          y=                                     errors. However, it turns out to be better to use the sum of squares
   d1 (x1, f (x1))

          (x1, y1)

0 x1              xi xn             x

                                                                               S       =  d2   +  d2   +   ·  ·  ·  +  d2

                                                                                            1       2                    n

as the measure of error, and the line y = f (x) is to be chosen so as to make this sum as small

as possible. This line is said to be the least squares approximating line for the data points

(x1, y1), (x2, y2), . . . , (xn, yn).

The  square         of   the      error  di  is  given  by  d2   =  [yi  -  f  (xi)]2  for  each  i,   so  the      quantity  S  to  be  minimized  is

the sum:                                                      i

                                       S = [y1 - f (x1)]2 + [y2 - f (x2)]2 + · · · + [yn - f (xn)]2

Note that all the numbers xi and yi are given here; what is required is that the function f be chosen in such

a way as to minimize S. Because f (x) = r0 + r1x, this amounts to choosing r0 and r1 to minimize S. This
problem can be solved using Theorem 5.6.1. The following notation is convenient.

                                                                                            f (x1)                  r0 + r1x1    
                                     x1            y1

                                     x2            y2                                       f (x2)                  r0 + r1x2    
                                                                                                                                 
                         x =  ..  y =  ..  and f (x) =  ..  =  .. 
                                  . .                                                   .  . 

                                     xn            yn                                       f (xn)                  r0 + r1xn

Then the problem takes the following form: Choose r0 and r1 such that
                      S = [y1 - f (x1)]2 + [y2 - f (x2)]2 + · · · + [yn - f (xn)]2 = y - f (x) 2

is as small as possible. Now write

                                                            1 x1     

                                                        1        x2                               r0
                                                                     
                                                 M =  .. ..  and r =
                                                        . .                                       r1

                                                            1 xn

Then Mr = f (x), so we are looking for a column r = r0 such that y - Mr 2 is as small as possible.
                                                                    r1

In other words, we are looking for a best approximation z to the system Mr = y. Hence Theorem 5.6.1
applies directly, and we have

Theorem 5.6.2
Suppose that n data points (x1, y1), (x2, y2), . . . , (xn, yn) are given, where at least two of
                                                              .6. Best Approximation and Least Squares 6

x1, x2, . . . , xn are distinct. Put

                                                                    1         
                                            y1                             x1

                                            y2                1            x2       
                                                                                    
                                      y =  ..  M =  ..                         ...
                                          .  .                                      
                                                                                    

                                            yn                      1      xn

Then the least squares approximating line for these data points has equation

                                            y = z0 + z1x

where z = z0 is found by gaussian elimination from the normal equations
                z1

                                            (MT M)z = MT y

The condition that at least two of x1, x2, . . . , xn are distinct ensures that MT M is an invertible
matrix, so z is unique:

                                               z = (MT M)-1MT y

Example 5.6.3

Let data points (x1, y1), (x2, y2), . . . , (x5, y5) be given as in the accompanying table. Find the
least squares approximating line for these data.

                                                         xy
                                                         11
                                                         32
                                                         43
                                                         64
                                                         75

Solution. In this case we have                                   1 x1               
                           MT M =
                                   =  1 1 ··· 1            1        ..     x2..     
                                      x1 x2 · · · x5                .      .        
                       and MT y =
                                                                                    
                                                                                    

                                                                 1 x5

                                         5             x1 + · · · + x5                =   5 21
                                                       x2                  x2            21 111
                                      x1 + · · · + x5      +  ·  ·  ·   +
                                                        1                   5

                                                           
                                                              y1

                                      1  1 ··· 1                 y2     
                                                                        
                                                            .. 
                                      x1 x2 · · · x5       .

                                                                 y5
Vector Space Rn

                            = y1 + y2 + · · · + y5 x1y1 + x2y2 + · · · + x5y5 = 15 78

so the normal equations (MT M)z = MT y for z = z0 become
                                                              z1

                                    5 21                        z0 z1 = 15 78
                                   21 111

The solution (using gaussian elimination) is z = z0 z1 = 0.24 0.66 to two decimal places, so the
least squares approximating line for these data is y = 0.24 + 0.66x. Note that MT M is indeed
invertible here (the determinant is 114), and the exact solution is

             z = (MT M)-1MT y = 1                     111 -21         15       1 27              19
                                                      -21 5           78 = 114 75 = 38 25
                                                 114

Least Squares Approximating Polynomials

Suppose now that, rather than a straight line, we want to find a polynomial

                            y = f (x) = r0 + r1x + r2x2 + · · · + rmxm

of degree m that best approximates the data pairs (x1, y1), (x2, y2), . . . , (xn, yn). As before, write

                                                                                       f (x1)  
                            x1                            y1

                            x2                            y2                           f (x2)  
                                                                                               
                          x =  ..  y =  ..  and f (x) =  .. 
                          . .                                                  .

                            xn                            yn                           f (xn)

For each xi we have two values of the variable y, the observed value yi, and the computed value f (xi). The
problem is to choose f (x)--that is, choose r0, r1, . . . , rm --such that the f (xi) are as close as possible to
the yi. Again we define "as close as possible" by the least squares condition: We choose the ri such that

                          y - f (x) 2 = [y1 - f (x1)]2 + [y2 - f (x2)]2 + · · · + [yn - f (xn)]2

is as small as possible.

Definition 5.16 Least Squares Approximation

A polynomial f (x) satisfying this condition is called a least squares approximating polynomial
of degree m for the given data pairs.

If we write                                                                      
                                                                                    r0
                              1    x1                 x2  ···   xm

                                                       1         1

                              1                       x2                         
                                                          ···   xm 
                                   x2                  2         2                     r1      
                                                                                               
                          M= . . .                              .  and r =  . 
                             .. .. ..                           ..                .. 

                                                      x2                         
                                                                                    rm
                              1    xn                  n  ···   xm

                                                                 n
                                                     .6. Best Approximation and Least Squares

we see that f (x) = Mr. Hence we want to find r such that y - Mr 2 is as small as possible; that is, we
want a best approximation z to the system Mr = y. Theorem 5.6.1 gives the first part of Theorem 5.6.3.

Theorem 5.6.3
Let n data pairs (x1, y1), (x2, y2), . . . , (xn, yn) be given, and write

                             1                   x2            

                     1           x1               1  ···  xm                     z0
   y
                                                           1

                             1                   x2            
                     y2                              ···  xm 
                                 x2               2        2                     z1  
                                                                                     
y =  ..  M =  . . .                                       .  z= . 
 .   .. .. ..                                             ..   .. 

                     yn                          x2            

                             1   xn               n  ···  xm                     zm

                                                           n

1. If z is any solution to the normal equations

                                 (MT M)z = MT y

then the polynomial        z0 + z1x + z2x2 + · · · + zmxm

is a least squares approximating polynomial of degree m for the given data pairs.

2. If at least m + 1 of the numbers x1, x2, . . . , xn are distinct (so n  m + 1), the matrix MT M is
   invertible and z is uniquely determined by

                             z = (MT M)-1MT y

Proof. It remains to prove (2), and for that we show that the columns of M are linearly independent
(Theorem 5.4.3). Suppose a linear combination of the columns vanishes:

                                                       
                     1       x1                           xm                  0

                     1                                     1
                                                      xm   0 
                             x2                       2  
                                 
r0  ..  + r1  ..  + · · · + rm  ..  =  .. 
                     .  .                             .  .

                     1       xn                           xm                  0

                                                           n

If we write q(x) = r0 + r1x + · · · + rmxm, equating coefficients shows that

                         q(x1) = q(x2) = · · · = q(xn) = 0

Hence q(x) is a polynomial of degree m with at least m + 1 distinct roots, so q(x) must be the zero poly-
nomial (see Appendix D or Theorem 6.5.4). Thus r0 = r1 = · · · = rm = 0 as required.

Example 5.6.4
Find the least squares approximating quadratic y = z0 + z1x + z2x2 for the following data points.

                                   (-3, 3), (-1, 1), (0, 1), (1, 2), (3, 4)
Vector Space Rn

Solution. This is an instance of Theorem 5.6.3 with m = 2. Here

                            3   1 -3 9 

                            1   1 -1 1 

                 y =  1  M =  1 0 0 

                            2     1 1 1 

                                                4      1 39

Hence,                                                    1 -3 9 
                                                1 1   1 -1 1   5 0
                   1 11                         1 3   1 0 0  =  0 20            20 
        MT M =  -3 -1 0                         1 9  1 1 1  20 0                  0

                        9 10                                1 39               164

                                       1                            3
                             MT y =  -3           1 1 1 1   1   11 
                                                -1 0 1 3   1  =  4 
                                             9    1 0 1 9  2  66

The normal equations for z are                                         4

                  5 0 20   11                                          1.15 
                                                       whence z =  0.20 
                  0 20 0  z =  4 
                                                                         0.26
                 20 0 164                          66

This means that the least squares approximating quadratic for these data is
y = 1.15 + 0.20x + 0.26x2.

Other Functions

There is an extension of Theorem 5.6.3 that should be mentioned. Given data pairs (x1, y1), (x2, y2),
. . . , (xn, yn), that theorem shows how to find a polynomial

                                               f (x) = r0 + r1x + · · · + rmxm

such that y - f (x) 2 is as small as possible, where x and f (x) are as before. Choosing the appropriate
polynomial f (x) amounts to choosing the coefficients r0, r1, . . . , rm, and Theorem 5.6.3 gives a formula
for the optimal choices. Here f (x) is a linear combination of the functions 1, x, x2, . . . , xm where the ri
are the coefficients, and this suggests applying the method to other functions. If f0(x), f1(x), . . . , fm(x)
are given functions, write

                                       f (x) = r0 f0(x) + r1 f1(x) + · · · + rm fm(x)
                                                 .6. Best Approximation and Least Squares

where the ri are real numbers. Then the more general question is whether r0, r1, . . . , rm can be found such
that y - f (x) 2 is as small as possible where

                                                 f (x1)  

                                                 f (x2)  
                                                         
                             f (x) =  .. 
                                     .

                                                 f (xm)

Such a function f (x) is called a least squares best approximation for these data pairs of the form
r0 f0(x) + r1 f1(x) + · · · + rm fm(x), ri in R. The proof of Theorem 5.6.3 goes through to prove

Theorem 5.6.4
Let n data pairs (x1, y1), (x2, y2), . . . , (xn, yn) be given, and suppose that m + 1 functions
f0(x), f1(x), . . . , fm(x) are specified. Write

                             f0(x1)  f1(x1) · · ·                         
   y1                        f0(x2)  f1(x2) · · ·          fm(x1)            z1
                           
  y2                 M =       ..      ..                  fm(x2)...        z2  
                               .       .                                        
y =  ..                                                               z =  .. 
.                            f0(xn)  f1(xn) · · ·                         .
                                                                   

  yn                                                       fm(xn)           zm

1. If z is any solution to the normal equations

                                    (MT M)z = MT y

then the function

                     z0 f0(x) + z1 f1(x) + · · · + zm fm(x)

is the best approximation for these data among all functions of the form
r0 f0(x) + r1 f1(x) + · · · + rm fm(x) where the ri are in R.

2. If MT M is invertible (that is, if rank (M) = m + 1), then z is uniquely determined; in fact,
   z = (MT M)-1(MT y).

Clearly Theorem 5.6.4 contains Theorem 5.6.3 as a special case, but there is no simple test in gen-
eral for whether MT M is invertible. Conditions for this to hold depend on the choice of the functions
f0(x), f1(x), . . . , fm(x).

Example 5.6.5

Given the data pairs (-1, 0), (0, 1), and (1, 4), find the least squares approximating function of
the form r0x + r12x.

Solution. The functions are f0(x) = x and f1(x) = 2x, so the matrix M is

                    f0(x1) f1(x1)   -1 2-1   -2 1 
                                                 0 20      = 1        0 2
M =  f0(x2) f1(x2)  =                            1 21
                                                                   2
                     f0(x3) f1(x3)                                    24
Vector Space Rn

In this case M M = 4T     18 6        is invertible, so the normal equations
                                6 21

                                                18 6        z=   4

                                                4  6 21          9

have a unique solution z = 11   1 10            . Hence the best-fitting function of the form r0x + r12 isx
                                      16

                                                               
                                                                 -2
                                                   f (-1)  11                 0

f (x)  =  10 x +  16 2x.  Note  that  f (x)  =     f (0)    =    16  , compared with y =  1 
                                                                 11
          11      11
                                                   f (1)  42  4
                                                                 11

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!
                                           . . An Application to Correlation and Variance

. An Application to Correlation and Variance

Suppose the heights h1, h2, . . . , hn of n men are measured. Such a data set is called a sample of the heights
of all the men in the population under study, and various questions are often asked about such a sample:
What is the average height in the sample? How much variation is there in the sample heights, and how can
it be measured? What can be inferred from the sample about the heights of all men in the population? How
do these heights compare to heights of men in neighbouring countries? Does the prevalence of smoking
affect the height of a man?

    The analysis of samples, and of inferences that can be drawn from them, is a subject called mathemat-
ical statistics, and an extensive body of information has been developed to answer many such questions.
In this section we will describe a few ways that linear algebra can be used.

    It is convenient to represent a sample {x1, x2, . . . , xn} as a sample vector15 x = x1 x2 · · · xn
in Rn. This being done, the dot product in Rn provides a convenient tool to study the sample and describe
some of the statistical concepts related to it. The most widely known statistic for describing a data set is
the sample mean x defined by16

                             1n (x1                          1  n
                                                             n
                       x  =          + x2  + · · · + xn)  =          xi

                                                                i=1

The mean x is "typical" of the sample values xi, but may not itself be one of them. The number xi - x is

called the deviation of xi from the mean x. The deviation is positive if xi > x and it is negative if xi < x.
Moreover, the sum of these deviations is zero:

                        n             n    - nx = nx - nx = 0                              (5.6)

Sample x               (xi - x) =     xi

                       i=1           i=1

-1 0 1                     This is described by saying that the sample mean x is central to the
                x      sample values xi.

                           If the mean x is subtracted from each data value xi, the resulting data
                       xi - x are said to be centred. The corresponding data vector is

           Centred                         xc = x1 - x x2 - x · · · xn - x
          Sample xc

-3 -2 -1               and (5.6) shows that the mean xc = 0. For example, we have plotted the

                   xc  sample x = -1 0 1 4 6 in the first diagram. The mean is x = 2,

and the centred sample xc = -3 -2 -1 2 4 is also plotted. Thus, the effect of centring is to shift

the data by an amount x (to the left if x is positive) so that the mean moves to 0.

Another question that arises about samples is how much variability there is in the sample

                                                 x = x1 x2 · · · xn

that is, how widely are the data "spread out" around the sample mean x. A natural measure of variability
would be the sum of the deviations of the xi about the mean, but this sum is zero by (5.6); these deviations

   15We write vectors in Rn as row matrices, for convenience.
   16The mean is often called the "average" of the sample values xi, but statisticians use the term "mean".
    6 Vector Space Rn

cancel out. To avoid this cancellation, statisticians use the squares (xi - x)2 of the deviations as a measure
                                                                                                          s2  defined17
of  variability.  More  precisely,  they      compute      a  statistic  called    the  sample  variance                 as  follows:
                                                                                                           x

                        s2     n-1 1 [(x1     x)2             x)2                     x)2]      1   n       x)2
                                                                                               n-1
                         x  =              -       +  (x2  -       +  ···  +  (xn  -        =       (xi  -

                                                                                                    i=1

The sample variance will be large if there are many xi at a large distance from the mean x, and it will
be small if all the xi are tightly clustered about the mean. The variance is clearly nonnegative (hence the
notation s2x), and the square root sx of the variance is called the sample standard deviation.

    The sample mean and variance can be conveniently described using the dot product. Let

                                                      1 = 1 1 ··· 1

denote the row with every entry equal to 1. If x = x1 x2 · · · xn , then x · 1 = x1 + x2 + · · · + xn, so
the sample mean is given by the formula

                                                        x = 1n (x · 1)
Moreover, remembering that x is a scalar, we have x1 = x x · · · x , so the centred sample vector xc
is given by

                                    xc = x - x1 = x1 - x x2 - x · · · xn - x

Thus we obtain a formula for the sample variance:

                                            s n- 2x = 1 1 xc n- 2 = 1 1 x - x1 2

Linear algebra is also useful for comparing two different samples. To illustrate how, consider two exam-
ples.

                                          The following table represents the number of sick days at work per
                                      year and the yearly number of visits to a physician for 10 individuals.

Sick                                               Individual 1 2 3 4 5 6 7 8 9 10
Days                                              Doctor visits 2 6 8 1 5 10 3 9 7 4

             Doctor Visits                          Sick days 2 4 8 3 5 9 4 7 7 2

                                    The data are plotted in the scatter diagram where it is evident that,
                                    roughly speaking, the more visits to the doctor the more sick days. This is
                                    an example of a positive correlation between sick days and doctor visits.

                                        Now consider the following table representing the daily doses of vita-
                                    min C and the number of sick days.

Sick                                                Individual 1 2 3 4 5 6 7 8 9 10
Days                                                Vitamin C 1 5 7 0 4 9 2 8 6 3
                                                     Sick days 5 2 2 6 2 1 4 3 2 5
           Vitamin C Doses
                                    The scatter diagram is plotted as shown and it appears that the more vita-
                                    min C taken, the fewer sick days. In this case there is a negative correla-
                                    tion between daily vitamin C and sick days.

   17Since there are n sample values, it seems more natural to divide by n here, rather than by n - 1. The reason for using n - 1
is that then the sample variance s2x provides a better estimate of the variance of the entire population from which the sample
was drawn.
          . . An Application to Correlation and Variance

    In both these situations, we have paired samples, that is observations of two variables are made for ten
individuals: doctor visits and sick days in the first case; daily vitamin C and sick days in the second case.
The scatter diagrams point to a relationship between these variables, and there is a way to use the sample
to compute a number, called the correlation coefficient, that measures the degree to which the variables
are associated.

    To motivate the definition of the correlation coefficient, suppose two paired samples
x = x1 x2 · · · xn , and y = y1 y2 · · · yn are given and consider the centred samples

xc = x1 - x x2 - x · · · xn - x and yc = y1 - y y2 - y · · · yn - y

If xk is large among the xi's, then the deviation xk - x will be positive; and xk - x will be negative if xk
is small among the xi's. The situation is similar for y, and the following table displays the sign of the
quantity (xi - x)(yk - y) in all four cases:

Sign of (xi - x)(yk - y) :

yi large  xi large  xi small
yi small  positive  negative

          negative  positive

Intuitively, if x and y are positively correlated, then two things happen:

1. Large values of the xi tend to be associated with large values of the yi, and
2. Small values of the xi tend to be associated with small values of the yi.

    It follows from the table that, if x and y are positively correlated, then the dot product

                                                                               n

                                 xc · yc = (xi - x)(yi - y)

                                                                             i=1

is positive. Similarly xc · yc is negative if x and y are negatively correlated. With this in mind, the sample
correlation coefficient18 r is defined by

                                                  r = r(x, y) = xc y xc·yc c

Bearing the situation in R3 in mind, r is the cosine of the "angle" between the vectors xc and yc, and so
we would expect it to lie between -1 and 1. Moreover, we would expect r to be near 1 (or -1) if these
vectors were pointing in the same (opposite) direction, that is the "angle" is near zero (or ).

    This is confirmed by Theorem 5.7.1 below, and it is also borne out in the examples above. If we
compute the correlation between sick days and visits to the physician (in the first scatter diagram above)
the result is r = 0.90 as expected. On the other hand, the correlation between daily vitamin C doses and
sick days (second scatter diagram) is r = -0.84.

    However, a word of caution is in order here. We cannot conclude from the second example that taking
more vitamin C will reduce the number of sick days at work. The (negative) correlation may arise because

   18The idea of using a single number to measure the degree of relationship between different variables was pioneered by
Francis Galton (1822-1911). He was studying the degree to which characteristics of an offspring relate to those of its parents.
The idea was refined by Karl Pearson (1857-1936) and r is often referred to as the Pearson correlation coefficient.
8 Vector Space Rn

of some third factor that is related to both variables. For example, case it may be that less healthy people
are inclined to take more vitamin C. Correlation does not imply causation. Similarly, the correlation
between sick days and visits to the doctor does not mean that having many sick days causes more visits to
the doctor. A correlation between two variables may point to the existence of other underlying factors, but
it does not necessarily mean that there is a causality relationship between the variables.

    Our discussion of the dot product in Rn provides the basic properties of the correlation coefficient:

Theorem 5.7.1                                                             be (nonzero) paired samples, and let
Let x = x1 x2 · · · xn and y = y1 y2 · · · yn
r = r(x, y) denote the correlation coefficient. Then:

1. -1  r  1.
2. r = 1 if and only if there exist a and b > 0 such that yi = a + bxi for each i.
3. r = -1 if and only if there exist a and b < 0 such that yi = a + bxi for each i.

Proof. The Cauchy inequality (Theorem 5.3.2) proves (1), and also shows that r = ±1 if and only if one
of xc and yc is a scalar multiple of the other. This in turn holds if and only if yc = bxc for some b = 0, and
it is easy to verify that r = 1 when b > 0 and r = -1 when b < 0.

    Finally, yc = bxc means yi - y = b(xi - x) for each i; that is, yi = a + bxi where a = y - bx. Conversely,
if yi = a + bxi, then y = a + bx (verify), so y1 - y = (a + bxi) - (a + bx) = b(x1 - x) for each i. In other
words, yc = bxc. This completes the proof.

    Properties (2) and (3) in Theorem 5.7.1 show that r(x, y) = 1 means that there is a linear relation
with positive slope between the paired data (so large x values are paired with large y values). Similarly,

r(x, y) = -1 means that there is a linear relation with negative slope between the paired data (so small x
values are paired with small y values). This is borne out in the two scatter diagrams above.

We conclude by using the dot product to derive some useful formulas for computing variances and

correlation coefficients. Given samples x = x1 x2 · · · xn and y = y1 y2 · · · yn , the key ob-

servation is the following formula:

                                                    xc · yc = x · y - nx y                                      (5.7)

Indeed, remembering that x and y are scalars:

                                      xc · yc = (x - x1) · (y - y1)
                                             = x · y - x · (y1) - (x1) · y + (x1)(y1)
                                             = x · y - y(x · 1) - x(1 · y) + xy(1 · 1)
                                             = x · y - y(nx) - x(ny) + x y(n)
                                             = x · y - nx y

Taking  y  =  x  in  (5.7)  gives  a  formula  for  the  variance  s2  =   1   xc  2 of x.
                                                                          n-1
                                                                    x
                                                                   . . An Application to Correlation and Variance

   Theorem: Variance Formula

   If  x  is  a   sample      vector,   then   s2  =   1     xc 2 - nx2 .
                                                      n-1
                                                x

We also get a convenient formula for the correlation coefficient, r = r(x, y) = xxc·yc c yc . Moreover, (5.7)
and   the  fact     that  s2      1         2 give:
                              =  n-1    xc
                           x

   Theorem: Correlation Formula
   If x and y are sample vectors, then

                                              r = r(x, y) = x · y - nx y
                                                                (n - 1)sxsy

      Finally, we give a method that simplifies the computations of variances and correlations.

   Theorem: Data Scaling
   Let x = x1 x2 · · · xn and y = y1 y2 · · · yn be sample vectors. Given constants a, b,
   c, and d, consider new samples z = z1 z2 · · · zn and w = w1 w2 · · · wn where
   zi = a + bxi, for each i and wi = c + dyi for each i. Then:

       a. z = a + bx

       b.     s2    =  b2s2x ,  so  sz  =  |b|sx

               z

       c. If b and d have the same sign, then r(x, y) = r(z, w).

The verification is left as an exercise. For example, if x = 101 98 103 99 100 97 , subtracting

100 yields z =            1      -2     3   -1     0  -3     .  A  routine  calculation  shows  that  z  =  -1    and  s2  =  14 ,  so

                 1                      s2     14                                                              3        z     3
                 3                             3
x  =  100  -        =  99.67,    and     z  =      =  4.67.
 8 Vector Space Rn

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.
                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.
                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!
Chapter 6

                Vector Spaces

In this chapter we introduce vector spaces in full generality. The reader will notice some similarity with
the discussion of the space Rn in Chapter 5. In fact much of the present material has been developed in
that context, and there is some repetition. However, Chapter 6 deals with the notion of an abstract vector
space, a concept that will be new to most readers. It turns out that there are many systems in which a
natural addition and scalar multiplication are defined and satisfy the usual rules familiar from Rn. The
study of abstract vector spaces is a way to deal with all these examples simultaneously. The new aspect is
that we are dealing with an abstract system in which all we know about the vectors is that they are objects
that can be added and multiplied by a scalar and satisfy rules familiar from Rn.

    The novel thing is the abstraction. Getting used to this new conceptual level is facilitated by the work
done in Chapter 5: First, the vector manipulations are familiar, giving the reader more time to become
accustomed to the abstract setting; and, second, the mental images developed in the concrete setting of Rn
serve as an aid to doing many of the exercises in Chapter 6.

    The concept of a vector space was first introduced in 1844 by the German mathematician Hermann
Grassmann (1809-1877), but his work did not receive the attention it deserved. It was not until 1888 that
the Italian mathematician Guiseppe Peano (1858-1932) clarified Grassmann's work in his book Calcolo
Geometrico and gave the vector space axioms in their present form. Vector spaces became established with
the work of the Polish mathematician Stephan Banach (1892-1945), and the idea was finally accepted in
1918 when Hermann Weyl (1885-1955) used it in his widely read book Raum-Zeit-Materie ("Space-Time-
Matter"), an introduction to the general theory of relativity.

           281
 8 Vector Spaces

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

6. Examples and Basic Properties

Many mathematical entities have the property that they can be added and multiplied by a number. Numbers
themselves have this property, as do m × n matrices: The sum of two such matrices is again m × n as is any
scalar multiple of such a matrix. Polynomials are another familiar example, as are the geometric vectors
in Chapter 4. It turns out that there are many other types of mathematical objects that can be added and
multiplied by a scalar, and the general study of such systems is introduced in this chapter. Remarkably,
much of what we could say in Chapter 5 about the dimension of subspaces in Rn can be formulated in this
generality.

   Definition 6.1 Vector Spaces
   A vector space consists of a nonempty set V of objects (called vectors) that can be added, that can
   be multiplied by a real number (called a scalar in this context), and for which certain axioms
   hold.1If v and w are two vectors in V , their sum is expressed as v + w, and the scalar product of v
   by a real number a is denoted as av. These operations are called vector addition and scalar
   multiplication, respectively, and the following axioms are assumed to hold.

    1The scalars will usually be real numbers, but they could be complex numbers, or elements of an algebraic system called a
field. Another example is the field Q of rational numbers. We will look briefly at finite fields in Section 8.8.
                                                                            6. . Examples and Basic Properties 8

Axioms for vector addition

 A1. If u and v are in V , then u + v is in V .

 A2. u + v = v + u for all u and v in V .
 A3. u + (v + w) = (u + v) + w for all u, v, and w in V .

 A4. An element 0 in V exists such that v + 0 = v = 0 + v for every v in V .
 A5. For each v in V , an element -v in V exists such that -v + v = 0 and v + (-v) = 0.

Axioms for scalar multiplication

  S1. If v is in V , then av is in V for all a in R.

  S2. a(v + w) = av + aw for all v and w in V and all a in R.

  S3. (a + b)v = av + bv for all v in V and all a and b in R.

  S4. a(bv) = (ab)v for all v in V and all a and b in R.
  S5. 1v = v for all v in V .

The content of axioms A1 and S1 is described by saying that V is closed under vector addition and scalar
multiplication. The element 0 in axiom A4 is called the zero vector, and the vector -v in axiom A5 is
called the negative of v.

    The rules of matrix arithmetic, when applied to Rn, give

   Example 6.1.1
   Rn is a vector space using matrix addition and scalar multiplication.2

    It is important to realize that, in a general vector space, the vectors need not be n-tuples as in Rn. They
can be any kind of objects at all as long as the addition and scalar multiplication are defined and the axioms
are satisfied. The following examples illustrate the diversity of the concept.

    The space Rn consists of special types of matrices. More generally, let Mmn denote the set of all m × n
matrices with real entries. Then Theorem 2.1.1 gives:

   Example 6.1.2
   The set Mmn of all m × n matrices is a vector space using matrix addition and scalar multiplication.
   The zero element in this vector space is the zero matrix of size m × n, and the vector space negative
   of a matrix (required by axiom A5) is the usual matrix negative discussed in Section 2.1. Note that
   Mmn is just Rmn in different notation.

In Chapter 5 we identified many important subspaces of Rn such as im A and null A for a matrix A. These
are all vector spaces.

    2We will usually write the vectors in Rn as n-tuples. However, if it is convenient, we will sometimes denote them as rows
or columns.
 8 Vector Spaces

   Example 6.1.3
   Show that every subspace of Rn is a vector space in its own right using the addition and scalar
   multiplication of Rn.

   Solution. Axioms A1 and S1 are two of the defining conditions for a subspace U of Rn (see
   Section 5.1). The other eight axioms for a vector space are inherited from Rn. For example, if x
   and y are in U and a is a scalar, then a(x + y) = ax + ay because x and y are in Rn. This shows that
   axiom S2 holds for U ; similarly, the other axioms also hold for U .

   Example 6.1.4
   Let V denote the set of all ordered pairs (x, y) and define addition in V as in R2. However, define a
   new scalar multiplication in V by

                                                    a(x, y) = (ay, ax)
   Determine if V is a vector space with these operations.

   Solution. Axioms A1 to A5 are valid for V because they hold for matrices. Also a(x, y) = (ay, ax)
   is again in V , so axiom S1 holds. To verify axiom S2, let v = (x, y) and w = (x1, y1) be typical
   elements in V and compute

                             a(v + w) = a(x + x1, y + y1) = (a(y + y1), a(x + x1))
                              av + aw = (ay, ax) + (ay1, ax1) = (ay + ay1, ax + ax1)

   Because these are equal, axiom S2 holds. Similarly, the reader can verify that axiom S3 holds.
   However, axiom S4 fails because

                                         a(b(x, y)) = a(by, bx) = (abx, aby)

   need not equal ab(x, y) = (aby, abx). Hence, V is not a vector space. (In fact, axiom S5 also fails.)

    Sets of polynomials provide another important source of examples of vector spaces, so we review some
basic facts. A polynomial in an indeterminate x is an expression

                                          p(x) = a0 + a1x + a2x2 + · · · + anxn

where a0, a1, a2, . . . , an are real numbers called the coefficients of the polynomial. If all the coefficients
are zero, the polynomial is called the zero polynomial and is denoted simply as 0. If p(x) = 0, the
highest power of x with a nonzero coefficient is called the degree of p(x) denoted as deg p(x). The
coefficient itself is called the leading coefficient of p(x). Hence deg (3 + 5x) = 1, deg (1 + x + x2) = 2,
and deg (4) = 0. (The degree of the zero polynomial is not defined.)

    Let P denote the set of all polynomials and suppose that

                                              p(x) = a0 + a1x + a2x2 + · · ·
                                              q(x) = b0 + b1x + b2x2 + · · ·
                                                                            6. . Examples and Basic Properties 8

are two polynomials in P (possibly of different degrees). Then p(x) and q(x) are called equal [written
p(x) = q(x)] if and only if all the corresponding coefficients are equal--that is, a0 = b0, a1 = b1, a2 = b2,
and so on. In particular, a0 + a1x + a2x2 + · · · = 0 means that a0 = 0, a1 = 0, a2 = 0, . . . , and this is the
reason for calling x an indeterminate. The set P has an addition and scalar multiplication defined on it as
follows: if p(x) and q(x) are as before and a is a real number,

                              p(x) + q(x) = (a0 + b0) + (a1 + b1)x + (a2 + b2)x2 + · · ·
                                    ap(x) = aa0 + (aa1)x + (aa2)x2 + · · ·

Evidently, these are again polynomials, so P is closed under these operations, called pointwise addition
and scalar multiplication. The other vector space axioms are easily verified, and we have

   Example 6.1.5
   The set P of all polynomials is a vector space with the foregoing addition and scalar multiplication.
   The zero vector is the zero polynomial, and the negative of a polynomial
    p(x) = a0 + a1x + a2x2 + . . . is the polynomial -p(x) = -a0 - a1x - a2x2 - . . . obtained by
   negating all the coefficients.

There is another vector space of polynomials that will be referred to later.

   Example 6.1.6
   Given n  1, let Pn denote the set of all polynomials of degree at most n, together with the zero
   polynomial. That is

                          Pn = {a0 + a1x + a2x2 + · · · + anxn | a0, a1, a2, . . . , an in R}.

   Then Pn is a vector space. Indeed, sums and scalar multiples of polynomials in Pn are again in Pn,
   and the other vector space axioms are inherited from P. In particular, the zero vector and the
   negative of a polynomial in Pn are the same as those in P.

    If a and b are real numbers and a < b, the interval [a, b] is defined to be the set of all real numbers
x such that a  x  b. A (real-valued) function f on [a, b] is a rule that associates to every number x in
[a, b] a real number denoted f (x). The rule is frequently specified by giving a formula for f (x) in terms of
x. For example, f (x) = 2x, f (x) = sin x, and f (x) = x2 + 1 are familiar functions. In fact, every polynomial
p(x) can be regarded as the formula for a function p.

   y                                The set of all functions on [a, b] is denoted F[a, b]. Two functions
                                f and g in F[a, b] are equal if f (x) = g(x) for every x in [a, b], and we
                y = x2 = f (x)  describe this by saying that f and g have the same action. Note that two

                                polynomials are equal in P (defined prior to Example 6.1.5) if and only if

1                               they are equal as functions.

      y = f (x) + g(x)          If f and g are two functions in F[a, b], and if r is a real number, define
          = x2 - x
                                x the sum f + g and the scalar product r f by
O  1

                                ( f + g)(x) = f (x) + g(x) for each x in [a, b]

      y = -x = g(x)
86 Vector Spaces

                  (r f )(x) = r f (x)          for each x in [a, b]

    In other words, the action of f + g upon x is to associate x with the number f (x) + g(x), and r f
associates x with r f (x). The sum of f (x) = x2 and g(x) = -x is shown in the diagram. These operations
on F[a, b] are called pointwise addition and scalar multiplication of functions and they are the usual
operations familiar from elementary algebra and calculus.

Example 6.1.7

The set F[a, b] of all functions on the interval [a, b] is a vector space using pointwise addition and
scalar multiplication. The zero function (in axiom A4), denoted 0, is the constant function defined
by

                                         0(x) = 0 for each x in [a, b]

The negative of a function f is denoted - f and has action defined by

                                    (- f )(x) = - f (x) for each x in [a, b]

Axioms A1 and S1 are clearly satisfied because, if f and g are functions on [a, b], then f + g and
r f are again such functions. The verification of the remaining axioms is left as Exercise ??.

    Other examples of vector spaces will appear later, but these are sufficiently varied to indicate the scope
of the concept and to illustrate the properties of vector spaces to be discussed. With such a variety of
examples, it may come as a surprise that a well-developed theory of vector spaces exists. That is, many
properties can be shown to hold for all vector spaces and hence hold in every example. Such properties
are called theorems and can be deduced from the axioms. Here is an important example.

Theorem 6.1.1: Cancellation
Let u, v, and w be vectors in a vector space V . If v + u = v + w, then u = w.

Proof. We are given v + u = v + w. If these were numbers instead of vectors, we would simply subtract v
from both sides of the equation to obtain u = w. This can be accomplished with vectors by adding -v to
both sides of the equation. The steps (using only the axioms) are as follows:

                           v+u = v+w                                            (axiom A5)
                  -v + (v + u) = -v + (v + w)                                   (axiom A3)
                  (-v + v) + u = (-v + v) + w                                   (axiom A5)
                                                                                (axiom A4)
                           0+u = 0+w
                                u=w

This is the desired conclusion.3

    As with many good mathematical theorems, the technique of the proof of Theorem 6.1.1 is at least as
important as the theorem itself. The idea was to mimic the well-known process of numerical subtraction

    3Observe that none of the scalar multiplication axioms are needed here.
                                                                            6. . Examples and Basic Properties 8

in a vector space V as follows: To subtract a vector v from both sides of a vector equation, we added -v
to both sides. With this in mind, we define difference u - v of two vectors in V as

                                                     u - v = u + (-v)
We shall say that this vector is the result of having subtracted v from u and, as in arithmetic, this operation
has the property given in Theorem 6.1.2.

   Theorem 6.1.2
   If u and v are vectors in a vector space V , the equation

                                                         x+v = u
   has one and only one solution x in V given by

                                                         x = u-v

Proof. The difference x = u - v is indeed a solution to the equation because (using several axioms)

                        x + v = (u - v) + v = [u + (-v)] + v = u + (-v + v) = u + 0 = u
To see that this is the only solution, suppose x1 is another solution so that x1 + v = u. Then x + v = x1 + v
(they both equal u), so x = x1 by cancellation.

    Similarly, cancellation shows that there is only one zero vector in any vector space and only one
negative of each vector (Exercises ?? and ??). Hence we speak of the zero vector and the negative of a
vector.

    The next theorem derives some basic properties of scalar multiplication that hold in every vector space,
and will be used extensively.

   Theorem 6.1.3
   Let v denote a vector in a vector space V and let a denote a real number.

       1. 0v = 0.
       2. a0 = 0.
       3. If av = 0, then either a = 0 or v = 0.
       4. (-1)v = -v.
       5. (-a)v = -(av) = a(-v).

Proof.

   1. Observe that 0v + 0v = (0 + 0)v = 0v = 0v + 0 where the first equality is by axiom S3. It follows
       that 0v = 0 by cancellation.
 88 Vector Spaces

   2. The proof is similar to that of (1), and is left as Exercise ??(a).

   3. Assume that av = 0. If a = 0, there is nothing to prove; if a = 0, we must show that v = 0. But
       a = 0 means we can scalar-multiply the equation av = 0 by the scalar 1a. The result (using (2) and
       Axioms S5 and S4) is
                                              v = 1v = 1a a v = 1a (av) = 1a 0 = 0

   4. We have -v + v = 0 by axiom A5. On the other hand,

                                      (-1)v + v = (-1)v + 1v = (-1 + 1)v = 0v = 0

       using (1) and axioms S5 and S3. Hence (-1)v + v = -v + v (because both are equal to 0), so
       (-1)v = -v by cancellation.
   5. The proof is left as Exercise ??.4

The properties in Theorem 6.1.3 are familiar for matrices; the point here is that they hold in every vector
space. It is hard to exaggerate the importance of this observation.

    Axiom A3 ensures that the sum u + (v + w) = (u + v) + w is the same however it is formed, and we
write it simply as u + v + w. Similarly, there are different ways to form any sum v1 + v2 + · · · + vn, and
Axiom A3 guarantees that they are all equal. Moreover, Axiom A2 shows that the order in which the
vectors are written does not matter (for example: u + v + w + z = z + u + w + v).

    Similarly, Axioms S2 and S3 extend. For example

                        a(u + v + w) = a [u + (v + w)] = au + a(v + w) = au + av + aw

for all a, u, v, and w. Similarly (a + b + c)v = av + bv + cv hold for all values of a, b, c, and v (verify).
More generally,

                                    a(v1 + v2 + · · · + vn) = av1 + av2 + · · · + avn
                                    (a1 + a2 + · · · + an)v = a1v + a2v + · · · + anv

hold for all n  1, all numbers a, a1, . . . , an, and all vectors, v, v1, . . . , vn. The verifications are by
induction and are left to the reader (Exercise ??). These facts--together with the axioms, Theorem 6.1.3,
and the definition of subtraction--enable us to simplify expressions involving sums of scalar multiples of
vectors by collecting like terms, expanding, and taking out common factors. This has been discussed for
the vector space of matrices in Section 2.1 (and for geometric vectors in Section 4.1); the manipulations
in an arbitrary vector space are carried out in the same way. Here is an illustration.

   Example 6.1.8
   If u, v, and w are vectors in a vector space V , simplify the expression

                             2(u + 3w) - 3(2w - v) - 3[2(2u + v - 4w) - 4(u - 2w)]

   Solution. The reduction proceeds as though u, v, and w were matrices or variables.

                             2(u + 3w) - 3(2w - v) - 3[2(2u + v - 4w) - 4(u - 2w)]
                                                                            6. . Examples and Basic Properties 8

                              = 2u + 6w - 6w + 3v - 3[4u + 2v - 8w - 4u + 8w]
                              = 2u + 3v - 3[2v]
                              = 2u + 3v - 6v
                              = 2u - 3v

    Condition (2) in Theorem 6.1.3 points to another example of a vector space.

   Example 6.1.9
   A set {0} with one element becomes a vector space if we define

                                     0 + 0 = 0 and a0 = 0 for all scalars a.
   The resulting space is called the zero vector space and is denoted {0}.

The vector space axioms are easily verified for {0}. In any vector space V , Theorem 6.1.3 shows that the
zero subspace (consisting of the zero vector of V alone) is a copy of the zero vector space.

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.
                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.
                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!
         Vector Spaces

6. Subspaces and Spanning Sets

Chapter 5 is essentially about the subspaces of Rn. We now extend this notion.

   Definition 6.2 Subspaces of a Vector Space
   If V is a vector space, a nonempty subset U  V is called a subspace of V if U is itself a vector
   space using the addition and scalar multiplication of V .

Subspaces of Rn (as defined in Section 5.1) are subspaces in the present sense by Example 6.1.3. Moreover,
the defining properties for a subspace of Rn actually characterize subspaces in general.

   Theorem 6.2.1: Subspace Test
   A subset U of a vector space is a subspace of V if and only if it satisfies the following three
   conditions:

       1. 0 lies in U where 0 is the zero vector of V .
       2. If u1 and u2 are in U , then u1 + u2 is also in U .
       3. If u is in U, then au is also in U for each scalar a.

Proof. If U is a subspace of V , then (2) and (3) hold by axioms A1 and S1 respectively, applied to the
vector space U . Since U is nonempty (it is a vector space), choose u in U . Then (1) holds because 0 = 0u
is in U by (3) and Theorem 6.1.3.

    Conversely, if (1), (2), and (3) hold, then axioms A1 and S1 hold because of (2) and (3), and axioms
A2, A3, S2, S3, S4, and S5 hold in U because they hold in V . Axiom A4 holds because the zero vector 0
of V is actually in U by (1), and so serves as the zero of U . Finally, given u in U , then its negative -u in V
is again in U by (3) because -u = (-1)u (again using Theorem 6.1.3). Hence -u serves as the negative
of u in U.
Note that the proof of Theorem 6.2.1 shows that if U is a subspace of V , then U and V share the same zero
vector, and that the negative of a vector in the space U is the same as its negative in V .

   Example 6.2.1
   If V is any vector space, show that {0} and V are subspaces of V .

   Solution. U = V clearly satisfies the conditions of the subspace test. As to U = {0}, it satisfies the
   conditions because 0 + 0 = 0 and a0 = 0 for all a in R.

    The vector space {0} is called the zero subspace of V .
                                                                             6. . Subspaces and Spanning Sets

   Example 6.2.2
   Let v be a vector in a vector space V . Show that the set

                                                    Rv = {av | a in R}

   of all scalar multiples of v is a subspace of V .

   Solution. Because 0 = 0v, it is clear that 0 lies in Rv. Given two vectors av and a1v in Rv, their
   sum av + a1v = (a + a1)v is also a scalar multiple of v and so lies in Rv. Hence Rv is closed under
   addition. Finally, given av, r(av) = (ra)v lies in Rv for all r  R, so Rv is closed under scalar
   multiplication. Hence the subspace test applies.

In particular, given d = 0 in R3, Rd is the line through the origin with direction vector d.
    The space Rv in Example 6.2.2 is described by giving the form of each vector in Rv. The next example

describes a subset U of the space Mnn by giving a condition that each matrix of U must satisfy.

   Example 6.2.3
   Let A be a fixed matrix in Mnn. Show that U = {X in Mnn | AX = X A} is a subspace of Mnn.
   Solution. If 0 is the n × n zero matrix, then A0 = 0A, so 0 satisfies the condition for membership in
   U . Next suppose that X and X1 lie in U so that AX = X A and AX1 = X1A. Then

                                 A(X + X1) = AX + AX1 = X A + X1A = (X + X1)A
                                      A(aX ) = a(AX ) = a(X A) = (aX )A

   for all a in R, so both X + X1 and aX lie in U . Hence U is a subspace of Mnn.

    Suppose p(x) is a polynomial and a is a number. Then the number p(a) obtained by replacing x by a
in the expression for p(x) is called the evaluation of p(x) at a. For example, if p(x) = 5 - 6x + 2x2, then
the evaluation of p(x) at a = 2 is p(2) = 5 - 12 + 8 = 1. If p(a) = 0, the number a is called a root of p(x).

   Example 6.2.4
   Consider the set U of all polynomials in P that have 3 as a root:

                                               U = {p(x)  P | p(3) = 0}

   Show that U is a subspace of P.

   Solution. Clearly, the zero polynomial lies in U . Now let p(x) and q(x) lie in U so p(3) = 0 and
   q(3) = 0. We have (p + q)(x) = p(x) + q(x) for all x, so (p + q)(3) = p(3) + q(3) = 0 + 0 = 0, and
   U is closed under addition. The verification that U is closed under scalar multiplication is similar.

    Recall that the space Pn consists of all polynomials of the form
                                               a0 + a1x + a2x2 + · · · + anxn
         Vector Spaces

where a0, a1, a2, . . . , an are real numbers, and so is closed under the addition and scalar multiplication in
P. Moreover, the zero polynomial is included in Pn. Thus the subspace test gives Example 6.2.5.

   Example 6.2.5
   Pn is a subspace of P for each n  0.

    The next example involves the notion of the derivative f  of a function f . (If the reader is not fa-
miliar with calculus, this example may be omitted.) A function f defined on the interval [a, b] is called
differentiable if the derivative f (r) exists at every r in [a, b].

   Example 6.2.6
   Show that the subset D[a, b] of all differentiable functions on [a, b] is a subspace of the vector
   space F[a, b] of all functions on [a, b].
   Solution. The derivative of any constant function is the constant function 0; in particular, 0 itself is
   differentiable and so lies in D[a, b]. If f and g both lie in D[a, b] (so that f  and g exist), then it is
   a theorem of calculus that f + g and r f are both differentiable for any r  R. In fact,
   ( f + g) = f  + g and (r f ) = r f , so both lie in D[a, b]. This shows that D[a, b] is a subspace of
   F[a, b].

Linear Combinations and Spanning Sets

One of the crucial concept in linear algebra is that of a span of a set of vectors, obtained by considering
all possible linear combinations of vectors in that set.

   Definition 6.3 Linear Combinations and Spanning
   Let {v1, v2, . . . , vn} be a set of vectors in a vector space V . As in Rn, a vector v is called a linear
   combination of the vectors v1, v2, . . . , vn if it can be expressed in the form

                                             v = a1v1 + a2v2 + · · · + anvn
   where a1, a2, . . . , an are scalars, called the coefficients of v1, v2, . . . , vn. The set of all linear
   combinations of these vectors is called their span, and is denoted by

                           span {v1, v2, . . . , vn} = {a1v1 + a2v2 + · · · + anvn | ai in R}

If it happens that V = span {v1, v2, . . . , vn}, these vectors are called a spanning set for V . For example,
the span of two vectors v and w is the set

                                        span {v, w} = {sv + tw | s and t in R}
of all sums of scalar multiples of these vectors.
                                                                             6. . Subspaces and Spanning Sets

   Example 6.2.7
   Consider the vectors p1 = 1 + x + 4x2 and p2 = 1 + 5x + x2 in P2. Determine whether p1 and p2 lie
   in span {1 + 2x - x2, 3 + 5x + 2x2}.

   Solution. For p1, we want to determine if s and t exist such that
                                         p1 = s(1 + 2x - x2) + t(3 + 5x + 2x2)

   Equating coefficients of powers of x (where x0 = 1) gives

                                  1 = s + 3t, 1 = 2s + 5t, and 4 = -s + 2t

   These equations have the solution s = -2 and t = 1, so p1 is indeed in
   span {1 + 2x - x2, 3 + 5x + 2x2}.
   Turning to p2 = 1 + 5x + x2, we are looking for s and t such that

                                         p2 = s(1 + 2x - x2) + t(3 + 5x + 2x2)

   Again equating coefficients of powers of x gives equations 1 = s + 3t, 5 = 2s + 5t, and 1 = -s + 2t.
   But in this case there is no solution, so p2 is not in span {1 + 2x - x2, 3 + 5x + 2x2}.

    We saw in Example 5.1.6 that Rm = span {e1, e2, . . . , em} where the vectors e1, e2, . . . , em are the
columns of the m × m identity matrix. Of course Rm = Mm1 is the set of all m × 1 matrices, and there is
an analogous spanning set for each space Mmn. For example, each 2 × 2 matrix has the form

                            a b c d = a 1 0 0 0 + b 0 1 0 0 + c 0 0 1 0 + d 0 0 0 1
so

                           M22 = span 1 0 0 0 , 0 1 0 0 , 0 0 1 0 , 0 0 0 1
Similarly, we obtain

   Example 6.2.8
   Mmn is the span of the set of all m × n matrices with exactly one entry equal to 1, and all other
   entries zero.

    The fact that every polynomial in Pn has the form a0 + a1x + a2x2 + · · · + anxn where each ai is in R
shows that

   Example 6.2.9
   Pn = span {1, x, x2, . . . , xn}.

In Example 6.2.2 we saw that span {v} = {av | a in R} = Rv is a subspace for any vector v in a vector
space V . More generally, the span of any set of vectors is a subspace. In fact, the proof of Theorem 5.1.1
goes through to prove:
    Vector Spaces

Theorem 6.2.2
Let U = span {v1, v2, . . . , vn} in a vector space V . Then:

   1. U is a subspace of V containing each of v1, v2, . . . , vn.

   2. U is the "smallest" subspace containing these vectors in the sense that any subspace that
       contains each of v1, v2, . . . , vn must contain U .

    Here is how condition 2 in Theorem 6.2.2 is used. Given vectors v1, . . . , vk in a vector space V and a
subspace U  V , then:

                                        span {v1, . . . , vn}  U  each vi  U

The following examples illustrate this.

Example 6.2.10
Show that P3 = span {x2 + x3, x, 2x2 + 1, 3}.

Solution. Write U = span {x2 + x3, x, 2x2 + 1, 3}. Then U  P3, and we use the fact that
                     x2,  x3}                                                         1
P3  =  span {1,  x,            to  show  that  P3        U.  In  fact,  x  and  1  =  3  ·3  clearly  lie  in  U.  But  then

successively,             x2          1 [(2x2                           x3 = (x2 + x3) - x2

                                   =  2        +  1)  -  1]  and

also lie in U . Hence P3  U by Theorem 6.2.2.

Example 6.2.11
Let u and v be two vectors in a vector space V . Show that

                                      span {u, v} = span {u + 2v, u - v}

Solution. We have span {u + 2v, u - v}  span {u, v} by Theorem 6.2.2 because both u + 2v and
u - v lie in span {u, v}. On the other hand,

                      u = 13 (u + 2v) + 23 (u - v) and v = 31 (u + 2v) - 13 (u - v)
so span {u, v}  span {u + 2v, u - v}, again by Theorem 6.2.2.
                                                                      6. . Linear Independence and Dimension

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.
                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.
                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

6. Linear Independence and Dimension

In addition to the span of a set of vectors covered in the previous section, another central concept in
linear algebra is that of an independent set of vectors, meaning roughly that the set does not contain any
redundant vector.

   Definition 6.4 Linear Independence and Dependence
   As in Rn, a set of vectors {v1, v2, . . . , vn} in a vector space V is called linearly independent (or
   simply independent) if it satisfies the following condition:

                      If s1v1 + s2v2 + · · · + snvn = 0, then s1 = s2 = · · · = sn = 0.
   A set of vectors that is not linearly independent is said to be linearly dependent (or simply
   dependent).

The trivial linear combination of the vectors v1, v2, . . . , vn is the one with every coefficient zero:
                                                   0v1 + 0v2 + · · · + 0vn

This is obviously one way of expressing 0 as a linear combination of the vectors v1, v2, . . . , vn, and they
are linearly independent when it is the only way.
6 Vector Spaces

Example 6.3.1
Show that {1 + x, 3x + x2, 2 + x - x2} is independent in P2.
Solution. Suppose a linear combination of these polynomials vanishes.

                                  s1(1 + x) + s2(3x + x2) + s3(2 + x - x2) = 0
Equating the coefficients of 1, x, and x2 gives a set of linear equations.

                                                 s1 + + 2s3 = 0
                                                 s1 + 3s2 + s3 = 0

                                                         s2 - s3 = 0
The only solution is s1 = s2 = s3 = 0.

Example 6.3.2

Show that {sin x, cos x} is independent in the vector space F[0, 2] of functions defined on the
interval [0, 2].

Solution. Suppose that a linear combination of these functions vanishes.

                                                  s1(sin x) + s2(cos x) = 0

This must hold for all values of x in [0, 2] (by the definition of equality in F[0, 2]). Taking                             
                                                                                                                            2
x  =  0  yields  s2  =  0  (because     sin 0  =  0  and  cos 0  =  1).  Similarly,  s1  =  0  follows  from  taking  x  =

(because  sin        =  1  and  cos     =  0).
                 2                   2

Example 6.3.3
Suppose that {u, v} is an independent set in a vector space V . Show that {u + 2v, u - 3v} is also
independent.

Solution. Suppose a linear combination of u + 2v and u - 3v vanishes:

                                            s(u + 2v) + t(u - 3v) = 0

We must deduce that s = t = 0. Collecting terms involving u and v gives

                                            (s + t)u + (2s - 3t)v = 0

Because {u, v} is independent, this yields linear equations s + t = 0 and 2s - 3t = 0. The only
solution is s = t = 0.
                                                                  6. . Linear Independence and Dimension

Example 6.3.4
Show that any set of polynomials of distinct degrees is independent.

Solution. Let p1, p2, . . . , pm be polynomials where deg (pi) = di. By relabelling if necessary, we
may assume that d1 > d2 > · · · > dm. Suppose that a linear combination vanishes:

                                          t1 p1 + t2 p2 + · · · + tm pm = 0

where each ti is in R. As deg (p1) = d1, let axd1 be the term in p1 of highest degree, where a = 0.
Since d1 > d2 > · · · > dm, it follows that t1axd1 is the only term of degree d1 in the linear
combination t1 p1 + t2 p2 + · · · + tm pm = 0. This means that t1axd1 = 0, whence t1a = 0, hence
t1 = 0 (because a = 0). But then t2 p2 + · · · + tm pm = 0 so we can repeat the argument to show that
t2 = 0. Continuing, we obtain ti = 0 for each i, as desired.

Example 6.3.5
Suppose that A is an n × n matrix such that Ak = 0 but Ak-1 = 0. Show that
B = {I, A, A2, . . . , Ak-1} is independent in Mnn.
Solution. Suppose r0I + r1A + r2A2 + · · · + rk-1Ak-1 = 0. Multiply by Ak-1:

                                r0Ak-1 + r1Ak + r2Ak+1 + · · · + rk-1A2k-2 = 0
Since Ak = 0, all the higher powers are zero, so this becomes r0Ak-1 = 0. But Ak-1 = 0, so r0 = 0,
and we have r1A1 + r2A2 + · · · + rk-1Ak-1 = 0. Now multiply by Ak-2 to conclude that r1 = 0.
Continuing, we obtain ri = 0 for each i, so B is independent.

The next example collects several useful properties of independence for reference.

Example 6.3.6
Let V denote a vector space.

   1. If v = 0 in V , then {v} is an independent set.
   2. No independent set of vectors in V can contain the zero vector.

Solution.

1.  Let tv = 0, t  in R.  If t = 0, then v = 1v =  1  (t  v)  =  1  0  =  0,  contrary  to  assumption.  So  t  =  0.
                                                   t             t

2. If {v1, v2, . . . , vk} is independent and (say) v2 = 0, then 0v1 + 1v2 + · · · + 0vk = 0 is a
   nontrivial linear combination that vanishes, contrary to the independence of
   {v1, v2, . . . , vk}.
   8 Vector Spaces

    A set of vectors is independent if 0 is a linear combination in a unique way. The following theorem
shows that every linear combination of these vectors has uniquely determined coefficients, and so extends
Theorem 5.2.1.

    Theorem 6.3.1
    Let {v1, v2, . . . , vn} be a linearly independent set of vectors in a vector space V . If a vector v has
    two (ostensibly different) representations

                                             v = s1v1 + s2v2 + · · · + snvn
                                             v = t1v1 + t2v2 + · · · + tnvn
    as linear combinations of these vectors, then s1 = t1, s2 = t2, . . . , sn = tn. In other words, every
    vector in V can be written in a unique way as a linear combination of the vi.

Proof. Subtracting the equations given in the theorem gives

                                   (s1 - t1)v1 + (s2 - t2)v2 + · · · + (sn - tn)vn = 0

The independence of {v1, v2, . . . , vn} gives si - ti = 0 for each i, as required.
    The following theorem extends (and proves) Theorem 5.2.4, and is one of the most useful results in

linear algebra.

    Theorem 6.3.2: Fundamental Theorem
    Suppose a vector space V can be spanned by n vectors. If a set of m vectors in V is linearly
    independent, then m  n.

Proof. Let V = span {v1, v2, . . . , vn}, and suppose that {u1, u2, . . . , um} is an independent set in V .
Then u1 = a1v1 + a2v2 + · · · + anvn where each ai is in R. As u1 = 0 (Example 6.3.6), not all of the ai are
zero, say a1 = 0 (after relabelling the vi). Then V = span {u1, v2, v3, . . . , vn} as the reader can verify.
Hence, write u2 = b1u1 + c2v2 + c3v3 + · · · + cnvn. Then some ci = 0 because {u1, u2} is independent;
so, as before, V = span {u1, u2, v3, . . . , vn}, again after possible relabelling of the vi. If m > n, this
procedure continues until all the vectors vi are replaced by the vectors u1, u2, . . . , un. In particular,
V = span {u1, u2, . . . , un}. But then un+1 is a linear combination of u1, u2, . . . , un contrary to the
independence of the ui. Hence, the assumption m > n cannot be valid, so m  n and the theorem is proved.

    If V = span {v1, v2, . . . , vn}, and if {u1, u2, . . . , um} is an independent set in V , the above proof
shows not only that m  n but also that m of the (spanning) vectors v1, v2, . . . , vn can be replaced by
the (independent) vectors u1, u2, . . . , um and the resulting set will still span V . In this form the result is
called the Steinitz Exchange Lemma.

    Definition 6.5 Basis of a Vector Space
    As in Rn, a set {v1, v2, . . . , vn} of vectors in a vector space V is called a basis of V if it satisfies
    the following two conditions:
                                                                      6. . Linear Independence and Dimension

       1. {v1, v2, . . . , vn} is linearly independent
       2. V = span {v1, v2, . . . , vn}

Thus if a set of vectors {v1, v2, . . . , vn} is a basis, then every vector in V can be written as a linear
combination of these vectors in a unique way (Theorem 6.3.1). But even more is true: Any two (finite)
bases of V contain the same number of vectors.

   Theorem 6.3.3: Invariance Theorem
   Let {v1, v2, . . . , vn} and {u1, u2, . . . , um} be two bases of a vector space V . Then n = m.

Proof. Because V = span {v1, v2, . . . , vn} and {u1, u2, . . . , um} is independent, it follows from Theo-
rem 6.3.2 that m  n. Similarly n  m, so n = m, as asserted.

    Theorem 6.3.3 guarantees that no matter which basis of V is chosen it contains the same number of
vectors as any other basis. Hence there is no ambiguity about the following definition.

   Definition 6.6 Dimension of a Vector Space
   If {v1, v2, . . . , vn} is a basis of the nonzero vector space V , the number n of vectors in the basis is
   called the dimension of V , and we write

                                                         dim V = n

   The zero vector space {0} is defined to have dimension 0:

                                                       dim {0} = 0

In our discussion to this point we have always assumed that a basis is nonempty and hence that the di-
mension of the space is at least 1. However, the zero space {0} has no basis (by Example 6.3.6) so our
insistence that dim {0} = 0 amounts to saying that the empty set of vectors is a basis of {0}. Thus the
statement that "the dimension of a vector space is the number of vectors in any basis" holds even for the
zero space.

    We saw in Example 5.2.10 that dim (Rn) = n due to the standard basis {e1, e2, . . . , en} of Rn. In
Example 6.3.7 below, similar considerations apply to the space Mmn of all m × n matrices; the verifications
are left to the reader.

   Example 6.3.7
   The space Mmn has dimension mn, and one basis consists of all m × n matrices with exactly one
   entry equal to 1 and all other entries equal to 0. We call this the standard basis of Mmn.
     Vector Spaces

Example 6.3.8
Show that dim Pn = n + 1 and that {1, x, x2, . . . , xn} is a basis, called the standard basis of Pn.

Solution. Each polynomial p(x) = a0 + a1x + · · · + anxn in Pn is clearly a linear combination of
1, x, . . . , xn, so Pn = span {1, x, . . . , xn}. However, if a linear combination of these vectors
vanishes, a01 + a1x + · · · + anxn = 0, then a0 = a1 = · · · = an = 0 because x is an indeterminate. So
{1, x, . . . , xn} is linearly independent and hence is a basis containing n + 1 vectors. Thus,
dim (Pn) = n + 1.

Example 6.3.9
If v = 0 is any nonzero vector in a vector space V , show that span {v} = Rv has dimension 1.

Solution. {v} clearly spans Rv, and it is linearly independent by Example 6.3.6. Hence {v} is a
basis of Rv, and so dim Rv = 1.

Example 6.3.10   and consider the subspace
Let A = 1 1 0 0

                 U = {X in M22 | AX = X A}

of M22. Show that dim U = 2 and find a basis of U .

Solution. It was shown in Example 6.2.3 that U is a subspace for any choice of the matrix A. In the
present case, if X = x y is in U , the condition AX = X A gives z = 0 and x = y + w. Hence

                           zw
each matrix X in U can be written

                               X = y + w y 0 w = y 1 1 0 0 + w 1 0 0 1

so U = span B where B = 1 1 0 0 , 1 0 0 1            . Moreover, the set B is linearly independent
(verify this), so it is a basis of U and dim U = 2.

Example 6.3.11
Show that the set V of all symmetric 2 × 2 matrices is a vector space, and find the dimension of V .
Solution. A matrix A is symmetric if AT = A. If A and B lie in V , then

                        (A + B)T = AT + BT = A + B and (kA)T = kAT = kA
using Theorem 2.1.2. Hence A + B and kA are also symmetric. As the 2 × 2 zero matrix is also in
                                                                      6. . Linear Independence and Dimension

   V , this shows that V is a vector space (being a subspace of M22). Now a matrix A is symmetric
   when entries directly across the main diagonal are equal, so each 2 × 2 symmetric matrix has the
   form

                                    a c c b = a 1 0 0 0 + b 0 0 0 1 + c 0 1 1 0
   Hence the set B = 1 0 0 0 , 0 0 0 1 , 0 1 1 0 spans V , and the reader can verify that B is
   linearly independent. Thus B is a basis of V , so dim V = 3.

    It is frequently convenient to alter a basis by multiplying each basis vector by a nonzero scalar. The
next example shows that this always produces another basis. The proof is left as Exercise ??.

   Example 6.3.12
   Let B = {v1, v2, . . . , vn} be nonzero vectors in a vector space V . Given nonzero scalars
   a1, a2, . . . , an, write D = {a1v1, a2v2, . . . , anvn}. If B is independent or spans V , the same is true
   of D. In particular, if B is a basis of V , so also is D.

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.
                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!
Vector Spaces

6. Finite Dimensional Spaces

Up to this point, we have had no guarantee that an arbitrary vector space has a basis--and hence no
guarantee that one can speak at all of the dimension of V . However, Theorem 6.4.1 will show that any
space that is spanned by a finite set of vectors has a (finite) basis: The proof requires the following basic
lemma, of interest in itself, that gives a way to enlarge a given independent set of vectors.

   Lemma 6.4.1: Independent Lemma

   Let {v1, v2, . . . , vk} be an independent set of vectors in a vector space V . If u  V but5
   u / span {v1, v2, . . . , vk}, then {u, v1, v2, . . . , vk} is also independent.

Proof. Let tu + t1v1 + t2v2 + · · · + tkvk = 0; we must show that all the coefficients are zero. First, t = 0
because, otherwise, u = -t1t v1 - tt2 v2 - · · · - ttk vk is in span {v1, v2, . . . , vk}, contrary to our assumption.
Hence t = 0. But then t1v1 + t2v2 + · · · + tkvk = 0 so the rest of the ti are zero by the independence of
{v1, v2, . . . , vk}. This is what we wanted.

z Note that the converse of Lemma 6.4.1 is also true: if

u                           {u, v1, v2, . . . , vk} is independent, then u is not in
                            span {v1, v2, . . . , vk}.

                   v1           As an illustration, suppose that {v1, v2} is inde-
                            pendent in R3. Then v1 and v2 are not parallel, so
                        v2  span {v1, v2} is a plane through the origin (shaded in
                            the diagram). By Lemma 6.4.1, u is not in this plane if
  0
                        y   and only if {u, v1, v2} is independent.

x

           span {v1, v2}

Definition 6.7 Finite Dimensional and Infinite Dimensional Vector Spaces

A vector space V is called finite dimensional if it is spanned by a finite set of vectors. Otherwise,
V is called infinite dimensional.

Thus the zero vector space {0} is finite dimensional because {0} is a spanning set.

Lemma 6.4.2
Let V be a finite dimensional vector space. If U is any subspace of V , then any independent subset
of U can be enlarged to a finite basis of U.

Proof. Suppose that I is an independent subset of U . If span I = U then I is already a basis of U . If

span I = U , choose u1  U such that u1 / span I. Hence the set I  {u1} is independent by Lemma 6.4.1.
If span (I  {u1}) = U we are done; otherwise choose u2  U such that u2 / span (I  {u1}). Hence
I  {u1, u2} is independent, and the process continues. We claim that a basis of U will be reached
eventually. Indeed, if no basis of U is ever reached, the process creates arbitrarily large independent sets

in V . But this is impossible by the fundamental theorem because V is finite dimensional and so is spanned

by a finite set of vectors.

5If X is a set, we write a  X to indicate that a is an element of the set X. If a is not an element of X, we write a / X.
                                                              6. . Finite Dimensional Spaces

Theorem 6.4.1
Let V be a finite dimensional vector space spanned by m vectors.

   1. V has a finite basis, and dim V  m.
   2. Every independent set of vectors in V can be enlarged to a basis of V by adding vectors from

       any fixed basis of V .

   3. If U is a subspace of V , then

          a. U is finite dimensional and dim U  dim V .
          b. If dim U = dim V then U = V .

Proof.

   1. If V = {0}, then V has an empty basis and dim V = 0  m. Otherwise, let v = 0 be a vector in V .
       Then {v} is independent, so (1) follows from Lemma 6.4.2 with U = V .

   2. We refine the proof of Lemma 6.4.2. Fix a basis B of V and let I be an independent subset of V .
       If span I = V then I is already a basis of V . If span I = V , then B is not contained in I (because
       B spans V ). Hence choose b1  B such that b1 / span I. Hence the set I  {b1} is independent by
       Lemma 6.4.1. If span (I  {b1}) = V we are done; otherwise a similar argument shows that (I 
       {b1, b2}) is independent for some b2  B. Continue this process. As in the proof of Lemma 6.4.2,
       a basis of V will be reached eventually.

   3. a. This is clear if U = {0}. Otherwise, let u = 0 in U . Then {u} can be enlarged to a finite basis
             B of U by Lemma 6.4.2, proving that U is finite dimensional. But B is independent in V , so
             dim U  dim V by the fundamental theorem (Theorem 6.3.2).

          b. This is clear if U = {0} because V has a basis. Otherwise, assume dim V = n. Then dim U = n,
             so U has a basis B of n vectors. If U = V , then (by Lemma 6.4.2) B can be enlarged to a basis
             of V containing more than n vectors. This contradicts the invariance theorem (Theorem 6.3.3)
             because dim V = n. So we conclude that U = V .

Theorem 6.4.1 shows that a vector space V is finite dimensional if and only if it has a finite basis (possibly
empty), and that every subspace of a finite dimensional space is again finite dimensional.

Example 6.4.1                    1 1 1 0 , 0 1 1 1 , 1 0 1 1  to a basis of M22.
Enlarge the independent set D =

Solution. The standard basis of M22 is 1 0 0 0 , 0 1 0 0 , 0 0 1 0 , 0 0 0 1 , so
including one of these in D will produce a basis by Theorem 6.4.1. In fact including any of these

matrices in D produces an independent set (verify), and hence a basis by Theorem 6.4.4. Of course
these vectors are not the only possibilities, for example, including 1 1 0 1 works as well.
         Vector Spaces

   Example 6.4.2
   Find a basis of P3 containing the independent set {1 + x, 1 + x2}.
   Solution. The standard basis of P3 is {1, x, x2, x3}, so including two of these vectors will do. If
   we use 1 and x3, the result is {1, 1 + x, 1 + x2, x3}. This is independent because the polynomials
   have distinct degrees (Example 6.3.4), and so is a basis by Theorem 6.4.1. Of course, including
   {1, x} or {1, x2} would not work!

   Example 6.4.3
   Show that the space P of all polynomials is infinite dimensional.
   Solution. For each n  1, P has a subspace Pn of dimension n + 1. Suppose P is finite dimensional,
   say dim P = m. Then dim Pn  dim P by Theorem 6.4.1, that is n + 1  m. This is impossible
   since n is arbitrary, so P must be infinite dimensional.

    The next example illustrates how (2) of Theorem 6.4.1 can be used.

   Example 6.4.4
   If c1, c2, . . . , ck are independent columns in Rn, show that they are the first k columns in some
   invertible n × n matrix.
   Solution. By Theorem 6.4.1, expand {c1, c2, . . . , ck} to a basis {c1, c2, . . . , ck, ck+1, . . . , cn} of
   Rn. Then the matrix A = c1 c2 . . . ck ck+1 . . . cn with this basis as its columns is an
   n × n matrix and it is invertible by Theorem 5.2.3.

   Theorem 6.4.2
   Let U and W be subspaces of the finite dimensional space V .

       1. If U  W , then dim U  dim W .
       2. If U  W and dim U = dim W , then U = W .

Proof. Since W is finite dimensional, (1) follows by taking V = W in part (3) of Theorem 6.4.1. Now
assume dim U = dim W = n, and let B be a basis of U . Then B is an independent set in W . If U = W ,
then span B = W , so B can be extended to an independent set of n + 1 vectors in W by Lemma 6.4.1.
This contradicts the fundamental theorem (Theorem 6.3.2) because W is spanned by dim W = n vectors.
Hence U = W , proving (2).

    Theorem 6.4.2 is very useful. This was illustrated in Example 5.2.13 for R2 and R3; here is another
example.
                                                                  6. . Finite Dimensional Spaces

         Example 6.4.5
         If a is a number, let W denote the subspace of all polynomials in Pn that have a as a root:

                                              W = {p(x) | p(x)  Pn and p(a) = 0}

         Show that {(x - a), (x - a)2, . . . , (x - a)n} is a basis of W .

         Solution. Observe first that (x - a), (x - a)2, . . . , (x - a)n are members of W , and that they are
         independent because they have distinct degrees (Example 6.3.4). Write

                                          U = span {(x - a), (x - a)2, . . . , (x - a)n}

         Then we have U  W  Pn, dim U = n, and dim Pn = n + 1. Hence n  dim W  n + 1 by
         Theorem 6.4.2. Since dim W is an integer, we must have dim W = n or dim W = n + 1. But then
         W = U or W = Pn, again by Theorem 6.4.2. Because W = Pn, it follows that W = U , as required.

    A set of vectors is called dependent if it is not independent, that is if some nontrivial linear combina-
tion vanishes. The next result is a convenient test for dependence.

   Lemma 6.4.3: Dependent Lemma
   A set D = {v1, v2, . . . , vk} of vectors in a vector space V is dependent if and only if some vector
   in D is a linear combination of the others.

Proof. Let v2 (say) be a linear combination of the rest: v2 = s1v1 + s3v3 + · · · + skvk. Then

                                                s1v1 + (-1)v2 + s3v3 + · · · + skvk = 0

is a nontrivial linear combination that vanishes, so D is dependent. Conversely, if D is dependent, let

t1v1 + t2v2 + · · · + tkvk = 0 where some coefficient is nonzero. If (say) t2 = 0, then v2 = -tt12 v1 - tt32 v3 -
            tk      is  a  linear  combination  of  the  others.
·  ·  ·  -  t2  vk

    Lemma 6.4.1 gives a way to enlarge independent sets to a basis; by contrast, Lemma 6.4.3 shows that
spanning sets can be cut down to a basis.

         Theorem 6.4.3

         Let V be a finite dimensional vector space. Any spanning set for V can be cut down (by deleting
         vectors) to a basis of V .

Proof. Since V is finite dimensional, it has a finite spanning set S. Among all spanning sets contained in S,

choose S0 containing the smallest number of vectors. It suffices to show that S0 is independent (then S0 is a
basis, proving the theorem). Suppose, on the contrary, that S0 is not independent. Then, by Lemma 6.4.3,
some vector u  S0 is a linear combination of the set S1 = S0 \ {u} of vectors in S0 other than u. It follows
that span S0 = span S1, that is, V = span S1. But S1 has fewer elements than S0 so this contradicts the
choice of S0. Hence S0 is independent after all.
   6 Vector Spaces

Note that, with Theorem 6.4.1, Theorem 6.4.3 completes the promised proof of Theorem 5.2.6 for the case
V = Rn.

    Example 6.4.6
    Find a basis of P3 in the spanning set S = {1, x + x2, 2x - 3x2, 1 + 3x - 2x2, x3}.
    Solution. Since dim P3 = 4, we must eliminate one polynomial from S. It cannot be x3 because
    the span of the rest of S is contained in P2. But eliminating 1 + 3x - 2x2 does leave a basis (verify).
    Note that 1 + 3x - 2x2 is the sum of the first three polynomials in S.

Theorems 6.4.1 and 6.4.3 have other useful consequences.

    Theorem 6.4.4
    Let V be a vector space with dim V = n, and suppose S is a set of exactly n vectors in V . Then S is
    independent if and only if S spans V .

Proof. Assume first that S is independent. By Theorem 6.4.1, S is contained in a basis B of V . Hence
|S| = n = |B| so, since S  B, it follows that S = B. In particular S spans V .

    Conversely, assume that S spans V , so S contains a basis B by Theorem 6.4.3. Again |S| = n = |B| so,
since S  B, it follows that S = B. Hence S is independent.
One of independence or spanning is often easier to establish than the other when showing that a set of
vectors is a basis. For example if V = Rn it is easy to check whether a subset S of Rn is orthogonal (hence
independent) but checking spanning can be tedious. Here are three more examples.

    Example 6.4.7
    Consider the set S = {p0(x), p1(x), . . . , pn(x)} of polynomials in Pn. If deg pk(x) = k for each k,
    show that S is a basis of Pn.

    Solution. The set S is independent--the degrees are distinct--see Example 6.3.4. Hence S is a
    basis of Pn by Theorem 6.4.4 because dim Pn = n + 1.

    Example 6.4.8
    Let V denote the space of all symmetric 2 × 2 matrices. Find a basis of V consisting of invertible
    matrices.

    Solution. We know that dim V = 3 (Example 6.3.11), so what is needed is a set of three invertible,
    symmetric matrices that (using Theorem 6.4.4) is either independent or spans V . The set

         1 0 0 1 , 1 0 0 -1 , 0 1 1 0 is independent (verify) and so is a basis of the required type.
                                             6. . Finite Dimensional Spaces

   Example 6.4.9

   Let A be any n × n matrix. Show that there exist n2 + 1 scalars a0, a1, a2, . . . , an2 not all zero,
   such that

                                          a0I + a1A + a2A2 + · · · + an2An2 = 0
   where I denotes the n × n identity matrix.

   Solution. The space Mnn of all n × n matrices has dimension n2 by Example 6.3.7. Hence the
   n2 + 1 matrices I, A, A2, . . . , An2 cannot be independent by Theorem 6.4.4, so a nontrivial linear
   combination vanishes. This is the desired conclusion.

The result in Example 6.4.9 can be written as f (A) = 0 where f (x) = a0 + a1x + a2x2 + · · · + an2xn2. In
other words, A satisfies a nonzero polynomial f (x) of degree at most n2. In fact we know that A satisfies
a nonzero polynomial of degree n (this is the Cayley-Hamilton theorem--see Theorem 8.7.10), but the
brevity of the solution in Example 6.4.6 is an indication of the power of these methods.

    If U and W are subspaces of a vector space V , there are two related subspaces that are of interest, their
sum U +W and their intersection U W , defined by

                                        U +W = {u + w | u  U and w  W }
                                         U W = {v  V | v  U and v  W }

It is routine to verify that these are indeed subspaces of V , that U W is contained in both U and W , and
that U +W contains both U and W . We conclude this section with a useful fact about the dimensions of
these spaces. The proof is a good illustration of how the theorems in this section are used.

   Theorem 6.4.5
   Suppose that U and W are finite dimensional subspaces of a vector space V . Then U +W is finite
   dimensional and

                                  dim (U +W ) = dim U + dim W - dim (U W ).

Proof. Since U W  U , it has a finite basis, say {x1, . . . , xd}. Extend it to a basis {x1, . . . , xd, u1, . . . , um}
of U by Theorem 6.4.1. Similarly extend {x1, . . . , xd} to a basis {x1, . . . , xd, w1, . . . , wp} of W . Then

U +W = span {x1, . . . , xd, u1, . . . , um, w1, . . . , wp}

as the reader can verify, so U +W is finite dimensional. For the rest, it suffices to show that
{x1, . . . , xd, u1, . . . , um, w1, . . . , wp} is independent (verify). Suppose that

r1x1 + · · · + rdxd + s1u1 + · · · + smum + t1w1 + · · · + tpwp = 0                              (6.1)

where the ri, s j, and tk are scalars. Then

r1x1 + · · · + rdxd + s1u1 + · · · + smum = -(t1w1 + · · · + tpwp)

is in U (left side) and also in W (right side), and so is in U  W . Hence (t1w1 + · · · + tpwp) is a linear
combination of {x1, . . . , xd}, so t1 = · · · = tp = 0, because {x1, . . . , xd, w1, . . . , wp} is independent.
   8 Vector Spaces

Similarly, s1 = · · · = sm = 0, so (6.1) becomes r1x1 + · · · + rdxd = 0. It follows that r1 = · · · = rd = 0, as
required.

    Theorem 6.4.5 is particularly interesting if U  W = {0}. Then there are no vectors xi in the above
proof, and the argument shows that if {u1, . . . , um} and {w1, . . . , wp} are bases of U and W respectively,
then {u1, . . . , um, w1, . . . , wp} is a basis of U + W . In this case U +W is said to be a direct sum (written
U W ); we return to this in Chapter 9.

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                     Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                   Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

6. An Application to Polynomials

The vector space of all polynomials of degree at most n is denoted Pn, and it was established in Section 6.3
that Pn has dimension n + 1; in fact, {1, x, x2, . . . , xn} is a basis. More generally, any n + 1 polynomials
of distinct degrees form a basis, by Theorem 6.4.4 (they are independent by Example 6.3.4). This proves

    Theorem 6.5.1
    Let p0(x), p1(x), p2(x), . . . , pn(x) be polynomials in Pn of degrees 0, 1, 2, . . . , n, respectively.
    Then {p0(x), . . . , pn(x)} is a basis of Pn.

    An immediate consequence is that {1, (x - a), (x - a)2, . . . , (x - a)n} is a basis of Pn for any number
a. Hence we have the following:
                                                                  6. . An Application to Polynomials

Corollary 6.5.1

If a is any number, every polynomial f (x) of degree at most n has an expansion in powers of

(x - a):         f (x) = a0 + a1(x - a) + a2(x - a)2 + · · · + an(x - a)n

                                                                                                    (6.2)

    If f (x) is evaluated at x = a, then equation (6.2) becomes
                                    f (a) = a0 + a1(a - a) + · · · + an(a - a)n = a0

Hence a0 = f (a), and equation (6.2) can be written f (x) = f (a) + (x - a)g(x), where g(x) is a polynomial
of degree n - 1 (this assumes that n  1). If it happens that f (a) = 0, then it is clear that f (x) has the form
f (x) = (x - a)g(x). Conversely, every such polynomial certainly satisfies f (a) = 0, and we obtain:

   Corollary 6.5.2
   Let f (x) be a polynomial of degree n  1 and let a be any number. Then:
   Remainder Theorem

       1. f (x) = f (a) + (x - a)g(x) for some polynomial g(x) of degree n - 1.

   Factor Theorem

       2. f (a) = 0 if and only if f (x) = (x - a)g(x) for some polynomial g(x).

The polynomial g(x) can be computed easily by using "long division" to divide f (x) by (x - a)--see
Appendix D.

    All the coefficients in the expansion (6.2) of f (x) in powers of (x-a) can be determined in terms of the
derivatives of f (x).6 These will be familiar to students of calculus. Let f (n)(x) denote the nth derivative
of the polynomial f (x), and write f (0)(x) = f (x). Then, if

                 f (x) = a0 + a1(x - a) + a2(x - a)2 + · · · + an(x - a)n

it is clear that a0 = f (a) = f (0)(a). Differentiation gives

                 f (1)(x) = a1 + 2a2(x - a) + 3a3(x - a)2 + · · · + nan(x - a)n-1

                 (1)                                                            f (2)(a)  f (3)(a)    f (k)(a)
and substituting x = a yields a1 = f (a). This continues to give a2 = 2! , a3 = 3! , . . . , ak = k! ,
where k! is defined as k! = k(k - 1) · · ·2 · 1. Hence we obtain the following:

Corollary 6.5.3: Taylor's Theorem
If f (x) is a polynomial of degree n, then

                 f (1)(a)                   f (2)(a)           2                f (n)(a)  n
                 f (x) = f (a) + 1! (x - a) + 2! (x - a) + · · · + n! (x - a)

6The discussion of Taylor's theorem can be omitted with no loss of continuity.
Vector Spaces

Example 6.5.1
Expand f (x) = 5x3 + 10x + 2 as a polynomial in powers of x - 1.

Solution. The derivatives are f (1)(x) = 15x2 + 10, f (2)(x) = 30x, and f (3)(x) = 30. Hence the
Taylor expansion is

               f (1)(1)     f (2)(1)    2 f (3)(1)                                              3
               f (x) = f (1) + 1! (x - 1) + 2! (x - 1) + 3! (x - 1)

               = 17 + 25(x - 1) + 15(x - 1)2 + 5(x - 1)3

    Taylor's theorem is useful in that it provides a formula for the coefficients in the expansion. It is dealt
with in calculus texts and will not be pursued here.

    Theorem 6.5.1 produces bases of Pn consisting of polynomials of distinct degrees. A different criterion
is involved in the next theorem.

Theorem 6.5.2

Let f0(x), f1(x), . . . , fn(x) be nonzero polynomials in Pn. Assume that numbers a0, a1, . . . , an
exist such that

               fi(ai) = 0   for each i
               fi(a j) = 0  if i = j

Then

1. { f0(x), . . . , fn(x)} is a basis of Pn.
2. If f (x) is any polynomial in Pn, its expansion as a linear combination of these basis vectors is

               f (x) = f0(a0) f (a0) f0(x) + f1(a1) f (a1) f1(x) + · · · + fn(an) f (an) fn(x)

Proof.

   1. It suffices (by Theorem 6.4.4) to show that { f0(x), . . . , fn(x)} is linearly independent (because
       dim Pn = n + 1). Suppose that

                                        r0 f0(x) + r1 f1(x) + · · · + rn fn(x) = 0, ri  R
       Because fi(a0) = 0 for all i > 0, taking x = a0 gives r0 f0(a0) = 0. But then r0 = 0 because f0(a0) = 0.
       The proof that ri = 0 for i > 0 is analogous.
   2. By (1), f (x) = r0 f0(x) + · · · + rn fn(x) for some numbers ri. Once again, evaluating at a0 gives
       f (a0) = r0 f0(a0), so r0 = f (a0)/ f0(a0). Similarly, ri = f (ai)/ fi(ai) for each i.
                                                                              6. . An Application to Polynomials

   Example 6.5.2
   Show that {x2 - x, x2 - 2x, x2 - 3x + 2} is a basis of P2.
   Solution. Write f0(x) = x2 - x = x(x - 1), f1(x) = x2 - 2x = x(x - 2), and
    f2(x) = x2 - 3x + 2 = (x - 1)(x - 2). Then the conditions of Theorem 6.5.2 are satisfied with
   a0 = 2, a1 = 1, and a2 = 0.

    We investigate one natural choice of the polynomials fi(x) in Theorem 6.5.2. To illustrate, let a0, a1,
and a2 be distinct numbers and write

                     f0(x) = (a0-a1)(a0-a2) (x-a1)(x-a2) f1(x) = (a1-a0)(a1-a2) (x-a0)(x-a2) f2(x) = (a2-a0)(a2-a1) (x-a0)(x-a1)
Then f0(a0) = f1(a1) = f2(a2) = 1, and fi(a j) = 0 for i = j. Hence Theorem 6.5.2 applies, and because
fi(ai) = 1 for each i, the formula for expanding any polynomial is simplified.

    In fact, this can be generalized with no extra effort. If a0, a1, . . . , an are distinct numbers, define the
Lagrange polynomials 0(x), 1(x), . . . , n(x) relative to these numbers as follows:

                                       k(x) = i=k(ak-ai) i=k(x-ai) k = 0, 1, 2, . . . , n
Here the numerator is the product of all the terms (x - a0), (x - a1), . . . , (x - an) with (x - ak) omitted,
and a similar remark applies to the denominator. If n = 2, these are just the polynomials in the preceding
paragraph. For another example, if n = 3, the polynomial 1(x) takes the form

                                              1(x) = (a1-a0)(a1-a2)(a1-a3) (x-a0)(x-a2)(x-a3)
In the general case, it is clear that i(ai) = 1 for each i and that i(a j) = 0 if i = j. Hence Theorem 6.5.2
specializes as Theorem 6.5.3.

   Theorem 6.5.3: Lagrange Interpolation Expansion
   Let a0, a1, . . . , an be distinct numbers. The corresponding set

                                               {0(x), 1(x), . . . , n(x)}
   of Lagrange polynomials is a basis of Pn, and any polynomial f (x) in Pn has the following unique
   expansion as a linear combination of these polynomials.

                                f (x) = f (a0)0(x) + f (a1)1(x) + · · · + f (an)n(x)

   Example 6.5.3
   Find the Lagrange interpolation expansion for f (x) = x2 - 2x + 1 relative to a0 = -1, a1 = 0, and
   a2 = 1.
Vector Spaces

Solution. The Lagrange polynomials are

                                        (x-0)(x-1)  12
               0 = (-1-0)(-1-1) = 2 (x - x)

               1 = (0+1)(0-1) (x+1)(x-1) = -(x2 - 1)

                        (x+1)(x-0) 1 2

               2 = (1+1)(1-0) = 2 (x + x)

Because f (-1) = 4, f (0) = 1, and f (1) = 0, the expansion is

               f (x) = 2(x2 - x) - (x2 - 1)

The Lagrange interpolation expansion gives an easy proof of the following important fact.

Theorem 6.5.4
Let f (x) be a polynomial in Pn, and let a0, a1, . . . , an denote distinct numbers. If f (ai) = 0 for all
i, then f (x) is the zero polynomial (that is, all coefficients are zero).

Proof. All the coefficients in the Lagrange expansion of f (x) are zero.

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!
                            6.6. An Application to Differential Equations

6.6 An Application to Differential Equations

Call a function f : R  R differentiable if it can be differentiated as many times as we want. If f
is a differentiable function, the nth derivative f (n) of f is the result of differentiating n times. Thus
f (0) = f , f (1) = f , f (2) = f (1), . . . and, in general, f (n+1) = f (n) for each n  0. For small values of n
these are often written as f , f , f , f , . . . .

    If a, b, and c are numbers, the differential equations

                            f  + a f  + b f = 0 or f  + a f  + b f  + c f = 0

are said to be of second-order and third-order, respectively. In general, an equation

f (n) + an-1 f (n-1) + an-2 f (n-2) + · · · + a2 f (2) + a1 f (1) + a0 f (0) = 0, ai in R      (6.3)

is called a differential equation of order n. In this section we investigate the set of solutions to (6.3) and,
if n is 1 or 2, find explicit solutions. Of course an acquaintance with calculus is required.

    Let f and g be solutions to (6.3). Then f + g is also a solution because ( f + g)(k) = f (k) + g(k) for all
k, and a f is a solution for any a in R because (a f )(k) = a f (k). It follows that the set of solutions to (6.3) is
a vector space, and we ask for the dimension of this space.

    We have already dealt with the simplest case (see Theorem 3.7.1):

Theorem 6.6.1

The set of solutions of the first-order differential equation f  + a f = 0 is a one-dimensional vector
space and {e-ax} is a basis.

There is a far-reaching generalization of Theorem 6.6.1 that will be proved in Theorem 7.4.1.

Theorem 6.6.2
The set of solutions to the nth order equation (6.3) has dimension n.

Remark

Every differential equation of order n can be converted into a system of n linear first-order equations (see
Exercises ?? and ??). In the case that the matrix of this system is diagonalizable, this approach provides a
proof of Theorem 6.6.2. But if the matrix is not diagonalizable, Theorem 7.4.1 is required.

Theorem 6.6.1 suggests that we look for solutions to (6.3) of the form ex for some number  . This is

a good idea. If we write f (x) = ex, it is easy to verify that f (k)(x) =  kex for each k  0, so substituting

f in (6.3) gives  ( n + an-1 n-1 + an-2 n-2 + · · · + a2 2 + a1 1 + a0)e x = 0

Since ex = 0 for all x, this shows that ex is a solution of (6.3) if and only if  is a root of the characteristic
polynomial c(x), defined to be

                  c(x) = xn + an-1xn-1 + an-2xn-2 + · · · + a2x2 + a1x + a0

This proves Theorem 6.6.3.
    Vector Spaces

Theorem 6.6.3
If  is real, the function ex is a solution of (6.3) if and only if  is a root of the characteristic
polynomial c(x).

Example 6.6.1
Find a basis of the space U of solutions of f  - 2 f  - f  - 2 f = 0.
Solution. The characteristic polynomial is x3 - 2x2 - x - 1 = (x - 1)(x + 1)(x - 2), with roots
1 = 1, 2 = -1, and 3 = 2. Hence ex, e-x, and e2x are all in U . Moreover they are independent
(by Lemma 6.6.1 below) so, since dim (U ) = 3 by Theorem 6.6.2, {ex, e-x, e2x} is a basis of U .

Lemma 6.6.1
If 1, 2, . . . , k are distinct, then {e1x, e2x, . . . , ekx} is linearly independent.

Proof. Suppose that a1e1x + a2e2x + · · · + akekx = 0 for all x. By repeatedly differentiating this equation
k - 1 times, we obtain the following k equations:

a1e1x + a2e2x + · · · + akekx = 0

a11e1x + a22e2x + · · · + akkekx = 0
a1 2e1x      a2 2e2x                                              ak 2k ekx
          +                                             +···+                      =0
     1            2
...

(k-1) 1x     (k-1) 2x                                                    (k-1) kx
a11 e + a22 e + · · · + akk e = 0

These can be written as the following matrix equation.

 1 1 . . . 1  1x                                                           0
 1 2 . . . k  a1e
 2 2         2                                              a  2  e2  x    0
                                                                           
 1 2 . . . k   .  =  .                                                   
             .. .. ..   ..   .. 
 ..          . . .
     .
(k-1) (k-1) (k-1)  akekx 0
1 2 . . . k

    The matrix on the left is a Vandermonde matrix, and by Theorem 3.2.7 its determinant is given by

1 j<ik(i -  j) and hence nonzero as the i's are assumed to all be distinct. Hence that matrix is
invertible and it follows that aieix = 0 for each i and any value of x. Using x = 0 we conclude that ai = 0
for each i and the proof is complete.

Theorem 6.6.4
Let U denote the space of solutions to the second-order equation

                                                 f  + a f  + b f = 0
                                                                  6.6. An Application to Differential Equations

   where a and b are real constants. Assume that the characteristic polynomial x2 + ax + b has two
   real roots  and µ. Then

       1. If  = µ, then {ex, eµx} is a basis of U .
       2. If  = µ, then {ex, xex} is a basis of U .

Proof. Since dim (U ) = 2 by Theorem 6.6.2, (1) follows by Lemma 6.6.1, and (2) follows because the set
{ex, xex} is independent (Exercise ??).

   Example 6.6.2
   Find the solution of f  + 4 f  + 4 f = 0 that satisfies the boundary conditions f (0) = 1,
    f (1) = -1.
   Solution. The characteristic polynomial is x2 + 4x + 4 = (x + 2)2, so -2 is a double root. Hence
   {e-2x, xe-2x} is a basis for the space of solutions, and the general solution takes the form
    f (x) = ce-2x + dxe-2x. Applying the boundary conditions gives 1 = f (0) = c and
   -1 = f (1) = (c + d)e-2. Hence c = 1 and d = -(1 + e2), so the required solution is

                                               f (x) = e-2x - (1 + e2)xe-2x

    One other question remains: What happens if the roots of the characteristic polynomial are not real?
To answer this, we must first state precisely what ex means when  is not real. If q is a real number,
define

                                                    eiq = cos q + i sin q
where i2 = -1. Then the relationship eiqeiq1 = ei(q+q1) holds for all real q and q1, as is easily verified. If
 = p + iq, where p and q are real numbers, we define

                                             e = epeiq = ep(cos q + i sin q)
Then it is a routine exercise to show that

   1. e eµ = e +µ
   2. e = 1 if and only if  = 0
   3. (ex) =  ex

These easily imply that f (x) = ex is a solution to f  + a f  + b f = 0 if  is a (possibly complex) root of
the characteristic polynomial x2 + ax + b. Now write  = p + iq so that

                                        f (x) = ex = epx cos(qx) + iepx sin(qx)
For convenience, denote the real and imaginary parts of f (x) as u(x) = epx cos(qx) and v(x) = epx sin(qx).
Then the fact that f (x) satisfies the differential equation gives

                              0 = f  + a f  + b f = (u + au + bu) + i(v + av + bv)
   6 Vector Spaces

Equating real and imaginary parts shows that u(x) and v(x) are both solutions to the differential equation.
This proves part of Theorem 6.6.5.

   Theorem 6.6.5
   Let U denote the space of solutions of the second-order differential equation

                                                     f  + a f  + b f = 0
   where a and b are real. Suppose  is a nonreal root of the characteristic polynomial x2 + ax + b. If
    = p + iq, where p and q are real, then

                                                {epx cos(qx), epx sin(qx)}

   is a basis of U.

Proof. The foregoing discussion shows that these functions lie in U . Because dim U = 2 by Theo-
rem 6.6.2, it suffices to show that they are linearly independent. But if

                                               repx cos(qx) + sepx sin(qx) = 0

for all x, then r cos(qx) + s sin(qx) = 0 for all x (because epx = 0). Taking x = 0 gives r = 0, and taking
      
x  =  2q  gives  s  =  0  (q  =  0  because    is  not  real).  This  is  what  we  wanted.

   Example 6.6.3

   Find the solution f (x) to f  - 2 f  + 2 f = 0 that satisfies f (0) = 2 and f ( 2 ) = 0.

   Solution. The characteristic polynomial x2 - 2x + 2 has roots 1 + i and 1 - i. Taking  = 1 + i
   (quite arbitrarily) gives p = q = 1 in the notation of Theorem 6.6.5, so {ex cos x, ex sin x} is a basis
   for the space of solutions. The general solution is thus f (x) = ex(r cos x + s sin x). The boundary
   conditions yield 2 = f (0) = r and 0 = f ( 2 ) = e/2s. Thus r = 2 and s = 0, and the required
   solution is f (x) = 2ex cos x.

   The following theorem is an important special case of Theorem 6.6.5.

   Theorem 6.6.6
   If q = 0 is a real number, the space of solutions to the differential equation f  + q2 f = 0 has basis
   {cos(qx), sin(qx)}.

Proof. The characteristic polynomial x2 + q2 has roots qi and -qi, so Theorem 6.6.5 applies with p = 0.

    In many situations, the displacement s(t) of some object at time t turns out to have an oscillating form
s(t) = c sin(at) + d cos(at). These are called simple harmonic motions. An example follows.
  6.6. An Application to Differential Equations

Example 6.6.4

                        A weight is attached to an extension spring (see diagram). If it is pulled
                        from the equilibrium position and released, it is observed to oscillate up
                        and down. Let d(t) denote the distance of the weight below the equilibrium
                        position t seconds later. It is known (Hooke's law) that the acceleration
                        d(t) of the weight is proportional to the displacement d(t) and in the opposite
                        direction. That is,

                                                              d(t) = -kd(t)

               d(t) where k > 0 is called the spring constant. Find d(t) if the maximum extension
                        is 10 cm below the equilibrium position and find the period of the oscillation

(time taken for the weight to make a full oscillation).

Solution. It follows from Theorem 6.6.6 (with q2 = k) that

                                                            
d(t) = r sin( k t) + s cos( k t)

where r and s are constants. The condition d(0) = 0 gives s = 0, so d(t) = r sin(k t). Now the
maximum value of the function sin x is 1 (when x = 2 ), so r = 10 (when t = 2 k ). Hence

                
d(t) = 10 sin( k t)

Finally, the weight goes through a full oscillation as k t increases from 0 to 2. The time taken is
t = 2 , the period of the oscillation.

         k
  8 Vector Spaces

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.
                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.
                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!
Chapter 7

                Linear Transformations

If V and W are vector spaces, a function T : V  W is a rule that assigns to each vector v in V a uniquely
determined vector T (v) in W . As mentioned in Section 2.2, two functions S : V  W and T : V  W
are equal if S(v) = T (v) for every v in V . A function T : V  W is called a linear transformation if
T (v + v1) = T (v) + T (v1) for all v, v1 in V and T (rv) = rT (v) for all v in V and all scalars r. T (v) is
called the image of v under T . We have already studied linear transformation T : Rn  Rm and shown
(in Section 2.6) that they are all given by multiplication by a uniquely determined m × n matrix A; that
is T (x) = Ax for all x in Rn. In the case of linear operators R2  R2, this yields an important way to
describe geometric functions such as rotations about the origin and reflections in a line through the origin.

    In the present chapter we will describe linear transformations in general, introduce the kernel and
image of a linear transformation, and prove a useful result (called the dimension theorem) that relates the
dimensions of the kernel and image, and unifies and extends several earlier results. Finally we study the
notion of isomorphic vector spaces, that is, spaces that are identical except for notation, and relate this to
composition of transformations that was introduced in Section 2.3.

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

           319
Linear Transformations

. Examples and Elementary Properties

Definition 7.1 Linear Transformations of Vector Spaces

                           If V and W are two vector spaces, a function T : V  W is called
      T a linear transformation if it satisfies the following axioms.

v     T (v)             T1. T (v + v1) = T (v) + T (v1) for all v and v1 in V .
                        T2. T (rv) = rT (v)             for all v in V and r in R.
   V         W

A linear transformation T : V  V is called a linear operator on V . The situation can be
visualized as in the diagram.

    Axiom T1 is just the requirement that T preserves vector addition. It asserts that the result T (v + v1)
of adding v and v1 first and then applying T is the same as applying T first to get T (v) and T (v1) and
then adding. Similarly, axiom T2 means that T preserves scalar multiplication. Note that, even though the
additions in axiom T1 are both denoted by the same symbol +, the addition on the left forming v + v1 is
carried out in V , whereas the addition T (v) + T (v1) is done in W . Similarly, the scalar multiplications rv
and rT (v) in axiom T2 refer to the spaces V and W , respectively.

    We have already seen many examples of linear transformations T : Rn  Rm. In fact, writing vectors
in Rn as columns, Theorem 2.6.2 shows that, for each such T , there is an m × n matrix A such that
T (x) = Ax for every x in Rn. Moreover, the matrix A is given by A = T (e1) T (e2) · · · T (en)
where {e1, e2, . . . , en} is the standard basis of Rn. We denote this transformation by TA : Rn  Rm,
defined by

                                              TA(x) = Ax for all x in Rn

    Example 7.1.1 lists three important linear transformations that will be referred to later. The verification
of axioms T1 and T2 is left to the reader.

Example 7.1.1
If V and W are vector spaces, the following are linear transformations:

      Identity operator V  V    1V : V  V    where 1V (v) = v for all v in V
      Zero transformation V  W   0:V W       where 0(v) = 0 for all v in V
      Scalar operator V  V
                                 a:V V       where a(v) = av for all v in V

                                                (Here a is any real number.)

    The symbol 0 will be used to denote the zero transformation from V to W for any spaces V and W . It
was also used earlier to denote the zero function [a, b]  R.

    The next example gives two important transformations of matrices. Recall that the trace tr A of an
n × n matrix A is the sum of the entries on the main diagonal.
                                                                . . Examples and Elementary Properties

Example 7.1.2
Show that the transposition and trace are linear transformations. More precisely,

            R : Mmn  Mnm                          where R(A) = AT for all A in Mmn
            S : Mmn  R                            where S(A) = tr A for all A in Mnn

are both linear transformations.

Solution. Axioms T1 and T2 for transposition are (A + B)T = AT + BT and (rA)T = r(AT ),
respectively (using Theorem 2.1.2). The verifications for the trace are left to the reader.

Example 7.1.3
If a is a scalar, define Ea : Pn  R by Ea(p) = p(a) for each polynomial p in Pn. Show that Ea is a
linear transformation (called evaluation at a).

Solution. If p and q are polynomials and r is in R, we use the fact that the sum p + q and scalar
product rp are defined as for functions:

                             (p + q)(x) = p(x) + q(x) and (rp)(x) = rp(x)

for all x. Hence, for all p and q in Pn and all r in R:

                    Ea(p + q) = (p + q)(a) = p(a) + q(a) = Ea(p) + Ea(q), and
                        Ea(rp) = (rp)(a) = rp(a) = rEa(p).

Hence Ea is a linear transformation.

The next example involves some calculus.

Example 7.1.4

Show that the differentiation and integration operations on Pn are linear transformations. More
precisely,

                      D : Pn  Pn-1      where D [p(x)] = p(x) for all p(x) in Pn
                       I : Pn  Pn+1
                                                                      x
are linear transformations.
                                        where I [p(x)] = p(t)dt for all p(x) in Pn

                                                                     0

Solution. These restate the following fundamental properties of differentiation and integration.
              [p(x) + q(x)] = p(x) + q(x) and [rp(x)] = (rp)(x)

x  [ p(t )  +  q(t )] dt  =          x  p(t  )dt  +  0x q(t)dt  and  x  r  p(t  )dt  =  r  x  p(t  )d  t
0                                    0                               0                     0
         Linear Transformations

    The next theorem collects three useful properties of all linear transformations. They can be described
by saying that, in addition to preserving addition and scalar multiplication (these are the axioms), linear
transformations preserve the zero vector, negatives, and linear combinations.

   Theorem 7.1.1
   Let T : V  W be a linear transformation.

       1. T (0) = 0.
       2. T (-v) = -T (v) for all v in V .
       3. T (r1v1 + r2v2 + · · · + rkvk) = r1T (v1) + r2T (v2) + · · · + rkT (vk) for all vi in V and all ri in R.

Proof.

   1. T (0) = T (0v) = 0T (v) = 0 for any v in V .
   2. T (-v) = T [(-1)v] = (-1)T (v) = -T (v) for any v in V .
   3. The proof of Theorem 2.6.1 goes through.

    The ability to use the last part of Theorem 7.1.1 effectively is vital to obtaining the benefits of linear
transformations. Example 7.1.5 and Theorem 7.1.2 provide illustrations.

   Example 7.1.5
   Let T : V  W be a linear transformation. If T (v - 3v1) = w and T (2v - v1) = w1, find T (v) and
   T (v1) in terms of w and w1.
   Solution. The given relations imply that

                                                   T (v) - 3T (v1) = w
                                                   2T (v) - T (v1) = w1
   by Theorem 7.1.1. Subtracting twice the first from the second gives T (v1) = 51(w1 - 2w). Then
   substitution gives T (v) = 15(3w1 - w).

    The full effect of property (3) in Theorem 7.1.1 is this: If T : V  W is a linear transformation and
T (v1), T (v2), . . . , T (vn) are known, then T (v) can be computed for every vector v in span {v1, v2, . . . , vn}.
In particular, if {v1, v2, . . . , vn} spans V , then T (v) is determined for all v in V by the choice of
T (v1), T (v2), . . . , T (vn). The next theorem states this somewhat differently. As for functions in gen-
eral, two linear transformations T : V  W and S : V  W are called equal (written T = S) if they have
the same action; that is, if T (v) = S(v) for all v in V .
                                                                       . . Examples and Elementary Properties

    Theorem 7.1.2
    Let T : V  W and S : V  W be two linear transformations. Suppose that
   V = span {v1, v2, . . . , vn}. If T(vi) = S(vi) for each i, then T = S.

Proof. If v is any vector in V = span {v1, v2, . . . , vn}, write v = a1v1 + a2v2 + · · · + anvn where each ai
is in R. Since T (vi) = S(vi) for each i, Theorem 7.1.1 gives

                                     T (v) = T (a1v1 + a2v2 + · · · + anvn)
                                            = a1T (v1) + a2T (v2) + · · · + anT (vn)
                                            = a1S(v1) + a2S(v2) + · · · + anS(vn)
                                            = S(a1v1 + a2v2 + · · · + anvn)
                                            = S(v)

    Since v was arbitrary in V , this shows that T = S.

    Example 7.1.6
    Let V = span {v1, . . . , vn}. Let T : V  W be a linear transformation. If T (v1) = · · · = T (vn) = 0,
    show that T = 0, the zero transformation from V to W .

    Solution. The zero transformation 0 : V  W is defined by 0(v) = 0 for all v in V (Example 7.1.1),
    so T (vi) = 0(vi) holds for each i. Hence T = 0 by Theorem 7.1.2.

    Theorem 7.1.2 can be expressed as follows: If we know what a linear transformation T : V  W does
to each vector in a spanning set for V , then we know what T does to every vector in V . If the spanning set
is a basis, we can say much more.

    Theorem 7.1.3
    Let V and W be vector spaces and let {b1, b2, . . . , bn} be a basis of V . Given any vectors
    w1, w2, . . . , wn in W (they need not be distinct), there exists a unique linear transformation
    T : V  W satisfying T (bi) = wi for each i = 1, 2, . . . , n. In fact, the action of T is as follows:
    Given v = v1b1 + v2b2 + · · · + vnbn in V , vi in R, then

                        T (v) = T (v1b1 + v2b2 + · · · + vnbn) = v1w1 + v2w2 + · · · + vnwn.

Proof. If a transformation T does exist with T (bi) = wi for each i, and if S is any other such transformation,
then T (bi) = wi = S(bi) holds for each i, so S = T by Theorem 7.1.2. Hence T is unique if it exists, and
it remains to show that there really is such a linear transformation. Given v in V , we must specify T (v) in
W . Because {b1, . . . , bn} is a basis of V , we have v = v1b1 + · · · + vnbn, where v1, . . . , vn are uniquely
determined by v (this is Theorem 6.3.1). Hence we may define T : V  W by

                        T (v) = T (v1b1 + v2b2 + · · · + vnbn) = v1w1 + v2w2 + · · · + vnwn
for all v = v1b1 + · · · + vnbn in V . This satisfies T (bi) = wi for each i; the verification that T is linear is
left to the reader.
Linear Transformations

    This theorem shows that linear transformations can be defined almost at will: Simply specify where

the basis vectors go, and the rest of the action is dictated by the linearity. Moreover, Theorem 7.1.2 shows

that deciding whether two linear transformations are equal comes down to determining whether they have

the same effect on the basis vectors. So, given a basis {b1, . . . , bn} of a vector space V , there is a different
linear transformation V  W for every ordered selection w1, w2, . . . , wn of vectors in W (not necessarily
distinct).

Example 7.1.7                                                           and T (1 + x2) = 0 0 0 1 .
Find a linear transformation T : P2  M22 such that

          T (1 + x) = 1 0 0 0 , T (x + x2) = 0 1 1 0 ,

Solution. The set {1 + x, x + x2, 1 + x2} is a basis of P2, so every vector p = a + bx + cx2 in P2 is a
linear combination of these vectors. In fact

p(x)  =  1 (a  +  b        -  c)(1  +  x)  +  21 (-a  +  b  +  c)(x  +  x2)  +  1 (a  -  b  +  c)(1  +  x2)

         2                                                                      2

Hence Theorem 7.1.3 gives

T [p(x)] = 2 (a + b - c)1     10 1                                   01 1                               00
                                           + 2(-a + b + c)                      + 2(a - b + c)
                              00                                     10                                 01

   1 a + b - c -a + b + c
= 2 -a + b + c a - b + c
                      . . Kernel and Image of a Linear Transformation

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                               Engage Active Learning App!

                         Vretta-Lyryx Engage is an active learning app designed to increase
                        student engagement in reading linear algebra material. The content is
                      "chunked" into small blocks, each with an interactive assessment activity

                                                 to promote comprehension.

                       Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

 . Kernel and Image of a Linear Transformation

This section is devoted to two important subspaces associated with a linear transformation T : V  W .

   Definition 7.2 Kernel and Image of a Linear Transformation
   The kernel of T (denoted ker T ) and the image of T (denoted im T or T (V )) are defined by

                                           ker T = {v in V | T (v) = 0}
                                            im T = {T (v) | v in V } = T (V )

       T              The kernel of T is often called the nullspace of T because it consists of all
                      vectors v in V satisfying the condition that T (v) = 0. The image of T is
ker T              W  often called the range of T and consists of all vectors w in W of the form
                      w = T (v) for some v in V . These subspaces are depicted in the diagrams.

                0

ExVample 7.2.1

Let TA : Rn  Rm be the linear transformation induced by the
m × n matrix A, that is TA(x) = Ax for all columns x in Rn. Then

V      T  imkeTr TAW= {x | Ax = 0} = null A and
6 Linear Transformations
                  im TA = {Ax | x in Rn} = im A

 Hence the following theorem extends Example 5.1.2.
                                                             . . Kernel and Image of a Linear Transformation

   Theorem 7.2.1
   Let T : V  W be a linear transformation.

       1. ker T is a subspace of V .
       2. im T is a subspace of W .

Proof. The fact that T (0) = 0 shows that ker T and im T contain the zero vector of V and W respectively.

   1. If v and v1 lie in ker T , then T (v) = 0 = T (v1), so
                                        T (v + v1) = T (v) + T (v1) = 0 + 0 = 0
                                             T (rv) = rT (v) = r0 = 0 for all r in R

       Hence v + v1 and rv lie in ker T (they satisfy the required condition), so ker T is a subspace of V
       by the subspace test (Theorem 6.2.1).
   2. If w and w1 lie in im T , write w = T (v) and w1 = T (v1) where v, v1  V . Then

                                          w + w1 = T (v) + T (v1) = T (v + v1)
                                                rw = rT (v) = T (rv) for all r in R

       Hence w + w1 and rw both lie in im T (they have the required form), so im T is a subspace of W .

Given a linear transformation T : V  W :
            dim ( ker T ) is called the nullity of T and denoted as nullity (T )
            dim ( im T ) is called the rank of T and denoted as rank (T )

The rank of a matrix A was defined earlier to be the dimension of col A, the column space of A. The two
usages of the word rank are consistent in the following sense. Recall the definition of TA in Example 7.2.1.

   Example 7.2.2
   Given an m × n matrix A, show that im TA = col A, so rank TA = rank A.
   Solution. Write A = c1 · · · cn in terms of its columns. Then

                               im TA = {Ax | x in Rn} = {x1c1 + · · · + xncn | xi in R}
   using Definition 2.5. Hence im TA is the column space of A; the rest follows.

    Often, a useful way to study a subspace of a vector space is to exhibit it as the kernel or image of a
linear transformation. Here is an example.
8 Linear Transformations

Example 7.2.3
Define a transformation P : Mnn  Mnn by P(A) = A - AT for all A in Mnn. Show that P is linear
and that:

   a. ker P consists of all symmetric matrices.

   b. im P consists of all skew-symmetric matrices.

Solution. The verification that P is linear is left to the reader. To prove part (a), note that a matrix

A lies in ker P just when 0 = P(A) = A - AT , and this occurs if and only if A = AT --that is, A is
symmetric. Turning to part (b), the space im P consists of all matrices P(A), A in Mnn. Every such
matrix is skew-symmetric because

                             P(A)T = (A - AT )T = AT - A = -P(A)

On the other hand, if S is skew-symmetric (that is, ST = -S), then S lies in im P. In fact,

P 1 S = 1 S - 1S T = 1(S - ST ) = 1(S + S) = S22222

One-to-One and Onto Transformations

   Definition 7.3 One-to-one and Onto Linear Transformations
   Let T : V  W be a linear transformation.

       1. T is said to be onto if im T = W .
       2. T is said to be one-to-one if T (v) = T (v1) implies v = v1.

    A vector w in W is said to be hit by T if w = T (v) for some v in V . Then T is onto if every vector in W
is hit at least once, and T is one-to-one if no element of W gets hit twice. Clearly the onto transformations
T are those for which im T = W is as large a subspace of W as possible. By contrast, Theorem 7.2.2
shows that the one-to-one transformations T are the ones with ker T as small a subspace of V as possible.

   Theorem 7.2.2
   If T : V  W is a linear transformation, then T is one-to-one if and only if ker T = {0}.

Proof. If T is one-to-one, let v be any vector in ker T . Then T (v) = 0, so T (v) = T (0). Hence v = 0
because T is one-to-one. Hence ker T = {0}.

    Conversely, assume that ker T = {0} and let T (v) = T (v1) with v and v1 in V . Then
T (v - v1) = T (v) - T (v1) = 0, so v - v1 lies in ker T = {0}. This means that v - v1 = 0, so v = v1,
proving that T is one-to-one.
                                                         . . Kernel and Image of a Linear Transformation

Example 7.2.4
The identity transformation 1V : V  V is both one-to-one and onto for any vector space V .

Example 7.2.5
Consider the linear transformations

                                S : R3  R2              given by S(x, y, z) = (x + y, x - y)
                                T : R2  R3              given by T (x, y) = (x + y, x - y, x)

Show that T is one-to-one but not onto, whereas S is onto but not one-to-one.

Solution. The verification that they are linear is omitted. T is one-to-one because

                                ker T = {(x, y) | x + y = x - y = x = 0} = {(0, 0)}

However, it is not onto. For example (0, 0, 1) does not lie in im T because if

(0, 0, 1) = (x + y, x - y, x) for some x and y, then x + y = 0 = x - y and x = 1, an impossibility.

Turning to S, it is not one-to-one by Theorem 7.2.2 because (0, 0, 1) lies in ker S. But every
element (s, t) in R2 lies in im S because (s, t) = (x + y, x - y) = S(x, y, z) for some x, y, and z (in
             1 (s               1 (s
fact,  x  =        + t),  y  =        - t),  and  z  =  0).  Hence  S  is  onto.
             2                  2

   Example 7.2.6
   Let U be an invertible m × m matrix and define

                               T : Mmn  Mmn by T (X ) = U X for all X in Mmn
   Show that T is a linear transformation that is both one-to-one and onto.

   Solution. The verification that T is linear is left to the reader. To see that T is one-to-one, let
   T (X ) = 0. Then U X = 0, so left-multiplication by U -1 gives X = 0. Hence ker T = {0}, so T is
   one-to-one. Finally, if Y is any member of Mmn, then U -1Y lies in Mmn too, and
   T (U -1Y ) = U (U -1Y ) = Y . This shows that T is onto.

    The linear transformations Rn  Rm all have the form TA for some m × n matrix A (Theorem 2.6.2).
The next theorem gives conditions under which they are onto or one-to-one. Note the connection with
Theorem 5.4.3 and Theorem 5.4.4.
         Linear Transformations

   Theorem 7.2.3
   Let A be an m × n matrix, and let TA : Rn  Rm be the linear transformation induced by A, that is
   TA(x) = Ax for all columns x in Rn.

       1. TA is onto if and only if rank A = m.
       2. TA is one-to-one if and only if rank A = n.

Proof.

   1. We have that im TA is the column space of A (see Example 7.2.2), so TA is onto if and only if the
       column space of A is Rm. Because the rank of A is the dimension of the column space, this holds if
       and only if rank A = m.

   2. ker TA = {x in Rn | Ax = 0}, so (using Theorem 7.2.2) TA is one-to-one if and only if Ax = 0 implies
       x = 0. This is equivalent to rank A = n by Theorem 5.4.3.

The Dimension Theorem

Let A denote an m × n matrix of rank r and let TA : Rn  Rm denote the corresponding matrix transfor-
mation given by TA(x) = Ax for all columns x in Rn. It follows from Example 7.2.1 and Example 7.2.2
that im TA = col A, so dim ( im TA) = dim ( col A) = r. On the other hand Theorem 5.4.2 shows that
dim ( ker TA) = dim ( null A) = n - r. Combining these we see that

                            dim ( im TA) + dim ( ker TA) = n for every m × n matrix A
The main result of this section is a deep generalization of this observation.

   Theorem 7.2.4: Dimension Theorem
   Let T : V  W be any linear transformation and assume that ker T and im T are both finite
   dimensional. Then V is also finite dimensional and

                                          dim V = dim ( ker T ) + dim ( im T )
   In other words, dim V = nullity (T ) + rank (T ).

Proof. Every vector in im T = T (V ) has the form T (v) for some v in V . Hence let {T (e1), T (e2), . . . , T (er)}
be a basis of im T , where the ei lie in V . Let {f1, f2, . . . , fk} be any basis of ker T . Then dim ( im T ) = r
and dim ( ker T ) = k, so it suffices to show that B = {e1, . . . , er, f1, . . . , fk} is a basis of V .

   1. B spans V . If v lies in V , then T (v) lies in im V , so
                                     T (v) = t1T (e1) + t2T (e2) + · · · + trT (er) ti in R

       This implies that v -t1e1 -t2e2 - · · · -trer lies in ker T and so is a linear combination of f1, . . . , fk.
       Hence v is a linear combination of the vectors in B.
. . Kernel and Image of a Linear Transformation

2. B is linearly independent. Suppose that ti and s j in R satisfy

t1e1 + · · · + trer + s1f1 + · · · + skfk = 0                       (7.1)

Applying T gives t1T (e1) +· · ·+trT (er) = 0 (because T (fi) = 0 for each i). Hence the independence
of {T (e1), . . . , T (er)} yields t1 = · · · = tr = 0. But then (7.1) becomes

s1f1 + · · · + skfk = 0

so s1 = · · · = sk = 0 by the independence of {f1, . . . , fk}. This proves that B is linearly independent.

Note that the vector space V is not assumed to be finite dimensional in Theorem 7.2.4. In fact, verify-
ing that ker T and im T are both finite dimensional is often an important way to prove that V is finite
dimensional.

    Note further that r + k = n in the proof so, after relabelling, we end up with a basis

                                        B = {e1, e2, . . . , er, er+1, . . . , en}

of V with the property that {er+1, . . . , en} is a basis of ker T and {T (e1), . . . , T (er)} is a basis of im T .
In fact, if V is known in advance to be finite dimensional, then any basis {er+1, . . . , en} of ker T can be
extended to a basis {e1, e2, . . . , er, er+1, . . . , en} of V by Theorem 6.4.1. Moreover, it turns out that, no
matter how this is done, the vectors {T (e1), . . . , T (er)} will be a basis of im T . This result is useful, and
we record it for reference. The proof is much like that of Theorem 7.2.4 and is left as Exercise ??.

   Theorem 7.2.5
   Let T : V  W be a linear transformation, and let {e1, . . . , er, er+1, . . . , en} be a basis of V such
   that {er+1, . . . , en} is a basis of ker T . Then {T (e1), . . . , T (er)} is a basis of im T , and hence
   r = rank T .

    The dimension theorem is one of the most useful results in all of linear algebra. It shows that if
either dim ( ker T ) or dim ( im T ) can be found, then the other is automatically known. In many cases it is
easier to compute one than the other, so the theorem is a real asset. The rest of this section is devoted to
illustrations of this fact. The next example uses the dimension theorem to give a different proof of the first
part of Theorem 5.4.2.

   Example 7.2.7
   Let A be an m × n matrix of rank r. Show that the space null A of all solutions of the system
   Ax = 0 of m homogeneous equations in n variables has dimension n - r.

   Solution. The space in question is just ker TA, where TA : Rn  Rm is defined by TA(x) = Ax for
   all columns x in Rn. But dim ( im TA) = rank TA = rank A = r by Example 7.2.2, so
   dim ( ker TA) = n - r by the dimension theorem.
         Linear Transformations

   Example 7.2.8
   If T : V  W is a linear transformation where V is finite dimensional, then

                                dim ( ker T )  dim V and dim ( im T )  dim V
   Indeed, dim V = dim ( ker T ) + dim ( im T ) by Theorem 7.2.4. Of course, the first inequality also
   follows because ker T is a subspace of V .

   Example 7.2.9
   Let D : Pn  Pn-1 be the differentiation map defined by D [p(x)] = p(x). Compute ker D and
   hence conclude that D is onto.
   Solution. Because p(x) = 0 means p(x) is constant, we have dim ( ker D) = 1. Since
   dim Pn = n + 1, the dimension theorem gives

                              dim ( im D) = (n + 1) - dim ( ker D) = n = dim (Pn-1)
   This implies that im D = Pn-1, so D is onto.

    Of course it is not difficult to verify directly that each polynomial q(x) in Pn-1 is the derivative of some
polynomial in Pn (simply integrate q(x)!), so the dimension theorem is not needed in this case. However,
in some situations it is difficult to see directly that a linear transformation is onto, and the method used in
Example 7.2.9 may be by far the easiest way to prove it. Here is another illustration.

   Example 7.2.10
   Given a in R, the evaluation map Ea : Pn  R is given by Ea [p(x)] = p(a). Show that Ea is linear
   and onto, and hence conclude that {(x - a), (x - a)2, . . . , (x - a)n} is a basis of ker Ea, the
   subspace of all polynomials p(x) for which p(a) = 0.
   Solution. Ea is linear by Example 7.1.3; the verification that it is onto is left to the reader. Hence
   dim ( im Ea) = dim (R) = 1, so dim ( ker Ea) = (n + 1) - 1 = n by the dimension theorem. Now
   each of the n polynomials (x - a), (x - a)2, . . . , (x - a)n clearly lies in ker Ea, and they are
   linearly independent (they have distinct degrees). Hence they are a basis because dim ( ker Ea) = n.

We conclude by applying the dimension theorem to the rank of a matrix.

   Example 7.2.11
   If A is any m × n matrix, show that rank A = rank AT A = rank AAT .
   Solution. It suffices to show that rank A = rank AT A (the rest follows by replacing A with AT ).
   Write B = AT A, and consider the associated matrix transformations

                                         TA : Rn  Rm and TB : Rn  Rn
                                                                            . . Isomorphisms and Composition

   The dimension theorem and Example 7.2.2 give
                               rank A = rank TA = dim ( im TA) = n - dim ( ker TA)
                               rank B = rank TB = dim ( im TB) = n - dim ( ker TB)

   so it suffices to show that ker TA = ker TB. Now Ax = 0 implies that Bx = AT Ax = 0, so ker TA is
   contained in ker TB. On the other hand, if Bx = 0, then AT Ax = 0, so

                                       Ax 2 = (Ax)T (Ax) = xT AT Ax = xT 0 = 0
   This implies that Ax = 0, so ker TB is contained in ker TA.

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.
                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.
                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

 . Isomorphisms and Composition

Often two vector spaces can consist of quite different types of vectors but, on closer examination, turn out
to be the same underlying space displayed in different symbols. For example, consider the spaces

                            R2 = {(a, b) | a, b  R} and P1 = {a + bx | a, b  R}
    Compare the addition and scalar multiplication in these spaces:

            (a, b) + (a1, b1) = (a + a1, b + b1) (a + bx) + (a1 + b1x) = (a + a1) + (b + b1)x
Linear Transformations

r(a, b) = (ra, rb)      r(a + bx) = (ra) + (rb)x

Clearly these are the same vector space expressed in different notation: if we change each (a, b) in R2 to
a + bx, then R2 becomes P1, complete with addition and scalar multiplication. This can be expressed by
noting that the map (a, b)  a + bx is a linear transformation R2  P1 that is both one-to-one and onto.
In this form, we can describe the general situation.

Definition 7.4 Isomorphic Vector Spaces

A linear transformation T : V  W is called an isomorphism if it is both onto and one-to-one. The
vector spaces V and W are said to be isomorphic if there exists an isomorphism T : V  W , and
we write V = W when this is the case.

Example 7.3.1
The identity transformation 1V : V  V is an isomorphism for any vector space V .

Example 7.3.2
If T : Mmn  Mnm is defined by T (A) = AT for all A in Mmn, then T is an isomorphism (verify).
Hence Mmn = Mnm.

   Example 7.3.3
   Isomorphic spaces can "look" quite different. For example, M22 = P3 because the map
   T : M22  P3 given by T a b c d = a + bx + cx2 + dx3 is an isomorphism (verify).

    The word isomorphism comes from two Greek roots: iso, meaning "same," and morphos, meaning
"form." An isomorphism T : V  W induces a pairing

                                                         v  T (v)

between vectors v in V and vectors T (v) in W that preserves vector addition and scalar multiplication.
Hence, as far as their vector space properties are concerned, the spaces V and W are identical except
for notation. Because addition and scalar multiplication in either space are completely determined by the
same operations in the other space, all vector space properties of either space are completely determined
by those of the other.

    One of the most important examples of isomorphic spaces was considered in Chapter 4. Let A denote
the set of all "arrows" with tail at the origin in space, and make A into a vector space using the paral-
lelogram law and the scalar multiple law (see Section 4.1). Then define a transformation T : R3  A by
taking

                            x
                          T  y  = the arrow v from the origin to the point P(x, y, z).

                               z
                                                . . Isomorphisms and Composition

In Section 4.1 matrix addition and scalar multiplication were shown to correspond to the parallelogram

law and the scalar multiplication law for these arrows, so the map T is a linear transformation. Moreover T

is an isomorphism: it is one-to-one by Theorem 4.1.2, and it is onto because, given an arrow v in A with tip

x                                               x

P(x, y, z), we have T  y  = v. This justifies the identification v =  y  in Chapter 4 of the geometric

z                                               z

arrows with the algebraic matrices. This identification is very useful. The arrows give a "picture" of the
matrices and so bring geometric intuition into R3; the matrices are useful for detailed calculations and so

bring analytic precision into geometry. This is one of the best examples of the power of an isomorphism

to shed light on both spaces being considered.

    The following theorem gives a very useful characterization of isomorphisms: They are the linear
transformations that preserve bases.

Theorem 7.3.1
If V and W are finite dimensional spaces, the following conditions are equivalent for a linear
transformation T : V  W .

   1. T is an isomorphism.

   2. If {e1, e2, . . . , en} is any basis of V , then {T (e1), T (e2), . . . , T (en)} is a basis of W .
   3. There exists a basis {e1, e2, . . . , en} of V such that {T (e1), T (e2), . . . , T (en)} is a basis of

       W.

Proof. (1)  (2). Let {e1, . . . , en} be a basis of V . If t1T (e1) + · · · + tnT (en) = 0 with ti in R, then
T (t1e1 + · · · + tnen) = 0, so t1e1 + · · · + tnen = 0 (because ker T = {0}). But then each ti = 0 by the
independence of the ei, so {T (e1), . . . , T (en)} is independent. To show that it spans W , choose w in
W . Because T is onto, w = T (v) for some v in V , so write v = t1e1 + · · · + tnen. Hence we obtain
w = T (v) = t1T (e1) + · · · + tnT (en), proving that {T (e1), . . . , T (en)} spans W .

    (2)  (3). This is because V has a basis.
    (3)  (1). If T (v) = 0, write v = v1e1 + · · · + vnen where each vi is in R. Then

                                          0 = T (v) = v1T (e1) + · · · + vnT (en)

so v1 = · · · = vn = 0 by (3). Hence v = 0, so ker T = {0} and T is one-to-one. To show that T is onto, let
w be any vector in W . By (3) there exist w1, . . . , wn in R such that

                               w = w1T (e1) + · · · + wnT (en) = T (w1e1 + · · · + wnen)

Thus T is onto.

    Theorem 7.3.1 dovetails nicely with Theorem 7.1.3 as follows. Let V and W be vector spaces of
dimension n, and suppose that {e1, e2, . . . , en} and {f1, f2, . . . , fn} are bases of V and W , respectively.
Theorem 7.1.3 asserts that there exists a linear transformation T : V  W such that

                                          T (ei) = fi for each i = 1, 2, . . . , n
  6 Linear Transformations

Then {T (e1), . . . , T (en)} is evidently a basis of W , so T is an isomorphism by Theorem 7.3.1. Further-
more, the action of T is prescribed by

                                        T (r1e1 + · · · + rnen) = r1f1 + · · · + rnfn
so isomorphisms between spaces of equal dimension can be easily defined as soon as bases are known. In
particular, this shows that if two vector spaces V and W have the same dimension then they are isomorphic,
that is V = W . This is half of the following theorem.

   Theorem 7.3.2
   If V and W are finite dimensional vector spaces, then V = W if and only if dim V = dim W .

Proof. It remains to show that if V = W then dim V = dim W . But if V = W , then there exists an isomor-
phism T : V  W . Since V is finite dimensional, let {e1, . . . , en} be a basis of V . Then {T (e1), . . . , T (en)}
is a basis of W by Theorem 7.3.1, so dim W = n = dim V .

   Corollary 7.3.1
   Let U, V , and W denote vector spaces. Then:

       1. V = V for every vector space V .
       2. If V = W then W = V .
       3. If U = V and V = W , then U = W .

The proof is left to the reader. By virtue of these properties, the relation = is called an equivalence relation
on the class of finite dimensional vector spaces. Since dim (Rn) = n it follows that

   Corollary 7.3.2
   If V is a vector space and dim V = n, then V is isomorphic to Rn.

    If V is a vector space of dimension n, note that there are important explicit isomorphisms V  Rn.

Fix a basis B = {b1, b2, . . . , bn} of V and write {e1, e2, . . . , en} for the standard basis of Rn. By
Theorem 7.1.3 there is a unique linear transformation CB : V  Rn given by


   v1

  v2  
      
CB(v1b1 + v2b2 + · · · + vnbn) = v1e1 + v2e2 + · · · + vnen =  .. 
.

  vn

where each vi is in R. Moreover, CB(bi) = ei for each i so CB is an isomorphism by Theorem 7.3.1, called
the coordinate isomorphism corresponding to the basis B. These isomorphisms will play a central role

in Chapter 9.
                                                                            . . Isomorphisms and Composition

    The conclusion in the above corollary can be phrased as follows: As far as vector space properties
are concerned, every n-dimensional vector space V is essentially the same as Rn; they are the "same"
vector space except for a change of symbols. This appears to make the process of abstraction seem less
important--just study Rn and be done with it! But consider the different "feel" of the spaces P8 and M33
even though they are both the "same" as R9: For example, vectors in P8 can have roots, while vectors in
M33 can be multiplied. So the merit in the abstraction process lies in identifying common properties of
the vector spaces in the various examples. This is important even for finite dimensional spaces. However,
the payoff from abstraction is much greater in the infinite dimensional case, particularly for spaces of
functions.

   Example 7.3.4
   Let V denote the space of all 2 × 2 symmetric matrices. Find an isomorphism T : P2  V such that
   T (1) = I, where I is the 2 × 2 identity matrix.

   Solution. {1, x, x2} is a basis of P2, and we want a basis of V containing I. The set
         1 0 0 1 , 0 1 1 0 , 0 0 0 1 is independent in V , so it is a basis because dim V = 3 (by

   Example 6.3.11). Hence define T : P2  V by taking T (1) = 1 0 0 1 , T (x) = 0 1 1 0 ,
   T (x2) = 0 0 0 1 , and extending linearly as in Theorem 7.1.3. Then T is an isomorphism by
   Theorem 7.3.1, and its action is given by

                           T (a + bx + cx2) = aT (1) + bT (x) + cT (x2) = a b b a + c

    The dimension theorem (Theorem 7.2.4) gives the following useful fact about isomorphisms.

   Theorem 7.3.3
   If V and W have the same dimension n, a linear transformation T : V  W is an isomorphism if it
   is either one-to-one or onto.

Proof. The dimension theorem asserts that dim ( ker T ) + dim ( im T ) = n, so dim ( ker T ) = 0 if and only
if dim ( im T ) = n. Thus T is one-to-one if and only if T is onto, and the result follows.

Composition

Suppose that T : V  W and S : W  U are linear transformations. They link together as in the diagram
so, as in Section 2.3, it is possible to define a new function V  U by first applying T and then S.
8 Linear Transformations

Definition 7.5 Composition of Linear Transformations

                          Given linear transformations V - W - U, the compositeTS

   T           S          ST : V  U of T and S is defined by

                                  ST (v) = S [T (v)] for all v in V

V     W            U      The operation of forming the new function ST is called composition.1

The action of ST can be described compactly as follows: ST means first T then S.

    Not all pairs of linear transformations can be composed. For example, if T : V  W and S : W  U
are linear transformations then ST : V  U is defined, but T S cannot be formed unless U = V because
S : W  U and T : V  W do not "link" in that order.2

    Moreover, even if ST and T S can both be formed, they may not be equal. In fact, if S : Rm  Rn and
T : Rn  Rm are induced by matrices A and B respectively, then ST and T S can both be formed (they are
induced by AB and BA respectively), but the matrix products AB and BA may not be equal (they may not
even be the same size). Here is another example.

Example 7.3.5

Define: S : M22  M22 and T : M22  M22 by S a b c d = c d a b        and T (A) = AT for
A  M22. Describe the action of ST and T S, and show that ST = T S.

Solution. ST a b = S a c = b d , whereas
               cd         bd  ac

TS a b =T c d = c a .
cd                ab      db

It is clear that T S a b need not equal ST a b , so T S = ST .
                  cd              cd

The next theorem collects some basic properties of the composition operation.

Theorem 7.3.4: 3

T     SR
Let V - W - U - Z be linear transformations.

1. The composite ST is again a linear transformation.
2. T 1V = T and 1W T = T .
3. (RS)T = R(ST ).

1In Section 2.3 we denoted the composite as S  T . However, it is more convenient to use the simpler notation ST .
2Actually, all that is required is U  V .
                                                                            . . Isomorphisms and Composition

Proof. The proofs of (1) and (2) are left as Exercise ??. To prove (3), observe that, for all v in V :
                    {(RS)T }(v) = (RS) [T (v)] = R{S [T (v)]} = R{(ST )(v)} = {R(ST )}(v)

    Up to this point, composition seems to have no connection with isomorphisms. In fact, the two notions
are closely related.

   Theorem 7.3.5
   Let V and W be finite dimensional vector spaces. The following conditions are equivalent for a
   linear transformation T : V  W .

       1. T is an isomorphism.
       2. There exists a linear transformation S : W  V such that ST = 1V and T S = 1W .

   Moreover, in this case S is also an isomorphism and is uniquely determined by T :

                                 If w in W is written as w = T (v), then S(w) = v.

Proof. (1)  (2). If B = {e1, . . . , en} is a basis of V , then D = {T (e1), . . . , T (en)} is a basis of W by
Theorem 7.3.1. Hence (using Theorem 7.1.3), define a linear transformation S : W  V by

S[T (ei)] = ei for each i                                                                                     (7.2)

Since ei = 1V (ei), this gives ST = 1V by Theorem 7.1.2. But applying T gives T [S [T (ei)]] = T (ei) for
each i, so T S = 1W (again by Theorem 7.1.2, using the basis D of W ).

    (2)  (1). If T (v) = T (v1), then S [T (v)] = S [T (v1)]. Because ST = 1V by (2), this reads v = v1; that
is, T is one-to-one. Given w in W , the fact that T S = 1W means that w = T [S(w)], so T is onto.

    Finally, S is uniquely determined by the condition ST = 1V because this condition implies (7.2). S
is an isomorphism because it carries the basis D to B. As to the last assertion, given w in W , write
w = r1T (e1) + · · · + rnT (en). Then w = T (v), where v = r1e1 + · · · + rnen. Then S(w) = v by (7.2).

    Given an isomorphism T : V  W , the unique isomorphism S : W  V satisfying condition (2) of
Theorem 7.3.5 is called the inverse of T and is denoted by T -1. Hence T : V  W and T -1 : W  V are
related by the fundamental identities:

T -1 [T (v)] = v for all v in V and T T -1(w) = w for all w in W

In other words, each of T and T -1 reverses the action of the other. In particular, equation (7.2) in the proof
of Theorem 7.3.5 shows how to define T -1 using the image of a basis under the isomorphism T . Here is
an example.

    3Theorem 7.3.4 can be expressed by saying that vector spaces and linear transformations are an example of a category. In
general a category consists of certain objects and, for any two objects X and Y , a set mor (X, Y ). The elements  of mor (X, Y )
are called morphisms from X to Y and are written  : X  Y . It is assumed that identity morphisms and composition are defined
in such a way that Theorem 7.3.4 holds. Hence, in the category of vector spaces the objects are the vector spaces themselves and
the morphisms are the linear transformations. Another example is the category of metric spaces, in which the objects are sets
equipped with a distance function (called a metric), and the morphisms are continuous functions (with respect to the metric).
The category of sets and functions is a very basic example.
Linear Transformations

Example 7.3.6
Define T : P1  P1 by T (a + bx) = (a - b) + ax. Show that T has an inverse, and find the action of
T -1.

Solution. The transformation T is linear (verify). Because T (1) = 1 + x and T (x) = -1, T carries
the basis B = {1, x} to the basis D = {1 + x, -1}. Hence T is an isomorphism, and T -1 carries D

back to B, that is,     T -1(1 + x) = 1 and T -1(-1) = x

Because a + bx = b(1 + x) + (b - a)(-1), we obtain

                     T -1(a + bx) = bT -1(1 + x) + (b - a)T -1(-1) = b + (b - a)x

    Sometimes the action of the inverse of a transformation is apparent.

   Example 7.3.7
   If B = {b1, b2, . . . , bn} is a basis of a vector space V , the coordinate transformation CB : V  Rn
   is an isomorphism defined by

                                 CB(v1b1 + v2b2 + · · · + vnbn) = (v1, v2, . . . , vn)T
   The way to reverse the action of CB is clear: C-1 B : Rn  V is given by

                         CB-1(v1, v2, . . . , vn) = v1b1 + v2b2 + · · · + vnbn for all vi in V

    Condition (2) in Theorem 7.3.5 characterizes the inverse of a linear transformation T : V  W as the
(unique) transformation S : W  V that satisfies ST = 1V and T S = 1W . This often determines the inverse.

   Example 7.3.8
   Define T : R3  R3 by T (x, y, z) = (z, x, y). Show that T 3 = 1R3, and hence find T -1.
   Solution. T 2(x, y, z) = T [T (x, y, z)] = T (z, x, y) = (y, z, x). Hence

                               T 3(x, y, z) = T T 2(x, y, z) = T (y, z, x) = (x, y, z)
   Since this holds for all (x, y, z), it shows that T 3 = 1R3, so T (T 2) = 1R3 = (T 2)T . Thus T -1 = T 2
   by (2) of Theorem 7.3.5.

Example 7.3.9
Define T : Pn  Rn+1 by T (p) = (p(0), p(1), . . . , p(n)) for all p in Pn. Show that T -1 exists.

Solution. The verification that T is linear is left to the reader. If T (p) = 0, then p(k) = 0 for
k = 0, 1, . . . , n, so p has n + 1 distinct roots. Because p has degree at most n, this implies that
                                                                    . . A Theorem about Differential Equations

    p = 0 is the zero polynomial (Theorem 6.5.4) and hence that T is one-to-one. But
   dim Pn = n + 1 = dim Rn+1, so this means that T is also onto and hence is an isomorphism. Thus
   T -1 exists by Theorem 7.3.5. Note that we have not given a description of the action of T -1, we
   have merely shown that such a description exists. To give it explicitly requires some ingenuity; one
   method involves the Lagrange interpolation expansion (Theorem 6.5.3).

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

 . A Theorem about Differential Equations

Differential equations are instrumental in solving a variety of problems throughout science, social science,
and engineering. In this brief section, we will see that the set of solutions of a linear differential equation
(with constant coefficients) is a vector space and we will calculate its dimension. The proof is pure linear
algebra, although the applications are primarily in analysis. However, a key result (Lemma 7.4.3 below)
can be applied much more widely.

    We denote the derivative of a function f : R  R by f , and f will be called differentiable if it can
be differentiated any number of times. If f is a differentiable function, the nth derivative f (n) of f is the
result of differentiating n times. Thus f (0) = f , f (1) = f , f (2) = f (1), . . . , and in general f (n+1) = f (n)
for each n  0. For small values of n these are often written as f , f , f , f , . . . .

    If a, b, and c are numbers, the differential equations
                                 f  - a f  - b f = 0 or f  - a f  - b f  - c f = 0
Linear Transformations

are said to be of second order and third-order, respectively. In general, an equation                                                                 (7.3)
                  f (n) - an-1 f (n-1) - an-2 f (n-2) - · · · - a2 f (2) - a1 f (1) - a0 f (0) = 0, ai in R

is called a differential equation of order n. We want to describe all solutions of this equation. Of course
a knowledge of calculus is required.

    The set F of all functions R  R is a vector space with operations as described in Example 6.1.7. If f
and g are differentiable, we have ( f + g) = f  + g and (a f ) = a f  for all a in R. With this it is a routine
matter to verify that the following set is a subspace of F:

                               Dn = { f : R  R | f is differentiable and is a solution to (7.3)}

Our sole objective in this section is to prove

Theorem 7.4.1
The space Dn has dimension n.

    As will be clear later, the proof of Theorem 7.4.1 requires that we enlarge Dn somewhat and allow our
differentiable functions to take values in the set C of complex numbers. To do this, we must clarify what

it means for a function f : R  C to be differentiable. For each real number x write f (x) in terms of its
real and imaginary parts fr(x) and fi(x):

                                                                        f (x) = fr(x) + i fi(x)

This produces new functions fr : R  R and fi : R  R, called the real and imaginary parts of f ,

respectively. We say that f is differentiable if both fr and fi are differentiable (as real functions), and we
define the derivative f  of f by

                                                                             f  =     f     +   i  f                                                  (7.4)
                                                                                         r            i

We refer to this frequently in what follows.4

    With this, write D for the set of all differentiable complex valued functions f : R  C . This is a
complex vector space using pointwise addition (see Example 6.1.7), and the following scalar multiplica-

tion: For any w in C and f in D, we define w f : R  C by (w f )(x) = w f (x) for all x in R. We will be
working in D for the rest of this section. In particular, consider the following complex subspace of D:

                                                    D    =    {f     :  R    C  |  f     is  a  solution  to  (7.3)}

                                                      n

Clearly,  Dn       Dn,  and    our      interest         in   D      comes   from

                                                                n

Lemma 7.4.1
If dim C(Dn) = n, then dim R(Dn) = n.

4Write |w| for the absolute value of any complex number w. As for functions R  R, we say that limt0 f (t) = w if, for all
 > 0 there exists  > 0 such that | f (t) - w| < whenever |t| <  . (Note that t represents a real number here.) In particular,
                                                                                                                      1
given a real number x, we define the derivative f  of a function f : R  C by f (x) = limt0                            t  [  f  (x  +  t)  -  f  (x)]  and we say

that f is differentiable if f (x) exists for all x in R. Then we can prove that f is differentiable if and only if both fr and fi are
                            f                    
differentiable,  and  that     =  f  r  +  i  f  i  in  this  case.
                                                                         . . A Theorem about Differential Equations

Proof. Observe first that if dim C(Dn) = n, then dim R(Dn) = 2n. [In fact, if {g1, . . . , gn} is a C-basis of
D    then  {g1,  ...,  gn,  ig1,   ...,        is  a  R-basis  of  Dn].    Now    observe     that  the    set            of  all  ordered
                                         ign}                                                                   Dn × Dn
  n
pairs ( f , g) with f and g in Dn is a real vector space with componentwise operations. Define

                                :  D      Dn   × Dn   given by        ( f ) = ( fr,      fi) for    f  in  D

                                     n                                                                       n

One verifies that  is onto and one-to-one, and it is R-linear because f  fr and f  fi are both R-linear.
       D    = Dn × Dn                                 dim R(Dn)
Hence                       as  R-spaces.      Since               is    finite,  it  follows  that    dim R(Dn)      is  finite,  and  we
         n
have

                                   2 dim R(Dn) = dim R(Dn × Dn) = dim R(Dn) = 2n

Hence dim R(Dn) = n, as required.

It follows that to prove Theorem 7.4.1 it suffices to show that dim C(Dn) = n.

    There is one function that arises frequently in any discussion of differential equations. Given a complex

number w = a + ib (where a and b are real), we have ew = ea(cos b + i sin b). The law of exponents,
ewev = ew+v for all w, v in C is easily verified using the formulas for sin(b + b1) and cos(b + b1). If x is a
variable and w = a + ib is a complex number, define the exponential function ewx by

                                                   ewx = eax(cos bx + i sin bx)

Hence ewx is differentiable because its real and imaginary parts are differentiable for all x. Moreover, the
following can be proved using (7.4):

                                                       (ewx) = wewx

In addition, (7.4) gives the product rule for differentiation:

                                         If f and g are in D, then ( f g) = f g + f g

We omit the verifications.
    To prove that dim C(Dn) = n, two preliminary results are required. Here is the first.

     Lemma 7.4.2
     Given f in D and w in C, there exists g in D such that g - wg = f .

Proof. Define p(x) = f (x)e-wx. Then p is differentiable, whence pr and pi are both differentiable, hence
                                                                      q                  qi.
continuous,  and  so   both     have    antiderivatives,  say  pr  =       and  pi    =       Then     the  function  q  =  qr  + iqi  is  in
                                                                        r
D, and q = p by (7.4). Finally define g(x) = q(x)ewx. Then

                                   g = qewx + qwewx = pewx + w(qewx) = f + wg

by the product rule, as required.
    The second preliminary result is important in its own right.
       Linear Transformations

Lemma 7.4.3: Kernel Lemma

Let V be a vector space, and let S and T be linear operators V  V . If S is onto and both ker (S)
and ker (T ) are finite dimensional, then ker (T S) is also finite dimensional and
dim [ ker (T S)] = dim [ ker (T )] + dim [ ker (S)].

Proof. Let {u1, u2, . . . , um} be a basis of ker (T ) and let {v1, v2, . . . , vn} be a basis of ker (S). Since S
is onto, let ui = S(wi) for some wi in V . It suffices to show that

                                    B = {w1, w2, . . . , wm, v1, v2, . . . , vn}

is a basis of ker (T S). Note B  ker (T S) because T S(wi) = T (ui) = 0 for each i and T S(v j) = T (0) = 0
for each j.

Spanning. If v is in ker (T S), then S(v) is in ker (T ), say S(v) =  riui =  riS (wi) = S ( riwi). It follows
that v -  riwi is in ker (S) = span {v1, v2, . . . , vn}, proving that v is in span (B).

Independence. Let  riwi +  t jv j = 0. Applying S, and noting that S(v j) = 0 for each j, yields
0 =  riS(wi) =  riui. Hence ri = 0 for each i, and so t jv j = 0. This implies that each t j = 0, and so
proves the independence of B.

Proof of Theorem 7.4.1. By Lemma 7.4.1, it suffices to prove that dim C(Dn) = n. This holds for n = 1
because  the   proof  of  Theorem   3.7.1  goes  through  to  show   that  D       Cea0x.  Hence  we  proceed  by  induction
                                                                                =
                                                                             1
on n. With an eye on equation (7.3), consider the polynomial

                              p(t) = tn - an-1tn-1 - an-2tn-2 - · · · - a2t2 - a1t - a0

(called the characteristic polynomial of equation (7.3)). Now define a map D : D  D by D( f ) = f 
for all f in D. Then D is a linear operator, whence p(D) : D  D is also a linear operator. Moreover,
since Dk( f ) = f (k) for each k  0, equation (7.3) takes the form p(D)( f ) = 0. In other words,

                                                    D    =  ker  [ p(D)]

                                                      n

By the fundamental theorem of algebra,5 let w be a complex root of p(t), so that p(t) = q(t)(t -w) for some
complex polynomial q(t) of degree n - 1. It follows that p(D) = q(D)(D - w1D). Moreover D - w1D is
onto by Lemma 7.4.2, dim C[ ker (D - w1D)] = 1 by the case n = 1 above, and dim C( ker [q(D)]) = n - 1
by induction. Hence Lemma 7.4.3 shows that ker [P(D)] is also finite dimensional and

            dim C( ker [p(D)]) = dim C( ker [q(D)]) + dim C( ker [D - w1D]) = (n - 1) + 1 = n.

Since  D    =  ker  [ p(D)],  this  completes  the  induction,  and  so  proves    Theorem  7.4.1.

         n

5This is the reason for allowing our solutions to (7.3) to be complex valued.
                                                                                 . . More on Linear Recurrences

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

 . More on Linear Recurrences6

In Section 3.4 we used diagonalization to study linear recurrences, and gave several examples. We now
apply the theory of vector spaces and linear transformations to study the problem in more generality.

    Consider the linear recurrence
                                             xn+2 = 6xn - xn+1 for n  0

If the initial values x0 and x1 are prescribed, this gives a sequence of numbers. For example, if x0 = 1 and
x1 = 1 the sequence continues

                                 x2 = 5, x3 = 1, x4 = 29, x5 = -23, x6 = 197, . . .
as the reader can verify. Clearly, the entire sequence is uniquely determined by the recurrence and the two
initial values. In this section we define a vector space structure on the set of all sequences, and study the
subspace of those sequences that satisfy a particular recurrence.

    Sequences will be considered entities in their own right, so it is useful to have a special notation for
them. Let

                                  [xn) denote the sequence x0, x1, x2, . . . , xn, . . .

    6This section requires only Sections 7.1-7.3.
6 Linear Transformations

Example 7.5.1

               [n)        is the sequence 0, 1, 2, 3, . . .
               [n + 1)    is the sequence 1, 2, 3, 4, . . .
                          is the sequence 1, 2, 22, 23, . . .
               [2n)       is the sequence 1, -1, 1, -1, . . .
               [(-1)n)    is the sequence 5, 5, 5, 5, . . .
               [5)

Sequences of the form [c) for a fixed number c will be referred to as constant sequences, and those of the
form [ n),  some number, are power sequences.

    Two sequences are regarded as equal when they are identical:

                               [xn) = [yn) means xn = yn for all n = 0, 1, 2, . . .
Addition and scalar multiplication of sequences are defined by

                                                  [xn) + [yn) = [xn + yn)
                                                         r[xn) = [rxn)

These operations are analogous to the addition and scalar multiplication in Rn, and it is easy to check that
the vector-space axioms are satisfied. The zero vector is the constant sequence [0), and the negative of a
sequence [xn) is given by -[xn) = [-xn).

    Now suppose k real numbers r0, r1, . . . , rk-1 are given, and consider the linear recurrence relation
determined by these numbers.

                          xn+k = r0xn + r1xn+1 + · · · + rk-1xn+k-1                  (7.5)

When r0 = 0, we say this recurrence has length k.7 For example, the relation xn+2 = 2xn + xn+1 is of
length 2.

    A sequence [xn) is said to satisfy the relation (7.5) if (7.5) holds for all n  0. Let V denote the set of
all sequences that satisfy the relation. In symbols,

                    V = {[xn) | xn+k = r0xn + r1xn+1 + · · · + rk-1xn+k-1 hold for all n  0}

It is easy to see that the constant sequence [0) lies in V and that V is closed under addition and scalar
multiplication of sequences. Hence V is vector space (being a subspace of the space of all sequences).
The following important observation about V is needed (it was used implicitly earlier): If the first k terms
of two sequences agree, then the sequences are identical. More formally,

Lemma 7.5.1
Let [xn) and [yn) denote two sequences in V . Then

                     [xn) = [yn) if and only if x0 = y0, x1 = y1, . . . , xk-1 = yk-1

7We shall usually assume that r0 = 0; otherwise, we are essentially dealing with a recurrence of shorter length than k.
                                                                                 . . More on Linear Recurrences

Proof. If [xn) = [yn) then xn = yn for all n = 0, 1, 2, . . . . Conversely, if xi = yi for all i = 0, 1, . . . , k - 1,
use the recurrence (7.5) for n = 0.

                      xk = r0x0 + r1x1 + · · · + rk-1xk-1 = r0y0 + r1y1 + · · · + rk-1yk-1 = yk

Next the recurrence for n = 1 establishes xk+1 = yk+1. The process continues to show that xn+k = yn+k
holds for all n  0 by induction on n. Hence [xn) = [yn).

    This shows that a sequence in V is completely determined by its first k terms. In particular, given a
k-tuple v = (v0, v1, . . . , vk-1) in Rk, define

                      T (v) to be the sequence in V whose first k terms are v0, v1, . . . , vk-1

The rest of the sequence T (v) is determined by the recurrence, so T : Rk  V is a function. In fact, it is
an isomorphism.

   Theorem 7.5.1
   Given real numbers r0, r1, . . . , rk-1, let

                      V = {[xn) | xn+k = r0xn + r1xn+1 + · · · + rk-1xn+k-1, for all n  0}

   denote the vector space of all sequences satisfying the linear recurrence relation (7.5) determined
   by r0, r1, . . . , rk-1. Then the function T : Rk  V
   defined above is an isomorphism. In particular:

       1. dim V = k.

       2. If {v1, . . . , vk} is any basis of Rk, then {T (v1), . . . , T (vk)} is a basis of V .

Proof. (1) and (2) will follow from Theorem 7.3.1 and Theorem 7.3.2 as soon as we show that T is an
isomorphism. Given v and w in Rk, write v = (v0, v1, . . . , vk-1) and w = (w0, w1, . . . , wk-1). The first
k terms of T (v) and T (w) are v0, v1, . . . , vk-1 and w0, w1, . . . , wk-1, respectively, so the first k terms of
T (v) + T (w) are v0 + w0, v1 + w1, . . . , vk-1 + wk-1. Because these terms agree with the first k terms of
T (v + w), Lemma 7.5.1 implies that T (v + w) = T (v) + T (w). The proof that T (rv) + rT (v) is similar, so
T is linear.

    Now let [xn) be any sequence in V , and let v = (x0, x1, . . . , xk-1). Then the first k terms of [xn) and
T (v) agree, so T (v) = [xn). Hence T is onto. Finally, if T (v) = [0) is the zero sequence, then the first k
terms of T (v) are all zero (all terms of T (v) are zero!) so v = 0. This means that ker T = {0}, so T is
one-to-one.

   Example 7.5.2
   Show that the sequences [1), [n), and [(-1)n) are a basis of the space V of all solutions of the
   recurrence

                                                xn+3 = -xn + xn+1 + xn+2
8 Linear Transformations

Then find the solution satisfying x0 = 1, x1 = 2, x2 = 5.

Solution. The verifications that these sequences satisfy the recurrence (and hence lie in V ) are left
to the reader. They are a basis because [1) = T (1, 1, 1), [n) = T (0, 1, 2), and
[(-1)n) = T (1, -1, 1); and {(1, 1, 1), (0, 1, 2), (1, -1, 1)} is a basis of R3. Hence the
sequence [xn) in V satisfying x0 = 1, x1 = 2, x2 = 5 is a linear combination of this basis:

                                               [xn) = t1[1) + t2[n) + t3[(-1)n)

The nth term is xn = t1 + nt2 + (-1)nt3, so taking n = 0, 1, 2 gives

                                                      1 = x0 = t1 + 0 + t3
                                                      2 = x1 = t1 + t2 - t3
                                                      5 = x2 = t1 + 2t2 + t3

This     has  the  solution  t1  =  t3  =  1,  t2  =  2,  so  xn  =  1  +  2n  +  21 (-1)n.
                                                                     2
                                           2

    This technique clearly works for any linear recurrence of length k: Simply take your favourite basis

{v1, . . . , vk} of Rk--perhaps the standard basis--and compute T (v1), . . . , T (vk). This is a basis of V all
right, but the nth term of T (vi) is not usually given as an explicit function of n. (The basis in Example 7.5.2
was carefully chosen so that the nth terms of the three sequences were 1, n, and (-1)n, respectively, each

a simple function of n.)

    However, it turns out that an explicit basis of V can be given in the general situation. Given the
recurrence (7.5) again:

                                        xn+k = r0xn + r1xn+1 + · · · + rk-1xn+k-1

the idea is to look for numbers  such that the power sequence [ n) satisfies (7.5). This happens if and

only if                                  n+k = r0 n + r1 n+1 + · · · + rk-1 n+k-1

holds for all n  0. This is true just when the case n = 0 holds; that is,

                                                k = r0 + r1 + · · · + rk-1 k-1

The polynomial                             p(x) = xk - rk-1xk-1 - · · · - r1x - r0

is called the polynomial associated with the linear recurrence (7.5). Thus every root  of p(x) provides a
sequence [ n) satisfying (7.5). If there are k distinct roots, the power sequences provide a basis. Inciden-
tally, if  = 0, the sequence [ n) is 1, 0, 0, . . . ; that is, we accept the convention that 00 = 1.

Theorem 7.5.2

Let r0, r1, . . . , rk-1 be real numbers; let

                   V = {[xn) | xn+k = r0xn + r1xn+1 + · · · + rk-1xn+k-1 for all n  0}

denote the vector space of all sequences satisfying the linear recurrence relation determined by

r0, r1, . . . , rk-1; and let              p(x) = xk - rk-1xk-1 - · · · - r1x - r0
                                                                                        . . More on Linear Recurrences

denote the polynomial associated with the recurrence relation. Then
   1. [ n) lies in V if and only if  is a root of p(x).
   2. If 1, 2, . . . , k are distinct real roots of p(x), then {[1n), [2n), . . . , [kn)} is a basis of V .

Proof. It remains to prove (2). But [in) = T (vi) where vi = (1, i, i2, . . . ,  k-1 i ), so (2) follows by
Theorem 7.5.1, provided that (v1, v2, . . . , vn) is a basis of Rk. This is true provided that the matrix with

the vi as its rows                            2 k-1 
                                                  1 1 1 · · · 1
                                                  1         2              k-1 
                                                       2          ···      2
                                                  ...  ...    2              ...
                                                               ... . . .
                                                                                      
                                                                                      

                                                  1    k    2     ···     kk-1

                                                              k

is invertible. But this is a Vandermonde matrix and so is invertible if the i are distinct (Theorem 3.2.7).
This proves (2).

Example 7.5.3

Find the solution of xn+2 = 2xn + xn+1 that satisfies x0 = a, x1 = b.

Solution. The associated polynomial is p(x) = x2 - x - 2 = (x - 2)(x + 1). The roots are 1 = 2
and 2 = -1, so the sequences [2n) and [(-1)n) are a basis for the space of solutions by
Theorem 7.5.2. Hence every solution [xn) is a linear combination

                                             [xn) = t1[2n) + t2[(-1)n)

This means that xn = t12n + t2(-1)n holds for n = 0, 1, 2, . . ., so (taking n = 0, 1) x0 = a and
x1 = b give

                                                             t1 + t2 = a
                                                            2t1 - t2 = b

These  are  easily  solved:  t1  =  1 (a  +  b)   and  t2   =  31 (2a  -  b),     so

                                    3

                                    tn    =    1  [(a  +    b)2n  +  (2a  -  b)(-1)n]
                                               3
Linear Transformations

The Shift Operator

If p(x) is the polynomial associated with a linear recurrence relation of length k, and if p(x) has k distinct
roots 1, 2, . . . , k, then p(x) factors completely:

                             p(x) = (x - 1)(x - 2) · · · (x - k)

Each root i provides a sequence [in) satisfying the recurrence, and they are a basis of V by Theorem 7.5.2.
In this case, each i has multiplicity 1 as a root of p(x). In general, a root  has multiplicity m if
p(x) = (x -  )mq(x), where q( ) = 0. In this case, there are fewer than k distinct roots and so fewer
than k sequences [ n) satisfying the recurrence. However, we can still obtain a basis because, if  has
multiplicity m (and  = 0), it provides m linearly independent sequences that satisfy the recurrence. To
prove this, it is convenient to give another way to describe the space V of all sequences satisfying a given
linear recurrence relation.

    Let S denote the vector space of all sequences and define a function

                             S : S  S by S[xn) = [xn+1) = [x1, x2, x3, . . . )

S is clearly a linear transformation and is called the shift operator on S. Note that powers of S shift the
sequence further: S2[xn) = S[xn+1) = [xn+2). In general,

                Sk[xn) = [xn+k) = [xk, xk+1, . . . ) for all k = 0, 1, 2, . . .

But then a linear recurrence relation

                xn+k = r0xn + r1xn+1 + · · · + rk-1xn+k-1 for all n = 0, 1, . . .

can be written               Sk[xn) = r0[xn) + r1S[xn) + · · · + rk-1Sk-1[xn)

                                                                                           (7.6)

Now let p(x) = xk -rk-1xk-1 -· · ·-r1x-r0 denote the polynomial associated with the recurrence relation.
The set L[S, S] of all linear transformations from S to itself is a vector space (verify8) that is closed under

composition. In particular,  p(S) = Sk - rk-1Sk-1 - · · · - r1S - r0

is a linear transformation called the evaluation of p at S. The point is that condition (7.6) can be written
as

                                                      p(S){[xn)} = 0

In other words, the space V of all sequences satisfying the recurrence relation is just ker [p(S)]. This is the
first assertion in the following theorem.

Theorem 7.5.3
Let r0, r1, . . . , rk-1 be real numbers, and let

                  V = {[xn) | xn+k = r0xn + r1xn+1 + · · · + rk-1xn+k-1 for all n  0}
denote the space of all sequences satisfying the linear recurrence relation determined by

8See Exercises ?? and ??.
                                                                                        . . More on Linear Recurrences

r0, r1, . . . , rk-1. Let                p(x) = xk - rk-1xk-1 - · · · - r1x - r0

denote the corresponding polynomial. Then:

1. V = ker [p(S)], where S is the shift operator.

2. If p(x) = (x -  )mq(x), where  = 0 and m > 1, then the sequences

                                               {[ n), [n n), [n2 n), . . . , [nm-1 n)}

all lie in V and are linearly independent.

                                                       n n(n-1)···(n-k+1)
Proof (Sketch). It remains to prove (2). If k =                      k!    denotes the binomial coefficient, the idea

is to use (1) to show that the sequence sk = nk  n is a solution for each k = 0, 1, . . . , m - 1. Then
(2) of Theorem 7.5.1 can be applied to show that {s0, s1, . . . , sm-1} is linearly independent. Finally, the
sequences         [nk n),        0,  1,  ...,  m - 1,  in  the  present  theorem        can  be  given  by           m-1  ak js j,  where
           tk  =           k  =                                                                             tk  =
                                                                                                                    j=0

A = ai j is an invertible matrix. Then (2) follows. We omit the details.

    This theorem combines with Theorem 7.5.2 to give a basis for V when p(x) has k real roots (not neces-
sarily distinct) none of which is zero. This last requirement means r0 = 0, a condition that is unimportant
in practice (see Remark 1 below).

Theorem 7.5.4
Let r0, r1, . . . , rk-1 be real numbers with r0 = 0; let

                   V = {[xn) | xn+k = r0xn + r1xn+1 + · · · + rk-1xn+k-1 for all n  0}

denote the space of all sequences satisfying the linear recurrence relation of length k determined by
r0, . . . , rk-1; and assume that the polynomial

                                         p(x) = xk - rk-1xk-1 - · · · - r1x - r0

factors completely as

                                     p(x) = (x - 1)m1(x - 2)m2 · · · (x - p)mp

where 1, 2, . . . , p are distinct real numbers and each mi  1. Then i = 0 for each i, and

                                                n , n n , . . . , nm1-1 n111

                                                n , n n , . . . , nm2-1 n222
                                                                ..
                                                                  .

                                               n    ,  n pn     , ...,   nmp-1 n

                                                 p                                   p

is a basis of V .

Proof. There are m1 + m2 + · · · + mp = k sequences in all so, because dim V = k, it suffices to show that
they are linearly independent. The assumption that r0 = 0, implies that 0 is not a root of p(x). Hence each
Linear Transformations

i = 0, so {[in),  [nin),  ...,  [nmi-1 n)}     is  linearly  independent  by  Theorem  7.5.3.  The proof that the

                                            i
whole set of sequences is linearly independent is omitted.

Example 7.5.4
Find a basis for the space V of all sequences [xn) satisfying

                                         xn+3 = -9xn - 3xn+1 + 5xn+2

Solution. The associated polynomial is

                                  p(x) = x3 - 5x2 + 3x + 9 = (x - 3)2(x + 1)

Hence 3 is a double root, so [3n) and [n3n) both lie in V by Theorem 7.5.3 (the reader should verify
this). Similarly,  = -1 is a root of multiplicity 1, so [(-1)n) lies in V . Hence
{[3n), [n3n), [(-1)n)} is a basis by Theorem 7.5.4.

Remark 1
If r0 = 0 [so p(x) has 0 as a root], the recurrence reduces to one of shorter length. For example, consider

                                        xn+4 = 0xn + 0xn+1 + 3xn+2 + 2xn+3                     (7.7)

If we set yn = xn+2, this recurrence becomes yn+2 = 3yn + 2yn+1, which has solutions [3n) and [(-1)n).
These give the following solution to (7.5):

                                        0, 0, 1, 3, 32, . . .
                                        0, 0, 1, -1, (-1)2, . . .

In addition, it is easy to verify that

                                               [1, 0, 0, 0, 0, . . . )

                                               [0, 1, 0, 0, 0, . . . )

are also solutions to (7.7). The space of all solutions of (7.5) has dimension 4 (Theorem 7.5.1), so these
sequences are a basis. This technique works whenever r0 = 0.

Remark 2

Theorem 7.5.4 completely describes the space V of sequences that satisfy a linear recurrence relation for
which the associated polynomial p(x) has all real roots. However, in many cases of interest, p(x) has
complex roots that are not real. If p(µ) = 0, µ complex, then p(µ) = 0 too (µ the conjugate), and the
main observation is that [µn + µn) and [i(µn + µn)) are real solutions. Analogs of the preceding theorems
can then be proved.
                                                                                 . . More on Linear Recurrences

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.
                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.
                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!
Chapter 8

                                                                           Orthogonality

In Section 5.3 we introduced the dot product in Rn and extended the basic geometric notions of length and

distance. A set {f1, f2, . . . , fm} of nonzero vectors in Rn was called an orthogonal set if fi · f j = 0 for
all i = j, and it was proved that every orthogonal set is independent. In particular, it was observed that
the expansion of a vector as a linear combination of orthogonal basis vectors is easy to obtain because

formulas exist for the coefficients. Hence the orthogonal bases are the "nice" bases, and much of this
chapter is devoted to extending results about bases to orthogonal bases. This leads to some very powerful
methods and theorems. Our first task is to show that every subspace of Rn has an orthogonal basis.

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                    Engage Active Learning App!

              Vretta-Lyryx Engage is an active learning app designed to increase
             student engagement in reading linear algebra material. The content is
           "chunked" into small blocks, each with an interactive assessment activity

                                      to promote comprehension.

            Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

8. Orthogonal Complements and Projections

If {v1, . . . , vm} is linearly independent in a general vector space, and if vm+1 is not in span {v1, . . . , vm},
then {v1, . . . , vm, vm+1} is independent (Lemma 6.4.1). Here is the analog for orthogonal sets in Rn.

Lemma 8.1.1: Orthogonal Lemma
Let {f1, f2, . . . , fm} be an orthogonal set in Rn. Given x in Rn, write

           1fm+1 = x - x·f1 f 2 f1 - x·f2 f 2 f2 - · · · - x·fm f 2 fm2m

              355
  6 Orthogonality

    Then:

       1. fm+1 · fk = 0 for k = 1, 2, . . . , m.
       2. If x is not in span {f1, . . . , fm}, then fm+1 = 0 and {f1, . . . , fm, fm+1} is an orthogonal set.

Proof. For convenience, write ti = (x · fi)/ fi 2 for each i. Given 1  k  m:
                           fm+1 · fk = (x - t1f1 - · · · - tkfk - · · · - tmfm) · fk
                                     = x · fk - t1(f1 · fk) - · · · - tk(fk · fk) - · · · - tm(fm · fk)
                                     = x · fk - tk fk 2
                                     =0

This proves (1), and (2) follows because fm+1 = 0 if x is not in span {f1, . . . , fm}.
    The orthogonal lemma has three important consequences for Rn. The first is an extension for orthog-

onal sets of the fundamental fact that any independent set is part of a basis (Theorem 6.4.1).

   Theorem 8.1.1
   Let U be a subspace of Rn.

       1. Every orthogonal subset {f1, . . . , fm} in U is a subset of an orthogonal basis of U .
       2. U has an orthogonal basis.

Proof.

1. If span {f1, . . . , fm} = U , it is already a basis. Otherwise, there exists x in U outside span {f1, . . . , fm}.
   If fm+1 is as given in the orthogonal lemma, then fm+1 is in U and {f1, . . . , fm, fm+1} is orthogonal.
   If span {f1, . . . , fm, fm+1} = U , we are done. Otherwise, the process continues to create larger and
   larger orthogonal subsets of U . They are all independent by Theorem 5.3.5, so we have a basis when
   we reach a subset containing dim U vectors.

2. If U = {0}, the empty basis is orthogonal. Otherwise, if f = 0 is in U , then {f} is orthogonal, so (2)
   follows from (1).

    We can improve upon (2) of Theorem 8.1.1. In fact, the second consequence of the orthogonal lemma
is a procedure by which any basis {x1, . . . , xm} of a subspace U of Rn can be systematically modified to
yield an orthogonal basis {f1, . . . , fm} of U . The fi are constructed one at a time from the xi.

    To start the process, take f1 = x1. Then x2 is not in span {f1} because {x1, x2} is independent, so take

        f2 = x2 - x2·f1 f1 2 f1

Thus {f1, f2} is orthogonal by Lemma 8.1.1. Moreover, span {f1, f2} = span {x1, x2} (verify), so x3 is
not in span {f1, f2}. Hence {f1, f2, f3} is orthogonal where

        x3·f1  x3·f2
        f3 = x3 - f 2 f1 - f 2 f2
        1      2
                                                               8. . Orthogonal Complements and Projections

Again, span {f1, f2, f3} = span {x1, x2, x3}, so x4 is not in span {f1, f2, f3} and the process continues.
At the mth iteration we construct an orthogonal set {f1, . . . , fm} such that

                                span {f1, f2, . . . , fm} = span {x1, x2, . . . , xm} = U
Hence {f1, f2, . . . , fm} is the desired orthogonal basis of U . The procedure can be summarized as follows.
8 Orthogonality

                x3                     Theorem 8.1.2: Gram-Schmidt Orthogonalization Algorithm1
                                       If {x1, x2, . . . , xm} is any basis of a subspace U of Rn, construct
                                       f1, f2, . . . , fm in U successively as follows:

0               f2                              f1 = x1
                                                f2 = x2 - x2·f1 f1 2 f1
            f1                                                  x3·f1      x3·f2
                                                f3 = x3 - f 2 f1 - f 2 f2
               span {f1, f2}

                                                 ..             1          2

            Gram-Schmidt                        .

                                                                xk ·f1     xk ·f2  xk ·fk-1
                                                fk = xk - f 2 f1 - f 2 f2 - · · · - f 2 fk-1
                                                                1          2       k-1

            f3                         for each k = 2, 3, . . . , m. Then

0               f2                         1. {f1, f2, . . . , fm} is an orthogonal basis of U .
                                           2. span {f1, f2, . . . , fk} = span {x1, x2, . . . , xk} for each
            f1
                                              k = 1, 2, . . . , m.
               span {f1, f2}

    The process (for k = 3) is depicted in the diagrams. Of course, the algorithm converts any basis of Rn
itself into an orthogonal basis.

Example 8.1.1

                                                             1 1 -1 -1 
Find an orthogonal basis of the row space of A =  3 2 0 1 .

                                                               10 1 0

Solution. Let x1, x2, x3 denote the rows of A and observe that {x1, x2, x3} is linearly independent.
Take f1 = x1. The algorithm gives

                f2 = x2 - xf2·f1 1 2 f1 = (3, 2, 0, 1) - 44 (1, 1, -1, -1) = (2, 1, 1, 2)
                f3 = x3 - x3·f1 f1 2 f1 - x3·f2 f2 2 f2 = x3 - 04 f1 - 130 f2 = 110 (4, -3, 7, -6)

Hence {(1,  1,  -1,  -1),     (2,  1,  1,  2),  1 (4,  -3,  7,  -6)} is the orthogonal basis provided by the

                                                10
algorithm. In hand calculations it may be convenient to eliminate fractions (see the Remark

below), so {(1, 1, -1, -1), (2, 1, 1, 2), (4, -3, 7, -6)} is also an orthogonal basis for row A.

    1Erhardt Schmidt (1876-1959) was a German mathematician who studied under the great David Hilbert and later developed
the theory of Hilbert spaces. He first described the present algorithm in 1907. Jörgen Pederson Gram (1850-1916) was a Danish
actuary.
                                         8. . Orthogonal Complements and Projections

Remark

Observe that the vector xf·fi i 2 fi is unchanged if a nonzero scalar multiple of fi is used in place of fi. Hence,
if a newly constructed fi is multiplied by a nonzero scalar at some stage of the Gram-Schmidt algorithm,
the subsequent fs will be unchanged. This is useful in actual calculations.

Projections

                  Suppose a point x and a plane U through the origin in R3 are given, and

           x x-p  we want to find the point p in the plane that is closest to x. Our geometric
0p                intuition assures us that such a point p exists. In fact (see the diagram), p
                  must be chosen in such a way that x - p is perpendicular to the plane.

U                 Now we make two observations: first, the plane U is a subspace of R3

                  (because U contains the origin); and second, that the condition that x - p

is perpendicular to the plane U means that x - p is orthogonal to every vector in U . In these terms the

whole discussion makes sense in Rn. Furthermore, the orthogonal lemma provides exactly what is needed

to find p in this more general setting.

Definition 8.1 Orthogonal Complement of a Subspace of Rn
If U is a subspace of Rn, define the orthogonal complement U  of U (pronounced "U -perp") by

                                   U  = {x in Rn | x · y = 0 for all y in U }

    The following lemma collects some useful properties of the orthogonal complement; the proof of (1)
and (2) is left as Exercise ??.

   Lemma 8.1.2
   Let U be a subspace of Rn.

       1. U  is a subspace of Rn.
       2. {0} = Rn and (Rn) = {0}.
       3. If U = span {x1, x2, . . . , xk}, then U  = {x in Rn | x · xi = 0 for i = 1, 2, . . . , k}.

Proof.
   3. Let U = span {x1, x2, . . . , xk}; we must show that U  = {x | x · xi = 0 for each i}. If x is in U 
       then x · xi = 0 for all i because each xi is in U . Conversely, suppose that x · xi = 0 for all i; we must
       show that x is in U , that is, x · y = 0 for each y in U . Write y = r1x1 + r2x2 + · · · + rkxk, where each
       ri is in R. Then, using Theorem 5.3.1,

                        x · y = r1(x · x1) + r2(x · x2) + · · · + rk(x · xk) = r10 + r20 + · · · + rk0 = 0

       as required.
6 Orthogonality

Example 8.1.2
Find U  if U = span {(1, -1, 2, 0), (1, 0, -2, 3)} in R4.

Solution. By Lemma 8.1.2, x = (x, y, z, w) is in U  if and only if it is orthogonal to both
(1, -1, 2, 0) and (1, 0, -2, 3); that is,

                               x - y + 2z  =0

                               x - 2z + 3w = 0

Gaussian elimination gives U  = span {(2, 4, 1, 0), (3, 3, 0, -1)}.

                         Now consider vectors x and d = 0 in R3. The projection p = projd x
                    of x on d was defined in Section 4.2 as in the diagram.
                 U

x           d            The following formula for p was derived in Theorem 4.2.4

         p                                                           x·d
                                           p = projd x = d 2 d

        0

                                      where it is shown that x - p is orthogonal to d. Now observe that the line
                                      U = Rd = {td | t  R} is a subspace of R3, that {d} is an orthogonal basis
of U , and that p  U and x - p  U  (by Theorem 4.2.4).

In this form, this makes sense for any vector x in Rn and any subspace U of Rn, so we generalize it

as follows. If {f1, f2, . . . , fm} is an orthogonal basis of U , we define the projection p of x on U by the

formula                  x·f1      x·f2                    x·fm

                    p = f 2 f1 + f 2 f2 + · · · + f 2 fm12 m                                 (8.1)

Then p  U and (by the orthogonal lemma) x - p  U , so it looks like we have a generalization of
Theorem 4.2.4.

    However there is a potential problem: the formula (8.1) for p must be shown to be independent of the
choice of the orthogonal basis {f1, f2, . . . , fm}. To verify this, suppose that {f1, f2, . . . , fm} is another
orthogonal basis of U , and write

                    p =  x·f1      x·f2                    x·fm 

                           2 f1 +    2 f2 + · · · +        f 2 fm

                         f1        f2                       m

As before, p  U and x - p  U , and we must show that p = p. To see this, write the vector p - p as
follows:

                                               p - p = (x - p) - (x - p)

This vector is in U (because p and p are in U ) and it is in U  (because x - p and x - p are in U ), and
so it must be zero (it is orthogonal to itself!). This means p = p as desired.

    Hence, the vector p in equation (8.1) depends only on x and the subspace U , and not on the choice
of orthogonal basis {f1, . . . , fm} of U used to compute it. Thus, we are entitled to make the following
definition:
                                      8. . Orthogonal Complements and Projections 6

Definition 8.2 Projection onto a Subspace of Rn
Let U be a subspace of Rn with orthogonal basis {f1, f2, . . . , fm}. If x is in Rn, the vector

projU x = x·f1 f 2 f1 + x·f2 f 2 f2 + · · · + x·fm f 2 fm12        m

is called the orthogonal projection of x on U. For the zero subspace U = {0}, we define

                             proj{0} x = 0

The preceding discussion proves (1) of the following theorem.

   Theorem 8.1.3: Projection Theorem
   If U is a subspace of Rn and x is in Rn, write p = projU x. Then:

       1. p is in U and x - p is in U .
       2. p is the vector in U closest to x in the sense that

                                           x - p < x - y for all y  U , y = p

Proof.

   1. This is proved in the preceding discussion (it is clear if U = {0}).
   2. Write x - y = (x - p) + (p - y). Then p - y is in U and so is orthogonal to x - p by (1). Hence, the

       Pythagorean theorem gives
                                          x-y 2 = x-p 2+ p-y 2 > x-p 2

       because p - y = 0. This gives (2).

Example 8.1.3

Let U = span {x1, x2} in R4 where x1 = (1, 1, 0, 1) and x2 = (0, 1, 1, 2). If x = (3, -1, 0, 2),
find the vector in U closest to x and express x as the sum of a vector in U and a vector orthogonal
to U.

Solution. {x1, x2} is independent but not orthogonal. The Gram-Schmidt process gives an
orthogonal basis {f1, f2} of U where f1 = x1 = (1, 1, 0, 1) and

f2 = x2 - x2·f1 f1 2 f1 = x2 - 33 f1 = (-1, 0, 1, 1)

Hence, we can compute the projection using {f1, f2}:

p = projU x =  x·f1       x·f2     =  34 f1           +  3-1 f2 =  1  5 4 -1 3
                                                                   3
                  2 f1 +     2 f2
               f1         f2
6 Orthogonality

  Thus, p is the vector in U closest to x, and x - p = 13(4, -7, 1, 3) is orthogonal to every vector in
  U . (This can be verified by checking that it is orthogonal to the generators x1 and x2 of U .) The
  required decomposition of x is thus

                               x = p + (x - p) = 13 (5, 4, -1, 3) + 13 (4, -7, 1, 3)

Example 8.1.4
Find the point in the plane with equation 2x + y - z = 0 that is closest to the point (2, -1, -3).
Solution. We write R3 as rows. The plane is the subspace U whose points (x, y, z) satisfy
z = 2x + y. Hence

                       U = {(s, t, 2s + t) | s, t in R} = span {(0, 1, 1), (1, 0, 2)}
The Gram-Schmidt process produces an orthogonal basis {f1, f2} of U where f1 = (0, 1, 1) and
f2 = (1, -1, 1). Hence, the vector in U closest to x = (2, -1, -3) is

                          projU x = x·f1 f1 2 f1 + x·f2 f2 2 f2 = -2f1 + 0f2 = (0, -2, -2)
Thus, the point in U closest to (2, -1, -3) is (0, -2, -2).

The next theorem shows that projection on a subspace of Rn is actually a linear operator Rn  Rn.

Theorem 8.1.4
Let U be a fixed subspace of Rn. If we define T : Rn  Rn by

                                       T (x) = projU x for all x in Rn
   1. T is a linear operator.
   2. im T = U and ker T = U .
   3. dim U + dim U  = n.

Proof. If U = {0}, then U  = Rn, and so T (x) = proj{0} x = 0 for all x. Thus T = 0 is the zero (linear)
operator, so (1), (2), and (3) hold. Hence assume that U = {0}.

1. If {f1, f2, . . . , fm} is an orthonormal basis of U , then

T (x) = (x · f1)f1 + (x · f2)f2 + · · · + (x · fm)fm for all x in Rn  (8.2)

by the definition of the projection. Thus T is linear because

(x + y) · fi = x · fi + y · fi and (rx) · fi = r(x · fi) for each i
                                                                             8. . Orthogonal Diagonalization 6

2. We have im T  U by (8.2) because each fi is in U . But if x is in U , then x = T (x) by (8.2) and the
   expansion theorem applied to the space U . This shows that U  im T , so im T = U .
   Now suppose that x is in U . Then x · fi = 0 for each i (again because each fi is in U ) so x is in
   ker T by (8.2). Hence U   ker T . On the other hand, Theorem 8.1.3 shows that x - T (x) is in U 
   for all x in Rn, and it follows that ker T  U . Hence ker T = U , proving (2).

3. This follows from (1), (2), and the dimension theorem (Theorem 7.2.4).

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                  Engage Active Learning App!

            Vretta-Lyryx Engage is an active learning app designed to increase
           student engagement in reading linear algebra material. The content is
         "chunked" into small blocks, each with an interactive assessment activity

                                    to promote comprehension.

          Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

8. Orthogonal Diagonalization

Recall (Theorem 5.5.3) that an n × n matrix A is diagonalizable if and only if it has n linearly independent

eigenvectors. Moreover, the matrix P with these eigenvectors as columns is a diagonalizing matrix for A,

that is  P-1AP is diagonal.

As we have seen, the really nice bases of Rn are the orthogonal ones, so a natural question is: which n × n
matrices have an orthogonal basis of eigenvectors? These turn out to be precisely the symmetric matrices,
and this is the main result of this section.

    Before proceeding, recall that an orthogonal set of vectors is called orthonormal if v = 1 for each
vector v in the set, and that any orthogonal set {v1, v2, . . . , vk} can be "normalized", that is converted into
an orthonormal set { v11 v1, v12 v2, . . . , v1k vk}. In particular, if a matrix A has n orthogonal eigenvectors,
they can (by normalizing) be taken to be orthonormal. The corresponding diagonalizing matrix P has
orthonormal columns, and such matrices are very easy to invert.
6 Orthogonality

     Theorem 8.2.1
     The following conditions are equivalent for an n × n matrix P.

        1. P is invertible and P-1 = PT .
        2. The rows of P are orthonormal.
        3. The columns of P are orthonormal.

Proof. First recall that condition (1) is equivalent to PPT = I by Corollary 2.4.2 of Theorem 2.4.5. Let

x1,  x2,  ...,  xn  denote  the  rows  of  P.  Then  xT   is the  jth column of PT , so the (i,  j)-entry of PPT  is xi · x j.

                                                       j

Thus PPT = I means that xi · x j = 0 if i = j and xi · x j = 1 if i = j. Hence condition (1) is equivalent to

(2). The proof of the equivalence of (1) and (3) is similar.

     Definition 8.3 Orthogonal Matrices

     An n × n matrix P is called an orthogonal matrix2if it satisfies one (and hence all) of the
     conditions in Theorem 8.2.1.

     Example 8.2.1               cos  - sin          is orthogonal for any angle  .
     The rotation matrix         sin  cos 

    These orthogonal matrices have the virtue that they are easy to invert--simply take the transpose. But
they have many other important properties as well. If T : Rn  Rn is a linear operator, we will prove
(Theorem 10.4.3) that T is distance preserving if and only if its matrix is orthogonal. In particular, the
matrices of rotations and reflections about the origin in R2 and R3 are all orthogonal (see Example 8.2.1).

    It is not enough that the rows of a matrix A are merely orthogonal for A to be an orthogonal matrix.
Here is an example.

     Example 8.2.2

                     2 1 1

     The matrix  -1 1 1  has orthogonal rows but the columns are not orthogonal. However, if
                        0 -1 1

                                                                  2 1 1     

                                                           6 6 6
     the rows are normalized, the resulting matrix  -1 1 1  is orthogonal (so the columns are
                                                           3 3 3
                                                                  0 -12 12
     now orthonormal as the reader can verify).

2In view of (2) and (3) of Theorem 8.2.1, orthonormal matrix might be a better name. But orthogonal matrix is standard.
                                                                            8. . Orthogonal Diagonalization 6

Example 8.2.3
If P and Q are orthogonal matrices, then PQ is also orthogonal, as is P-1 = PT .
Solution. P and Q are invertible, so PQ is also invertible and

                                    (PQ)-1 = Q-1P-1 = QT PT = (PQ)T
Hence PQ is orthogonal. Similarly,

                                        (P-1)-1 = P = (PT )T = (P-1)T
shows that P-1 is orthogonal.

Definition 8.4 Orthogonally Diagonalizable Matrices
An n × n matrix A is said to be orthogonally diagonalizable when an orthogonal matrix P can be
found such that P-1AP = PT AP is diagonal.

This condition turns out to characterize the symmetric matrices.

Theorem 8.2.2: Principal Axes Theorem
The following conditions are equivalent for an n × n matrix A.

   1. A has an orthonormal set of n eigenvectors.
   2. A is orthogonally diagonalizable.
   3. A is symmetric.

Proof. (1)  (2). Given (1), let x1, x2, . . . , xn be orthonormal eigenvectors of A. Then P = x1 x2 . . . xn
is orthogonal, and P-1AP is diagonal by Theorem 3.4.1. This proves (2). Conversely, given (2) let P-1AP

be diagonal where P is orthogonal. If x1, x2, . . . , xn are the columns of P then {x1, x2, . . . , xn} is an
orthonormal basis of Rn that consists of eigenvectors of A by Theorem 3.4.1. This proves (1).

    (2)  (3). If PT AP = D is diagonal, where P-1 = PT , then A = PDPT . But DT = D, so this gives
AT = PT T DT PT = PDPT = A.

    (3)  (2). If A is an n × n symmetric matrix, we proceed by induction on n. If n = 1, A is already

diagonal. If n > 1, assume that (3)  (2) for (n - 1) × (n - 1) symmetric matrices. By Theorem 5.5.7 let

1 be a (real) eigenvalue of A, and let Ax1 = 1x1, where x1 = 1. Use the Gram-Schmidt algorithm to
find an orthonormal basis {x1, x2, . . . , xn} for Rn. Let P1 = x1 x2 . . . xn , so P1 is an orthogonal

matrix and P1 AP1 =T 1 B      in block form by Lemma 5.5.2. But P1T AP1 is symmetric (A is), so it
                     0 A1
follows that B = 0 and A1 is symmetric. Then, by induction, there exists an (n - 1) × (n - 1) orthogonal
matrix Q such that QT A1Q = D1 is diagonal. Observe that P2 = 1 0 0 Q is orthogonal, and compute:

                     (P1P2)T  A(P1P2)  =  PT  (P1T  AP1)P2

                                           2
66 Orthogonality

                                      = 0 QT 1 0     1 0      10
                                      = 1 0 0 D1     0 A1     0Q

is diagonal. Because P1P2 is orthogonal, this proves (2).

A set of orthonormal eigenvectors of a symmetric matrix A is called a set of principal axes for A. The
name comes from geometry, and this is discussed in Section 8.9. Because the eigenvalues of a (real)
symmetric matrix are real, Theorem 8.2.2 is also called the real spectral theorem, and the set of distinct
eigenvalues is called the spectrum of the matrix. In full generality, the spectral theorem is a similar result
for matrices with complex entries (Theorem 8.7.8).

Example 8.2.4

                                                                                     1 0 -1 
Find an orthogonal matrix P such that P-1AP is diagonal, where A =  0 1 2 .

                                                                                       -1 2 5

Solution. The characteristic polynomial of A is (adding twice row 1 to row 2):

                                  x-1          0    1
                  cA(x) = det  0             x-1
                                              -2   -2  = x(x - 1)(x - 6)
                                      1           x-5

Thus the eigenvalues are  = 0, 1, and 6, and corresponding eigenvectors are

                                    1   2   -1 

                     x1 =  -2  x2 =  1  x3 =  2 
                                      1           0        5

respectively. Moreover, by what appears to be remarkably good luck, these eigenvectors are
orthogonal. We have x1 2 = 6, x2 2 = 5, and x3 2 = 30, so

                                                      5       
                                                           2 6 -1               

                  P = 1 x1 1 x2 1 x3 = 1  -25                 6 2
                  6                5     30          30 
                                                          5 05

is an orthogonal matrix. Thus P-1 = PT and

                                                0 0 0
                                      PT AP =  0 1 0 

                                                    006

by the diagonalization algorithm.

    Actually, the fact that the eigenvectors in Example 8.2.4 are orthogonal is no coincidence. Theo-
rem 5.5.4 guarantees they are linearly independent (they correspond to distinct eigenvalues); the fact that
                                                              8. . Orthogonal Diagonalization 6

the matrix is symmetric implies that they are orthogonal. To prove this we need the following useful fact
about symmetric matrices.

Theorem 8.2.3
If A is an n × n symmetric matrix, then

                                         (Ax) · y = x · (Ay)

for all columns x and y in Rn.3

Proof. Recall that x · y = xT y for all columns x and y. Because AT = A, we get
                                   (Ax) · y = (Ax)T y = xT AT y = xT Ay = x · (Ay)

Theorem 8.2.4

If A is a symmetric matrix, then eigenvectors of A corresponding to distinct eigenvalues are
orthogonal.

Proof. Let Ax =  x and Ay = µy, where  = µ. Using Theorem 8.2.3, we compute

                            (x · y) = ( x) · y = (Ax) · y = x · (Ay) = x · (µy) = µ(x · y)

Hence ( - µ)(x · y) = 0, and so x · y = 0 because  = µ.

    Now the procedure for diagonalizing a symmetric n × n matrix is clear. Find the distinct eigenvalues
(all real by Theorem 5.5.7) and find orthonormal bases for each eigenspace (the Gram-Schmidt algorithm
may be needed). Then the set of all these basis vectors is orthonormal (by Theorem 8.2.4) and contains n
vectors. Here is an example.

Example 8.2.5

                                                                8 -2 2 
Orthogonally diagonalize the symmetric matrix A =  -2 5 4 .

                                                                     2 45

Solution. The characteristic polynomial is

                x-8                           2   -2 
cA(x) = det  2                              x-5   -4  = x(x - 9)2
                                             -4  x-5
                   -2

Hence the distinct eigenvalues are 0 and 9 of multiplicities 1 and 2, respectively, so dim (E0) = 1
and dim (E9) = 2 by Theorem 5.5.6 (A is diagonalizable, being symmetric). Gaussian elimination

3The converse also holds (Exercise ??).
68 Orthogonality

gives

                                         1                                        -2   2  
                                  x1 =  2  ,            and E9(A) = span  1  ,  0 
       E0(A) = span {x1},                                                        0                  1
                                           -2

The eigenvectors in E9 are both orthogonal to x1 as Theorem 8.2.4 guarantees, but not to each
other. However, the Gram-Schmidt process yields an orthogonal basis

                                                            -2                            2

                      {x2, x3} of E9(A)      where         x2 =  1  and x3 =  4 

                                                                    0                     5

Normalizing gives orthonormal vectors { 13 x1, 15 x2, 31 5 x3}, so

                                                                5 -6 2 
                            P = 1 x1 1 x2 1 x3 = 1  25
                               3      5           35       35                    3 4

                                                                    -2 5 0 5

is an orthogonal matrix such that P-1AP is diagonal.

It is worth noting that other, more convenient, diagonalizing matrices P exist. For example,

       2                     -2 

y2 =  1  and y3 =  2  lie in E9(A) and they are orthogonal. Moreover, they both have
       2                    1

norm 3 (as does x1), so

                                                            1 2 -2 
                                      1 x1   1y       1y   = 1
                            Q=                                      21           2
                                      3      32       33       3
                                                                    -2 2 1

is a nicer orthogonal matrix with the property that Q-1AQ is diagonal.

       x2                          If A is symmetric and a set of orthogonal eigenvectors of A is given,

                        x1     the eigenvectors are called principal axes of A. The name comes from
                               geometry. An expression q = ax21 +bx1x2 +cx22 is called a quadratic form
       O x1x2 = 1              in the variables x1 and x2, and the graph of the equation q = 1 is called a
                               conic in these variables. For example, if q = x1x2, the graph of q = 1 is
                               given in the first diagram.

y2                y1               But if we introduce new variables y1 and y2 by setting x1 = y1 + y2 and
                                                                          y2     y22,
                               x2  =  y1  -  y2,  then  q  becomes  q  =      -        a  diagonal  form  with  no  cross
                                                                           1
                               term y1y2 (see the second diagram). Because of this, the y1 and y2 axes

       O   y2  -  y2  =  1     are called the principal axes for the conic (hence the name). Orthogonal

            1      2

                               diagonalization provides a systematic method for finding principal axes.

                               Here is an illustration.
                                                                                        8. . Orthogonal Diagonalization 6

Example 8.2.6

Find  principal   axes  for    the  quadratic    form    q  =  x2  -  4x1x2  +   x22.

                                                                1

Solution. In order to utilize diagonalization, we first express q in matrix form. Observe that

                                           q = x1 x2 1 -4 x1 0 1 x2

The matrix here is not symmetric, but we can remedy that by writing

                                              q  =  x2   -  2x1  x2  -  2x2  x1  +  x2

                                                     1                               2

Then we have                                                  1 -2
                                                            -2 1
                                    q = x1 x2                                    x1 = xT Ax
                                                                                 x2

where x = x1 x2 and A = 1 -2 -2 1 is symmetric. The eigenvalues of A are 1 = 3 and
2 = -1, with corresponding (orthogonal) eigenvectors x1 = 1 -1 and x2 = 11 . Since

                   
 x1 = x2 = 2, so

                  P = 12              11         is orthogonal and PT AP = D =                     30
                                    -1 1                                                           0 -1

Now define new variables y1 = y by y = PT x, equivalently x = Py (since P-1 = PT ). Hence
                                  y2

                                    y1 = 12 (x1 - x2) and y2 = 12 (x1 + x2)

In terms of y1 and y2, q takes the form

                        q   =  xT   Ax  =  (Py)T  A(Py)     =  yT    (PT  AP)y   =     yT  Dy  =  3y21  -  y2

                                                                                                            2

Note  that  y  =  PT x  is  obtained    from  x  by   a  counterclockwise           rotation   of     (see  Theorem  2.4.6).
                                                                                                   4

Observe that the quadratic form q in Example 8.2.6 can be diagonalized in other ways. For example

                                           q  =    2  -  4x1x2     +   2  =   2  -  1   2
                                                                                    3
                                                  x1                  x2     z1        z2

where z1 = x1 - 2x2 and z2 = 3x2. We examine this more carefully in Section 8.9.

    If we are willing to replace "diagonal" by "upper triangular" in the principal axes theorem, we can
weaken the requirement that A is symmetric to insisting only that A has real eigenvalues.
Orthogonality

Theorem 8.2.5: Triangulation Theorem
If A is an n × n matrix with n real eigenvalues, an orthogonal matrix P exists such that PT AP is
upper triangular.4

Proof. We modify the proof of Theorem 8.2.2. If Ax1 = 1x1 where x1 = 1, let {x1, x2, . . . , xn} be an

orthonormal basis of Rn, and let P1 =  x1  x2  ···  xn  .  Then  P1  is  orthogonal  and  PT  AP1  =  1  B

                                                                                           1          0 A1

in block form. By induction, let QT A1Q = T1 be upper triangular where Q is of size (n - 1) × (n - 1) and
orthogonal. Then P2 = 1 0 is orthogonal, so P = P1P2 is also orthogonal and PT AP = 1 BQ
                      0Q                                                                              0 T1

is upper triangular.

The proof of Theorem 8.2.5 gives no way to construct the matrix P. However, an algorithm will be given in
Section 11.1 where an improved version of Theorem 8.2.5 is presented. In a different direction, a version
of Theorem 8.2.5 holds for an arbitrary matrix with complex entries (Schur's theorem in Section 8.7).

    As for a diagonal matrix, the eigenvalues of an upper triangular matrix are displayed along the main
diagonal. Because A and PT AP have the same determinant and trace whenever P is orthogonal, Theo-
rem 8.2.5 gives:

Corollary 8.2.1
If A is an n × n matrix with real eigenvalues 1, 2, . . . , n (possibly not all distinct), then
det A = 12 . . . n and tr A = 1 + 2 + · · · + n.

This corollary remains true even if the eigenvalues are not real (using Schur's theorem).

    4There is also a lower triangular version.
                                                                                   8. . Positive Definite Matrices

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.
                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.
                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

8. Positive Definite Matrices

All the eigenvalues of any symmetric matrix are real; this section is about the case in which the eigenvalues
are positive. These matrices, which arise whenever optimization (maximum and minimum) problems are
encountered, have countless applications throughout science and engineering. They also arise in statistics
(for example, in factor analysis used in the social sciences) and in geometry (see Section 8.9). We will
encounter them again in Chapter 10 when describing all inner products in Rn.

   Definition 8.5 Positive Definite Matrices
   A square matrix is called positive definite if it is symmetric and all its eigenvalues  are positive,
   that is  > 0.

    Because these matrices are symmetric, the principal axes theorem plays a central role in the theory.

   Theorem 8.3.1
   If A is positive definite, then it is invertible and det A > 0.

Proof. If A is n × n and the eigenvalues are 1, 2, . . . , n, then det A = 12 · · · n > 0 by the principal
axes theorem (or the corollary to Theorem 8.2.5).
      Orthogonality

    If x is a column in Rn and A is any real n × n matrix, we view the 1 × 1 matrix xT Ax as a real number.
With this convention, we have the following characterization of positive definite matrices.

   Theorem 8.3.2
   A symmetric matrix A is positive definite if and only if xT Ax > 0 for every column x = 0 in Rn.

Proof. A is symmetric so, by the principal axes theorem, let PT AP = D = diag (1, 2, . . . , n) where
P-1 = PT and the i are the eigenvalues of A. Given a column x in Rn, write y = PT x = y1 y2 . . . yn T .

Then                 xT Ax = xT (PDPT )x = yT Dy = 1y21 + 2y22 + · · · + ny2n

                                                                                         (8.3)

If A is positive definite and x = 0, then xT Ax > 0 by (8.3) because some y j = 0 and every i > 0. Con-

versely, if xT Ax > 0 whenever x = 0, let x = Pe j = 0 where e j is column j of In. Then y = e j, so (8.3)
reads  j = xT Ax > 0.

Note that Theorem 8.3.2 shows that the positive definite matrices are exactly the symmetric matrices A for
which the quadratic form q = xT Ax takes only positive values.

Example 8.3.1
If U is any invertible n × n matrix, show that A = U TU is positive definite.
Solution. If x is in Rn and x = 0, then

                              xT Ax = xT (U TU )x = (U x)T (U x) = U x 2 > 0
because U x = 0 (U is invertible). Hence Theorem 8.3.2 applies.

    It is remarkable that the converse to Example 8.3.1 is also true. In fact every positive definite matrix
A can be factored as A = U TU where U is an upper triangular matrix with positive elements on the main
diagonal. However, before verifying this, we introduce another concept that is central to any discussion of
positive definite matrices.

    If A is any n × n matrix, let (r)A denote the r × r submatrix in the upper left corner of A; that is, (r)A is
the matrix obtained from A by deleting the last n - r rows and columns. The matrices (1)A, (2)A, (3)A, . . . ,
(n)A = A are called the principal submatrices of A.

Example 8.3.2        2                            10 5  and (3)A = A.
                     2  then (1)A = [10], (2)A =   53
         10 5        3
If A =  5 3

             22

Lemma 8.3.1
If A is positive definite, so is each principal submatrix (r)A for r = 1, 2, . . . , n.
                                                                                     8. . Positive Definite Matrices

Proof. Write A = (r)A P in block form. If y = 0 in Rr, write x = y in Rn.
                             QR                                                          0

   Then x = 0, so the fact that A is positive definite gives

                                 0 < xT Ax = yT 0              (r)A P           y = yT ((r)A)y
                                                                 QR             0

This shows that (r)A is positive definite by Theorem 8.3.2.5

    If A is positive definite, Lemma 8.3.1 and Theorem 8.3.1 show that det ((r)A) > 0 for every r. This
proves part of the following theorem which contains the converse to Example 8.3.1, and characterizes the
positive definite matrices among the symmetric ones.

   Theorem 8.3.3
   The following conditions are equivalent for a symmetric n × n matrix A:

      1. A is positive definite.

      2. det ((r)A) > 0 for each r = 1, 2, . . . , n.

      3. A = U TU where U is an upper triangular matrix with positive entries on the main diagonal.

   Furthermore, the factorization in (3) is unique (called the Cholesky factorization6of A).

Proof. First, (3)  (1) by Example 8.3.1, and (1)  (2) by Lemma 8.3.1 and Theorem 8.3.1.

take U = [ a]. If n > 1, write B =(n- (2)  (3). Assume (2) and proceed1) A. Then B is symmetric and satisfies (2) so, by induction, we by induction on n. If n = 1, then A = [a] where a > 0 by (2), so
have B = U TU as in (3) where U is of size (n - 1) × (n - 1). Then, as A is symmetric, it has block form

A = BpT pb where p is a column in Rn-1 and b is in R. If we write x = (U T )-1p and c = b - xT x,

block multiplication gives

                                                   UTU p               UT 0        Ux
                                            A= T               =T                  0c
                                                   pb                  x1

as the reader can verify. Taking determinants and applying Theorem 3.1.5 gives det A = det (U T ) det U ·
c = c( det U )2. Hence c > 0 because det A > 0 by (2), so the above factorization can be written

                                                           UT 0         U0 xc
                                                   A= T 

                                                           xc

Since U has positive diagonal entries, this proves (3).

   As to the uniqueness, suppose that A = U TU                 =  UT    U1  are two Cholesky factorizations.          Now write
      UU -1      (U T )-1U T .                                                     = UU -1,
                                                                    1                               lower
   =       1  =                 1    Then      is  upper  triangular,  because              1  and          triangular,   because
D     (U T )-1U T ,                         D                                   D

D  =                 1  and  so  it  is  a  diagonal  matrix.  Thus  U  =  DU1  and  U1  =  DU ,    so  it  suffices  to  show  that

    5A similar argument shows that, if B is any matrix obtained from a positive definite matrix A by deleting certain rows and
deleting the same columns, then B is also positive definite.

    6Andre-Louis Cholesky (1875-1918), was a French mathematician who died in World War I. His factorization was published
in 1924 by a fellow officer.
         Orthogonality

D = I. But eliminating U1 gives U = D2U , so D2 = I because U is invertible. Since the diagonal entries
of D are positive (this is true of U and U1), it follows that D = I.

    The remarkable thing is that the matrix U in the Cholesky factorization is easy to obtain from A using
row operations. The key is that Step 1 of the following algorithm is possible for any positive definite
matrix A. A proof of the algorithm is given following Example 8.3.3.

   Theorem: Algorithm for the Cholesky Factorization
   If A is a positive definite matrix, the Cholesky factorization A = U TU can be obtained as follows:

      Step 1. Carry A to an upper triangular matrix U1 with positive diagonal entries using row
               operations each of which adds a multiple of a row to a lower row.

      Step 2. Obtain U from U1 by dividing each row of U1 by the square root of the diagonal entry in
               that row.

Example 8.3.3

                                                10 5 2 
Find the Cholesky factorization of A =  5 3 2 .

                                                    223

Solution. The matrix A is positive definite by Theorem 8.3.3 because det (1)A = 10 > 0,
det (2)A = 5 > 0, and det (3)A = det A = 3 > 0. Hence Step 1 of the algorithm is carried out as

follows:   10 5 2   10 5 2   10 5 2 

          A=  5  3  2                   0  1  1  0 1                      1  = U1
                                           2
                                                                       2
              223                       0 1 13           003
                                              5                           5

                                      10    5                 

                                                         2
                                                 10 10 
                                                 1 

Now carry out Step 2 on U1 to obtain U =  0  2 .
                                                 2 
                                           0 0 35

The reader can verify that U TU = A.

Proof of the Cholesky Algorithm. If A is positive definite, let A = U TU be the Cholesky factorization,
and let D = diag (d1, . . . , dn) be the common diagonal of U and U T . Then U T D-1 is lower triangular
with ones on the diagonal (call such matrices LT-1). Hence L = (U T D-1)-1 is also LT-1, and so In  L
by a sequence of row operations each of which adds a multiple of a row to a lower row (verify; modify
columns right to left). But then A  LA by the same sequence of row operations (see the discussion
preceding Theorem 2.5.1). Since LA = [D(U T )-1][U TU ] = DU is upper triangular with positive entries
on the diagonal, this shows that Step 1 of the algorithm is possible.

    Turning to Step 2, let A  U1 as in Step 1 so that U1 = L1A where L1 is LT-1. Since A is symmetric,
                                                                                                 8. . QR-Factorization

we get

                                  L1U1T   =   L1(L1A)T     =  L1AT LT1   =   L1ALT1  =  U1  LT                                     (8.4)

                                                                                              1

Let  D1  =  diag (e1,  ...,  en)  denote  the  diagonal    of  U1.  Then     (8.4)  gives  L1(U1T D-1 1)  =  U1   LT     -1  .  This  is

                                                                                                                    1  D1
both  upper  triangular  (right   side)  and  LT-1  (left  side),   and  so  must   equal  In.   In  particular,  U T D-1          L-1.
                                                                                                                                =
                                                                                           -1                       11               1
Now let D2 = diag ( e1, . . . , en), so that D2 = D1. If we write U = D2 U1 we have2

              U TU = (U1T D-2 1)(D-2 1U1) = U1T (D22)-1U1 = (U1T D-1 1)U1 = (L-1 1)U1 = A

This  proves  Step  2  because    U  =   D-1U1  is  formed     by   dividing  each   row    of   U1  by  the  square   root     of  its

diagonal entry (verify).                   2

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

8. QR-Factorization7

One of the main virtues of orthogonal matrices is that they can be easily inverted--the transpose is the
inverse. This fact, combined with the factorization theorem in this section, provides a useful way to
simplify many matrix calculations (for example, in least squares approximation).

   Definition 8.6 QR-factorization
   Let A be an m × n matrix with independent columns. A QR-factorization of A expresses it as
   A = QR where Q is m × n with orthonormal columns and R is an invertible and upper triangular

    7This section is not used elsewhere in the book
6 Orthogonality

matrix with positive diagonal entries.

The importance of the factorization lies in the fact that there are computer algorithms that accomplish it
with good control over round-off error, making it particularly useful in matrix calculations. The factoriza-
tion is a matrix version of the Gram-Schmidt process.

    Suppose A = c1 c2 · · · cn is an m × n matrix with linearly independent columns c1, c2, . . . , cn.
The Gram-Schmidt algorithm can be applied to these columns to provide orthogonal columns f1, f2, . . . , fn

where f1 = c1 and                       ck ·f1    ck ·f2      ck·fk-1

                           fk = ck - f 2 f1 + f 2 f2 - · · · - f 2 fk-112
                                                              k-1

for each k = 2, 3, . . . , n. Now write qk = f1k fk for each k. Then q1, q2, . . . , qn are orthonormal columns,
and the above equation becomes

                      fk qk = ck - (ck · q1)q1 - (ck · q2)q2 - · · · - (ck · qk-1)qk-1

Using these equations, express each ck as a linear combination of the qi:

                      c1 = f1 q1

                      c2 = (c2 · q1)q1 + f2 q2

                      c3 = (c3 · q1)q1 + (c3 · q2)q2 + f3 q3....

                        .    .

                      cn = (cn · q1)q1 + (cn · q2)q2 + (cn · q3)q3 + · · · + fn qn

These equations have a matrix form that gives the required factorization:

                   A = c1 c2 c3 · · · cn

                                                                                           
                                                    f1    c2 · q1 c3 · q1 · · · cn · q1

                                                0         f2  c3 · q2      ···  cn · q2  
                                                                                         
                                                     0    0   f3           ···  cn · q3

                   =       q1 q2 q3 · · · qn         ..   ..  ..           ...  ..            (8.5)
                                                     .    .   .                 .        

                                                                                         
                                                                                         

                                                     0    0   0 · · · fn

Here the first factor Q = q1 q2 q3 · · · qn has orthonormal columns, and the second factor is an
n × n upper triangular matrix R with positive diagonal entries (and so is invertible). We record this in the
following theorem.

Theorem 8.4.1: QR-Factorization

Every m × n matrix A with linearly independent columns has a QR-factorization A = QR where Q
has orthonormal columns and R is upper triangular with positive diagonal entries.

The matrices Q and R in Theorem 8.4.1 are uniquely determined by A; we return to this below.
                                                                             8. . QR-Factorization

Example 8.4.1

                                         1 1 0
Find the QR-factorization of A =  0 1 1   -1 0 1 .

                                              001

Solution. Denote the columns of A as c1, c2, and c3, and observe that {c1, c2, c3} is independent.
If we apply the Gram-Schmidt algorithm to these columns, the result is:

             1                                  1                                     0
f1 = c1 =  0   -1  ,                                2
                                                1
                 0                   1              2                     1           0
                         f2 = c2 - 2 f1 =               ,  and  f3 = c3 + 2 f1 - f2 =        .
                                                 1                                      0  

                                                    0                                   1

Write q j = f1j 2 f j for each j, so {q1, q2, q3} is orthonormal. Then equation (8.5) preceding
Theorem 8.4.1 gives A = QR where

                             1 1 0                         

                             2 6                                3 1 0        

                         =  2 6  =  -1 1 0  1  - 3 1
                                                        
Q = q1 q2 q3              0 2 0  6 0 2  6  0                              0
                                                                              

                                                                              

                             0 01                               006

                                              2    1                     2 1       -1 
          f1  c2 · q1 c3 · q1                                                      3 
                                                           -1
R= 0                                                   2 2                           2
                                                       3   3         1    0     3
          0
              f2         c3 · q2           =    0                 =     
                                                                
                                                           2 2 2
                      0  f3                                               00
                                                  0 01

The reader can verify that indeed A = QR.

If a matrix A has independent rows and we apply QR-factorization to AT , the result is:

Corollary 8.4.1
If A has independent rows, then A factors uniquely as A = LP where P has orthonormal rows and L
is an invertible lower triangular matrix with positive main diagonal entries.

Since a square matrix with orthonormal columns is orthogonal, we have

   Theorem 8.4.2
   Every square, invertible matrix A has factorizations A = QR and A = LP where Q and P are
   orthogonal, R is upper triangular with positive diagonal entries, and L is lower triangular with
   positive diagonal entries.
8 Orthogonality

Remark
In Section 5.6 we found how to find a best approximation z to a solution of a (possibly inconsistent) system
Ax = b of linear equations: take z to be any solution of the "normal" equations (AT A)z = AT b. If A has
independent columns this z is unique (AT A is invertible by Theorem 5.4.3), so it is often desirable to com-
pute (AT A)-1. This is particularly useful in least squares approximation (Section 5.6). This is simplified
if we have a QR-factorization of A (and is one of the main reasons for the importance of Theorem 8.4.1).
For if A = QR is such a factorization, then QT Q = In because Q has orthonormal columns (verify), so we
obtain

                                                 AT A = RT QT QR = RT R
Hence computing (AT A)-1 amounts to finding R-1, and this is a routine matter because R is upper trian-
gular. Thus the difficulty in computing (AT A)-1 lies in obtaining the QR-factorization of A.

    We conclude by proving the uniqueness of the QR-factorization.

   Theorem 8.4.3
   Let A be an m × n matrix with independent columns. If A = QR and A = Q1R1 are
   QR-factorizations of A, then Q1 = Q and R1 = R.

Proof. Write Q = c1 c2 · · · cn and Q1 = d1 d2 · · · dn in terms of their columns, and ob-
serve  first  that                            because           and       have  orthonormal  columns.      Hence  it  suffices  to  show
                    QT  Q  =  In  =  QT   Q1               Q          Q1                                                           R1R-1;

that             (then         QT      1  QT          R).  Since   QT       =  In,  the   equation               gives  QT      =
          =                 =          =           =                                                    =
      Q1      Q         R1       1  A         A                      1  Q1                          QR     Q1R1           1  Q

for convenience we write this matrix as

                                                      QT        Q  =  R1R-1  =      ti j

                                                        1

This matrix is upper triangular with positive diagonal elements (since this is true for R and R1), so tii > 0
for each i and ti j = 0 if i > j. On the other hand, the (i, j)-entry of QT1 Q is dTi c j = di · c j, so we have
di · c j = ti j for all i and j. But each c j is in span {d1, d2, . . . , dn} because Q = Q1(R1R-1). Hence the
expansion theorem gives

                  c j = (d1 · c j)d1 + (d2 · c j)d2 + · · · + (dn · c j)dn = t1 jd1 + t2 jd2 + · · · + t j jdi
because di · c j = ti j = 0 if i > j. The first few equations here are

                                              c1 = t11d1

                                              c2 = t12d1 + t22d2

                                              c3 = t13d1 + t23d2 + t33d3

                                              ..c4 = t14d1 + t24d2 + t34d3 + t44d4..

                                                .            .

The first of these equations gives 1 = c1 = t11d1 = |t11| d1 = t11, whence c1 = d1. But then we
have t12 = d1 · c2 = c1 · c2 = 0, so the second equation becomes c2 = t22d2. Now a similar argument gives
c2 = d2, and then t13 = 0 and t23 = 0 follows in the same way. Hence c3 = t33d3 and c3 = d3. Continue in
this way to get ci = di for all i. This means that Q1 = Q, which is what we wanted.
                       8. . Computing Eigenvalues

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                          Vretta-Lyryx Engage is an active learning app designed to increase
                         student engagement in reading linear algebra material. The content is
                       "chunked" into small blocks, each with an interactive assessment activity

                                                  to promote comprehension.

                        Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

8. Computing Eigenvalues

In practice, the problem of finding eigenvalues of a matrix is virtually never solved by finding the roots
of the characteristic polynomial. This is difficult for large matrices and iterative methods are much better.
Two such methods are described briefly in this section.

The Power Method

In Chapter 3 our initial rationale for diagonalizing matrices was to be able to compute the powers of a
square matrix, and the eigenvalues were needed to do this. In this section, we are interested in efficiently
computing eigenvalues, and it may come as no surprise that the first method we discuss uses the powers
of a matrix.

    Recall that an eigenvalue  of an n × n matrix A is called a dominant eigenvalue if  has multiplicity
1, and

                                        | | > |µ| for all eigenvalues µ = 

Any corresponding eigenvector is called a dominant eigenvector of A. When such an eigenvalue exists,
one technique for finding it is as follows: Let x0 in Rn be a first approximation to a dominant eigenvector
 , and compute successive approximations x1, x2, . . . as follows:

In general, we define  x1 = Ax0 x2 = Ax1 x3 = Ax2 · · ·
                            xk+1 = Axk for each k  0
8 Orthogonality

If the first estimate x0 is good enough, these vectors xn will approximate the dominant eigenvector  (see
below). This technique is called the power method (because xk = Akx0 for each k  1). Observe that if z
is any eigenvector corresponding to  , then

                                          z 2 z·(Az) = z 2 z·( z) = 

Because the vectors x1, x2, . . . , xn, . . . approximate dominant eigenvectors, this suggests that we define

the Rayleigh quotients as follows:             xk ·xk+1

                                       rk = x 2              for k  1

                                                       k

Then the numbers rk approximate the dominant eigenvalue  .

Example 8.5.1                                                                            1 1 2 0 .
Use the power method to approximate a dominant eigenvector and eigenvalue of A =

Solution. The eigenvalues of A are 2 and -1, with eigenvectors 11 and 1 -2 . Take
x0 = 10 as the first approximation and compute x1, x2, . . . , successively, from
x1 = Ax0, x2 = Ax1, . . . . The result is

               x1 = 12 , x2 = 32 , x3 = 56 , x4 = 11 10 , x3 = 21 22 , . . .

These vectors are approaching scalar multiples of the dominant eigenvector               11 . Moreover, the

Rayleigh quotients are      r1 = 7 , r2 = 27 , r3 = 115 , r4 = 451 , . . .

                                    5        13              61        221

and these are approaching the dominant eigenvalue 2.

    To see why the power method works, let 1, 2, . . . , m be eigenvalues of A with 1 dominant and
let y1, y2, . . . , ym be corresponding eigenvectors. What is required is that the first approximation x0 be a
linear combination of these eigenvectors:

                        x0 = a1y1 + a2y2 + · · · + amym with a1 = 0

If k  1, the fact that xk = Akx0 and Akyi = ikyi for each i gives

                        xk = a11ky1 + a22ky2 + · · · + ammk ym for k  1

Hence                                                     k                       k

                        1   xk  =   a1y1  +  a2  2           y2 + · · · + am  m      ym
                                                                              
                         k
                        1                         1                            1

The right side approaches a1y1 as k increases because 1 is dominant i1 < 1 for each i > 1 . Because
a1 = 0, this means that xk approximates the dominant eigenvector a11ky1.
                                                       8. . Computing Eigenvalues 8

    The power method requires that the first approximation x0 be a linear combination of eigenvectors.
(In Example 8.5.1 the eigenvectors form a basis of R2.) But even in this case the method fails if a1 = 0,
where a1 is the coefficient of the dominant eigenvector (try x0 = -12 in Example 8.5.1). In general,
the rate of convergence is quite slow if any of the ratios i1 is near 1. Also, because the method requires
repeated multiplications by A, it is not recommended unless these multiplications are easy to carry out (for
example, if most of the entries of A are zero).

QR-Algorithm

A much better method for approximating the eigenvalues of an invertible matrix A depends on the factor-
ization (using the Gram-Schmidt algorithm) of A in the form

                                                           A = QR

where Q is orthogonal and R is invertible and upper triangular (see Theorem 8.4.2). The QR-algorithm
uses this repeatedly to create a sequence of matrices A1 = A, A2, A3, . . . , as follows:

   1. Define A1 = A and factor it as A1 = Q1R1.

   2. Define A2 = R1Q1 and factor it as A2 = Q2R2.

   3. Define A3 = R2Q2 and factor it as A3 = Q3R3.
                    ..
                    .

In general, Ak is factored as Ak = QkRk and we define Ak+1 = RkQk. Then Ak+1 is similar to Ak [in fact,
Ak+1 = RkQk = (Q-k 1Ak)Qk], and hence each Ak has the same eigenvalues as A. If the eigenvalues of A are
real and have distinct absolute values, the remarkable thing is that the sequence of matrices A1, A2, A3, . . .
converges to an upper triangular matrix with these eigenvalues on the main diagonal. [See below for the
case of complex eigenvalues.]

Example 8.5.2
If A = 1 1 2 0 as in Example 8.5.1, use the QR-algorithm to approximate the eigenvalues.

Solution. The matrices A1, A2, and A3 are as follows:

A1 = 1 1 2 0 = Q1R1 where Q1 = 51 1 2 2 -1 and R1 = 51 5 1 0 2

      17    9=  1.4 -1.8 = Q2R2
A2 = 5  4 -2    -0.8 -0.4

where Q2 = 165  74    and R2 = 165                     13 11
                4 -7                                    0 10

1 27 -5         2.08 -0.38
A3 = 13 8 -14 = 0.62 -1.08
8 Orthogonality

This is converging to  2             and so is approximating the eigenvalues 2 and -1 on the main
diagonal.              0 -1

    It is beyond the scope of this book to pursue a detailed discussion of these methods. The reader is
referred to J. M. Wilkinson, The Algebraic Eigenvalue Problem (Oxford, England: Oxford University
Press, 1965) or G. W. Stewart, Introduction to Matrix Computations (New York: Academic Press, 1973).
We conclude with some remarks on the QR-algorithm.

Shifting. Convergence is accelerated if, at stage k of the algorithm, a number sk is chosen and Ak - skI is
factored in the form QkRk rather than Ak itself. Then

                                   Q-k 1AkQk = Q-k 1(QkRk + skI)Qk = RkQk + skI

so we take Ak+1 = RkQk + skI. If the shifts sk are carefully chosen, convergence can be greatly improved.
Preliminary Preparation. A matrix such as

                                            

                                            
                                         0     
                                         0 0    

                                           000

is said to be in upper Hessenberg form, and the QR-factorizations of such matrices are greatly simplified.

Given an n × n matrix A, a series of orthogonal matrices H1, H2, . . . , Hm (called Householder matrices)

can be easily constructed such that

                                     B  =  HT   ·  ·  ·  HT   AH1  ·  ·  ·  Hm

                                             m             1

is in upper Hessenberg form. Then the QR-algorithm can be efficiently applied to B and, because B is
similar to A, it produces the eigenvalues of A.

Complex Eigenvalues. If some of the eigenvalues of a real matrix A are not real, the QR-algorithm con-
verges to a block upper triangular matrix where the diagonal blocks are either 1 × 1 (the real eigenvalues)
or 2 × 2 (each providing a pair of conjugate complex eigenvalues of A).
                                                                        8.6. The Singular Value Decomposition 8

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

8.6 The Singular Value Decomposition

When working with a square matrix A it is clearly useful to be able to "diagonalize" A, that is to find
a factorization A = Q-1DQ where Q is invertible and D is diagonal. Unfortunately such a factorization
may not exist for A. However, even if A is not square gaussian elimination provides a factorization of
the form A = PDQ where P and Q are invertible and D is diagonal--the Smith Normal form (Theorem
2.5.3). However, if A is real we can choose P and Q to be orthogonal real matrices and D to be real. Such
a factorization is called a singular value decomposition (SVD) for A, one of the most useful tools in
applied linear algebra. In this Section we show how to explicitly compute an SVD for any real matrix A,
and illustrate some of its many applications.

    We need a fact about two subspaces associated with an m × n matrix A:
                   im A = {Ax | x in Rn} and col A = span {a | a is a column of A}

Then im A is called the image of A (so named because of the linear transformation Rn  Rm with x  Ax);
and col A is called the column space of A (Definition 5.10). Surprisingly, these spaces are equal:

   Lemma 8.6.1
   For any m × n matrix A, im A = col A.

Proof. Let A = a1 a2 · · · an in terms of its columns. Let x  im A, say x = Ay, y in Rn. If
 8 Orthogonality

y = y1 y2 · · · yn T , then Ay = y1a1 + y2a2 + · · · + ynan  col A by Definition 2.5. This shows that
im A  col A. For the other inclusion, each ak = Aek where ek is column k of In.

8.6. Singular Value Decompositions

We know a lot about any real symmetric matrix: Its eigenvalues are real (Theorem 5.5.7), and it is orthog-
onally diagonalizable by the Principal Axes Theorem (Theorem 8.2.2). So for any real matrix A (square
or not), the fact that both AT A and AAT are real and symmetric suggests that we can learn a lot about A by
studying them. This section shows just how true this is.

    The following Lemma reveals some similarities between AT A and AAT which simplify the statement
and the proof of the SVD we are constructing.

   Lemma 8.6.2
   Let A be a real m × n matrix. Then:

       1. The eigenvalues of AT A and AAT are real and non-negative.

       2. AT A and AAT have the same set of positive eigenvalues.

Proof.

   1. Since both matrices AT A and AAT are real and symmetric, then their eigenvalues are also real by
       Theorem 5.5.7. Not let  be an eigenvalue of AT A, with eigenvector 0 = q  Rn. Then:
                            Aq 2 = (Aq)T (Aq) = qT (AT Aq) = qT ( q) =  (qT q) =  q 2

       Then (1.) follows for AT A, and the case AAT follows by replacing A by AT .

   2. Write N(B) for the set of positive eigenvalues of a matrix B. We must show that N(AT A) = N(AAT ).
       If   N(AT A) with eigenvector 0 = q  Rn, then Aq  Rm and
                                         AAT (Aq) = A[(AT A)q] = A( q) =  (Aq)

       Moreover, Aq = 0 since AT Aq =  q = 0 as both  = 0 and q = 0. Hence  is also a positive
       eigenvalue of AAT , proving N(AT A)  N(AAT ). For the other inclusion replace A by AT .

    To analyze an m × n matrix A we have two symmetric matrices to work with: AT A and AAT . In view
of Lemma 8.6.2, we choose AT A (sometimes called the Gram matrix of A), and derive a series of facts
which we will need. This narrative is a bit long, but trust that it will be worth the effort. We parse it out in
several steps:

1. The n × n matrix AT A is real and symmetric so, by the Principal Axes Theorem 8.2.2, let

{q1, q2, . . . , qn}  Rn be an orthonormal basis of eigenvectors of AT A, with corresponding eigenval-
ues 1, 2, . . . , n. By Lemma 8.6.2(1), i is real for each i and i  0. By re-ordering the qi we may
(and do) assume that
                      1  2  · · ·  r > 0 and 8 i = 0 if i > r
                                                                                                          (i)

8Of course they could all be positive (r = n) or all zero (so AT A = 0, and hence A = 0 by Exercise ??).
                                                              8.6. The Singular Value Decomposition 8

By Theorems 8.2.1 and 3.4.1, the matrix

Q = q1 q2 · · · qn is orthogonal and orthogonally diagonalizes AT A.                                             (ii)

2. Even though the i are the eigenvalues of AT A, the number r in (i) turns out to be rank A. To understand
   why, consider the vectors Aqi  im A. For all i, j:

Aqi  ·  Aq j  =  (Aqi)T  Aq j     =    T  (AT  A)q j  =    T  (  jq  j)  =    j (qTi  q  j)  =    j (qi  ·  qj)

                                     qi                  qi

Because {q1, q2, . . . , qn} is an orthonormal set, this gives

     Aqi · Aq j = 0 if i = j and                      Aqi 2 = i qi 2 = i for each i                              (iii)

We can extract two conclusions from (iii) and (i):

{Aq1, Aq2, . . . , Aqr}  im A is an orthogonal set and Aqi = 0 if i > r                                          (iv)

With this write U = span {Aq1, Aq2, . . . , Aqr}  im A; we claim that U = im A, that is im A  U .
For this we must show that Ax  U for each x  Rn. Since {q1, . . . , qr, . . . , qn} is a basis of Rn (it
is orthonormal), we can write x = t1q1 + · · · + trqr + · · · + tnqn where each t j  R. Then, using (iv) we
obtain

                      Ax = t1Aq1 + · · · + trAqr + · · · + tnAqn = t1Aq1 + · · · + trAqr  U

This shows that U = im A, and so

              {Aq1, Aq2, . . . , Aqr} is an orthogonal basis of im (A)                                           (v)

But col A = im A by Lemma 8.6.1, and rank A = dim ( col A) by Theorem 5.4.1, so

                                                                              (v)
                 rank A = dim ( col A) = dim ( im A) = r                                                         (vi)

3. Before proceeding, some definitions are in order:

Definition 8.7
                             (iii)

The real numbers i = i = Aqi for i = 1, 2, . . . , n, are called the singular values of the
matrix A.

Clearly 1, 2, . . . , r are the positive singular values of A. By (i) we have

                 1  2  · · ·  r > 0 and i = 0 if i > r                                                           (vii)

With (vi) this makes the following definitions depend only upon A.
86 Orthogonality

Definition 8.8
Let A be a real, m × n matrix of rank r, with positive singular values 1  2  · · ·  r > 0 and
i = 0 if i > r. Define:

                   DA = diag (1, . . . , r) and A = DA 0
                                                                                  0 0 m×n

Here A is in block form and is called the singular matrix of A.

The singular values i and the matrices DA and A will be referred to frequently below.

4. Returning to our narrative, normalize the vectors Aq1, Aq2, . . . , Aqr, by defining

                  pi =   1  i  Aqi  Rm              for each i = 1, 2, . . . , r                (viii)
                        Aq

By (v) and Lemma 8.6.1, we conclude that

                  {p1, p2, . . . , pr} is an orthonormal basis of col A  Rm                     (ix)

Employing the Gram-Schmidt algorithm (or otherwise), construct pr+1, . . . , pm so that

                  {p1, . . . , pr, . . . , pm} is an orthonormal basis of Rm                    (x)

5. By (x) and (ii) we have two orthogonal matrices

P = p1 · · · pr · · · pm of size m × m and Q = q1 · · · qr · · · qn of size n × n

These matrices are related. In fact we have:

                        (iii)                 (viii)
                  ipi = ipi = Aqi pi = Aqi for each i = 1, 2, . . . , r                         (xi)

This yields the following expression for AQ in terms of its columns:

                                                      (iv)                                      (xii)
AQ = Aq1 · · · Aqr Aqr+1 · · · Aqn = 1p1 · · · rpr 0 · · · 0

Then we compute:

                                                       1 · · · 0           ..     0 ··· 0 
                                                            ..   ...       .      ..     .. 
                                                            .                       .    .
                                                      
                                                0 · · · r                         0 · · · 0 
PA = p1 · · · pr pr+1 · · · pm 
                                                       0 ··· 0                    0 · · · 0 
                                                            ..        ..            ..    ..
                                                              .         .         .      .    
                                                                                              

                                                            0 ··· 0               0 ··· 0

                  = 1p1 · · · rpr 0 · · · 0

                  (xii)

                  = AQ

Finally, as Q-1 = QT it follows that A = PAQT .
With this we can state the main theorem of this Section.
                                                     8.6. The Singular Value Decomposition 8

Theorem 8.6.1
Let A be a real m × n matrix, and let 1  2  · · ·  r > 0 be the positive singular values of A.
Then r is the rank of A and we have the factorization

                         A = PAQT where P and Q are orthogonal matrices

    The factorization A = PAQT in Theorem 8.6.1, where P and Q are orthogonal matrices, is called a
Singular Value Decomposition (SVD) of A. This decomposition is not unique. For example if r < m then

the vectors pr+1, . . . , pm can be any extension of {p1, . . ., pr} to an orthonormal basis of Rm, and each
will lead to a different matrix P in the decomposition. For a more dramatic example, if A = In then A = In,
and A = PAPT is a SVD of A for any orthogonal n × n matrix P.

Example 8.6.1                                1 0 1 -1 1 0 .
Find a singular value decomposition for A =

                                 2 -1 1 
Solution. We have AT A =  -1 1 0 , so the characteristic polynomial is

                                      1 01

                  x-2                          1   -1 
cAT A(x) = det  1                            x-1    0  = (x - 3)(x - 1)x
                                                  x-1
                      -1                       0

Hence the eigenvalues of AT A (in descending order) are 1 = 3, 2 = 1 and 3 = 0 with,
respectively, unit eigenvectors

           2                 0                                             -1 
q1 = 16  -1  ,     q2 = 12  1  ,                        and q3 = 13  -1 

                1               1                                              1

It follows that the orthogonal matrix Q in Theorem 8.6.1 is

                                                             
                                                     2 0 -2

       Q = q1 q2 q3 = 61  -1 3 -2 
                                                     13 2

The singular values here are 1 = 3, 2 = 1 and 3 = 0, so rank (A) = 2--clear in this
case--and the singular matrix is

                   1 0 0                             3 0 0
           A = 0 2 0 = 0 1 0

So it remains to find the 2 × 2 orthogonal matrix P in Theorem 8.6.1. This involves the vectors

             1,                              2 1             and Aq3 = 00
        6                                            ,
Aq1 =  2   -1      Aq2 = 2                        1
88 Orthogonality

Normalize Aq1 and Aq2 to get

                          p1 = 2 - 1 11 and p2 = 1 1 2 1

In this case, {p1, p2} is already a basis of R2 (so the Gram-Schmidt algorithm is not needed), and
we have the 2 × 2 orthogonal matrix

                                       P = p1 p2 = 1 1 1 2 -1 1
Finally (by Theorem 8.6.1) the singular value decomposition for A is

    A = PAQT = 12               11       3 0 0     2                    -1  1 
                              -1 1        0 10  61  0                   3   3 
                                                                      -2
                                                      -2                      2

Of course this can be confirmed by direct matrix multiplication.

    Thus, computing an SVD for a real matrix A is a routine matter, and we now describe a systematic
procedure for doing so.

Theorem: SVD Algorithm
Given a real m × n matrix A, find an SVD A = PAQT as follows:

1. Use the Diagonalization Algorithm (see page 156) to find the (real and non-negative)
   eigenvalues 1, 2, . . . , n of AT A with corresponding (orthonormal) eigenvectors
   q1, q2, . . . , qn. Reorder the qi (if necessary) to ensure that the nonzero eigenvalues are
   1  2  · · ·  r > 0 and i = 0 if i > r.

2. The integer r is the rank of the matrix A.

3. The n × n orthogonal matrix Q in the SVD is Q = q1 q2 · · · qn .

4.  Define pi =    1  Aqi for i = 1, 2, . . . , r (where r is as in step 1). Then {p1, p2, . . . , pr} is
                  Aq  im
    orthonormal in R so (using Gram-Schmidt or otherwise) extend it to an orthonormal basis

    {p1, . . . , pr, . . . , pm} in Rm.

5. The m × m orthogonal matrix P in the SVD is P = p1 · · · pr · · · pm .

6. The singular values for A are 1, 2, . . . , n where i = i for each i. Hence the nonzero
    singular values are 1  2  · · ·  r > 0, and so the singular matrix of A in the SVD is
    A = diag (1, . . . , r) 0 .
                      0       0 m×n

7. Thus A = PQT is a SVD for A.

In practice the singular values i, the matrices P and Q, and even the rank of an m × n matrix are not
                                                                  8.6. The Singular Value Decomposition 8

calculated this way. There are sophisticated numerical algorithms for calculating them maybe not exactly
but to a high degree of accuracy. The reader is referred to books on numerical linear algebra.

    So the main virtue of Theorem 8.6.1 is that it provides a way of constructing an SVD for every real
matrix A. In particular it shows that every real matrix A has a singular value decomposition9 in the
following, more general, sense:

Definition 8.9

A Singular Value Decomposition (SVD) of an m × n matrix A is a factorization A = PQT where
P and Q are orthogonal and  = D 0
                                            0 0 m×n   in block form where D = diag (d1, d2, . . . , dr)

where each di > 0, and r  m and r  n.

Note that for any SVD A = PQT we immediately obtain some information about A:

   Lemma 8.6.3
   If A = PQT is any SVD for A as in Definition 8.9, then:

       1. r = rank A.
       2. The numbers d1, d2, . . . , dr are the singular values of A in some order.

Proof. Use the notation of Definition 8.9. We have
                                       AT A = (QT PT )(PQT ) = Q(T )QT

so T  and AT A are similar n ×n matrices (Definition 5.12). Hence r = rank A by Corollary 5.4.3, proving
(1.). Furthermore, T  and AT A have the same eigenvalues by Theorem 5.5.1; that is (using (1.)):

                             {d12, d22, . . . , dr2} = {1, 2, . . . , r} are equal as sets

where 1, 2, . . . , r are the positive eigenvalues of AT A. Hence there is a permutation  of {1, 2, · · · , r}
such  that  d2   =  i  for each i = 1,  2,  ...,  r.  Hence di =  i = i for each i by Definition 8.7. This

              i
proves (2.).

    We note in passing that more is true. Let A be m × n of rank r, and let A = PQT be any SVD for A.
Using the proof of Lemma 8.6.3 we have di = i for some permutation  of {1, 2, . . . , r}. In fact, it can
be shown that there exist orthogonal matrices P1 and Q1 obtained from P and Q by -permuting columns
and rows respectively, such that A = P1AQT1 is an SVD of A.

    9In fact every complex matrix has an SVD [J.T. Scheick, Linear Algebra with Applications, McGraw-Hill, 1997]
         Orthogonality

8.6. Fundamental Subspaces

It turns out that any singular value decomposition contains a great deal of information about an m ×
n matrix A and the subspaces associated with A. For example, in addition to Lemma 8.6.3, the set
{p1, p2, . . . , pr} of vectors constructed in the proof of Theorem 8.6.1 is an orthonormal basis of col A
(by (v) and (viii) in the proof). There are more such examples, which is the thrust of this subsection.
In particular, there are four subspaces associated to a real m × n matrix A that have come to be called
fundamental:

   Definition 8.10
   The fundamental subspaces of an m × n matrix A are:

           row A = span {x | x is a row of A}

           col A = span {x | x is a column of A}

           null A = {x  Rn | Ax = 0}

           null AT = {x  Rn | AT x = 0}

If A = PQT is any SVD for the real m ×n matrix A, then orthonormal bases for each of these fundamental
subspaces can be obtained from the columns of P and Q. We are going to show how exactly, but first we
need three properties related to the orthogonal complement U  of a subspace U of Rn, where (Definition
8.1):

                                       U  = {x  Rn | u · x = 0 for all u  U }
The orthogonal complement plays an important role in the Projection Theorem (Theorem 8.1.3), and we
return to it in Section 10.2. For now we need:

   Lemma 8.6.4
   If A is any matrix then:

       1. ( row A) = null A and ( col A) = null AT .

       2. If U is any subspace of Rn then U  = U .

       3. Let {f1, . . . , fm} be an orthonormal basis of Rm. If U = span {f1, . . . , fk}, then

                                                  U  = span {fk+1, . . . , fm}

Proof.
   1. Assume A is m × n, and let b1, . . . , bm be the rows of A. If x is a column in Rn, then entry i of Ax is
       bi · x, so Ax = 0 if and only if bi · x = 0 for each i. Thus:
               x  null A  bi · x = 0 for each i  x  ( span {b1, . . . , bm}) = ( row A)

       Hence null A = ( row A). Now replace A by AT to get null AT = ( row AT ) = ( col A), which is
       the other identity in (1).
                                                                     8.6. The Singular Value Decomposition

2. If x  U then y · x = 0 for all y  U , that is x  U . This proves that U  U , so it is enough to
   show that dim U = dim U . By Theorem 8.1.4 we see that dim V  = n - dim V for any subspace
   V  Rn. Hence
                       dim U  = n - dim U  = n - (n - dim U ) = dim U , as required

3. We have span {fk+1, . . . , fm}  U  because {f1, . . . , fm} is orthogonal. For the other inclusion, let
   x  U  so fi · x = 0 for i = 1, 2, . . . , k. By the Expansion Theorem 5.3.6:

              x = (f1 · x)f1 + · · · + (fk · x)fk + (fk+1 · x)fk+1 + · · · + (fm · x)fm
                  = 0 + · · · + 0 + (fk+1 · x)fk+1 + · · · + (fm · x)fm

   Hence U   span {fk+1, . . . , fm}.

    With this we can see how any SVD for a matrix A provides orthonormal bases for each of the four
fundamental subspaces of A.

Theorem 8.6.2

Let A be an m × n real matrix, let A = PQT be any SVD for A where P and Q are orthogonal of
size m × m and n × n respectively, and let

        = D 0            where    D = diag (d1, d2, . . . , dr), with each di > 0
                0 0 m×n

Write P = p1 · · · pr · · · pm and Q = q1 · · · qr · · · qn , so {p1, . . . , pr, . . . , pm}
and {q1, . . . , qr, . . . , qn} are orthonormal bases of Rm and Rn respectively. Then

   1. r = rank A, and the singular values of A are d1, d2, . . . , dr.

2. The fundamental spaces are described as follows:

        a. {p1, . . . , pr} is an orthonormal basis of col A.
        b. {pr+1, . . . , pm} is an orthonormal basis of null AT .
        c. {qr+1, . . . , qn} is an orthonormal basis of null A.
        d. {q1, . . . , qr} is an orthonormal basis of row A.

Proof.

1. This is Lemma 8.6.3.
2. a. As col A = col (AQ) by Lemma 5.4.3 and AQ = P, (a.) follows from

        P = p1 · · · pr · · · pm  diag (d1, d2, . . . , dr) 0 = d1p1 · · · drpr 0 · · · 000
Orthogonality

                      (a.)                               
b. We have ( col A) = ( span {p1, . . . , pr}) = span {pr+1, . . . , pm} by Lemma 8.6.4(3). This
proves (b.) because ( col A) = null AT by Lemma 8.6.4(1).

c. We have dim ( null A) + dim ( im A) = n by the Dimension Theorem 7.2.4, applied to
   T : Rn  Rm where T (x) = Ax. Since also im A = col A by Lemma 8.6.1, we obtain

               dim ( null A) = n - dim ( col A) = n - r = dim ( span {qr+1, . . . , qn})

So to prove (c.) it is enough to show that q j  null A whenever j > r. To this end write

                  dr+1 = · · · = dn = 0,         so      T   =   diag (d12,    ...,     dr2,    2    ...,  dn2)

                                                                                              dr+1,

Observe that each d j is an eigenvalue of T  with eigenvector e j = column j of In. Thus
q j = Qe j for each j. As AT A = QT QT (proof of Lemma 8.6.3), we obtain

               (AT A)v j = (QT QT )(Qe j) = Q(T e j) = Q d2j e j = d2j Qe j = d2j q j

for 1  j  n. Thus each q j is an eigenvector of AT A corresponding to d2j . But then

Aq j           2  =  (Aq j)T  Aq j  =  qT   (AT  Aq  j)  =  qT   (d2j q j)  =  d2   qj  2 = d2       for i = 1, . . . , n

                                         j                    j                  j               j

In particular, Aq j = 0 whenever j > r, so q j  null A if j > r, as desired. This proves (c).

                                            (c.)                            
d. Observe that span {qr+1, . . . , qn} = null A = ( row A) by Lemma 8.6.4(1). But then parts
(2) and (3) of Lemma 8.6.4 show

               row A = ( row A)  = ( span {qr+1, . . . , qn}) = span {q1, . . ., qr}

This proves (d.), and hence Theorem 8.6.2.

Example 8.6.2
Consider the homogeneous linear system

                                     Ax = 0 of m equations in n variables

Then the set of all solutions is null A. Hence if A = PQT is any SVD for A then (in the notation
of Theorem 8.6.2) {qr+1, . . . , qn} is an orthonormal basis of the set of solutions for the system.
As such they are a set of basic solutions for the system, the most basic notion in Chapter 1.
8.6. The Singular Value Decomposition

8.6. The Polar Decomposition of a Real Square Matrix

If A is real and n × n the factorization in the title is related to the polar decomposition A. Unlike the SVD,
in this case the decomposition is uniquely determined by A.

    Recall (Section 8.3) that a symmetric matrix A is called positive definite if and only if xT Ax > 0 for
every column x = 0  Rn. Before proceeding, we must explore the following weaker notion:

Definition 8.11
A real n × n matrix G is called positive10if it is symmetric and

                                           xT Gx  0 for all x  Rn

Clearly every positive definite matrix is positive, but the converse fails. Indeed, A =  1 1 1 1 is positive
                                                                                         -1 T , so A is not
because, if x = a b T in R2, then xT Ax = (a + b)2  0. But yT Ay = 0 if y = 1
positive definite.

Lemma 8.6.5
Let G denote an n × n positive matrix.

   1. If A is any ×m matrix and G is positive, then AT GA is positive (and m × m).
   2. If G = diag (d1, d2, · · · , dn) and each di  0 then G is positive.

Proof.
   1. xT (AT GA)x = (Ax)T G(Ax)  0 because G is positive.
   2. If x = x1 x2 · · · xn T , then
                                             xT Gx = d1x21 + d2x22 + · · · + dnx2n  0
       because di  0 for each i.

 Definition 8.12
 If A is a real n × n matrix, a factorization

                               A = GQ where G is positive and Q is orthogonal
 is called a polar decomposition for A.

 Any SVD for a real square matrix A yields a polar form for A.

10Also called positive semi-definite.
Orthogonality

Theorem 8.6.3
Every square real matrix has a polar form.

Proof. Let A = U V T be a SVD for A with  as in Definition 8.9 and m = n. Since U TU = In here we
have

                                  A = U V T = (U )(U TU )V T = (U U T )(UV T )

So if we write G = U U T and Q = UV T , then Q is orthogonal, and it remains to show that G is positive.
But this follows from Lemma 8.6.5.

    The SVD for a square matrix A is not unique (In = PInPT for any orthogonal matrix P). But given the
proof of Theorem 8.6.3 it is surprising that the polar decomposition is unique.11 We omit the proof.

    The name "polar form" is reminiscent of the same form for complex numbers (see Appendix A). This
is no coincidence. To see why, we represent the complex numbers as real 2 × 2 matrices. Write M2(R) for
the set of all real 2 × 2 matrices, and define

                        : C  M2(R) by  (a + bi) = a -b for all a + bi in C
                                                                      ba

One verifies that  preserves addition and multiplication in the sense that
                             (zw) =  (z) (w) and  (z + w) =  (z) +  (w)

for all complex numbers z and w. Since  is one-to-one we may identify each complex number a + bi with
the matrix  (a + bi), that is we write

               a + bi = a -b                for all a + bi in C
                            ba

Thus 0 = 0 0 0 0 , 1 = 1 0 0 1 = I2, i = 0 -1 1 0 , and r = r 0 0 r if r is real.
    If z = a + bi is nonzero then the absolute value r = |z| = a2 + b2 = 0. If  is the angle of z in standard

position, then cos  = a/r and sin  = b/r. Observe:

a -b b a = r 0 0 r a/r -b/r b/r a/r = r 0 0 r cos  - sin  sin  cos  = GQ    (xiii)

where G = r 0 0 r is positive and Q = cos  - sin  sin  cos  is orthogonal. But in C we have G = r and
Q = cos  + i sin  so (xiii) reads z = r(cos  + i sin  ) = rei which is the classical polar form for the
complex number a + bi. This is why (xiii) is called the polar form of the matrix a -b ; Definition

                                                                                                      ba
8.12 simply adopts the terminology for n × n matrices.

   11See J.T. Scheick, Linear Algebra with Applications, McGraw-Hill, 1997, page 379.
                                                                        8.6. The Singular Value Decomposition

8.6. The Pseudoinverse of a Matrix

It is impossible for a non-square matrix A to have an inverse (see the footnote to Definition 2.11). Nonethe-
less, one candidate for an "inverse" of A is an m × n matrix B such that

                                            ABA = A and BAB = B
Such a matrix B is called a middle inverse for A. If A is invertible then A-1 is the unique middle inverse for

                                                                                                                   1 0
A, but a middle inverse is not unique in general, even for square matrices. For example, if A =  0 0 

                                                                                                                      00
then B = 1 0 0 b 0 0 is a middle inverse for A for any b.

    If ABA = A and BAB = B it is easy to see that AB and BA are both idempotent matrices. In 1955 Roger
Penrose observed that the middle inverse is unique if both AB and BA are symmetric. We omit the proof.

   Theorem 8.6.4: Penrose' Theorem12
   Given any real m × n matrix A, there is exactly one n × m matrix B such that A and B satisfy the
   following conditions:

      P1 ABA = A and BAB = B.
      P2 Both AB and BA are symmetric.

   Definition 8.13
   Let A be a real m × n matrix. The pseudoinverse of A is the unique n × m matrix A+ such that A
   and A+ satisfy P1 and P2, that is:

                AA+A = A, A+AA+ = A+, and both AA+ and A+A are symmetric13

    If A is invertible then A+ = A-1 as expected. In general, the symmetry in conditions P1 and P2 shows
that A is the pseudoinverse of A+, that is A++ = A.

   12R. Penrose, A generalized inverse for matrices, Proceedings of the Cambridge Philosophical Society 5l (1955), 406-413.
In fact Penrose proved this for any complex matrix, where AB and BA are both required to be hermitian (see Definition 8.18 in
the following section).

   13Penrose called the matrix A+ the generalized inverse of A, but the term pseudoinverse is now commonly used. The matrix
A+ is also called the Moore-Penrose inverse after E.H. Moore who had the idea in 1935 as part of a larger work on "General
Analysis". Penrose independently re-discovered it 20 years later.
6 Orthogonality

Theorem 8.6.5
Let A be an m × n matrix.

   1. If rank A = m then AAT is invertible and A+ = AT (AAT )-1.
   2. If rank A = n then AT A is invertible and A+ = (AT A)-1AT .

Proof. Here AAT (respectively AT A) is invertible by Theorem 5.4.4 (respectively Theorem 5.4.3). The rest
is a routine verification.

In general, given an m × n matrix A, the pseudoinverse A+ can be computed from any SVD for A. To

see how, we need some notation. Let A = PQT be an SVD for A (as in Definition 8.9) where P and Q
are orthogonal and  = D 0
                        0 0 m×n     in block form where D = diag (d1, d2, . . . , dr) where each di > 0.

Hence D is invertible, so we make:

Definition 8.14

 =  D-1 0            .
     00
                 n×m

A routine calculation gives:        ·  =  Ir 0
   Lemma 8.6.6                      ·  =  0 0 m×m
        ·  = 
                                          Ir 0
        ·  =                              0 0 n×n

That is,  is the pseudoinverse of .
    Now given A = PQT , define B = QPT . Then

                          ABA = (PQT )(QPT )(PQT ) = P()QT = PV T = A

by Lemma 8.6.6. Similarly BAB = B. Moreover AB = P()PT and BA = Q()QT are both symmetric
again by Lemma 8.6.6. This proves

   Theorem 8.6.6
   Let A be real and m × n, and let A = PQT is any SVD for A as in Definition 8.9. Then
   A+ = QPT .
                                                             8.6. The Singular Value Decomposition

Of course we can always use the SVD constructed in Theorem 8.6.1 to find the pseudoinverse. If

     1 0                                    100              is a middle inverse for A for any b. Furthermore
A =  0 0 , we observed above that B =       b00

        00

AB is symmetric, and BA is symmetric exactly when b = 0. In this case, B is the pseudoinverse of A found

in Example 8.6.3.

Example 8.6.3

                  1 0
Find A+ if A =  0 0 .

                      00

Solution. AT A = 1 0 0 0 with eigenvalues 1 = 1 and 2 = 0 and corresponding eigenvectors

q1 = 10 and q2 = 01 . Hence Q = q1 q2 = I2. Also A has rank 1 with singular values

                                   1     0                   100
1 = 1 and 2 = 0, so A =  0                                   000
                                         0  =A  and       =        = AT in this case.
                                      0
                                         0             A

1                         0                                  1

Since Aq1 =  0  and Aq2 =  0 , we have p1 =  0  which extends to an orthonormal
                   0                     0                   0

                                            0                   0
basis {p1, p2, p3} of R3 where (say) p2 =  1  and p3 =  0 . Hence
                                                0               1

P = p1 p2 p3 = I, so the SVD for A is A = PAQT . Finally, the pseudoinverse of A is

A+ = QAPT = A = 1 0 0 0 0 0 . Note that A+ = AT in this case.

    The following Lemma collects some properties of the pseudoinverse that mimic those of the inverse.
Its verification is left as an exercise.

   Lemma 8.6.7
   Let A be an m × n matrix.

       1. A++ = A.
       2. If A is invertible then A+ = A-1.
       3. (AT )+ = (A+)T .
       4. (kA)+ = k-1A+ for any real k = 0.
       5. (PAQ)+ = PT (A+)QT whenever P and Q are orthogonal.
8 Orthogonality

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                          Engage Active Learning App!

                    Vretta-Lyryx Engage is an active learning app designed to increase
                   student engagement in reading linear algebra material. The content is
                 "chunked" into small blocks, each with an interactive assessment activity

                                            to promote comprehension.

                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

8. Complex Matrices

If A is an n × n matrix, the characteristic polynomial cA(x) is a polynomial of degree n and the eigenvalues
of A are just the roots of cA(x). In most of our examples these roots have been real numbers (in fact,

the examples have been carefully chosen so this will be the case!); but it need not happen, even when

the characteristic polynomial has real coefficients. For example, if A =    01        then cA(x) = x2 + 1
                                                                          -1 0

has roots i and -i, where i is a complex number satisfying i2 = -1. Therefore, we have to deal with the

possibility that the eigenvalues of a (real) square matrix might be complex numbers.

    In fact, nearly everything in this book would remain true if the phrase real number were replaced by
complex number wherever it occurs. Then we would deal with matrices with complex entries, systems
of linear equations with complex coefficients (and complex solutions), determinants of complex matrices,
and vector spaces with scalar multiplication by any complex number allowed. Moreover, the proofs of
most theorems about (the real version of) these concepts extend easily to the complex case. It is not our
intention here to give a full treatment of complex linear algebra. However, we will carry the theory far
enough to give another proof that the eigenvalues of a real symmetric matrix A are real (Theorem 5.5.7)
and to prove the spectral theorem, an extension of the principal axes theorem (Theorem 8.2.2).

    The set of complex numbers is denoted C . We will use only the most basic properties of these numbers
(mainly conjugation and absolute values), and the reader can find this material in Appendix A.

    If n  1, we denote the set of all n-tuples of complex numbers by Cn. As with Rn, these n-tuples will
be written either as row or column matrices and will be referred to as vectors. We define vector operations
                                                                                           8. . Complex Matrices

on Cn as follows:

                    (v1, v2, . . . , vn) + (w1, w2, . . . , wn) = (v1 + w1, v2 + w2, . . . , vn + wn)
                                           u(v1, v2, . . . , vn) = (uv1, uv2, . . . , uvn) for u in C

With these definitions, Cn satisfies the axioms for a vector space (with complex scalars) given in Chapter 6.
Thus we can speak of spanning sets for Cn, of linearly independent subsets, and of bases. In all cases,
the definitions are identical to the real case, except that the scalars are allowed to be complex numbers. In
particular, the standard basis of Rn remains a basis of Cn, called the standard basis of Cn.

    A matrix A = ai j is called a complex matrix if every entry ai j is a complex number. The notion of
conjugation for complex numbers extends to matrices as follows: Define the conjugate of A = ai j to be
the matrix

                                                        A = ai j
obtained from A by conjugating every entry. Then (using Appendix A)

                                           A + B = A + B and AB = A B

holds for all (complex) matrices of appropriate size.

The Standard Inner Product

There is a natural generalization to Cn of the dot product in Rn.

   Definition 8.15 Standard Inner Product in Rn
   Given z = (z1, z2, . . . , zn) and w = (w1, w2, . . . , wn) in Cn, define their standard inner product
     z, w by

                                       z, w = z1w1 + z2w2 + · · · + znwn = z · w
   where w is the conjugate of the complex number w.

Clearly, if z and w actually lie in Rn, then z, w = z · w is the usual dot product.

   Example 8.7.1
   If z = (2, 1 - i, 2i, 3 - i) and w = (1 - i, -1, -i, 3 + 2i), then

                        z, w = 2(1 + i) + (1 - i)(-1) + (2i)(i) + (3 - i)(3 - 2i) = 6 - 6i
                         z, z = 2 · 2 + (1 - i)(1 + i) + (2i)(-2i) + (3 - i)(3 + i) = 20

    Note that z, w is a complex number in general. However, if w = z = (z1, z2, . . . , zn), the definition
gives z, z = |z1|2 + · · · + |zn|2 which is a nonnegative real number, equal to 0 if and only if z = 0. This
explains the conjugation in the definition of z, w , and it gives (4) of the following theorem.
         Orthogonality

   Theorem 8.7.1
   Let z, z1, w, and w1 denote vectors in Cn, and let  denote a complex number.

       1. z + z1, w = z, w + z1, w and z, w + w1 = z, w + z, w1 .
       2.  z, w =  z, w and z,  w =  z, w .
       3. z, w = w, z .
       4. z, z  0, and z, z = 0 if and only if z = 0.

Proof. We leave (1) and (2) to the reader (Exercise ??), and (4) has already been proved. To prove (3),
write z = (z1, z2, . . ., zn) and w = (w1, w2, . . . , wn). Then

                             w, z = (w1z1 + · · · + wnzn) = w1z1 + · · · + wnzn
                                                               = z1w1 + · · · + znwn = z, w

Definition 8.16 Norm and Length in Cn
As for the dot product on Rn, property (4) enables us to define the norm or length z of a vector
z = (z1, z2, . . . , zn) in Cn:

                                  z = z, z = |z1|2 + |z2|2 + · · · + |zn|2

The only properties of the norm function we will need are the following (the proofs are left to the reader):

   Theorem 8.7.2
   If z is any vector in Cn, then

       1. z  0 and z = 0 if and only if z = 0.
       2.  z = | | z for all complex numbers  .

A vector u in Cn is called a unit vector if u = 1. Property (2) in Theorem 8.7.2 then shows that if
                                             1
z = 0 is any nonzero vector in Cn, then u =  z  z  is a unit vector.

Example 8.7.2

In C4, find a unit vector u that is a positive real multiple of z = (1 - i, i, 2, 3 + 4i).

                                                                      1
Solution. z = 2 + 1 + 4 + 25 = 32 = 4 2, so take u =  z.
                                                                      42

    Transposition of complex matrices is defined just as in the real case, and the following notion is fun-
damental.
                                                                   8. . Complex Matrices

Definition 8.17 Conjugate Transpose in Cn
The conjugate transpose AH of a complex matrix A is defined by

                                                AH = (A)T = (AT )

Observe that AH = AT when A is real.14

Example 8.7.3

               3 1 - i 2 + i H  3 -2i 
               2i 5 + 2i -i =  1 + i 5 - 2i 

                                              2-i i

    The following properties of AH follow easily from the rules for transposition of real matrices and
extend these rules to complex matrices. Note the conjugate in property (3).

   Theorem 8.7.3
   Let A and B denote complex matrices, and let  be a complex number.

       1. (AH)H = A.
       2. (A + B)H = AH + BH .
       3. ( A)H =  AH.
       4. (AB)H = BHAH.

Hermitian and Unitary Matrices

If A is a real symmetric matrix, it is clear that AH = A. The complex matrices that satisfy this condition
turn out to be the most natural generalization of the real symmetric matrices:

   Definition 8.18 Hermitian Matrices
   A square complex matrix A is called hermitian15if AH = A, equivalently if A = AT .

Hermitian matrices are easy to recognize because the entries on the main diagonal must be real, and the
"reflection" of each nondiagonal entry in the main diagonal must be the conjugate of that entry.

   14Other notations for AH are A and A.
   15The name hermitian honours Charles Hermite (1822-1901), a French mathematician who worked primarily in analysis and
is remembered as the first to show that the number e from calculus is transcendental--that is, e is not a root of any polynomial
with integer coefficients.
Orthogonality

Example 8.7.4

 3 i 2+i                          1 i and 1 i are not.

 -i -2 -7  is hermitian, whereas  i -2  -i i
   2 - i -7 1

    The following Theorem extends Theorem 8.2.3, and gives a very useful characterization of hermitian
matrices in terms of the standard inner product in Cn.

   Theorem 8.7.4
   An n × n complex matrix A is hermitian if and only if

                                                    Az, w = z, Aw

   for all n-tuples z and w in Cn.

Proof. If A is hermitian, we have AT = A. If z and w are columns in Cn, then z, w = zT w, so

                            Az, w = (Az)T w = zT AT w = zT Aw = zT (Aw) = z, Aw

To prove the converse, let e j denote column j of the identity matrix. If A = ai j , the condition gives

                                            ai j = ei, Ae j = Aei, e j = ai j

Hence A = AT , so A is hermitian.

    Let A be an n ×n complex matrix. As in the real case, a complex number  is called an eigenvalue of A
if Ax =  x holds for some column x = 0 in Cn. In this case x is called an eigenvector of A corresponding
to  . The characteristic polynomial cA(x) is defined by

                                                   cA(x) = det (xI - A)

This polynomial has complex coefficients (possibly nonreal). However, the proof of Theorem 3.3.2 goes
through to show that the eigenvalues of A are the roots (possibly complex) of cA(x).

    It is at this point that the advantage of working with complex numbers becomes apparent. The real
numbers are incomplete in the sense that the characteristic polynomial of a real matrix may fail to have
all its roots real. However, this difficulty does not occur for the complex numbers. The so-called funda-
mental theorem of algebra ensures that every polynomial of positive degree with complex coefficients has
a complex root. Hence every square complex matrix A has a (complex) eigenvalue. Indeed (Appendix A),
cA(x) factors completely as follows:

                                         cA(x) = (x - 1)(x - 2) · · · (x - n)

where 1, 2, . . . , n are the eigenvalues of A (with possible repetitions due to multiple roots).
    The next result shows that, for hermitian matrices, the eigenvalues are actually real. Because symmet-

ric real matrices are hermitian, this re-proves Theorem 5.5.7. It also extends Theorem 8.2.4, which asserts
that eigenvectors of a symmetric real matrix corresponding to distinct eigenvalues are actually orthogonal.
In the complex context, two n-tuples z and w in Cn are said to be orthogonal if z, w = 0.
8. . Complex Matrices

Theorem 8.7.5
Let A denote a hermitian matrix.

   1. The eigenvalues of A are real.
   2. Eigenvectors of A corresponding to distinct eigenvalues are orthogonal.

Proof. Let  and µ be eigenvalues of A with (nonzero) eigenvectors z and w. Then Az =  z and Aw = µw,
so Theorem 8.7.4 gives

 z, w =  z, w = Az, w = z, Aw = z, µw = µ z, w                                                    (8.6)

If µ =  and w = z, this becomes  z, z =  z, z . Because z, z = z 2 = 0, this implies  =  .
Thus  is real, proving (1). Similarly, µ is real, so equation (8.6) gives  z, w = µ z, w . If  = µ, this
implies z, w = 0, proving (2).

    The principal axes theorem (Theorem 8.2.2) asserts that every real symmetric matrix A is orthogonally
diagonalizable--that is PT AP is diagonal where P is an orthogonal matrix (P-1 = PT ). The next theorem
identifies the complex analogs of these orthogonal real matrices.

Definition 8.19 Orthogonal and Orthonormal Vectors in Cn
As in the real case, a set of nonzero vectors {z1, z2, . . . , zm} in Cn is called orthogonal if
 zi, z j = 0 whenever i = j, and it is orthonormal if, in addition, zi = 1 for each i.

   Theorem 8.7.6
   The following are equivalent for an n × n complex matrix A.

       1. A is invertible and A-1 = AH.
       2. The rows of A are an orthonormal set in Cn.
       3. The columns of A are an orthonormal set in Cn.

Proof. If A = c1 c2 · · · cn is a complex matrix with jth column c j, then AT A = ci, c j , as in
Theorem 8.2.1. Now (1)  (2) follows, and (1)  (3) is proved in the same way.

   Definition 8.20 Unitary Matrices
   A square complex matrix U is called unitary if U -1 = U H.

Thus a real matrix is unitary if and only if it is orthogonal.
Orthogonality

Example 8.7.5

The matrix A = 1 + i 1 1 - i i has orthogonal columns, but the rows are not orthogonal.
                                                    
                                                  1 1+i  2
Normalizing the columns gives the unitary matrix 2  1 - i 2i  .

    Given a real symmetric matrix A, the diagonalization algorithm in Section 3.3 leads to a procedure for
finding an orthogonal matrix P such that PT AP is diagonal (see Example 8.2.4). The following example
illustrates Theorem 8.7.5 and shows that the technique works for complex matrices.

Example 8.7.6

Consider the hermitian matrix A = 3 2 + i 2 - i 7 . Find the eigenvalues of A, find two
orthonormal eigenvectors, and so find a unitary matrix U such that U HAU is diagonal.

Solution. The characteristic polynomial of A is

               cA(x) = det (xI - A) = det x - 3 -2 - i -2 + i x - 7 = (x - 2)(x - 8)

Hence the eigenvalues are 2 and 8 (both real as expected), and corresponding eigenvectors are
  2 + i -1 and 1 2 - i (orthogonal as expected). Each has length 6 so, as in the (real)

diagonalization algorithm, let U = 16  2+i 1        be the unitary matrix with the normalized
eigenvectors as columns.               -1 2 - i

Then U HAU = 2 0 0 8 is diagonal.

Unitary Diagonalization

An n × n complex matrix A is called unitarily diagonalizable if U HAU is diagonal for some unitary
matrix U . As Example 8.7.6 suggests, we are going to prove that every hermitian matrix is unitarily
diagonalizable. However, with only a little extra effort, we can get a very important theorem that has this
result as an easy consequence.

    A complex matrix is called upper triangular if every entry below the main diagonal is zero. We owe
the following theorem to Issai Schur.16

   16Issai Schur (1875-1941) was a German mathematician who did fundamental work in the theory of representations of
groups as matrices.
                                                         8. . Complex Matrices

Theorem 8.7.7: Schur's Theorem
If A is any n × n complex matrix, there exists a unitary matrix U such that

                                                    U HAU = T

is upper triangular. Moreover, the entries on the main diagonal of T are the eigenvalues
1, 2, . . . , n of A (including multiplicities).

Proof. We use induction on n. If n = 1, A is already upper triangular. If n > 1, assume the theorem is valid
for (n - 1) × (n - 1) complex matrices. Let 1 be an eigenvalue of A, and let y1 be an eigenvector with

 y1 = 1. Then y1 is part of a basis of Cn (by the analog of Theorem 6.4.1), so the (complex analog of
the) Gram-Schmidt process provides y2, . . . , yn such that {y1, y2, . . . , yn} is an orthonormal basis of Cn.
If U1 = y1 y2 · · · yn is the matrix with these vectors as its columns, then (see Lemma 5.4.3)

                                      UH   AU1  =  1 X1
                                                   0 A1
                                        1

in block form. Now apply induction to find a unitary (n - 1) × (n - 1) matrix W1 such that W1HA1W1 = T1
                                10
is upper triangular. Then U2 =  0 W1  is a unitary n × n matrix. Hence U = U1U2 is unitary (using

Theorem 8.7.6), and

                     U HAU = U2H (U1HAU1)U2

                     = H 1 0               1 X1    1 0 0 W1 = 1 X1W1 0 T1
                          0 W1             0 A1

is upper triangular. Finally, A and U HAU = T have the same eigenvalues by (the complex version of)
Theorem 5.5.1, and they are the diagonal entries of T because T is upper triangular.

    The fact that similar matrices have the same traces and determinants gives the following consequence
of Schur's theorem.

Corollary 8.7.1
Let A be an n × n complex matrix, and let 1, 2, . . . , n denote the eigenvalues of A, including
multiplicities. Then

                           det A = 12 · · · n and tr A = 1 + 2 + · · · + n

    Schur's theorem asserts that every complex matrix can be "unitarily triangularized." However, we
cannot substitute "unitarily diagonalized" here. In fact, if A = 1 1 0 1 , there is no invertible complex
matrix U at all such that U -1AU is diagonal. However, the situation is much better for hermitian matrices.

   Theorem 8.7.8: Spectral Theorem

   If A is hermitian, there is a unitary matrix U such that U HAU is diagonal.
6 Orthogonality

Proof. By Schur's theorem, let U HAU = T be upper triangular where U is unitary. Since A is hermitian,
this gives

                                    T H = (U HAU )H = U HAHU HH = U HAU = T

This means that T is both upper and lower triangular. Hence T is actually diagonal.

    The principal axes theorem asserts that a real matrix A is symmetric if and only if it is orthogonally
diagonalizable (that is, PT AP is diagonal for some real orthogonal matrix P). Theorem 8.7.8 is the complex
analog of half of this result. However, the converse is false for complex matrices: There exist unitarily
diagonalizable matrices that are not hermitian.

Example 8.7.7                                    01  is unitarily diagonalizable.
Show that the non-hermitian matrix A =         -1 0

Solution. The characteristic polynomial is cA(x) = x2 + 1. Hence the eigenvalues are i and -i, and
it is easy to verify that i -1 and -1i are corresponding eigenvectors. Moreover, these
eigenvectors are orthogonal and both have length 2, so U = 1 i -1 2 -1 i is a unitary matrix

such that U HAU = i 0 0 -i is diagonal.

There is a very simple way to characterize those complex matrices that are unitarily diagonalizable.

To this end, an n × n complex matrix N is called normal if NNH = NH N. It is clear that every hermitian

or unitary matrix is normal, as is the matrix    01  in Example 8.7.7. In fact we have the following
result.                                        -1 0

Theorem 8.7.9
An n × n complex matrix A is unitarily diagonalizable if and only if A is normal.

Proof. Assume first that U HAU = D, where U is unitary and D is diagonal. Then DDH = DHD as is
easily verified. Because DDH = U H(AAH)U and DHD = U H(AHA)U , it follows by cancellation that
AAH = AHA.

    Conversely, assume A is normal--that is, AAH = AH A. By Schur's theorem, let U HAU = T , where T
is upper triangular and U is unitary. Then T is normal too:

                                     T T H = U H(AAH)U = U H(AHA)U = T HT

Hence it suffices to show that a normal n × n upper triangular matrix T must be diagonal. We induct on n;
it is clear if n = 1. If n > 1 and T = ti j , then equating (1, 1)-entries in T T H and T H T gives

                                           |t11|2 + |t12|2 + · · · + |t1n|2 = |t11|2
                                                                8. . Complex Matrices

This implies t12 = t13 = · · · = t1n = 0, so T = t11 0 in block form. Hence T = H t11 0 so
      0 T1                                                      0 T1

T T H = T HT implies T1T1H = T1T1H. Thus T1 is diagonal by induction, and the proof is complete.

    We conclude this section by using Schur's theorem (Theorem 8.7.7) to prove a famous theorem about

matrices. Recall that the characteristic polynomial of a square matrix A is defined by cA(x) = det (xI - A),
and that the eigenvalues of A are just the roots of cA(x).

Theorem 8.7.10: Cayley-Hamilton Theorem17
If A is an n × n complex matrix, then cA(A) = 0; that is, A is a root of its characteristic polynomial.

Proof. If p(x) is any polynomial with complex coefficients, then p(P-1AP) = P-1 p(A)P for any invertible
complex matrix P. Hence, by Schur's theorem, we may assume that A is upper triangular. Then the
eigenvalues 1, 2, . . . , n of A appear along the main diagonal, so

      cA(x) = (x - 1)(x - 2)(x - 3) · · ·(x - n)

Thus

      cA(A) = (A - 1I)(A - 2I)(A - 3I) · · ·(A - nI)

Note that each matrix A - iI is upper triangular. Now observe:

1. A - 1I has zero first column because column 1 of A is (1, 0, 0, . . . , 0)T .
2. Then (A - 1I)(A - 2I) has the first two columns zero because the second column of (A - 2I) is

   (b, 0, 0, . . . , 0)T for some constant b.
3. Next (A - 1I)(A - 2I)(A - 3I) has the first three columns zero because column 3 of (A - 3I) is

   (c, d, 0, . . . , 0)T for some constants c and d.

Continuing in this way we see that (A - 1I)(A - 2I)(A - 3I) · · ·(A - nI) has all n columns zero; that
is, cA(A) = 0.

   17Named after the English mathematician Arthur Cayley (1821-1895) and William Rowan Hamilton (1805-1865), an Irish
mathematician famous for his work on physical dynamics.
   8 Orthogonality

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

8.8 An Application to Linear Codes over Finite Fields

For centuries mankind has been using codes to transmit messages. In many cases, for example transmit-
ting financial, medical, or military information, the message is disguised in such a way that it cannot be
understood by an intruder who intercepts it, but can be easily "decoded" by the intended receiver. This
subject is called cryptography and, while intriguing, is not our focus here. Instead, we investigate methods
for detecting and correcting errors in the transmission of the message.

    The stunning photos of the planet Saturn sent by the space probe are a very good example of how
successful these methods can be. These messages are subject to "noise" such as solar interference which
causes errors in the message. The signal is received on Earth with errors that must be detected and cor-
rected before the high-quality pictures can be printed. This is done using error-correcting codes. To see
how, we first discuss a system of adding and multiplying integers while ignoring multiples of a fixed
integer.
8.8. An Application to Linear Codes over Finite Fields

Modular Arithmetic

We work in the set Z = {0, ±1, ±2, ±3, . . . } of integers, that is the set of whole numbers. Everyone is
familiar with the process of "long division" from arithmetic. For example, we can divide an integer a by 5
and leave a remainder "modulo 5" in the set {0, 1, 2, 3, 4}. As an illustration

                                                       19 = 3 · 5 + 4

so the remainder of 19 modulo 5 is 4. Similarly, the remainder of 137 modulo 5 is 2 because we have
137 = 27 · 5 + 2. This works even for negative integers: For example,

                                                    -17 = (-4) · 5 + 3

so the remainder of -17 modulo 5 is 3.
    This process is called the division algorithm. More formally, let n  2 denote an integer. Then every

integer a can be written uniquely in the form

                            a = qn + r where q and r are integers and 0  r  n - 1

Here q is called the quotient of a modulo n, and r is called the remainder of a modulo n. We refer to n
as the modulus. Thus, if n = 6, the fact that 134 = 22 · 6 + 2 means that 134 has quotient 22 and remainder
2 modulo 6.

    Our interest here is in the set of all possible remainders modulo n. This set is denoted

                                              Zn = {0, 1, 2, 3, . . . , n - 1}

and is called the set of integers modulo n. Thus every integer is uniquely represented in Zn by its remain-
der modulo n.

    We are going to show how to do arithmetic in Zn by adding and multiplying modulo n. That is, we
add or multiply two numbers in Zn by calculating the usual sum or product in Z and taking the remainder
modulo n. It is proved in books on abstract algebra that the usual laws of arithmetic hold in Zn for any
modulus n  2. This seems remarkable until we remember that these laws are true for ordinary addition
and multiplication and all we are doing is reducing modulo n.

    To illustrate, consider the case n = 6, so that Z6 = {0, 1, 2, 3, 4, 5}. Then 2 + 5 = 1 in Z6 because 7
leaves a remainder of 1 when divided by 6. Similarly, 2 · 5 = 4 in Z6, while 3 + 5 = 2, and 3 + 3 = 0. In
this way we can fill in the addition and multiplication tables for Z6; the result is:

Tables for Z6

+012345   ×012345
0 012345  0 000000
1 123450  1 012345
2 234501  2 024024
3 345012  3 030303
4 450123  4 042042
5 501234  5 054321
        Orthogonality

Calculations in Z6 are carried out much as in Z . As an illustration, consider the familiar "distributive law"
a(b + c) = ab + ac from ordinary arithmetic. This holds for all a, b, and c in Z6; we verify a particular
case:

                                             3(5 + 4) = 3 · 5 + 3 · 4 in Z6

In fact, the left side is 3(5 + 4) = 3 · 3 = 3, and the right side is (3 · 5) + (3 · 4) = 3 + 0 = 3 too. Hence
doing arithmetic in Z6 is familiar. However, there are differences. For example, 3 · 4 = 0 in Z6, in contrast
to the fact that a · b = 0 in Z can only happen when either a = 0 or b = 0. Similarly, 32 = 3 in Z6, unlike
Z.

    Note that we will make statements like -30 = 19 in Z7; it means that -30 and 19 leave the same
remainder 5 when divided by 7, and so are equal in Z7 because they both equal 5. In general, if n  2 is
any modulus, the operative fact is that

                               a = b in Zn if and only if a - b is a multiple of n

In this case we say that a and b are equal modulo n, and write a = b( mod n).
    Arithmetic in Zn is, in a sense, simpler than that for the integers. For example, consider negatives.

Given the element 8 in Z17, what is -8? The answer lies in the observation that 8 + 9 = 0 in Z17, so
-8 = 9 (and -9 = 8). In the same way, finding negatives is not difficult in Zn for any modulus n.

Finite Fields

In our study of linear algebra so far the scalars have been real (possibly complex) numbers. The set R
of real numbers has the property that it is closed under addition and multiplication, that the usual laws of
arithmetic hold, and that every nonzero real number has an inverse in R. Such a system is called a field.
Hence the real numbers R form a field, as does the set C of complex numbers. Another example is the set
Q of all rational numbers (fractions); however the set Z of integers is not a field--for example, 2 has no
inverse in the set Z because 2 · x = 1 has no solution x in Z .

    Our motivation for isolating the concept of a field is that nearly everything we have done remains valid
if the scalars are restricted to some field: The gaussian algorithm can be used to solve systems of linear
equations with coefficients in the field; a square matrix with entries from the field is invertible if and only
if its determinant is nonzero; the matrix inversion algorithm works in the same way; and so on. The reason
is that the field has all the properties used in the proofs of these results for the field R, so all the theorems
remain valid.

    It turns out that there are finite fields--that is, finite sets that satisfy the usual laws of arithmetic and in
which every nonzero element a has an inverse, that is an element b in the field such that ab = 1. If n  2 is
an integer, the modular system Zn certainly satisfies the basic laws of arithmetic, but it need not be a field.
For example we have 2 · 3 = 0 in Z6 so 3 has no inverse in Z6 (if 3a = 1 then 2 = 2 · 1 = 2(3a) = 0a = 0
in Z6, a contradiction). The problem is that 6 = 2 · 3 can be properly factored in Z.

    An integer p  2 is called a prime if p cannot be factored as p = ab where a and b are positive integers
and neither a nor b equals 1. Thus the first few primes are 2, 3, 5, 7, 11, 13, 17, . . . . If n  2 is not a
prime and n = ab where 2  a, b  n - 1, then ab = 0 in Zn and it follows (as above in the case n = 6)
that b cannot have an inverse in Zn, and hence that Zn is not a field. In other words, if Zn is a field, then n
must be a prime. Surprisingly, the converse is true:
             8.8. An Application to Linear Codes over Finite Fields

Theorem 8.8.1
If p is a prime, then Zp is a field using addition and multiplication modulo p.

The proof can be found in books on abstract algebra.18 If p is a prime, the field Zp is called the field of
integers modulo p.

For example, consider the case n = 5. Then Z5 = {0, 1, 2, 3, 4} and the addition and multiplication
tables are:
             +01234   ×01234

             0 01234  0 00000

             1 12340  1 01234

             2 23401  2 02413

             3 34012  3 03142

             4 40123  4 04321

Hence 1 and 4 are self-inverse in Z5, and 2 and 3 are inverses of each other, so Z5 is indeed a field. Here
is another important example.

Example 8.8.1
If p = 2, then Z2 = {0, 1} is a field with addition and multiplication modulo 2 given by the tables

             +01      ×01

             0 0 1 and 0 0 0

             1 10     1 01

This is binary arithmetic, the basic algebra of computers.

    While it is routine to find negatives of elements of Zp, it is a bit more difficult to find inverses in Zp.
For example, how does one find 14-1 in Z17? Since we want 14-1 · 14 = 1 in Z17, we are looking for an
integer a with the property that a · 14 = 1 modulo 17. Of course we can try all possibilities in Z17 (there are
only 17 of them!), and the result is a = 11 (verify). However this method is of little use for large primes
p, and it is a comfort to know that there is a systematic procedure (called the euclidean algorithm) for
finding inverses in Zp for any prime p. Furthermore, this algorithm is easy to program for a computer. To
illustrate the method, let us once again find the inverse of 14 in Z17.

   Example 8.8.2
   Find the inverse of 14 in Z17.

   Solution. The idea is to first divide p = 17 by 14:

                                                       17 = 1 · 14 + 3

   Now divide (the previous divisor) 14 by the new remainder 3 to get

                                                       14 = 4 · 3 + 2

   18See, for example, W. Keith Nicholson, Introduction to Abstract Algebra, 4th ed., (New York: Wiley, 2012).
Orthogonality

and then divide (the previous divisor) 3 by the new remainder 2 to get

                                                    3 = 1·2+1

It is a theorem of number theory that, because 17 is a prime, this procedure will always lead to a
remainder of 1. At this point we eliminate remainders in these equations from the bottom up:

1 = 3-1·2                                         since 3 = 1 · 2 + 1
  = 3 - 1 · (14 - 4 · 3) = 5 · 3 - 1 · 14         since 2 = 14 - 4 · 3
  = 5 · (17 - 1 · 14) - 1 · 14 = 5 · 17 - 6 · 14  since 3 = 17 - 1 · 14

Hence (-6) · 14 = 1 in Z17, that is, 11 · 14 = 1. So 14-1 = 11 in Z17.

    As mentioned above, nearly everything we have done with matrices over the field of real numbers can
be done in the same way for matrices with entries from Zp. We illustrate this with one example. Again
the reader is referred to books on abstract algebra.

   Example 8.8.3

   Determine if the matrix A = 1 4 6 5 from Z7 is invertible and, if so, find its inverse.

   Solution. Working in Z7 we have det A = 1 · 5 - 6 · 4 = 5 - 3 = 2 = 0 in Z7, so A is invertible.
   Hence Example 2.4.4 gives A-1 = 2-1 5 -4 -6 1 . Note that 2-1 = 4 in Z7 (because 2 · 4 = 1 in
   Z7). Note also that -4 = 3 and -6 = 1 in Z7, so finally A-1 = 4 5 3 1 1 = 6 5 4 4 . The reader
   can verify that indeed 1 4 6 5 6 5 4 4 = 1 0 0 1 in Z7.

    While we shall not use them, there are finite fields other than Zp for the various primes p. Surprisingly,
for every prime p and every integer n  1, there exists a field with exactly pn elements, and this field is
unique.19 It is called the Galois field of order pn, and is denoted GF(pn).

   19See, for example, W. K. Nicholson, Introduction to Abstract Algebra, 4th ed., (New York: Wiley, 2012).
                                                       8.8. An Application to Linear Codes over Finite Fields

Error Correcting Codes

Coding theory is concerned with the transmission of information over a channel that is affected by noise.
The noise causes errors, so the aim of the theory is to find ways to detect such errors and correct at least
some of them. General coding theory originated with the work of Claude Shannon (1916-2001) who
showed that information can be transmitted at near optimal rates with arbitrarily small chance of error.

    Let F denote a finite field and, if n  1, let

                            Fn denote the F-vector space of 1 × n row matrices over F

with the usual componentwise addition and scalar multiplication. In this context, the rows in Fn are
called words (or n-words) and, as the name implies, will be written as [a b c d] = abcd. The individual
components of a word are called its digits. A nonempty subset C of Fn is called a code (or an n-code),
and the elements in C are called code words. If F = Z2, these are called binary codes.

    If a code word w is transmitted and an error occurs, the resulting word v is decoded as the code word
"closest" to v in Fn. To make sense of what "closest" means, we need a distance function on Fn analogous
to that in Rn (see Theorem 5.3.3). The usual definition in Rn does not work in this situation. For example,
if w = 1111 in (Z2)4 then the square of the distance of w from 0 is

                                    (1 - 0)2 + (1 - 0)2 + (1 - 0)2 + (1 - 0)2 = 0

even though w = 0.
    However there is a satisfactory notion of distance in Fn due to Richard Hamming (1915-1998). Given

a word w = a1a2 · · · an in Fn, we first define the Hamming weight wt(w) to be the number of nonzero
digits in w:

                                        wt(w) = wt(a1a2 · · · an) = |{i | ai = 0}|
Clearly, 0  wt(w)  n for every word w in Fn. Given another word v = b1b2 · · · bn in Fn, the Hamming
distance d(v, w) between v and w is defined by

                                         d(v, w) = wt(v - w) = |{i | bi = ai}|

In other words, d(v, w) is the number of places at which the digits of v and w differ. The next result
justifies using the term distance for this function d.

   Theorem 8.8.2
   Let u, v, and w denote words in Fn. Then:

       1. d(v, w)  0.

       2. d(v, w) = 0 if and only if v = w.

       3. d(v, w) = d(w, v).

       4. d(v, w)  d(v, u) + d(u, w)

Proof. (1) and (3) are clear, and (2) follows because wt(v) = 0 if and only if v = 0. To prove (4), write
x = v - u and y = u - w. Then (4) reads wt(x + y)  wt(x) + wt(y). If x = a1a2 · · · an and y = b1b2 · · · bn,
this follows because ai + bi = 0 implies that either ai = 0 or bi = 0.
        Orthogonality

    Given a word w in Fn and a real number r > 0, define the ball Br(w) of radius r (or simply the r-ball)
about w as follows:

                                            Br(w) = {x  Fn | d(w, x)  r}
Using this we can describe one of the most useful decoding methods.

   Theorem: Nearest Neighbour Decoding
   Let C be an n-code, and suppose a word v is transmitted and w is received. Then w is decoded as
   the code word in C closest to it. (If there is a tie, choose arbitrarily.)

    Using this method, we can describe how to construct a code C that can detect (or correct) t errors.
Suppose a code word c is transmitted and a word w is received with s errors where 1  s  t. Then s is
the number of places at which the c- and w-digits differ, that is, s = d(c, w). Hence Bt(c) consists of all
possible received words where at most t errors have occurred.

    Assume first that C has the property that no code word lies in the t-ball of another code word. Because
w is in Bt(c) and w = c, this means that w is not a code word and the error has been detected. If we
strengthen the assumption on C to require that the t-balls about code words are pairwise disjoint, then w
belongs to a unique ball (the one about c), and so w will be correctly decoded as c.

    To describe when this happens, let C be an n-code. The minimum distance d of C is defined to be the
smallest distance between two distinct code words in C; that is,

                                       d = min {d(v, w) | v and w in C; v = w}

   Theorem 8.8.3
   Let C be an n-code with minimum distance d. Assume that nearest neighbour decoding is used.
   Then:

       1. If t < d, then C can detect t errors.20

       2. If 2t < d, then C can correct t errors.

Proof.
   1. Let c be a code word in C. If w  Bt(c), then d(w, c)  t < d by hypothesis. Thus the t-ball Bt(c)
       contains no other code word, so C can detect t errors by the preceding discussion.

   2. If 2t < d, it suffices (again by the preceding discussion) to show that the t-balls about distinct code
       words are pairwise disjoint. But if c = c are code words in C and w is in Bt(c)  Bt(c), then
       Theorem 8.8.2 gives
                                       d(c, c)  d(c, w) + d(w, c)  t + t = 2t < d
       by hypothesis, contradicting the minimality of d.

   20We say that C detects (corrects) t errors if C can detect (or correct) t or fewer errors.
                                                       8.8. An Application to Linear Codes over Finite Fields

   Example 8.8.4
   If F = Z3 = {0, 1, 2}, the 6-code {111111, 111222, 222111} has minimum distance 3 and so can
   detect 2 errors and correct 1 error.

   Let c be any word in Fn. A word w satisfies d(w, c) = r if and only if w and c differ in exactly r digits.

If |F| = q, there are exactly   n     (q - 1)r  such words where  n            is the binomial coefficient. Indeed, choose
                                r                                 r
the r places where they differ in     n  ways, and then fill those places in w in (q - 1)r ways. It follows that
                                      r
the number of words in the t-ball about c is

               |Bt(c)| =  n  +     n  (q - 1) +     n  (q - 1)2 + · · · +      n  (q - 1)t  = it=0  ni (q - 1)i
                          0        1                2                          t

This leads to a useful bound on the size of error-correcting codes.

   Theorem 8.8.4: Hamming Bound

   Let C be an n-code over a field F that can correct t errors using nearest neighbour decoding. If

   |F| = q, then                                    |C|  t n i=0 (i)(q-1)i qn

Proof.  Write  k  =    t  n  (q - 1)i.   The t-balls centred at distinct code words each contain k words, and
                          i
                     i=0
there are |C| of them. Moreover they are pairwise disjoint because the code corrects t errors (see the

discussion preceding Theorem 8.8.3). Hence they contain k · |C| distinct words, and so k · |C|  |Fn| = qn,

proving the theorem.

   A code is called perfect if there is equality in the Hamming bound; equivalently, if every word in Fn

lies in exactly one t-ball about a code word. For example, if F = Z2, n = 3, and t = 1, then q = 2 and
3       3                                       23
 0 + 1 = 4, so the Hamming bound is 4 = 2. The 3-code C = {000, 111} has minimum distance 3 and
so can correct 1 error by Theorem 8.8.3. Hence C is perfect.

Linear Codes

Up to this point we have been regarding any nonempty subset of the F-vector space Fn as a code. However
many important codes are actually subspaces. A subspace C  Fn of dimension k  1 over F is called an
(n, k)-linear code, or simply an (n, k)-code. We do not regard the zero subspace (that is, k = 0) as a code.

   Example 8.8.5

   If F = Z2 and n  2, the n-parity-check code is constructed as follows: An extra digit is added to
   each word in Fn-1 to make the number of 1s in the resulting word even (we say such words have
   even parity). The resulting (n, n - 1)-code is linear because the sum of two words of even parity
   again has even parity.

    Many of the properties of general codes take a simpler form for linear codes. The following result gives
a much easier way to find the minimal distance of a linear code, and sharpens the results in Theorem 8.8.3.
6 Orthogonality

Theorem 8.8.5

Let C be an (n, k)-code with minimum distance d over a finite field F, and use nearest neighbour
decoding.

1. d = min {wt(w) | 0 = w  C}.

2. C can detect t  1 errors if and only if t < d.

3. C can correct t  1 errors if and only if 2t < d.

4. If C can correct t  1 errors and |F| = q, then

                 n  +  n  (q - 1) +  n  (q - 1)2 + · · · +  n  (q - 1)t  qn-k
                 0     1             2                      t

Proof.

   1. Write d = min {wt(w) | 0 = w in C}. If v = w are words in C, then d(v, w) = wt(v - w)  d
       because v - w is in the subspace C. Hence d  d. Conversely, given w = 0 in C then, since 0 is in
       C, we have wt(w) = d(w, 0)  d by the definition of d. Hence d  d and (1) is proved.

   2. Assume that C can detect t errors. Given w = 0 in C, the t-ball Bt(w) about w contains no other
       code word (see the discussion preceding Theorem 8.8.3). In particular, it does not contain the code
       word 0, so t < d(w, 0) = wt(w). Hence t < d by (1). The converse is part of Theorem 8.8.3.

   3. We require a result of interest in itself.
       Claim. Suppose c in C has wt(c)  2t. Then Bt(0)  Bt(c) is nonempty.
       Proof. If wt(c)  t, then c itself is in Bt(0)  Bt(c). So assume t < wt(c)  2t. Then c has more than
       t nonzero digits, so we can form a new word w by changing exactly t of these nonzero digits to zero.
       Then d(w, c) = t, so w is in Bt(c). But wt(w) = wt(c) - t  t, so w is also in Bt(0). Hence w is in
       Bt(0)  Bt(c), proving the Claim.
       If C corrects t errors, the t-balls about code words are pairwise disjoint (see the discussion preceding
       Theorem 8.8.3). Hence the claim shows that wt(c) > 2t for all c = 0 in C, from which d > 2t by (1).
       The other inequality comes from Theorem 8.8.3.

   4. We have |C| = qk because dim F C = k, so this assertion restates Theorem 8.8.4.

   Example 8.8.6

   If F = Z2, then

       C = {0000000, 0101010, 1010101, 1110000, 1011010, 0100101, 0001111, 1111111}

   is a (7, 3)-code; in fact C = span {0101010, 1010101, 1110000}. The minimum distance for C is
   3, the minimum weight of a nonzero word in C.
                                             8.8. An Application to Linear Codes over Finite Fields

Matrix Generators

Given a linear n-code C over a finite field F, the way encoding works in practice is as follows. A message
stream is blocked off into segments of length k  n called messages. Each message u in Fk is encoded as a
code word, the code word is transmitted, the receiver decodes the received word as the nearest code word,
and then re-creates the original message. A fast and convenient method is needed to encode the incoming
messages, to decode the received word after transmission (with or without error), and finally to retrieve
messages from code words. All this can be achieved for any linear code using matrix multiplication.

    Let G denote a k × n matrix over a finite field F, and encode each message u in Fk as the word uG in
Fn using matrix multiplication (thinking of words as rows). This amounts to saying that the set of code
words is the subspace C = {uG | u in Fk} of Fn. This subspace need not have dimension k for every
k × n matrix G. But, if {e1, e2, . . . , ek} is the standard basis of Fk, then eiG is row i of G for each I and
{e1G, e2G, . . . , ekG} spans C. Hence dim C = k if and only if the rows of G are independent in Fn, and
these matrices turn out to be exactly the ones we need. For reference, we state their main properties in
Lemma 8.8.1 below (see Theorem 5.4.4).

   Lemma 8.8.1
   The following are equivalent for a k × n matrix G over a finite field F:

       1. rank G = k.

       2. The columns of G span Fk.

       3. The rows of G are independent in Fn.

       4. The system GX = B is consistent for every column B in Rk.

       5. GK = Ik for some n × k matrix K.

Proof. (1)  (2). This is because dim ( col G) = k by (1).

(2)  (4). G x1 · · · xn T = x1c1 + · · · + xncn where c j is column j of G.

(4)  (5). G k1 · · · kk = Gk1 · · · Gkk for columns k j.

(5)  (3). If a1R1 + · · · + akRk = 0 where Ri is row i of G, then                    a1 · · · ak G = 0, so by (5),
  a1 · · · ak = 0. Hence each ai = 0, proving (3).

(3)  (1). rank G = dim ( row G) = k by (3).

Note that Theorem 5.4.4 asserts that, over the real field R, the properties in Lemma 8.8.1 hold if and only if
GGT is invertible. But this need not be true in general. For example, if F = Z2 and G = 1 0 1 0 0 1 0 1 ,

then GGT = 0. The reason is that the dot product w · w can be zero for w in Fn even if w = 0. However,
even though GGT is not invertible, we do have GK = I2 for some 4 × 2 matrix K over F as Lemma 8.8.1
asserts (in fact, K = 1 0 0 0 T is one such matrix).

                           0100

                                                                                                                      
                                                                                                                        w1
                                                                                                                        ..
Let  C    Fn  be  an  (n,  k)-code  over  a  finite  field  F.  If  {w1,  ...,  wk}  is  a  basis  of  C,  let  G  =    .   
                                                                                                                            

                                                                                                                        wk
  8 Orthogonality

be the k × n matrix with the wi as its rows. Let {e1, . . . , ek} is the standard basis of Fk regarded as rows.
Then wi = eiG for each i, so C = span {w1, . . . , wk} = span {e1G, . . . , ekG}. It follows (verify) that

                                                   C = {uG | u in Fk}

Because of this, the k × n matrix G is called a generator of the code C, and G has rank k by Lemma 8.8.1
because its rows wi are independent.

    In fact, every linear code C in Fn has a generator of a simple, convenient form. If G is a generator
matrix for C, let R be the reduced row-echelon form of G. We claim that C is also generated by R. Since
G  R by row operations, Theorem 2.5.1 shows that these same row operations G Ik  R W ,
performed on G Ik , produce an invertible k ×k matrix W such that R = W G. Then C = {uR | u in Fk}.
[In fact, if u is in Fk, then uG = u1R where u1 = uW -1 is in Fk, and uR = u2G where u2 = uW is in Fk].
Thus R is a generator of C, so we may assume that G is in reduced row-echelon form.

    In that case, G has no row of zeros (since rank G = k) and so contains all the columns of Ik. Hence a
series of column interchanges will carry G to the block form G = Ik A for some k × (n - k) matrix
A. Hence the code C = {uG | u in Fk} is essentially the same as C; the code words in C are obtained
from those in C by a series of column interchanges. Hence if C is a linear (n, k)-code, we may (and shall)
assume that the generator matrix G has the form

                                 G = Ik A for some k × (n - k) matrix A

Such a matrix is called a standard generator, or a systematic generator, for the code C. In this case,
if u is a message word in Fk, the first k digits of the encoded word uG are just the first k digits of u, so
retrieval of u from uG is very simple indeed. The last n - k digits of uG are called parity digits.

Parity-Check Matrices

We begin with an important theorem about matrices over a finite field.

   Theorem 8.8.6
   Let F be a finite field, let G be a k × n matrix of rank k, let H be an (n - k) × n matrix of rank n - k,
   and let C = {uG | u in Fk} and D = {vH | V in Fn-k} be the codes they generate. Then the
   following conditions are equivalent:

       1. GHT = 0.

       2. HGT = 0.

       3. C = {w in Fn | wHT = 0}.

       4. D = {w in Fn | wGT = 0}.

Proof. First, (1)  (2) holds because HGT and GHT are transposes of each other.
    (1)  (3) Consider the linear transformation T : Fn  Fn-k defined by T (w) = wHT for all w in Fn.

To prove (3) we must show that C = ker T . We have C  ker T by (1) because T (uG) = uGHT = 0 for all
u in Fk. Since dim C = rank G = k, it is enough (by Theorem 6.4.2) to show dim ( ker T ) = k. However
                        8.8. An Application to Linear Codes over Finite Fields

the dimension theorem (Theorem 7.2.4) shows that dim ( ker T ) = n - dim ( im T ), so it is enough to show
that dim ( im T ) = n - k. But if R1, . . . , Rn are the rows of HT , then block multiplication gives

                           im T = {wHT | w in Rn} = span {R1, . . . , Rn} = row (HT )

Hence dim ( im T ) = rank (HT ) = rank H = n - k, as required. This proves (3).
    (3)  (1) If u is in Fk, then uG is in C so, by (3), u(GHT ) = (uG)HT = 0. Since u is arbitrary in Fk,

it follows that GHT = 0.
    (2)  (4) The proof is analogous to (1)  (3).

The relationship between the codes C and D in Theorem 8.8.6 will be characterized in another way in the
next subsection.
If C is an (n, k)-code, an (n - k) × n matrix H is called a parity-check matrix for C if C = {w | wHT = 0}
as in Theorem 8.8.6. Such matrices are easy to find for a given code C. If G = Ik A is a standard
generator for C where A is k × (n - k), the (n - k) × n matrix

                        H = -AT In-k

is a parity-check matrix for C. Indeed, rank H = n - k because the rows of H are independent (due to the

presence of In-k), and

                        GHT = Ik A  -A = -A + A = 0
                                    In-k

by block multiplication. Hence H is a parity-check matrix for C and we have C = {w in Fn | wHT = 0}.
Since wHT and HwT are transposes of each other, this shows that C can be characterized as follows:

                        C = {w in Fn | HwT = 0}

by Theorem 8.8.6.

    This is useful in decoding. The reason is that decoding is done as follows: If a code word c is trans-
mitted and v is received, then z = v - c is called the error. Since HcT = 0, we have HzT = HvT and this
word

                                                      s = HzT = HvT

is called the syndrome. The receiver knows v and s = HvT , and wants to recover c. Since c = v - z, it is
enough to find z. But the possibilities for z are the solutions of the linear system

                                    HzT = s

where s is known. Now recall that Theorem 2.2.3 shows that these solutions have the form z = x + s where
x is any solution of the homogeneous system HxT = 0, that is, x is any word in C (by Lemma 8.8.1). In
other words, the errors z are the elements of the set

                        C + s = {c + s | c in C}

    The set C + s is called a coset of C. Let |F| = q. Since |C + s| = |C| = qn-k the search for z is reduced
from qn possibilities in Fn to qn-k possibilities in C + s. This is called syndrome decoding, and various
methods for improving efficiency and accuracy have been devised. The reader is referred to books on
coding for more details.21

   21For an elementary introduction, see V. Pless, Introduction to the Theory of Error-Correcting Codes, 3rd ed., (New York:
Wiley, 1998).
         Orthogonality

Orthogonal Codes

Let F be a finite field. Given two words v = a1a2 · · · an and w = b1b2 · · · bn in Fn, the dot product v · w is
defined (as in Rn) by

                                            v · w = a1b1 + a2b2 + · · · + anbn
Note that v · w is an element of F, and it can be computed as a matrix product: v · w = vwT .

    If C  Fn is an (n, k)-code, the orthogonal complement C is defined as in Rn:

                                        C = {v in Fn | v · c = 0 for all c in C}

This is easily seen to be a subspace of Fn, and it turns out to be an (n, n - k)-code. This follows when
F = R because we showed (in the projection theorem) that n = dim U  + dim U for any subspace U of
Rn. However the proofs break down for a finite field F because the dot product in Fn has the property that
w · w = 0 can happen even if w = 0. Nonetheless, the result remains valid.

    Theorem 8.8.7
    Let C be an (n, k)-code over a finite field F, let G = Ik A be a standard generator for C where
    A is k × (n - k), and write H = -AT In-k for the parity-check matrix. Then:

       1. H is a generator of C.
       2. dim (C) = n - k = rank H.
       3. C = C and dim (C) + dim C = n.

Proof. As in Theorem 8.8.6, let D = {vH | v in Fn-k} denote the code generated by H. Observe first that,
for all w in Fn and all u in Fk, we have

                                    w · (uG) = w(uG)T = w(GT uT ) = (wGT ) · u

Since C = {uG | u in Fk}, this shows that w is in C if and only if (wGT ) · u = 0 for all u in Fk; if and
only if22 wGT = 0; if and only if w is in D (by Theorem 8.8.6). Thus C = D and a similar argument
shows that D = C.

   1. H generates C because C = D = {vH | v in Fn-k}.
   2. This follows from (1) because, as we observed above, rank H = n - k.
   3. Since C = D and D = C, we have C = (C) = D = C. Finally the second equation in (3)

       restates (2) because dim C = k.

We note in passing that, if C is a subspace of Rk, we have C + C = Rk by the projection theorem
(Theorem 8.1.3), and C  C = {0} because any vector x in C  C satisfies x 2 = x · x = 0. How-
ever, this fails in general. For example, if F = Z2 and C = span {1010, 0101} in F4 then C = C, so
C +C = C = C C.

   22If v · u = 0 for every u in Fk, then v = 0--let u range over the standard basis of Fk.
            8.8. An Application to Linear Codes over Finite Fields

    We conclude with one more example. If F = Z2, consider the standard matrix G below, and the
corresponding parity-check matrix H:

1 0 0 0 1 1 1     1 1 1 0 1 0 0

G = 0 0 1 0 1 0 1   0 1 0 0 1 1 0  and H =  1 1 0 1 0 1 0 
                  1011001
   0001011

The code C = {uG | u in F4} generated by G has dimension k = 4, and is called the Hamming (7, 4)-code.
The vectors in C are listed in the first table below. The dual code generated by H has dimension n - k = 3
and is listed in the second table.

u  uG

0000 0000000

0001 0001011

0010 0010101

0011 0011110                  v vH
                            000 0000000
0100 0100110                001 1011001
                            010 1101010
0101 0101101      C : 011 0110011
                            100 1110100
0110 0110011                101 0101101
                            110 0011110
C : 0111 0111000            111 1000111

1000 1000111

1001 1001100

1010 1010010

1011 1011001

1100 1100001

1101 1101010

1110 1110100

1111 1111111

Clearly each nonzero code word in C has weight at least 3, so C has minimum distance d = 3. Hence C
can detect two errors and correct one error by Theorem 8.8.5. The dual code has minimum distance 4 and
so can detect 3 errors and correct 1 error.
    Orthogonality

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                          Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

8. An Application to Quadratic Forms

An  expression  like  x2  +  x2  +  x2  - 2x1x3     + x2x3          is  called   a  quadratic     form  in   the         variables  x1,  x2,  and  x3.

                       1      2      3
In this section we show that new variables y1, y2, and y3 can always be found so that the quadratic form,
when expressed in terms of the new variables, has no cross terms y1y2, y1y3, or y2y3. Moreover, we do this

for forms involving any finite number of variables using orthogonal diagonalization. This has far-reaching

applications; quadratic forms arise in such diverse areas as statistics, physics, the theory of functions of

several variables, number theory, and geometry.

    Definition 8.21 Quadratic Form
    A quadratic form q in the n variables x1, x2, . . . , xn is a linear combination of terms
    x21, x22, . . . , x2n, and cross terms x1x2, x1x3, x2x3, . . . .

    If n = 3, q has the form
          q = a11x21 + a22x22 + a33x23 + a12x1x2 + a21x2x1 + a13x1x3 + a31x3x1 + a23x2x3 + a32x3x2

In general                              x2          x2                       x2

                             q  =  a11   1  +  a22   2  +  ·  ·  ·  +   ann   n  +  a12x1  x2  +  a13  x1x3  +  ·  ·  ·

This sum can be written compactly as a matrix product

                                                        q = q(x) = xT Ax
                                                                                        8. . An Application to Quadratic Forms

where x = (x1, x2, . . . , xn) is thought of as a column, and A = ai j is a real n × n matrix. Note that if

i = j, two separate terms ai jxix j and a jix jxi are listed, each of which involves xix j, and they can (rather

cleverly) be replaced by               1                                            1
                                       2                                            2
                                          (ai  j  +   a  j  i  )xi  x  j    and        (ai  j  +  a  j  i  )x  j  xi

respectively, without altering the quadratic form. Hence there is no loss of generality in assuming that xix j

and x jxi have the same coefficient in the sum for q. In other words, we may assume that A is symmetric.

Example 8.9.1

Write  q  =  x2  +  3x23  +  2x1x2  -  x1x3       in  the      form       q(x)   =  xT  Ax,    where           A  is  a  symmetric          3  ×  3  matrix.

              1

Solution. The cross terms are 2x1x2 = x1x2 + x2x1 and -x1x3 = - 21 x1x3 - 12 x3x1.
Of course, x2x3 and x3x2 both have coefficient zero, as does x22. Hence

                                                                                   1 1 -2      1  
                                                                                                                  x1

                             q(x) = x1 x2 x3  1 0 0   x2 
                                                                               -1 0
                                                                                               3                  x3
                                                                                  2

is the required form (verify).

We shall assume from now on that all quadratic forms are given by

                                                                    q(x) = xT Ax

where A is symmetric. Given such a form, the problem is to find new variables y1, y2, . . . , yn, related to
x1, x2, . . . , xn, with the property that when q is expressed in terms of y1, y2, . . . , yn, there are no cross
terms. If we write

                                                  y = (y1, y2, . . . , yn)T

this amounts to asking that q = yT Dy where D is diagonal. It turns out that this can always be accomplished

and, not surprisingly, that D is the matrix obtained when the symmetric matrix A is orthogonally diagonal-
ized. In fact, as Theorem 8.2.2 shows, a matrix P can be found that is orthogonal (that is, P-1 = PT ) and

diagonalizes A:

                                                                            1 0 · · · 0                    

                                          T                               0        2    ···       0
                                                                                                     
                                          P AP = D =  .. ..                                       .. 
                                                                          . .                     .

                                                                            0 0 · · · n

The diagonal entries 1, 2, . . . , n are the (not necessarily distinct) eigenvalues of A, repeated according
to their multiplicities in cA(x), and the columns of P are corresponding (orthonormal) eigenvectors of A.
As A is symmetric, the i are real by Theorem 5.5.7.

    Now define new variables y by the equations

                                          x = Py equivalently y = PT x

Then substitution in q(x) = xT Ax gives

                    q  =  (Py)T  A(Py)    =       yT  (PT      AP)y         =  yT  Dy   =   1y21     +     2      y2  +  ·  ·  ·  +  n  y2

                                                                                                                   2                     n

Hence this change of variables produces the desired simplification in q.
Orthogonality

Theorem 8.9.1: Diagonalization Theorem
Let q = xT Ax be a quadratic form in the variables x1, x2, . . . , xn, where x = (x1, x2, . . . , xn)T and
A is a symmetric n × n matrix. Let P be an orthogonal matrix such that PT AP is diagonal, and
define new variables y = (y1, y2, . . . , yn)T by

                                      x = Py equivalently y = PT x

If q is expressed in terms of these new variables y1, y2, . . . , yn, the result is
                                          q = 1y21 + 2y22 + · · · + ny2n

where 1, 2, . . . , n are the eigenvalues of A repeated according to their multiplicities.

    Let q = xT Ax be a quadratic form where A is a symmetric matrix and let 1, . . . , n be the (real) eigen-
values of A repeated according to their multiplicities. A corresponding set {f1, . . . , fn} of orthonormal
eigenvectors for A is called a set of principal axes for the quadratic form q. (The reason for the name

will become clear later.) The orthogonal matrix P in Theorem 8.9.1 is given as P = f1 · · · fn , so the
variables X and Y are related by

                                                     
                                                        y1

                                                       y2    
                                                             
               x = Py = f1 f2 · · · fn  ..  = y1f1 + y2f2 + · · · + ynfn
                                                     .

                                                       yn

Thus the new variables yi are the coefficients when x is expanded in terms of the orthonormal basis
{f1, . . . , fn} of Rn. In particular, the coefficients yi are given by yi = x · fi by the expansion theorem
(Theorem 5.3.6). Hence q itself is easily computed from the eigenvalues i and the principal axes fi:

                                       q = q(x) = 1(x · f1)2 + · · · + n(x · fn)2

Example 8.9.2

Find new variables y1, y2, y3, and y4 such that

q  =  3(x21    +   2  +   2   +   2  )  +  2x1x2  -  10x1x3  +  10x1x4  +  10x2  x3  -  10x2  x4  +  2x3  x4

                  x2     x3      x4

has diagonal form, and find the corresponding principal axes.

Solution. The form can be written as q = xT Ax, where

                                                        3 1 -5 5 
                                 x1

                      x =  x3   x2  and A =  -5 5 3 1   1 3 5 -5 

                                 x4                             5 -5 1 3

A routine calculation yields

                         cA(x) = det (xI - A) = (x - 12)(x + 8)(x - 4)2
                                                                     8. . An Application to Quadratic Forms

    so the eigenvalues are 1 = 12, 2 = -8, and 3 = 4 = 4. Corresponding orthonormal
    eigenvectors are the principal axes:

                              1              1                          1             1

                         1      -1       1            -1             1    1      1     1
                                                                                         
                         f1 = 2  -1      f2 = 2  1             f3 = 2  1         f4 = 2  -1 

                                1                     -1                  1            -1

    The matrix                                                  1 1 1 1

                                                            1    -1     -1    1  1
                                                                                   
                                P = f1 f2 f3 f4 = 2  -1 1 1 -1 

                                                                     1 -1 1 -1

    is thus orthogonal, and P-1AP = PT AP is diagonal. Hence the new variables y and the old
    variables x are related by y = PT x and x = Py. Explicitly,

                         y1 = 12 (x1 - x2 - x3 + x4)                 x1 = 21 (y1 + y2 + y3 + y4)
                         y2 = 12 (x1 - x2 + x3 - x4)                 x2 = 21 (-y1 - y2 + y3 + y4)
                         y3 = 12 (x1 + x2 + x3 + x4)                 x3 = 21 (-y1 + y2 + y3 - y4)
                         y4 = 12 (x1 + x2 - x3 - x4)                 x4 = 21 (y1 - y2 + y3 - y4)

    If these xi are substituted in the original expression for q, the result is

                                         q = 12y21 - 8y22 + 4y23 + 4y24

    This is the required diagonal form.

    It is instructive to look at the case of quadratic forms in two variables x1 and x2. Then the principal
axes can always be found by rotating the x1 and x2 axes counterclockwise about the origin through an
angle  . This rotation is a linear transformation R : R2  R2, and it is shown in Theorem 2.6.4 that R
has matrix P = cos  - sin  sin  cos  . If {e1, e2} denotes the standard basis of R2, the rotation produces a
new basis {f1, f2} given by

                          f1 = R (e1) = cos  sin  and f2 = R (e2) = - sin  cos  (8.7)

        x2 Given a point p = x1 = x1e1 + x2e2 in the original system, let y1
                                                                 x2
y2                                  and y2 be the coordinates of p in the new system (see the diagram). That

                      p  y1         is,
        x2

    y2          y1                          x1 x2 = p = y1f1 + y2f2 = cos  - sin  y sin 1  cos  y2           (8.8)

        O x1 1               x

                                                      x1                  y1
                                    Writing x = x2 and y = y2 , this reads x = Py so, since P is or-
6 Orthogonality

thogonal, this is the change of variables formula for the rotation as in Theorem 8.9.1.
    If r = 0 = s, the graph of the equation rx21 + sx22 = 1 is called an ellipse if rs > 0 and a hyperbola if

rs < 0. More generally, given a quadratic form
                          q = ax21 + bx1x2 + cx22 where not all of a, b, and c are zero

the graph of the equation q = 1 is called a conic. We can now completely describe this graph. There are
two special cases which we leave to the reader.

   1. If exactly one of a and c is zero, then the graph of q = 1 is a parabola.

So we assume that a = 0 and c = 0. In this case, the description depends on the quantity b2 - 4ac, called
the discriminant of the quadratic form q.

   2. If b2 - 4ac =0, theneither both a 0 and c 0, or both a  0 and c  0.
       Hence q = ( ax1 + cx2)2 or q = ( -ax1 + -cx2)2, so the graph of q = 1 is a pair of straight
       lines in either case.

So we also assume that b2 - 4ac = 0. But then the next theorem asserts that there exists a rotation of
the plane about the origin which transforms the equation ax21 + bx1x2 + cx22 = 1 into either an ellipse or a
hyperbola, and the theorem also provides a simple way to decide which conic it is.

Theorem 8.9.2
Consider the quadratic form q = ax21 + bx1x2 + cx22 where a, c, and b2 - 4ac are all nonzero.

1. There is a counterclockwise rotation of the coordinate axes about the origin such that, in the
   new coordinate system, q has no cross term.

2. The graph of the equation             ax21 + bx1x2 + cx22 = 1

      is an ellipse if b2 - 4ac < 0 and an hyperbola if b2 - 4ac > 0.

Proof. If b = 0, q already has no cross term and (1) and (2) are clear. So assume b = 0. The matrix
    a 1b
                                                              2                  12
A= 1      2  of q has characteristic polynomial cA(x) = x - (a + c)x - 4(b - 4ac). If we write

    2b c
d = b2 + (a - c)2 for convenience; then the quadratic formula gives the eigenvalues

                 1  =              1 [a  + c - d]  and  2  =  1 [a  +  c  +  d]

                                   2                          2

with corresponding principal axes

                 f1 = b2+(a-c-d)2 1 a - c - d b and

                 f2 =  1 b2+(a-c-d)2                  -b
                                                   a-c-d
                                                      8. . An Application to Quadratic Forms

as the reader can verify. These agree with equation (8.7) above if  is an angle such that

                              cos  =  a-c-d b2+(a-c-d)2 and sin  =  b b2+(a-c-d)2

Then P = f1 f2 = cos  - sin  sin  cos   diagonalizes A and equation (8.8) becomes the formula x = Py
in Theorem 8.9.1. This proves (1).

Finally, A is similar to      1 0  so 12 = det A = 14 (4ac - b2). Hence the graph of 1y21 + 2y22 = 1
                              0 2
is an ellipse if b2 < 4ac and an hyperbola if b2 > 4ac. This proves (2).

Example 8.9.3
Consider the equation x2 + xy + y2 = 1. Find a rotation so that the equation has no cross term.

Solution.

           x2                 Here a = b = c = 1 in the notation of Theorem 8.9.2, so
                              cos  = -12 and sin  = 12 . Hence  = 34 will do it. The new
y1                            variables are y1 = 12 (x2 - x1) and y2 = -12 (x2 + x1) by (8.8),
                              and  the  equation      becomes  y2  + 3y22     2.  The  angle    has  been
                          3                                                =                  
                           4                                    1
                              x1 chosen such that the new y1 and y2 axes are the axes of symmetry
y2                            of the ellipse (see the diagram). The eigenvectors f1 = 1 -1 2 1

                              and f2 = 1 -1 2 -1 point along these axes of symmetry, and
                              this is the reason for the name principal axes.

The determinant of any orthogonal matrix P is either 1 or -1 (because PPT = I). The orthogonal

matrices  cos  - sin          arising from rotations all have determinant 1. More generally, given any
          sin  cos 

quadratic form q = xT Ax, the orthogonal matrix P such that PT AP is diagonal can always be chosen so

that det P = 1 by interchanging two eigenvalues (and hence the corresponding columns of P). It is shown

in Theorem 10.4.4 that orthogonal 2 × 2 matrices with determinant 1 correspond to rotations. Similarly,

it can be shown that orthogonal 3 × 3 matrices with determinant 1 correspond to rotations about a line

through the origin. This extends Theorem 8.9.2: Every quadratic form in two or three variables can be

diagonalized by a rotation of the coordinate system.
  8 Orthogonality

Congruence

We return to the study of quadratic forms in general.

   Theorem 8.9.3
   If q(x) = xT Ax is a quadratic form given by a symmetric matrix A, then A is uniquely determined
   by q.

Proof. Let q(x) = xT Bx for all x where BT = B. If C = A - B, then CT = C and xTCx = 0 for all x. We
must show that C = 0. Given y in Rn,

                              0 = (x + y)TC(x + y) = xTCx + xTCy + yTCx + yTCy
                                                         = xTCy + yTCx

    But yTCx = (xTCy)T = xTCy (it is 1 × 1). Hence xTCy = 0 for all x and y in Rn. If e j is column j of
In, then the (i, j)-entry of C is eTi Ce j = 0. Thus C = 0.

Hence we can speak of the symmetric matrix of a quadratic form.

     On the other hand, a quadratic form q in variables xi can be written in several ways as a linear combi-

nation of squares of new variables, even if the new variables are required to be linear combinations of the
                              2x21                 x2
xi.  For  example,  if  q  =        -  4x1x2   +       then
                                                    2

                                 q  =  2(x1   -  x2  )2  -  x2  and  q = -2x21 + (2x1 - x2)2

                                                             2

The question arises: How are these changes of variables related, and what properties do they share? To
investigate this, we need a new concept.

    Let a quadratic form q = q(x) = xT Ax be given in terms of variables x = (x1, x2, . . . , xn)T . If the new
variables y = (y1, y2, . . . , yn)T are to be linear combinations of the xi, then y = Ax for some n × n matrix
A. Moreover, since we want to be able to solve for the xi in terms of the yi, we ask that the matrix A be
invertible. Hence suppose U is an invertible matrix and that the new variables y are given by

                                              y = U -1x, equivalently x = U y

In terms of these new variables, q takes the form

                                       q = q(x) = (U y)T A(U y) = yT (U T AU )y

That is, q has matrix U T AU with respect to the new variables y. Hence, to study changes of variables

in quadratic forms, we study the following relationship on matrices: Two n × n matrices A and B are
                                    c
called    congruent,    written  A     B,  if   B  =   U T AU   for  some  invertible  matrix  U.  Here  are  some  properties  of
                                    

congruence:

              c

     1. A  A for all A.

     2. If A  B, then B  A.c  c

     3. If A  B and B  C, then A  C.cc       c
                                                                               8. . An Application to Quadratic Forms

                  c

   4. If A  B, then A is symmetric if and only if B is symmetric.

                  c

   5. If A  B, then rank A = rank B.
The converse to (5) can fail even for symmetric matrices.

Example 8.9.4

The symmetric matrices A = 1 0 0 1 and B = 1 0 0 -1 have the same rank but are not

congruent.  Indeed,  if  A  c  B,  an  invertible   matrix          U    exists    such        that   B  =  U T AU  =  UTU.  But  then

                            
-1 = det B = ( det U )2, a contradiction.

    The key distinction between A and B in Example 8.9.4 is that A has two positive eigenvalues (counting
multiplicities) whereas B has only one.

   Theorem 8.9.4: Sylvester's Law of Inertia

             c

   If A  B, then A and B have the same number of positive eigenvalues, counting multiplicities.

The proof is given at the end of this section.

    The index of a symmetric matrix A is the number of positive eigenvalues of A. If q = q(x) = xT Ax
is a quadratic form, the index and rank of q are defined to be, respectively, the index and rank of the
matrix A. As we saw before, if the variables expressing a quadratic form q are changed, the new matrix is
congruent to the old one. Hence the index and rank depend only on q and not on the way it is expressed.

    Now let q = q(x) = xT Ax be any quadratic form in n variables, of index k and rank r, where A is
symmetric. We claim that new variables z can be found so that q is completely diagonalized--that is,

                                       q(z)  =   2  +  ·  ·  ·  +   zk2  -  zk2+1  -  ·  ·  ·  -   2

                                                z1                                                zr

If k  r  n, let Dn(k, r) denote the n×n diagonal matrix whose main diagonal consists of k ones, followed
by r - k minus ones, followed by n - r zeros. Then we seek new variables z such that

                                                q(z) = zT Dn(k, r)z

To determine z, first diagonalize A as follows: Find an orthogonal matrix P0 such that

                               P0T AP0 = D = diag (1, 2, . . . , r, 0, . . . , 0)

is diagonal with the nonzero eigenvalues 1, 2, . . . , r of A on the main diagonal (followed by n - r
zeros). By reordering the columns of P0, if necessary, we may assume that 1, . . . , k are positive and
k+1, . . . , r are negative. This being the case, let D0 be the n × n diagonal matrix

                     D0 = diag 1 , . . . , 1 ,  1 , . . . ,  1 , 1, . . . , 1
                                       1                     k           -k+1                  -r

Then DT0 DD0 = Dn(k, r), so if new variables z are given by x = (P0D0)z, we obtain

                            q(z)   =  zT Dn(k,  r)z    =        z2  + · · · + z2k  - z2k+1        - · · · - z2r

                                                                 1

as required. Note that the change-of-variables matrix P0D0 from z to x has orthogonal columns (in fact,
scalar multiples of the columns of P0).
        Orthogonality

Example 8.9.5
Completely diagonalize the quadratic form q in Example 8.9.2 and find the index and rank .

Solution. In the notation of Example 8.9.2, the eigenvalues of the matrix A of q are 12, -8, 4, 4; so

the index is 3 and the rank is 4. Moreover, the corresponding orthogonal eigenvectors are f1, f2, f3
(see Example 8.9.2), and f4. Hence P0 = f1 f3 f4 f2 is orthogonal and

                                              P0T AP0 = diag (12, 4, 4, -8)

As before, take D0 = diag ( 112 , 12 , 21 , 18 ) and define the new variables z by x = (P0D0)z. Hence
                                              D-1PT
the     new  variables  are  given  by  z  =         x.  The  result  is
                                                00

                                                    
                                              z1 = 3(x1 - x2 - x3 + x4)

                                              z2 = x1 + x2 + x3 + x4

                                              z3 = x1 + x2 - x3 - x4
                                                    

                                              z4 = 2(x1 - x2 + x3 - x4)

This discussion gives the following information about symmetric matrices.

Theorem 8.9.5
Let A and B be symmetric n × n matrices, and let 0  k  r  n.

                                                                            c

   1. A has index k and rank r if and only if A  Dn(k, r).

              c

   2. A  B if and only if they have the same rank and index.

Proof.

1. If A has index k and rank r, take U = P0D0 where P0 and D0 are as described prior to Example 8.9.5.
   Then U T AU = Dn(k, r). The converse is true because Dn(k, r) has index k and rank r (using

   Theorem 8.9.4).

2. If A and B both have index k and rank r, then A  Dn(k, r)  B by (1). The converse was givencc

earlier.
                                                                        8. . An Application to Quadratic Forms

Proof of Theorem 8.9.4.

By Theorem 8.9.1, A  D1 and B  D2 where D1 and D2 are diagonal and have the same eigenvalues as Acc

and B, respectively. We have D1  D2, (because A  B), so we may assume that A and B are both diagonal.cc

Consider the quadratic form q(x) = xT Ax. If A has k positive eigenvalues, q has the form

                 q(x)    =  a1x21  +  ·  ·  ·  +  ak  x2  -  ak+1   2    -  ·  ·  ·  -  ar  x2  ,  ai > 0

                                                       k           xk+1                      r

where r = rank A = rank B. The subspace W1 = {x | xk+1 = · · · = xr = 0} of Rn has dimension n - r + k
and satisfies q(x) > 0 for all x = 0 in W1.

On the other hand, if B = U T AU , define new variables y by x = U y. If B has k positive eigenvalues, q

has the form     q(x) = b1y21 + · · · + bk yk2 - bk+1y2k+1 - · · · - bry2r , bi > 0

Let f1, . . . , fn denote the columns of U . They are a basis of Rn and

                                                             
                                                               y1
                                                                   ...
                         x =Uy =      f1       · · · fn                  = y1f1 + · · · + ynfn
                                                             

                                                               yn

Hence the subspace W2 = span {fk+1, . . . , fr} satisfies q(x) < 0 for all x = 0 in W2. Note dim W2 = r - k.
It follows that W1 and W2 have only the zero vector in common. Hence, if B1 and B2 are bases of W1 and
W2, respectively, then (Exercise ??) B1  B2 is an independent set of (n - r + k) + (r - k) = n + k - k
vectors in Rn. This implies that k  k, and a similar argument shows k  k.

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!
         Orthogonality

8. An Application to Constrained Optimization

It is a frequent occurrence in applications that a function q = q(x1, x2, . . . , xn) of n variables, called an
objective function, is to be made as large or as small as possible among all vectors x = (x1, x2, . . . , xn)
lying in a certain region of Rn called the feasible region. A wide variety of objective functions q arise in
practice; our primary concern here is to examine one important situation where q is a quadratic form. The
next example gives some indication of how such problems arise.
                                                              8. . An Application to Constrained Optimization

Example 8.10.1

                                          A politician proposes to spend x1 dollars annually on health
                                          care and x2 dollars annually on education. She is constrained
   x2

5                                         in her spending by various budget pressures, and one model of this

                 5x21 + 3x22  15          is that the expenditures x1 and x2 should satisfy a constraint like

2

1                      c=2                                                  5x21 + 3x22  15

                       c=1                Since xi  0 for each i, the feasible region is the shaded area

O 1  x1 3 2 shown in the diagram. Any choice of feasible point (x1, x2) in this
                                 region will satisfy the budget constraints. However, these choices

                                          have different effects on voters, and the politician wants to choose

x = (x1, x2) to maximize some measure q = q(x1, x2) of voter satisfaction. Thus the assumption is

that, for any value of c, all points on the graph of q(x1, x2) = c have the same appeal to voters.
Hence the goal is to find the largest value of c for which the graph of q(x1, x2) = c contains a

feasible point.

The choice of the function q depends upon many factors; we will show how to solve the problem

for any quadratic form q (even with more than two variables). In the diagram the function q is

given by

                                                     q(x1, x2) = x1x2

and the graphs of q(x1, x2) = c are shown for c = 1 and c = 2. As c increases the graph of

q(x1, x2) = c moves up and to the right. From this it is clearthat there will be a solution for some1
value  of  c  between  1  and     2  (in  fact  the  largest  value  is  c  =  2  15 = 1.94 to two decimal places).

The constraint 5x21 + 3x22  15 in Example 8.10.1 can be put in a standard form. If we divide through
by 15, it becomes x1 2 3 + x2 2 5  1. This suggests that we introduce new variables y = (y1, y2) where
y1 = x1 and y2 = x2 . Then the constraint becomes y 2  1, equivalently y  1. In terms of these new
3                5                              

variables, the objective function is q = 15y1y2, and we want to maximize this subject to y  1. When
this is done, the maximizing values of x1 and x2 are obtained from x1 = 3y1 and x2 = 5y2.

Hence, for constraints like that in Example 8.10.1, there is no real loss in generality in assuming that

the constraint takes the form x  1. In this case the principal axes theorem solves the problem. Recall

that a vector in Rn of length 1 is called a unit vector.

Theorem 8.10.1
Consider the quadratic form q = q(x) = xT Ax where A is an n × n symmetric matrix, and let 1
and n denote the largest and smallest eigenvalues of A, respectively. Then:

   1. max {q(x) | x  1} = 1, and q(f1) = 1 where f1 is any unit 1-eigenvector.
   2. min {q(x) | x  1} = n, and q(fn) = n where fn is any unit n-eigenvector.

Proof. Since A is symmetric, let the (real) eigenvalues i of A be ordered as to size as follows:
                                                   1  2  · · ·  n
Orthogonality

By the principal axes theorem, let P be an orthogonal matrix such that PT AP = D = diag (1, 2, . . . , n).
Define y = PT x, equivalently x = Py, and note y = x because y 2 = yT y = xT (PPT )x = xT x = x 2.
If we write y = (y1, y2, . . . , yn)T , then

                             q(x) = q(Py) = (Py)T A(Py)

                                  = yT (PT AP)y = yT Dy

                                  =  1   y2  +  2  y2  +  ·  ·  ·  +  n  y2             (8.9)

                                          1         2                     n

Now assume that x  1. Since i  1 for each i, (8.9) gives

q(x) = 1y21 + 2y22 + · · · + ny2n  1y21 + 1y22 + · · · + 1y2n = 1 y 2  1

because y = x  1. This shows that q(x) cannot exceed 1 when x  1. To see that this maximum
is actually achieved, let f1 be a unit eigenvector corresponding to 1. Then

               q(f1)  =  fT  Af1  =  fT  (1f1)  =  1(fT1     f1)   =  1      f1  2 = 1

                          1           1

Hence 1 is the maximum value of q(x) when x  1, proving (1). The proof of (2) is analogous.

    The set of all vectors x in Rn such that x  1 is called the unit ball. If n = 2, it is often called the
unit disk and consists of the unit circle and its interior; if n = 3, it is the unit sphere and its interior. It is
worth noting that the maximum value of a quadratic form q(x) as x ranges throughout the unit ball is (by
Theorem 8.10.1) actually attained for a unit vector x on the boundary of the unit ball.

    Theorem 8.10.1 is important for applications involving vibrations in areas as diverse as aerodynamics
and particle physics, and the maximum and minimum values in the theorem are often found using advanced
calculus to minimize the quadratic form on the unit ball. The algebraic approach using the principal axes
theorem gives a geometrical interpretation of the optimal values because they are eigenvalues.

Example 8.10.2
Maximize and minimize the form q(x) = 3x21 + 14x1x2 + 3x22 subject to x  1.

Solution. The matrix of q is A = 3 7 7 3 , with eigenvalues 1 = 10 and 2 = -4, and
corresponding unit eigenvectors f1 = 12 (1, 1) and f2 = 12(1, -1). Hence, among all unit vectors
x in R2, q(x) takes its maximal value 10 at x = f1, and the minimum value of q(x) is -4 when
x = f2.

    As noted above, the objective function in a constrained optimization problem need not be a quadratic
form. We conclude with an example where the objective function is linear, and the feasible region is
determined by linear constraints.
                              8. . An Application to Constrained Optimization

Example 8.10.3570             A manufacturer makes x1 units of product 1, and x2 units
   x2 1200x1 + 1300x2 = 8700p=of product 2, at a profit of $70 and $50 per unit respectively,
                              and wants to choose x1 and x2 to maximize the total profit
           (4, 3)500          p(x1, x2) = 70x1 + 50x2. However x1 and x2 are not arbitrary; for
    p=                        example, x1  0 and x2  0. Other conditions also come into play.
  O x1                        Each unit of product 1 costs $1200 to produce and requires 2000
  430                         square feet of warehouse space; each unit of product 2 costs $1300
   2000x1 + 1100x2 = 11300p=  to produce and requires 1100 square feet of space. If the total
                              warehouse space is 11 300 square feet, and if the total production
                              budget is $8700, x1 and x2 must also satisfy the conditions

                                                      2000x1 + 1100x2  11300

                                                      1200x1 + 1300x2  8700

The feasible region in the plane satisfying these constraints (and x1  0, x2  0) is shaded in the
diagram. If the profit equation 70x1 + 50x2 = p is plotted for various values of p, the resulting
lines are parallel, with p increasing with distance from the origin. Hence the best choice occurs for

the line 70x1 + 50x2 = 430 that touches the shaded region at the point (4, 3). So the profit p has a
maximum of p = 430 for x1 = 4 units and x2 = 3 units.

    Example 8.10.3 is a simple case of the general linear programming problem23 which arises in eco-
nomic, management, network, and scheduling applications. Here the objective function is a linear com-
bination q = a1x1 + a2x2 + · · · + anxn of the variables, and the feasible region consists of the vectors
x = (x1, x2, . . . , xn)T in Rn which satisfy a set of linear inequalities of the form b1x1 +b2x2 +· · ·+bnxn  b.
There is a good method (an extension of the gaussian algorithm) called the simplex algorithm for finding
the maximum and minimum values of q when x ranges over such a feasible set. As Example 8.10.3 sug-
gests, the optimal values turn out to be vertices of the feasible set. In particular, they are on the boundary
of the feasible region, as is the case in Theorem 8.10.1.

   23More information is available in "Linear Programming and Extensions" by N. Wu and R. Coppins, McGraw-Hill, 1981.
  6 Orthogonality

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

8. An Application to Statistical Principal Component
        Analysis

Linear algebra is important in multivariate analysis in statistics, and we conclude with a very short look
at one application of diagonalization in this area. A main feature of probability and statistics is the idea
of a random variable X , that is a real-valued function which takes its values according to a probability
law (called its distribution). Random variables occur in a wide variety of contexts; examples include the
number of meteors falling per square kilometre in a given region, the price of a share of a stock, or the
duration of a long distance telephone call from a certain city.

    The values of a random variable X are distributed about a central number µ, called the mean of X .
The mean can be calculated from the distribution as the expectation E(X ) = µ of the random variable X .
Functions of a random variable are again random variables. In particular, (X - µ)2 is a random variable,
and the variance of the random variable X , denoted var (X ), is defined to be the number

                                     var (X ) = E{(X - µ)2} where µ = E(X )
It is not difficult to see that var (X )  0 for every random variable X . The number  = var (X ) is called
the standard deviation of X , and is a measure of how much the values of X are spread about the mean
µ of X . A main goal of statistical inference is finding reliable methods for estimating the mean and the
standard deviation of a random variable X by sampling the values of X .

    If two random variables X and Y are given, and their joint distribution is known, then functions of X
and Y are also random variables. In particular, X +Y and aX are random variables for any real number a,
                                       8. . An Application to Statistical Principal Component Analysis

and we have                    E(X +Y ) = E(X ) + E(Y ) and E(aX ) = aE(X ).24

An important question is how much the random variables X and Y depend on each other. One measure of
this is the covariance of X and Y , denoted cov (X , Y ), defined by

                    cov (X , Y ) = E{(X - µ)(Y - )} where µ = E(X ) and  = E(Y )

Clearly, cov (X , X ) = var (X ). If cov (X , Y ) = 0 then X and Y have little relationship to each other and
are said to be uncorrelated.25

    Multivariate statistical analysis deals with a family X1, X2, . . . , Xn of random variables with means
               and  variances  2       var (Xi)  for  each  i.  Let          cov (Xi,       denote  the  covariance  of      and
µi  =  E (Xi)                       =                                i j  =            Xj)                               Xi
                                 i
Xj. Then the covariance matrix of the random variables X1, X2, . . . , Xn is defined to be the n × n matrix

                                                       = [i j]

whose (i, j)-entry is i j. The matrix  is clearly symmetric; in fact it can be shown that  is positive
semidefinite in the sense that   0 for every eigenvalue  of . (In reality,  is positive definite in most

cases of interest.) So suppose that the eigenvalues of  are 1  2  · · ·  n  0. The principal axes
theorem (Theorem 8.2.2) shows that an orthogonal matrix P exists such that

                                       PT P = diag (1, 2, . . . , n)

If we write X = (X1, X2, . . . , Xn), the procedure for diagonalizing a quadratic form gives new variables
Y = (Y1, Y2, . . . , Yn) defined by

                                                          Y = PT X

These new random variables Y1, Y2, . . . , Yn are called the principal components of the original random
variables Xi, and are linear combinations of the Xi. Furthermore, it can be shown that

                    cov (Yi, Yj) = 0 if i = j and var (Yi) = i for each i

Of course the principal components Yi point along the principal axes of the quadratic form q = XT X.

    The sum of the variances of a set of random variables is called the total variance of the variables, and
determining the source of this total variance is one of the benefits of principal component analysis. The
fact that the matrices  and diag (1, 2, . . . , n) are similar means that they have the same trace, that is,

                                       11 + 22 + · · · + nn = 1 + 2 + · · · + n

This means that the principal components Yi have the same total variance as the original random variables
Xi. Moreover, the fact that 1  2  · · ·  n  0 means that most of this variance resides in the first few
Yi. In practice, statisticians find that studying these first few Yi (and ignoring the rest) gives an accurate
analysis of the total system variability. This results in substantial data reduction since often only a few Yi
suffice for all practical purposes. Furthermore, these Yi are easily obtained as linear combinations of the
Xi. Finally, the analysis of the principal components often reveals relationships among the Xi that were not
previously suspected, and so results in interpretations that would not otherwise have been made.

   24Hence E( ) is a linear transformation from the vector space of all random variables to the space of real numbers.
   25If X and Y are independent in the sense of probability theory, then they are uncorrelated; however, the converse is not true
in general.
  8 Orthogonality

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.
                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.
                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!
Chapter 9

           Change of Basis

If A is an m × n matrix, the corresponding matrix transformation TA : Rn  Rm is defined by

                                         TA(x) = Ax for all columns x in Rn

It was shown in Theorem 2.6.2 that every linear transformation T : Rn  Rm is a matrix transformation;
that is, T = TA for some m × n matrix A. Furthermore, the matrix A is uniquely determined by T . In fact,
A is given in terms of its columns by

                                          A = T (e1) T (e2) · · · T (en)

where {e1, e2, . . . , en} is the standard basis of Rn.
    In this chapter we show how to associate a matrix with any linear transformation T : V  W where V

and W are finite-dimensional vector spaces, and we describe how the matrix can be used to compute T (v)
for any v in V . The matrix depends on the choice of a basis B in V and a basis D in W , and is denoted
MDB(T ). The case when W = V is particularly important. If B and D are two bases of V , we show that the
matrices MBB(T ) and MDD(T ) are similar, that is MDD(T ) = P-1MBB(T )P for some invertible matrix P.
Moreover, we give an explicit method for constructing P depending only on the bases B and D. This leads
to some of the most important theorems in linear algebra, as we shall see in Chapter 11.

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

                                                             439
Change of Basis

 . The Matrix of a Linear Transformation

Let T : V  W be a linear transformation where dim V = n and dim W = m. The aim in this section is to
describe the action of T as multiplication by an m × n matrix A. The idea is to convert a vector v in V into
a column in Rn, multiply that column by A to get a column in Rm, and convert this column back to get
T (v) in W .

    Converting vectors to columns is a simple matter, but one small change is needed. Up to now the order
of the vectors in a basis has been of no importance. However, in this section, we shall speak of an ordered
basis {b1, b2, . . . , bn}, which is just a basis where the order in which the vectors are listed is taken into
account. Hence {b2, b1, b3} is a different ordered basis from {b1, b2, b3}.

    If B = {b1, b2, . . . , bn} is an ordered basis in a vector space V , and if

                                        v = v1b1 + v2b2 + · · · + vnbn, vi  R

is a vector in V , then the (uniquely determined) numbers v1, v2, . . . , vn are called the coordinates of v
with respect to the basis B.

Definition 9.1 Coordinate Vector CB(v) of v for a basis B
The coordinate vector of v with respect to B is defined to be

                                                               
                                                                  v1

                                                                 v2  
                                                                     
                 CB(v) = CB(v1b1 + v2b2 + · · · + vnbn) =  .. 
                                                               .

                                                                 vn

The reason for writing CB(v) as a column instead of a row will become clear later. Note that CB(bi) = ei
is column i of In.

   Example 9.1.1
   The coordinate vector for v = (2, 1, 3) with respect to the ordered basis

                                                                     0
   B = {(1, 1, 0), (1, 0, 1), (0, 1, 1)} of R3 is CB(v) =  2  because

                                                                         1

                                v = (2, 1, 3) = 0(1, 1, 0) + 2(1, 0, 1) + 1(0, 1, 1)

Theorem 9.1.1
If V has dimension n and B = {b1, b2, . . . , bn} is any ordered basis of V , the coordinate
                            . . The Matrix of a Linear Transformation

transformation CB : V  Rn is an isomorphism. In fact, CB-1 : Rn  V is given by

                                                                           
                  v1                                                          v1

                      v2                                                     v2    n
               -1                                                                
               CB  ..  = v1b1 + v2b2 + · · · + vnbn               for all   ..  in R .
               .                                                           .

                      vn                                                     vn

Proof. The verification that CB is linear is Exercise ??. If T : Rn  V is the map denoted C-1 B in the
theorem, one verifies (Exercise ??) that TCB = 1V and CBT = 1Rn. Note that CB(b j) is column j of the
identity matrix, so CB carries the basis B to the standard basis of Rn, proving again that it is an isomorphism
(Theorem 7.3.1)

            T  W                Now let T : V  W be any linear transformation where dim V = n and
V                 CD        dim W = m, and let B = {b1, b2, . . . , bn} and D be ordered bases of V and
                            W , respectively. Then CB : V  Rn and CD : W  Rm are isomorphisms
  CB           Rm           and we have the situation shown in the diagram where A is an m ×n matrix
Rn TA                       (to be determined). In fact, the composite

                                            CDTCB-1 : Rn  Rm is a linear transformation

so Theorem 2.6.2 shows that a unique m × n matrix A exists such that

                          CDTCB-1 = TA, equivalently CDT = TACB

TA acts by left multiplication by A, so this latter condition is

                            CD[T (v)] = ACB(v) for all v in V

This requirement completely determines A. Indeed, the fact that CB(b j) is column j of the identity matrix
gives

                                        column j of A = ACB(b j) = CD[T (b j)]

for all j. Hence, in terms of its columns,

                      A = CD[T (b1)] CD[T (b2)] · · · CD[T (bn)]

   Definition 9.2 Matrix MDB(T ) of T : V  W for bases D and B
   This is called the matrix of T corresponding to the ordered bases B and D, and we use the
   following notation:

                             MDB(T ) = CD[T (b1)] CD[T (b2)] · · · CD[T (bn)]

This discussion is summarized in the following important theorem.
Change of Basis

Theorem 9.1.2
Let T : V  W be a linear transformation where dim V = n and dim W = m, and let
B = {b1, . . . , bn} and D be ordered bases of V and W , respectively. Then the matrix MDB(T ) just
given is the unique m × n matrix A that satisfies

                                                   CDT = TACB
Hence the defining property of MDB(T ) is

                                   CD[T (v)] = MDB(T )CB(v) for all v in V
The matrix MDB(T ) is given in terms of its columns by

                         MDB(T ) = CD[T (b1)] CD[T (b2)] · · · CD[T (bn)]

The fact that T = CD-1TACB means that the action of T on a vector v in V can be performed by first taking
coordinates (that is, applying CB to v), then multiplying by A (applying TA), and finally converting the
resulting m-tuple back to a vector in W (applying C-1 D ).

Example 9.1.2
Define T : P2  R2 by T (a + bx + cx2) = (a + c, b - a - c) for all polynomials a + bx + cx2. If
B = {b1, b2, b3} and D = {d1, d2} where

                        b1 = 1, b2 = x, b3 = x2 and d1 = (1, 0), d2 = (0, 1)

compute MDB(T ) and verify Theorem 9.1.2.

Solution. We have T (b1) = d1 - d2, T (b2) = d2, and T (b3) = d1 - d2. Hence

MDB(T ) = CD[T (b1)] CD[T (b2)] CD[T (bn)] = 1 0 1 -1 1 -1

If v = a + bx + cx2 = ab1 + bb2 + cb3, then T (v) = (a + c)d1 + (b - a - c)d2, so

                                           a
CD[T (v)] = a + c = 1 0 1  b  = MDB(T )CB(v)
                           b-a-c           -1 1 -1 c

as Theorem 9.1.2 asserts.

The next example shows how to determine the action of a transformation from its matrix.
                                      . . The Matrix of a Linear Transformation

Example 9.1.3
                                                                         1 -1 0 0 

Suppose T : M22(R)  R3 is linear with matrix MDB(T ) =  0 1 -1 0  where
                                                                           0 0 1 -1

   B = 1 0 0 0 , 0 1 0 0 , 0 0 1 0 , 0 0 0 1 and D = {(1, 0, 0), (0, 1, 0), (0, 0, 1)}

Compute T (v) where v = a b .
                                  cd

Solution. The idea is to compute CD[T (v)] first, and then obtain T (v). We have

                                            
                                            a
                                    1 -1 0                             a-b        
CD[T (v)] = MDB(T )CB(v) =  0 1 -1          0 b
                                            0     =  b - c 
                                      0 0 1 -1  c  c - d
                                            d

                      Hence T (v) = (a - b)(1, 0, 0) + (b - c)(0, 1, 0) + (c - d)(0, 0, 1)
                                      = (a - b, b - c, c - d)

The next two examples will be referred to later.

   Example 9.1.4
   Let A be an m × n matrix, and let TA : Rn  Rm be the matrix transformation induced by
   A : TA(x) = Ax for all columns x in Rn. If B and D are the standard bases of Rn and Rm,
   respectively (ordered as usual), then

                                                       MDB(TA) = A
   In other words, the matrix of TA corresponding to the standard bases is A itself.

   Solution. Write B = {e1, . . . , en}. Because D is the standard basis of Rm, it is easy to verify that
   CD(y) = y for all columns y in Rm. Hence

                 MDB(TA) = TA(e1) TA(e2) · · · TA(en) = Ae1 Ae2 · · · Aen = A

   because Ae j is the jth column of A.

Example 9.1.5
Let V and W have ordered bases B and D, respectively. Let dim V = n.

   1. The identity transformation 1V : V  V has matrix MBB(1V ) = In.
   2. The zero transformation 0 : V  W has matrix MDB(0) = 0.
   Change of Basis

The first result in Example 9.1.5 is false if the two bases of V are not equal. In fact, if B is the standard
basis of Rn, then the basis D of Rn can be chosen so that MDB(1Rn) turns out to be any invertible matrix
we wish (Exercise ??).

    The next two theorems show that composition of linear transformations is compatible with multiplica-
tion of the corresponding matrices.

Theorem 9.1.3

                    TLet V  W  U be linear transformations and let B, D, and E beS

V  T  W   S    U    finite ordered bases of V , W , and U, respectively. Then

      ST                  MEB(ST ) = MED(S) · MDB(T )

Proof. We use the property in Theorem 9.1.2 three times. If v is in V ,
                  MED(S)MDB(T )CB(v) = MED(S)CD[T (v)] = CE[ST (v)] = MEB(ST )CB(v)

If B = {e1, . . . , en}, then CB(e j) is column j of In. Hence taking v = e j shows that MED(S)MDB(T ) and
MEB(ST ) have equal jth columns. The theorem follows.
                                   . . The Matrix of a Linear Transformation

Theorem 9.1.4
Let T : V  W be a linear transformation, where dim V = dim W = n. The following are
equivalent.

   1. T is an isomorphism.

   2. MDB(T ) is invertible for all ordered bases B and D of V and W .

   3. MDB(T ) is invertible for some pair of ordered bases B and D of V and W .
When this is the case, [MDB(T )]-1 = MBD(T -1).

                          T  T -1
Proof. (1)  (2). We have V  W  V , so Theorem 9.1.3 and Example 9.1.5 give

                   MBD(T -1)MDB(T ) = MBB(T -1T ) = MBB(1v) = In

Similarly, MDB(T )MBD(T -1) = In, proving (2) (and the last statement in the theorem).

(2)  (3). This is clear.

Rn R TA-1 n TA Rn      (3)  (1). Suppose that TDB(T ) is invertible for some bases B and D and, for
         TATA-1    convenience, write A = MDB(T ). Then we have CDT = TACB by Theorem 9.1.2,
                   so

                                                        T = (CD)-1TACB

                               by Theorem 9.1.1 where (CD)-1 and CB are isomorphisms. Hence (1) follows if
we can demonstrate that TA : Rn  Rn is also an isomorphism. But A is invertible by (3) and one verifies
that TATA-1 = 1Rn = TA-1TA. So TA is indeed invertible (and (TA)-1 = TA-1).

    In Section 7.2 we defined the rank of a linear transformation T : V  W by rank T = dim ( im T ).

Moreover, if A is any m × n matrix and TA : Rn  Rm is the matrix transformation, we showed that
rank (TA) = rank A. So it may not be surprising that rank T equals the rank of any matrix of T .

Theorem 9.1.5

Let T : V  W be a linear transformation where dim V = n and dim W = m. If B and D are any
ordered bases of V and W , then rank T = rank [MDB(T )].

Proof. Write A = MDB(T ) for convenience. The column space of A is U = {Ax | x in Rn}. This means
rank A = dim U and so, because rank T = dim ( im T ), it suffices to find an isomorphism S : im T  U .
Now every vector in im T has the form T (v), v in V . By Theorem 9.1.2, CD[T (v)] = ACB(v) lies in U . So
define S : im T  U by

                                  S[T (v)] = CD[T (v)] for all vectors T (v)  im T

The fact that CD is linear and one-to-one implies immediately that S is linear and one-to-one. To see that
S is onto, let Ax be any member of U , x in Rn. Then x = CB(v) for some v in V because CB is onto. Hence
Ax = ACB(v) = CD[T (v)] = S[T (v)], so S is onto. This means that S is an isomorphism.
6 Change of Basis

Example 9.1.6

Define T : P2  R3 by T (a + bx + cx2) = (a - 2b, 3c - 2a, 3c - 4b) for a, b, c  R. Compute
rank T .

Solution. Since rank T = rank [MDB(T )] for any bases B  P2 and D  R3, we choose the most
convenient ones: B = {1, x, x2} and D = {(1, 0, 0), (0, 1, 0), (0, 0, 1)}. Then
MDB(T ) = CD[T (1)] CD[T (x)] CD[T (x2)] = A where

 1 -2 0                                         1 -2 0   1 -2 0 
                                                       1 -3 
A =  -2            0 3  . Since A   0 -4 3    0
                                                               4
                   0 -4 3                      0 -4 3  00 0

we have rank A = 2. Hence rank T = 2 as well.

    We conclude with an example showing that the matrix of a linear transformation can be made very
simple by a careful choice of the two bases.

   Example 9.1.7
   Let T : V  W be a linear transformation where dim V = n and dim W = m. Choose an ordered
   basis B = {b1, . . . , br, br+1, . . . , bn} of V in which {br+1, . . . , bn} is a basis of ker T , possibly
   empty. Then {T (b1), . . . , T (br)} is a basis of im T by Theorem 7.2.5, so extend it to an ordered
   basis D = {T (b1), . . . , T (br), fr+1, . . . , fm} of W . Because T (br+1) = · · · = T (bn) = 0, we have

          MDB(T ) = CD[T (b1)] · · · CD[T (br)] CD[T (br+1)] · · · CD[T (bn)] = Ir 0 0 0

   Incidentally, this shows that rank T = r by Theorem 9.1.5.
                             . . Operators and Similarity

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                          Vretta-Lyryx Engage is an active learning app designed to increase
                         student engagement in reading linear algebra material. The content is
                       "chunked" into small blocks, each with an interactive assessment activity

                                                  to promote comprehension.

                        Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

. Operators and Similarity

While the study of linear transformations from one vector space to another is important, the central prob-

lem of linear algebra is to understand the structure of a linear transformation T : V  V from a space

V to itself. Such transformations are called linear operators. If T : V  V is a linear operator where

dim (V ) = n, it is possible to choose bases B and D of V such that the matrix MDB(T ) has a very simple
                 Ir 0
form: MDB(T ) =  00    where r = rank T (see Example 9.1.7). Consequently, only the rank of T

is revealed by determining the simplest matrices MDB(T ) of T where the bases B and D can be chosen

arbitrarily. But if we insist that B = D and look for bases B such that MBB(T ) is as simple as possible, we

learn a great deal about the operator T . We begin this task in this section.

The B-matrix of an Operator

Definition 9.3 Matrix MDB(T ) of T : V  W for basis B

If T : V  V is an operator on a vector space V , and if B is an ordered basis of V , define
MB(T ) = MBB(T ) and call this the B-matrix of T .

    Recall that if T : Rn  Rn is a linear operator and E = {e1, e2, . . . , en} is the standard basis of
Rn, then CE (x) = x for every x  Rn, so ME(T ) = [T (e1), T (e2), . . . , T (en)] is the matrix obtained in
Theorem 2.6.2. Hence ME(T ) will be called the standard matrix of the operator T .
  8 Change of Basis

    For reference the following theorem collects some results from Theorem 9.1.2, Theorem 9.1.3, and
Theorem 9.1.4, specialized for operators. As before, CB(v) denoted the coordinate vector of v with respect
to the basis B.

   Theorem 9.2.1
   Let T : V  V be an operator where dim V = n, and let B be an ordered basis of V .

       1. CB(T (v)) = MB(T )CB(v) for all v in V .
       2. If S : V  V is another operator on V , then MB(ST ) = MB(S)MB(T ).
       3. T is an isomorphism if and only if MB(T ) is invertible. In this case MD(T ) is invertible for

           every ordered basis D of V .
       4. If T is an isomorphism, then MB(T -1) = [MB(T )]-1.
       5. If B = {b1, b2, . . . , bn}, then MB(T ) = CB[T (b1)] CB[T (b2)] · · · CB[T (bn)] .

    For a fixed operator T on a vector space V , we are going to study how the matrix MB(T ) changes when
the basis B changes. This turns out to be closely related to how the coordinates CB(v) change for a vector
v in V . If B and D are two ordered bases of V , and if we take T = 1V in Theorem 9.1.2, we obtain

                                       CD(v) = MDB(1V )CB(v) for all v in V

   Definition 9.4 Change Matrix PDB for bases B and D
   With this in mind, define the change matrix PDB by

                             PDB = MDB(1V ) for any ordered bases B and D of V
                                                             . . Operators and Similarity

This proves equation 9.2 in the following theorem:

Theorem 9.2.2
Let B = {b1, b2, . . . , bn} and D denote ordered bases of a vector space V . Then the change matrix
PDB is given in terms of its columns by

                           PDB = CD(b1) CD(b2) · · · CD(bn)                                (9.1)

and has the property that

                            CD(v) = PDBCB(v) for all v in V                                (9.2)

Moreover, if E is another ordered basis of V , we have

1. PBB = In
2. PDB is invertible and (PDB)-1 = PBD

3. PEDPDB = PEB

Proof. The formula 9.2 is derived above, and 9.1 is immediate from the definition of PDB and the formula
for MDB(T ) in Theorem 9.1.2.

1. PBB = MBB(1V ) = In as is easily verified.
2. This follows from (1) and (3).

3. Let V  W  U be operators, and let B, D, and E be ordered bases of V , W , and U respectively.TS

We have MEB(ST ) = MED(S)MDB(T ) by Theorem 9.1.3. Now (3) is the result of specializing

V = W = U and T = S = 1V .

Property (3) in Theorem 9.2.2 explains the notation PDB.

Example 9.2.1
In P2 find PDB if B = {1, x, x2} and D = {1, (1 - x), (1 - x)2}. Then use this to express
p = p(x) = a + bx + cx2 as a polynomial in powers of (1 - x).

Solution. To compute the change matrix PDB, express 1, x, x2 in the basis D:

                                           1 = 1 + 0(1 - x) + 0(1 - x)2
                                           x = 1 - 1(1 - x) + 0(1 - x)2
                                          x2 = 1 - 2(1 - x) + 1(1 - x)2
Change of Basis

                 1 1 1                                                                      a
Hence PDB = CD(1), CD(x), CD(x)2 =  0 -1 -2 . We have CB(p) =  b , so
                 001
                                                                                            c

                  1 1 1  a   a+b+c 

CD(p) = PDBCB(p) =  0 -1 -2   b  =  -b - 2c 
                 001 c
                                                                                         c

Hence p(x) = (a + b + c) - (b + 2c)(1 - x) + c(1 - x)2 by Definition 9.1.1

    Now let B = {b1, b2, . . . , bn} and B0 be two ordered bases of a vector space V . An operator T : V  V
has different matrices MB[T ] and MB0[T ] with respect to B and B0. We can now determine how these
matrices are related. Theorem 9.2.2 asserts that

                                          CB0(v) = PB0BCB(v) for all v in V
On the other hand, Theorem 9.2.1 gives

                                        CB[T (v)] = MB(T )CB(v) for all v in V
Combining these (and writing P = PB0B for convenience) gives

                                            PMB(T )CB(v) = PCB[T (v)]
                                                              = CB0[T (v)]
                                                              = MB0(T )CB0(v)
                                                              = MB0(T )PCB(v)

This holds for all v in V . Because CB(b j) is the jth column of the identity matrix, it follows that

                                                   PMB(T ) = MB0(T )P
Moreover P is invertible (in fact, P-1 = PBB0 by Theorem 9.2.2), so this gives

                                                 MB(T ) = P-1MB0(T )P
This asserts that MB0(T ) and MB(T ) are similar matrices, and proves Theorem 9.2.3.

   Theorem 9.2.3: Similarity Theorem
   Let B0 and B be two ordered bases of a finite dimensional vector space V . If T : V  V is any
   linear operator, the matrices MB(T ) and MB0(T ) of T with respect to these bases are similar. More
   precisely,

                                                 MB(T ) = P-1MB0(T )P
   where P = PB0B is the change matrix from B to B0.

1This also follows from Taylor's theorem (Corollary 6.5.3 of Theorem 6.5.1 with a = 1).
                                                             . . Operators and Similarity

Example 9.2.2
Let T : R3  R3 be defined by T (a, b, c) = (2a - b, b + c, c - 3a). If B0 denotes the standard
basis of R3 and B = {(1, 1, 0), (1, 0, 1), (0, 1, 0)}, find an invertible matrix P such that
P-1MB0(T )P = MB(T ).

Solution. We have

MB0(T ) =          CB0(2, 0, -3) CB0(-1, 1, 0) CB0(0, 1, 1)      2 -1 0 
                                                             = 0 1 1 

                                                                   -3 0 1

MB(T ) =           CB(1, 1, -3) CB(2, 1, -2) CB(-1, 1, 0)        4 4 -1 
                                                             =  -3 -2 0 

                                                                   -3 -3 2

P = PB0B =           CB0(1, 1, 0) CB0(1, 0, 1) CB0(0, 1, 0)     1 1 0
                                                             = 1 0 1 

                                                                   010

The reader can verify that P-1MB0(T )P = MB(T ); equivalently that MB0(T )P = PMB(T ).

    A square matrix is diagonalizable if and only if it is similar to a diagonal matrix. Theorem 9.2.3 comes
into this as follows: Suppose an n × n matrix A = MB0(T ) is the matrix of some operator T : V  V with
respect to an ordered basis B0. If another ordered basis B of V can be found such that MB(T ) = D is
diagonal, then Theorem 9.2.3 shows how to find an invertible P such that P-1AP = D. In other words, the
"algebraic" problem of finding P such that P-1AP is diagonal comes down to the "geometric" problem
of finding a basis B such that MB(T ) is diagonal. This shift of emphasis is one of the most important
techniques in linear algebra.

    Each n × n matrix A can be easily realized as the matrix of an operator. In fact, (Example 9.1.4),

                                                        ME (TA) = A

where TA : Rn  Rn is the matrix operator given by TA(x) = Ax, and E is the standard basis of Rn. The
first part of the next theorem gives the converse of Theorem 9.2.3: Any pair of similar matrices can be
realized as the matrices of the same linear operator with respect to different bases. This is part 1 of the
following theorem.

Theorem 9.2.4
Let A be an n × n matrix and let E be the standard basis of Rn.

   1. Let A be similar to A, say A = P-1AP, and let B be the ordered basis of Rn consisting of the
       columns of P in order. Then TA : Rn  Rn is linear and

                                         ME(TA) = A and MB(TA) = A

2. If B is any ordered basis of Rn, let P be the (invertible) matrix whose columns are the vectors

in B in order. Then  MB(TA) = P-1AP
        Change of Basis

Proof.

1. We have ME(TA) = A by Example 9.1.4. Write P = b1 · · · bn             in terms of its columns so
   B = {b1, . . . , bn} is a basis of Rn. Since E is the standard basis,

                             PEB = CE (b1) · · · CE (bn) = b1 · · · bn = P
   Hence Theorem 9.2.3 (with B0 = E) gives MB(TA) = P-1ME(TA)P = P-1AP = A.
2. Here P and B are as above, so again PEB = P and MB(TA) = P-1AP.

Example 9.2.3

Given A = 10 6 -18 -11 , P = 2 -1 -3 2 , and D = 1 0 0 -2 , verify that P-1AP = D and
use this fact to find a basis B of R2 such that MB(TA) = D.

Solution. P-1AP = D holds if AP = PD; this verification is left to the reader. Let B consist of the
columns of P in order, that is B = 2 -3 , -12 . Then Theorem 9.2.4 gives

MB(TA) = P-1AP = D. More explicitly,

MB(TA) = CB TA 2 -3      CB TA -12 = CB 2 -3 CB 2 -4 = 1 0 0 -2 = D

    Let A be an n × n matrix. As in Example 9.2.3, Theorem 9.2.4 provides a new way to find an invertible
matrix P such that P-1AP is diagonal. The idea is to find a basis B = {b1, b2, . . . , bn} of Rn such that

MB(TA) = D is diagonal and take P =  b1 b2 · · · bn to be the matrix with the b j as columns. Then,
by Theorem 9.2.4,                     P-1AP = MB(TA) = D

As mentioned above, this converts the algebraic problem of diagonalizing A into the geometric problem of
finding the basis B. This new point of view is very powerful and will be explored in the next two sections.

    Theorem 9.2.4 enables facts about matrices to be deduced from the corresponding properties of oper-
ators. Here is an example.

Example 9.2.4

   1. If T : V  V is an operator where V is finite dimensional, show that T ST = T for some
       invertible operator S : V  V .

   2. If A is an n × n matrix, show that AUA = A for some invertible matrix U .

Solution.
   1. Let B = {b1, . . . , br, br+1, . . . , bn} be a basis of V chosen so that
       ker T = span {br+1, . . . , bn}. Then {T (b1), . . . , T (br)} is independent (Theorem 7.2.5), so
       complete it to a basis {T (b1), . . . , T (br), fr+1, . . . , fn} of V .
                                                    . . Operators and Similarity

By Theorem 7.1.3, define S : V  V by

              S[T (bi)] = bi          for 1  i  r
                  S(f j) = b j        for r < j  n

Then S is an isomorphism by Theorem 7.3.1, and T ST = T because these operators agree on
the basis B. In fact,

                    (T ST )(bi) = T [ST (bi)] = T (bi) if 1  i  r, and
              (T ST )(bj) = T S[T (b j)] = T S(0) = 0 = T (b j) for r < j  n

2. Given A, let T = TA : Rn  Rn. By (1) let T ST = T where S : Rn  Rn is an isomorphism.
   If E is the standard basis of Rn, then A = ME(T ) by Theorem 9.2.4. If U = ME(S) then, by
   Theorem 9.2.1, U is invertible and

              AUA = ME(T )ME(S)ME(T ) = ME(T ST ) = ME(T ) = A

as required.

The reader will appreciate the power of these methods if he/she tries to find U directly in part 2 of Exam-
ple 9.2.4, even if A is 2 × 2.

    A property of n × n matrices is called a similarity invariant if, whenever a given n × n matrix A has
the property, every matrix similar to A also has the property. Theorem 5.5.1 shows that rank , determinant,
trace, and characteristic polynomial are all similarity invariants.

    To illustrate how such similarity invariants are related to linear operators, consider the case of rank .
If T : V  V is a linear operator, the matrices of T with respect to various bases of V all have the same
rank (being similar), so it is natural to regard the common rank of all these matrices as a property of T
itself and not of the particular matrix used to describe T . Hence the rank of T could be defined to be the
rank of A, where A is any matrix of T . This would be unambiguous because rank is a similarity invariant.
Of course, this is unnecessary in the case of rank because rank T was defined earlier to be the dimension
of im T , and this was proved to equal the rank of every matrix representing T (Theorem 9.1.5). This
definition of rank T is said to be intrinsic because it makes no reference to the matrices representing T .
However, the technique serves to identify an intrinsic property of T with every similarity invariant, and
some of these properties are not so easily defined directly.

    In particular, if T : V  V is a linear operator on a finite dimensional space V , define the determinant
of T (denoted det T ) by

                                       det T = det MB(T ), B any basis of V

This is independent of the choice of basis B because, if D is any other basis of V , the matrices MB(T ) and
MD(T ) are similar and so have the same determinant. In the same way, the trace of T (denoted tr T ) can
be defined by

                                         tr T = tr MB(T ), B any basis of V

This is unambiguous for the same reason.

    Theorems about matrices can often be translated to theorems about linear operators. Here is an exam-
ple.
     Change of Basis

Example 9.2.5
Let S and T denote linear operators on the finite dimensional space V . Show that

                                              det (ST ) = det S det T

Solution. Choose a basis B of V and use Theorem 9.2.1.

                    det (ST ) = det MB(ST ) = det [MB(S)MB(T )]
                                                  = det [MB(S)] det [MB(T )] = det S det T

    Recall next that the characteristic polynomial of a matrix is another similarity invariant: If A and A are
similar matrices, then cA(x) = cA(x) (Theorem 5.5.1). As discussed above, the discovery of a similarity
invariant means the discovery of a property of linear operators. In this case, if T : V  V is a linear
operator on the finite dimensional space V , define the characteristic polynomial of T by

                                 cT (x) = cA(x) where A = MB(T ), B any basis of V

In other words, the characteristic polynomial of an operator T is the characteristic polynomial of any
matrix representing T . This is unambiguous because any two such matrices are similar by Theorem 9.2.3.

Example 9.2.6
Compute the characteristic polynomial cT (x) of the operator T : P2  P2 given by
T (a + bx + cx2) = (b + c) + (a + c)x + (a + b)x2.

Solution. If B = {1, x, x2}, the corresponding matrix of T is

                       MB(T ) =  CB[T (1)] CB[T (x)] CB[T (x2)]                  0 1 1

                                                                              = 1 0 1 
                                                                                    110

Hence cT (x) = det [xI - MB(T )] = x3 - 3x - 2 = (x + 1)2(x - 2).

    In Section 4.4 we computed the matrix of various projections, reflections, and rotations in R3. How-
ever, the methods available then were not adequate to find the matrix of a rotation about a line through the
origin. We conclude this section with an example of how Theorem 9.2.3 can be used to compute such a
matrix.

Example 9.2.7

Let  L  be  the  line  in  R3  through  the  origin  with  (unit)  direction  vector  d  =  1  2 1 2 T.
                                                                                            3
Compute the matrix of the rotation about L through an angle  measured counterclockwise when

viewed in the direction of d.
                                                               . . Operators and Similarity

                           Solution. Let R : R3  R3 be the rotation. The idea is to first find

                L          a basis B0 for which the matrix of MB0(R) of R is easy to compute,
                           and then use Theorem 9.2.3 to compute the "standard" matrix
d = R(d)                   ME(R) with respect to the standard basis E = {e1, e2, e3} of R3.
              R(g)         To construct the basis B0, let K denote the plane through the

               g           origin with d as normal, shaded in the diagram. Then the vectors
       0
                           f= 1     -2    2  1  T  and g = 31  1 2 -2 T are both in K
               R(f)
               f                 3
                           (they are orthogonal to d) and are independent (they are orthogonal

                           to each other).
                           Hence B0 = {d, f, g} is an orthonormal basis of R3, and the

                           effect of R on B0 is easy to determine. In fact R(d) = d and
                           (as in Theorem 2.6.4) the second diagram gives

                                    R(f) = cos  f + sin  g and R(g) = - sin  f + cos  g

                           because f = 1 = g . Hence

R(g) g                                                         1 0                    0

                           MB0(R) = CB0(d) CB0(f) CB0(g) =  0 cos  - sin  
                     R(f)                                         0 sin  cos 

                     f     Now Theorem 9.2.3 (with B = E) asserts that

0                          ME (R) = P-1MB0(R)P where

                                                                2 1 2
                                                      = 1  -2
             P = PB0E =    CB0(e1) CB0(e2) CB0(e3)             2                   1
                                                          3
                                                               1 2 -2

using the expansion theorem (Theorem 5.3.6). Since P-1 = PT (P is orthogonal), the matrix of R
with respect to E is

ME(R) = PT MB0(R)P

                 5 cos  + 4               6 sin  - 2 cos  + 2  4 - 3 sin  - 4 cos  
                1                              8 cos  + 1      6 sin  - 2 cos  + 2 
             =       2 - 6 sin  - 2 cos 
                9                         2 - 6 sin  - 2 cos        5 cos  + 4
                     3 sin  - 4 cos  + 4

As a check one verifies that this is the identity matrix when  = 0, as it should.

    Note that in Example 9.2.7 not much motivation was given to the choices of the (orthonormal) vectors

f and g in the basis B0, which is the key to the solution. However, if we begin with any basis containing
d the Gram-Schmidt algorithm will produce an orthogonal basis containing d, and the other two vectors
will automatically be in L = K.
6 Change of Basis

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                            Engage Active Learning App!

                      Vretta-Lyryx Engage is an active learning app designed to increase
                     student engagement in reading linear algebra material. The content is
                   "chunked" into small blocks, each with an interactive assessment activity

                                              to promote comprehension.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

 . Invariant Subspaces and Direct Sums

A fundamental question in linear algebra is the following: If T : V  V is a linear operator, how can a
basis B of V be chosen so the matrix MB(T ) is as simple as possible? A basic technique for answering
such questions will be explained in this section. If U is a subspace of V , write its image under T as

V  T  V                      T (U ) = {T (u) | u in U }

UT U                  Definition 9.5 T -invariant Subspace
                      Let T : V  V be an operator. A subspace U  V is called
                      T -invariant if T (U )  U , that is, T (u)  U for every vector
                      u  U. Hence T is a linear operator on the vector space U.

                       This is illustrated in the diagram, and the fact that T : U  U is an op-
                   erator on U is the primary reason for our interest in T -invariant subspaces.

Example 9.3.1
Let T : V  V be any linear operator. Then:

   1. {0} and V are T -invariant subspaces.
                                                                  . . Invariant Subspaces and Direct Sums

   2. Both ker T and im T = T (V ) are T -invariant subspaces.
   3. If U and W are T -invariant subspaces, so are T (U ), U W , and U +W .

Solution. Item 1 is clear, and the rest is left as Exercises ?? and ??.

Example 9.3.2
Define T : R3  R3 by T (a, b, c) = (3a + 2b, b - c, 4a + 2b - c). Then
U = {(a, b, a) | a, b in R} is T -invariant because

                                    T (a, b, a) = (3a + 2b, b - a, 3a + 2b)
is in U for all a and b (the first and last entries are equal).

If a spanning set for a subspace U is known, it is easy to check whether U is T -invariant.

Example 9.3.3
Let T : V  V be a linear operator, and suppose that U = span {u1, u2, . . . , uk} is a subspace of
V . Show that U is T -invariant if and only if T (ui) lies in U for each i = 1, 2, . . . , k.
Solution. Given u in U , write it as u = r1u1 + · · · + rkuk, ri in R. Then

                                        T (u) = r1T (u1) + · · · + rkT (uk)
and this lies in U if each T (ui) lies in U . This shows that U is T -invariant if each T (ui) lies in U ;
the converse is clear.

Example 9.3.4
Define T : R2  R2 by T (a, b) = (b, -a). Show that R2 contains no T -invariant subspace except
0 and R2.
Solution. Suppose, if possible, that U is T -invariant, but U = 0, U = R2. Then U has dimension 1
so U = Rx where x = 0. Now T (x) lies in U --say T (x) = rx, r in R. If we write x = (a, b), this is
(b, -a) = r(a, b), which gives b = ra and -a = rb. Eliminating b gives r2a = rb = -a, so
(r2 + 1)a = 0. Hence a = 0. Then b = ra = 0 too, contrary to the assumption that x = 0. Hence no
one-dimensional T -invariant subspace exists.
8 Change of Basis

Definition 9.6 Restriction of an Operator
Let T : V  V be a linear operator. If U is any T -invariant subspace of V , then

                                                     T :U U
is a linear operator on the subspace U, called the restriction of T to U.

This is the reason for the importance of T -invariant subspaces and is the first step toward finding a basis
that simplifies the matrix of T .

   Theorem 9.3.1
   Let T : V  V be a linear operator where V has dimension n and suppose that U is any T -invariant
   subspace of V . Let B1 = {b1, . . . , bk} be any basis of U and extend it to a basis
   B = {b1, . . . , bk, bk+1, . . . , bn} of V in any way. Then MB(T ) has the block triangular form

                                                MB(T ) = MB1(T ) Y
                                                                  0Z

   where Z is (n - k) × (n - k) and MB1(T ) is the matrix of the restriction of T to U .

Proof. The matrix of (the restriction) T : U  U with respect to the basis B1 is the k × k matrix

                             MB1(T ) = CB1[T (b1)] CB1[T (b2)] · · · CB1[T (bk)]
Now compare the first column CB1[T (b1)] here with the first column CB[T (b1)] of MB(T ). The fact that
T (b1) lies in U (because U is T -invariant) means that T (b1) has the form

                                T (b1) = t1b1 + t2b2 + · · · + tkbk + 0bk+1 + · · · + 0bn
Consequently,

                                        t1 

                                        t2 
                      t1                 ..
                                             
                                             
                     t2                .
                           k           CB[T (b1)] =  tk  in Rn
                              whereas
CB1[T (b1)] =  ..  in R
                   .                    0 

                     tk                .
                                        .. 

                                         0

This shows that the matrices MB(T ) and MB1(T ) Y have identical first columns.
                                                        0Z

    Similar statements apply to columns 2, 3, . . . , k, and this proves the theorem.

    The block upper triangular form for the matrix MB(T ) in Theorem 9.3.1 is very useful because the
determinant of such a matrix equals the product of the determinants of each of the diagonal blocks. This
is recorded in Theorem 9.3.2 for reference, together with an important application to characteristic poly-
nomials.
                                                          . . Invariant Subspaces and Direct Sums

Theorem 9.3.2
Let A be a block upper triangular matrix, say

                   A11 A12 A13 · · · A1n                               

              0                                A22   A23  ···     A2n  
                                                                       
              A =  0                           0          ···
                    ..                           ..  ..A33        A3n..
                                                                       

                   ...                                            .    
                                                                       

                   0 0 0 · · · Ann

where the diagonal blocks are square. Then:

1. det A = ( det A11)( det A22)( det A33) · · · ( det Ann).
2. cA(x) = cA11 (x)cA22(x)cA33(x)· · ·cAnn(x).

Proof. If n = 2, (1) is Theorem 3.1.5; the general case (by induction on n) is left to the reader. Then (2)
follows from (1) because

              xI - A11 -A12 -A13 · · · -A1n                                    

            0           xI - A22 -A23 · · ·                              -A2n  
                                                                               
            
xI - A =  0                                    0 xI - A33 · · ·          -A3n
              ...                              ..             ..         ...   
                                                                               

                                                 .           .                 
                                                                               

              0                                0             0 · · · xI - Ann

where, in each diagonal block, the symbol I stands for the identity matrix of the appropriate size.

Example 9.3.5
Consider the linear operator T : P2  P2 given by

                   T (a + bx + cx2) = (-2a - b + 2c) + (a + b)x + (-6a - 2b + 5c)x2

Show that U = span {x, 1 + 2x2} is T -invariant, use it to find a block upper triangular matrix for T ,
and use that to compute cT (x).

Solution. U is T -invariant by Example 9.3.3 because U = span {x, 1 + 2x2} and both T (x) and
T (1 + 2x2) lie in U :

                   T (x) = -1 + x - 2x2 = x - (1 + 2x2)
            T (1 + 2x2) = 2 + x + 4x2 = x + 2(1 + 2x2)

Extend the basis B1 = {x, 1 + 2x2} of U to a basis B of P2 in any way at all--say,
B = {x, 1 + 2x2, x2}. Then

MB(T ) =    CB[T (x)] CB[T (1 + 2x2)] CB[T (x2)]
         =  CB(-1 + x - 2x2) CB(2 + x + 4x2) CB(2 + 5x2)
6 Change of Basis

                       1 1 0
                   =  -1 2 2 

                           001

is in block upper triangular form as expected. Finally,

                x-1     -1        0
                       x-2       -2  = (x2 - 3x + 3)(x - 1)
cT (x) = det  1                 x-1
                    0    0

Eigenvalues

Let T : V  V be a linear operator. A one-dimensional subspace Rv, v = 0, is T -invariant if and only if
T (rv) = rT (v) lies in Rv for all r in R. This holds if and only if T (v) lies in Rv; that is, T (v) =  v for
some  in R. A real number  is called an eigenvalue of an operator T : V  V if

                                                        T (v) =  v

holds for some nonzero vector v in V . In this case, v is called an eigenvector of T corresponding to  .
The subspace

                                            E (T ) = {v in V | T (v) =  v}
is called the eigenspace of T corresponding to  . These terms are consistent with those used in Section 5.5
for matrices. If A is an n × n matrix, a real number  is an eigenvalue of the matrix operator TA : Rn  Rn
if and only if  is an eigenvalue of the matrix A. Moreover, the eigenspaces agree:

                                       E (TA) = {x in Rn | Ax =  x} = E (A)

The following theorem reveals the connection between the eigenspaces of an operator T and those of the
matrices representing T .

   Theorem 9.3.3
   Let T : V  V be a linear operator where dim V = n, let B denote any ordered basis of V , and let
   CB : V  Rn denote the coordinate isomorphism. Then:

       1. The eigenvalues  of T are precisely the eigenvalues of the matrix MB(T ) and thus are the
           roots of the characteristic polynomial cT (x).

       2. In this case the eigenspaces E (T ) and E [MB(T )] are isomorphic via the restriction
           CB : E (T )  E [MB(T )].

Proof. Write A = MB(T ) for convenience. If T (v) =  v, then CB(v) = CB[T (v)] = ACB(v) because CB
is linear. Hence CB(v) lies in E (A), so we do have a function CB : E (T )  E (A). It is clearly linear
and one-to-one; we claim it is onto. If x is in E (A), write x = CB(v) for some v in V (CB is onto). This v
actually lies in E (T ). To see why, observe that

                               CB[T (v)] = ACB(v) = Ax =  x = CB(v) = CB( v)
                                                                      . . Invariant Subspaces and Direct Sums 6

Hence T (v) =  v because CB is one-to-one, and this proves (2). As to (1), we have already shown that
eigenvalues of T are eigenvalues of A. The converse follows, as in the foregoing proof that CB is onto.

Theorem 9.3.3 shows how to pass back and forth between the eigenvectors of an operator T and the
eigenvectors of any matrix MB(T ) of T :

                            v lies in E (T ) if and only if CB(v) lies in E [MB(T )]

Example 9.3.6
Find the eigenvalues and eigenspaces for T : P2  P2 given by

                      T (a + bx + cx2) = (2a + b + c) + (2a + b - 2c)x - (a + 2c)x2

Solution. If B = {1, x, x2}, then

MB(T ) =  CB[T (1)] CB[T (x)] CB[T (x2)]                                        2 1 1
                                                                            =  2 1 -2 

                                                                                  -1 0 -2

Hence cT (x) = det [xI - MB(T )] = (x + 1)2(x - 3) as the reader can verify.
          -1                                                                5
                                                                     

Moreover, E-1[MB(T )] = R  2  and E3[MB(T )] = R  6 , so Theorem 9.3.3 gives
                                   1                                    -1

E-1(T ) = R(-1 + 2x + x2) and E3(T ) = R(5 + 6x - x2).

Theorem 9.3.4
Each eigenspace of a linear operator T : V  V is a T -invariant subspace of V .

Proof. If v lies in the eigenspace E (T ), then T (v) =  v, so T [T (v)] = T ( v) =  T (v). This shows that
T (v) lies in E (T ) too.

Direct Sums

Sometimes vectors in a space V can be written naturally as a sum of vectors in two subspaces. For example,
in the space Mnn of all n × n matrices, we have subspaces

U = {P in Mnn | P is symmetric } and W = {Q in Mnn | Q is skew symmetric}

where a matrix Q is called skew-symmetric if QT = -Q. Then every matrix A in Mnn can be written as
the sum of a matrix in U and a matrix in W ; indeed,

                                      A  =  1 (A  +  AT  )  +  1 (A  -  AT  )

                                            2                  2
6 Change of Basis

where  1 (A  +     AT )  is       symmetric  and  1 (A  - AT )   is    skew     symmetric.      Remarkably, this representation is

       2                                          2
unique: If A = P + Q where PT = P and QT = -Q, then AT = PT + QT = P - Q; adding this to A = P + Q
             1                                                      1
gives  P  =  2  (  A  +  A  T  )  ,  and  subtracting  gives  Q  =  2  (  A  -  A  T  )  .  In  addition,  this  uniqueness  turns  out  to  be

closely related to the fact that the only matrix in both U and W is 0. This is a useful way to view matrices,

and the idea generalizes to the important notion of a direct sum of subspaces.

    If U and W are subspaces of V , their sum U + W and their intersection U  W were defined in Sec-
tion 6.4 as follows:

                                             U +W = {u + w | u in U and w in W }
                                             U W = {v | v lies in both U and W }

These are subspaces of V , the sum containing both U and W and the intersection contained in both U and
W . It turns out that the most interesting pairs U and W are those for which U W is as small as possible
and U +W is as large as possible.

Definition 9.7 Direct Sum of Subspaces
A vector space V is said to be the direct sum of subspaces U and W if

                                      U W = {0} and U +W = V

In this case we write V = U W . Given a subspace U, any subspace W such that V = U W is
called a complement of U in V .

Example 9.3.7

In the space R5, consider the subspaces U = {(a, b, c, 0, 0) | a, b, and c in R} and
W = {(0, 0, 0, d, e) | d and e in R}. Show that R5 = U W .

Solution. If x = (a, b, c, d, e) is any vector in R5, then x = (a, b, c, 0, 0) + (0, 0, 0, d, e), so x
lies in U +W . Hence R5 = U +W . To show that U W = {0}, let x = (a, b, c, d, e) lie in U W .
Then d = e = 0 because x lies in U , and a = b = c = 0 because x lies in W . Thus
x = (0, 0, 0, 0, 0) = 0, so 0 is the only vector in U W . Hence U W = {0}.

Example 9.3.8
If U is a subspace of Rn, show that Rn = U U .
Solution. The equation Rn = U +U  holds because, given x in Rn, the vector projU x lies in U
and x - projU x lies in U . To see that U U  = {0}, observe that any vector in U U  is
orthogonal to itself and hence must be zero.
                                                                      . . Invariant Subspaces and Direct Sums 6

   Example 9.3.9
   Let {e1, e2, . . . , en} be a basis of a vector space V , and partition it into two parts: {e1, . . . , ek} and
   {ek+1, . . . , en}. If U = span {e1, . . . , ek} and W = span {ek+1, . . . , en}, show that V = U W .
   Solution. If v lies in U W , then v = a1e1 + · · · + akek and v = bk+1ek+1 + · · · + bnen hold for
   some ai and b j in R. The fact that the ei are linearly independent forces all ai = b j = 0, so v = 0.
   Hence U W = {0}. Now, given v in V , write v = v1e1 + · · · + vnen where the vi are in R. Then
   v = u + w, where u = v1e1 + · · · + vkek lies in U and w = vk+1ek+1 + · · · + vnen lies in W . This
   proves that V = U +W .

    Example 9.3.9 is typical of all direct sum decompositions.

   Theorem 9.3.5
   Let U and W be subspaces of a finite dimensional vector space V . The following three conditions
   are equivalent:

       1. V = U W .
       2. Each vector v in V can be written uniquely in the form

                                                v = u + w u in U, w in W

       3. If {u1, . . . , uk} and {w1, . . . , wm} are bases of U and W , respectively, then
           B = {u1, . . . , uk, w1, . . . , wm} is a basis of V .

   (The uniqueness in (2) means that if v = u1 + w1 is another such representation, then u1 = u and
   w1 = w.)

Proof. Example 9.3.9 shows that (3)  (1).
    (1)  (2). Given v in V , we have v = u + w, u in U , w in W , because V = U +W .

If also v = u1 + w1, then u - u1 = w1 - w lies in U W = {0}, so u = u1 and w = w1.
    (2)  (3). Given v in V , we have v = u+w, u in U , w in W . Hence v lies in span B; that is, V = span B.

To see that B is independent, let a1u1 + · · · + akuk + b1w1 + · · · + bmwm = 0. Write u = a1u1 + · · · + akuk
and w = b1w1 + · · · + bmwm. Then u + w = 0, and so u = 0 and w = 0 by the uniqueness in (2). Hence
ai = 0 for all i and b j = 0 for all j.

    Condition (3) in Theorem 9.3.5 gives the following useful result.

   Theorem 9.3.6
   If a finite dimensional vector space V is the direct sum V = U W of subspaces U and W , then

                                                dim V = dim U + dim W

    These direct sum decompositions of V play an important role in any discussion of invariant subspaces.
6 Change of Basis

If T : V  V is a linear operator and if U1 is a T -invariant subspace, the block upper triangular matrix

                   MB(T ) = MB1(T ) Y                                                              (9.3)
                                     0Z

in Theorem 9.3.1 is achieved by choosing any basis B1 = {b1, . . . , bk} of U1 and completing it to a basis
B = {b1, . . . , bk, bk+1, . . . , bn} of V in any way at all. The fact that U1 is T -invariant ensures that the
first k columns of MB(T ) have the form in (9.3) (that is, the last n - k entries are zero), and the question
arises whether the additional basis vectors bk+1, . . . , bn can be chosen such that

                   U2 = span {bk+1, . . . , bn}

is also T -invariant. In other words, does each T -invariant subspace of V have a T -invariant complement?
Unfortunately the answer in general is no (see Example 9.3.11 below); but when it is possible, the matrix
MB(T ) simplifies further. The assumption that the complement U2 = span {bk+1, . . . , bn} is T -invariant
too means that Y = 0 in equation 9.3 above, and that Z = MB2(T ) is the matrix of the restriction of T to
U2 (where B2 = {bk+1, . . . , bn}). The verification is the same as in the proof of Theorem 9.3.1.

Theorem 9.3.7
Let T : V  V be a linear operator where V has dimension n. Suppose V = U1 U2 where both U1
and U2 are T -invariant. If B1 = {b1, . . . , bk} and B2 = {bk+1, . . . , bn} are bases of U1 and U2
respectively, then

                                       B = {b1, . . . , bk, bk+1, . . . , bn}

is a basis of V , and MB(T ) has the block diagonal form

                   MB(T ) =  MB1(T ) 0
                                0 MB2(T )

where MB1(T ) and MB2(T ) are the matrices of the restrictions of T to U1 and to U2 respectively.

   Definition 9.8 Reducible Linear Operator
   The linear operator T : V  V is said to be reducible if nonzero T -invariant subspaces U1 and U2
   can be found such that V = U1 U2.

    Then T has a matrix in block diagonal form as in Theorem 9.3.7, and the study of T is reduced to
studying its restrictions to the lower-dimensional spaces U1 and U2. If these can be determined, so can T .
Here is an example in which the action of T on the invariant subspaces U1 and U2 is very simple indeed.
The result for operators is used to derive the corresponding similarity theorem for matrices.

   Example 9.3.10
   Let T : V  V be a linear operator satisfying T 2 = 1V (such operators are called involutions).
   Define

                               U1 = {v | T (v) = v} and U2 = {v | T (v) = -v}
                                   . . Invariant Subspaces and Direct Sums 6

a. Show that V = U1 U2.

b. If dim V = n, find a basis B of V such that MB(T ) =  Ik 0     for some k.
                                                         0 -In-k

c. Conclude that, if A is an n × n matrix such that A2 = I, then A is similar to Ik 0 for
                                                                                                 0 -In-k

   some k.

Solution.

a. The verification that U1 and U2 are subspaces of V is left to the reader. If v lies in U1 U2,
   then v = T (v) = -v, and it follows that v = 0. Hence U1 U2 = {0}. Given v in V , write

                                        v = 12 {[v + T (v)] + [v - T (v)]}

   Then v + T (v) lies in U1, because T [v + T (v)] = T (v) + T 2(v) = v + T (v). Similarly,
   v - T (v) lies in U2, and it follows that V = U1 +U2. This proves part (a).

b. U1 and U2 are easily shown to be T -invariant, so the result follows from Theorem 9.3.7 if
   bases B1 = {b1, . . . , bk} and B2 = {bk+1, . . . , bn} of U1 and U2 can be found such that
   MB1(T ) = Ik and MB2(T ) = -In-k. But this is true for any choice of B1 and B2:

           MB1(T ) = CB1[T (b1)] CB1[T (b2)] · · · CB1[T (bk)]
                     = CB1(b1) CB1(b2) · · · CB1(bk)
                     = Ik

   A similar argument shows that MB2(T ) = -In-k, so part (b) follows with
   B = {b1, b2, . . . , bn}.
c. Given A such that A2 = I, consider TA : Rn  Rn. Then (TA)2(x) = A2x = x for all x in Rn,
   so (TA)2 = 1V . Hence, by part (b), there exists a basis B of Rn such that

                         MB(TA) =  Ir 0
                                   0 -In-r

But Theorem 9.2.4 shows that MB(TA) = P-1AP for some invertible matrix P, and this
proves part (c).

Note that the passage from the result for operators to the analogous result for matrices is routine and can
be carried out in any situation, as in the verification of part (c) of Example 9.3.10. The key is the analysis
of the operators. In this case, the involutions are just the operators satisfying T 2 = 1V , and the simplicity
of this condition means that the invariant subspaces U1 and U2 are easy to find.

    Unfortunately, not every linear operator T : V  V is reducible. In fact, the linear operator in Exam-
ple 9.3.4 has no invariant subspaces except 0 and V . On the other hand, one might expect that this is the
only type of nonreducible operator; that is, if the operator has an invariant subspace that is not 0 or V , then
some invariant complement must exist. The next example shows that even this is not valid.
66 Change of Basis

Example 9.3.11

Consider the operator T : R2  R2 given by T ab = a + b b . Show that U1 = R 10 is
T -invariant but that U1 has not T -invariant complement in R2.

Solution. Because U1 = span 10 and T 10 = 10 , it follows (by Example 9.3.3) that
U1 is T -invariant. Now assume, if possible, that U1 has a T -invariant complement U2 in R2. Then
U1 U2 = R2 and T (U2)  U2. Theorem 9.3.6 gives

                               2 = dim R2 = dim U1 + dim U2 = 1 + dim U2

so dim U2 = 1. Let U2 = Ru2, and write u2 = pq . We claim that u2 is not in U1. For if u2  U1,

then u2  U1 U2 = {0}, so u2 = 0. But then U2 = Ru2 = {0}, a contradiction, as dim U2 = 1. So

u2 / U1, from which q = 0. On the other hand, T (u2)  U2 = Ru2 (because U2 is T -invariant), say

T (u2) =  u2 =  pq .

Thus                  p + q = T p =  p where   R

                      q  q  q

Hence p + q =  p and q =  q. Because q = 0, the second of these equations implies that  = 1, so
the first equation implies q = 0, a contradiction. So a T -invariant complement of U1 does not exist.

    This is as far as we take the theory here, but in Chapter 11 the techniques introduced in this section will
be refined to show that every matrix is similar to a very nice matrix indeed--its Jordan canonical form.
                                                                      . . Invariant Subspaces and Direct Sums 6

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.
                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.
                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!
Chapter 10

            Inner Product Spaces

  . Inner Products and Norms

The dot product was introduced in Rn to provide a natural generalization of the geometrical notions of
length and orthogonality that were so important in Chapter 4. The plan in this chapter is to define an inner
product on an arbitrary real vector space V (of which the dot product is an example in Rn) and use it to
introduce these concepts in V . While this causes some repetition of arguments in Chapter 8, it is well
worth the effort because of the much wider scope of the results when stated in full generality.

   Definition 10.1 Inner Product Spaces
   An inner product on a real vector space V is a function that assigns a real number v, w to every
   pair v, w of vectors in V in such a way that the following axioms are satisfied.

 P1. v, w is a real number for all v and w in V .
 P2. v, w = w, v for all v and w in V .
 P3. v + w, u = v, u + w, u for all u, v, and w in V .
 P4. rv, w = r v, w for all v and w in V and all r in R.
 P5. v, v > 0 for all v = 0 in V .

A real vector space V with an inner product , will be called an inner product space. Note that every
subspace of an inner product space is again an inner product space using the same inner product.1

   Example 10.1.1
   Rn is an inner product space with the dot product as inner product:

                                            v, w = v · w for all v, w  Rn
   See Theorem 5.3.1. This is also called the euclidean inner product, and Rn, equipped with the dot
   product, is called euclidean n-space.

   Example 10.1.2
   If A and B are m × n matrices, define A, B = tr (ABT ) where tr (X ) is the trace of the square
   matrix X . Show that , is an inner product in Mmn.

    1If we regard Cn as a vector space over the field C of complex numbers, then the "standard inner product" on Cn defined in
Section 8.7 does not satisfy Axiom P4 (see Theorem 8.7.1(3)).

                                                             469
Inner Product Spaces

Solution. P1 is clear. Since tr (P) = tr (PT ) for every square matrix P, we have P2:

                            A, B = tr (ABT ) = tr [(ABT )T ] = tr (BAT ) = B, A

Next, P3 and P4 follow because trace is a linear transformation Mmn  R (Exercise ??). Turning
to P5, let r1, r2, . . . , rm denote the rows of the matrix A. Then the (i, j)-entry of AAT is ri · r j, so

                               A, A = tr (AAT ) = r1 · r1 + r2 · r2 + · · · + rm · rm

But r j · r j is the sum of the squares of the entries of r j, so this shows that A, A is the sum of the
squares of all nm entries of A. Axiom P5 follows.

The importance of the next example in analysis is difficult to overstate.

Example 10.1.3: 2

Let C[a, b] denote the vector space of continuous functions from [a, b] to R, a subspace of
F[a, b]. Show that

                                                                              b

                                              f , g = f (x)g(x)dx

                                                                             a

defines an inner product on C[a, b].

Solution. Axioms P1 and P2 are clear. As to axiom P4,

                         b     b
r f , g = r f (x)g(x)dx = r f (x)g(x)dx = r f , g

                      a     a

Axiom P3 is similar. Finally, theorems of calculus show that  f, f  =      b  f (x)2dx    0  and,  if  f  is
                                                                           a
continuous, that this is zero if and only if f is the zero function. This gives axiom P5.

    If v is any vector, then, using axiom P3, we get

                                          0, v = 0 + 0, v = 0, v + 0, v

and it follows that the number 0, v must be zero. This observation is recorded for reference in the
following theorem, along with several other properties of inner products. The other proofs are left as
Exercise ??.

   Theorem 10.1.1
   Let , be an inner product on a space V ; let v, u, and w denote vectors in V ; and let r denote a
   real number.

       1. u, v + w = u, v + u, w

    2This example (and others later that refer to it) can be omitted with no loss of continuity by students with no calculus
background.
                                                           . . Inner Products and Norms

2. v, rw = r v, w = rv, w
3. v, 0 = 0 = 0, v
4. v, v = 0 if and only if v = 0

If , is an inner product on a space V , then, given u, v, and w in V ,

ru + sv, w = ru, w + sv, w = r u, w + s v, w

for all r and s in R by axioms P3 and P4. Moreover, there is nothing special about the fact that there are
two terms in the linear combination or that it is in the first component:

  r1v1 + r2v2 + · · · + rnvn, w   = r1 v1, w + r2 v2, w + · · · + rn vn, w , and
v, s1w1 + s2w2 + · · · + smwm     = s1 v, w1 + s2 v, w2 + · · · + sm v, wm

hold for all ri and si in R and all v, w, vi, and w j in V . These results are described by saying that inner
products "preserve" linear combinations. For example,

2u - v, 3u + 2v = 2u, 3u + 2u, 2v + -v, 3u + -v, 2v
                     = 6 u, u + 4 u, v - 3 v, u - 2 v, v
                     = 6 u, u + u, v - 2 v, v

    If A is a symmetric n × n matrix and x and y are columns in Rn, we regard the 1 × 1 matrix xT Ay as a
number. If we write

                                      x, y = xT Ay for all columns x, y in Rn

then axioms P1-P4 follow from matrix arithmetic (only P2 requires that A is symmetric). Axiom P5 reads

                                       xT Ax > 0 for all columns x = 0 in Rn

and this condition characterizes the positive definite matrices (Theorem 8.3.2). This proves the first asser-
tion in the next theorem.

Theorem 10.1.2
If A is any n × n positive definite matrix, then

x, y = xT Ay for all columns x, y in Rn

defines an inner product on Rn, and every inner product on Rn arises in this way.

                                                                                                                                                                 n

Proof. Given an inner product , on Rn, let {e1, e2, . . . , en} be the standard basis of Rn. If x =  xiei

                                                                                                                                                               i=1
                n

and y =  y je j are two vectors in Rn, compute x, y by adding the inner product of each term xiei to

              j=1

each term y je j. The result is a double sum.

nn                                                nn

x, y =   xiei, y je j =   xi ei, e j y j
i=1 j=1                                           i=1 j=1
Inner Product Spaces

As the reader can verify, this is a matrix product:

                                                     e1, e2 · · · e1, en     
                                   e1, e1                                     y1

                                 e2, e1              e2, e2  ···  e2, en      y2  
                                                                                  
x, y = x1 x2 · · · xn  ..                            ...     ...  ...         .. 
                               .                                             . 

                                 en, e1              en, e2 · · · en, en      yn

Hence x, y = xT Ay, where A is the n × n matrix whose (i, j)-entry is ei, e j . The fact that

                               ei, e j = e j, ei

shows that A is symmetric. Finally, A is positive definite by Theorem 8.3.2.

Thus, just as every linear operator Rn  Rn corresponds to an n × n matrix, every inner product on Rn
corresponds to a positive definite n × n matrix. In particular, the dot product corresponds to the identity
matrix In.

Remark

If we refer to the inner product space Rn without specifying the inner product, we mean that the dot
product is to be used.

Example 10.1.4
Let the inner product , be defined on R2 by

                      v1 , w1    = 2v1w1 - v1w2 - v2w1 + v2w2
                      v2  w2

Find a symmetric 2 × 2 matrix A such that x, y = xT Ay for all x, y in R2.

Solution. The (i, j)-entry of the matrix A is the coefficient of viw j in the expression, so
A = 2 -1 -1 1 . Incidentally, if x = xy , then

                      x, x = 2x2 - 2xy + y2 = x2 + (x - y)2  0

for all x, so x, x = 0 implies x = 0. Hence , is indeed an inner product, so A is positive
definite.

    Let , be an inner product on Rn given as in Theorem 10.1.2 by a positive definite matrix A. If
x = x1 x2 · · · xn T , then x, x = xT Ax is an expression in the variables x1, x2, . . . , xn called a
quadratic form. These are studied in detail in Section 8.9.
                                                                                    . . Inner Products and Norms

Norm and Distance

   Definition 10.2 Norm and Distance
   As in Rn, if , is an inner product on a space V , the norm3 v of a vector v in V is defined by

                                                       v = v, v
   We define the distance between vectors v and w in an inner product space V to be

                                                   d (v, w) = v - w

Note that axiom P5 guarantees that v, v  0, so v is a real number.

Example 10.1.5             The norm of a continuous function f = f (x) in C[a, b]
                           (with the inner product from Example 10.1.3) is given by
          y

           y = f (x)2                                                b

                           f=                                          f (x)2dx

                                                                    a

           || f ||2        Hence f 2 is the area beneath the graph of y = f (x)2
                           between x = a and x = b (shaded in the diagram).
Oa                      x

                     b

Example 10.1.6
Show that u + v, u - v = u 2 - v 2 in any inner product space.

Solution.               u + v, u - v = u, u - u, v + v, u - v, v
                                        = u 2 - u, v + u, v - v 2
                                        = u 2- v 2

    A vector v in an inner product space V is called a unit vector if v = 1. The set of all unit vectors in
V is called the unit ball in V . For example, if V = R2 (with the dot product) and v = (x, y), then

                                         v 2 = 1 if and only if x2 + y2 = 1

Hence the unit ball in R2 is the unit circle x2 + y2 = 1 with centre at the origin and radius 1. However, the
shape of the unit ball varies with the choice of inner product.

3If the dot product is used in Rn, the norm x of a vector x is usually called the length of x.
Inner Product Spaces

Example 10.1.7

         y                  Let a > 0 and b > 0. If v = (x, y) and w = (x1, y1), define an
                            inner product on R2 by
            (0, b)
                                                                                   xx1 yy1
(-a, 0)     (a, 0)       x
                                                          v, w = a2 + b2
O                           The reader can verify (Exercise ??) that this is indeed an inner
                            product. In this case
            (0, -b)
                                              v 2 = 1 if and only if ax22 + by22 = 1
                            so the unit ball is the ellipse shown in the diagram.

Example 10.1.7 graphically illustrates the fact that norms and distances in an inner product space V vary
with the choice of inner product in V .

Theorem 10.1.3

If v = 0 is any vector in an inner product space V , then  1  v is the unique unit vector that is a
                                                           v
positive multiple of v.

    The next theorem reveals an important and useful fact about the relationship between norms and inner
products, extending the Cauchy inequality for Rn (Theorem 5.3.2).

   Theorem 10.1.4: Cauchy-Schwarz Inequality4
   If v and w are two vectors in an inner product space V , then

                                                   v, w 2  v 2 w 2

   Moreover, equality occurs if and only if one of v and w is a scalar multiple of the other.

Proof. Write v = a and w = b. Using Theorem 10.1.1 we compute:

                    bv - aw 2 = b2 v 2 - 2ab v, w + a2 w 2 = 2ab(ab - v, w )                         (10.1)
                    bv + aw 2 = b2 v 2 + 2ab v, w + a2 w 2 = 2ab(ab + v, w )

It follows that ab - v, w  0 and ab + v, w  0, and hence that -ab  v, w  ab. But then
| v, w |  ab = v w , as desired.

    Conversely, if | v, w | = v w = ab then v, w = ±ab. Hence (10.1) shows that bv - aw = 0 or
bv + aw = 0. It follows that one of v and w is a scalar multiple of the other, even if a = 0 or b = 0.

    4Hermann Amandus Schwarz (1843-1921) was a German mathematician at the University of Berlin. He had strong geo-
metric intuition, which he applied with great ingenuity to particular problems. A version of the inequality appeared in 1885.
                     . . Inner Products and Norms

Example 10.1.8
If f and g are continuous functions on the interval [a, b], then (see Example 10.1.3)

   b  2     b     b
      f (x)g(x)dx  f (x)2dx g(x)2dx

a        a     a

    Another famous inequality, the so-called triangle inequality, also comes from the Cauchy-Schwarz
inequality. It is included in the following list of basic properties of the norm of a vector.

   Theorem 10.1.5
   If V is an inner product space, the norm · has the following properties.

       1. v  0 for every vector v in V .
       2. v = 0 if and only if v = 0.
       3. rv = |r| v for every v in V and every r in R.
       4. v + w  v + w for all v and w in V (triangle inequality).

Proof. Because v = v, v , properties (1) and (2) follow immediately from (3) and (4) of Theo-
rem 10.1.1. As to (3), compute

                                          rv 2 = rv, rv = r2 v, v = r2 v 2

Hence (3) follows by taking positive square roots. Finally, the fact that v, w  v w by the Cauchy-
Schwarz inequality gives

                                v + w 2 = v + w, v + w = v 2 + 2 v, w + w 2
                                                                 v 2+2 v w + w 2
                                                                = ( v + w )2

Hence (4) follows by taking positive square roots.

It is worth noting that the usual triangle inequality for absolute values,

                                     |r + s|  |r| + |s| for all real numbers r and s

is a special case of (4) where V = R = R1 and the dot product r, s = rs is used.
    In many calculations in an inner product space, it is required to show that some vector v is zero. This

is often accomplished most easily by showing that its norm v is zero. Here is an example.

   Example 10.1.9
   Let {v1, . . . , vn} be a spanning set for an inner product space V . If v in V satisfies v, vi = 0 for
   each i = 1, 2, . . . , n, show that v = 0.
  6 Inner Product Spaces

   Solution. Write v = r1v1 + · · · + rnvn, ri in R. To show that v = 0, we show that v 2 = v, v = 0.
   Compute:

                          v, v = v, r1v1 + · · · + rnvn = r1 v, v1 + · · · + rn v, vn = 0
   by hypothesis, and the result follows.

    The norm properties in Theorem 10.1.5 translate to the following properties of distance familiar from
geometry. The proof is Exercise ??.

   Theorem 10.1.6
   Let V be an inner product space.

       1. d (v, w)  0 for all v, w in V .
       2. d (v, w) = 0 if and only if v = w.
       3. d (v, w) = d (w, v) for all v and w in V .
       4. d (v, w)  d (v, u) + d (u, w) for all v, u, and w in V .

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.
                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.
                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!
                                                                                  . . Orthogonal Sets of Vectors

  . Orthogonal Sets of Vectors

The idea that two lines can be perpendicular is fundamental in geometry, and this section is devoted to
introducing this notion into a general inner product space V . To motivate the definition, recall that two
nonzero geometric vectors x and y in Rn are perpendicular (or orthogonal) if and only if x · y = 0. In
general, two vectors v and w in an inner product space V are said to be orthogonal if

                                                          v, w = 0
A set {f1, f2, . . . , fn} of vectors is called an orthogonal set of vectors if

   1. Each fi = 0.
   2. fi, f j = 0 for all i = j.

If, in addition, fi = 1 for each i, the set {f1, f2, . . . , fn} is called an orthonormal set.

   Example 10.2.1
   {sin x, cos x} is orthogonal in C[-, ] because

                                                       1

                                            sin x cos x dx = -4 cos 2x - = 0

                                                     -

    The first result about orthogonal sets extends Pythagoras' theorem in Rn (Theorem 5.3.4) and the same
proof works.

   Theorem 10.2.1: Pythagoras' Theorem
   If {f1, f2, . . . , fn} is an orthogonal set of vectors, then

                                  f1 + f2 + · · · + fn 2 = f1 2 + f2 2 + · · · + fn 2

    The proof of the next result is left to the reader.

   Theorem 10.2.2
   Let {f1, f2, . . . , fn} be an orthogonal set of vectors.

       1. {r1f1, r2f2, . . . , rnfn} is also orthogonal for any ri = 0 in R.
       2. f11 f1, f12 f2, . . . , f1n fn is an orthonormal set.

As before, the process of passing from an orthogonal set to an orthonormal one is called normalizing the
orthogonal set. The proof of Theorem 5.3.5 goes through to give
8 Inner Product Spaces

 Theorem 10.2.3
 Every orthogonal set of vectors is linearly independent.

Example 10.2.2

             2   0   0  
Show that  -1  ,  1  ,  -1  is an orthogonal basis of R3 with inner product

0 1                          2

                                                               1 1 0
                             v, w = vT Aw, where A =  1 2 0 

                                                                  001

Solution. We have

 2 0                                 1 1 0  0                          0

 -1  ,  1  = 2 -1 0  1 2 0   1  = 1 0 0  1  = 0

0                     1             001 1                              1

and the reader can verify that the other pairs are orthogonal too. Hence the set is orthogonal, so it
is linearly independent by Theorem 10.2.3. Because dim R3 = 3, it is a basis.

The proof of Theorem 5.3.6 generalizes to give the following:

Theorem 10.2.4: Expansion Theorem
Let {f1, f2, . . . , fn} be an orthogonal basis of an inner product space V . If v is any vector in V , then

                             v, f1  v, f2                  v, fn
                             v = f 2 f1 + f 2 f2 + · · · + f 2 fn
                             1      2                          n

is the expansion of v as a linear combination of the basis vectors.

                v, f1 v, f2  v, fn
The coefficients f 2 , f 2 , . . . , f 2 in the expansion theorem are sometimes called the Fourier
                   1     2      n
coefficients of v with respect to the orthogonal basis {f1, f2, . . . , fn}. This is in honour of the French

mathematician J.B.J. Fourier (1768-1830). His original work was with a particular orthogonal set in the

space C[a, b], about which there will be more to say in Section 10.5.

Example 10.2.3
If a0, a1, . . . , an are distinct numbers and p(x) and q(x) are in Pn, define

                         p(x), q(x) = p(a0)q(a0) + p(a1)q(a1) + · · · + p(an)q(an)

This is an inner product on Pn. (Axioms P1-P4 are routinely verified, and P5 holds because 0 is
the only polynomial of degree n with n + 1 distinct roots. See Theorem 6.5.4 or Appendix D.)
Recall that the Lagrange polynomials 0(x), 1(x), . . . , n(x) relative to the numbers
                                                                              . . Orthogonal Sets of Vectors

a0, a1, . . . , an are defined as follows (see Section 6.5):
                                   k(x) = i=k(ak-ai) i=k(x-ai) k = 0, 1, 2, . . . , n

where i=k(x - ai) means the product of all the terms
                                   (x - a0), (x - a1), (x - a2), . . . , (x - an)

except that the kth term is omitted. Then {0(x), 1(x), . . . , n(x)} is orthonormal with respect to
 , because k(ai) = 0 if i = k and k(ak) = 1. These facts also show that p(x), k(x) = p(ak)

so the expansion theorem gives

                            p(x) = p(a0)0(x) + p(a1)1(x) + · · · + p(an)n(x)
for each p(x) in Pn. This is the Lagrange interpolation expansion of p(x), Theorem 6.5.3, which
is important in numerical integration.

Lemma 10.2.1: Orthogonal Lemma
Let {f1, f2, . . . , fm} be an orthogonal set of vectors in an inner product space V , and let v be any
vector not in span {f1, f2, . . . , fm}. Define

                                         v, f1         v, f2                    v, fm
     fm+1 = v - f 2 f1 - f 2 f2 - · · · - f 2 fm
                                                 1             2                m

Then {f1, f2, . . . , fm, fm+1} is an orthogonal set of vectors.

    The proof of this result (and the next) is the same as for the dot product in Rn (Lemma 8.1.1 and
Theorem 8.1.2).

Theorem 10.2.5: Gram-Schmidt Orthogonalization Algorithm
Let V be an inner product space and let {v1, v2, . . . , vn} be any basis of V . Define vectors
f1, f2, . . . , fn in V successively as follows:

f1 = v1                                  v2, f1  f1
f2 = v2 -                                 f1 2   f1 -

 f3 = v3 -                               v3, f1  f1 -  v3, f2     f2
 ..                                  ..   f1 2          f2 2      f2 - · · · -
.                                   .
                                         vk, f1        vk, f2
fk = vk -                                 f1 2          f2 2                    vk, fk-1  fk-1
                                                                                 fk-1 2

for each k = 2, 3, . . . , n. Then

1. {f1, f2, . . . , fn} is an orthogonal basis of V .
2. span {f1, f2, . . . , fk} = span {v1, v2, . . . , vk} holds for each k = 1, 2, . . . , n.
8 Inner Product Spaces

The purpose of the Gram-Schmidt algorithm is to convert a basis of an inner product space into an or-
thogonal basis. In particular, it shows that every finite dimensional inner product space has an orthogonal
basis.

Example 10.2.4

Consider V = P3 with the inner product                 p, q     =  1         p(x)q(x)dx.            If  the  Gram-Schmidt  algorithm
                                                                   -1
is applied to the basis {1, x, x2, x3}, show that the result is the orthogonal basis

                                        {1,  x,     1  (  3x2   -  1)  ,  1  (  5  x3  -  3  x)  }
                                                    3                     5

Solution. Take f1 = 1. Then the algorithm gives

                                                          x, f1                    0
                                           f2 = x - f 2 f1 = x - 2 f1 = x
                                                          1

                                                    2 x2, f1                    x2, f2
                                           f3 = x - f 2 f1 - f 2 f2
                                                                1                  2

                                                          2
                                             = x2 - 3 1 - 0 x
                                                          2        2
                                                                   3

                                             =   1 (3x2   - 1)

                                                 3

The  verification  that  f4  =  1 (5x3  -  3x)  is  omitted.

                                5

The polynomials in Example 10.2.4 are such that the leading coefficient is 1 in each case. In other contexts
(the study of differential equations, for example) it is customary to take multiples p(x) of these polynomials
such that p(1) = 1. The resulting orthogonal basis of P3 is

                                        {1,  x,     1  (  3x2   -  1)  ,  1  (  5  x3  -  3  x)  }
                                                    3                     5

and these are the first four Legendre polynomials, so called to honour the French mathematician A. M.
Legendre (1752-1833). They are important in the study of differential equations.

    If V is an inner product space of dimension n, let E = {f1, f2, . . . , fn} be an orthonormal basis of V
(by Theorem 10.2.5). If v = v1f1 + v2f2 + · · · + vnfn and w = w1f1 + w2f2 + · · · + wnfn are two vectors in
V , we have CE(v) = v1 v2 · · · vn T and CE(w) = w1 w2 · · · wn T . Hence

     v, w =  vifi,  w jf j =  viw j fi, f j =  viwi = CE(v) ·CE(w)
                                i       j                 i, j                            i

This shows that the coordinate isomorphism CE : V  Rn preserves inner products, and so proves

Corollary 10.2.1
If V is any n-dimensional inner product space, then V is isomorphic to Rn as inner product spaces.
More precisely, if E is any orthonormal basis of V , the coordinate isomorphism

                               CE : V  Rn satisfies v, w = CE(v) ·CE(w)

for all v and w in V .
                                                                                   . . Orthogonal Sets of Vectors 8

    The orthogonal complement of a subspace U of Rn was defined (in Chapter 8) to be the set of all vectors
in Rn that are orthogonal to every vector in U . This notion has a natural extension in an arbitrary inner
product space. Let U be a subspace of an inner product space V . As in Rn, the orthogonal complement
U  of U in V is defined by

                                     U  = {v | v  V , v, u = 0 for all u  U }

Theorem 10.2.6
Let U be a finite dimensional subspace of an inner product space V .

   1. U  is a subspace of V and V = U U .
   2. If dim V = n, then dim U + dim U  = n.
   3. If dim V = n, then U  = U .

Proof.

1. U  is a subspace by Theorem 10.1.1. If v is in U U , then v, v = 0, so v = 0 again by Theo-

rem 10.1.1. Hence U U  = {0}, and it remains to show that U +U  = V . Given v in V , we must

show that v is in U +U , and this is clear if v is in U . If v is not in U , let {f1, f2, . . . , fm} be an or-
                                                                      v, f1  v, f2  v, fm
thogonal basis of U . Then the orthogonal lemma shows that v- f 2 f1 + f 2 f2 + · · · + f 2 fm
                                                                      1      2      m

is in U , so v is in U +U  as required.

2. This follows from Theorem 9.3.6.

3. We have dim U  = n- dim U  = n-(n- dim U ) = dim U , using (2) twice. As U  U  always
   holds (verify), (3) follows by Theorem 6.4.2.

    We digress briefly and consider a subspace U of an arbitrary vector space V . As in Section 9.3, if W
is any complement of U in V , that is, V = U W , then each vector v in V has a unique representation as a
sum v = u + w where u is in U and w is in W . Hence we may define a function T : V  V as follows:

        T (v) = u where v = u + w, u in U , w in W

Thus, to compute T (v), express v in any way at all as the sum of a vector u in U and a vector in W ; then
T (v) = u.

    This function T is a linear operator on V . Indeed, if v1 = u1 + w1 where u1 is in U and w1 is in W ,
then v + v1 = (u + u1) + (w + w1) where u + u1 is in U and w + w1 is in W , so

        T (v + v1) = u + u1 = T (v) + T (v1)

Similarly, T (av) = aT (v) for all a in R, so T is a linear operator. Furthermore, im T = U and ker T = W
as the reader can verify, and T is called the projection on U with kernel W .

    If U is a subspace of V , there are many projections on U , one for each complementary subspace W
with V = U W . If V is an inner product space, we single out one for special attention. Let U be a finite
dimensional subspace of an inner product space V .
8 Inner Product Spaces

  Definition 10.3 Orthogonal Projection on a Subspace
  The projection on U with kernel U  is called the orthogonal projection on U (or simply the
  projection on U ) and is denoted projU : V  V .

Theorem 10.2.7: Projection Theorem
Let U be a finite dimensional subspace of an inner product space V and let v be a vector in V .

1. projU : V  V is a linear operator with image U and kernel U .
2. projU v is in U and v - projU v is in U .

3. If {f1, f2, . . . , fm} is any orthogonal basis of U , then

                               v, f1  v, f2                     v, fm
                               projU v = f 2 f1 + f 2 f2 + · · · + f 2 fm
                               1      2                         m

Proof. Only (3) remains to be proved. But since {f1, f2, . . . , fn} is an orthogonal basis of U and since
projU v is in U , the result follows from the expansion theorem (Theorem 10.2.4) applied to the finite
dimensional space U .

    Note that there is no requirement in Theorem 10.2.7 that V is finite dimensional.

Example 10.2.5

Let U be a subspace of the finite dimensional inner product space V . Show that
projU v = v - projU v for all v  V .

Solution. We have V = U  U  by Theorem 10.2.6. If we write p = projU v, then
v = (v - p) + p where v - p is in U  and p is in U = U  by Theorem 10.2.7. Hence
projU v = v - p. See Exercise ??.

            v                      The vectors v, projU v, and v- projU v in Theorem 10.2.7 can be visu-
                  v - projU v  alized geometrically as in the diagram (where U is shaded and dim U = 2).
                               This suggests that projU v is the vector in U closest to v. This is, in fact,
0                              the case.
       projU v

  U

Theorem 10.2.8: Approximation Theorem
Let U be a finite dimensional subspace of an inner product space V . If v is any vector in V , then
projU v is the vector in U that is closest to v. Here closest means that

                                             v - projU v < v - u

for all u in U , u = projU v.
                                                                                  . . Orthogonal Sets of Vectors 8

Proof. Write p = projU v, and consider v - u = (v - p) + (p - u). Because v - p is in U  and p - u is in
U , Pythagoras' theorem gives

                                      v-u 2 = v-p 2+ p-u 2 > v-p 2
because p - u = 0. The result follows.

Example 10.2.6

Consider the space C[-1, 1] of real-valued continuous functions on the interval [-1, 1] with inner
                   1
product  f, g   =  -1  f      (x)g(x)dx.   Find  the  polynomial  p  =  p(x)  of  degree  at  most  2  that  best

approximates the absolute-value function f given by f (x) = |x|.

convenien-c1e,       y        p(x)                    Solution. Here we want the vector p in the             basis
                              y = f (x)                                                                      for
                          y=                          subspace U = P2 of C[-1, 1] that is closest            is
                                       x              to f . In Example 10.2.4 the Gram-Schmidt
                weOhave       chan1ged f3
                                                      algorithm was applied to give an orthogonal
                                                      {f1 = 1, f2 = x, f3 = 3x2 - 1} of P2 (where,
                                           by a numerical factor). Hence the required polynomial

                                           p = projP2 f     f , f2      f , f3

                                                    f , f1     2 f2 +   f3 2 f3

                                             = f 2 f1 +     f2

                                                        1

                                              1             1/2
                                           = 2 f1 + 0f2 + 8/5 f3

                                           =  3 (5x2  + 1)

                                              16

The graphs of p(x) and f (x) are given in the diagram.

    If polynomials of degree at most n are allowed in Example 10.2.6, the polynomial in Pn is projPn f ,
and it is calculated in the same way. Because the subspaces Pn get larger as n increases, it turns out that the
approximating polynomials projPn f get closer and closer to f . In fact, solving many practical problems
comes down to approximating some interesting vector v (often a function) in an infinite dimensional inner
product space V by vectors in finite dimensional subspaces (which can be computed). If U1  U2 are finite
dimensional subspaces of V , then

                                             v - projU2 v  v - projU1 v

by Theorem 10.2.8 (because projU1 v lies in U1 and hence in U2). Thus projU2 v is a better approximation
to v than projU1 v. Hence a general method in approximation theory might be described as follows: Given
v, use it to construct a sequence of finite dimensional subspaces

                                                   U1  U2  U3  · · ·

of V in such a way that v - projUk v approaches zero as k increases. Then projUk v is a suitable ap-
proximation to v if k is large enough. For more information, the interested reader may wish to consult
Interpolation and Approximation by Philip J. Davis (New York: Blaisdell, 1963).
8 Inner Product Spaces

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

         Engage Active Learning App!

   Vretta-Lyryx Engage is an active learning app designed to increase
  student engagement in reading linear algebra material. The content is
"chunked" into small blocks, each with an interactive assessment activity

                           to promote comprehension.

 Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

  . Orthogonal Diagonalization

There is a natural way to define a symmetric linear operator T on a finite dimensional inner product
space V . If T is such an operator, it is shown in this section that V has an orthogonal basis consisting of
eigenvectors of T . This yields another proof of the principal axes theorem in the context of inner product
spaces.

   Theorem 10.3.1
   Let T : V  V be a linear operator on a finite dimensional space V . Then the following conditions
   are equivalent.

       1. V has a basis consisting of eigenvectors of T .

       2. There exists a basis B of V such that MB(T ) is diagonal.

Proof. We have MB(T ) = CB[T (b1)] CB[T (b2)] · · · CB[T (bn)] where B = {b1, b2, . . . , bn} is any
basis of V . By comparing columns:

                        1 0 · · · 0  

            0           2 · · ·  0
                         ..         
MB(T ) =  ..             .
            .                    ..  if and only if T (bi) = ibi for each i
                                 .

                        0 0 · · · n
                                                                                  . . Orthogonal Diagonalization 8

Theorem 10.3.1 follows.

   Definition 10.4 Diagonalizable Linear Operators
   A linear operator T on a finite dimensional space V is called diagonalizable if V has a basis
   consisting of eigenvectors of T .

Example 10.3.1
Let T : P2  P2 be given by

                               T (a + bx + cx2) = (a + 4c) - 2bx + (3a + 2c)x2

Find the eigenspaces of T and hence find a basis of eigenvectors.

Solution. If B0 = {1, x, x2}, then

                                                 1 0 4
                                    MB0(T ) =  0 -2 0 

                                                    3 02

socT (x) =(x + 2)2(x- 5), and the eigenvalues of T are  = -2 and  = 5. One sees that
0  4            1
 1  ,  0  ,  0  is a basis of eigenvectors of MB0(T ), so B = {x, 4 - 3x2, 1 + x2} is a
 0 -3 1 
basis of P2 consisting of eigenvectors of T .

    If V is an inner product space, the expansion theorem gives a simple formula for the matrix of a linear
operator with respect to an orthogonal basis.

Theorem 10.3.2

Let T : V  V be a linear operator on an inner product space V . If B = {b1, b2, . . . , bn} is an
orthogonal basis of V , then

                                    MB(T ) =   bi, T (b j)
                                                  bi 2

Proof. Write MB(T ) = ai j . The jth column of MB(T ) is CB[T (e j)], so
                                       T (b j) = a1 jb1 + · · · + ai jbi + · · · + an jbn

On the other hand, the expansion theorem (Theorem 10.2.4) gives
                                     v = b1 2 b1, v b1 + · · · + bi 2 bi, v bi + · · · + bn 2 bn, v bn

for any v in V . The result follows by taking v = T (b j).
86 Inner Product Spaces

Example 10.3.2
Let T : R3  R3 be given by

                          T (a, b, c) = (a + 2b - c, 2a + 3c, -a + 3b + 2c)

If the dot product in R3 is used, find the matrix of T with respect to the standard basis
B = {e1, e2, e3} where e1 = (1, 0, 0), e2 = (0, 1, 0), e3 = (0, 0, 1).

Solution. The basis B is orthonormal, so Theorem 10.3.2 gives

                            e1 · T (e1) e1 · T (e2) e1 · T (e3)       1 2 -1 
                                                                      2 0 3
MB(T ) =  e2 · T (e1) e2 · T (e2) e2 · T (e3)  = 

                            e3 · T (e1) e3 · T (e2) e3 · T (e3)       -1 3 2

Of course, this can also be found in the usual way.

    It is not difficult to verify that an n × n matrix A is symmetric if and only if x · (Ay) = (Ax) · y holds for
all columns x and y in Rn. The analog for operators is as follows:

   Theorem 10.3.3
   Let V be a finite dimensional inner product space. The following conditions are equivalent for a
   linear operator T : V  V .

       1. v, T (w) = T (v), w for all v and w in V .
       2. The matrix of T is symmetric with respect to every orthonormal basis of V .

       3. The matrix of T is symmetric with respect to some orthonormal basis of V .

       4. There is an orthonormal basis B = {f1, f2, . . . , fn} of V such that fi, T (f j) = T (fi), f j
           holds for all i and j.

Proof. (1)  (2). Let B = {f1, . . . , fn} be an orthonormal basis of V , and write MB(T ) = ai j . Then
ai j = fi, T (f j) by Theorem 10.3.2. Hence (1) and axiom P2 give

                          ai j = fi, T (f j) = T (fi), f j = f j, T (fi) = a ji

for all i and j. This shows that MB(T ) is symmetric.

(2)  (3). This is clear.

    (3)  (4). Let B = {f1, . . . , fn} be an orthonormal basis of V such that MB(T ) is symmetric. By (3)
and Theorem 10.3.2, fi, T (f j) = f j, T (fi) for all i and j, so (4) follows from axiom P2.

                                                                 n               n

(4)  (1). Let v and w be vectors in V and write them as v =  vifi and w =  w jf j. Then
                                                                 i=1             j=1

v, T (w) =  vifi,  w jT f j =   viw j fi, T (fj)
                            i  j                       ij
           . . Orthogonal Diagonalization 8

        =   viw j T (fi), f j

             ij

        =  viT (fi),  w jf j
        i           j

        = T (v), w

where we used (4) at the third stage. This proves (1).

A linear operator T on an inner product space V is called symmetric if v, T (w) = T (v), w holds for
all v and w in V .

Example 10.3.3

If A is an n × n matrix, let TA : Rn  Rn be the matrix operator given by TA(v) = Av for all
columns v. If the dot product is used in Rn, then TA is a symmetric operator if and only if A is a
symmetric matrix.

Solution. If E is the standard basis of Rn, then E is orthonormal when the dot product is used. We
have ME(TA) = A (by Example 9.1.4), so the result follows immediately from part (3) of
Theorem 10.3.3.

    It is important to note that whether an operator is symmetric depends on which inner product is being
used (see Exercise ??).

    If V is a finite dimensional inner product space, the eigenvalues of an operator T : V  V are the
same as those of MB(T ) for any orthonormal basis B (see Theorem 9.3.3). If T is symmetric, MB(T ) is a
symmetric matrix and so has real eigenvalues by Theorem 5.5.7. Hence we have the following:

   Theorem 10.3.4
   A symmetric linear operator on a finite dimensional inner product space has real eigenvalues.

    If U is a subspace of an inner product space V , recall that its orthogonal complement is the subspace

U  of V defined by
                                      U  = {v in V | v, u = 0 for all u in U }

Theorem 10.3.5
Let T : V  V be a symmetric linear operator on an inner product space V , and let U be a
T -invariant subspace of V . Then:

   1. The restriction of T to U is a symmetric linear operator on U.

   2. U  is also T -invariant.

Proof.
 88 Inner Product Spaces

   1. U is itself an inner product space using the same inner product, and condition 1 in Theorem 10.3.3
       that T is symmetric is clearly preserved.

   2. If v is in U , our task is to show that T (v) is also in U ; that is, T (v), u = 0 for all u in U . But if
       u is in U , then T (u) also lies in U because U is T -invariant, so

                                                       T (v), u = v, T (u)

       using the symmetry of T and the definition of U .

    The principal axes theorem (Theorem 8.2.2) asserts that an n × n matrix A is symmetric if and only if
Rn has an orthogonal basis of eigenvectors of A. The following result not only extends this theorem to an
arbitrary n-dimensional inner product space, but the proof is much more intuitive.

    Theorem 10.3.6: Principal Axes Theorem
    The following conditions are equivalent for a linear operator T on a finite dimensional inner
    product space V .

       1. T is symmetric.

       2. V has an orthogonal basis consisting of eigenvectors of T .

Proof. (1)  (2). Assume that T is symmetric and proceed by induction on n = dim V . If n = 1, every
nonzero vector in V is an eigenvector of T , so there is nothing to prove. If n  2, assume inductively
that the theorem holds for spaces of dimension less than n. Let 1 be a real eigenvalue of T (by Theo-
rem 10.3.4) and choose an eigenvector f1 corresponding to 1. Then U = Rf1 is T -invariant, so U  is
also T -invariant by Theorem 10.3.5 (T is symmetric). Because dim U  = n - 1 (Theorem 10.2.6), and
because the restriction of T to U  is a symmetric operator (Theorem 10.3.5), it follows by induction that
U  has an orthogonal basis {f2, . . . , fn} of eigenvectors of T . Hence B = {f1, f2, . . . , fn} is an orthogonal
basis of V , which proves (2).

    (2)  (1). If B = {f1, . . . , fn} is a basis as in (2), then MB(T ) is symmetric (indeed diagonal), so T is
symmetric by Theorem 10.3.3.

    The matrix version of the principal axes theorem is an immediate consequence of Theorem 10.3.6. If A
is an n × n symmetric matrix, then TA : Rn  Rn is a symmetric operator, so let B be an orthonormal basis
of Rn consisting of eigenvectors of TA (and hence of A). Then PT AP is diagonal where P is the orthogonal
matrix whose columns are the vectors in B (see Theorem 9.2.4).

    Similarly, let T : V  V be a symmetric linear operator on the n-dimensional inner product space V
and let B0 be any convenient orthonormal basis of V . Then an orthonormal basis of eigenvectors of T can
be computed from MB0(T ). In fact, if PT MB0(T )P is diagonal where P is orthogonal, let B = {f1, . . . , fn}
be the vectors in V such that CB0(fj) is column j of P for each j. Then B consists of eigenvectors of T by
Theorem 9.3.3, and they are orthonormal because B0 is orthonormal. Indeed

                                                 fi, f j = CB0(fi) · CB0(f j)
holds for all i and j, as the reader can verify. Here is an example.
                                                               . . Orthogonal Diagonalization 8

Example 10.3.4
Let T : P2  P2 be given by

T (a + bx + cx2) = (8a - 2b + 2c) + (-2a + 5b + 4c)x + (2a + 4b + 5c)x2

Using the inner product a + bx + cx2, a + bx + cx2 = aa + bb + cc, show that T is symmetric
and find an orthonormal basis of P2 consisting of eigenvectors.

                                                       8 -2 2 
Solution. If B0 = {1, x, x2}, then MB0(T ) =  -2 5 4  is symmetric, so T is symmetric.

                                                            2 45

This matrix was analyzed in Example 8.2.5, where it was found that an orthonormal basis of

eigenvectors is  1                     T1                  T1  -2 2 1 T . Because B0 is
                 3
                    1 2 -2 , 3             2 1 2 ,3

orthonormal, the corresponding orthonormal basis of P2 is

                    B = 13 (1 + 2x - 2x2), 13 (2 + x + 2x2), 13 (-2 + 2x + x2)

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!
Inner Product Spaces

  . Isometries

We saw in Section 2.6 that rotations about the origin and reflections in a line through the origin are linear
operators on R2. Similar geometric arguments (in Section 4.4) establish that, in R3, rotations about a line
through the origin and reflections in a plane through the origin are linear. We are going to give an algebraic
proof of these results that is valid in any inner product space. The key observation is that reflections and
rotations are distance preserving in the following sense. If V is an inner product space, a transformation
S : V  V (not necessarily linear) is said to be distance preserving if the distance between S(v) and S(w)
is the same as the distance between v and w for all vectors v and w; more formally, if

                      S(v) - S(w) = v - w for all v and w in V                      (10.2)

Distance-preserving maps need not be linear. For example, if u is any vector in V , the transformation

Su : V  V defined by Su(v) = v + u for all v in V is called translation by u, and it is routine to verify that
Su is distance preserving for any u. However, Su is linear only if u = 0 (since then Su(0) = 0). Remarkably,
distance-preserving operators that do fix the origin are necessarily linear.

Lemma 10.4.1

Let V be an inner product space of dimension n, and consider a distance-preserving transformation
S : V  V . If S(0) = 0, then S is linear.

Proof. We have S(v) - S(w) 2 = v - w 2 for all v and w in V by (10.2), which gives

                      S(v), S(w) = v, w for all v and w in V                        (10.3)

Now let {f1, f2, . . . , fn} be an orthonormal basis of V . Then {S(f1), S(f2), . . . , S(fn)} is orthonormal by
(10.3) and so is a basis because dim V = n. Now compute:

S(v + w) - S(v) - S(w), S(fi) = S(v + w), S(fi) - S(v), S(fi) - S(w), S(fi)
                                      = v + w, fi - v, fi - w, fi
                                      =0

for each i. It follows from the expansion theorem (Theorem 10.2.4) that S(v + w) - S(v) - S(w) = 0; that
is, S(v + w) = S(v) + S(w). A similar argument shows that S(av) = aS(v) holds for all a in R and v in V ,
so S is linear after all.

Definition 10.5 Isometries
Distance-preserving linear operators are called isometries.

    It is routine to verify that the composite of two distance-preserving transformations is again distance
preserving. In particular the composite of a translation and an isometry is distance preserving. Surpris-
ingly, the converse is true.
                                                                                   . . Isometries

Theorem 10.4.1

If V is a finite dimensional inner product space, then every distance-preserving transformation
S : V  V is the composite of a translation and an isometry.

Proof. If S : V  V is distance preserving, write S(0) = u and define T : V  V by T (v) = S(v) - u for
all v in V . Then T (v) - T (w) = v - w for all vectors v and w in V as the reader can verify; that is, T
is distance preserving. Clearly, T (0) = 0, so it is an isometry by Lemma 10.4.1. Since

                                   S(v) = u + T (v) = (Su  T )(v) for all v in V

we have S = Su  T , and the theorem is proved.

In Theorem 10.4.1, S = Su  T factors as the composite of an isometry T followed by a translation Su. More
is true: this factorization is unique in that u and T are uniquely determined by S; and w  V exists such
that S = T  Sw is uniquely the composite of translation by w followed by the same isometry T (Exercise
??).

    Theorem 10.4.1 focuses our attention on the isometries, and the next theorem shows that, while they
preserve distance, they are characterized as those operators that preserve other properties.

Theorem 10.4.2

Let T : V  V be a linear operator on a finite dimensional inner product space V .

The following conditions are equivalent:

1. T is an isometry.                                                          (T preserves distance)

2. T (v) = v for all v in V .                                                 (T preserves norms)

3. T (v), T (w) = v, w for all v and w in V .                                 (T preserves inner products)

4. If {f1, f2, . . . , fn} is an orthonormal basis of V ,

then {T (f1), T (f2), . . . , T (fn)} is also an orthonormal basis. (T preserves orthonormal bases)

5. T carries some orthonormal basis to an orthonormal basis.

Proof. (1)  (2). Take w = 0 in (10.2).
    (2)  (3). Since T is linear, (2) gives T (v) - T (w) 2 = T (v - w) 2 = v - w 2. Now (3) follows.
    (3)  (4). By (3), {T (f1), T (f2), . . . , T (fn)} is orthogonal and T (fi) 2 = fi 2 = 1. Hence it is a

basis because dim V = n.

(4)  (5). This needs no proof.

(5)  (1). By (5), let {f1, . . . , fn} be an orthonormal basis of V such that{T (f1), . . . , T (fn)} is also

orthonormal. Given v = v1f1 + · · · + vnfn in V , we have T (v) = v1T (f1) + · · · + vnT (fn) so Pythagoras'
theorem gives
                                          2     v2                 v2     v2
                                T (v)        =      +  ·  ·  ·  +      =
                                                 1                  n

Hence T (v) = v for all v, and (1) follows by replacing v by v - w.

Before giving examples, we note some consequences of Theorem 10.4.2.
         Inner Product Spaces

   Corollary 10.4.1
   Let V be a finite dimensional inner product space.

       1. Every isometry of V is an isomorphism.5
       2. a. 1V : V  V is an isometry.

              b. The composite of two isometries of V is an isometry.
              c. The inverse of an isometry of V is an isometry.

Proof. (1) is by (4) of Theorem 10.4.2 and Theorem 7.3.1. (2a) is clear, and (2b) is left to the reader. If
T : V  V is an isometry and {f1, . . . , fn} is an orthonormal basis of V , then (2c) follows because T -1
carries the orthonormal basis {T (f1), . . . , T (fn)} back to {f1, . . . , fn}.

    The conditions in part (2) of the corollary assert that the set of isometries of a finite dimensional inner
product space forms an algebraic system called a group. The theory of groups is well developed, and
groups of operators are important in geometry. In fact, geometry itself can be fruitfully viewed as the
study of those properties of a vector space that are preserved by a group of invertible linear operators.

   Example 10.4.1
   Rotations of R2 about the origin are isometries, as are reflections in lines through the origin: They
   clearly preserve distance and so are linear by Lemma 10.4.1. Similarly, rotations about lines
   through the origin and reflections in planes through the origin are isometries of R3.

   Example 10.4.2
   Let T : Mnn  Mnn be the transposition operator: T (A) = AT . Then T is an isometry if the inner

  product is A, B = tr (ABT ) =  ai jbi j. In fact, T permutes the basis consisting of all matrices

                                                        i, j

   with one entry 1 and the other entries 0.

    The proof of the next result requires the fact (see Theorem 10.4.2) that, if B is an orthonormal basis,
then v, w = CB(v) ·CB(w) for all vectors v and w.

   Theorem 10.4.3
   Let T : V  V be an operator where V is a finite dimensional inner product space. The following
   conditions are equivalent.

       1. T is an isometry.
       2. MB(T ) is an orthogonal matrix for every orthonormal basis B.
       3. MB(T ) is an orthogonal matrix for some orthonormal basis B.

    5V must be finite dimensional--see Exercise ??.
                                                                                                      . . Isometries

Proof. (1)  (2). Let B = {e1, . . . , en} be an orthonormal basis. Then the jth column of MB(T ) is
CB[T (e j)], and we have

                                 CB[T (e j)] ·CB[T (ek)] = T (e j), T (ek) = e j, ek
using (1). Hence the columns of MB(T ) are orthonormal in Rn, which proves (2).

    (2)  (3). This is clear.
    (3)  (1). Let B = {e1, . . . , en} be as in (3). Then, as before,

                                         T (e j), T (ek) = CB[T (e j)] ·CB[T (ek)]
so {T (e1), . . . , T (en)} is orthonormal by (3). Hence Theorem 10.4.2 gives (1).
It is important that B is orthonormal in Theorem 10.4.3. For example, T : V  V given by T (v) = 2v
preserves orthogonal sets but is not an isometry, as is easily checked.

    If P is an orthogonal square matrix, then P-1 = PT . Taking determinants yields ( det P)2 = 1, so
det P = ±1. Hence:

    Corollary 10.4.2
    If T : V  V is an isometry where V is a finite dimensional inner product space, then det T = ±1.

    Example 10.4.3
    If A is any n × n matrix, the matrix operator TA : Rn  Rn is an isometry if and only if A is
    orthogonal using the dot product in Rn. Indeed, if E is the standard basis of Rn, then ME(TA) = A
    by Theorem 9.2.4.

    Rotations and reflections that fix the origin are isometries in R2 and R3 (Example 10.4.1); we are going
to show that these isometries (and compositions of them in R3) are the only possibilities. In fact, this will
follow from a general structure theorem for isometries. Surprisingly enough, much of the work involves
the two-dimensional case.

    Theorem 10.4.4
    Let T : V  V be an isometry on the two-dimensional inner product space V . Then there are two
    possibilities.
    Either (1) There is an orthonormal basis B of V such that

                                     MB(T ) = cos  - sin  sin  cos  , 0   < 2
    or (2) There is an orthonormal basis B of V such that

                                                   MB(T ) = 1 0 0 -1
    Furthermore, type (1) occurs if and only if det T = 1, and type (2) occurs if and only if
    det T = -1.
Inner Product Spaces

Proof. The final statement follows from the rest because det T = det [MB(T )] for any basis B. Let
B0 = {e1, e2} be any ordered orthonormal basis of V and write

A = MB0(T ) = a b c d ; that is, T (e1) = ae1 + ce2 T (e2) = be1 + de2

Then A is orthogonal by Theorem 10.4.3, so its columns (and rows) are orthonormal. Hence

                          a2 + c2 = 1 = b2 + d2

so (a, c) and (d, b) lie on the unit circle. Thus angles  and  exist such that

                      a = cos  , c = sin  0   < 2
                      d = cos , b = sin  0   < 2

Then sin( + ) = cd + ab = 0 because the columns of A are orthogonal, so  +  = k for some integer
k. This gives d = cos(k -  ) = (-1)k cos  and b = sin(k -  ) = (-1)k+1 sin  . Finally

                      A=  cos  (-1)k+1 sin 

                          ksin  (-1) cos 

If k is even we are in type (1) with B = B0, so assume k is odd. Then A = a c c -a . If a = -1 and c = 0,
we are in type (1) with B = {e2, e2}. Otherwise A has eigenvalues 1 = 1 and 2 = -1 with corresponding
eigenvectors x1 = 1 + a c and x2 = -c 1 + a as the reader can verify. Write

                                f1 = (1 + a)e1 + ce2 and f2 = -ce2 + (1 + a)e2

Then f1 and f2 are orthogonal (verify) and CB0(fi) = CB0(ifi) = xi for each i. Moreover

                           CB0[T (fi)] = ACB0(fi) = Axi = ixi = iCB0(fi) = CB0(ifi)
so T (fi) = ifi for each i. Hence MB(T ) = 1 0 0 2 = 1 0 0 -1 and we are in type (2) with
B = f11 f1, f12 f2 .

Corollary 10.4.3
An operator T : R2  R2 is an isometry if and only if T is a rotation or a reflection.

In fact, if E is the standard basis of R2, then the clockwise rotation R about the origin through an angle
 has matrix

                                            ME(R ) = cos  - sin  sin  cos 

(see Theorem 2.6.4). On the other hand, if S : R2  R2 is the reflection in a line through the origin (called
the fixed line of the reflection), let f1 be a unit vector pointing along the fixed line and let f2 be a unit vector
perpendicular to the fixed line. Then B = {f1, f2} is an orthonormal basis, S(f1) = f1 and S(f2) = -f2, so

                                                   MB(S) = 1 0 0 -1
                                                                                           . . Isometries

Thus S is of type 2. Note that, in this case, 1 is an eigenvalue of S, and any eigenvector corresponding to
1 is a direction vector for the fixed line.

Example 10.4.4

In each case, determine whether TA : R2  R2 is a rotation or a reflection, and then find the angle

or fixed line:                              1 3                      1 -3 4
                                        -3 1               (b) A = 5
                         (a)   A  =  1
                                     2                                      43

Solution. Both matrices are orthogonal, so (because ME(TA) = A, where E is the standard basis)

TA is an isometry in both cases. In the firstcase, det A = 1, so TA is a counterclockwise rotation
through  ,  where  cos      1  and   sin         3.  Thus       - .   In  (b),  det        -1,  so      is  a
                         =  2              =  -              =                       A  =           TA
                                                 2                 3
reflection in this case. We verify that d = 12 is an eigenvector corresponding to the eigenvalue

1. Hence the fixed line Rd has equation y = 2x.

    We now give a structure theorem for isometries. The proof requires three preliminary results, each of
interest in its own right.

   Lemma 10.4.2
   Let T : V  V be an isometry of a finite dimensional inner product space V . If U is a T -invariant
   subspace of V , then U  is also T -invariant.

Proof. Let w lie in U . We are to prove that T (w) is also in U ; that is, T (w), u = 0 for all u in U . At
this point, observe that the restriction of T to U is an isometry U  U and so is an isomorphism by the
corollary to Theorem 10.4.2. In particular, each u in U can be written in the form u = T (u1) for some u1
in U, so

                                      T (w), u = T (w), T (u1) = w, u1 = 0

because w is in U . This is what we wanted.

    To employ Lemma 10.4.2 above to analyze an isometry T : V  V when dim V = n, it is necessary to
show that a T -invariant subspace U exists such that U = 0 and U = V . We will show, in fact, that such a
subspace U can always be found of dimension 1 or 2. If T has a real eigenvalue  then Ru is T -invariant
where u is any  -eigenvector. But, in case (1) of Theorem 10.4.4, the eigenvalues of T are ei and e-i
(the reader should check this), and these are nonreal if  = 0 and  = . It turns out that every complex
eigenvalue  of T has absolute value 1 (Lemma 10.4.3 below); and that U has a T -invariant subspace of
dimension 2 if  is not real (Lemma 10.4.4).

   Lemma 10.4.3

   Let T : V  V be an isometry of the finite dimensional inner product space V . If  is a complex
   eigenvalue of T , then | | = 1.
   6 Inner Product Spaces

Proof. Choose an orthonormal basis B of V , and let A = MB(T ). Then A is a real orthogonal matrix so,
using the standard inner product x, y = xT y in C, we get

                                    Ax 2 = (Ax)T (Ax) = xT AT Ax = xT Ix = x 2
for all x in Cn. But Ax =  x for some x = 0, whence x 2 =  x 2 = | |2 x 2. This gives | | = 1, as
required.

   Lemma 10.4.4
   Let T : V  V be an isometry of the n-dimensional inner product space V . If T has a nonreal
   eigenvalue, then V has a two-dimensional T -invariant subspace.

Proof. Let B be an orthonormal basis of V , let A = MB(T ), and (using Lemma 10.4.3) let  = ei be a
nonreal eigenvalue of A, say Ax =  x where x = 0 in Cn. Because A is real, complex conjugation gives
Ax =  x, so  is also an eigenvalue. Moreover  =  ( is nonreal), so {x, x} is linearly independent in
Cn (the argument in the proof of Theorem 5.5.4 works). Now define

                                           z1 = x + x and z2 = i(x - x)

Then z1 and z2 lie in Rn, and {z1, z2} is linearly independent over R because {x, x} is linearly independent
over C. Moreover

                                       x = 12 (z1 - iz2) and x = 12 (z1 + iz2)
Now  +  = 2 cos  and  -  = 2i sin , and a routine computation gives

                                               Az1 = z1 cos  + z2 sin 
                                               Az2 = -z1 sin  + z2 cos 

Finally, let e1 and e2 in V be such that z1 = CB(e1) and z2 = CB(e2). Then

                              CB[T (e1)] = ACB(e1) = Az1 = CB(e1 cos  + e2 sin )

using Theorem 9.1.2. Because CB is one-to-one, this gives the first of the following equations (the other is
similar):

                                             T (e1) = e1 cos  + e2 sin 
                                             T (e2) = -e1 sin  + e2 cos 

Thus U = span {e1, e2} is T -invariant and two-dimensional.
    We can now prove the structure theorem for isometries.

   Theorem 10.4.5
   Let T : V  V be an isometry of the n-dimensional inner product space V . Given an angle  , write
   R( ) = cos  - sin  sin  cos  . Then there exists an orthonormal basis B of V such that MB(T ) has
                                                                                                    . . Isometries

one of the following block diagonal forms, classified for convenience by whether n is odd or even:

             1 0 ··· 0                                         -1 0 · · · 0 

            0                R(1)  ···     0                  0           R(1) · · ·           0
                                                                                                    
n = 2k + 1     ...           ...   ...     ..  or  ..                          ... . . .       ...
                                           .                  .                                     
                                                                                                    

               0 0 · · · R(k)                                   0 0 · · · R(k)

            R(1) 0 · · · 0                                     -1 0 0 · · · 0 

0                            R(2) · · ·         0              0 1 0 ···                            0
                                                                                                          
                                                                  0       0 R(1) · · ·              0
n = 2k  .                         ..            .         or                                              
 ..                               .. . . .. 
                                                                     ...  ...    ...      ...       ...

            0                0 · · · R(k)                                                                
                                                                                                         

                                                                     0 0 0 · · · R(k-1)

Proof. We show first, by induction on n, that an orthonormal basis B of V can be found such that MB(T )
is a block diagonal matrix of the following form:

                                           Ir 0 0 · · · 0                      

                                         0         -Is    0   ···         0
                                         
                                                                               
                             MB(T ) =  0 0 R(1) · · ·                     0
                                           ...     ...    ... . . .       ...  

                                                                               
                                                                               

                                           0 0 0 · · · R(t)

where the identity matrix Ir, the matrix -Is, or the matrices R(i) may be missing. If n = 1 and V = Rv,
this holds because T (v) =  v and  = ±1 by Lemma 10.4.3. If n = 2, this follows from Theorem 10.4.4. If
n  3, either T has a real eigenvalue and therefore has a one-dimensional T -invariant subspace U = Ru for
any eigenvector u, or T has no real eigenvalue and therefore has a two-dimensional T -invariant subspace
U by Lemma 10.4.4. In either case U  is T -invariant (Lemma 10.4.2) and dim U  = n - dim U < n.
Hence, by induction, let B1 and B2 be orthonormal bases of U and U  such that MB1(T ) and MB2(T ) have
the form given. Then B = B1  B2 is an orthonormal basis of V , and MB(T ) has the desired form with a
suitable ordering of the vectors in B.

Now observe that R(0) = 1 0 0 1 and R() = -1 0 0 -1 . It follows that an even number of 1s or -1s
can be written as R(1)-blocks. Hence, with a suitable reordering of the basis B, the theorem follows.

    As in the dimension 2 situation, these possibilities can be given a geometric interpretation when V = R3
is taken as euclidean space. As before, this entails looking carefully at reflections and rotations in R3. If
Q : R3  R3 is any reflection in a plane through the origin (called the fixed plane of the reflection), take

{f2, f3} to be any orthonormal basis of the fixed plane and take f1 to be a unit vector perpendicular to

the fixed plane. Then Q(f1) = -f1, whereas Q(f2) = f2 and Q(f3) = f3. Hence B = {f1, f2, f3} is an

orthonormal basis such that                              -1 0 0 

                                      MB(Q) =  0 1 0 
                                                       001

Similarly, suppose that R : R3  R3 is any rotation about a line through the origin (called the axis of the
rotation), and let f1 be a unit vector pointing along the axis, so R(f1) = f1. Now the plane through the
8 Inner Product Spaces

origin perpendicular to the axis is an R-invariant subspace of R2 of dimension 2, and the restriction of R

to this plane is a rotation. Hence, by Theorem 10.4.4, there is an orthonormal basis B1 = {f2, f3} of this
plane such that MB1(R) = cos  - sin  sin  cos  . But then B = {f1, f2, f3} is an orthonormal basis of R3 such

that the matrix of R is         1 0                           0

                         MB(R) =  0 cos  - sin  
                                       0 sin  cos 

However, Theorem 10.4.5 shows that there are isometries T in R3 of a third type: those with a matrix of

the form                  -1 0                                0

                         MB(T ) =  0 cos  - sin  
                                         0 sin  cos 

If B = {f1, f2, f3}, let Q be the reflection in the plane spanned by f2 and f3, and let R be the ro-
tation corresponding to  about the line spanned by f1. Then MB(Q) and MB(R) are as above, and
MB(Q)MB(R) = MB(T ) as the reader can verify. This means that MB(QR) = MB(T ) by Theorem 9.2.1,

and this in turn implies that QR = T because MB is one-to-one (see Exercise ??). A similar argument
shows that RQ = T , and we have Theorem 10.4.6.

Theorem 10.4.6
If T : R3  R3 is an isometry, there are three possibilities.

                         1 0                     0

a. T is a rotation, and MB(T ) =  0 cos  - sin   for some orthonormal basis B.

                         0 sin  cos 

                                          -1 0 0 
b. T is a reflection, and MB(T ) =  0 1 0  for some orthonormal basis B.

                                               001

c. T = QR = RQ where Q is a reflection, R is a rotation about an axis perpendicular to the fixed

                          -1 0                   0

          plane of Q and MB(T ) =  0 cos  - sin   for some orthonormal basis B.

                         0 sin  cos 

Hence T is a rotation if and only if det T = 1.

Proof. It remains only to verify the final observation that T is a rotation if and only if det T = 1. But
clearly det T = -1 in parts (b) and (c).

    A useful way of analyzing a given isometry T : R3  R3 comes from computing the eigenvalues of T .
Because the characteristic polynomial of T has degree 3, it must have a real root. Hence, there must be at
least one real eigenvalue, and the only possible real eigenvalues are ±1 by Lemma 10.4.3. Thus Table 10.1
includes all possibilities.
                                                                                        . . Isometries

                                                      Table 10.1

Eigenvalues of T                                                       Action of T

(1) 1, no other real eigenvalues         Rotation about the line Rf where f is an eigenvector corresponding
(2) -1, no other real eigenvalues        to 1. [Case (a) of Theorem 10.4.6.]

(3) -1, 1, 1                             Rotation about the line Rf followed by reflection in the plane (Rf)
(4) 1, -1, -1                            where f is an eigenvector corresponding to -1. [Case (c) of Theo-
                                         rem 10.4.6.]

                                         Reflection in the plane (Rf) where f is an eigenvector correspond-
                                         ing to -1. [Case (b) of Theorem 10.4.6.]

                                         This is as in (1) with a rotation of .

(5) -1, -1, -1                           Here T (x) = -x for all x. This is (2) with a rotation of .

(6) 1, 1, 1                              Here T is the identity isometry.

Example 10.4.5

                                                        x  y 
Analyze the isometry T : R3  R3 given by T  y  =  z .

                                                            z          -x

                                                                        0 1 0
Solution. If B0 is the standard basis of R3, then MB0(T ) =  0 0 1 , so

                                                                          -1 0 0
cT (x) = x3 + 1 = (x + 1)(x2 - x + 1). This is (2) in Table 10.1. Write:

                                      1                      1                   1
                            f1 = 13  -1             f2 = 16  2         f3 = 12  0 

                                           1                     1                  -1

Here f1 is a unit eigenvector corresponding to 1 = -1, so T is a rotation (through an angle  )
about the line L = Rf1, followed by reflection in the plane U through the origin perpendicular to f1
(with equation x - y + z = 0). Then, {f1, f2} is chosen as an orthonormal basis of U , so
B = {f1, f2, f3} is an orthonormal basis of R3 and

                                                                              

                                                          -1 0             0
                                                                         3 
                                                            01         -2 
                                         MB(T ) = 
                                                                    2         
                                                                 3
                                                                             1
                                                            02               2

                                                  
Hence        is  given  by  cos      1,  sin      3,  so       .
                                  =            =            =
                                     2            2            3
Inner Product Spaces

    Let V be an n-dimensional inner product space. A subspace of V of dimension n - 1 is called a
hyperplane in V . Thus the hyperplanes in R3 and R2 are, respectively, the planes and lines through the
origin. Let Q : V  V be an isometry with matrix

                      MB(Q) =  -1 0
                                 0 In-1

for some orthonormal basis B = {f1, f2, . . . , fn}. Then Q(f1) = -f1 whereas Q(u) = u for each u in
U = span {f2, . . . , fn}. Hence U is called the fixed hyperplane of Q, and Q is called reflection in U .
Note that each hyperplane in V is the fixed hyperplane of a (unique) reflection of V . Clearly, reflections in
R2 and R3 are reflections in this more general sense.

    Continuing the analogy with R2 and R3, an isometry T : V  V is called a rotation if there exists an
orthonormal basis {f1, . . . , fn} such that

                                   Ir 0 0 
                      MB(T ) =  0 R( ) 0 

                                     0 0 Is

in block form, where R( ) = cos  - sin  sin  cos  , and where either Ir or Is (or both) may be missing. If
R( ) occupies columns i and i + 1 of MB(T ), and if W = span {fi, fi+1}, then W is T -invariant and the
matrix of T : W  W with respect to {fi, fi+1} is R( ). Clearly, if W is viewed as a copy of R2, then
T is a rotation in W . Moreover, T (u) = u holds for all vectors u in the (n - 2)-dimensional subspace
U = span {f1, . . . , fi-1, fi+1, . . . , fn}, and U is called the fixed axis of the rotation T . In R3, the axis of
any rotation is a line (one-dimensional), whereas in R2 the axis is U = {0}.

    With these definitions, the following theorem is an immediate consequence of Theorem 10.4.5 (the
details are left to the reader).

Theorem 10.4.7

Let T : V  V be an isometry of a finite dimensional inner product space V . Then there exist
isometries T1, . . . , T such that

                                                T = TkTk-1 · · · T2T1

where each Ti is either a rotation or a reflection, at most one is a reflection, and TiTj = TjTi holds
for all i and j. Furthermore, T is a composite of rotations if and only if det T = 1.
                                                                   . . An Application to Fourier Approximation

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

  . An Application to Fourier Approximation6

If U is an orthogonal basis of a vector space V , the expansion theorem (Theorem 10.2.4) presents a vector
v  V as a linear combination of the vectors in U . Of course this requires that the set U is finite since
otherwise the linear combination is an infinite sum and makes no sense in V .

    However, given an infinite orthogonal set U = {f1, f2, . . . , fn, . . . }, we can use the expansion theorem
for {f1, f2, . . . , fn} for each n to get a series of "approximations" vn for a given vector v. A natural
question is whether these vn are getting closer and closer to v as n increases. This turns out to be a very
fruitful idea.

    In this section we shall investigate an important orthogonal set in the space C[-, ] of continuous
functions on the interval [-, ], using the inner product.

                                                                                  

                                                  f , g = f (x)g(x)dx

                                                                                -

Of course, calculus will be needed. The orthogonal set in question is
                             {1, sin x, cos x, sin(2x), cos(2x), sin(3x), cos(3x), . . . }

    Standard techniques of integration give

                                                             

                                   1 2 = 12dx = 2

                                                           -
    6The name honours the French mathematician J.B.J. Fourier (1768-1830) who used these techniques in 1822 to investigate
heat conduction in solids.
Inner Product Spaces

sin kx 2 =                                          for any k = 1, 2, 3, . . .
cos kx 2 =                                          for any k = 1, 2, 3, . . .
                                   sin2(kx)dx = 

                                -
                                 

                                   cos2(kx)dx = 

                                -

We leave the verifications to the reader, together with the task of showing that these functions are orthog-
onal:

                              sin(kx), sin(mx) = 0 = cos(kx), cos(mx) if k = m

and
                                   sin(kx), cos(mx) = 0 for all k  0 and m  0

(Note that 1 = cos(0x), so the constant function 1 is included.)
    Now define the following subspace of C[-, ]:

Fn = span {1, sin x, cos x, sin(2x), cos(2x), . . . , sin(nx), cos(nx)}

The aim is to use the approximation theorem (Theorem 10.2.8); so, given a function f in C[-, ], define
the Fourier coefficients of f by

a0 =                  f (x), 1  1                   f (x)dx
ak =                  1 2 = 2
bk =                            -                     

                      cos(kx) 2 f (x), cos(kx) = 1      f (x) cos(kx)dx  k = 1, 2, . . .
                                                                         k = 1, 2, . . .
                      f (x), sin(kx) 1              -
                                                     
                       sin(kx) 2 = 
                                                        f (x) sin(kx)dx

                                                    -

Then the approximation theorem (Theorem 10.2.8) gives Theorem 10.5.1.

Theorem 10.5.1
Let f be any continuous real-valued function defined on the interval [-, ]. If a0, a1, . . . , and b0,
b1, . . . are the Fourier coefficients of f , then given n  0,

        fn(x) = a0 + a1 cos x + b1 sin x + a2 cos(2x) + b2 sin(2x) + · · · + an cos(nx) + bn sin(nx)
is a function in Fn that is closest to f in the sense that

                                                  f - fn  f - g
holds for all functions g in Fn.

The function fn is called the nth Fourier approximation to the function f .

Example 10.5.1
Find the fifth Fourier approximation to the function f (x) defined on [-, ] as follows:

                      f (x) =            + x if -   x < 0
                                         - x if 0  x  
                                                          . . An Application to Fourier Approximation

Solution. The graph of y = f (x) appears on the left below. The Fourier coefficients are computed
as follows, although the details of the integrations (usually by parts) are omitted.

       1                      
       2                      2
a0  =            f  (x)dx  =

          -

ak  =  1  f (x)  cos(kx)dx = k22 [1 - cos(k)] = 04                                 if k is even
          -                                                                        if k is odd
                                                                           k2
          
       1       f (x) sin(kx)dx = 0 for all k = 1, 2, . . .
bk  =     -

Hence the fifth Fourier approximation is

             f5(x)  =      +  4           cos  x  +   1   cos(3x)  +  1   cos(5x)
                       2                              32              52

This is plotted in the middle diagram and is already a reasonable approximation to f (x). By

comparison, f13(x) is also plotted and the difference is barely noticeable.

            y                                      y                         y

                                                4                         4

                                                3                         3

                                                2                         2

                                                1                         1

    -       0            x                                  x                                 x

                                 -4 -3 -2 -1 0 1 2 3 4             -4 -3 -2 -1 0 1 2 3 4

          f (x)                                   f5(x)                   f13(x)

    We say that a function f is an even function if f (x) = f (-x) holds for all x; f is called an odd
function if f (-x) = - f (x) holds for all x. Examples of even functions include the function in Example
10.5.1, all constant functions, the even powers x2, x4, . . . , and cos(kx); these functions are characterized
by the fact that the graph of y = f (x) is symmetric about the y axis. Examples of odd functions are the odd
powers x, x3, . . . , and sin(kx) where k > 0, and the graph of y = f (x) is symmetric about the origin if f is
odd. The usefulness of these functions stems from the fact that

                           f (x)dx        =  0                 if f is odd
                    -
                           f (x)dx = 2                f (x)dx  if f is even
                    -                             0

These facts often simplify the computations of the Fourier coefficients. For example:

1. The Fourier sine coefficients bk all vanish if f is even.
2. The Fourier cosine coefficients ak all vanish if f is odd.

This is because f (x) sin(kx) is odd in the first case and f (x) cos(kx) is odd in the second case.

    The functions 1, cos(kx), and sin(kx) that occur in the Fourier approximation for f (x) are all easy to
generate as an electrical voltage (when x is time). By summing these signals (with the amplitudes given
by the Fourier coefficients), it is possible to produce an electrical signal with (the approximation to) f (x)
as the voltage. Hence these Fourier approximations play a fundamental role in electronics.
Inner Product Spaces

    Finally, the Fourier approximations f1, f2, . . . of a function f in some cases get better and better as n
increases. This is in particular the case when the function f (x) is piecewise smooth, that is the function can
be broken into distinct pieces and on each piece both the function and its derivative, f (x), are continuous.
A piecewise smooth function may not be continuous everywhere however the only discontinuities that are
allowed are a finite number of jump discontinuities. The reason is that the subspaces Fn increase:

                                             F1  F2  F3  · · ·  Fn  · · ·
So, because fn = projFn f , we get (see the discussion following Example 10.2.6)

                                       f - f1  f - f2  ···  f - fn  ···

Under some conditions these numbers f - fn approach zero; in fact, we have the following fundamental
theorem.

   Theorem 10.5.2
   Let f in C[-, ] be piecewise smooth. Then

                              fn(x) approaches f (x) for all x such that -  < x < .7

It shows that f has a representation as an infinite series, called the Fourier series of f :

                     f (x) = a0 + a1 cos x + b1 sin x + a2 cos(2x) + b2 sin(2x) + · · ·

whenever - < x < . A full discussion of Theorem 10.5.2 is beyond the scope of this book. This subject
had great historical impact on the development of mathematics, and has become one of the standard tools
in science and engineering.

    Thus the Fourier series for the function f in Example 10.5.1 is

                  f (x)  =       +   4  cos    x  +  1   cos(3x)     +  1   cos(5x)  +  1   cos(7x)         +  ·  ·  ·
                              2                      32                 52              72

Since f (0) =  and cos(0) = 1, taking x = 0 leads to the series

                                                 2             111
                                                  8 = 1 + 32 + 52 + 72 + · · ·

Example 10.5.2

Expand  f (x)  =  x  on  the  interval     [- ,      ]  in  a  Fourier  series,  and  so    obtain       a  series      expansion  of  .

                                                                                                                                       4

Solution. Here f is an odd function so all the Fourier cosine coefficients ak are zero. As to the sine
coefficients:
                                               
                                        1         x sin(kx)dx = 2k (-1)k+1 for k  1
                                 bk  =      -

where we omit the details of the integration by parts. Hence the Fourier series for x is

                            x    =   2[sin  x  -  1  sin(2x)   +  1  sin(3x)  -  1  sin(4x)  +  .  .  .  ]
                                                  2               3              4

7We have to be careful at the end points x =  or x = - because sin(k) = sin(-k) and cos(k) = cos(-k).
                                                             . . An Application to Fourier Approximation

for  -  <  x  <  .  In  particular,  taking  x  =     gives  an  infinite  series  for  .
                                                   2
                                                                                        4

                                      = 1- 1 + 1 - 1 + 1 -···4
                                                      3579

Many other such formulas can be proved using Theorem 10.5.2.

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!
Chapter 11

                 Canonical Forms

Given a matrix A, the effect of a sequence of row-operations on A is to produce UA where U is invertible.
Under this "row-equivalence" operation the best that can be achieved is the reduced row-echelon form for
A. If column operations are also allowed, the result is UAV where both U and V are invertible, and the
best outcome under this "equivalence" operation is called the Smith canonical form of A (Theorem 2.5.3).
There are other kinds of operations on a matrix and, in many cases, there is a "canonical" best possible
result.

    If A is square, the most important operation of this sort is arguably "similarity" wherein A is carried
to U -1AU where U is invertible. In this case we say that matrices A and B are similar, and write A  B,
when B = U -1AU for some invertible matrix U . Under similarity the canonical matrices, called Jordan
canonical matrices, are block triangular with upper triangular "Jordan" blocks on the main diagonal. In
this short chapter we are going to define these Jordan blocks and prove that every matrix is similar to a
Jordan canonical matrix.

    Here is the key to the method. Let T : V  V be an operator on an n-dimensional vector space V , and
suppose that we can find an ordered basis B of V so that the matrix MB(T ) is as simple as possible. Then,
if B0 is any ordered basis of V , the matrices MB(T ) and MB0(T ) are similar; that is,

                               MB(T ) = P-1MB0(T )P for some invertible matrix P

Moreover, P = PB0B is easily computed from the bases B and B0 (Theorem 9.2.3). This, combined with
the invariant subspaces and direct sums studied in Section 9.3, enables us to calculate the Jordan canonical
form of any square matrix A. Along the way we derive an explicit construction of an invertible matrix P
such that P-1AP is block triangular.

    This technique is important in many ways. For example, if we want to diagonalize an n × n matrix A,
let TA : Rn  Rn be the operator given by TA(x) = Ax or all x in Rn, and look for a basis B of Rn such that
MB(TA) is diagonal. If B0 = E is the standard basis of Rn, then ME(TA) = A, so

                                           P-1AP = P-1ME (TA)P = MB(TA)

and we have diagonalized A. Thus the "algebraic" problem of finding an invertible matrix P such that
P-1AP is diagonal is converted into the "geometric" problem of finding a basis B such that MB(TA) is
diagonal. This change of perspective is one of the most important techniques in linear algebra.

            507
8 Canonical Forms

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                            Engage Active Learning App!

                      Vretta-Lyryx Engage is an active learning app designed to increase
                     student engagement in reading linear algebra material. The content is
                   "chunked" into small blocks, each with an interactive assessment activity

                                              to promote comprehension.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

  . Block Triangular Form

We have shown (Theorem 8.2.5) that any n × n matrix A with every eigenvalue real is orthogonally similar
to an upper triangular matrix U . The following theorem shows that U can be chosen in a special way.

Theorem 11.1.1: Block Triangulation Theorem
Let A be an n × n matrix with every eigenvalue real and let

                   cA(x) = (x - 1)m1(x - 2)m2 · · · (x - k)mk

where 1, 2, . . . , k are the distinct eigenvalues of A. Then an invertible matrix P exists such that

                                 U1 0 0 · · · 0                   

                               0      U2   0    ···          0
                                                                
                   P-1           0    0    U3 · · ·          0
                        AP  =    ...  ...  ...                  ...
                               

                                                                  
                                                                  

                                 0 0 0 · · · Uk

where, for each i, Ui is an mi × mi upper triangular matrix with every entry on the main diagonal
equal to i.
                                         . . Block Triangular Form

The proof is given at the end of this section. For now, we focus on a method for finding the matrix P. The
key concept is as follows.

   Definition 11.1 Generalized Eigenspaces
   If A is as in Theorem 11.1.1, the generalized eigenspace Gi(A) is defined by

                                              Gi(A) = null [(iI - A)mi]
   where mi is the multiplicity of i.

Observe that the eigenspace Ei(A) = null (iI - A) is a subspace of Gi(A). We need three technical
results.

   Lemma 11.1.1
   Using the notation of Theorem 11.1.1, we have dim [Gi(A)] = mi.

Proof. Write Ai = (iI - A)mi for convenience and let P be as in Theorem 11.1.1. The spaces
Gi(A) = null (Ai) and null (P-1AiP) are isomorphic via x  P-1x, so we show dim [ null (P-1AiP)] = mi.
Now P-1AiP = (iI - P-1AP)mi. If we use the block form in Theorem 11.1.1, this becomes

       iI -U1 0 · · ·           0 mi

-1  0             iI -U2 · · ·  0
                                      
P AiP =  ..        ..           ..
    .                .          .      
                                       

       0           0 · · · iI -Uk

       (iI -U1)mi       0       ···      0

             0       (iI -U2)mi · · ·    0        

    =        ..          ..              ..       
                                         .
               .        .                         
                                                  

             0          0       · · · (iI -Uk)mi

The matrix (iI -Uj)mi is invertible if j = i and zero if j = i (because then Ui is an mi ×mi upper triangular
matrix with each entry on the main diagonal equal to i). It follows that mi = dim [ null (P-1AiP)], as
required.

Lemma 11.1.2
If P is as in Theorem 11.1.1, denote the columns of P as follows:

                 p11, p12, . . . , p1m1; p21, p22, . . . , p2m2 ; . . . ; pk1, pk2, . . . , pkmk
Then {pi1, pi2, . . . , pimi} is a basis of Gi(A).

Proof. It suffices by Lemma 11.1.1 to show that each pi j is in Gi(A). Write the matrix in Theorem 11.1.1
as P-1AP = diag (U1, U2, . . . , Uk). Then

                                             AP = P diag (U1, U2, . . . , Uk)
Canonical Forms

Comparing columns gives, successively:

                 Ap11 = 1p11,                so (1I - A)p11 = 0
                 Ap12 = up11 + 1p12,
                                            so (1I - A)2p12 = 0
                 Ap13 = wp11 + vp12 + 1p13
                         ...                so (1I - A)3p13 = 0
                                                               ...

where u, v, w are in R. In general, (1I - A) jp1 j = 0 for j = 1, 2, . . . , m1, so p1 j is in Gi(A). Similarly,
pi j is in Gi(A) for each i and j.

Lemma 11.1.3
If Bi is any basis of Gi(A), then B = B1  B2  · · ·  Bk is a basis of Rn.

Proof. It suffices by Lemma 11.1.1 to show that B is independent. If a linear combination from B vanishes,
let xi be the sum of the terms from Bi. Then x1 + · · · + xk = 0. But xi =  j ri jpi j by Lemma 11.1.2, so
i, j ri jpi j = 0. Hence each xi = 0, so each coefficient in xi is zero.

    Lemma 11.1.2 suggests an algorithm for finding the matrix P in Theorem 11.1.1. Observe that there is
an ascending chain of subspaces leading from Ei(A) to Gi(A):

               Ei(A) = null [(iI - A)]  null [(iI - A)2]  · · ·  null [(iI - A)mi] = Gi(A)

We construct a basis for Gi(A) by climbing up this chain.

Theorem: Triangulation Algorithm
Suppose A has characteristic polynomial

                 cA(x) = (x - 1)m1(x - 2)m2 · · · (x - k)mk

1. Choose a basis of null [(1I - A)]; enlarge it by adding vectors (possibly none) to a basis of
   null [(1I - A)2]; enlarge that to a basis of null [(1I - A)3], and so on. Continue to obtain an
   ordered basis {p11, p12, . . . , p1m1} of G1(A).

2. As in (1) choose a basis {pi1, pi2, . . . , pimi} of Gi(A) for each i.

3. Let P = p11p12 · · · p1m1 ; p21p22 · · · p2m2 ; · · · ; pk1pk2 · · · pkmk  be the matrix with these
   basis vectors (in order) as columns.

Then P-1AP = diag (U1, U2, . . . , Uk) as in Theorem 11.1.1.

Proof. Lemma 11.1.3 guarantees that B = {p11, . . . , pkm1} is a basis of Rn, and Theorem 9.2.4 shows that
P-1AP = MB(TA). Now Gi(A) is TA-invariant for each i because

                       (iI - A)mix = 0 implies (iI - A)mi(Ax) = A(iI - A)mix = 0
                                                                                         . . Block Triangular Form

By Theorem 9.3.7 (and induction), we have
                                     P-1AP = MB(TA) = diag (U1, U2, . . . , Uk)

where Ui is the matrix of the restriction of TA to Gi(A), and it remains to show that Ui has the desired
upper triangular form. Given s, let pi j be a basis vector in null [(iI - A)s+1]. Then (iI - A)pi j is in
null [(iI - A)s], and therefore is a linear combination of the basis vectors pit coming before pi j. Hence

                                        TA(pi j) = Api j = ipi j - (iI - A)pi j
shows that the column of Ui corresponding to pi j has i on the main diagonal and zeros below the main
diagonal. This is what we wanted.

Example 11.1.1

         2 0 0 1
If A =  -1 1 2 0   0 2 0 -1 , find P such that P-1AP is block triangular.

             000 2

Solution. cA(x) = det [xI - A] = (x - 2)4, so 1 = 2 is the only eigenvalue and we are in the case
k = 1 of Theorem 11.1.1. Compute:

              0 0 0 -1                          0 0 0 0                    (2I - A)3 = 0
(2I - A) =  1 -1 0 0   0 0 0 1   (2I - A)2 =  0 0 0 -2   0 0 0 0 

                 0 00 0                            000 0

By gaussian elimination find a basis {p11, p12} of null (2I - A); then extend in any way to a basis
{p11, p12, p13} of null [(2I - A)2]; and finally get a basis {p11, p12, p13, p14} of
null [(2I - A)3] = R4. One choice is

                  1                0              0              0
           p11 =  0   1     p12 =  1   0   p13 =  0   1   p14 =  0   0 

                      0                0              0              1

                            1 0 0 0                                        2 0 0 1

Hence P =  p11 p12 p13 p14  =  0 1 0 0   1 0 1 0  gives P-1AP =  0 0 2 -2   0 2 1 0 

                                 0001                                      000 2
Canonical Forms

Example 11.1.2

         2 0 1 1
If A =  -4 -3 -3 -1   3 5 4 1 , find P such that P-1AP is block triangular.

             1012

Solution. The eigenvalues are 1 = 1 and 2 = 2 because

                        x-2         0   -1   -1             x-1 0                  0 -x + 1
                         -3       x-5   -4
             cA(x) =                   x+3   -11 = -3 x - 5 -4 -1 4 3 x + 3 1
                    =      4        3   -1
                         -1         0        x-2            -1 0 -1 x - 2
                                          0
                        x-1         0   -4    0 -4 x - 5 -4 -4
                         -3       x-5  x+3      5 = (x - 1) 3 x + 3 5
                                        -1
                           4        3        x - 3 0 -1 x - 3
                         -1         0   -4
                                       x+3
                                  x-5   -1        0                    x - 5 -4 0
                                    3
             = (x - 1)              0  x-5   -x + 2 = (x - 1) 3 x + 2 0
                                         3
                                             x-2                          0 -1 x - 2

             = (x - 1)(x - 2)                -4 x + 2 = (x - 1)2(x - 2)2

By solving equations, we find null (I - A) = span {p11} and null (I - A)2 = span {p11, p12} where

                                          1                     0
                                  p11 =  -2   1         p12 =  -4   3 

                                               1                     1

Since 1 = 1 has multiplicity 2 as a root of cA(x), dim G1(A) = 2 by Lemma 11.1.1. Since p11
and p12 both lie in G1(A), we have G1(A) = span {p11, p12}. Turning to 2 = 2, we find that
null (2I - A) = span {p21} and null [(2I - A)2] = span {p21, p22} where

                                        1                            0

                                  p21 =  -1   0  and p22 =  3   -4 

                                       1                               0

Again,  dim  G2 (A)  =  2  as  2  has  multiplicity 2,  so  G2 (A)  =  span {p21,  p22}.  Hence
                                                                       
        1010                                            1 -3 0 0

P =  -2 -4 -1 3   1 3 0 -4  gives P-1AP =  00 0 2 3  1 0 0 .

        1110                                            0 002

If p(x) is a polynomial and A is an n ×n matrix, then p(A) is also an n ×n matrix if we interpret A0 = In.
                                                                                         . . Block Triangular Form

For example, if p(x) = x2 - 2x + 3, then p(A) = A2 - 2A + 3I. Theorem 11.1.1 provides another proof
of the Cayley-Hamilton theorem (see also Theorem 8.7.10). As before, let cA(x) denote the characteristic
polynomial of A.

   Theorem 11.1.2: Cayley-Hamilton Theorem
   If A is a square matrix with every eigenvalue real, then cA(A) = 0.

Proof. As in Theorem 11.1.1, write cA(x) = (x - 1)m1 · · · (x - k)mk = ki=1(x - i)mi, and write
                                           P-1AP = D = diag (U1, . . . , Uk)

Hence

                            cA(Ui) = ki=1(Ui - iImi)mi = 0 for each i

because the factor (Ui - iImi)mi = 0. In fact Ui - iImi is mi × mi and has zeros on the main diagonal. But
then

                            P-1cA(A)P = cA(D) = cA[ diag (U1, . . . , Uk)]
                                                     = diag [cA(U1), . . . , cA(Uk)]
                                                     =0

It follows that cA(A) = 0.

Example 11.1.3

If A = 1 3 -1 2 , then cA(x) = det x - 1 -3 1 x - 2 = x2 - 3x + 5. Then
cA(A) = A2 - 3A + 5I2 = -2 9 -3 1 - 3 9 -3 6 + 5 0 0 5 = 0 0 0 0 .

    Theorem 11.1.1 will be refined even further in the next section.

Proof of Theorem . .

The proof of Theorem 11.1.1 requires the following simple fact about bases, the proof of which we leave
to the reader.

   Lemma 11.1.4
   If {v1, v2, . . . , vn} is a basis of a vector space V , so also is {v1 + sv2, v2, . . . , vn} for any scalar s.

Proof of Theorem 11.1.1. Let A be as in Theorem 11.1.1, and let T = TA : Rn  Rn be the matrix
transformation induced by A. For convenience, call a matrix a  -m-ut matrix if it is an m × m up-
per triangular matrix and every diagonal entry equals  . Then we must find a basis B of Rn such that

MB(T ) = diag (U1, U2, . . . , Uk) where Ui is a i-mi-ut matrix for each i. We proceed by induction on n.
If n = 1, take B = {v} where v is any eigenvector of T .
    Canonical Forms

    If n > 1, let v1 be a 1-eigenvector of T , and let B0 = {v1, w1, . . . , wn-1} be any basis of Rn containing
v1. Then (see Lemma 5.5.2)

                                                      MB0(T ) =              1 X
                                                                             0 A1

in block form where A1 is (n - 1) × (n - 1). Moreover, A and MB0(T ) are similar, so

                                         cA(x) = cMB0(T )(x) = (x - 1)cA1(x)
Hence cA1(x) = (x - 1)m1-1(x - 2)m2 · · · (x - k)mk so (by induction) let

                                           Q-1A1Q = diag (Z1, U2, . . . , Uk)

where Z1 is a 1-(m1 - 1)-ut matrix and Ui is a i-mi-ut matrix for each i > 1.

    If P =  10                       -1                      1 X Q                     = A, say. Hence A  MB0(T )  A so by
                        , then P MB0(T ) =                               -1
            0Q                                                  0 Q A1Q

Theorem 9.2.4(2) there is a basis B of Rn such that MB1(TA) = A, that is MB1(T ) = A. Hence MB1(T )
takes the block form

                                                                                                           Y
                                                                                          1 X1

                    MB1(T ) = 1 X Q                                                    0     Z1    0       0  0
                                                                                                                 
                                                                                                   U2 · · ·   0
                                                                                  =                                 (11.1)
                                     0 diag (Z1, U2, . . . , Uk)                                   ..             
                                                                                                              .. 
                                                                                       0 .                    .

                                                                                                   0 · · · Uk

If we write U1 = 1 X1 0 Z1 , the basis B1 fulfills our needs except that the row matrix Y may not be zero.

    We remedy this defect as follows. Observe that the first vector in the basis B1 is a 1 eigenvector of T ,
which we continue to denote as v1. The idea is to add suitable scalar multiples of v1 to the other vectors in
B1. This results in a new basis by Lemma 11.1.4, and the multiples can be chosen so that the new matrix
of T is the same as (11.1) except that Y = 0. Let {w1, . . . , wm2} be the vectors in B1 corresponding to 2
(giving rise to U2 in (11.1)). Write

                                 2 u12 u13 · · · u1m2                    

                            0        2       u23      ···       u2m2     
                                                                         
                    U2 =  0              0   2        ···
                                 ..      ..       ..            u3m2..         and     Y=    y1 y2 · · · ym2
                                                                         

                                 .. .                           .        
                                                                         

                                 0 0 0 · · · 2

We  first  replace  w1  by  w    =   w1  +   sv1      where  s  is   to  be  determined.     Then  (11.1)  gives

                              1

                                             T (w1) = T (w1) + sT (v1)

                                                      = (y1v1 + 2w1) + s1v1

                                                      = y1v1 + 2(w1 - sv1) + s1v1

                                                      =  2   w       +   [(y1  -  s(2  -  1  )]v1

                                                               1
                                                                                    . . The Jordan Canonical Form

Because  2  =  1  we  can  choose  s  such  that  T (w1)  =  2w1.    Similarly,         let  w    =  w2 + tv1  where  t  is  to  be

chosen. Then, as before,                                                                       2

                           T (w2) = T (w2) + tT (v1)

                                   = (y2v1 + u12w1 + 2w2) + t1v1

                                   =  u12      +  2      +  [(y2  -  u12  s)  -  t  (2  -  1  )]v1

                                           w1        w2

Again, t can be chosen so that T (w2) = u12w1 + 2w2. Continue in this way to eliminate y1, . . . , ym2.
This procedure also works for 3, 4, . . . and so produces a new basis B such that MB(T ) is as in (11.1)
but with Y = 0.

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!

  . The Jordan Canonical Form

Two m × n matrices A and B are called row-equivalent if A can be carried to B using row operations
and, equivalently, if B = UA for some invertible matrix U . We know (Theorem 2.6.4) that each m × n
matrix is row-equivalent to a unique matrix in reduced row-echelon form, and we say that these reduced
row-echelon matrices are canonical forms for m × n matrices using row operations. If we allow column
operations as well, then A  UAV = Ir 0 0 0 for invertible U and V , and the canonical forms are the
matrices Ir 0 0 0 where r is the rank (this is the Smith normal form and is discussed in Theorem 2.6.3).
In this section, we discover the canonical forms for square matrices under similarity: A  P-1AP.
6 Canonical Forms

    If A is an n × n matrix with distinct real eigenvalues 1, 2, . . . , k, we saw in Theorem 11.1.1 that A
is similar to a block triangular matrix; more precisely, an invertible matrix P exists such that

                         U1 0 · · ·   0

                   -1  0  U2  ···     0                                (11.2)
                                         
                   P AP =  .. .. . .
                       . . .          ..  = diag (U1, U2, . . . , Uk)
                                      .

                         0 0 0 Uk

where, for each i, Ui is upper triangular with i repeated on the main diagonal. The Jordan canonical form
is a refinement of this theorem. The proof we gave of (11.2) is matrix theoretic because we wanted to give

an algorithm for actually finding the matrix P. However, we are going to employ abstract methods here.

Consequently, we reformulate Theorem 11.1.1 as follows:

Theorem 11.2.1
Let T : V  V be a linear operator where dim V = n. Assume that 1, 2, . . . , k are the distinct
eigenvalues of T , and that the i are all real. Then there exists a basis F of V such that
MF (T ) = diag (U1, U2, . . . , Uk) where, for each i, Ui is square, upper triangular, with i repeated
on the main diagonal.

Proof. Choose any basis B = {b1, b2, . . . , bn} of V and write A = MB(T ). Since A has the same eigenval-
ues as T , Theorem 11.1.1 shows that an invertible matrix P exists such that P-1AP = diag (U1, U2, . . . , Uk)
where the Ui are as in the statement of the Theorem. If p j denotes column j of P and CB : V  Rn is the
coordinate isomorphism, let f j = CB-1(p j) for each j. Then F = {f1, f2, . . . , fn} is a basis of V and
CB(f j) = p j for each j. This means that PBF = CB(f j) = p j = P, and hence (by Theorem 9.2.2) that
PFB = P-1. With this, column j of MF (T ) is

                           CF (T (f j)) = PFBCB(T (f j)) = P-1MB(T )CB(f j) = P-1Ap j
for all j. Hence

            MF (T ) = CF (T (f j)) = P-1Ap j = P-1A p j = P-1AP = diag (U1, U2, . . . , Uk)
as required.

Definition 11.2 Jordan Blocks

If n  1, define the Jordan block Jn( ) to be the n × n matrix with  s on the main diagonal, 1s on
the diagonal above, and 0s elsewhere. We take J1( ) = [ ].

Hence

                                       1 0   1 0 0
                                               0  1 0
J1( ) = [ ] , J2( ) =  1 0  , J3( ) =  0  1  , J4( ) =  0 0  1  , . . .
                                      0 0
                                            0 0 0
                                                                                  . . The Jordan Canonical Form

We are going to show that Theorem 11.2.1 holds with each block Ui replaced by Jordan blocks corre-
sponding to eigenvalues. It turns out that the whole thing hinges on the case  = 0. An operator T is
called nilpotent if T m = 0 for some m  1, and in this case  = 0 for every eigenvalue  of T . Moreover,
the converse holds by Theorem 11.1.1. Hence the following lemma is crucial.

   Lemma 11.2.1
   Let T : V  V be a linear operator where dim V = n, and assume that T is nilpotent; that is,
   T m = 0 for some m  1. Then V has a basis B such that

                                             MB(T ) = diag (J1, J2, . . . , Jk)
   where each Ji is a Jordan block corresponding to  = 0.1

A proof is given at the end of this section.

   Theorem 11.2.2: Real Jordan Canonical Form
   Let T : V  V be a linear operator where dim V = n, and assume that 1, 2, . . . , m are the
   distinct eigenvalues of T and that the i are all real. Then there exists a basis E of V such that

                                            ME(T ) = diag (U1, U2, . . . , Uk)
   in block form. Moreover, each Uj is itself block diagonal:

                                               Uj = diag (J1, J2, . . . , Jk)
   where each Ji is a Jordan block corresponding to some i.

Proof. Let E = {e1, e2, . . . , en} be a basis of V as in Theorem 11.2.1, and assume that Ui is an ni × ni
matrix for each i. Let

   E1 = {e1, . . . , en1}, E2 = {en1+1, . . . , en1+n2}, . . . , Ek = {en1+···+nk-1+1, . . . , en1+···+nk }
where nk = n, and define Vi = span {Ei} for each i. Because the matrix ME(T ) = diag (U1, U2, . . . , Um)
is block diagonal, it follows that each Vi is T -invariant and MEi(T ) = Ui for each i. Let Ui have i repeated
along the main diagonal, and consider the restriction T : Vi  Vi. Then MEi(T - iIni) is a nilpotent matrix,
and hence (T - iIni) is a nilpotent operator on Vi. But then Lemma 11.2.1 shows that Vi has a basis Bi
such that MBi(T - iIni) = diag (K1, K2, . . . , Kti) where each Ki is a Jordan block corresponding to  = 0.
Hence

                         MBi(T ) = MBi(iIni ) + MBi(T - iIni)
                                  = iIni + diag (K1, K2, . . . , Kti) = diag (J1, J2, . . . , Jk)

where Ji = iIfi + Ki is a Jordan block corresponding to i (where Ki is fi × fi). Finally,
                                                  B = B1  B2  · · ·  Bk

is a basis of V with respect to which T has the desired matrix.

    1The converse is true too: If MB(T ) has this form for some basis B of V , then T is nilpotent.
  8 Canonical Forms

   Corollary 11.2.1
   If A is an n × n matrix with real eigenvalues, an invertible matrix P exists such that
   P-1AP = diag (J1, J2, . . . , Jk) where each Ji is a Jordan block corresponding to an eigenvalue i.

Proof. Apply Theorem 11.2.2 to the matrix transformation TA : Rn  Rn to find a basis B of Rn such that
MB(TA) has the desired form. If P is the (invertible) n × n matrix with the vectors of B as its columns, then
P-1AP = MB(TA) by Theorem 9.2.4.

    Of course if we work over the field C of complex numbers rather than R, the characteristic polynomial
of a (complex) matrix A splits completely as a product of linear factors. The proof of Theorem 11.2.2 goes
through to give

   Theorem 11.2.3: Jordan Canonical Form2
   Let T : V  V be a linear operator where dim V = n, and assume that 1, 2, . . . , m are the
   distinct eigenvalues of T . Then there exists a basis F of V such that

                                            MF(T ) = diag (U1, U2, . . . , Uk)
   in block form. Moreover, each Uj is itself block diagonal:

                                               Uj = diag (J1, J2, . . . , Jtj )
   where each Ji is a Jordan block corresponding to some i.

Except for the order of the Jordan blocks Ji, the Jordan canonical form is uniquely determined by the
operator T . That is, for each eigenvalue  the number and size of the Jordan blocks corresponding to 
is uniquely determined. Thus, for example, two matrices (or two operators) are similar if and only if they
have the same Jordan canonical form. We omit the proof of uniqueness; it is best presented using modules
in a course on abstract algebra.

Proof of Lemma

   Lemma 11.2.2
   Let T : V  V be a linear operator where dim V = n, and assume that T is nilpotent; that is,
   T m = 0 for some m  1. Then V has a basis B such that

                                             MB(T ) = diag (J1, J2, . . . , Jk)
   where each Ji = Jni(0) is a Jordan block corresponding to  = 0.

Proof. The proof proceeds by induction on n. If n = 1, then T is a scalar operator, and so T = 0 and the
lemma holds. If n  1, we may assume that T = 0, so m  1 and we may assume that m is chosen such

    2This was first proved in 1870 by the French mathematician Camille Jordan (1838-1922) in his monumental Traité des
substitutions et des équations algébriques.
                                                                                  . . The Jordan Canonical Form

that T m = 0, but T m-1 = 0. Suppose T m-1u = 0 for some u in V .3
Claim. {u, T u, T 2u, . . . , T m-1u} is independent.
Proof. Suppose a0u+a1T u+a2T 2u +· · ·+am-1T m-1u = 0 where each ai is in R. Since T m = 0, applying
T m-1 gives 0 = T m-10 = a0T m-1u, whence a0 = 0. Hence a1T u + a2T 2u + · · · + am-1T m-1u = 0 and
applying T m-2 gives a1 = 0 in the same way. Continue in this fashion to obtain ai = 0 for each i. This
proves the Claim.

    Now define P = span {u, T u, T 2u, . . . , T m-1u}. Then P is a T -invariant subspace (because T m = 0),
and T : P  P is nilpotent with matrix MB(T ) = Jm(0) where B = {u, T u, T 2u, . . . , T m-1u}. Hence we
are done, by induction, if V = P  Q where Q is T -invariant (then dim Q = n - dim P < n because P = 0,
and T : Q  Q is nilpotent). With this in mind, choose a T -invariant subspace Q of maximal dimension
such that P  Q = {0}.4 We assume that V = P  Q and look for a contradiction.

    Choose x  V such that x / P  Q. Then T mx = 0  P  Q while T 0x = x / P  Q. Hence there exists
k, 1  k  m, such that T kx  P  Q but T k-1x / P  Q. Write v = T k-1x, so that

                                             v / P  Q and T v  P  Q

Let T v = p + q with p in P and q in Q. Then 0 = T m-1(T v) = T m-1p + T m-1q so, since P and Q are
T -invariant, T m-1p = -T m-1q  P  Q = {0}. Hence

                                                         T m-1p = 0

Since p  P we have p = a0u + a1T u + a2T 2u + · · · + am-1T m-1u for ai  R. Since T m = 0, applying
T m-1 gives 0 = T m-1p = a0T m-1u, whence a0 = 0. Thus p = T (p1) where

                                      p1 = a1u + a2T u + · · · + am-1T m-2u  P

If we write v1 = v - p1 we have

                                        T (v1) = T (v - p1) = T v - p = q  Q

Since T (Q)  Q, it follows that T (Q + Rv1)  Q  Q + Rv1. Moreover v1 / Q (otherwise v = v1 + p1 
P  Q, a contradiction). Hence Q  Q + Rv1 so, by the maximality of Q, we have (Q + Rv1)  P = {0},
say

                         0 = p2 = q1 + av1 where p2  P, q1  Q, and a  R
Thus av1 = p2 - q1  P  Q. But since v1 = v - p1 we have

                                        av = av1 + ap1  (P  Q) + P = P  Q

Since v / P  Q, this implies that a = 0. But then p2 = q1  P  Q = {0}, a contradiction. This completes
the proof.

    3If S : V  V is an operator, we abbreviate S(u) by Su for simplicity.
    4Observe that there is at least one such subspace: Q = {0}.
         Canonical Forms

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.
                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.
                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!
Appendix A

            Complex Numbers

The fact that the square of every real number is nonnegative shows that the equation x2 + 1 = 0 has no real
root; in other words, there is no real number u such that u2 = -1. So the set of real numbers is inadequate
for finding all roots of all polynomials. This kind of problem arises with other number systems as well.
The set of integers contains no solution of the equation 3x + 2 = 0, and the rational numbers had to be
invented to solve such equations. But the set of rational numbers is also incomplete because, for example,
it contains no root of the polynomial x2 - 2. Hence the real numbers were invented. In the same way, the
set of complex numbers was invented, which contains all real numbers together with a root of the equation
x2 + 1 = 0. However, the process ends here: the complex numbers have the property that every polynomial
with complex coefficients has a (complex) root. This fact is known as the fundamental theorem of algebra.

    One pleasant aspect of the complex numbers is that, whereas describing the real numbers in terms of
the rationals is a rather complicated business, the complex numbers are quite easy to describe in terms of
real numbers. Every complex number has the form

                                                            a + bi

where a and b are real numbers, and i is a root of the polynomial x2 + 1. Here a and b are called the real
part and the imaginary part of the complex number, respectively. The real numbers are now regarded as
special complex numbers of the form a + 0i = a, with zero imaginary part. The complex numbers of the
form 0 + bi = bi with zero real part are called pure imaginary numbers. The complex number i itself is
called the imaginary unit and is distinguished by the fact that

                                                           i2 = -1

As the terms complex and imaginary suggest, these numbers met with some resistance when they were
first used. This has changed; now they are essential in science and engineering as well as mathematics,
and they are used extensively. The names persist, however, and continue to be a bit misleading: These
numbers are no more "complex" than the real numbers, and the number i is no more "imaginary" than -1.

    Much as for polynomials, two complex numbers are declared to be equal if and only if they have the
same real parts and the same imaginary parts. In symbols,

                                 a + bi = a + bi if and only if a = a and b = b

The addition and subtraction of complex numbers is accomplished by adding and subtracting real and
imaginary parts:

                                      (a + bi) + (a + bi) = (a + a) + (b + b)i
                                      (a + bi) - (a + bi) = (a - a) + (b - b)i

This is analogous to these operations for linear polynomials a + bx and a + bx, and the multiplication of
complex numbers is also analogous with one difference: i2 = -1. The definition is

                                    (a + bi)(a + bi) = (aa - bb) + (ab + ba)i

With these definitions of equality, addition, and multiplication, the complex numbers satisfy all the basic
arithmetical axioms adhered to by the real numbers (the verifications are omitted). One consequence of

                                                             521
       Complex Numbers

this is that they can be manipulated in the obvious fashion, except that i2 is replaced by -1 wherever it
occurs, and the rule for equality must be observed.

Example A.1

If  z  =  2 - 3i  and  w  =  -1 + i,  write  each   of  the  following  in  the  form  a + bi:  z + w,  z - w,  zw,  1 z,

and z2.                                                                                                              3

Solution.

                          z + w = (2 - 3i) + (-1 + i) = (2 - 1) + (-3 + 1)i = 1 - 2i

                          z - w = (2 - 3i) - (-1 + i) = (2 + 1) + (-3 - 1)i = 3 - 4i

                             zw = (2 - 3i)(-1 + i) = (-2 - 3i2) + (2 + 3)i = 1 + 5i

                             1z  =  13 (2 - 3i)  =  2  -i
                                                    3
                             3

                             z2 = (2 - 3i)(2 - 3i) = (4 + 9i2) + (-6 - 6)i = -5 - 12i

   Example A.2
   Find all complex numbers z such as that z2 = i.

   Solution. Write z = a + bi; we must determine a and b. Now z2 = (a2 - b2) + (2ab)i, so the
   condition z2 = i becomes

                                                (a2 - b2) + (2ab)i = 0 + i
   Equating real and imaginary parts, we find that a2 = b2 and 2ab = 1. The solution is a = b = ±12,
   so the complex numbers required are z = 12 + 12 i and z = - 12 - 12 i.

    As for real numbers, it is possible to divide by every nonzero complex number z. That is, there exists
a complex number w such that wz = 1. As in the real case, this number w is called the inverse of z and
is denoted by z-1 or 1z . Moreover, if z = a + bi, the fact that z = 0 means that a = 0 or b = 0. Hence
a2 + b2 = 0, and an explicit formula for the inverse is

                                                    z1 = a2+b a 2 - a2+b b 2 i
In actual calculations, the work is facilitated by two useful notions: the conjugate and the absolute value
of a complex number. The next example illustrates the technique.
Example A.3

Write  3+2i    in  the  form    a + bi.
       2+5i

Solution. Multiply top and bottom by the complex number 2 - 5i (obtained from the denominator
by negating the imaginary part). The result is

                                  3+2i (2-5i)(3+2i) (6+10)+(4-15)i 16 11

                                  2+5i = (2-5i)(2+5i) = 22-(5i)2 = 29 - 29 i

Hence  the     simplified   form  is  16  -  11 i,  as  required.
                                      29
                                             29

    The key to this technique is that the product (2 - 5i)(2 + 5i) = 29 in the denominator turned out to be
a real number. The situation in general leads to the following notation: If z = a + bi is a complex number,
the conjugate of z is the complex number, denoted z, given by

                                             z = a - bi where z = a + bi

Hence z is obtained from z by negating the imaginary part. Thus (2 + 3i) = 2 - 3i and (1 - i) = 1 + i. If
we multiply z = a + bi by z, we obtain

                                            zz = a2 + b2 where z = a + bi

    The real number a2 + b2 is always nonnegative, so we can state the following definition: Theabsolute
value or modulus of a complex number z = a + bi, denoted by |z|, is the positive square root a2 + b2;
that is,

For example, |2 - 3i| =                   |z| = a2 + b2 where z = a + bi
                                22 + (-3)2 = 13 and |1 + i| = 12 + 12 = 2.

    Note that if areal number a is viewed as the complex number a + 0i, its absolute value (as a complex
number) is |a| = a2, which agrees with its absolute value as a real number.

With these notions in hand, we can describe the technique applied in Example A.3 as follows: When

converting  a  quotient  z  of  complex   numbers       to  the  form  a + bi,  multiply  top  and  bottom  by  the  conjugate
                         w
w of the denominator.

    The following list contains the most important properties of conjugates and absolute values. Through-
out, z and w denote complex numbers.

            C1. z ± w = z ± w                               C7.    z1 = |z|12 z
            C2. zw = z w                                    C8.    |z|  0 for all complex numbers z
            C3. zw = zw                                     C9.    |z| = 0 if and only if z = 0
            C4. (z) = z                                     C10.
            C5. z is real if and only if z = z              C11.   |zw| = |z||w|
            C6. zz = |z|2                                   C12.   | wz | = |w| |z|
                                                                   |z + w|  |z| + |w| (triangle inequality)

All these properties (except property C12) can (and should) be verified by the reader for arbitrary complex
numbers z = a + bi and w = c + di. They are not independent; for example, property C10 follows from
properties C2 and C6.
Complex Numbers

    The triangle inequality, as its name suggests, comes from a geometric representation of the complex
numbers analogous to identification of the real numbers with the points of a line. The representation is
achieved as follows:

            y                           Introduce a rectangular coordinate system in the plane (Figure A.1),

                (a, b) = a + bi     and identify the complex number a + bi with the point (a, b). When this
(0, b) = bi                         is done, the plane is called the complex plane. Note that the point (a, 0)
                                    on the x axis now represents the real number a = a + 0i, and for this rea-
i              (a, 0) = a           son, the x axis is called the real axis. Similarly, the y axis is called the

01                               x  imaginary axis. The identification (a, b) = a + bi of the geometric point
                                    (a, b) and the complex number a + bi will be used in what follows without
                                    comment. For example, the origin will be referred to as 0.

    (a, -b) = a - bi                    This representation of the complex numbers in the complex plane gives
                                    a useful way of describing the absolute valueand conjugate of a complex
Figure A.1                          number z = a + bi. The absolute value |z| = a2 + b2 is just the distance
                                    from z to the origin. This makes properties C8 and C9 quite obvious. The

                                    conjugate z = a - bi of z is just the reflection of z in the real axis (x axis),

a fact that makes properties C4 and C5 clear.

    Given two complex numbers z1 = a1 + b1i = (a1, b1) and z2 = a2 + b2i = (a2, b2), the absolute value
of their difference

                                         |z1 - z2| = (a1 - a2)2 + (b1 - b2)2

is just the distance between them. This gives the complex distance formula:

              y

z + w |(z + w) - w| = |z|                              |z1 - z2| is the distance between z1 and z2

|z + w|           w                     This useful fact yields a simple verification of the triangle inequality,
         0     |w|                  property C12. Suppose z and w are given complex numbers. Consider the
                                    triangle in Figure A.2 whose vertices are 0, w, and z + w. The three sides
                            x       have lengths |z|, |w|, and |z + w| by the complex distance formula, so the
                                    inequality
Figure A.2
                                                                     |z + w|  |z| + |w|
y
        z + w = (a + c, b + d)      expresses the obvious geometric fact that the sum of the lengths of two
                                    sides of a triangle is at least as great as the length of the third side.
      z = (a, b)
                                        The representation of complex numbers as points in the complex plane
                                    has another very useful property: It enables us to give a geometric de-
                                    scription of the sum and product of two complex numbers. To obtain the
                                    description for the sum, let

                                                                     z = a + bi = (a, b)

                                                                    w = c + di = (c, d)

0 = (0, 0)     w = (c, d)           denote two complex numbers. We claim that the four points 0, z, w, and
                        x           z + w form the vertices of a parallelogram. In fact, in Figure A.3 the lines
                                    from 0 to z and from w to z + w have slopes
Figure A.3

                                               b-0 b  (b+d)-d b
                                               a-0 = a and (a+c)-c = a
respectively, so these lines are parallel. (If it happens that a = 0, then both these lines are vertical.)
Similarly, the lines from z to z + w and from 0 to w are also parallel, so the figure with vertices 0, z, w, and
z + w is indeed a parallelogram. Hence, the complex number z + w can be obtained geometrically from
z and w by completing the parallelogram. This is sometimes called the parallelogram law of complex
addition. Readers who have studied mechanics will recall that velocities and accelerations add in the same

way; in fact, these are all special cases of vector addition.

Polar Form

                                          The geometric description of what happens when two complex numbers

Unit             y                        are multiplied is at least as elegant as the parallelogram law of addition, but
circle                                    it requires that the complex numbers be represented in polar form. Before
                            Radian        discussing this, we pause to recall the general definition of the trigono-
-1                 i measure              metric functions sine and cosine. An angle  in the complex plane is in
                                          standard position if it is measured counterclockwise from the positive
                       P of               real axis as indicated in Figure A.4. Rather than using degrees to measure
                    1

                                x

             0           1

                                          angles, it is more natural to use radian measure. This is defined as follows:

                  -i                      The circle with its centre at the origin and radius 1 (called the unit circle)

                                          is drawn in Figure A.4. It has circumference 2, and the radian measure

          Figure A.4                      of  is the length of the arc on the unit circle counterclockwise from 1 to

                                          the  point  P    on  the  unit  circle  determined  by    .  Hence  90  =  ,  45  =  ,

                                                                                                                     2         4
180 = , and a full circle has the angle 360 = 2. Angles measured clockwise from 1 are negative; for
                                    -              3 ).
example,     -i   corresponds   to        (or  to
                                       2           2
                                                            .
Consider an angle               in the range 0                      If     is plotted in standard position as in Figure A.4,
                                                               2
it determines a unique point P on the unit circle, and P has coordinates (cos  , sin  ) by elementary

trigonometry. However, any angle  (acute or not) determines a unique point on the unit circle, so we

define the cosine and sine of  (written cos  and sin  ) to be the x and y coordinates of this point. For

example, the points

                                1 = (1, 0) i = (0, 1) -1 = (-1, 0) -i = (0, -1)

plotted  in  Figure   A.4  are  determined     by  the     angles   0,  ,  ,  3   ,  respectively.  Hence
                                                                              2
                                                                        2

                                cos 0 = 1          cos     =   0    cos  = -1        cos  3   =     0
                                sin0 = 0                2           sin = 0               2

                                                   sin     =   1                     sin  3   =     -1
                                                        2                                 2

Now we can describe the polar form of a complex number. Let z = a + bi be a complex number, and

write the absolute value of z as

   y                                                     r = |z| = a2 + b2

                      z = (a, b)          If z = 0, the angle  shown in Figure A.5 is called an argument of z and
                                          is denoted

                                                                                 = arg z

          r           b                   This angle is not unique ( + 2k would do as well for any

                                    k = 0, ±1, ±2, . . . ). However, there is only one argument  in the range
                            x - <   , and this is sometimes called the principal argument of z.
0              a

          Figure A.5
6 Complex Numbers

                                          Returning to Figure A.5, we find that the real and imaginary parts a
and b of z are related to r and  by

                                                        a = r cos 
                                                        b = r sin 

Hence the complex number z = a + bi has the form

                                     z = r(cos  + i sin  ) r = |z|,  = arg (z)

The combination cos  + i sin  is so important that a special notation is used:

                                                   ei = cos  + i sin 

is called Euler's formula after the great Swiss mathematician Leonhard Euler (1707-1783). With this
notation, z is written

                                             z = rei r = |z|,  = arg (z)
This is a polar form of the complex number z. Of course it is not unique, because the argument can be
changed by adding a multiple of 2.

Example A.4
Write z1 = -2 + 2i and z2 = -i in polar form.

Solution.

z1 = -2 + 2i y                     The two numbers are plotted in the complex plane in Figure A.6.
                                   The absolute values are
                        1
                2 0                                    r1 = | - 2 + 2i| = (-2)2 + 22 = 22

                       z2 = -i  x                      r2 = | - i| = 02 + (-1)2 = 1

                                   By inspection of Figure A.6, arguments of z1 and z2 are

                                         1    =   arg (-2 + 2i)   =   3
                                                                      4

       Figure A.6                        2    =   arg (-i)  =  3
                                                               2

The corresponding polar forms are z1 = -2 + 2i = 22e3i/4 and z2 = -i = e3i/2. Of course, we
                                   -                                                      e- i/2 .
could  have  taken  the  argument        for  z2  and  obtained  the  polar  form  z2  =
                                      2

    In Euler's formula ei = cos  + i sin  , the number e is the familiar constant e = 2.71828 . . . from
calculus. The reason for using e will not be given here; the reason why cos  + i sin  is written as an
exponential function of  is that the law of exponents holds:

                                                     ei · ei = ei( +)

where  and  are any two angles. In fact, this is an immediate consequence of the addition identities for
sin( +  ) and cos( +  ):
                         ei ei = (cos  + i sin  )(cos  + i sin  )
                                 = (cos  cos  - sin  sin  ) + i(cos  sin  + sin  cos  )
                                 = cos( +  ) + i sin( +  )
                                 = ei( +)

This is analogous to the rule eaeb = ea+b, which holds for real numbers a and b, so it is not unnatural to
use the exponential notation ei for the expression cos  + i sin  . In fact, a whole theory exists wherein
functions such as ez, sin z, and cos z are studied, where z is a complex variable. Many deep and beautiful
theorems can be proved in this theory, one of which is the so-called fundamental theorem of algebra
mentioned later (Theorem A.4). We shall not pursue this here.

    The geometric description of the multiplication of two complex numbers follows from the law of
exponents.

   Theorem A.1: Multiplication Rule

   If z1 = r1ei1 and z2 = r2ei2 are complex numbers in polar form, then

                                                    z1z2 = r1r2ei(1+2)

In other words, to multiply two complex numbers, simply multiply the absolute values and add the ar-
guments. This simplifies calculations considerably, particularly when we observe that it is valid for any
arguments 1 and 2.

Example A.5
Multiply (1 - i)(1 + 3i) in two ways.

Solution.

   y                            We have |1 - i| = 2 and |1 + 3i| = 2 so, from Figure A.7,
                                                              1 - i = 2e-i/4
      1 + 3i                                                  1 + 3i = 2ei/3

               (1 - i)(1 + 3i)  Hence, by the multiplication rule,
                                                 (1 - i)(1 + 3i) = (2e-i/4)(2ei/3)
                                                                     = 22ei(-/4+/3)
      3                                                              = 22ei/12
               12    x
             
0          -4

         1-i

         Figure A.7

This gives the required product in polar form. Of course, direct multiplication gives
(1 - i)(1 + 3i) = ( 3 + 1) + ( 3 - 1)i. Hence, equating real and imaginary parts gives the
formulas cos( 12 ) = 32+12 and sin( 12 ) = 32-12 .
8 Complex Numbers

Roots of Unity

If a complex number z = rei is given in polar form, the powers assume a particularly simple form. In
fact, z2 = (rei )(rei ) = r2e2i , z3 = z2 · z = (r2e2i )(rei ) = r3e3i , and so on. Continuing in this way,
it follows by induction that the following theorem holds for any positive integer n. The name honours
Abraham De Moivre (1667-1754).

   Theorem A.2: De Moivre's Theorem
   If  is any angle, then (ei )n = ein holds for all integers n.

Proof. The case n > 0 has been discussed, and the reader can verify the result for n = 0. To derive it for

n < 0, first observe that  if z = rei = 0 then z-1 = 1r e-i

In fact, (rei )( 1r e-i ) = 1ei0 = 1 by the multiplication rule. Now assume that n is negative and write it as
n = -m, m > 0. Then

                           (rei )n = [(rei )-1]m = ( 1r e-i )m = r-mei(-m ) = rnein

If r = 1, this is De Moivre's theorem for negative n.

Example A.6                Verify that (-1 + 3i)3 = 8.

              y            Solution. We have | - 1 + 3i| = 2, so -1 + 3i = 2e2i/3
                           (see Figure A.8). Hence De Moivre's theorem gives
       -1 + 3i
                                    (-1 + 3i)3 = (2e2i/3)3 = 8e3(2i/3) = 8e2i = 8
2                2
                 3

   0x

Figure A.8

    De Moivre's theorem can be used to find nth roots of complex numbers where n is positive. The next
example illustrates this technique.

   Example A.7
   Find the cube roots of unity; that is, find all complex numbers z such that z3 = 1.
   Solution. First write z = rei and 1 = 1ei0 in polar form. We must use the condition z3 = 1 to
   determine r and  . Because z3 = r3e3i by De Moivre's theorem, this requirement becomes

                                                        r3e3i = 1e0i
   These two complex numbers are equal, so their absolute values must be equal and the arguments
must either be equal or differ by an integral multiple of 2:

                        r3 = 1       k some integer
                        3 = 0 + 2k,

Because r is real and positive, the condition r3 = 1 implies that r = 1. However,

                         = 2k ,      k some integer

                                 3

              y         seems at first glance to yield infinitely many different angles for
                        z. However, choosing k = 0, 1, 2 gives three possible arguments
                         (where 0   < 2), and the corresponding roots are

  1 + 3i
-2 2

                 2                      1e0i = 1
                 3
                                                               
            4       1x
                                     1e2i/3 = - 1 + 3 i
            30
                                                        22
                                                               

  1 - 3i                             1e4i/3 = - 1 - 3 i
-2 2
                                                        22

Figure A.9              These are displayed in Figure A.9. All other values of k yield

                        values of  that differ from one of these by a multiple of 2--and

so do not give new roots. Hence we have found all the roots.

    The same type of calculation gives all complex nth roots of unity; that is, all complex numbers z such
that zn = 1. As before, write 1 = 1e0i and

                                                           z = rei
in polar form. Then zn = 1 takes the form

                        rneni = 1e0i

using De Moivre's theorem. Comparing absolute values and arguments yields

                        rn = 1       k some integer
                        n = 0 + 2k,

Hence r = 1, and the n values  = 2kn , k = 0, 1, 2, . . . , n - 1

of  all lie in the range 0   < 2. As in Example A.7, every choice of k yields a value of  that differs
from one of these by a multiple of 2, so these give the arguments of all the possible roots.

Theorem A.3: nth Roots of Unity
If n  1 is an integer, the nth roots of unity (that is, the solutions to zn = 1) are given by

                                     z = e2ki/n, k = 0, 1, 2, . . . , n - 1
Complex Numbers

e4 i/5      y                        The nth roots of unity can be found geometrically as the points on the unit
e6 i/5                               circle that cut the circle into n equal sectors, starting at 1. The case n = 5
                 e2 i/5              is shown in Figure A.10, where the five fifth roots of unity are plotted.

                     1 = e0ix            The method just used to find the nth roots of unity works equally well
                                     to find the nth roots of any complex number in polar form. We give one
              0                      example.
                 e8 i/5

        Figure A.10

Example A.8
Find the fourth roots of 2 + 2i.

Solution. First write 2 + 2i = 2ei/4 in polar form. If z = rei satisfies z4 = 2 + 2i, then De

Moivre's theorem gives                            r4ei(4 ) = 2ei/4

Hence   r4  =  2  and  4  =       +  2k ,  k  an  integer.  We  obtain    four   distinct   roots  (and  hence  all)  by
                               4

                                     r = 4 2,       =       =   2k     k  =  0,  1,  2,  3
                                                        16
                                                                16 ,

Thus the four roots are

                              4 2ei/16 4 2e9i/16 4 2e17i/16 4 2e25i/16

Of course, reducing these roots to the form a + bi would require the computation of 4 2 and the
sine and cosine of the various angles.

    An expression of the form ax2 + bx + c, where the coefficients a = 0, b, and c are real numbers, is
called a real quadratic. A complex number u is called a root of the quadratic if au2 + bu + c = 0. The
roots are given by the famous quadratic formula:

                                                  u=        -b±b2-4ac
                                                                2a

The quantity d = b2 - 4ac is called the discriminant of the quadratic ax2 + bx + c, and there is no real
root if and only if d < 0. In this case the quadratic is said to be irreducible. Moreover, the fact that d < 0
means that d = i |d|, so the two (complex) roots are conjugates of each other:

                               u  =  1 (-b    +i  |d|)      and     u  =  1 (-b      -i     |d|)

                                     2a                                   2a

The converse of this is true too: Given any nonreal complex number u, then u and u are the roots of some
real irreducible quadratic. Indeed, the quadratic

                                     x2 - (u + u)x + uu = (x - u)(x - u)

has real coefficients (uu = |u|2 and u + u is twice the real part of u) and so is irreducible because its roots
u and u are not real.
   Example A.9
   Find a real irreducible quadratic with u = 3 - 4i as a root.
   Solution. We have u + u = 6 and |u|2 = 25, so x2 - 6x + 25 is irreducible with u and u = 3 + 4i as
   roots.

Fundamental Theorem of Algebra

As we mentioned earlier, the complex numbers are the culmination of a long search by mathematicians
to find a set of numbers large enough to contain a root of every polynomial. The fact that the complex
numbers have this property was first proved by Gauss in 1797 when he was 20 years old. The proof is
omitted.

   Theorem A.4: Fundamental Theorem of Algebra
   Every polynomial of positive degree with complex coefficients has a complex root.

If f (x) is a polynomial with complex coefficients, and if u1 is a root, then the factor theorem (Section 6.5)
asserts that

                                                    f (x) = (x - u1)g(x)
where g(x) is a polynomial with complex coefficients and with degree one less than the degree of f (x).
Suppose that u2 is a root of g(x), again by the fundamental theorem. Then g(x) = (x - u2)h(x), so

                                               f (x) = (x - u1)(x - u2)h(x)

This process continues until the last polynomial to appear is linear. Thus f (x) has been expressed as a
product of linear factors. The last of these factors can be written in the form u(x - un), where u and un are
complex (verify this), so the fundamental theorem takes the following form.

   Theorem A.5
   Every complex polynomial f (x) of degree n  1 has the form

                                          f (x) = u(x - u1)(x - u2) · · · (x - un)

   where u, u1, . . . , un are complex numbers and u = 0. The numbers u1, u2, . . . , un are the roots of
    f (x) (and need not all be distinct), and u is the coefficient of xn.

This form of the fundamental theorem, when applied to a polynomial f (x) with real coefficients, can be
used to deduce the following result.

   Theorem A.6
   Every polynomial f (x) of positive degree with real coefficients can be factored as a product of
   linear and irreducible quadratic factors.
Complex Numbers

In fact, suppose f (x) has the form
                                        f (x) = anxn + an-1xn-1 + · · · + a1x + a0

where the coefficients ai are real. If u is a complex root of f (x), then we claim first that u is also a root. In
fact, we have f (u) = 0, so

                 0 = 0 = f (u) = anun + an-1un-1 + · · · + a1u + a0

                 = anun + an-1un-1 + · · · + a1u + a0
                 = anun + an-1un-1 + · · · + a1u + a0
                 = anun + an-1un-1 + · · · + a1u + a0

                 = f (u)

where ai = ai for each i because the coefficients ai are real. Thus if u is a root of f (x), so is its conjugate
u. Of course some of the roots of f (x) may be real (and so equal their conjugates), but the nonreal roots

come in pairs, u and u. By Theorem A.6, we can thus write f (x) as a product:

f (x) = an(x - r1) · · ·(x - rk)(x - u1)(x - u1) · · ·(x - um)(x - um)              (A.1)

where an is the coefficient of xn in f (x); r1, r2, . . . , rk are the real roots; and u1, u1, u2, u2, . . . , um, um
are the nonreal roots. But the product

                                     (x - u j)(x - u j) = x2 - (u j + u j)x + (u ju j)

is a real irreducible quadratic for each j (see the discussion preceding Example A.9). Hence (A.1) shows
that f (x) is a product of linear and irreducible quadratic factors, each with real coefficients. This is the
conclusion in Theorem A.6.

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!
Appendix B

            Proofs

Logic plays a basic role in human affairs. Scientists use logic to draw conclusions from experiments,
judges use it to deduce consequences of the law, and mathematicians use it to prove theorems. Logic
arises in ordinary speech with assertions such as "If John studies hard, he will pass the course," or "If an
integer n is divisible by 6, then n is divisible by 3."1 In each case, the aim is to assert that if a certain
statement is true, then another statement must also be true. In fact, if p and q denote statements, most
theorems take the form of an implication: "If p is true, then q is true." We write this in symbols as

                                                            pq

and read it as "p implies q." Here p is the hypothesis and q the conclusion of the implication. The
verification that p  q is valid is called the proof of the implication. In this section we examine the most
common methods of proof2 and illustrate each technique with some examples.

Method of Direct Proof

To prove that p  q, demonstrate directly that q is true whenever p is true.

   Example B.1
   If n is an odd integer, show that n2 is odd.

   Solution. If n is odd, it has the form n = 2k + 1 for some integer k. Then
   n2 = 4k2 + 4k + 1 = 2(2k2 + 2k) + 1 also is odd because 2k2 + 2k is an integer.

    Note that the computation n2 = 4k2 + 4k + 1 in Example B.1 involves some simple properties of arith-
metic that we did not prove. These properties, in turn, can be proved from certain more basic properties
of numbers (called axioms)--more about that later. Actually, a whole body of mathematical information
lies behind nearly every proof of any complexity, although this fact usually is not stated explicitly. Here is
a geometrical example.

    1By an integer we mean a "whole number"; that is, a number in the set 0, ±1, ±2, ±3, . . .
    2For a more detailed look at proof techniques see D. Solow, How to Read and Do Proofs, 2nd ed. (New York: Wiley, 1990);
or J. F. Lucas. Introduction to Abstract Mathematics, Chapter 2 (Belmont, CA: Wadsworth, 1986).

                                                             535
6 Proofs

Example B.2
In a right triangle, show that the sum of the two acute angles is 90 degrees.

Solution.

                             The right triangle is shown in the diagram. Construct a rectangle
                             with sides of the same length as the short sides of the original
                             triangle, and draw a diagonal as shown. The original triangle
                             appears on the bottom of the rectangle, and the top triangle is
                             identical to the original (but rotated). Now it is clear that  + 
                             is a right angle.

                       

    Geometry was one of the first subjects in which formal proofs were used--Euclid's Elements was
published about 300 B.C. The Elements is the most successful textbook ever written, and contains many
of the basic geometrical theorems that are taught in school today. In particular, Euclid included a proof of
an earlier theorem (about 500 B.C.) due to Pythagoras. Recall that, in a right triangle, the side opposite
the right angle is called the hypotenuse of the triangle.

Example B.3: Pythagoras' Theorem

   a c                       In a right-angled triangle, show that the square of the length
                             of the hypotenuse equals the sum of the squares of the lengths
                             of the other two sides.

             b               Solution. Let the sides of the right triangle have lengths a, b, and
     a                       c as shown. Consider two squares with sides of length a + b, and
                             place four copies of the triangle in these squares as in the diagram.
a a2                         The central rectangle in the second square shown is itself a square
                             because the angles  and  add to 90 degrees (using Example B.2),
                   b2     b  so its area is c2 as shown. Comparing areas shows that both
                             a2 + b2 and c2 each equal the area of the large square minus
                   b         four times the area of the original triangle, and hence are equal.

           b           a

                       

a                         b

 c2 

b
                      a

       

   a               b

    Sometimes it is convenient (or even necessary) to break a proof into parts, and deal with each case
separately. We formulate the general method as follows:
Method of Reduction to Cases

To prove that p  q, show that p implies at least one of a list p1, p2, . . . , pn of statements (the cases) and
then show that pi  q for each i.

   Example B.4
   Show that n2  0 for every integer n.

   Solution. This statement can be expressed as an implication: If n is an integer, then n2  0. To
   prove it, consider the following three cases:

                                          (1) n > 0; (2) n = 0; (3) n < 0.
   Then n2 > 0 in Cases (1) and (3) because the product of two positive (or two negative) integers is
   positive. In Case (2) n2 = 02 = 0, so n2  0 in every case.

Example B.5
If n is an integer, show that n2 - n is even.

Solution. We consider two cases:

                                          (1) n is even; (2) n is odd.
We have n2 - n = n(n - 1), so this is even in Case (1) because any multiple of an even number is
again even. Similarly, n - 1 is even in Case (2) so n(n - 1) is again even for the same reason.
Hence n2 - n is even in any case.

    The statements used in mathematics are required to be either true or false. This leads to a proof
technique which causes consternation in many beginning students. The method is a formal version of a
debating strategy whereby the debater assumes the truth of an opponent's position and shows that it leads
to an absurd conclusion.

Method of Proof by Contradiction

To prove that p  q, show that the assumption that both p is true and q is false leads to a contradiction. In
other words, if p is true, then q must be true; that is, p  q.

Example B.6
If r is a rational number (fraction), show that r2 = 2.

Solution. To argue by contradiction, we assume that r is a rational number and that r2 = 2, and

show  that  this  assumption  leads  to  a  contradiction.  Let  m  and  n  be  integers  such  that  r  =  m  is  in
                                                                                                            n
lowest terms (so, in particular, m and n are not both even). Then r2 = 2 gives m2 = 2n2, so m2 is
even. This means m is even (Example B.1), say m = 2k. But then 2n2 = m2 = 4k2, so n2 = 2k2 is
8 Proofs

even, and hence n is even. This shows that n and m are both even, contrary to the choice of these
numbers.

Example B.7: Pigeonhole Principle
If n + 1 pigeons are placed in n holes, then some hole contains at least 2 pigeons.

Solution. Assume the conclusion is false. Then each hole contains at most one pigeon and so,
since there are n holes, there must be at most n pigeons, contrary to assumption.

    The next example involves the notion of a prime number, that is an integer that is greater than 1 which
cannot be factored as the product of two smaller positive integers both greater than 1. The first few primes
are 2, 3, 5, 7, 11, . . . .

Example B.8
If 2n - 1 is a prime number, show that n is a prime number.

Solution. We must show that p  q where p is the statement "2n - 1 is a prime", and q is the

statement "n is a prime." Suppose that p is true but q is false so that n is not a prime, say n = ab

where a  2 and b  2 are integers. If we write 2a = x, then 2n = 2ab = (2a)b = xb. Hence 2n - 1

factors:  2n - 1 = xb - 1 = (x - 1)(xb-1 + xb-2 + · · · + x2 + x + 1)

As x  4, this expression is a factorization of 2n - 1 into smaller positive integers, contradicting the
assumption that 2n - 1 is prime.

The next example exhibits one way to show that an implication is not valid.

Example B.9
Show that the implication "n is a prime  2n - 1 is a prime" is false.

Solution. The first four primes are 2, 3, 5, and 7, and the corresponding values for 2n - 1 are 3, 7,
31, 127 (when n = 2, 3, 5, 7). These are all prime as the reader can verify. This result seems to be
evidence that the implication is true. However, the next prime is 11 and 211 - 1 = 2047 = 23 · 89,
which is clearly not a prime.

We say that n = 11 is a counterexample to the (proposed) implication in Example B.9. Note that, if you
can find even one example for which an implication is not valid, the implication is false. Thus disproving
implications is in a sense easier than proving them.

    The implications in Example B.8 and Example B.9 are closely related: They have the form p  q and
q  p, where p and q are statements. Each is called the converse of the other and, as these examples
show, an implication can be valid even though its converse is not valid. If both p  q and q  p are valid,
the statements p and q are called logically equivalent. This is written in symbols as

                                                            pq
and is read "p if and only if q". Many of the most satisfying theorems make the assertion that two
statements, ostensibly quite different, are in fact logically equivalent.

   Example B.10

   If n is an integer, show that "n is odd  n2 is odd."

   Solution. In Example B.1 we proved the implication "n is odd  n2 is odd." Here we prove the
   converse by contradiction. If n2 is odd, we assume that n is not odd. Then n is even, say n = 2k, so
   n2 = 4k2, which is also even, a contradiction.

    Many more examples of proofs can be found in this book and, although they are often more complex,
most are based on one of these methods. In fact, linear algebra is one of the best topics on which the
reader can sharpen his or her skill at constructing proofs. Part of the reason for this is that much of linear
algebra is developed using the axiomatic method. That is, in the course of studying various examples
it is observed that they all have certain properties in common. Then a general, abstract system is studied
in which these basic properties are assumed to hold (and are called axioms). In this system, statements
(called theorems) are deduced from the axioms using the methods presented in this appendix. These
theorems will then be true in all the concrete examples, because the axioms hold in each case. But this
procedure is more than just an efficient method for finding theorems in the examples. By reducing the
proof to its essentials, we gain a better understanding of why the theorem is true and how it relates to
analogous theorems in other abstract systems.

    The axiomatic method is not new. Euclid first used it in about 300 B.C. to derive all the propositions of
(euclidean) geometry from a list of 10 axioms. The method lends itself well to linear algebra. The axioms
are simple and easy to understand, and there are only a few of them. For example, the theory of vector
spaces contains a large number of theorems derived from only ten simple axioms.
         Proofs

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.
                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.
                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!
Appendix C

                                            Mathematical Induction

Suppose one is presented with the following sequence of equations:

                                      1=1
                                  1+3 = 4
                             1+3+5 = 9
                        1 + 3 + 5 + 7 = 16
                    1 + 3 + 5 + 7 + 9 = 25

It is clear that there is a pattern. The numbers on the right side of the equations are the squares 12, 22, 32,
42, and 52 and, in the equation with n2 on the right side, the left side is the sum of the first n odd numbers.
The odd numbers are

                    1 = 2·1-1
                    3 = 2·2-1
                    5 = 2·3-1
                    7 = 2·4-1
                    9 = 2·5-1

and from this it is clear that the nth odd number is 2n - 1. Hence, at least for n = 1, 2, 3, 4, or 5, the

following is true:  1 + 3 + · · · + (2n - 1) = n2

                                                                                              (Sn)

The question arises whether the statement Sn is true for every n. There is no hope of separately verifying
all these statements because there are infinitely many of them. A more subtle approach is required.

    The idea is as follows: Suppose it is verified that the statement Sn+1 will be true whenever Sn is true.
That is, suppose we prove that, if Sn is true, then it necessarily follows that Sn+1 is also true. Then, if we
can show that S1 is true, it follows that S2 is true, and from this that S3 is true, hence that S4 is true, and
so on and on. This is the principle of induction. To express it more compactly, it is useful to have a short
way to express the assertion "If Sn is true, then Sn+1 is true." As in Appendix B, we write this assertion as

                    Sn  Sn+1

and read it as " Sn implies Sn+1." We can now state the principle of mathematical induction.

                    541
Mathematical Induction

Theorem: The Principle of Mathematical Induction
Suppose Sn is a statement about the natural number n for each n = 1, 2, 3, . . . .
Suppose further that:

   1. S1 is true.
   2. Sn  Sn+1 for every n  1.

Then Sn is true for every n  1.

This is one of the most useful techniques in all of mathematics. It applies in a wide variety of situations,
as the following examples illustrate.

Example C.1

Show that 1 + 2 + · · · + n = 12n(n + 1) for n  1.

Solution. Let Sn be the statement: 1 + 2 + · · · + n = 12 n(n + 1) for n  1. We apply induction.

1.  S1 is true.  The statement S1 is 1 =      1  1(1  +  1),   which  is   true.
                                              2

2. Sn  Sn+1. We assume that Sn is true for some n  1--that is, that

                                           1 + 2 + · · · + n = 21n(n + 1)

We must prove that the statement

                 Sn+1   :         1  +  2  +  ···  +  (n  +  1)  =   1 (n  +  1)(n  +  2)

                                                                     2

is also true, and we are entitled to use Sn to do so. Now the left side of Sn+1 is the sum of the first
n + 1 positive integers. Hence the second-to-last term is n, so we can write

                 1 + 2 + · · · + (n + 1) = (1 + 2 + · · · + n) + (n + 1)

                                                 = 12 n(n + 1) + (n + 1)            using Sn

                                                 =    1 (n  +  1)(n  +  2)

                                                      2

This shows that Sn+1 is true and so completes the induction.

    In the verification that Sn  Sn+1, we assume that Sn is true and use it to deduce that Sn+1 is true. The
assumption that Sn is true is sometimes called the induction hypothesis.

Example C.2

                                                                 2            n xn+1-1
If x is any number such that x = 1, show that 1 + x + x + · · · + x = x-1 for n  1.

                                                 2               n xn+1-1
Solution. Let Sn be the statement: 1 + x + x + · · · + x = x-1 .
1.  S1 is true.   S1 reads 1 + x =  x2-1      ,  which  is  true  because  x2  -  1  =  (x  -  1)(x  +  1).
                                    x-1

                                                            2              n xn+1-1
2. Sn  Sn+1. Assume the truth of Sn : 1 + x + x + · · · + x = x-1 .

We  must  deduce  from  this  the  truth  of  Sn+1:  1  + x + x2  +·  + xn+1   =     xn+2 -1   Starting  with  the  left

side of Sn+1 and using the induction hypothesis, we find                              x-1 .

                        1 + x + x2 + · · · + xn+1 = (1 + x + x2 + · · · + xn) + xn+1

                                                                 xn+1-1 n+1

                                                   = x-1 + x
                                                   = x-1 xn+1-1+xn+1(x-1)

                                                                 xn+2-1

                                                   = x-1

This shows that Sn+1 is true and so completes the induction.

    Both of these examples involve formulas for a certain sum, and it is often convenient to use summation
notation. For example, nk=1(2k - 1) means that in the expression (2k - 1), k is to be given the values
k = 1, k = 2, k = 3, . . . , k = n, and then the resulting n numbers are to be added. The same thing applies
to other expressions involving k. For example,

                                    n

                   k3 = 13 + 23 + · · · + n3

                                  k=1
                           5

              (3k - 1) = (3 · 1 - 1) + (3 · 2 - 1) + (3 · 3 - 1) + (3 · 4 - 1) + (3 · 5 - 1)

                         k=1

The next example involves this notation.

Example C.3

Show that nk=1(3k2 - k) = n2(n + 1) for each n  1.
Solution. Let Sn be the statement: nk=1(3k2 - k) = n2(n + 1).

   1. S1 is true. S1 reads (3 · 12 - 1) = 12(1 + 1), which is true.

2. Sn  Sn+1. Assume that Sn is true. We must prove Sn+1:

                              n+1                n
                               (3k2 - k) =  (3k2 - k) + [3(n + 1)2 - (n + 1)]
                              k=1                k=1

                                                 = n2(n + 1) + (n + 1)[3(n + 1) - 1]                         (using Sn)

                                                 = (n + 1)[n2 + 3n + 2]

                                                 = (n + 1)[(n + 1)(n + 2)]

                                                 = (n + 1)2(n + 2)

This proves that Sn+1 is true.
Mathematical Induction

We now turn to examples wherein induction is used to prove propositions that do not involve sums.

   Example C.4
   Show that 7n + 2 is a multiple of 3 for all n  1.
   Solution.

       1. S1 is true: 71 + 2 = 9 is a multiple of 3.
       2. Sn  Sn+1. Assume that 7n + 2 is a multiple of 3 for some n  1; say, 7n + 2 = 3m for some

           integer m. Then
                            7n+1 + 2 = 7(7n) + 2 = 7(3m - 2) + 2 = 21m - 12 = 3(7m - 4)

           so 7n+1 + 2 is also a multiple of 3. This proves that Sn+1 is true.

    In all the foregoing examples, we have used the principle of induction starting at 1; that is, we have
verified that S1 is true and that Sn  Sn+1 for each n  1, and then we have concluded that Sn is true for
every n  1. But there is nothing special about 1 here. If m is some fixed integer and we verify that

   1. Sm is true.

   2. Sn  Sn+1 for every n  m.

then it follows that Sn is true for every n  m. This "extended" induction principle is just as plausible as
the induction principle and can, in fact, be proved by induction. The next example will illustrate it. Recall
that if n is a positive integer, the number n! (which is read "n-factorial") is the product

                                             n! = n(n - 1)(n - 2) · · ·3 · 2 · 1

of all the numbers from n to 1. Thus 2! = 2, 3! = 6, and so on.

Example C.5
Show that 2n < n! for all n  4.

Solution. Observe that 2n < n! is actually false if n = 1, 2, 3.

1. S4 is true. 24 = 16 < 24 = 4!.

2. Sn  Sn+1 if n  4. Assume that Sn is true; that is, 2n < n!. Then

                                 2n+1 = 2 · 2n

                                   < 2 · n!     because 2n < n!

                                   < (n + 1)n! because 2 < n + 1

                                   = (n + 1)!

Hence Sn+1 is true.
              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!
Appendix D

                                                                         Polynomials

Expressions like 3 - 5x and 1 + 3x - 2x2 are examples of polynomials. In general, a polynomial is an

expression of the form          f (x) = a0 + a1x + a2x2 + · · · + anxn

where the ai are numbers, called the coefficients of the polynomial, and x is a variable called an indeter-

minate. The number a0 is called the constant coefficient of the polynomial. The polynomial with every
coefficient zero is called the zero polynomial, and is denoted simply as 0.

    If f (x) = 0, the coefficient of the highest power of x appearing in f (x) is called the leading coefficient
of f (x), and the highest power itself is called the degree of the polynomial and is denoted deg ( f (x)).
Hence

-1 + 5x + 3x2                   has constant coefficient - 1, leading coefficient 3, and degree 2,
7                               has constant coefficient 7, leading coefficient 7, and degree 0,
6x - 3x3 + x4 - x5
                                has constant coefficient 0, leading coefficient - 1, and degree 5.

We do not define the degree of the zero polynomial.
    Two polynomials f (x) and g(x) are called equal if every coefficient of f (x) is the same as the corre-

sponding coefficient of g(x). More precisely, if

                        f (x) = a0 + a1x + a2x2 + · · · and g(x) = b0 + b1x + b2x2 + · · ·

are polynomials, then

                       f (x) = g(x) if and only if a0 = b0, a1 = b1, a2 = b2, . . .

In particular, this means that

f (x) = 0 is the zero polynomial if and only if a0 = 0, a1 = 0, a2 = 0, . . .

This is the reason for calling x an indeterminate.
    Let f (x) and g(x) denote nonzero polynomials of degrees n and m respectively, say

               f (x) = a0 + a1x + a2x2 + · · · + anxn and g(x) = b0 + b1x + b2x2 + · · · + bmxm

where an = 0 and bm = 0. If these expressions are multiplied, the result is
                 f (x)g(x) = a0b0 + (a0b1 + a1b0)x + (a0b2 + a1b1 + a2b0)x2 + · · · + anbmxn+m

Since an and bm are nonzero numbers, their product anbm = 0 and we have

Theorem D.1

If f (x) and g(x) are nonzero polynomials of degrees n and m respectively, their product f (x)g(x) is
also nonzero and

                                              deg [ f (x)g(x)] = n + m

                                547
8 Polynomials

Example D.1
(2 - x + 3x2)(3 + x2 - 5x3) = 6 - 3x + 11x2 - 11x3 + 8x4 - 15x5.

    If f (x) is any polynomial, the next theorem shows that f (x) - f (a) is a multiple of the polynomial
x - a. In fact we have

   Theorem D.2: Remainder Theorem
   If f (x) is a polynomial of degree n  1 and a is any number, then there exists a polynomial q(x)
   such that

                                                f (x) = (x - a)q(x) + f (a)
   where deg (q(x)) = n - 1.

Proof. Write f (x) = a0 + a1x + a2x2 + · · · + anxn where the ai are numbers, so that
                                          f (a) = a0 + a1a + a2a2 + · · · + anan

If these expressions are subtracted, the constant terms cancel and we obtain

               f (x) - f (a) = a1(x - a) + a2(x2 - a2) + · · · + an(xn - an).

Hence it suffices to show that, for each k  1, xk - ak = (x - a)p(x) for some polynomial p(x) of degree
k - 1. This is clear if k = 1. If it holds for some value k, the fact that

               xk+1 - ak+1 = (x - a)xk + a(xk - ak)

shows that it holds for k + 1. Hence the proof is complete by induction.

    There is a systematic procedure for finding the polynomial q(x) in the remainder theorem. It is illus-
trated below for f (x) = x3 - 3x2 + x - 1 and a = 2. The polynomial q(x) is generated on the top line one
term at a time as follows: First x2 is chosen because x2(x - 2) has the same x3-term as f (x), and this is
subtracted from f (x) to leave a "remainder" of -x2 + x - 1. Next, the second term on top is -x because
-x(x - 2) has the same x2-term, and this is subtracted to leave -x - 1. Finally, the third term on top is
-1, and the process ends with a "remainder" of -3.

               x-2          x2 - x - 1
                    x3 - 3x2 + x - 1

                    x3 - 2x2
                         -x2 + x - 1
                         -x2 + 2x
                                  -x - 1
                                  -x + 2
                                      -3
Hence x3 - 3x2 + x - 1 = (x - 2)(x2 - x - 1) + (-3). The final remainder is -3 = f (2) as is easily verified.
This procedure is called the division algorithm.1

    A real number a is called a root of the polynomial f (x) if

                                                          f (a) = 0

Hence for example, 1 is a root of f (x) = 2 - x + 3x2 - 4x3, but -1 is not a root because f (-1) = 10 = 0.
If f (x) is a multiple of x - a, we say that x - a is a factor of f (x). Hence the remainder theorem shows
immediately that if a is root of f (x), then x - a is factor of f (x). But the converse is also true: If x - a is a
factor of f (x), say f (x) = (x - a)q(x), then f (a) = (a - a)q(a) = 0. This proves the

   Theorem D.3: Factor Theorem
   If f (x) is a polynomial and a is a number, then x - a is a factor of f (x) if and only if a is a root of
    f (x).

Example D.2
If f (x) = x3 - 2x2 - 6x + 4, then f (-2) = 0, so x - (-2) = x + 2 is a factor of f (x). In fact, the
division algorithm gives f (x) = (x + 2)(x2 - 4x + 2).

    Consider the polynomial f (x) = x3 -3x+2. Then 1 is clearly a root of f (x), and the division algorithm
gives f (x) = (x - 1)(x2 + x - 2). But 1 is also a root of x2 + x - 2; in fact, x2 + x - 2 = (x - 1)(x + 2).

Hence  f (x) = (x - 1)2(x + 2)

and we say that the root 1 has multiplicity 2.

    Note that non-zero constant polynomials f (x) = b = 0 have no roots. However, there do exist non-
constant polynomials with no roots. For example, if g(x) = x2 + 1, then g(a) = a2 + 1  1 for every real
number a, so a is not a root. However the complex number i is a root of g(x); we return to this below.

    Now suppose that f (x) is any nonzero polynomial. We claim that it can be factored in the following
form:

                                        f (x) = (x - a1)(x - a2) · · · (x - am)g(x)

where a1, a2, . . . , am are the roots of f (x) and g(x) has no root (where the ai may have repetitions, and
may not appear at all if f (x) has no real root).

    By the above calculation f (x) = x3 -3x +2 = (x -1)2(x +2) has roots 1 and -2, with 1 of multiplicity
two (and g(x) = 1). Counting the root -2 once, we say that f (x) has three roots counting multiplicities.
The next theorem shows that no polynomial can have more roots than its degree even if multiplicities are

counted.

Theorem D.4
If f (x) is a nonzero polynomial of degree n, then f (x) has at most n roots counting multiplicities.

    1This procedure can be used to divide f (x) by any nonzero polynomial d(x) in place of x - a; the remainder then is a
polynomial that is either zero or of degree less than the degree of d(x).
         Polynomials

Proof. If n = 0, then f (x) is a constant and has no roots. So the theorem is true if n = 0. (It also holds for
n = 1 because, if f (x) = a + bx where b = 0, then the only root is -ab .) In general, suppose inductively
that the theorem holds for some value of n  0, and let f (x) have degree n + 1. We must show that f (x)
has at most n + 1 roots counting multiplicities. This is certainly true if f (x) has no root. On the other hand,
if a is a root of f (x), the factor theorem shows that f (x) = (x - a)q(x) for some polynomial q(x), and q(x)
has degree n by Theorem D.1. By induction, q(x) has at most n roots. But if b is any root of f (x), then

                                                 (b - a)q(b) = f (b) = 0

so either b = a or b is a root of q(x). It follows that f (x) has at most n roots. This completes the induction
and so proves Theorem D.4.

    As we have seen, a polynomial may have no root, for example f (x) = x2 + 1. Of course f (x) has
complex roots i and -i, where i is the complex number such that i2 = -1. But Theorem D.4 even holds
for complex roots: the number of complex roots (counting multiplicities) cannot exceed the degree of the
polynomial. Moreover, the fundamental theorem of algebra asserts that the only nonzero polynomials with
no complex root are the non-zero constant polynomials. This is discussed more in Appendix A, Theorems
A.4 and A.5.

              Vretta-Lyryx Online and Text Exercises!

Vretta-Lyryx offers formative online exercises! The educational software captures instructors' experience
    using algorithms to analyze and evaluate students' work, identifying common errors and providing

 personalized feedback. Students are encouraged to try these randomized problems repeatedly, learning
                                           from their mistakes until mastery.

                   Vretta-Lyryx also offers end-of-chapter exercises for Vretta-Lyryx users.

                    Visit us at https://lyryx.com to register and access Vretta-Lyryx exercises!

                                Engage Active Learning App!

                                    Vretta-Lyryx Engage is an active learning app designed to increase
                                   student engagement in reading linear algebra material. The content is
                                 "chunked" into small blocks, each with an interactive assessment activity

                                                            to promote comprehension.

                                  Visit us at https://lyryx.com to register and access Vretta-Lyryx Engage!
  Selected Exercise Answers

551
                                                                                            Index

(i, j)-entry, 30              Interpolation and Approxi-        angle between two vec-         orthonormal basis, 480,
3-dimensional space, 51             mation (Davis), 483            tors, 196                      486
A-invariance, 151
                              Introduction to Matrix Com-       radian measure, 53, 93,        reducing subset to, 237
B-matrix, 447                       putations (Stewart),           525                         standard basis, 88, 231,
T -invariant, 456                   382
m × n matrix                                                    standard position, 93, 525        235, 237, 299, 399
                              Raum-Zeit-Materie ("Space-        unit circle, 93, 525           vector spaces, 298
   canonical forms, 515             Time-Matter")(Weyl),     approximation theorem, 482,    best approximation, 264
   defined, 30                      281                                                     best approximation theorem,
   difference, 32                                                  502
   elementary row operation,  The Algebraic Eigenvalue       Archimedes, 10                       265
                                    Problem (Wilkinson),     area                           binary codes, 413
      80                            382                                                     Binet formula, 169
   main diagonal, 38                                            linear transformations of,  block matrix, 132
   matrix transformation,     "Linear Programming and              216                      block multiplication, 65
                                    Extensions" (Wu and                                     block triangular form, 508
      439                           Coppins), 435               parallelogram               block triangular matrix, 458
   negative, 32                                                  equal to zero, 209         block triangulation theorem,
   subspaces, 224             "if and only if", 31
   transpose, 36              "mixed" cancellation, 72       argument, 525                        508
   zero matrix, 32            3-dimensional space, 181       arrows, 181                    blocks, 64
n-parity-check code, 415                                     associated homogeneous         boundary condition, 171,
n-tuples, 223, 246, 283       absolute value
n-vectors, 40                    complex number, 522,              system, 46                     315
n-words, 413                        523                      associative law, 32, 62
                                 notation, 93                attractor, 163                 cancellation, 286, 287
nth roots of unity, 529          real number, 182            augmented matrix, 3, 4, 12     cancellation laws, 72
r-ball, 414                      symmetric matrices, 262     auxiliary theorem, 80          canonical forms
x-axis, 181                      triangle inequality, 475    axiomatic method, 539
x-compression, 54                                            axioms, 535                       m × n matrix, 515
x-expansion, 54               abstract vector space, 281     axis, 181, 497                    block triangular form, 508
x-shear, 54                   action                                                           Jordan canonical form,
y-axis, 181                                                  back substitution, 13, 99
y-compression, 54                same action, 52, 285, 322   balanced reaction, 26                515
y-expansion, 54                  transformations, 52, 440,   ball, 414                      Cartesian coordinates, 181
                                                             Banach, Stephan, 281           cartesian geometry, 181
z-axis, 181                         442                      bases, 235                     category, 339
                              addition                       basic eigenvectors, 149        Cauchy inequality, 241, 278
Disquisitiones Arithmeticae                                  basic solutions, 20, 21, 392   Cauchy, Augustin Louis,
      (Gauss), 10                closed under, 283           basis
                                 closed under addition, 40,                                       134, 262
How to Read and Do Proofs                                       choice of basis, 439, 445   Cayley, Arthur, 29, 123
      (Solow), 535                  223                         enlarging subset to, 237    Cayley-Hamilton theorem,
                                 complex number, 521            geometric problem of
Introduction to Abstract Al-     matrix addition, 31                                              513
      gebra (Nicholson), 411     pointwise addition, 285           finding, 451, 452, 507   centred, 275
                                 transformations                independent set, 356        change matrix, 448
Introduction to    Abstract                                     isomorphisms, 335           channel, 413
                   (Lucas),       preserving addition, 86       linear operators            characteristic polynomial
      Mathematics                vector addition, 282, 525
      535                     adjacency matrix, 66               and choice of basis, 453      block triangular matrix,
                              adjugate, 69, 137                 matrix of T corresponding         459
Introduction to the Theory    adjugate formula, 139
                              adult survival rate, 146             to the ordered bases B      complex matrix, 402
of Error-Correcting           aerodynamics, 434                    and D, 441                  diagonalizable matrix,
Codes (Pless), 419            algebraic method, 4, 8            of subspace, 235
                              algebraic multiplicity, 259       ordered basis, 440, 441           256
Mécanique Analytique (La-     algebraic sum, 25                 orthogonal basis, 245,         eigenvalues, 148
      grange), 208            analytic geometry, 40                356, 479                    root of, 148, 313, 314
                              angles                                                           similarity invariant, 453,
Calcolo      Geometrico
                                                                                                  454
(Peano), 281

Elements (Euclid), 536

                              553
INDEX

   square matrix, 148, 407           126, 176                     absolute value, 342, 522,  converse, 538
chemical reaction, 26          cofactor matrix, 137                  523                     coordinate isomorphism,
choice of basis, 439, 445      column matrix, 30, 40
Cholesky algorithm, 374        column space, 246, 383             addition, 521                    336
Cholesky factorization, 373    column vectors, 147                advantage of working       coordinate transformation,
Cholesky, Andre-Louis, 373     columns
circuit rule, 24                                                     with, 402                     441
classical adjoint, 137            (i, j)-entry, 30                conjugate, 523             coordinate vectors, 187, 204,
closed economy, 110               as notations for ordered n-     equal, 521
closed under addition, 40,                                        extension of concepts to,        218, 440
                                     tuples, 234                                             coordinates, 181, 440
      223, 283                    convention, 30                     398                     correlation, 275
closed under scalar multipli-     elementary column opera-        form, 521                  correlation coefficient
                                                                  fundamental theorem of
      cation, 40, 223, 283           tions, 128                                                 computation
code                              equal, 18                          algebra, 521                with dot product, 277
                                  leading column, 101             imaginary axis, 524
   (n, k)-code, 415               shape of matrix, 30             imaginary part, 521           Pearson correlation coeffi-
   n-code, 413                    Smith normal form, 83           imaginary unit, 521              cient, 277
   binary codes, 413              transpose, 36                   in complex plane, 524
   decoding, 419               commutative law, 32                inverse, 522                  sample correlation coeffi-
   defined, 413                commute, 60, 63                    modulus, 523                     cient, 277
   error-correcting codes,     compatibility rule, 59             multiplication, 521
                               compatible                         parallelogram law, 525     correlation formula, 279
      408, 413                    blocks, 65                      polar form, 525            coset, 419
   Hamming (7,4)-code, 421        for multiplication, 59          product, 524               cosine, 93, 196, 525
   linear codes, 415           complement, 462                    pure imaginary numbers,    counterexample, 538
   matrix generators, 417      completely diagonalized,                                      covariance, 437
   minimum distance, 414                                             521                     covariance matrix, 437
   nearest neighbour decod-          429                          real axis, 524             Cramer's Rule, 134
                               complex conjugation, 262           real part, 521             Cramer, Gabriel, 141
      ing, 414                 complex distance formula,          regular representation,    cross product
   orthogonal codes, 420
   parity-check code, 415            524                             524                        and dot product, 204, 208
   parity-check matrices,      complex eigenvalues, 261,          roots of unity, 528           coordinate vectors, 204
                                                                  scalars, 410                  coordinate-free descrip-
      418                            382                          subtraction, 521
   perfect, 415                complex matrix                     sum, 525                         tion, 210
   syndrome decoding, 419                                         triangle inequality, 523      defined, 204, 207
   use of, 408                    Cayley-Hamilton theo-        complex plane, 524               determinant form, 204,
code words, 413, 414, 417,           rem, 407                  composite, 56, 91, 338
                                                               composition, 56, 338, 444           207
      421                         characteristic polynomial,   computer graphics, 219           Lagrange Identity, 208
coding theory, 413                   402                       conclusion, 535                  properties of, 208
coefficient matrix, 4, 143                                     congruence, 428                  right-hand rule, 211
coefficients                      conjugate, 399               congruent matrices, 428          shortest distance between
                                  conjugate transpose, 401     conic graph, 17, 426
   constant coefficient, 547      defined, 399                 conjugate, 399, 523                 nonparallel lines, 206
   Fourier coefficients, 245,     eigenvalues, 402             conjugate matrix, 262         cryptography, 408
                                  eigenvector, 402             conjugate transpose, 401
      478, 502, 503               hermitian matrix, 401        consistent system, 1, 15      data scaling, 279
   in linear equation, 1          normal, 406                  constant, 547                 Davis, Philip J., 483
   leading coefficient, 143,      Schur's theorem, 405         constant matrix, 4            De Moivre's Theorem, 528
                                  spectral theorem, 405        constant sequences, 346       De Moivre, Abraham, 528
      284, 547                    standard inner product,      constant term, 1              decoding, 419
   linear combination, 226,                                    constrained optimization,     defined, 59
                                     399                                                     defining transformation, 52
      230, 292                    unitarily diagonalizable,          432                     degree of the polynomial,
   of the polynomial, 272,                                     continuous functions, 470
                                     404                       contradiction, proof by, 537        284, 547
      284, 547                    unitary diagonalization,     convergence, 381              demand matrix, 112
   of vectors, 292                                             converges, 119                dependent, 232, 295, 305
   sample correlation coeffi-        404                                                     dependent lemma, 305
                                  unitary matrix, 403                                        derivative, 342
      cient, 277                  upper triangular matrix,                                   Descartes, René, 181
cofactor, 125                                                                                determinants
cofactor expansion, 126, 176         404
cofactor expansion theorem,    complex number                                                   3 × 3, 124
                                                                                                n × n, 126
                                                                                               INDEX

   adjugate, 69, 137                  424                          inner product space, 472       orthogonal eigenvectors,
   and eigenvalues, 123         diagonalizing matrix, 154          length, 473                       367, 402
   and inverses, 69, 124        difference                         of two ordered n-tuples,
   block matrix, 132                                                                              orthonormal basis, 489
   coefficient matrix, 143         m × n matrices, 32                 47                          principal axes, 368
   cofactor expansion, 125,        of two vectors, 185, 287        of two vectors, 195         eigenvectors, 147
                                differentiable function, 170,      variances                      eigenvalues, 147
      126, 176                                                                                 electrical networks, 24
   Cramer's Rule, 134                 292, 313, 341, 342            computation of, 278        elementary matrix
   cross product, 204, 207      differential equation of order                                    and inverses, 80
   defined, 69, 124, 130, 176,                                  economic models                   defined, 79
                                      n, 313, 342                  input-output, 109              LU-factorization, 100
      453                       differential equations, 170,                                      operating corresponding
   inductive method of deter-                                   economic system, 109
                                      313, 341                  edges, 66                            to, 79
      mination, 125             differential system, 172        eigenspace, 225, 259, 460,        permutation matrix, 104
   initial development of,                                                                        self-inverse, 80
                                   defined, 170                       509                         Smith normal form, 83
      218                          exponential function, 170    eigenvalues, 147                  uniqueness of reduced
   notation, 124                   general differential sys-
   polynomial interpolation,                                       and determinants, 123             row-echelon form, 84
                                      tems, 171                    and diagonalizable matri-   elementary operations, 5
      142                          general solution, 173                                       elementary row operations
   product of matrices (prod-      simplest differential sys-         ces, 154
                                                                   and eigenspace, 225, 259       corresponding, 79
      uct theorem), 134, 144          tem, 170                     and Google PageRank,           inverses, 7, 80
   similarity invariant, 453    differentiation, 321                                              matrices, 5
   square matrices, 124, 136    digits, 413                           165                         reversed, 7
   theory of determinants, 29   dimension, 235, 299, 342           complex eigenvalues, 152,      scalar product, 18
   triangular matrix, 132       dimension theorem, 319,                                           sum, 18
   Vandermonde determi-                                               261, 382                 elements of the set, 223
                                      330, 337                     complex matrix, 402         ellipse, 426
      nant, 143                 direct proof, 535                  computation of, 379         entries of the matrix, 30
   Vandermonde matrix, 131      direct sum, 308, 461, 462          defined, 147, 256           equal
deviation, 275                  directed graphs, 66                dominant eigenvalue, 161,      columns, 18
diagonal matrices, 147, 153,    direction, 182                                                    complex number, 521
                                direction vector, 189                 379                         fractions, 184
      256, 451                  discriminant, 426, 530             iterative methods, 379         functions, 285
diagonalizable linear opera-    distance, 242, 473                 linear operator, 460           linear transformations,
                                distance function, 339             multiple eigenvalues, 258
      tor, 485                  distance preserving, 212,          multiplicity, 156, 259            322
diagonalizable matrix, 154,                                        power method, 379              matrices, 31
                                      490                          real eigenvalues, 262          polynomials, 285, 547
      256, 451                  distance preserving isome-         root of the characteristic     sequences, 346
diagonalization                                                                                   sets, 223
                                      tries, 490                      polynomial, 148             transformation, 52, 322
   completely diagonalized,     distribution, 436                  solving for, 148            equal modulo, 410
      429                       distributive laws, 35, 62          spectrum of the matrix,     equilibrium, 111
                                division algorithm, 409, 549                                   equilibrium condition, 111
   described, 154, 256          dominant eigenvalue, 161,             366                      equilibrium price structures,
   eigenvalues, 123, 256                                           symmetric linear operator
   example, 146                       379                                                            111
   general differential sys-    dominant eigenvector, 379             on finite dimensional    equivalence relation, 254
                                dot product                           inner product space,     equivalent
      tems, 171                                                       487
   linear dynamical systems,       and cross product, 204,      eigenvector                       statements, 74
                                      208                          basic eigenvectors, 149        systems of linear equa-
      158                                                          complex matrix, 402
   matrix, 148                     and matrix multiplication,      defined, 147, 256                 tions, 4
   multivariate analysis, 437         58                           dominant eigenvector, 379   error, 268, 419
   orthogonal diagonaliza-                                         fractions, 149              error-correcting codes, 408
                                   as inner product, 469           linear combination, 380     Euclid, 536, 539
      tion, 363, 484               basic properties, 195           linear operator, 460        euclidean n-space, 469
   quadratic form, 422             correlation coefficients        nonzero linear combina-
   test, 258                                                          tion, 149
   unitary diagonalization,         computation of, 277            nonzero multiple, 149
                                   defined, 194                    nonzero vectors, 225, 259
      404                          dot product rule, 48, 58        orthogonal basis, 363
diagonalization algorithm,         in set of all ordered n-

      156                             tuples (Rn), 239, 399
diagonalization theorem,
6 INDEX

euclidean algorithm, 411          objective function, 432,     Gram, Jörgen Pederson, 358     image
euclidean geometry, 239              434                       Gram-Schmidt orthogonal-          of linear transformations,
euclidean inner product, 469                                                                        52, 325
Euler's formula, 526              odd function, 503                  ization algorithm, 245,     of the parallelogram, 216
Euler, Leonhard, 526              of a complex variable, 241         358, 365, 367, 376,
evaluation, 157, 291, 321,        pointwise addition, 286            455, 479                 image space, 224, 383
                                  real-valued, 285             graphs                         imaginary axis, 524
      350                         scalar multiplication, 286      attractor, 163              imaginary parts, 342, 521
even function, 503             fundamental identities, 78,        conic, 17                   imaginary unit, 521
even parity, 415                                                  directed graphs, 66         implication, 535
exact formula, 160                   339                          ellipse, 426                implies, 74
expansion theorem, 245,        fundamental subspaces, 390         hyperbola, 426              inconsistent system, 1
                               fundamental theorem, 235,          linear dynamical system,    independence, 230, 295
      478, 485                                                       163                      independence test, 230
expectation, 436                     298                          saddle point, 164           independent, 230, 295, 302
exponential function, 170,     fundamental theorem of al-         trajectory, 163             independent lemma, 302
                                                               Grassmann, Hermann, 281        indeterminate, 285, 547
      343                            gebra, 152, 261, 521      group theory, 29               index, 429
                                                               groups, 492                    induction
factor, 260, 549               Galois field, 412
factor theorem, 309, 549       Galton, Francis, 277            Hamming (7,4)-code, 421           cofactor expansion theo-
feasible region, 432           Gauss, Carl Friedrich, 10,      Hamming bound, 415                   rem, 177
Fibonacci sequence, 169                                        Hamming distance, 413
field, 282, 410                      152                       Hamming weight, 413               determinant
field of integers modulo, 411  gaussian algorithm, 10, 259,    Hamming, Richard, 413              determination of, 125
finite dimensional spaces,                                     heat conduction in solids,
                                     410                                                         mathematical induction,
      302                      gaussian elimination                  501                            87, 542
finite fields, 410                                             Hermite, Charles, 401
finite sets, 410                  defined, 12                  hermitian matrix, 401             on path of length r, 66
fixed axis, 500                   example, 13                  higher-dimensional geome-      induction hypothesis, 542
fixed hyperplane, 500             LU-factorization, 99                                        infinite dimensional, 302
fixed line, 494                   normal equations, 265              try, 29                  initial condition, 171
fixed plane, 497                  scalar multiple, 33          Hilbert spaces, 358            initial state vector, 117
formal proofs, 536                systems of linear equa-      Hilbert, David, 358            inner product
forward substitution, 99                                       hit, 328
Fourier approximation, 502           tions and, 8              homogeneous coordinates,          and norms, 473
Fourier coefficients, 245,     general differential systems,                                     coordinate isomorphism,
                                                                     220
      478, 502, 503                  171                       homogeneous equations                336
Fourier expansion, 245         general solution, 2, 13, 173                                      defined, 469
Fourier series, 504            general theory of relativity,      associated homogeneous         euclidean inner product,
Fourier, J.B.J., 478                                                 system, 46
fractions                            281                                                            469
                               generalized eigenspace, 509        basic solutions, 20            positive definite n × n ma-
   eigenvectors, 149           generalized inverse, 266           defined, 16
   equal fractions, 184        generator, 418                     general solution, 19              trix, 471
   field, 410                  geometric vectors                  linear combinations, 18        properties of, 471
   probabilities, 115                                             nontrivial solution, 16,    inner product space
free variables, 12                defined, 183                                                   defined, 469
function                          described, 184                     148                         distance, 473
   composition, 338               difference, 185                 trivial solution, 16           dot product
   continuous functions, 470      intrinsic descriptions, 184  homogeneous system, 21
   defined, 285                   midpoint, 188                Hooke's law, 317                   use of, 472
   derivative, 342                parallelogram law, 184       Householder matrices, 382         Fourier approximation,
   differentiable function,       Pythagoras' theorem, 193     hyperbola, 426
                                  scalar multiple law, 186,    hyperplanes, 3, 500                  501
      170, 292, 313, 341, 342                                  hypotenuse, 536                   isometries, 490
   equal, 285                        189                       hypothesis, 535                   norms, 473
   even function, 503             scalar multiplication, 186                                     orthogonal diagonaliza-
   exponential function, 170,     scalar product, 186          identity matrix, 45, 49, 104
                                  sum, 185                     identity operator, 320               tion, 484
      343                         tip-to-tail rule, 185        identity transformation, 53       orthogonal sets of vectors,
                                  unit vector, 187
                                  vector subtraction, 186                                           477
                               geometry, 29                                                      unit vector, 473
                               Google PageRank, 165                                           input-output economic mod-
                               Gram matrix, 384
                                                                                                    els, 109
                                                                                                         INDEX

input-output matrix, 110       Jordan blocks, 516                 in space, 189                choice of basis, 453
integers, 409, 535             Jordan canonical form, 515         least squares approximat-
integers modulo, 409           Jordan canonical matrices,                                      defined, 212, 320, 447
integration, 321                                                     ing line, 268
interpolating polynomial,            507                          parametric equations of a    diagonalizable, 485
                               Jordan, Camille, 518
      143                      junction rule, 22, 24                 line, 190                 distance preserving, 212
intersection, 307              juvenile survival rate, 146        perpendicular lines, 194
interval, 285                                                     point-slope formula, 192     distance            preserving
intrinsic descriptions, 183    kernel, 325                        shortest distance between
invariance theorem, 235        kernel lemma, 344                                               isometries, 490
invariant subspaces, 456       Kirchhoff's Laws, 24                  nonparallel lines, 206
invariants, 151                                                   straight, pair of, 426       eigenvalues, 460
inverse theorem, 75            Lagrange identity, 208             through the origin, 224
inverses                       Lagrange interpolation ex-         vector equation of a line,   eigenvector, 460

   adjugate, 137                     pansion, 311, 479               190                       involutions, 464
   and elementary matrices,    Lagrange polynomials, 311,      linear codes, 415
                                                               linear combinations             isometries, 213, 490
      80                             478
   and linear systems, 70      Lagrange, Joseph Louis, 208        and linear transforma-       on finite dimensional in-
   and zero matrices, 68       Lancaster, P., 111                    tions, 86
   cancellation laws, 72       Laplace, Pierre Simon de,                                       ner product space, 484
   complex number, 522                                            defined, 18, 86
   Cramer's Rule, 134, 141           126                          eigenvectors, 380            projection, 213, 362
   defined, 67                 law of cosines, 196                homogeneous equations,
   determinants, 124, 134      law of exponents, 526                                           properties of matrices,
   elementary row opera-       leading 1, 8                          18
                               leading coefficient, 284, 547      of columns of coefficient    451
      tions, 7, 80             leading column, 101
   finite fields, 410          leading variables, 12                 matrix, 42                reducible, 464
   generalized inverse, 266    least squares approximating        of orthogonal basis, 245
   inverse theorem, 75                                            of solutions to homoge-      reflections, 213
   inversion algorithm, 71           line, 268
   linear transformation, 77,  least squares approximating           neous system, 21          restriction, 458
                                                                  spanning sets, 226, 292
      339                            polynomial, 270              trivial, 230, 295            rotations, 215
   matrix transformations,     least squares approximation,       unique, 230
                                                                  vanishes, 230                standard matrix, 447
      77                             267                          vectors, 292
   nonzero matrix, 68          least squares best approxi-     linear discrete dynamical       symmetric, 487
   properties of inverses, 72
   square matrices                   mation, 273                     system, 147               transformations of areas
                               left cancelled invertible ma-   linear dynamical system,
    application to, 71, 135                                                                    and volumes, 216
inversion algorithm, 71              trix, 72                        147, 158
invertibility condition, 135   Legendre polynomials, 480       linear equation                 linear programming, 435
invertible matrix              Legendre, A.M., 480
                               Leibnitz, 123                      conic graph, 17              linear recurrence relation,
   "mixed" cancellation, 72    lemma, 80                          constant term, 1
   defined, 67                 length                             Cramer's Rule, 140           166, 346
   determinants, 124                                              defined, 1
   left cancelled, 72             geometric vector, 183           vs. linear inequalities, 15  linear recurrences
   LU-factorization, 106          linear recurrence, 346       linear independence
   orthogonal matrices, 377       linear recurrence relation,     dependent, 232, 295, 305     diagonalization, 168
   product of elementary ma-                                      geometric description,
                                     166                                                       length, 166, 346
      trix, 81                    norm, where dot product            232
   right cancelled, 72                                            independent, 230, 295,       linear transformations,
involutions, 464                     is used, 400
irreducible, 530                  norm, where dot product            302                       345
isometries, 213, 490                                              orthogonal sets, 244, 478
isomorphic, 334                      used, 473                    properties, 297              polynomials associated
isomorphism, 334, 445, 492        path of length, 66              set of vectors, 230, 295
                                  recurrence, 346                 vector spaces, 295           with the linear recur-
                                  vector, 181, 239, 400        linear inequalities, 15
                               Leontief, Wassily, 109          linear operator                 rence, 348
                               line                               B-matrix, 447
                                  fixed line, 494                 change matrix, 448           shift operator, 350

                                                                                               vector spaces, 345

                                                                                               linear system of differential

                                                                                               equations, 172

                                                                                               linear transformations

                                                                                               m × n matrix, 439

                                                                                               action of a transformation,

                                                                                               440

                                                                                               as category, 339

                                                                                               as matrix transformation,

                                                                                               439

                                                                                               association with matrix,

                                                                                               439

                                                                                               composite, 56, 91, 338

                                                                                               composition, 338, 444

                                                                                               coordinate transforma-

                                                                                               tion, 441

                                                                                               defined, 52, 86, 320, 324

                                                                                               described, 29, 86

                                                                                               differentiation, 321

                                                                                               dimension theorem, 330
8 INDEX

   distance preserving, 212      Markov, Andrei Andreye-            positive, 393                matrix form
   equal, 52, 322                      vich, 114                    positive semi-definite, 393     defined, 43
   evaluation, 321, 350                                             pseudoinverse, 395              reduced row-echelon
   examples, 320                 mathematical induction, 87,        rank, 14, 247                      form, 9, 84
   fundamental identities,             542                          reduced row-echelon ma-         row-echelon form, 8, 9
                                                                                                    upper Hessenberg form,
      339                        mathematical statistics, 275          trix, 9, 84                     382
   hit, 328                      matrices, 30                       regular stochastic matrix,
   identity operator, 320        matrix, 30                                                      matrix generators, 417
   image, 52, 325                                                      120                       matrix inversion algorithm,
   in computer graphics, 219        (i, j)-entry, 30                row matrix, 30
   integration, 321                 adjacency matrix, 66            row-echelon matrix, 8, 9,          71, 82
   inverses, 77, 339                augmented matrix, 3, 4,                                      matrix multiplication
   isomorphism, 334                                                    247
   kernel, 325                         12                           shapes, 30                      and composition of trans-
   linear recurrences, 345          block matrix, 64                similar matrices, 254              formations, 56
   matrix of T corresponding        change matrix, 448              singular matrix, 386
                                    coefficient matrix, 4, 140      spectrum, 366                   associative law, 62
      to the ordered bases B        column matrix, 30               standard generator, 418         block, 64
      and D, 441                    congruent matrices, 428         standard matrix, 447            commute, 60, 63
   matrix of a linear transfor-     conjugate matrix, 262           stochastic matrices, 110,       compatibility rule, 59
      mation, 320, 440              constant matrix, 4                                              definition, 57
   matrix transformation in-        covariance matrix, 437             117                          directed graphs, 66
      duced, 52, 212                defined, 3, 30                  subtracting, 32                 distributive laws, 62
   matrix transformations           demand matrix, 112              systemic generator, 418         dot product rule, 58
    another perspective on,         diagonal matrices, 147,         transition matrix, 116          left-multiplication, 74
      86                                                            transpose, 36                   matrix of composite of
   nullity, 327                        153, 451                     triangular matrices, 99,
   nullspace, 325                   diagonalizable matrix,                                             two linear transforma-
   of areas, 216                                                       132                             tions, 91
   of volume, 216                      154                          unitary matrix, 403             matrix products, 57
   one-to-one transforma-           diagonalizing matrix, 154       upper triangular matrix,        non-commutative, 74
      tions, 328                    elementary matrix, 79                                           order of the factors, 62
   onto transformations, 328        elementary row opera-              99, 132, 404                 results of, 58
   projections, 97, 213                                             Vandermonde matrix, 131         right-multiplication, 74
   properties, 322                     tions, 5                     zero matrix, 32              matrix of T corresponding to
   range, 325                       entries of the matrix, 30       zeros, creating in matrix,         the ordered bases B and
   rank, 327                        equal matrices, 31                                                 D, 441
   reflections, 96, 213             Gram, 384                          128                       matrix of a linear transfor-
   rotations, 94, 215               hermitian matrix, 401        matrix addition, 31                   mation, 440
   scalar multiple law, 93          Householder matrices,        matrix algebra                  matrix recurrence, 147
   scalar operator, 320                                                                          matrix theory, 29
   zero transformation, 53,            382                          dot product, 47              matrix transformation in-
      320                           identity matrix, 45, 49         elementary matrix, 79              duced, 52, 86, 212
linearly dependent, 232, 295,       input-output matrix, 110        input-output economic        matrix transformations, 77
      305                           invertible matrix, 67                                        matrix-vector products, 42
linearly independent, 230,          linear transformation              models                    mean
      295, 302                                                       application to, 109            "average" of the sample
logically equivalent, 538            association with, 439          inverses, 67                       values, 275
lower reduced, 100                  lower triangular matrix,        LU-factorization, 101           calculation, 436
lower triangular matrix, 99,                                        Markov chains                   sample mean, 275
      132                              99, 132                       application to, 114         messages, 417
LU-algorithm, 101, 107              migration, 160                  matrices as entities, 33     metric, 339
LU-factorization, 101               Moore-Penrose inverse,          matrix addition, 31          midpoint, 188
                                                                    matrix multiplication, 55    migration matrix, 160
magnitude, 183                         395                          matrix subtraction, 32       minimum distance, 414
main diagonal, 38, 99               nullity, 250                    matrix-vector multiplica-    modular arithmetic, 409
Markov chains, 114                  orthogonal matrix, 137,            tion, 42, 55              modulo, 409
                                                                    numerical division, 67       modulus, 409, 523
                                       364                          scalar multiplication, 33    Moore-Penrose inverse, 395
                                    orthonormal matrix, 364         size of matrices, 30
                                    over finite field, 418          transformations, 51
                                    parity-check matrices,          transpose of a matrix, 36
                                                                    usefulness of, 29
                                       418
                                    partitioned into blocks, 64
                                    permutation matrix, 104
                                    polar decomposition, 393
                                                                                              INDEX

morphisms, 339                 orthogonal basis, 356, 479     orthogonally diagonalizable,       defined, 284, 547
multiplication                 orthogonal codes, 420                365                          degree of the polynomial,
                               orthogonal complement,
   block multiplication, 64                                   orthonormal basis, 480, 486           284, 547
   compatible, 59, 65                359, 420, 481            orthonormal matrix, 364            distinct degrees, 297
   matrix multiplication, 55   orthogonal diagonalization,    orthonormal set, 403, 477          division algorithm, 549
   matrix-vector multiplica-                                  orthonormal vector, 243            equal, 285, 547
                                     363, 484                                                    evaluation, 157, 291
      tion, 41                 orthogonal hermitian matrix,   PageRank, 165                      factor theorem, 549
   matrix-vector products, 42                                 paired samples, 277                form, 547
   scalar multiplication, 33,        403                      parabola, 426                      indeterminate, 547
                               orthogonal lemma, 356, 479     parallel, 189                      interpolating the polyno-
      282, 285                 orthogonal matrix, 137, 364    parallelepiped, 210, 217
multiplication rule, 527       orthogonal projection, 361,    parallelogram                         mial, 142
multiplicity, 156, 259, 350,                                                                     Lagrange polynomials,
                                     482                         area equal to zero, 209
      549                      orthogonal set of vectors,        defined, 93, 184                   311, 478
multivariate analysis, 437                                       determined by geometric         leading coefficient, 284,
                                     403, 477
nearest neighbour decoding,    orthogonal sets, 243, 355            vectors, 184                    547
      414                      orthogonal vectors, 197,          image, 216                      least squares approximat-
                                                                 law, 93, 184, 525
negative                             243, 403, 477               rhombus, 198                       ing polynomial, 270
   correlation, 276            orthogonality                  parameters, 2, 12                  Legendre polynomials,
   of m × n matrix, 32                                        parametric equations of a
   vector, 40, 283                complex matrices, 398                                             480
                                  constrained optimization,         line, 190                    nonconstant polynomial
negative x, 40                                                parametric form, 2
negative x-shear, 54                 432                      parity digits, 418                    with complex coeffi-
network flow, 22                  dot product, 239            parity-check code, 415                cients, 261
Newton, Sir Isaac, 10             eigenvalues, computation    parity-check matrices, 418         remainder theorem, 548
Nicholson, W. Keith, 411                                      particle physics, 434              root, 148, 379, 521
nilpotent, 517                       of, 379                  partitioned into blocks, 64        root of characteristic poly-
noise, 408                        expansion theorem, 478      path of length, 66                    nomial, 148, 313
nonleading variable, 17           finite fields, 410          Peano, Guiseppe, 281               Taylor's theorem, 309
nonlinear recurrences, 169        Fourier expansion, 245      Pearson correlation coeffi-        vector spaces, 284, 308
nontrivial solution, 16, 148      Gram-Schmidt orthogo-                                          with no root, 549, 550
nonzero scalar multiple of a                                        cient, 277                   zero polynomial, 547
                                     nalization algorithm,    perfect code, 415               position vector, 185
      basic solution, 21             358, 365, 367, 376,      period, 317                     positive x-shear, 54
nonzero vectors, 189, 241            455, 479                 permutation matrix, 104         positive correlation, 276
norm, 400, 473                    normalizing the orthogo-    perpendicular lines, 194        positive definite, 371
normal, 201, 406                     nal set, 243             physical dynamics, 407          positive definite matrix, 371,
normal equations, 265             orthogonal codes, 420       pigeonhole principle, 538             471
normalizing the orthogonal        orthogonal complement,      Pisano, Leonardo, 169           positive matrix, 393
                                     359, 420                 planes, 201, 224                positive semi-definite ma-
      set, 243, 477               orthogonal diagonaliza-     Pless, V., 419                        trix, 393
null space, 224                      tion, 363                PLU-factorization, 106          positive semidefinite, 437
nullity, 250, 327                 orthogonal projection,      point-slope formula, 192        power method, 379, 380
nullspace, 325                       361                      pointwise addition, 285, 286    power sequences, 346
                                  orthogonal sets, 243, 355   polar decomposition, 393        practical problems, 1
objective function, 432, 434      positive definite matrix,   polar form, 525                 prime, 410, 538
objects, 339                         371                      polynomials                     principal argument, 525
odd function, 503                 principal axes theorem,                                     principal axes, 366, 424
Ohm's Law, 24                        365                         as matrix entries and de-    principal axes theorem, 365,
one-to-one transformations,       projection theorem, 265           terminants, 130                 403, 406, 433, 488
                                  Pythagoras' theorem, 244                                    principal components, 437
      328                         QR-algorithm, 381              associated with the linear   principal submatrices, 372
onto transformations, 328         QR-factorization, 375             recurrence, 348           probabilities, 115
open model of the economy,        quadratic forms, 368, 422                                   probability law, 436
                                  real spectral theorem, 366     coefficients, 272, 284, 547  probability theory, 437
      112                         statistical principal com-     complex roots, 152, 402,     product
open sector, 112                     ponent analysis, 436
ordered n-tuple, 40, 234          triangulation theorem,            550
ordered basis, 440, 441              370                         constant, 547
origin, 181
6 INDEX

   complex number, 524         real Jordan canonical form,       fixed axis, 500                  preserving scalar multi-
   determinant of product of         517                         isometries, 493                    plication, 86
                                                                 linear operators, 215           vectors, 282
      matrices, 134            real numbers, 1, 40, 282,         linear transformations, 94   scalar operator, 320
   dot product, 194                  285, 398, 402, 410       round-off error, 149            scalar product
   matrix products, 57                                        row matrix, 30                     defined, 195
   matrix-vector products, 42  real parts, 342, 521           row space, 246                     elementary row opera-
   scalar product, 195         real quadratic, 530            row-echelon form, 8, 9                tions, 18
   standard inner product,     real spectral theorem, 366     row-echelon matrix, 8, 9           geometric vectors, 186
                               recurrence, 166                rows                            scatter diagram, 276
      399                      recursive algorithm, 10           (i, j)-entry, 30             Schmidt, Erhardt, 358
   theorem, 134, 144           recursive sequence, 166           as notations for ordered n-  Schur's theorem, 405
product rule, 343              reduced row-echelon form,                                      Schur, Issai, 404
projection                                                          tuples, 234               second-order differential
   linear operators, 213             9, 84                       convention, 30                     equation, 313, 342
   orthogonal projection,      reduced row-echelon matrix,       elementary row opera-        Seneta, E., 111
                                                                                              sequence
      361, 482                       9                              tions, 5                     Fibonacci, 169
projection on U with kernel    reducible, 464                    leading 1, 8                 sequences
                               reduction to cases, 537           shape of matrix, 30             constant sequences, 346
     W , 481                   reflections                       Smith normal form, 83           equal, 346
projection theorem, 265,                                         zero rows, 8                    Fibonacci, 169
                                  about a line through the                                       linear recurrences, 166
      361, 482                       origin, 137              saddle point, 164                  notation, 345
projections, 97, 198, 359                                     same action, 52, 285, 322          of column vectors, 147
proof                             fixed hyperplane, 500       sample                             ordered sequence of real
                                  fixed line, 494                                                   numbers, 40
   by contradiction, 537          fixed plane, 497               analysis of, 275                power sequences, 346
   defined, 535                   isometries, 493                comparison of two sam-          recursive sequence, 166
   direct proof, 535              linear operators, 213                                          satisfy the relation, 346
   formal proofs, 536             linear transformations, 96        ples, 276                 set, 223
   reduction to cases, 537     regular stochastic matrix,        defined, 275                 set notation, 224
proper subspace, 223, 238                                        paired samples, 277          set of all ordered n-tuples
pseudoinverse, 395                   120                      sample correlation coeffi-            (Rn)
pure imaginary numbers,        remainder, 409                                                    n-tuples, 246
                               remainder theorem, 309, 548          cient, 277                   as inner product space,
      521                      repellor, 163                  sample mean, 275                      469
Pythagoras, 193, 536           reproduction rate, 146         sample standard deviation,         closed under addition and
Pythagoras' theorem, 193,      restriction, 458                                                     scalar multiplication,
                               reversed, 7                          276                             40
      196, 244, 477, 536       rhombus, 198                   sample variance, 276               complex eigenvalues, 261
                               right cancelled invertible     sample vector, 275                 dimension, 235
QR-algorithm, 381                                             satisfy the relation, 346          dot product, 239, 399
QR-factorization, 375                matrix, 72               scalar, 34, 282, 410               expansion theorem, 245
quadratic form, 368, 422,      right-hand coordinate sys-     scalar equation of a plane,        linear independence, 230
                                                                                                 linear operators, 212
      472                            tems, 211                      201                          notation, 40
quadratic formula, 530         right-hand rule, 211           scalar multiple law, 93, 186,      orthogonal sets, 243
quotient, 409                  root                                                              projection on, 362
                                                                    189                          rank of a matrix, 247
radian measure, 93, 525           of characteristic polyno-   scalar multiples, 18, 33, 92       rules of matrix arithmetic,
random variable, 436                 mial, 148, 313, 314      scalar multiplication                 283
range, 325                                                                                       similar matrices, 254
rank                              of polynomials, 291, 379,      axioms, 283                     spanning sets, 226
                                     521                         basic properties, 287           special types of matrices,
   linear transformation,                                        closed under, 40, 283
      327, 445                    of the quadratic, 530          closed under scalar multi-
                               roots of unity, 528
   matrix, 14, 247, 327        rotation, 500                        plication, 223
   quadratic form, 429         rotations                         described, 33
   similarity invariant, 453                                     distributive laws, 35
   symmetric matrix, 429          about a line through the       geometric vectors, 186
   theorem, 248                      origin, 454                 geometrical description,
rational numbers, 282
Rayleigh quotients, 380           about the origin                  92
real axis, 524                     and orthogonal matrices,      of functions, 286
                                     137                         transformations

                                  axis, 497
                                  describing rotations, 94
                                                                                              INDEX 6

      283                          elementary matrix, 79          sum, 307                       coefficient matrix, 4
   standard basis, 88              hermitian matrix, 401          vector spaces, 290             consistent system, 1, 15
   subspaces, 223, 283             identity matrix, 45, 49        zero subspace, 223, 290        constant matrix, 4
   symmetric matrix, 262           invariants, 151             subtraction                       defined, 1
Shannon, Claude, 413               lower triangular matrix,       complex number, 521            electrical networks
shift operator, 350                                               matrix, 32
shifting, 382                         132                         vector, 186                     application to, 24
sign, 125                          matrix of an operator, 451  sum                               elementary operations, 5
similar matrices, 254              orthogonal matrix, 137         algebraic sum, 24              equivalent systems, 4
similarity invariant, 453          positive definite matrix,      complex number, 525            gaussian elimination, 8,
simple harmonic motions,                                          direct sum, 308, 461, 462
                                      371, 471                    elementary row opera-             12
      316                          similarity invariant, 453                                     general solution, 2
simplex algorithm, 15, 435         trace, 255                        tions, 18                   homogeneous equations,
sine, 93, 525                      triangular matrix, 132         geometric vectors, 185
single vector equation, 41         unitary matrix, 403            geometrical description,          16
singular matrix, 386               upper triangular matrix,                                      inconsistent system, 1
singular value decomposi-                                            92                          infinitely many solutions,
                                      132                         matrices of the same size,
      tion, 383, 389            staircase form, 9                                                   3
singular values, 385            standard basis, 88, 228, 231,        31                          inverses and, 70
size m × n matrix, 30                                             matrix addition, 31            matrix multiplication, 61
skew-symmetric, 461                   235, 237, 299, 399          of product of matrix en-       network flow application,
Smith normal form, 83           standard deviation, 436
solution                        standard generator, 418              tries, 130                     22
                                standard inner product, 399       of scalar multiples, 18        no solution, 3
   algebraic method, 4, 8       standard matrix, 447              of two vectors, 282            nontrivial solution, 16
   basic solutions, 20, 21,     standard position, 93, 525        of vectors in two sub-         normal equations, 265
                                state vectors, 117                                               positive integers, 27
      392                       statistical principal compo-         spaces, 461                 rank of a matrix, 15
   best approximation to,                                         subspaces, 307                 solutions, 1
                                      nent analysis, 436          subspaces of a vector          trivial solution, 16
      264                       steady-state vector, 120                                         unique solution, 3
   consistent system, 1, 15     stochastic matrices, 110, 117        space, 307                  with m × n coefficient ma-
   general solution, 2, 13      structure theorem, 495            variances of set of random
   geometric description, 3     subset, 223                                                         trix, 42
   in parametric form, 2        subspace test, 290                   variables, 437           systematic generator, 418
   inconsistent system, 1       subspaces                      summation notation, 176
   nontrivial solution, 16,                                    Sylvester's Law of Inertia,    tail, 183
                                   m × n matrix, 224                                          Taylor's theorem, 309, 450
      148                          basis, 235                        429                      theorems, 539
   solution to a system, 1, 12     closed under addition, 223  symmetric form, 423            theory of Hilbert spaces, 358
   to linear equation, 1           closed under scalar multi-  symmetric linear operator,     third-order differential equa-
   trivial solution, 16
solution to a system, 1               plication, 223                 487                            tion, 313, 342
span, 226, 292                     column space, 383           symmetric matrix               time, functions of, 147
spanning sets, 226, 292            defined, 223, 290                                          tip, 183
spectral theorem, 405              dimension, 235                 absolute value, 262         tip-to-tail rule, 185
spectrum, 366                      eigenspace, 259                congruence, 428             total variance, 437
sphere, 434                        fundamental, 390               defined, 38                 trace, 255, 320, 453
spring constant, 317               fundamental theorem, 235       index, 429                  trajectory, 163
square matrix (n × n matrix)       image, 325, 383                orthogonal eigenvectors,    transformations
   characteristic polynomial,      intersection, 307
                                   invariance theorem, 235           365                         action, 52, 440
      148, 407                     invariant subspaces, 456       positive definite, 371         composite, 56
   cofactor matrix, 137            kernel, 325                    rank and index, 429            defining, 52
   defined, 30                     planes and lines through       real eigenvalues, 262          described, 52
   determinants, 69, 124, 136                                  syndrome, 419                     equal, 52
   diagonal matrices, 147             the origin, 224          syndrome decoding, 419            identity transformation,
   diagonalizable matrix,          projection, 361             system of linear equations
                                   proper subspace, 223           algebraic method, 4               53
      154, 451                     spanning sets, 226             associated homogeneous         matrix transformation, 52
   diagonalizing matrix, 154       subspace test, 290                                            zero transformation, 53
                                                                     system, 46               transition matrix, 116
                                                                  augmented matrix, 3
                                                                  chemical reactions

                                                                   application to, 26
6 INDEX

transition probabilities, 115,  vector equation of a plane,           281                             403
      116                             202                          isomorphic, 334                 position vector, 185
                                                                   linear independence, 295        sample vector, 275
translation, 54, 490            vector geometry                    linear recurrences, 345         scalar multiplication, 282
transpose of a matrix, 36          angle between two vec-          linear transformations,         single vector equation, 41
transposition, 36, 321                tors, 196                                                    state vector, 115
triangle                           computer graphics, 219             320                          steady-state vector, 120
                                   cross product, 204              polynomials, 284, 308           subtracted, 287
   hypotenuse, 536                 defined, 181                    scalar multiplication           sum of two vectors, 282
   inequality, 242, 523            direction vector, 189                                           unit vector, 187, 240, 400
triangle inequality, 475, 523      line perpendicular to            basic properties of, 287       zero n-vector, 40
triangular matrices, 99, 132          plane, 194                   spanning sets, 292              zero vector, 223, 283
triangulation algorithm, 510       linear operators, 212           subspaces, 290, 307          velocity, 183
triangulation theorem, 370         lines in space, 189             theory of vector spaces,     vertices, 66
trigonometric functions, 93        planes, 201                                                  vibrations, 434
trivial linear combinations,       projections, 199                   286                       volume
                                   vector equation of a line,      zero vector space, 289          linear transformations of,
      230, 295                        190                       vectors
trivial solution, 16                                               addition, 282                      216
                                vector product, 204                arrow representation, 51        of parallelepiped, 210,
uncorrelated, 437               vector quantities, 183             column vectors, 147
unit ball, 434, 473             vector spaces                      complex matrices, 398              217
unit circle, 93, 473, 525                                          coordinate vectors, 187,
unit cube, 218                     3-dimensional space, 181                                     Weyl, Hermann, 281
unit square, 218                   abstract, 281                      204, 218, 440             whole number, 535
unit vector, 187, 240, 400,        as category, 339                defined, 40, 282, 398        Wilf, Herbert S., 165
                                   axioms, 283, 286, 288           difference of, 287           words, 413
      473                          basic properties, 282           direction of, 182
unitarily diagonalizable, 404      basis, 298                      direction vector, 189        zero n-vector, 40
unitary diagonalization, 404       cancellation, 286               initial state vector, 117    zero matrix
unitary matrix, 403                continuous functions, 470       intrinsic descriptions, 183
upper Hessenberg form, 382         defined, 282                    length, 183, 239, 400, 473      described, 32
upper triangular matrix, 99,       differential equations, 313     matrix recurrence, 147          no inverse, 68
                                   dimension, 299                  matrix-vector multiplica-       scalar multiplication, 34
      132, 404                     direct sum, 461                                              zero polynomial, 284
                                   examples, 282                      tion, 41                  zero rows, 8
Vandermonde determinant,           finite dimensional spaces,      matrix-vector products, 42   zero subspace, 223, 290
      143                                                          negative, 40                 zero transformation, 53, 320
                                      302                          nonzero, 189                 zero vector, 223, 283
Vandermonde matrix, 131            infinite dimensional, 302       orthogonal vectors, 197,     zero vector space, 289
variance, 275, 436                 introduction of concept,
variance formula, 279                                                 243, 403, 477
vector addition, 282, 525                                          orthonormal vector, 243,
vector equation of a line, 190
www.vrettalyryx.com
   www.lyryx.com
