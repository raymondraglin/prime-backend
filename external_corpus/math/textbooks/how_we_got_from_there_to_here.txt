 How We Got From There To
Here: A Story of Real Analysis
 How We Got From There To
Here: A Story of Real Analysis

                       Robert Rogers
                     SUNY, Fredonia
               Fredonia, New York, USA

                    Eugene C. Boman
            Penn State, Harrisburg campus

                  Harrisburg, PA, USA
Edition: 2.0.2
2014 Robert Rogers and Eugene Boman
How We Got from There to Here: A Story of Real Analysis by Robert Rogers
and Eugene Boman, is licensed under a Creative Commons Attribution-NonCommercial-
ShareAlike 4.0 International License1.

   1creativecommons.org/licenses/by-nc-sa/4.0/
Acknowledgements

Mactutor  While we have tried to tell the story of the development
  Desmos  of Real Analysis as completely as possible, our overriding
          goal was always to teach mathematics, not history. Thus
          we have necessarily left the history incomplete.
          The interested student can fill in the gaps we have left by
          making use of the extensive resources that can be found
          at the MacTutor2 history of mathematics repository.
          All of the portraits of mathematicians used in this text
          have been taken from MacTutor.
          MacTutor was created, and is maintained by Profes-
          sor Edmund Robertson (Emeritus), and Professor John
          O'Connor (Emeritus), both of the University of St. An-
          drews in Scotland.

          The interactive figures in Chapter 5 were created using
          the Desmos online calculator3.

2mathshistory.st-andrews.ac.uk
3www.desmos.com

                                               iv
To the Instructor

The irony of this section is that it exists to tell you that this book was not
written for you; it was written for your students. After all, we don't need to
teach you about Real Analysis. You already understand the subject. The pur-
pose of this text is to help your students make sense of the formal definitions,
theorems, and proofs that they will encounter in your course. We do this by im-
mersing the student in the story of how what is usually called Calculus evolved
into modern Real Analysis. Our hope and intention is that this will help the
student to appreciate why their intuitive understanding of topics encountered
in calculus needs to be replaced by the formalism of Real Analysis.

    The traditional approach to this topic (what we might call the "logical"
story of Real Analysis), starts with a rigorous development of the real num-
ber system and uses this to provide rigorous definitions of limits, continuity,
derivatives and integrals, and convergence of series; typically in that order.
This is a perfectly legitimate story of Real Analysis and, logically, it makes
the most sense. Indeed, this is a view of the subject that every mathematician-
in-training should eventually attain. However, it is our contention that your
students will appreciate the subject more, and hopefully retain it better, if they
see how the subject developed from the intuitive notions of Leibniz, Newton
and others in the seventeenth and eighteenth centuries to the more modern
approach developed in the nineteenth and twentieth centuries. After all, they
are coming at it from a viewpoint very similar to that of the mathematicians
of the seventeenth and eighteenth centuries. Our goal is to bring them into
the nineteenth and early twentieth centuries, mathematically speaking.

    We hasten to add that this is not a history of analysis book. It is an
introductory textbook on Real Analysis which uses the historical context of
the subject to frame the concepts and to show why mathematicians felt the
need to develop rigorous, non-intuitive definitions to replace their intuitive
notions.

    You will notice that most of the problems are embedded in the chapters,
rather than lumped together at the end of each chapter. This is done to
provide a context for the problems which, for the most part, are presented on
an as-needed basis.

    Thus the proofs of nearly all of the theorems appear as problems in the
text. Of course, it would be very unfair to ask most students at this level to
prove, say, the Bolzano-Weierstrass Theorem without some sort of guidance.
So in each case we provide an outline of the proof and the subsequent problem
will be to use the outline to develop a formal proof. Proof outlines will become
less detailed as the students progress. We have found that this approach helps
students develop their proof writing skills.

    We state in the text, and we encourage you to emphasize to your students,
that often they will use the results of problems as tools in subsequent problems.

                                                  v
                                             vi

Trained mathematicians do this naturally, but it is our experience that this is
still somewhat foreign to students who are used to simply "getting the problem
done and forgetting about it."

    The problems range from the fairly straightforward to the more challenging.
Some of them require the use of a computer algebra system (for example, to plot
partial sums of a power series). These tend to occur earlier in the book where
we encourage the students to use technology to explore the wonders of series.
A number of these problems can be done on a sufficiently advanced graphing
calculator or even on Wolfram Alpha, so you should assure your students that
they do not need to be super programmers to do this. Of course, this is up to
you.

    A testing strategy we have used successfully is to assign more time consum-
ing problems as collected homework and to assign other problems as possible
test questions. Students could then be given some subset of these (verbatim)
as an in-class test. Not only does this make test creation more straightforward,
but it allows the opportunity to ask questions that could not reasonably be
asked otherwise in a timed setting. Our experience is that this does not make
the tests "too easy," and there are worse things than having students study
by working together on possible test questions beforehand. If you are shocked
by the idea of giving students all of the possible test questions ahead of time,
think of how much (re)learning you did studying the list of possible questions
you knew would be asked on a qualifying exam.

    In the end, use this book as you see fit. We believe your students will find
it readable, as it is intended to be, and we are confident that it will help them
to make sense out of the rigorous, non-intuitive definitions and theorems of
Real Analysis and help them to develop their proof-writing skills.

    If you have suggestions for improvement, comments or criticisms of our text
please contact us at the email addresses below. We appreciate any feedback
you can give us on this.

    Thank you.

Robert R. Rogers            Eugene C. Boman
robert.rogers@fredonia.edu  ecb5@psu.edu
Contents

Acknowledgements                              iv

To the Instructor                             v

I In Which We Raise A Number Of Questions

1 Prologue: Three Lessons Before We Begin     2

2 Numbers, Real (R) and Rational (Q)          10

3 Calculus in the 17th and 18th Centuries     18

4 Questions Concerning Power Series           40

II Interregnum

5 Joseph Fourier: The Man Who Broke Calculus  52

III In Which We Find (Some) Answers

6 Convergence of Sequences and Series         61

7 A "Tayl" of Three Remainders                74

8 Continuity: What It Isn't and What It Is    85

9 Intermediate and Extreme Values             108

                   vii
CONTENTS                     viii

10 Back to Power Series      122

11 Back to the Real Numbers  134

12 Epilogues                 147

Back Matter

Bibliography                 167

Index                        169
           Part I
In Which We Raise A
Number Of Questions

                                       1
Chapter 1

Prologue: Three Lessons Be-
fore We Begin

1.1 Lesson One

Get a pad of paper and write down the answer to this question: What is . . . No,
really. We're serious. Get a writing pad. We'll wait.
Comment. We really are serious about this. Get a pad of paper!

    Got it? Good. Now write down your answer to this question: What is a
number? Don't think about it. Don't analyze it. Don't consider it. Just write
down the best answer you can without thinking. You are the only person who
ever needs to see what you've written.

    Done? Good.
    Now consider this: All of the objects listed below are "numbers" in a sense
we will not make explicit here. How many of them does your definition include?

   1
    -1
   0
    3/5

       
   2

       
    -1
    ii
    e5i
    4 + 3i - 2j + 6k (this is called a quaternion)
    dx (this is the differential you learned all about in calculus)
    1 2 (yes, matrices can be considered numbers).

         -2 1

    Surely you included 1. Almostsurely you included 3/5. But what about 0?
-1? Does your definition include 2? Do you consider dx a number? Leibniz
did. Any of the others? (And, yes, they really are all "numbers.")

                                                  2
CHAPTER 1. PROLOGUE: THREE LESSONS BEFORE WE BEGIN 3

    The lesson in this little demonstration is this: You don't really have a clear
notion of what we mean when we use the word "number." And this is fine. Not
knowing is acceptable.

Comment. Sometimes it is even encouraged.

    A principal goal of this course of study is to rectify this, at least a little bit.
When the course is over you may or may not be able to give a better definition
of the word "number" but you will have a deeper understanding of the real
numbers at least. That is enough for now.

1.2 Lesson Two

Read and understand the following development of the Quadratic Formula.
    Suppose a = 0. If

                            ax2 + bx + c = 0                                                    (I.1)

then

                                        x2 + b x = - c (I.2)
                                        a                a

Now   let  x  =  y-      b  giving
                        2a

                                           2             c b2                                   (I.3)
                                        y =- + 2
                                                         a 4a
                                           y =  b2 - 4ac (I.4)
                                                            2a
                                           x = -b  b2 - 4ac (I.5)
                                                              2a

Were you able to follow the argument? Probably the step from equa-

tion (I.1) to equation (I.2) presented no difficulties. But what about the next

step? Do you see where equation (I.3) came from? If so, good for you. Most

students, in fact most mathematicians, cannot make that step in their heads.

But are you sure? Is there, perhaps, some small detail you've overlooked?

Check to see.

That       is,  let  x  =   y-   b  in  equation  (I.2)  and  see  if  you  can  get  equation  (I.3).
                                2a
Do it on that handy pad of paper we told you to get out earlier. Do it now.

We'll wait.

Comment. If you still haven't gotten out a pad of paper, give up now. You're
going to fail this course. Seriously. Do you think we would spend so much time
on this, that we would repeat it so many times, if it weren't important. GET
OUT A PAD OF PAPER NOW! Last chance. You've been warned.

    Done? Good.
    Perhaps you haven't been able to fill in the details on your own. That's
ok. Many people can't. If not then get help: from a classmate, a friend, your
instructor, whomever. Unfortunately most people won't get help in this situa-
tion. Instead they will perceive this as "failure," hide it and berate themselves
or the problem as "stupid." In short they will let their personal insecurities
and demons overwhelm them. Don't do this. Get help. You are neither dumb
nor incapable. There are a thousand reasons that on any given day you might
not be able to solve this problem. But don't let a bad day interfere with the
education you are here for. Get someone to help you over this hump. Later
you will be able to help your helper in the same way. Really.
CHAPTER 1. PROLOGUE: THREE LESSONS BEFORE WE BEGIN 4

    At this point we assume that you've successfully negotiated the transition
from equation (I.2) to equation (I.5).

    See? It really wasn't that bad after all. Just a lot of elementary algebra.
Now that you've done it (or seen it done), it is easy to see that there really
wasn't much there.

    But this is the point! We left those computations out precisely because we
knew that they were routine and that you could fill in the details. Moreover,
filling in the details yourself gives you a little better insight into the computa-
tions. If we'd filled them in for you we would have robbed you of that insight.
And we would have made this book longer than it needs to be. We don't want
to do either of those things. If we fill in all of the details of every computation
for you, you won't learn to have confidence in your ability to do them yourself.
And this book will easily double in length.

    So the lesson here is this: Keep that pad of paper handy whenever you
are reading this (or any other) mathematics text. You will need it. Routine
computations will often be skipped. But calling them "routine" and skipping
them does not mean that they are unimportant. If they were truly unimportant
we would leave them out entirely.

    Moreover "routine" does not mean "obvious." Every step we took in the
development of the Quadratic Formula was "routine." But even routine com-
putations need to be understood and the best way to understand them is to
do them. This is the way to learn mathematics; it is the only way that really
works. Don't deprive yourself of your mathematics education by skipping the
most important parts.

Comment. If you didn't fill in those details then you are depriving yourself
of the education you are here to obtain. This is sad. There is a good reason
for putting these three lessons first. Stop wasting your time and intellect! Go
do it now.

As you saw when you filled in the details of our development of the Qua-

dratic  Formula  the  substitution  x  =  y  -   b  was   crucial  because  it  turned
                                                2a

                                        x2 + b x + c = 0
                                               aa

into
                                               y2 = k

where k depends only on a, b, and c. In the sixteenth century a similar tech-

nique was used by Ludovico Ferrari (1522-1565) to reduce the general cubic

equation

                      ax3 + bx2 + cx + d = 0                                            (I.6)

into the so-called "depressed cubic"

                      y3 + py + q = 0

where p, and q depend only on a, b, c, and d.
    The general depressed cubic had previously been solved by Tartaglia (the

Stutterer, 1500-1557) so converting the general cubic into a depressed cubic
provided a path for Ferrari to compute the "Cubic Formula" -- like the Qua-
dratic Formula but better.

Comment. It is not entirely clear why eliminating the quadratic term should
be depressing, but there it is.
CHAPTER 1. PROLOGUE: THREE LESSONS BEFORE WE BEGIN 5

Figure 1.2.1 Tartaglia4, "The Stutterer"
    Ferrari also knew how to compute the general solution of the "depressed

quartic" so when he and his teacher Girolomo Cardano (1501-1576) figured out
how to depress a general quartic they had a complete solution of the general
quartic as well.

Figure 1.2.2 Girolomo Cardano5
    Alas, their methods broke down entirely when they tried to solve the general

quintic equation. Unfortunately the rest of this story belongs in a course on
Abstract Algebra, not Real Analysis. But the lesson in this story applies to all
of mathematics: Every problem solved is a new theorem which then becomes
a tool for later use. Depressing a cubic would have been utterly useless had
not Tartaglia had a solution of the depressed cubic in hand. The technique
they used, with slight modifications, then allowed for a solution of the general
quartic as well.

    Keep this in mind as you proceed through this course and your mathemat-
ical education. Every problem you solve is really a theorem, a potential tool
that you can use later. We have chosen the problems in this text deliberately
with this in mind. Don't just solve the problems and move on. Just because
you have solved a problem does not mean you should stop thinking about it.
Keep thinking about the problems you've solved. Internalize them. Make the
ideas your own so that when you need them later you will have them at hand
to use.
Problem 1.2.3

    a Find M so that the substitution x = y - M depresses equation (I.6), the
       general cubic equation. Then find p and q in terms of a, b, c, and d.

    b Find K so that the substitution x = y - K depresses the general quartic
       equation. Make sure you demonstrate how you obtained that value or

   4mathshistory.st-andrews.ac.uk/Biographies/Tartaglia/
   5mathshistory.st-andrews.ac.uk/Biographies/Cardan
CHAPTER 1. PROLOGUE: THREE LESSONS BEFORE WE BEGIN 6

  why it works (if you guessed it).

c Find N so that the substitution x = y - N depresses a polynomial of
  degree n. Ditto on showing that this value works or showing how you
  obtained it.

                                                                                                    
Problem 1.2.4 Another Derivation of the Quadratic Formula. Here
is yet another way to solve a quadratic equation. Read the development below
with pencil and paper handy. Confirm all of the computations that are not
completely transparent to you. Then use your notes to present the solution
with all steps filled in.

Comment. Be sure you are clear on the purpose of this problem before you
begin. This is not about solving the Quadratic Equation. You already know
how to do that. Our purpose here is to give you practice filling in the skipped
details of mathematical exposition. We've chosen this particular problem be-
cause it should be a comfortable setting for you, but this particular solution is
probably outside of your previous experience.

    Suppose that r1 and r2 are solutions of ax2 + bx + c = 0. Without loss of
generality suppose that a > 0. Suppose further that r1  r2. Then

      ax2 + bx + c = a(x - r1)(x - r2)

                       = a x2 - (r1 + r2)x + (r1 + r2)2 - (r1 - r2)2 - 3r1r2 .

Therefore

                           b                                     (I.7)
           r1 + r2 = - a

and

           r1 - r2 =          b 2 - 4c .                         (I.8)
                              a   a

Equations (I.7) and (I.8) can be solved simultaneously to yield

                      
                 -b + b2 - 4ac
           r1 =
                      2a
                 -b - b2 - 4ac
           r2 =                   .
                              2a

                                                                 

1.3 Lesson Three

In the hustle and bustle of a typical college semester, with a lot of demands
on your time and very little time to think, it becomes very easy to see each
problem you solve as a small, isolated victory and then move on to the next
challenge. This is understandable. Each problem you solve is a small victory
and you've every right to be proud of it. But it is not isolated and it is a
mistake to think that it is.
CHAPTER 1. PROLOGUE: THREE LESSONS BEFORE WE BEGIN 7

Figure 1.3.1 George Polya6
    In his book How to Solve It the mathematician and teacher George Polya

gave four steps for problem solving. The steps may be paraphrased as
   1. Understand the problem.
   2. Formulate a plan.
   3. Execute the plan.
   4. Reflect on what you've done.
    This process is iterative. That is, once a plan is formulated and executed

we often find that our plan was not up to the task. So we have to ask what
went wrong, form a new plan and try again. This is the fourth step: Reflect
on what you've done.

    Almost everyone remembers this fourth step when their plan doesn't work.
After all, you've got to try again so you have to ask what went wrong. But it
is all too easy to neglect that crucial fourth step when the plan succeeds. In
that case, flush with success we usually move on to the next problem and start
over from scratch.

    This is a mistake. Having solved a problem is no reason to stop thinking
about it.

    That fourth step is at least as important when we have succeeded as when
we have failed. Each time you solve a problem stop and ask yourself a few
questions:

    Are there any easy consequences that follow from the result?
    How does it fit into the broader scheme of other problems you have

       solved?
    How might it be used in the future?
    Also, notice the structure of the problem. Some assumptions had to be
made. What were they? Were they all necessary? That is, did your solution
use everything that was assumed? If not, you may have something considerably
more general than it at first appears. What is that more general statement?
Even if you used all of the assumptions, was that really necessary? Can you
solve a similar problem with weaker assumptions?
    Take a moment to pack all of these questions (and their answers) away
in your mind so that when you see something similar in the future you will
be reminded of it. Don't solve any problem and then forget it and move on.
The nature of mathematics is cumulative. Remember, you are not here to
   6mathshistory.st-andrews.ac.uk/Biographies/Polya/
CHAPTER 1. PROLOGUE: THREE LESSONS BEFORE WE BEGIN 8

accumulate grade points. You are here to learn and understand the concepts
and methods of mathematics, to gain "mathematical maturity." Part of that
maturation process is the accumulation of a body of facts (theorems), and
techniques that can be used to prove new theorems (solve new problems).

    This text has been written with the maturation process in mind. You will
frequently find that the problems you solve today can be used to good effect
in the ones you attempt tomorrow, but only if you remember them. So take a
moment after you've solved each problem to think about how it fits into the
patterns you already know. This is important enough to bear repeating: A
problem, once solved, becomes a tool for solving subsequent problems!

    The purpose of the following sequence of problems is to help you become
accustomed to this notion (if you aren't already). It is a progression of results
about prime numbers. As you probably recall, a prime number is any integer
greater than 1 whose only factors are itself and 1. For example, 2, 3, 5, 7, 11
are prime, while 4, 6, 9 are not. A major result about prime numbers is the
following:

Theorem 1.3.2 The Fundamental Theorem of Arithmetic
    Any integer greater than 1 is either prime or it is a product of prime numbers.

Furthermore, this prime decomposition is unique up to the order of the factors.

    We will not prove this, but we will use it as a starting point to examine
the following problems. As you do these problems, notice how subsequent
problems make use of the previous results.

    Notice that the notation p |a simply means that the integer p divides the
integer a with no remainder.

Problem 1.3.3 Fermat's Little Theorem, step 1. Let p be a prime
number and a, b positive integers such that p |(a  b). Show that p |a or p |b.

Hint.

   If p |a then we are done. If not then notice that p is a prime factor of

a  b. What does the Fundamental Theorem of Arithmetic say about the prime

factors of a  b compared to the prime factors of a and b?                              

Problem 1.3.4 Fermat's Little Theorem, step 2. Let p be a prime num-

ber and let a1, a2, . . . , an be positive integers such that p | (a1  a2  a3  . . .  an).
Show that p |ak for some k  {1, 2, 3, . . . , n}.

Hint.

   Use induction on n and the result of the previous problem.                           

Problem 1.3.5 Fermat's Little Theorem, step 3. Let p be a prime

number and let k be an integer with 1  k  p - 1. Prove that p                   p  , where
                                                                                k
p  is the binomial coefficient k!(p-k)! p! .
k

Hint.

   We  know  p |p !,  so  p|  p  k!(p - k)!.  How  does  the  previous  result  apply?  
                              k

   We now have all the machinery in place to prove one of the really cool

theorems from number theory.

Theorem 1.3.6 Fermat's Little Theorem
    Let p be any prime number. Then p |(np - n) for all positive integers n.

Problem 1.3.7 Fermat's Little Theorem. Prove Fermat's Little Theo-
rem.

Hint.

   Use induction on n. To get from n to n + 1, use the binomial theorem on

(n + 1)p.                                                                               
CHAPTER 1. PROLOGUE: THREE LESSONS BEFORE WE BEGIN 9

    Fermat's Little Theorem is the foundational basis for a number of results
in number theory and encryption.
Chapter 2

Numbers, Real (R) and Ra-
tional (Q)

The set of real numbers (denoted, R) is badly named. The real numbers are
no more or less real -- in the non-mathematical sense that they exist -- than
any other set of numbers, just like the set of rational numbers (Q), the set of
integers (Z), or the set of natural numbers (N). The name "real numbers" is
(almost) an historical anomaly not unlike the name "Pythagorean Theorem"
which was actually known and understood long before Pythagoras lived.

    When calculus was being invented in the 17th century, numbers were thor-
oughly understood, or so it was believed.
Comment. Some would say "re-invented." See [13], or [9].

    They were, after all, just numbers. Combine them. We call that addition.
If you add them repeatedly we call it multiplication. Subtraction and division
were similarly understood.

    It was (and still is) useful to visualize these things in a more concrete way.
If we take a stick of length 2 and another of length 3 and lay them end-to-end
we get a length of 5. This is addition. If we lay them end-to-end but at right
angles then our two sticks are the length and width of a rectangle whose area
is 6. This is multiplication.

    Of course measuring lengths with whole numbers has limitations, but these
are not hard to fix. If we have a length (stick) of length 1 and another of length
2, then we can find another whose length when compared to 1 is the same (has
the same proportion) as 1 is to 2. That number of course, is 1/2.

Figure 2.0.1

                                                  10
CHAPTER 2. NUMBERS, REAL (R) AND RATIONAL (Q)              11

    Notice how fraction notation reflects the operation of comparing 1 to 2.
This comparison is usually referred to as the ratio of 1 to 2 so numbers of this
sort are called rational numbers. The set of rational numbers is denoted Q
for quotients. In grade school they were introduced to you as fractions. Once
fractions are understood, this visualization using line segments (sticks) leads
quite naturally to their representation with the rational number line.

Figure 2.0.2 The Rational Number Line

    This seems to work as a visualization because the rational numbers and
the points on a line seem to share certain properties. Chief among these is
that between any two points on the rational line there is another point, just
as between any two rational numbers there is another rational number.

Problem 2.0.3 Let a, b, c, d  N and find a rational number between a/b and
c/d.
                                                           

  This is all very clean and satisfying until we examine it just a bit closer.

Then it becomes quite mysterious. Consider again the rational numbers a/b

and c/d. If we think of these as lengths we can ask, "Is there a third length, say

, such that we can divide a/b into M pieces, each of length  and also divide

c/d into N pieces each of length ?" A few minutes thought should convince

you that this is the same as the problem of finding a common denominator so

  =   1   will  work  nicely.  (Confirm  this  yourself.)
      bd
  You may be wondering what we're making all of this fuss about. Obviously

this is always true. In fact the previous paragraph gives an outline of a very

nice little proof of this. Here are the theorem and its proof presented formally.

Theorem 2.0.4 Suppose a, b, c, and d are integers. There is a number   Q
such that M  = a/b and N  = c/d where M and N are also integers.

Proof. To prove this theorem we will display , M and N . It is your responsi-

bility to confirm that these actually work. Here they are:  = 1/bd, M = ad,

and N = cb.                                                

Problem 2.0.5 Confirm that , M, and N as given in the proof of Theo-

rem 2.0.4 satisfy the requirements of the theorem.         

  Theorem 2.0.4 suggests the following very deep and important question:

Are there lengths which can not be expressed as the ratio of two integer lengths?

The answer, of course, is yes. Otherwise we wouldn't have asked the question.

  One of the best known examples of such a number is the circumference of

a circle with diameter 1. This is the number usually denoted by . But circles

are extremely complex objects -- they only seem simple because they are so

familiar. Arising as it does from a circle, you would expect the number  to

be very complex as well and this is true. In fact  is an exceptionally weird

number for a variety of reasons. Let's start with something a little easier to

think about.

  Squares are simple. Two sets of parallel lines at right angles, all of the same

length. What could be simpler? If we construct a square with sides having
length 1 then its diagonal has length 2.
CHAPTER 2. NUMBERS, REAL (R) AND RATIONAL (Q)                12

                                           
Figure 2.0.6 A construction of 2

    This is a number which cannot be expressed as the ratio of two integers.
That is, it is irrational. This has been known since ancient times, but it is still
quite disconcerting when first encountered. It seems so counter-intuitive that
the intellect rebels. "This can't be right," it says. "That's just crazy!"

    Nevertheless it is true and we can prove it is true as follows.
    What happens if we suppose that the square root of two can be expressed
as a ratio of integers? We will show that this leads irrevocably to a conclusion
that is manifestly not true.
    Suppose 2 = a/b where a and b are integers. Suppose further that the
fraction a/b is in lowest terms. This assumption is crucial because if a/b is in
lowest terms we know that at most only one of them is even.
    So

                                             a
                                                = 2.

                                              b

Squaring both sides gives:

                          a2 = 2b2.

Therefore a2 is even. But if a2 is even then a must be even also (why?). If a
is even then a = 2k for some integer k. Therefore

                          4k2 = 2b2 or
                          2k2 = b2.

Therefore b2 is also even and so b must be even too. But this is impossible.

We've just concluded that a and b are both even and this conclusion follows

directly from our initial assumption that at most one of them could be even.

This is nonsense. Where is our error? It is not in any single step of our

reasoning. That was all solid. Check it again to be sure.  

Therefore our error must be in the initial assumption that 2 could be

expressed as a fraction. That assumption must therefore be false. In other
words, 2 cannot be so expressed.

Problem 2.0.7 Irrational  Numbers.   Show that each of the following
numbers is irrational:

       
 (a) 3

       
 (b) 5
CHAPTER 2. NUMBERS, REAL (R) AND RATIONAL (Q)                            13

(c) 3 2
(d) i (= -1)

(e) The square root of every positive integer which is not the square of an
     integer.

                                                                         
The fact that 2 is not rational is cute and interesting, but unless, like the

Pythagoreans of ancient Greece, you have a strongly held religious conviction

that all numbers are rational, it does not seem terribly important. On the
other hand, the very existence of 2 raises some interesting questions. For
example what can the symbol 4 2 possibly mean? If the exponent were a
rational  number,  say  m/n,  then  clearly  4m/n   =  n 4m.  But since  2 = m/n for

                                                  
any integers m and n how do we interpret 4 2? Does it have any meaning at all?

The more you think about this, the more puzzling the existence of irrational

numbers becomes. Suppose for example we reconsider the construction of a
line segment of length 2. It is clear that the construction works and that we

really can build such a line segment. It exists.

Repeat the construction but this time let's put the base side on the rational

line.

Figure 2.0.8
                                                              

    We know that the diagonal of this square is 2 as indicated. And we know
that 2 is not a rational number.

    Now leave the diagonal pinned at (0, 0) but allow it to rotate down so that
it coincides with the x-axis.

Figure 2.0.9
CHAPTER 2. NUMBERS, REAL (R) AND RATIONAL (Q)                                         14

                                                                                                  
    The end of our diagonal will trace out an arc of the circle with radius 2.

When the diagonal coincides with the x-axis, its endpoint will obviously be
the point ( 2, 0) as shown.

    But wait! We're using the rational number line for our x-axis. That means

the only points on the x-axis are those that correspond to rational numbers
(fractions). But we know that 2 is not rational! Conclusion: There is no
point ( 2, 0). It simply doesn't exist.
                                                                               
    Put differently, there is a hole in the rational number line right where 2

should be.

Figure 2.0.10

    This is weird!

    Recall that between any two rational numbers there is always another. This

fact is what led us to represent the rational numbers with a line in the first

place.                                                                         

    But it's even worse than that. It's straightforward to show that 3, 5,

etc. are all irrational too. So are  and e, though they aren't as easy to show.

It seems that the rational line has a bunch of holes in it. Infinitely many.

    And yet, the following theorem is true

Theorem 2.0.11

    a Between any two distinct real numbers there is a rational number.

    b Between any two distinct real numbers there is an irrational number.

    Both parts of this theorem rely on a judicious use of what is now called
the Archimedean Property of the Real Number System, which can be formally
stated as follows.

Principle 2.0.12 The Archimedean Property. Given any two positive
real numbers, a and b, there is a positive integer, n such that na > b.

    Physically this says that we can empty an ocean b with a teaspoon a,

provided we are willing to use the teaspoon a large number of times n.

    This is such an intuitively straightforward concept that it is easy to accept

it without proof. Until the invention of calculus, and even for some time after

that, it was simply assumed. However as the foundational problems posed by

the concepts of calculus were understood and solved we were eventually led to

a deeper understanding of the complexities of the real number system. The

Archimedean Property is no longer taken as an unproved axiom, but rather

it is now understood to be a consequence of other axioms. We will show this

later, but for now we will accept it as obviously true just as Archimedes did.

    With the invention of calculus, mathematicians of the seventeenth century

began to use objects which didn't satisfy the Archimedean Property (in fact, so

did Archimedes). As we shall see in the next chapter, when Leibniz wrote the

first paper on his version of the calculus, he followed this practice by explicitly

laying out rules for manipulating infinitely small quantities (infinitesimals).

These were taken to be actual numbers which are not zero and yet smaller

than any real number. The notation he used was dx (an infinitely small

displacement in the x direction), and dy (an infinitely small displacement in

the y direction). These symbols should look familiar to you. They are the same
                                                       dy
dy  and  dx  used   to  form  the  derivative  symbol  dx  that  you  learned  about  in

calculus.
CHAPTER 2. NUMBERS, REAL (R) AND RATIONAL (Q)  15

    Mathematicians of the seventeenth and eighteenth centuries made amaz-
ing scientific and mathematical progress exploiting these infinitesimals, even
though they were foundationally suspect. No matter how many times you add
the infinitesimal dx to itself the result will not be bigger than, say 10-1000,
which is very bizarre.

    When foundational issues came to the forefront, infinitesimals fell somewhat
out of favor. You probably didn't use them very much in calculus. Most of the
time you probably used the prime notation, f (x) introduced by Lagrange in
the eighteenth century. Some of the themes in this book are: Why differentials
fell out of favor, what were they replaced with and how the modern notations
you learned in calculus evolved over time.

    To conclude this aside on the Archimedean Property, the idea of infinitesi-
mals was revisited in the twentieth century by the logician Abraham Robinson
in [12]. Robinson was able to put the idea of infinitesimals on a solid logical
foundation. But in the 18th century, the existence of infinitesimal numbers
was shaky to say the very least. However this did not prevent mathematicians
from successfully exploiting these infinitely small quantities.

    We will come back to this saga in later chapters, but for now we return to
Theorem 2.0.11.
Sketch of Proof. We will outline the proof of part (a) of Theorem 2.0.11 and
indicate how it can be used to prove part b.
Let  and  be real numbers with  > . There are two cases.

    Case 1:.

        -  > 1. In this case there is at least one integer between  and .
       Since integers are rational we are done.

    Case 2:.

        -   1. In this case, by the Archimedean Property there is a positive
       integer, say n, such that n( - ) = n - n > 1. Now there will be an
       integer between n and n. You should now be able to find a rational
       number between  and .

                                                                                                    
    For part b, divide  and  by any positive irrational number and apply
part a. There are a couple of details to keep in mind. These are considered in
the following problem.

Problem 2.0.13

 (a) Prove that the product of a nonzero rational number and an irrational
       number is irrational.

 (b) Use the result of part (a) to prove Theorem 2.0.11.

                                                                                                    
    As a practical matter, the existence of irrational numbers isn't really very
important. In light of Theorem 2.0.11, any irrational number can be approxi-
mated arbitrarily closely by a rational number. So if we're designing a bridge
and 2 is needed we just use 1.414 instead. The error introduced is less than
0.001 = 1/1000 so it probably doesn't matter.
    But from a theoretical point of view this is devastating. When calculus was
invented, the rational numbers were suddenly not up to the task of justifying
the concepts and operations we needed to work with.
    Newton explicitly founded his version of calculus on the assumption that
we can think of variable quantities as being generated by a continuous motion.
CHAPTER 2. NUMBERS, REAL (R) AND RATIONAL (Q)  16

If our number system has holes in it such continuous motion is impossible
because we have no way to jump over the gaps. So Newton simply postulated
that there were no holes. He filled in the hole where 2 should be. He simply
said, yes there is a number there called 2 and he did the same with all of the
other holes.

    To be sure there is no record of Newton explicitly saying, "Here's how I'm
going to fill in the holes in the rational number line." Along with everyone else
at the time, he simply assumed there were no holes and moved on. It took
about 200 years of puzzling and arguing over the contradictions, anomalies and
paradoxes to work out the consequences of that apparently simple assumption.
The task may not yet be fully accomplished, but by the 20th century the
properties of the real number system (R) as an extension of the rational number
system (Q) were well understood. Here are both systems visualized as lines:

Figure 2.0.14 R and Q
    Impressive, no?
    The reason they look alike, except for the labels R and Q of course, is that

our ability to draw sketches of the objects we're studying utterly fails when
we try to sketch R, as different from Q. All of the holes in Q really are there,
but the non-holes are packed together so closely that we can't separate them
in a drawing. This inability to sketch the objects we study will be a frequent
source of frustration.

    Of course, this will not stop us from drawing sketches. When we do, our
imaginations will save us because it is possible to imagine Q as distinct from R.
But put away the idea that a sketch is an accurate representation of anything.
At best our sketches will only be aids to the imagination.

    So, at this point we will simply assume the existence of the real numbers.
We will assume also that they have all of the properties that we are used
to. This is perfectly acceptable as long as we make our assumptions explicit.
However we need to be aware that, so far, the existence and properties of the
real numbers is an assumption that has not been logically derived. Any time we
make an assumption we need to be prepared to either abandon it completely if
we find that it leads to nonsensical results, or to re-examine the assumption in
the light of these results to see if we can find another assumption that subsumes
the first and explains the (apparently) nonsensical results.

2.1 Additional Problems

Problem 2.1.1 Determine if each of the following is always rational or always
irrational. Justify your answers.

 (a) The sum of two rational numbers.

 (b) The sum of two irrational numbers.

 (c) The sum of a rational and an irrational number.

                                                                                                    
CHAPTER 2. NUMBERS, REAL (R) AND RATIONAL (Q)          17

Problem 2.1.2 Is it possible to have two rational numbers, a and b, such

that ab is irrational? If so, display an example of such a and b. If not, prove

that it is not possible.                               

Problem 2.1.3 Decide if it is possible to have two irrational numbers, a and

b, such that ab is rational. Prove it in either case.  
Chapter 3

Calculus in the 17th and 18th
Centuries

3.1 Newton and Leibniz Get Started

3.1.1 Leibniz's Calculus Rules

Figure 3.1.1 Gottfried Wilhelm Leibniz7
    The rules for calculus were first laid out in Gottfried Wilhelm Leibniz's

1684 paper Nova methodus pro maximis et minimis, itemque tangentibus, quae
nec fractas nec irrationales, quantitates moratur, et singulare pro illi calculi
genus (A New Method for Maxima and Minima as Well as Tangents, Which is
Impeded Neither by Fractional Nor by Irrational Quantities, and a Remarkable
Type of Calculus for This). Leibniz started with subtraction. That is, if x1
and x2 are very close together then their difference, x = x2 -x1, is very small.
He expanded this idea to say that if x1 and x2 are infinitely close together (but
still distinct) then their difference, dx, is infinitesimally small (but not zero).
Calculus Differentialis. This translates, loosely, as the calculus of differ-
ences.

    This idea is logically very suspect and Leibniz knew it. But he also knew
that when he used his calculus differentialis he was getting correct answers to
some very hard problems. So he persevered.

    Leibniz called both x and dx "differentials" (Latin for difference) because
he thought of them as, essentially, the same thing. Over time it has become

   7mathshistory.st-andrews.ac.uk/Biographies/Leibniz/

                                                  18
CHAPTER 3. CALCULUS IN THE 17TH AND 18TH CENTURIES 19

customary to refer to the infinitesimal dx as a differential, reserving "differ-
ence" for the finite case, x. This is why calculus is often called "differential
calculus."

    In his paper Leibniz gave rules for dealing with these infinitely small differ-
entials. Specifically, given a variable quantity x, dx represented an infinitesi-
mal change in x. Differentials are related via the slope of the tangent line to
a curve. That is, if y = f (x), then dy and dx are related by

                     dy = (slope of the tangent line)  dx.

Leibniz then divided by dx giving

                     dy = (slope of the tangent line).
                     dx

The elegant and expressive notation Leibniz invented was so useful that

it has been retained through the years despite some profound changes in the

underlying concepts. For example, Leibniz and his contemporaries would have
                     dy
viewed  the  symbol  dx  as  an  actual  quotient  of  infinitesimals,  whereas  today

we define it via the limit concept first suggested by Newton.

As a result the rules governing these differentials are very modern in ap-

pearance:

                         d( constant ) = 0

                     d(z - y + w + x) = dz - dy + dw + dx

                                 d(xv) = x dv + v dx

                                 d v = y dv - v dy
                                 y          yy

and, when a is an integer:

                                 d(xa) = axa-1 dx.

    Leibniz states these rules without proof: ". . . the demonstration of all this
will be easy to one who is experienced in such matters . . .." As an example,
mathematicians in Leibniz's day would be expected to understand intuitively
that if c is a constant, then d(c) = c - c = 0. Likewise, d(x + y) = dx + dy is
really an extension of (x2 + y2) - (x1 + y1) = (x2 - x1) + (y2 - y1).

3.1.2 Leibniz's Approach to the Product Rule

The explanation of the product rule using differentials is a bit more involved,
but Leibniz expected that mathematicans would be fluent enough to derive it.
The product p = xv can be thought of as the area of the following rectangle

Figure 3.1.2
CHAPTER 3. CALCULUS IN THE 17TH AND 18TH CENTURIES 20

    With this in mind, dp = d(xv) can be thought of as the change in area
when x is changed by dx and v is changed by dv. This can be seen as the L
shaped region in the following drawing.

Figure 3.1.3
    By dividing the L shaped region into 3 rectangles we obtain

                      d(xv) = x dv + v dx + dx dv.                       (I.1)

    Even though dx and dv are infinitely small, Leibniz reasoned that dx dv
is even more infinitely small (quadratically infinitely small?) compared to x dv
and v dx and can thus be ignored leaving

                              d(xv) = x dv + v dx.

    You should feel some discomfort at the idea of simply tossing the product
dx dv aside because it is "comparatively small." This means you have been well
trained, and have thoroughly internalized Newton's dictum [10]: "The smallest
errors may not, in mathematical matters, be scorned." It is logically untenable
to toss aside an expression just because it is small. Even less so should we
be willing to ignore an expression on the grounds that it is "infinitely smaller"
than another quantity which is itself "infinitely small."

    Newton and Leibniz both knew this as well as we do. But they also knew
that their methods worked. They gave verifiably correct answers to problems
which had, heretofore, been completely intractable. It is the mark of their
genius that both men persevered in spite of the very evident difficulties their
methods entailed.

3.1.3 Newton's Approach to the Product Rule

In the Principia, Newton "proved" the Product Rule as follows: Let x and v
be "flowing quantites" and consider the rectangle, R, whose sides are x and v.
R is also a flowing quantity and we wish to find its fluxion (derivative) at any
time.

Newton's `Method of Fluxions'. Newton's approach to calculus -- his
`Method of Fluxions' -- depended fundamentally on motion. That is, he viewed
his variables (fluents) as changing (flowing or fluxing) in time. The rate of
change of a fluent he called a fluxion. As a foundation both Leibniz's and
Newton's approaches have fallen out of favor, although both are still universally
used as a conceptual approach, a "way of thinking," about the ideas of calculus.

First  increment   x  and  v  by  x   and  v   respectively.  Then  the  corresponding
                                   2        2
increment of R is

       x + x v + v = xv + x v + v x + xv . (I.2)
                   2              2            2    2               4
CHAPTER 3. CALCULUS IN THE 17TH AND 18TH CENTURIES 21

Now decrement x and v by the same amounts:

x - x v - v = xv - x v - v x + xv . (I.3)
2  2     2                                  2               4

    Subtracting the right side of equation (I.3) from the right side of equa-
tion (I.2) gives

                                       R = xv + vx

which is the total change of R = xv over the intervals x and v and also
recognizably the Product Rule.

Figure 3.1.4 Isaac Newton8

    This argument is no better than Leibniz's as it relies heavily on the number
1/2 to make it work. If we take any other increments in x and v whose total
lengths are x and v it will simply not work. Try it and see.

    In Newton's defense, he wasn't really trying to justify his mathematical
methods in the Principia. His attention there was on physics, not math, so he
was really just trying to give a convincing demonstration of his methods. You
may decide for yourself how convincing his demonstration is.

    Notice that there is no mention of limits of difference quotients or deriva-
tives. In fact, the term derivative was not coined until 1797, by Lagrange. In a
sense, these topics were not necessary at the time, as Leibniz and Newton both
assumed that the curves they dealt with had tangent lines and, in fact, Leibniz
explicitly used the tangent line to relate two differential quantities. This was
consistent with the thinking of the time and for the duration of this chapter
we will also assume that all quantities are differentiable. As we will see later
this assumption leads to difficulties.

    Both Newton and Leibniz were satisfied that their calculus provided an-
swers that agreed with what was known at the time. For example d x2 =
d (xx) = x dx + x dx = 2x dx and d x3 = d x2x = x2 dx + x d x2
= x2 + x (2x dx) = 3x2 dx, results that were essentially derived by others
in different ways.

Problem 3.1.5

(a) Use Leibniz's product rule d (xv) = x dv + v dx to show that if n is a
     positive integer then d (xn) = nxn-1 dx

(b) Use Leibniz's product rule to derive the quotient rule

   d v = y dv - v dy .
      y  yy

8mathshistory.st-andrews.ac.uk/Biographies/Newton/
CHAPTER 3. CALCULUS IN THE 17TH AND 18TH CENTURIES 22
 (c) Use the quotient rule to show that if nis a positive integer, then
                                       d x-n = -nx-n-1 dx.

                                                                                                    

                                                                                                                                                                                                p

Problem 3.1.6 Let p and q be integers with q = 0. Show that d x q =
qp x pq -1 dx 

    Leibniz also provided applications of his calculus to prove its worth. As an
example he derived Snell's Law of Refraction from his calculus rules as follows.

    Given that light travels through air at a speed of va and travels through
water at a speed of vw the problem is to find the fastest path from point A to
point B.

Figure 3.1.7
    According to Fermat's Principle of Least Time, this fastest path is the one

that light will travel.
    Using the fact that Time = Distance / Velocity and the labeling in the

picture below we can obtain a formula for the time T it takes for light to travel
from A to B.

Figure 3.1.8

                    x2 + a2     (c - x)2 + b2
                                     vw
              T=    va       +

Using the rules of Leibniz's calculus, we obtain

dT = 1 1 x2 + a2 - (2x) + ((c - x) + b ) (2(c - x)(-1)) 12 1 1 2 2 - 21 dx
va 2                    vw 2

= 1  x - 1 c - x dx.
     va x2 + a2 vw (c - x)2 + b2
CHAPTER 3. CALCULUS IN THE 17TH AND 18TH CENTURIES 23

    Using the fact that at the minimum value for T , dT = 0, we have that the
fastest path from Ato B must satisfy v1  a x2x+a2 = v1w  c-x (c-x)2+b2 . Inserting
the following angles

Figure 3.1.9
    we get that the path that light travels must satisfy sinv a a = sinv w w which is

Snell's Law.
    To compare 18th century and modern techniques we will consider Johann

Bernoulli's solution of the Brachistochrone problem. In 1696, Bernoulli posed,
and solved, the Brachistochrone problem; that is, to find the shape of a fric-
tionless wire joining points A and B so that the time it takes for a bead to
slide down under the force of gravity is as small as possible.

Figure 3.1.10
    Bernoulli posed this "path of fastest descent" problem to challenge the

mathematicians of Europe and used his solution to demonstrate the power of
Leibniz's calculus as well as his own ingenuity.

       I, Johann Bernoulli, address the most brilliant mathematicians in
       the world. Nothing is more attractive to intelligent people than an
       honest, challenging problem, whose possible solution will bestow
       fame and remain as a lasting monument. Following the example
       set by Pascal, Fermat, etc., I hope to gain the gratitude of the whole
       scientific community by placing before the finest mathematicians of
       our time a problem which will test their methods and the strength
       of their intellect. If someone communicates to me the solution of
       the proposed problem, I shall publicly declare him worthy of praise.
       [11]
CHAPTER 3. CALCULUS IN THE 17TH AND 18TH CENTURIES 24

Figure 3.1.11 Johann Bernoulli9
In addition to Johann's, solutions were obtained from Newton, Leibniz, Jo-
hann's brother Jacob Bernoulli, and the Marquis de l'Hopital [15]. At the
time there was an ongoing and very vitriolic controversy raging over whether
Newton or Leibniz had been the first to invent calculus. An advocate of the
methods of Leibniz, Bernoulli did not believe Newton would be able to solve
the problem using his methods. Bernoulli attempted to embarrass Newton by
sending him the problem. However Newton did solve it.

    At this point in his life Newton had all but quit science and mathematics
and was fully focused on his administrative duties as Master of the Mint. In
part due to rampant counterfeiting, England's money had become severely
devalued and the nation was on the verge of economic collapse. The solution
was to recall all of the existing coins, melt them down, and strike new ones.
As Master of the Mint this job fell to Newton [8]. As you might imagine this
was a rather Herculean task. Nevertheless, according to his niece:

       When the problem in 1696 was sent by Bernoulli-Sir I.N. was in the
       midst of the hurry of the great recoinage and did not come home
       till four from the Tower very much tired, but did not sleep till he
       had solved it, which was by four in the morning.
He is later reported to have complained, "I do not love . . . to be . . . teezed
by forreigners about Mathematical things" [2].
    Newton submitted his solution anonymously, presumably to avoid more
controversy. Nevertheless the methods used were so distinctively Newton's
that Bernoulli is said to have exclaimed "Tanquam ex ungue leonem."
Tanquam ex ungue leonem. "I know the lion by his claw."
    Bernoulli's ingenious solution starts, interestingly enough, with Snell's Law
of Refraction. He begins by considering the stratified medium in the follow-
ing figure, where an object travels with velocities v1, v2, v3, . . . in the various
layers.
   9mathshistory.st-andrews.ac.uk/Biographies/Bernoulli_Johann/
CHAPTER 3. CALCULUS IN THE 17TH AND 18TH CENTURIES 25

Figure 3.1.12

By repeatedly applying Snell's Law he concluded that the fastest path must

satisfy        sin 1 = sin 2 = sin 3 =    .

               v1  v2  v3

    In other words, the ratio of the sine of the angle that the curve makes with
the vertical and the speed remains constant along this fastest path.

    If we think of a continuously changing medium as stratified into infini-
tesimal layers and extend Snell's law to an object whose speed is constantly
changing,

Figure 3.1.13
    then along the fastest path, the ratio of the sine of the angle that the curve's

tangent makes with the vertical, , and the speed, v, must remain constant.

                                             sin  = c.
                                               v

    If we include axes and let P denote the position of the bead at a particular
time then we have the following picture.
CHAPTER 3. CALCULUS IN THE 17TH AND 18TH CENTURIES 26

Figure 3.1.14

    In the above figure, s denotes the length that the bead has traveled down

to point P (that is, the arc length of the curve from the origin to that point)

and a denotes the tangential component of the acceleration due to gravity g.

Since  the  bead  travels  only  under  the  influence  of  gravity  then  dv  =  a.
                                                                           dt
    To get a sense of how physical problems were approached using Leibniz's
calculus we will use the above equation to show that v = 2gy.
                                             ds dy .
    By similar triangles we have  a     =             As a student of Leibniz, Bernoulli
                                  g

would have regarded  dy    as a fraction so
                     ds

                                  a ds = g dy

and since acceleration is the rate of change of velocity we have

                                 dv ds = g dy.
                                 dt

Again, 18th century European mathematicians regarded dv, dt, and ds as
infinitesimally small numbers which nevertheless obey all of the usual rules of
algebra. Thus we can rearrange the above to get

                                 ds dv = g dy.
                                 dt

Since  ds   is the rate of change of position with respect to time it is, in fact, the
       dt
velocity of the bead. That is

                                  v dv = g dy.

Bernoulli would have interpreted this as a statement that two rectangles of
height v and g, with respective widths dv and dy have equal area. Summing
(integrating) all such rectangles we get:

                                  v dv = g dy

                                        v2
                                            = gy

                                        2

or

                                        v = 2gy.                                      (I.4)
CHAPTER 3. CALCULUS IN THE 17TH AND 18TH CENTURIES 27

    You are undoubtedly uncomfortable with the cavalier manipulation of in-
finitesimal quantities you've just witnessed, so we'll pause for a moment now
to compare a modern development of equation (I.4) to Bernoulli's. As before
we begin with the equation:

                                               a dy
                                               g = ds

                                                       dy
                                               a = g ds .

Moreover, since acceleration is the derivative of velocity this is the same as:

                                       dv dy
                                       dt = g ds .

Now  observe    that  by  the   Chain  Rule   dv  =        ds dt dv ds .  The  physical  interpretation
                                              dt
of this formula is that velocity will depend on s, how far down the wire the

bead has moved, but that the distance traveled will depend on how much time

has elapsed. Therefore

                                         dv ds dy
                                         ds dt = g ds
or

                                ds dv dy
                                dt ds = g ds

and  since  ds  =  v  we  have
            dt

                                        dv dy
                                       v ds = g ds

Integrating both sides with respect to s gives:

                                 dv                        dy
                                v ds ds = g                ds ds

                                       vdv = g dy

and integrating gives

                                       v2
                                           = gy

                                       2

as before.

In effect, in the modern formulation we have traded the simplicity and

elegance of differentials for a comparatively cumbersome repeated use of the

Chain Rule. No doubt you noticed when taking Calculus that in the differential

notation of Leibniz, the Chain Rule looks like "canceling" an expression in the
                                       dy du      dx dy .
top and bottom of a fraction:          du dx  =            This is because for 18th century

mathematicians, this is exactly what it was.

To put it another way, 18th century mathematicians wouldn't have recog-

nized a need for what we call the Chain Rule because this operation was a

triviality for them. Just reduce the fraction. This begs the question: Why did

we abandon such a clear, simple interpretation of our symbols in favor of the,

comparatively, more cumbersome modern interpretation? This is one of the

questions we will try to answer in this course.
CHAPTER 3. CALCULUS IN THE 17TH AND 18TH CENTURIES 28

Returning to the Brachistochrone problem we observe that

                                        sin 
                                               =c

                                          v

and  since  sin   =  dx  we  see  that
                     ds

                                                             dx      (I.5)

                                                   ds = c
                                                     2gy

                                                 dx
                                                           =c

                                               2gy(ds)2

                                           dx = c.
                                  2gy [(dx)2 + (dy)2]

Bernoulli was then able to solve this differential equation.

Problem 3.1.15 Show that the equations x = 4gc2 -sin  , y = 4gc2 1-cos  satisfy

equation (I.5). Bernoulli recognized this solution to be an inverted cycloid, the

curve traced by a fixed point on a circle as the circle rolls along a horizontal

surface.                                                             

This illustrates the state of calculus in the late 1600's and early 1700's;

the foundations of the subject were a bit shaky but there was no denying its

power.

3.2 Power Series as Infinite Polynomials

Applied to polynomials, the rules of differential and integral calculus are straight-
forward. Indeed, differentiating and integrating polynomials represent some of
the easiest tasks in a calculus course. For example, computing (7 - x + x2) dx
is relatively easy compared to computing 3 1 + x3 dx. Unfortunately, not all
functions can be expressed as a polynomial. For example, f (x) = sin x cannot
be since a polynomial has only finitely many roots and the sine function has
infinitely many roots, namely {n| n  Z}. A standard technique in the 18th
century was to write such functions as an "infinite polynomial," what we typ-
ically refer to as a power series. Unfortunately an "infinite polynomial" is a
much more subtle object than a mere polynomial, which by definition is finite.
For now we will not concern ourselves with these subtleties. We will follow the
example of our forebears and manipulate all "polynomial-like" objects (finite
or infinite) as if they are polynomials.

Definition 3.2.1 A power series centered at a is a series of the form

             

                 an(x - a)n = a0 + a1(x - a) + a2(x - a)2 +    .

            n=0

    Often we will focus on the behavior of power series n=0  anxn, centered
around 0, as the series centered around other values of a are obtained by shifting

a series centered at 0.                                              

Before we continue, we will make the following notational comment. The

most advantageous way to represent a series is using summation notation since

there can be no doubt about the pattern to the terms. After all, this notation

contains a formula for the general term. This being said, there are instances

where writing this formula is not practical. In these cases, it is acceptable to

write the sum by supplying the first few terms and using ellipses (the three
CHAPTER 3. CALCULUS IN THE 17TH AND 18TH CENTURIES 29

dots). If this is done, then enough terms must be included to make the pattern

clear to the reader.

    Returning to our definition of a power series, consider, for example, the
geometric series n=0  xn = 1 + x + x2 +   . If we multiply this series by
(1 - x), we obtain

(1 - x)(1 + x + x2 +    ) = (1 + x + x2 +    ) - (x + x2 + x3 +    ) = 1.

This leads us to the power series representation

                             1 = 1 + x + x2 +    = xn  .
                           1 - x n=0

If    we  substitute   x   =    1   into  the  above,  we  obtain
                                10

                       1        12             13                  1 1 = 10 .
                 1+ +                  +             +렁 =  1 - 10 9
                                               10
                      10        10

    This agrees with the fact that .333 . . . = 13 , and so .111 . . . = 19 , and
1.111 . . . = 910 .

    There are limitations to these formal manipulations however. Substituting
x = 1 or x = 2 yields the questionable results

                    1 = 1 + 1 + 1 +    and 1 = 1 + 2 + 22 +    .
                    0                              -1

    We are missing something important here, though it may not be clear
exactly what. A series representation of a function works sometimes, but
there are some problems. For now, we will continue to follow the example of
our 18th century predecessors and ignore them. That is, for the rest of this
section we will focus on the formal manipulations to obtain and use power
series representations of various functions. Keep in mind that this is all highly
suspect until we can resolve problems like those just given.

    Power series became an important tool in analysis in the 1700's. By rep-
resenting various functions as power series they could be dealt with as if they
were (infinite) polynomials. The following is an example.

Example 3.2.2 Solve the following Initial Value problem: Find y(x) given
      dy
that  dx  =  y,  y(0)  =   1.

Comment. A few seconds of thought should convince you that the solution of
this problem is y(x) = ex. We will ignore this for now in favor of emphasising
the technique.

Assuming the solution can be expressed as a power series we have

                                

                       y = anxn = a0 + a1x + a2x2 +    .

                               n=0

Differentiating gives us

                       dy = a1 + 2a2x + 3a3x2 + 4a4x3 + . . . .
                       dx

Since     dy     =  y  we  see  that
          dx

                 a1 = a0 , 2a2 = a1 , 3a3 = a2 , . . . , nan = an-1 , . . . .
CHAPTER 3. CALCULUS IN THE 17TH AND 18TH CENTURIES 30

This leads to the relationship

                       1                 1 an-2 =    = 1 a0.
                an = an-1 =          n(n - 1)
                          n                                   n!

Thus the series solution of the differential equation is

                              y = a0 xn  = a 1 0 xn.
                                     n!                 n!
                                n=0                n=0

Using  the  initial  condition    y(0)  =  1,  we  get  1  =  a0(1  +  0  +  1   02  +        )  =  a0.
                                                                             2!
Thus the solution to the initial problem is y = n=0 n!  1 xn. Let's call this
function E(x). Then by definition

                 E(x) = 1 xn = 1 + x1 + x2 + x3 + . . . .
                                n!               1! 2! 3!
                          n=0

                                                                                                    
    Let's examine some properties of this function. The first property is clear
from the definition.
    Property 1. E(0) = 1
    Property 2. E(x + y) = E(x)E(y).
    To see this we multiply the two series together, so we have

E(x)E(y) =        1 xn                1 yn
             =       n!                  n!

                n=0                 n=0

                x0 x1 x2 x3                             y0 y1 y2 y3
                    + + + + ...                            + + + + ...

                0! 1! 2! 3!                             0! 1! 2! 3!

                x0 y0 x0 y1 x1 y0 x0 y2 x1 y1 x2 y0
            =          +          +         +           +           +
                0! 0! 0! 1! 1! 0! 0! 2! 1! 1! 2! 0!

                     x0 y3 x1 y2 x2 y1 x3 y0
                +            +          +          +          + ...
                     0! 3! 1! 2! 2! 1! 3! 0!

            =   x0 y0  +     x0 y1 x1 y0
                                    +
                0! 0! 0! 1! 1! 0!

                       x0 y2 x1 y1 x2 y0
                +              +         +
                          0! 2! 1! 1! 2! 0!

                +      x0 y3 x1 y2 x2 y1 x3 y0                + ...
                               +         +         +
                          0! 3! 1! 2! 2! 1! 3! 0!

               11            1! x0y1 + 1! x1y0
            =+                             1!0!
                0! 1! 0!1!

                + 1 2! x0y2 + 2! x1y1 + 2! x2y0
                     2! 0!2!             1!1!           2!0!

                + 1 3! x0y3 + 3! x1y2 + 3! x2y1 + 3! x3y0 + . . .
                     3! 0!3!             1!2!           2!1!              3!0!

                 11           1 x0y1 +      1 x1y0         2 x2y0         3 x3y0              +...
E(x)E(y) = +                  0             1              2              3
                             2 x0y2 +      2 x1y1 +        3 x2y1 +
                 0! 1!       0             1               2
                       1     3 x0y3 +      3 x1y2 +
                             0             1
                   +
                      2!
                       1

                   +
                      3!
CHAPTER 3. CALCULUS IN THE 17TH AND 18TH CENTURIES 31

            = 1 + 1 (x + y)1 + 1 (x + y)2 + 1 (x + y)3 + . . .
            0! 1!                  2!              3!

            = E(x + y).                                                      (I.6)

    Property 3. If m is a positive integer then E(mx) = (E(x))m. In partic-
ular, E(m) = (E(1))m.

Problem 3.2.3 Prove Property 3.                                                         

  Property 4. E(-x) = E(x) 1 = (E(x))-1.

Problem 3.2.4 Prove Property 4.                                                        
                                                                             n E(1) =
    Property 5. If n is an integer with n = 0, then E( 1n ) =
(E(1))1/n.

Problem 3.2.5 Prove Property 5.                                                         

  Property 6. If m and n are integers with n = 0, then E                 m   = (E(1))m/n.
                                                                         n

Problem 3.2.6 Prove Property 6.                                                         

Definition 3.2.7 Let E(1) be denoted by the number e. Using the series

e = E(1) =         n!1 ,  we  can  approximate  e  to       any  degree  of  accuracy.  In
              n=0
particular e  2.71828.
                                                                                        

  In light of Property 6, we see that for any rational number r, E(r) = er.

Not only does this give us the series representation er =  1 n=0 n! rn for any
rational number r, but it gives us a way to define ex for irrational values of x

as well. That is, we can define

                              ex  = E(x) = 1 xn
                                                    n!

                                                       n=0

for any real number x.                             n=0 n!  1 2 n. The expression

  As an illustration, we now have e 2 =

e 2 is meaningless if we try to interpret it as one irrational number raised to
another. What doesit mean to raise anything to the 2 power? However
            1             n
the series n=0 n! 2 does seem to have meaning and it can be used to
extend the exponential function to irrational exponents. In fact, defining the

exponential function via this series answers the question we raised in Chapter 2:
What does 4 2 mean?
                                    (2 log 4)n
              2      2 log 4

  It means 4 = e              =  n=0 n!         .

  This may seem to be the long way around just to define something as

simple as exponentiation. But this is a fundamentally misguided attitude. Ex-

ponentiation only seems simple because we've always thought of it as repeated

multiplication (in Z) or root-taking (in Q). When we expand the operation
to the real numbers this simply can't be the way we interpret something like
4 2. How do you take the product of 2 copiesof 4? The concept is mean-
ingless. What we need is an interpretation of 4 2 which is consistent with,
say 43/2 = 4 3 = 8. This is exactly what the series representation of ex

provides.

  We also have a means of computing integrals as series. For example, the

famous "bell shaped" curve given by the function f (x) = 1 e- 2 x2 is of vital

                                                                                                                 2

importance in statistics and must be integrated to calculate probabilities. The

power series we developed gives us a method of integrating this function. For
CHAPTER 3. CALCULUS IN THE 17TH AND 18TH CENTURIES 32

example, we have

b 1 - x2                                1 b  1 -x2 n
                e 2 dx =                                                 dx
x=0 2                                   2 x=0 n=0 n! 2

                                     1              (-1)n    b
                                =                    n!2n
                                                                 x2ndx
                                      2 n=0
                                                            x=0

                                =       1  (-1)n b2n+1               .

                                        2 n=0 n!2 (2n + 1)n

    This series can be used to approximate the integral to any degree of accu-
racy. The ability to provide such calculations made power series of paramount
importance in the 1700's.

Problem 3.2.8

(a) Show that if y =        n=0  anxn satisfies the differential equation dx2 d2y = -y,
     then                       an+2 = -1 an
                                           (n + 2) (n + 1)

and conclude that

y = a0+a1x- 1 a0x2- 1 a1x3+ 1 a0x4+ 1 a1x5- 1 a0x6- 1 a1x7+   .
                    2!          3!          4!      5!           6!         7!

(b) Since y = sin x satisfies dx2 d2y = -y, we see that

sin x = a0+a1x- 1 a0x2- 1 a1x3+ 1 a0x4+ 1 a1x5- 1 a0x6- 1 a1x7+  
                        2!          3!          4!      5!           6!         7!

for some constants a0 and a1. Show that in this case a0 = 0 and a1 = 1
and obtain

sin x = x - 1 x3 + 1 x5 - 1 x7  +    = (-1) x2n+1 n .
                        3!          5!      7!                   (2n + 1)!
                                                            n=0

                                                                                                    
Problem 3.2.9

(a) Use the series

sin x = x - 1 x3 + 1 x5 - 1 x7  +    = (-1) x2n+1 n
                        3!          5!      7!                   (2n + 1)!
                                                            n=0

to obtain the series

cos x = 1 - 1 x2 + 1 x4 - 1 x6  +    = (-1) x2n n .
                            2!          4!      6!                   (2n)!
                                                            n=0

(b) Let s(x, N ) = n=0 (2n+1)! N (-1) x n 2n+1 and c(x, N ) = n=0 (2n)! N (-1) x n 2n and
      use a computer algebra system to plot these for -4  x  4, N =
      1, 2, 5, 10, 15. Describe what is happening to the series as N becomes
      larger.

                                                                                                   
CHAPTER 3. CALCULUS IN THE 17TH AND 18TH CENTURIES 33

Problem      3.2.10       Use  the   geometric   series,    1      =   1 + x + x2 + x3 +         =
                                                          1-x
   n=0  xn,                                  1
             to  obtain   a  series  for  1+x2  and  use     this  to  obtain   the  series

             arctan x = x - 1 x3 + 1 x5 -    = (-1)n  1 x2n+1.
                                  35                                   2n + 1
                                                          n=0

   Use  the  series    above    to  obtain  the  series      =     n=0  (-1) 2n+1 n 1 .             
                                                          4

   The series for arctangent was known by James Gregory (1638-1675) and it is

sometimes referred to as "Gregory's series." Leibniz independently discovered

   =  1-  1  +   1  -  1  +렁  by   examining   the  area   of    a   circle.  Though   it  gives  us
4         3      5     7
a means for approximating  to any desired accuracy, the series converges too

slowly to be of any practical use. For example, if we compute the sum of the

first 1000 terms we get

                               1000 1
                          4          (-1)n             3.142591654
                               n=0 2n + 1

which only approximates  to two decimal places.
    Newton knew of these results and the general scheme of using series to

compute areas under curves. These results motivated Newton to provide a
series approximation for  as well, which, hopefully, would converge faster.
We will use modern terminology to streamline Newton's ideas. First notice
that 4 = x=0 1 1 - x2 dx as this integral gives the area of one quarter of the

                                                                             
unit circle. The trick now is to find series that represents 1 - x2.

    To this end we start with the binomial theorem

                                (a + b)N =      N

                                                      N N-n n
                                                          a b,
                                            n=0 n

where

                          N    =     N!

                          n n! (N - n)!

                               = N (N - 1) (N - 2)    (N - n + 1 )
                                                       n!

                               = j=0 n-1 (N - j) .
                                          n!

   Unfortunately, we now have a small problem with our notation which will

be a source of confusion later if we don't fix it. So we will pause to address

this matter. We will come back to the binomial expansion afterward.

   This last expression is becoming awkward in much the same way that an

expression like

                                1 12 13                                1k
                          1+ +              +            +...+
                                22               2                     2

is awkward. Just as this sum is less cumbersome when written as                          k   1n
the product
                                                                                         n=0 2

                             N (N - 1) (N - 2)    (N - n + 1 )

is less cumbersom when we write it as j=0 n-1 (N - j).
    A capital pi () is used to denote a product in the same way that a capital

sigma () is used to denote a sum. The most familiar example would be writing

                                                       n

                                            n! = j.

                                                     j=1
CHAPTER 3. CALCULUS IN THE 17TH AND 18TH CENTURIES 34

Just as it is convenient to define 0! = 1, we will find it convenient to

define  j=1 0 = 1. Similarly, the fact that                 N  = 1 leads to the convention
                                                            0

j=0 -1 (N - j) = 1. Strange as this may look, it is convenient and is consistent

with the convention j=0 -1 sj = 0.
    Returning to the binomial expansion and recalling our convention

                                           -1

                                               (N - j) = 1,

                                           j=0

we can write,

                               N           j=0 n-1 (N - j)              N     j=0 n-1 (N - j) xn.
                                               n!                                 n!
(1 + x)N = 1 +                                              xn =

                             n=1                                      n=0

These two representations probably look the same at first. Take a moment

and be sure you see where they differ.

There is an advantage to using this convention (especially when programing

a product into a computer), but this is not a deep mathematical insight. It is

just a notational convenience and we don't want you to fret over it, so we will

use both formulations (at least initially).

Notice that we can extend the above definition of                          N    to values n > N . In
                                                                           n
this case, j=0 n-1 (N - j) will equal 0 as one of the factors in the product will

be 0 (the one where n = N ). This gives us that                N           = 0 when n > N and so
                                                               n

                                           j=0 n-1 (N - j)                    j=0 n-1 (N - j) xn
                                               n!                                 n!
(1 + x)N = 1 +                                              xn =

                             n=1                                      n=0

holds true for any nonnegative integer N . Essentially Newton asked if it could

be possible that the above equation could hold values of N which are not

nonnegative integers.     For example,              if the equation held true for N  =  1          ,  we
                                                                                        2
would obtain

                                           j=0 2 n-1 1 - j                    j=0 2 n-1 1 - j xn
                                               n!                                 n!
                                1                           xn =

      (1 + x) 2 = 1 +                                                 n=0

                                      n=1

or

(1 + x) 12 = 1 + 1 1 1 x + 2 2 - 1 1 1 x2 + 2 2 - 1 12 - 2 x3 +    .                            (I.7)
                       2                   2!                  3!

    Notice that since 1/2 is not an integer the series no longer terminates.
Although Newton did not prove that this series was correct (nor did we), he
tested it by multiplying the series by itself. When he saw that by squaring the
series he started to obtain 1+ x + 0 x2 + 0 x3 +   , he was convinced that the
series was exactly equal to 1 + x.

Problem 3.2.11
    Consider the series representation

                                                            j=0 2 n-1 1 - j xn
                                                                n!
                                  1

               (1 + x) 2 = 1 +

                                            n=1

                                                    j=0 2 n-1 1 - j xn.
                                                        n!
                                           =

                                               n=0
CHAPTER 3. CALCULUS IN THE 17TH AND 18TH CENTURIES 35

Multiply this series by itself and compute the coefficients for

x0, x1, x2, x3, x4 in the resulting series.                                               

Problem 3.2.12 Let

                                          M   j=0 2 n-1 1 - j xn.
                                                  n!
                    S(x, M ) =

                                         n=0

Use a computer algebra system toplot S(x, M ) for M = 5, 10, 15, 95, 100
and compare these to the graph for 1 + x. What seemsto be happening?
For what values of x does the series appear to converge to 1 + x?                         

    Convinced that he had the correct series, Newton used it to find a series
representation of x=0 1 1 - x2 dx.

Problem 3.2.13                                                                j=0 2 n-1 1 - j xn to obtain
the series                                                                        n!
                                                                           1

                Use the series (1 + x) 2 =

                                                                   n=0

                1               1 - x2 dx
                    =
                4
                           x=0

                                j=0 2 n-1 1 - j    (-1)n
                                    n!             2n + 1
                    =

                        n=0

                    = 1- 1 - 1 - 1 - 5 -렁.
                            6 40 112 1152

Use a computer algebra system to sum the first 100 terms of this series and

compare the answer to 4 .                                                                 

Again, Newton had a series which could be verified (somewhat) computa-

tionally. This convinced him even further that he had the correct series.

Problem 3.2.14

a Show that

                1/2  2 (-1)n n-1 1 j=0 2 - j
                           x - x dx =            
                                              n=0 2 n! (2n + 3) 2n
                x=0

and use this to show that

                                 (-1)n j=0 2 n-1 1 - j                        .
                     = 16                     
                                n=0 2 n! (2n + 3) 2n

b We now have two series for calculating  : the one from part (a) and the
   one derived earlier, namely

                                  = 4 (-1)n .
                                               2n + 1

                                              n=0

We will explore which one converges to  faster. With this in mind, define
                    N (-1) j=0 n n-1( 21 -j)
                                                                                 N (-1)n
S1(N ) = 16                                      and S2(N ) = 4 n=0 2n+1 .
                    n=0        2 n!(2n+3)2n

Use a computer algebra system to compute S1(N )and S2(N ) for N =
5, 10, 15, 20. Which one appears to converge to  faster?

                                                                                          
CHAPTER 3. CALCULUS IN THE 17TH AND 18TH CENTURIES 36

In general the series representation

                                    j=0 n-1 ( - j) xn
                                        n!
       (1 + x) =

                            n=0

                  = 1 + x +  ( - 1) x2 +  ( - 1) ( - 2) x3 +   
                                            2!                     3!

is called the binomial series (or Newton's binomial series). This series is
correct when  is a non-negative integer (after all, that is how we got the
series). We can also see that it is correct when  = -1 as we obtain

                            j=0 n-1 (-1 - j) xn
                                 n!
(1 + x)-1 =

                       n=0

            = 1 + (-1)x + -1 (-1 - 1) x2 + -1 (-1 - 1) (-1 - 2) x3 +   
                                            2!                             3!

            = 1 - x + x2 - x3 +   

which  can  be  obtained    from    the    geometric    series       1  =  1 + x + x2    +렁  .
                                                                   1-x
In fact, the binomial series is the correct series representation for all values

of the exponent  (though we haven't proved this yet).

Problem 3.2.15 Let k be a positive integer. Find the power series, centered
at zero, for f (x) = (1 - x)-k by

(a) Differentiating the geometric series (k - 1) times.

(b) Applying the binomial series.

(c) Compare these two results.

                                                                                                         

Figure 3.2.16 Leonhard Euler10

Leonhard Euler was a master at exploiting power series. In 1735, the 28

year-old Euler won acclaim for what is now called the Basel problem: to find a

closed form for             n2 1 .  Other  mathematicans    knew        that   the  series  converged,
                  n=1
but Euler was the first to find its exact value. The following problem essentially

provides Euler's solution.

Problem 3.2.17 The Basel Problem.

(a)  Show   that  the       power   series  for  sin x  is  given  by   1-     13! x2 +  15! x4 -   
                                                   x

10mathshistory.st-andrews.ac.uk/Biographies/Euler/
CHAPTER 3. CALCULUS IN THE 17TH AND 18TH CENTURIES 37

(b) Use (a) to infer that the roots of 1 - 13! x2 + 15! x4 -    are given by
                                      x = , 2, 3, . . .

(c) Suppose p(x) = a0 + a1x +    + anxn is a polynomial with roots
     r1, r2, . . . , rn. Show that if a0 = 0, then all the roots are non-zero and

                     x    1- x 렁 1- x .
p(x) = a0 1 - r1                   r2                               rn

(d) Assuming that the result in part (c) holds for an infinite polynomial
     (power series), deduce that

1 - 1 x2 + 1 x4 -    = 1 - x 2          x2                                   x2
3! 5!                              1 - 2                                1 - 3   

(e) Expand this product to deduce
                                              1 2
                                                  n2 = 6 .

                                                               n=1

                                                                                                    
Problem 3.2.18 Euler's Formula.

 (a) Use the power series expansion of ex, sin x, and cos x to derive Euler's
       Formula:
                                          ei = cos + i sin .

 (b) Use Euler's formula to derive the Addition/Subtraction formulas from
       Trigonometry:

                               sin(  ) = sin  cos   sin  cos 

                               cos(  ) = cos  cos   sin  sin 

 (c) Use Euler's formula to show that

                                          sin 2 = 2 cos  sin 

                                        cos 2 = cos2  - sin2 

 (d) Use Euler's formula to show that

                                    sin 3 = 3 cos2  sin  - sin3 
                                     cos 3 = cos3  - cos  sin2 

 (e) Find a formula sin(n) and cos(n) for any positive integer n.
                                                                                                    
CHAPTER 3. CALCULUS IN THE 17TH AND 18TH CENTURIES 38

3.3 Additional Problems

Problem 3.3.1 Use the geometric series to obtain the series

                             ln (1 + x) = x - 1 x2 + 1 x3 -   
                                                   23

                                           = (-1) xn+1 n .
                                                   n+1

                                                    n=0

                                                                                                    
Problem 3.3.2 Without using Taylor's Theorem, represent the following
functions as power series expanded about 0 (i.e., in the form n=0  anxn).

 (a) ln 1 - x2

 (b) 1+x2 x

 (c) arctan x3

(d) ln (2 + x)

     Hint.
     2 + x = 2 1 + x2

                                                                                                    
Problem 3.3.3 Let a be a positive real number. Find a power series for ax
expanded about 0.

Hint.
    ax = eln (ax) 

Problem 3.3.4 Represent the function sin x as a power series expanded about
a (i.e., in the form n=0  an (x - a)n).

Hint.

sin x = sin (a + x - a).                                           

Problem 3.3.5 Without using Taylor's Theorem, represent the following

functions as a power series expanded about a for the given value of a (i.e., in
the form n=0  an (x - a)n).

(a) ln x, a = 1

(b) ex, a = 3

(c) x3 + 2x2 + 3 , a = 1

(d)    1  ,a=5
       x

                                                                                                    
Problem 3.3.6 Evaluate the following integrals as series.

1 (a) ex2 dx

          x=0

             1  1
                   4 dx
(b)
       x=0 1 + x

          1

(c)             3 1 - x3 dx

       x=0
CHAPTER 3. CALCULUS IN THE 17TH AND 18TH CENTURIES 39
                                                                                                    
Chapter 4

Questions Concerning Power
Series

4.1 Taylor's Formula

As we saw in the previous chapter, representing functions as power series was
a fruitful strategy for mathematicans in the eighteenth century (as it still is).
Differentiating and integrating power series term by term was relatively easy,
seemed to work, and led to many applications. Furthermore, power series
representations for all of the elementary functions could be obtained if one was
clever enough.

    However, cleverness is an unreliable tool. Is there some systematic way
to find a power series for a given function? To be sure, there were nagging
questions: If we can find a power series, how do we know that the series we've
created represents the function we started with? Even worse, is it possible for
a function to have more than one power series representation centered at a
given value a? This uniqueness issue is addressed by the following theorem.

Theorem 4.1.1 If f (x) = n=0  an(x - a)n, then an = n! f(n)(a) , where f (n)(a)
represents the nth derivative of f evaluated at a.

    A few comments about Theorem 4.1.1 are in order. Notice that we did
not start with a function and derive its series representation. Instead we de-
fined f (x) to be the series we wrote down. This assumes that the expression

   n=0  an(x - a)n actually has meaning (that it converges). At this point we
have every reason to expect that it does, however expectation is not proof so
we note that this is an assumption, not an established truth. Similarly, the
idea that we can differentiate an infinite polynomial term-by-term as we would
a finite polynomial is also assumed. As before, we follow in the footsteps of
our 18th century forebears in making these assumptions. For now.

Problem 4.1.2 Prove Theorem 4.1.1.

Hint.

f (a) = a0 + a1(a - a) + a2(a - a)2 +    = a0. Differentiate to obtain the

other terms.                                

From Theorem 4.1.1 we see that if we do start with the function f (x) then

no matter how we obtain its power series, the result will always be the same.

The series

 f (n)(a) (x - a)n = f (a) + f (a)(x - a) + f (a) (x - a)2 + f (a) (x - a)3 +   
       n!                           2!  3!
n=0

                      40
CHAPTER 4. QUESTIONS CONCERNING POWER SERIES                                  41

Figure 4.1.3 Brook Taylor11

is called the Taylor series for f expanded about (centered at) a. Although

this systematic "machine" for obtaining power series for a function seems to

have been known to a number of mathematicians in the early 1700's, Brook

Taylor was the first to publish this result in his Methodus Incrementorum

(1715). The special case when a = 0 was included by Colin Maclaurin in his

Treatise of Fluxions (1742). Thus when a = 0, the series        n=0 n!  f x (n)(0) n is
often called the Maclaurin Series for f .

The "prime notation" for the derivative was not used by Taylor, Maclaurin

or their contemporaries. It was introduced by Joseph Louis Lagrange in his

1779 work Thorie des Fonctions Analytiques. In that work, Lagrange sought to

get rid of Leibniz's infinitesimals and base calculus on the power series idea. His

idea was that by representing every function as a power series, calculus could

be done "algebraically" by manipulating power series and examining various

aspects of the series representation instead of appealing to the "controversial"

notion of infinitesimals. He implicitly assumed that every continuous function

could be replaced with its power series representation.

That is, he wanted to think of the Taylor series as a "great big polynomial,"

because polynomials are easy to work with. It was a very simple, yet exceed-

ingly clever and far-reaching idea. Since ex = 1 + x + x2/2 + . . ., for example,

why not just define the exponential to be the series and work with the series.

After all, the series is just a very long polynomial.

This idea did not come out of nowhere. Leonhard Euler had put exactly

that idea to work to solve many problems throughout the 18th century. Some

of his solutions are still quite breath-taking when you first see them [14].

Taking his cue from the Taylor series

                                f (x) = f (n)(a) (x - a)n
                                                  n!

                                                           n=0

11mathshistory.st-andrews.ac.uk/Biographies/Taylor/
CHAPTER 4. QUESTIONS CONCERNING POWER SERIES           42

Figure 4.1.4 Joseph-Louis Lagrange12

    Lagrange observed that the coefficient of (x - a)n provides the derivative
of f at a (divided by n!). Modifying the formula above to suit his purpose,
Lagrange supposed that every differentiable function could be represented as

                                                                 

                                  f (x) = gn(a)(x - a)n.

                                                               n=0

    If we regard the parameter a as a variable then g1 is the derivative of f ,
g2 = 2f  and generally

                                           gn = n!f (n).

    Lagrange dubbed his function g1 the "fonction drive" from which we get
the modern name "derivative."

    All in all, this was a very clever and insightful idea whose only real flaw is
that its fundamental assumption is not true. It turns out that not every differ-
entiable function can be represented as a Taylor series. This was demonstrated
very dramatically by Augustin Cauchy's famous counter-example

f (x) =  e- x2 1  x = 0 .                              (I.1)
         0        x=0

    This function is actually infinitely differentiable everywhere but its Maclau-
rin series (that is, a Taylor series with a = 0) does not converge to f because
all of its derivatives at the origin are equal to zero: f (n)(0) = 0, n  N.

    Computing these derivatives using the definition you learned in calculus
is not conceptually difficult but the formulas involved do become complicated
rather quickly. Some care must be taken to avoid error.

    To begin with, let's compute a few derivatives when x = 0.

f (0)(x) = ex-2         e-x-2 .
f (1)(x) = 2x-3e-x-2
f (2)(x) = 4x-6 - 6x-4

    As you can see the calculations are already getting a little complicated and
we've only taken the second derivative. To streamline things a bit we take
y = x-1, and define p2(x) = 4x6 - 6x4 so that

                           f (2)(x) = p2(x-1)e-x-2 = p2(y)e-y2 .

12mathshistory.st-andrews.ac.uk/Biographies/Lagrange/
CHAPTER 4. QUESTIONS CONCERNING POWER SERIES                                  43

Problem 4.1.5 Cauchy's Counterexample, Part 1.

 (a) Adopting the notation y = x-1 and f (n)(x) = pn(y)e-y2 , find pn+1(y)
       in terms of pn(y). [Note: Don't forget that you are differentiating with
       respect to x, not y.]

 (b) Use induction on n to show that pn(y) is a polynomial for all n  N.

                                                                                                    
    Unfortunately everything we've done so far only gives us the derivatives we
need when x is not zero, and we need the derivatives when x is zero. To find
these we need to get back to very basic ideas.
    Let's assume for the moment that we know that f (n)(0) = 0 and recall that

              f (n+1)(0) = lim f (n)(x) - f (n)(0)
                                          x0  x-0

              f (n+1)(0) = lim x-1pn(x-1)e-x-2

                                     x0

              f (n+1)(0) = lim y2 ypn(y) .
                              y e

    We can close the deal with the following problem.
Problem 4.1.6 Cauchy's Counterexample, Part 2.

                                                                          ym
 (a) Let m be a nonnegative integer. Show that lim y2 = 0.

                                                                 y e
       Hint.
       Induction and a dash of L'Hpital's rule should do the trick.

                              q(y)
(b) Prove that lim y2 = 0 for any polynomial q.

                     y e

(c) Let f (x) be as in equation (I.1) and show that for every nonnegative
     integer n, f (n)(0) = 0.

                                                                                                    
    This example showed that while it was fruitful to exploit Taylor series rep-
resentations of various functions, basing the foundations of calculus on power
series was not a sound idea.
    While Lagrange's approach wasn't totally successful, it was a major step
away from infinitesimals and toward the modern approach. We still use aspects
of it today. For instance we still use his prime notation (f ) to denote the
derivative.
    Turning Lagrange's idea on its head it is clear that if we know how to com-
pute derivatives, we can use this machine to obtain a power series when we are
not "clever enough" to obtain the series in other (typically shorter) ways. For
example, consider Newton's binomial series when  = 12 . Originally, we ob-
tained this series by extending the binomial theorem to non-integer exponents.
Taylor's formula provides a more systematic way to obtain this series:

                                       1       f (0) = 1

f (x) = (1 + x) 2 ;                           f (0) = 1
                                                         2
f (x) = 1 (1 + x) 12 -1;
           2                                  f (0) = 1
                                                         2
f (x) = 1     1 - 1 (1 + x) 12 -2                           1
           2  2                                             2 -1
CHAPTER 4. QUESTIONS CONCERNING POWER SERIES                                                44

and in general since

         f (n)(x) = 1 1 - 1    1 - (n - 1) (1 + x) 12 -n
                           22                          2

we have

         f (n)(0) = 1 1 - 1    1 - (n - 1) .
                           22                          2

Using Taylor's formula we obtain the series

 f (n)(0) xn = 1+ 2  1     12 - 1    12 - (n - 1) xn  = 1+                               j=0 2 n-1 1 - j xn
     n!                               n!                                                        n!
n=0                   n=1
                                                                                       n=1

which agrees with equation (I.7) in the previous chapter.
Problem 4.1.7 Use Taylor's formula to obtain the general binomial series

                                                          j=0 n-1 ( - j) xn.
                                                              n!
                      (1 + x) = 1 +

                                                  n=1

                                                                                            

Problem 4.1.8 Use Taylor's formula to obtain the Taylor series for the func-

tions ex, sin x, and cos x expanded about a.                                                

As you can see, Taylor's "machine" will produce the power series for a

function (if it has one), but is tedious to perform. We will find, generally, that

this tediousness can be an obstacle to understanding. In many cases it will be

better to be clever if we can. This is usually shorter. However, it is comforting

to have Taylor's formula available as a last resort.

The existence of a Taylor series is addressed (to some degree) by the fol-

lowing.

Theorem 4.1.9 If f , f , . . . , f (n+1) are all continuous on an interval con-
taining a and x, then

f (x) = f (a) + f (a) (x - a) + f (a) (x - a)2 +    + f (n)(a) (x - a)n
         1!                    2!                                 n!

                                                             1  x
                                                          +
                                                                   f (n+1)(t)(x - t)n dt.
                                                          n! t=a

Before we address the proof, notice that the n-th degree polynomial

         f (a) + f (a) (x - a) + f (a) (x - a)2 +    + f (n)(a) (x - a)n
         1!                    2!                                 n!

resembles the Taylor series and, in fact, is called the n-th degree Taylor poly-
nomial of f about a. Theorem 4.1.9 says that a function can be written as the
sum of this polynomial and a specific integral which we will analyze in the next
chapter. We will get the proof started and leave the formal induction proof as
an exercise.

    Notice that the case when n = 0 is really a restatement of the Fundamental
Theorem of Calculus. Specifically, the FTC says t=a x f (t) dt = f (x) - f (a)
which we can rewrite as

                      f (x) = f (a) + 1 x f (t)(x - t)0 dt
                                         0! t=a
CHAPTER 4. QUESTIONS CONCERNING POWER SERIES                                45

to provide the anchor step for our induction.
    To derive the case where n = 1, we use integration by parts. If we let

            u = f (t)                               dv = (x - t)0dt
           du = f (t)dt                              v = - 1 (x - t)1
                                                             1

we obtain

                    1  - 1 f (t)(x - t)1|t=a x + 1   x
f (x) = f (a) +        1          1
                                                        f (t)(x - t)1 dt
                   0!
                                                    t=a

             1         - 1 f (x)(x - x)1 + 1 f (a)(x - a)1 + 1   x
= f (a) +              1  1                         1
                                                                    f (t)(x - t)1 dt
             0!
                                                                t=a

= f (a) + 1 f (a) (x - a)1 + 1 x f (t)(x - t)1 dt.
           1!             1! t=a

Problem 4.1.10 Provide a formal induction proof for Theorem 4.1.9. 

4.2 Series Anomalies

Up to this point, we have been somewhat frivolous in our approach to series.
This approach mirrors eighteenth century mathematicians who ingeniously ex-
ploited calculus and series to provide mathematical and physical results which
were virtually unobtainable before. Mathematicans were eager to push these
techniques as far as they could to obtain their results and they often showed
good intuition regarding what was mathematically acceptable and what was
not. However, as the envelope was pushed, questions about the validity of the
methods surfaced.

    As an illustration consider the series expansion

                                  1 = 1 - x + x2 - x3 +    .
                               1+x

    If we substitute x = 1 into this equation, we obtain

                                   1 = 1-1+1-1+렁.
                                   2

    If we group the terms as follows (1-1)+(1 -1)+  , the series would equal
0. A regrouping of 1 + (-1 + 1) + (-1 + 1) +    provides an answer of 1. This
violation of the associative law of addition did not escape the mathematicians
of the 1700's. In his 1760 paper On Divergent Series Euler said:

       Notable enough, however are the controversies over the series 1 -
       1 + 1 - 1 + etc, whose sum was given by Leibniz as 12 , although
       others disagree . . . Understanding of this question is to be sought
       in the word "sum;" this idea, if thus conceived - namely, the sum
       of a series is said to be that quantity to which it is brought closer
       as more terms of a series are taken - has relevance only for the
       convergent series, and we should in general give up this idea of
       sum for divergent series. On the other hand, as series in analysis
       arise from the expansion of fractions or irrational quantities or even
       of transcendentals, it will, in turn, be permissible in calculation
       to substitute in place of such series that quantity out of whose
       development it is produced.
CHAPTER 4. QUESTIONS CONCERNING POWER SERIES                                                               46

Even with this formal approach to series, an interesting question arises. The

series   for    the  antiderivative       of      1   does  converge   for  x  =  1  while  this    one    does
                                                1+x
not. Specifically, taking the antiderivative of the above series, we obtain

                                    ln(1 + x) = x - 1 x2 + 1 x3 -    .
                                                         23

     If  we   substitute         x  =  1  into  this  series,  we  obtain  ln 2  =  1-  1  +  1  -   .  It  is
                                                                                        2     3
not hard to see that such an alternating series converges. The following picture

shows why. In this diagram, Sn denotes the partial sum 1- 21 + 31 -  + n (-1)n+1 .

    From the diagram we can see S2  S4  S6          S5  S3  S1
and S2k+1 - S2k = 2k+1 1 . It seems that the sequence of partial sums will
converge to whatever is in the "middle." Our diagram indicates that it is ln
2 in the middle but actually this is not obvious. Nonetheless it is interesting
that one series converges for x = 1 but the other does not.

Problem 4.2.1 Use the fact that

         1 - 1 + 1 -    + (-1)2k+1  ln 2  1 - 1 + 1 -    + (-1)2k+2
                23                        2k                           23                  2k + 1

to determine how many terms of the series n=1 n  (-1)n+1 should be added
together to approximate ln 2 to within .0001 without actually computing what

ln 2 is.                                                                                                       

     There is an even more perplexing situation brought about by these exam-

ples. An infinite sum such as 1 - 1 + 1 - 1 +    appears to not satisfy the

associative        law   for  addition.   While       a  convergent    series  such  as  1-      1  +  1  -렁
                                                                                                 2     3
does satisfy the associative law, it does not satisfy the commutative law. In

fact, it does not satisfy it rather spectacularly.

     A generalization of the following result was stated and proved by Bernhard

Riemann in 1854.

Theorem 4.2.2 Let a be any real number. There exists a rearrangement of

the  series     1  -  1  +    1  -  렁   which    converges   to  a.
                      2       3

     This theorem shows that a series is most decidedly not a great big sum. It

follows that a power series is not a great big polynomial.

     To set the stage, consider the harmonic series

                                           1 = 1+ 1 + 1 +렁.
                                                n           23
                                          n=1

    Even though the individual terms in this series converge to 0, the series
still diverges (to infinity) as evidenced by the inequality

             1           11               1111                         1             1
     1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 +    + 16 +   
CHAPTER 4. QUESTIONS CONCERNING POWER SERIES                                                          47

                  1 11                    1111                          1             1
              > 2 + 4 + 4 + 8 + 8 + 8 + 8 + 16 +    + 16 +   

                 1111
              = 2 + 2 + 2 + 2 +렁

              = .

Armed with this fact, we can see why Theorem 4.2.2 is true. First note

that              111                          1 11

                  - 2 - 4 - 6 -    = - 2 (1 + 2 + 3 +    ) = -

and                     1 + 1 + 1 +     1 + 1 + 1 + . . . = .

                                35             246

This     says     that  if  we  add  enough    terms   of   - 12  -  1  -  1  -  렁  we     can   make
                                                                     4     6
such  a  sum  as  small     as  we  wish  and  if  we  add  enough      terms  of         1     1
                                                                                   1  +   3  +  5  +      

we can make such a sum as large as we wish. This provides us with the general

outline of the proof. The trick is to add just enough positive terms until the

sum is just greater than a. Then we start to add on negative terms until the

sum is just less than a. Picking up where we left off with the positive terms,

we add on just enough positive terms until we are just above a again. We then

add on negative terms until we are below a. In essence, we are bouncing back

and forth around a. If we do this carefully, then we can get this rearrangement

to converge to a. The notation in the proof below gets a bit hairy, but keep

this general idea in mind as you read through it.

Let      O1   be  the   first   odd  integer  such    that  1+    1  +  1  + 렁 +     1  > a.     Now
                                                                  3     5             O1
choose E1 to be the first even integer such that

             -1 - 1 - 1 - 렁 - 1 < a - 1 + 1 + 1 + 렁 + 1 .
              246                         E1                   35                O1

Thus           1 + 1 + 1 +    + 1 - 1 - 1 - 1 -    - 1 < a.

                  35                      O1 2 4 6                         E1

    Notice that we still have O1+2 1 + O1+4 1 +    = . With this in mind, choose
O2 to be the first odd integer with

1 + 1 +   1 > a- 1 + 1 + 1 +    + 1 - 1 - 1 - 1 -    - 1 .
O1 + 2 O1 + 4               O2                 35                 O1 2 4 6                               E1

Thus we have

a < 1+ 1 + 1 +렁+ 1 - 1 - 1 - 1 -렁- 1 + 1 + 1 +렁+ 1 .
         35                 O1 2 4 6                   E1 O1 + 2 O1 + 4                            O2

Furthermore, since

11                1 111                            1        1           1                    1
1+ 3 + 5 +  + O1 - 2 - 4 - 6 -  - E1 + O1 + 2 + O1 + 4 +  + O2 - 2 < a

we have

                        11                    1 111
                  1 + 3 + 5 +    + O1 - 2 - 4 - 6 -   

                  - 1 + 1 + 1 +렁+ 1 -a < 1 .
                  E1 O1 + 2 O1 + 4                             O2              O2
CHAPTER 4. QUESTIONS CONCERNING POWER SERIES                                                 48

In a similar fashion choose E2 to be the first even integer such that

             11                 1 111
       1 + 3 + 5 +    + O1 - 2 - 4 - 6 -   

                                1           1              1
                             - E1 + O1 + 2 + O1 + 4 +   

                                1           1              1
                             + O2 - E1 + 2 - E1 + 4 -   

                             - 1 < a.
                                E2

Since

11                     1 111                         1
1 + 3 + 5 +    + O1 - 2 - 4 - 6 -    - E1

                 1           1                 1           1              1               1
       + O1 + 2 + O1 + 4 +    + O2 - E1 + 2 - E1 + 4 -    - E2 - 2 > a

then

      11               1 111                            1
1 + 3 + 5 +    + O1 - 2 - 4 - 6 -    - E1

                 1           1                 1              1           1               1
          + O1 + 2 + O1 + 4 +    + O2 - E1 + 2 - E1 + 4 -    - E2 - a

          < 1.
             E2

Again choose O3 to be the first odd integer such that

       11                    1 111
a < 1 + 3 + 5 +    + O1 - 2 - 4 - 6 -   

                 1        1              1                 1
             - E1 + O1 + 2 + O1 + 4 +    + O2 +   

                    1           1                 1           1              1            1
             - E1 + 2 - E1 + 4 -    - E2 + O2 + 2 + O2 + 4 +    + O3

and notice that

      11               1 11
1 + 3 + 5 +    + O1 - 2 - 4

          1            1           1           1                    1
      - 6 -    - E1 + O1 + 2 + O1 + 4 +    + O2 +   

             1            1                 1           1              1               1
      - E1 + 2 - E1 + 4 -    - E2 + O2 + 2 + O2 + 4 +    + O3 - a

       < 1.
          O3

Continue defining Okand Ek            in this fashion.     Since limk            1  =  limk   1  =
                                                                                Ok           Ek
0, it is evident that the partial sums

                    11                1 11
             1 + 3 + 5 +    + O1 - 2 - 4

                       1              1           1              1
                       - 6 -    - E1 + O1 + 2 + O1 + 4 +   

                       1                    1                    1     -렁
                       + +렁-                       -
                       O2             Ek-2 + 2 Ek-2 + 4
CHAPTER 4. QUESTIONS CONCERNING POWER SERIES                                                        49

                         -      1      +      1       +        1  +렁+            1

                              Ek-1 Ok-1 + 2 Ok-1 + 4                           Ok

and

                      11                  1 111
                1 + 3 + 5 +    + O1 - 2 - 4 - 6

                                    1          1            1              1
                         -    - E1 + O1 + 2 + O1 + 4 +    + O2 +   

                         -      1         -        1     -렁-    1

                            Ek-2 + 2 Ek-2 + 4                     Ek-1

must converge to a. Furthermore, it is evident that every partial sum of the
rearrangement

                      11                  1 111
                1 + 3 + 5 +    + O1 - 2 - 4 - 6

                                    1          1            1              1
                         -    - E1 + O1 + 2 + O1 + 4 +    + O2 +   

is trapped between two such extreme partial sums. This forces the entire
rearranged series to converge to a.

    The next two problems are similar to the above, but notationally are easier
since we don't need to worry about converging to an actual number. We only
need to make the rearrangement grow (or shrink in the case of problem 4.2.4)
without bound.

Problem         4.2.3    Show   that   there  is   a  rearrangement  of    1-      1  +  1  -    1  + 렁
                                                                                   2     3       4
which diverges to .                                                                                     

Problem         4.2.4    Show   that   there  is   a  rearrangement  of    1-      1  +  1  -    1  + 렁
                                                                                   2     3       4
which diverges to -.                                                                                    

    It is fun to know that we can rearrange some series to make them add up to

anything you like but there is a more fundamental idea at play here. That the

negative terms of the alternating Harmonic Series diverge to negative infinity

and the positive terms diverge to positive infinity make the convergence of the

alternating series very special.

    Consider, first we add 1. This is one of the positive terms so our sum is start-

ing to increase without bound. Next we add -1/2 which is one of the negative

terms so our sum has turned around and is now starting to decrease without

bound. Then another positive term is added: increasing without bound. Then

another negative term: decreasing. And so on. The convergence of the alter-

nating Harmonic Series is the result of a delicate balance between a tendency

to run off to positive infinity and back to negative infinity. When viewed in

this light it is not really too surprising that rearranging the terms can destroy

this delicate balance.

    Naturally, the alternating Harmonic Series is not the only such series. Any

such series is said to converge "conditionally" -- the condition being the specific

arrangement of the terms.

    To stir the pot a bit more, some series do satisfy the commutative property.

More      specifically,  one  can   show     that  any  rearrangement      of  the    series  1  -  1   +
                                                                                                    22
1               must  converge  to  the  same  value    as  the  original  series  (which     happens
32  -      

to be x=0 x 1 ln (1+x) dx  .8224670334). Why does one series behave so nicely
whereas the other does not?

    Issues such as these and, more generally, the validity of using the infinitely

small and infinitely large certainly existed in the 1700's, but they were over-

shadowed by the utility of the calculus. Indeed, foundational questions raised
QUESTIONS CONCERNING POWER SERIES  50

by the above examples, while certainly interesting and of importance, did not
significantly deter the exploitation of calculus in studying physical phenomena.
However, the envelope eventually was pushed to the point that not even the
most practically oriented mathematician could avoid the foundational issues.

4.3 Additional Problems

Problem 4.3.1 Use Taylor's formula to find the Taylor series of the given
function expanded about the given point a.

 (a) f (x) = ln (1 + x), a = 0
 (b) f (x) = ex, a = -1
 (c) f (x) = x3 + x2 + x + 1, a = 0
 (d) f (x) = x3 + x2 + x + 1, a = 1

                                                                                                    
    Part II
Interregnum

                     51
Chapter 5

Joseph Fourier: The Man Who
Broke Calculus

5.1 Joseph Fourier and His Series

Applying mathematics to physical problems such as heat flow in a solid body
drew much attention in the latter part of the 1700's and the early part of the
1800's. One of the people to attack the heat flow problem was

Figure 5.1.1 Jean Baptiste Joseph Fourier13
    Jean Baptiste Joseph Fourier. Fourier submitted a manuscript on the sub-

ject, Sur la propagation de la chaleur (On the Propagation of Heat), to the
Institut National des Sciences et des Arts in 1807. These ideas were subse-
quently published in La theorie analytique de la chaleur (The Analytic Theory
of Heat (1822)).

    To examine Fourier's ideas, consider the example of a thin wire of length
one, which is perfectly insulated and whose endpoints are held at a fixed tem-
perature of zero. Given an initial temperature distribution in the wire, the
problem is to monitor the temperature of the wire at any point x and at any
time t. Specifically, if we let u(x, t) denote the temperature of the wire at
point x  [0, 1] at time t  0, then it can be shown that u must satisfy the one-
dimensional heat equation  x2 2 2u = t u , where 2 is a positive constant known
as the thermal diffusivity. If the initial temperature distribution is given by

  13mathshistory.st-andrews.ac.uk/Biographies/Fourier/

                                                  52
CHAPTER 5. JOSEPH FOURIER: THE MAN WHO BROKE CALCULUS53

the function f (x), then the u we are seeking must satisfy all of the following
                                            2 2u u

                                            2=
                                              x t

                              u(0, t) = u(1, t) = 0,  t  0
                              u(x, 0) = f (x),  x  [ 0, 1].

     To solve this, Fourier employed what is now referred to as Fourier's method

of separation of variables. Specifically, Fourier looked for solutions of the form

u(x, t) = X(x)T (t); that is, solutions where the x-part can be separated from

the  t-part.  Assuming   that  u  has  this  form,  we  get  2u  =     X  T  and  u  =  X T.
                                                             x2                   t
Substituting these into the differential equation  x2 2 2u = t u , we obtain

                         2XT = XT  or X = .  T 
                                                  X 2T

    Since the left-hand side involves no t's and the right-hand side involves no
x's, both sides must equal a constant k. Thus we have

                              X = kX and T  = 2kT .

Problem 5.1.2 Show that T = Ce2kt satisfies the equation T  = 2kT ,
where C, and  are arbitrary constants. Use the physics of the problem to
show that if u is not constantly zero, then k < 0.

Hint.

     Consider limt u(x, t).                                                                

     Using the result from problem 5.1.2 that k < 0, we will let k = -p2.

Problem 5.1.3 Show that X = A sin (px) + B cos (px) satisfies the equation

X  = -p2X, where A and B are arbitrary constants. Use the boundary

conditions u(0, t) = u(1, t) = 0,  t  0 to show that B = 0 and A sin p = 0.

Conclude that if u is not constantly zero, then p = n, where n is any integer.

                                                                                           

Problem 5.1.4                                                 x2 2 2u

     Show  that  if  u1  and  u2  satisfy  the  equations              =  u  and  u(0, t)  =
                                                                          t
u(1, t) = 0,  t  0 then u = A1u1 + A2u2 satisfy these as well, where A1 and

A2 are arbitrary constants.                                                                

     Putting all of these results together, Fourier surmised that the general

solution to           2 2u u
                      2 = u(0, t) = u(1, t) = 0,  t  0

                        x t

could be expressed as the series

                                            

                         u(x, t) = Ane-(n)2t sin (nx) .

                                          n=1

    All that is left is to have u satisfy the initial condition u(x, 0) = f (x),
 x  [ 0, 1]. That is, we need to find coefficients An, such that

                                                          

                         f (x) = u(x, 0) = An sin (nx) .

                                                         n=1

    The idea of representing a function as a series of sine waves was proposed
by Daniel Bernoulli in 1753 while examining the problem of modeling a vibrat-
ing string. Unfortunately for Bernoulli, he didn't know how to compute the
CHAPTER 5. JOSEPH FOURIER: THE MAN WHO BROKE CALCULUS54

coefficients in such a series representation. What distinguished Fourier was
that he developed a technique to compute these coefficients. The key is the
result of the following problem.

Problem 5.1.5 Let n and m be positive integers. Show

                10                                 if n = m if n = m .
                   sin (nx) sin (mx) dx = 1

                x=0                             2

                                                                                                    
    Armed with the result from Problem 5.1.5, Fourier could compute the co-
efficients An in the series representation f (x) = n=1  An sin (nx) in the
following manner. Since we are trying to find An for a particular (albeit gen-
eral) n, we will temporarily change the index in the summation from n to j.

With this in mind, consider

                                                   

       1                        1     

           f (x) sin (nx) dx =      Aj sin (jx) sin (nx) dx

      x=0                       x=0 j=1

                                       1

                        = Aj               sin (jx) sin (nx) dx

                            j=1       x=0

                                 1
                        = An  2

    This leads to the formula An = 2 x=0 1 f (x) sin (nx) dx.
    The above series f (x) = n=1  An sin (nx) with An = 2 x=0 1 f (x) sin (nx) dx
is called the Fourier (sine) series of f .

Example  5.1.6  Let's apply this to the following function,  f (x) =    12 -  x-  1  ,
                                                                                  2
whose graph of this function is seen below.

                                                                                                    
Problem 5.1.7 Let n be a positive integer. Show that if

                                   1         1
                        f (x) = 2 - x - 2

then                 1

                                             2     n
                        f (x) sin (nx) dx = 2 sin
                x=0                          (n)   2
CHAPTER 5. JOSEPH FOURIER: THE MAN WHO BROKE CALCULUS55

and show that the Fourier sine series of f is given by

                 4            n                       4          (-1)k
                       2 sin      sin (nx) = 2                           2 sin ((2k + 1) x) .
f (x) =                       2                        k=0 (2k + 1)
          n=1 (n)

                                                                                                
To check if this series really does, in fact, represent f on [0, 1], let

                                    4N       (-1)k
                       SN (x) = 2                    2 sin ((2k + 1) x)
                                  k=0 (2k + 1)

be the N th partial sum of the series and use the graphing tool below to view

the graph of SN (x) for several values of N .
    That is, SN denotes the N th partial sum of the series. We will graph SN

for N = 1, 2, 5, 50.

As you can see, it appears that as we add more terms to the partial sum,

SN ,  it  looks  more  and  more  like  the  original    function   f (x) =  12 -  x-  1  .  This
                                                                                       2
would lead us to believe that the series converges to the function and that

                                  4     (-1)k                                                (II.1)
                       f (x) = 2                2 sin ((2k + 1) x) .
                               k=0 (2k + 1)

is a valid representation of f as a Fourier series.
    Recall, that when we represented a function as a power series, we freely

differentiated and integrated the series term by term as though it was a poly-
nomial. Let's do the same with this Fourier series.

    To start, notice that the derivative of

                                               1            1
                                  f (x) = 2 - x - 2

is given by

                              f (x) = 1           if 0  x < 21 .
                                           -1     if  1  <  x    1
                                                      2
CHAPTER 5. JOSEPH FOURIER: THE MAN WHO BROKE CALCULUS56

This  derivative   does  not  exist  at  x  =  1  and  its  graph  is  given  by
                                               2

    If we differentiate the Fourier series term-by-term, we obtain

                              4  (-1)k cos ((2k + 1) x) .
                               (2k + 1)

                                               k=0

    Again, if we let CN (x) =  k=0 (2k+1) 4 N (-1)k cos ((2k + 1) x) be the N th partial
sum of this Fourier cosine series and plot CN (x) for N = 5, we obtain

In  fact,  if  we  were  to  graph  the  series   4  k=0 (2k+1)  (-1)k cos((2k + 1) x), we
                                                  

would obtain
CHAPTER 5. JOSEPH FOURIER: THE MAN WHO BROKE CALCULUS57

    Notice that this agrees with the graph of f , except that f  didn't exist
at x = 12 , and this series takes on the value 0 at x = 12 . Notice also, that
every partial sum of this series is continuous, since it is a finite combination
of continuous cosine functions. This agrees with what you learned in calculus,
the (finite) sum of continuous functions is always continuous. In the 1700's,
this was also assumed to be true for infinite series, because every time a power
series converged to a function, that function happened to be continuous. This
never failed for power series, so this example was a bit disconcerting as it is an
example of the sum of infinitely many continuous functions which is, in this
case, discontinuous. Was it possible that there was some power series which
converged to a function which was not continuous? Even if there wasn't, what
was the difference between power series and this Fourier series?

    Even more disconcerting is what happens if we try differentiating the series

                               4  (-1)k cos ((2k + 1) x)
                                (2k + 1)

                                                k=0

term-by-term. Given the above graph of this series, the derivative of it should
be constantly 0, except at x = 12 , where the derivative wouldn't exist. Using
the old adage that the derivative of a sum is the sum of the derivatives, we
differentiate this series term-by-term to obtain the series

                                                

                               4 (-1)k+1 sin ((2k + 1) x) .

                                               k=0

    If we sum the first forty terms of this series, we get

We knew that there might be a problem at x =       1  but this is crazy!  The
                                                   2
series seems to not be converging to zero at all!

Problem 5.1.8      Show  that  when  x  =  1
                                           4

                                              11111
              k+1
4 (-1) sin ((2k + 1) x) = 4 -  +  +  -  -  +    .
                                              22222
k=0
CHAPTER 5. JOSEPH FOURIER: THE MAN WHO BROKE CALCULUS58

                                                                                                
Problem 5.1.8 shows that when we differentiate the series

                          4  (-1)k cos ((2k + 1) x)
                           (2k + 1)

                                         k=0

term by term, this differentiated series doesn't converge to anything at x = 14 ,
let alone converge to zero. In this case, the old calculus rule that the derivative
of a sum is the sum of the derivatives does not apply for this infinite sum,
though it did apply before. As if the continuity issue wasn't bad enough before,
this was even worse. Power series were routinely differentiated and integrated
term-by-term. This was part of their appeal. They were treated like "infinite
polynomials." Either there is some power series lurking that refuses to behave
nicely, or there is some property that power series have that not all Fourier
series have.

    Could it be that everything we did in Chapter 4 was bogus?
    Fortunately, the answer to that question is no. Power series are generally
much more well-behaved than Fourier series. Whenever a power series con-
verges, the function it converges to will be continuous. As long as one stays
inside the interval of convergence, power series can be differentiated and inte-
grated term-by-term. Power series have something going for them that your
average Fourier series does not. (We need to develop the machinery to know
what that something is.) None of this is any more obvious to us than it was
to mathematicians at the beginning of the nineteenth century. What they
did know was that relying on intuition was perilous and rigorous formulations
were needed to either justify or dismiss these intuitions. In some sense, the
nineteenth century was the "morning after" the mathematical party that went
on throughout the eighteenth century.

Problem 5.1.9 Let n and m be positive integers. Show

                    10                                            if n = m if n = m .
                       cos (nx) cos (mx) dx = 1

                    x=0                                     2

                                                                                                    
Problem 5.1.10 Use the result of Problem 5.1.9 to show that if

                                                                  

                                   f (x) = Bn cos (nx)

                                                                 n=1

on [0, 1], then

                                                 1

                              Bm = 2 f (x) cos (mx) dx.

                                               x=0

                                                                                               

Problem 5.1.11 Apply the result of Problem 5.1.10 to show that the Fourier

cosine  series  of  f (x)  =  x-  1  on  [0, 1]  is  given  by
                                  2

                           -4  1 cos ((2k + 1)x) .
                           2 k=0 (2k + 1)2

Let     C(x, N )    =  -4     N           1   cos ((2k  +   1)x)  and  plot  C(x, N )  for  N  =
                       2      k=0    (2k+1)2

1, 2, 5, 50  x  [ 0, 1].   How  does  this    compare   to  the   function   f (x) = x -    1  on
                                                                                            2
[ 0, 1]? What if you plot it for x  [ 0, 2]?                                                   
JOSEPH FOURIER: THE MAN WHO BROKE CALCULUS                                            59

Problem 5.1.12

(a) Differentiate the series

                         -4  1 cos ((2k + 1)x)
                         2 k=0 (2k + 1)2

term by term and plot various partial sums for that series on [ 0, 1]. How

does  this    compare  to  the  derivative  of  f (x)  =  x-  1  on  that  interval?
                                                              2

(b) Differentiate the series you obtained in part a and plot various partial

sums of that on [ 0, 1]. How does this compare to the second derivative

of  f (x)  =  x-  1  on  that  interval?
                  2

                                                                                      
             Part III
In Which We Find (Some)

             Answers

                                               60
Chapter 6

Convergence of Sequences and
Series

6.1 Sequences of Real Numbers

In Chapter 3, we developed the equation 1 + x + x2 + x3 +    = 1-x 1 , and
we mentioned there were limitations to this power series representation. For
example, substituting x = 1 and x = -1 into this expression leads to

                 1 + 1 + 1 +    = 1 and 1 - 1 + 1 - 1 +    = 1
                                         0                               2

which  are  rather  hard  to  accept.    On  the   other  hand,  if  we  substitute  x  =  1  into
                                                                                           2
                                         2+        3 +    = 2 which seems more palatable
the  expression  we  get  1  +  1  +  1      1
                                2     2      2
until we think about it. We can add two numbers together by the method we

all learned in elementary school. Or three. Or any finite set of numbers, at

least in principle. But infinitely many? What does that even mean? Before we

can add infinitely many numbers together we must find a way to give meaning

to the idea.

     To do this, we examine an infinite sum by thinking of it as a sequence of

finite partial sums. In our example, we would have the following sequence of

partial sums.

                                   12 1            13 n                            
       1, 1 + 1 , 1 + 1 +                ,1+ +          ,...,
                                   2            2                             j
                  22                               2 j=0
                                                                         1 , . . . .
                                                                         2

    We can plot these sums on a number line to see what they tend toward as
n gets large.

    Since each partial sum is located at the midpoint between the previous
partial sum and 2, it is reasonable to suppose that these sums tend to the
number 2. Indeed, you probably have seen an expression such as limn

     j=0 2 n 1 j = 2 justified by a similar argument. Of course, the reliance on
such pictures and words is fine if we are satisfied with intuition. However, we
must be able to make these intuitions rigorous without relying on pictures or
nebulous words such as "approaches."

                                                  61
CHAPTER 6. CONVERGENCE OF SEQUENCES AND SERIES                                                          62

     No doubt you are wondering "What's wrong with the word `approaches'?

It seems clear enough to me." This is often a sticking point. But if we think

carefully about what we mean by the word "approach" we see that there is an

implicit assumption that will cause us some difficulties later if we don't expose

it.

    To see this consider the sequence 1, 12 , 13 , 14 , . . . . Clearly it "approaches"
zero, right? But, doesn't it also "approach" -1? It does, in the sense that

each term gets closer to -1 than the one previous. But it also "approaches"

-2, -3, or even -1000 in the same sense. That's the problem with the word

"approaches." It just says that we're getting closer to something than we were

in the previous step. It does not tell us that we are actually getting close. Since

the moon moves in an elliptical orbit about the earth for part of each month it

is "approaching" the earth. The moon gets closer to the earth but, thankfully,

it does not get close to the earth. The implicit assumption we alluded to earlier
is this: When we say that the sequence n n=1 1  "approaches" zero we mean that
it is getting close not closer. Ordinarily this kind of vagueness in our language

is pretty innocuous. When we say "approaches" in casual conversation we can

usually tell from the context of the conversation whether we mean "getting

close to" or "getting closer to." But when speaking mathematically we need to

be more careful, more explicit, in the language we use.

     So how can we change the language we use so that this ambiguity is elimi-

nated? Let's start out by recognizing, rigorously, what we mean when we say

that a sequence converges to zero. For example, you would probably want to
say that the sequence 1, 12 , 13 , 14 , . . . = n n=1 1  converges to zero. Is there a
way to give this meaning without relying on pictures or intuition?

     One   way  would  be   to   say   that  we     can  make      1   as  close   to  zero   as  we   wish,
                                                                   n
provided we make n large enough. But even this needs to be made more

specific.  For  example,    we   can   get   1  to  within      a  distance    of  .1  of  0  provided  we
                                             n
make          10,  we  can  get  1  to  within      a  distance    of       of     provided       we   make
      n    >                     n                                     .01      0

n > 100, etc. After a few such examples it is apparent that given any arbitrary

distance    > 0,   we  can  get     1  to  within        of  0  provided     we    make       n>  1 .  This
                                    n
leads to the following definition.

Definition 6.1.1 Let (sn) = (s1, s2, s3, . . .) be a sequence of real numbers.

We say that (sn) converges to 0 and write limn sn = 0 provided for any

 > 0, there is a real number N such that if n > N , then |sn| < .                                       

     Notes on Definition 6.1.1:

     1. This definition is the formal version of the idea we just talked about;

        that is, given an arbitrary distance , we must be able to find a specific

        number N such that sn is within  of 0, whenever n > N . The N is the
        answer to the question of how large is "large enough" to put sn this close
        to 0.

     2. Even though we didn't need it in the example                        1   ,  the     absolute    value
                                                                            n
      appears in the definition because we need to make the distance from sn

      to 0 smaller than . Without the absolute value in the definition, we

      would be able to "prove" such outrageous statements as limn -n = 0,

      which we obviously don't want.

     3. The statement |sn| <  can also be written as - < sn <  or sn  (-, ).
        (See the Problem 6.1.2 below.) Any one of these equivalent formulations
        can be used to prove convergence. Depending on the application, one of
        these may be more advantageous to use than the others.
CHAPTER 6. CONVERGENCE OF SEQUENCES AND SERIES                                               63

4. Any time an N can be found that works for a particular , any number
   M > N will work for that  as well, since if n > M then n > N .

Problem 6.1.2 Let a and b be real numbers with b > 0. Prove |a| < b if and

only if -b < a < b. Notice that this can be extended to |a|  b if and only if

-b  a  b.                                                                                    

To illustrate how this definition makes the above ideas rigorous, let's use it to

prove that lim 1 = 0.
              n n

Proof.        Let  > 0 be given.     Let N = 1 .       If  n     >  N,  then  n   >  1  and  so
                                                                                     
| 1n |     1     .  Hence  by    definition,  limn  1      0.                                
        =  n  <                                     n  =

        Notice that this proof is rigorous and makes no reference to vague notions

such as "getting smaller" or "approaching infinity." It has three components:

1. provide the challenge of a distance  > 0,

2. identify a real number N , and

   3. show that this N works for this given .

There is also no explanation about where N came from. While it is true that
this choice of N is not surprising in light of the "scrapwork" we did before the
definition, the motivation for how we got it is not in the formal proof nor is
it required. In fact, such scrapwork is typically not included in a formal proof.
For example, consider the following.

Example 6.1.3 Use the definition of convergence to zero to prove

                                      lim sin n = 0.
                                     n n

Proof.        Let  > 0. Let N = 1 . If n > N , then n >                 1 and 1n  < .       
                                                                                        Thus
 sin(n)          1  <  .  Hence  by  definition,  limn    sin n  =  0.
     n           n                                          n                               

    Notice that the N came out of nowhere, but you can probably see the
thought process that went into this choice: We needed to use the inequality
|sin n|  1. Again this scrapwork is not part of the formal proof, but it is
typically necessary for finding what N should be. You might be able to do
the next problem without doing any scrapwork first, but don't hesitate to do
scrapwork if you need it.

Problem 6.1.4 Use the definition of convergence to zero to prove the follow-
ing.

              1
(a) lim 2 = 0

      n n
              1

(b) lim  = 0
      n n

                                                                                                    
    As the sequences get more complicated, doing scrapwork ahead of time will
become more necessary.

Example 6.1.5 Use the definition of convergence to zero to prove

                                                n+4
                                         lim 2 = 0.
                                        n n + 1
    SCRAPWORK
    Given an  > 0, we need to see how large to make n in order to guarantee
CHAPTER 6. CONVERGENCE OF SEQUENCES AND SERIES                                                              64

that  | n2+1 n+4 |  <  .  First  notice  that  n+4      <   n2 n+4 .  Also,  notice  that  if  n   >  4,  then
                                               n2 +1

n+4   <   n+n          =  2n.  So   as  long  as  n  >  4,  we  have  n+4    <  n+4  <         2n  =  2n .  We
                                                                      n2 +1      n2            n2

can make this less than  if we make n > 2 . This means we need to make n > 4
and n > 2 , simultaneously. These can be done if we let N be the maximum
of these two numbers. This sort of thing comes up regularly, so the notation

N = max 4, 2 was developed to mean the maximum of these two numbers.
Notice that if N = max 4, 2 then N  4 and N  2 . We're now ready for
the formal proof.                                                                                           

Proof. Let  > 0. Let N = max 4, 2 . If n > N , then n > 4 and n > 2 . Thus
we  have               and  2       .   Therefore
          n  >      4       n    <

                            n + 4 n + 4 n + 4 2n 2
                                     = 2 < 2 < 2 = < .
                          n2 + 1 n + 1 n                              nn

                                        n+4
Hence by definition, lim 2 = 0.                                                                             
                               n n + 1

    Again we emphasize that the scrapwork is not explicitly a part of the

formal proof. However, if you look carefully, you can always find the scrapwork

in the formal proof.

Problem 6.1.6 Use the definition of convergence to zero to prove

                                         lim   n2 + 4n + 1      = 0.

                                        n            n  3

                                                                                                    
Problem 6.1.7 Let b be a nonzero real number with |b| < 1 and let  > 0.

(a) Solve the inequality |b|n <  for n

(b) Use part (a) to prove limn bn = 0.

                                                                                                    
    We can negate this definition to prove that a particular sequence does not
converge to zero.

Example 6.1.8 Use the definition to prove that the sequence

                                 (1 + (-1)n)n=0  = (2, 0, 2, 0, 2, . . .)

does not converge to zero.                                                                                  

    Before we provide this proof, let's analyze what it means for a sequence (sn)

to not converge to zero. Converging to zero means that any time a distance

 > 0 is given, we must be able to respond with a number N such that |sn| < 

for every n > N . To have this not happen, we must be able to find some  > 0

such that no choice of N will work. Of course, if we find such an , then any

smaller one will fail to have such an N , but we only need one to mess us up. If

you stare at the example long enough, you see that any  with 0 <   2 will

cause problems. For our purposes, we will let  = 2.

Proof. Let  = 2 and let N  N be any integer. If we let k be any non-
negative integer with k > N2 , then n = 2k > N , but |1 + (-1)n| = 2. Thus no
choice of N will satisfy the conditions of the definition for this , (namely that

|1 + (-1)n| < 2 for all n > N ) and so limn (1 + (-1)n) = 0.                                                

Problem 6.1.9 Negate the definition of limn sn = 0 to provide a formal

definition for limn sn = 0.                                                                                 
CHAPTER 6. CONVERGENCE OF SEQUENCES AND SERIES                                            65

Problem       6.1.10  Use  the   definition     to  prove  limn     n       =   0.        
                                                                 n+100

Now that we have a handle on how to rigorously prove that a sequence

converges to zero, let's generalize this to a formal definition for a sequence

converging to something else. Basically, we want to say that a sequence (sn)
converges to a real number s, provided the difference (sn - s) converges to zero.
This leads to the following definition:

Definition 6.1.11 Let (sn) = (s1, s2, s3, . . .) be a sequence of real numbers

and let s be a real number. We say that (sn) {converges to} s and write

limn sn = s provided for any  > 0, there is a real number N such that if

n > N , then |sn - s| < .                                                                 

{Notes on Definition 6.1.11}

1. Clearly

                       lim sn = s if and only if lim (sn - s) = 0.
                      n                                    n

2. Again notice that this says that we can make sn as close to s as we wish
   (within ) by making n large enough (> N ). As before, this definition

   makes these notions very specific.

3. Notice that |sn - s| <  can be written in the following equivalent forms

        (a) |sn - s| < 
        (b) - < sn - s < 
        (c) s -  < sn < s + 
        (d) sn  (s - , s + )

      and we are free to use any one of these which is convenient at the time.

    As an example, let's use this definition to prove that the sequence in Prob-
lem 6.1.10, in fact, converges to 1.

Example 6.1.12 Prove lim n = 1.
                                 n n + 100

    SCRAPWORK

Given an  > 0, we need to get n+100 n - 1 < . This prompts us to do

some algebra.

                            n - 1 = n - (n + 100)  100 .
                       n + 100                      n + 100            n

This      in  turn,  seems  to  suggest      that   N  =  100  should  work.              
                                                           

Proof.  Let   >  0.   Let  N  =   100 .  If  n > N,    then  n>  100   and  so  100  < .  Hence
                                                                                 n

                 n - 1 = n - (n + 100) = 100 < 100 < .
              n + 100                    n + 100             n + 100 n

Thus  by  definition  limn          n        =  1.                                        
                                 n+100

Notice again that the scrapwork is not part of the formal proof and the

author of a proof is not obligated to tell where the choice of N came from

(although the thought process can usually be seen in the formal proof). The

formal proof contains only the requisite three parts: provide the challenge of

an arbitrary  > 0, provide a specific N , and show that this N works for the

given .

Also notice that given a specific sequence such as                        n     , the definition
                                                                       n+100

does not indicate what the limit would be if, in fact, it exists. Once an educated
CHAPTER 6. CONVERGENCE OF SEQUENCES AND SERIES                                                                 66

guess is made as to what the limit should be, the definition only verifies that

this intuition is correct.

This leads to the following question: If intuition is needed to determine

what a limit of a sequence should be, then what is the purpose of this relatively

non-intuitive, complicated definition?

Remember that when these rigorous formulations were developed, intuitive

notions of convergence were already in place and had been used with great suc-

cess. This definition was developed to address the foundational issues. Could

our intuitions be verified in a concrete fashion that was above reproach? This

was the purpose of this non-intuitive definition. It was to be used to verify

that our intuition was, in fact, correct and do so in a very prescribed manner.

For example, if b > 0 is a fixed number, then you would probably say as n

approaches infinity, b( n1 ) approaches b0 = 1. After all, we did already prove

that  limn    1  = 0.  We should be able to back up this intuition with our
              n
rigorous definition.

Problem 6.1.13 Let b > 0. Use the definition to prove lim b( n1 ) = 1.

                                                                                                            n

Hint. You will probably need to separate this into two cases: 0 < b < 1 and

b  1.                                                                                                          

Problem 6.1.14

(a) Provide a rigorous definition for lim sn = s.

                                                                      n

(b) Use your definition to show that for any real number a, lim ((-1)n) = a.

                                                                                                                n

      Hint. Choose  = 1 and use the fact that a - (-1)n < 1 is equivalent

      to (-1)n - 1 < a < (-1)n + 1 to show that no choice of N will work for
      this .

                                                                                                               

6.2 The Limit as a Primary Tool

As you've seen from the previous sections, the formal definition of the conver-
gence of a sequence is meant to capture rigorously our intuitive understanding
of convergence. However, the definition itself is an unwieldy tool. If only there
was a way to be rigorous without having to run back to the definition each
time. Fortunately, there is a way. If we can use the definition to prove some
general rules about limits then we could use these rules whenever they applied
and be assured that everything was still rigorous. A number of these should
look familiar from calculus.

Problem 6.2.1 Let (c)n=1  = (c, c, c, . . .) be a constant sequence. Show that
limn c = c.
                                                                                                               

In proving the familiar limit theorems, the following will prove to be a very

useful tool.

Lemma 6.2.2

         Triangle      Let a and b be real numbers. Then
      Inequality:                               |a + b|  |a| + |b|.
CHAPTER 6. CONVERGENCE OF SEQUENCES AND SERIES                      67

   Reverse     Let a and b be real numbers. Then
   Triangle                              |a| - |b|  |a - b|
Inequality:

Problem 6.2.3

(a) Prove Lemma 6.2.2.

Hint.
For the Reverse Triangle Inequality, consider |a| = |a - b + b|.

(b) Show ||a| - |b||  |a - b|.

Hint.
You want to show |a| - |b|  |a - b| and -(|a| - |b|)  |a - b|.

                                                                    

Theorem 6.2.4 If lim an = a and lim bn = b, then lim (an + bn) = a + b.
               n                    n        n

We will often informally state this theorem as "the limit of a sum is the

sum of the limits." However, to be absolutely precise, what it says is that if

we already know that two sequences converge, then the sequence formed by

summing the corresponding terms of those two sequences will converge and, in

fact, converge to the sum of those individual limits. We'll provide the scrapwork

for the proof of this and leave the formal write-up as an exercise. Note the use

of the triangle inequality in the proof.

Problem 6.2.5 Prove Theorem 6.2.4.

SCRAPWORK:

If we let  > 0, then we want N so that if n > N , then (an + bn) -

(a + b) < . We know that lim an = a and lim bn = b, so we can make
                               n          n

an - a and bn - b as small as we wish, provided we make n large enough.

Let's go back to what we want, to see if we can close the gap between what we

know and what we want.

|(an + bn) - (a + b)| = |(an - a) + (bn - b)|  |an - a| + |bn - b|

by the triangle inequality. To make this whole thing less than , it makes sense

to make each part less than 2 . it makes sense to make each part less than 2 .
Fortunately, we can do that as the definitions of lim an = a and lim bn = b
                                          n                  n

allow us to make an - a and bn - b arbitrarily small. Specifically, since

 lim an = a, there exists an N1 such that if n > N1 then an - a < 2 . Also

n

since lim bn = b, there exists an N2 such that if n > N2 then bn - b < 2 .

          n

Since we want both of these to occur, it makes sense to let N =max(N1, N2).

This should be the N that we seek.                                  

Theorem 6.2.6 If lim an = a and lim bn = b, then lim (anbn) = ab.
               n                    n        n

SCRAPWORK: Given  > 0, we want N so that if n > N , then anbn -

ab < . One of the standard tricks in analysis is to "uncancel." In this case we

will subtract and add a convenient term. Normally these would "cancel out,"

which is why we say that we will uncancel to put them back in. You already saw

an example of this in proving the Reverse Triangle Inequality (Problem 6.2.3).

In the present case, consider

               |anbn - ab| = |anbn - anb + anb - ab|
CHAPTER 6. CONVERGENCE OF SEQUENCES AND SERIES                                                         68

                                           |anbn - anb| + |anb - ab|
                                          = |an| |bn - b| + |b| |an - a| .

We can make this whole thing less than , provided we make each term in

the sum less than 2 . We can make                   b  an - a      <      if  we  make    an - a  < 2|b|  .
                                                                      2

But wait! What if b = 0? We could handle this as a separate case or we can do

the following "slick trick." Notice that we can add one more line to the above

string of inequalities: |an| |bn - b|+|b| |an - a| < |an| |bn - b|+(|b| + 1) |an - a|.

Now     we  can  make   |an     - a|  <             and   not      worry  about   dividing    by  zero.
                                          2(|b|+1)

Making           an     bn - b  <         requires     a  bit  more   finesse.    At first glance, one
                                      2
would be tempted to try and make bn - b < 2|an|  . Even if we ignore the fact

that we could be dividing by zero (which we could handle), we have a bigger

problem. According to the definition of limn bn = b, we can make bn - b

smaller than any given fixed positive number, as long as we make n large

enough (larger than some N which goes with a given epsilon). Unfortunately,

        is  not  fixed  as  it  has  the  variable     n  in  it;  there  is  no  reason  to  believe  that
2|an |
a single N will work with all of these simultaneously. To handle this impasse,

we need the following:

Lemma 6.2.7 A Convergent Sequence Is Bounded.
    If limn an = a, then there exists B > 0 such that |an|  B for all n.

Problem 6.2.8 Prove Lemma 6.2.7.

Hint.
    We know that there exists N such that if n > N , then |an - a| < 1. Let

B =max |a1| , |a2| , . . . , aN , |a| + 1 , where N  represents the smallest in-
teger greater than or equal to N . Also, notice that this is not a convergence
proof so it is not safe to think of N as a large number.

Comment. Actually, this is a dangerous habit to fall into even in convergence
proofs.

                                                                                                    
    Armed with this bound B, we can add on one more inequality to the above
scrapwork to get

                 |an  bn - a  b| = |an  bn - an  b + an  b - a  b|
                                     |an  bn - an  b| + |an  b - a  b|
                                    = |an| |bn - b| + |b| |an - a|
                                    < B |bn - b| + (|b| + 1) |an - a|

At this point, we should be able to make the last line of this less than .
END OF SCRAPWORK

Problem 6.2.9 Prove Theorem 6.2.6.                                                                       

Corollary 6.2.10 (Corollary to Theorem 6.2.6) If lim an = a and c  R,

                                                                                                     n

then lim c  an = c  a.

         n

Problem 6.2.11 Prove Corollary 6.2.10.                                                                   

Just as Theorem 6.2.6 says that the limit of a product is the product of the

limits, we can prove the analogue for quotients.

Theorem 6.2.12 Suppose lim an = a and lim bn = b. Also suppose b = 0
                                         n                         n

and bn = 0,  n. Then lim an = a .
                                n bn                b
CHAPTER 6. CONVERGENCE OF SEQUENCES AND SERIES                                69

Problem 6.2.13 Prove Theorem 6.2.12.
    SCRAPWORK
    To prove this, let's look at the special case of trying to prove

limn    1       = 1b . The general case will follow from this and Theorem 6.2.6.
        bn

Consider b1n - 1b = ||bb-n|b|nb|| . We are faced with the same dilemma as before;

we need to get  1     bounded above. This means we need to get |bn| bounded
                bn
away from zero (at least for large enough n).

    This can be done as follows. Since b = 0, then 2|b| > 0. Thus, by the
definition of limn bn = b , there exists N1 such that if n > N1, then
|b|-|bn|  b - bn < 2|b| . Thus when n > N1, 2|b| < |bn| and so |bn| 1 < |b|2 .

This says that for n > N1, |bn||b| |b-bn| < |b|2 2 |b - bn|. We should be able to make
this smaller than a given  > 0, provided we make n large enough.
                                                                              

    These theorems allow us to compute limits of complicated sequences and

rigorously verify that these are, in fact, the correct limits without resorting to

the definition of a limit.

Problem 6.2.14 Identify all of the theorems implicitly used to show that

                3n3 - 100n + 1                    n3 3 - n2 100 + n3 1  = 3.
            lim 3                      = lim 3                             5
            n 5n + 4n - 7 n n 5 + n - n32                   4     7

    Notice that this presumes that all of the individual limits exist. This will

become evident as the limit is decomposed.                                    

    There is one more tool that will prove to be valuable.

Theorem 6.2.15 Squeeze Theorem for Sequences

    Let (rn) , (sn), and (tn) be sequences of real numbers with rn  sn  tn, 
positive integers n. Suppose limn rn = s = limn tn. Then (sn) must
converge and limn sn = s.

Problem 6.2.16 Prove Theorem 6.2.15.

Hint. This is probably a place where you would want to use s- < sn < s+

instead of |sn - s| < .                                                       

    The Squeeze Theorem holds even if rn  sn  tn holds for only sufficiently

large n; i.e., for n larger than some fixed N0. This is true because when you

find an N1 that works in the original proof, this can be modified by choosing

N =max(N0, N1). Also note that this theorem really says two things: (sn)

converges and it converges to s. This subtle point affects how one should

properly use the Squeeze Theorem.

                                    n+1
Example 6.2.17 Prove lim 2 = 0.                                               
                            n n

Proof.  Notice  that  0     n+1     n+n  =  2n .  Since  lim 0 = 0 =    lim   2 , then by
                             n2      n2                  n              n n

                                 n+1
the Squeeze Theorem, lim 2 = 0.                                               
                         n n

    Notice that this proof is completely rigorous. Also notice that this is the

proper way to use the Squeeze Theorem. Here is an example of an {improper}

use of the Squeeze Theorem.

    How not to prove Example 6.2.17. Notice that

                                 n+1 n+n 2
                            0 2  2 = ,
                                    n             n      n

so                                          n+1                2

                      0 = lim 0  lim 2  lim = 0
                         n          n n              n n
CHAPTER 6. CONVERGENCE OF SEQUENCES AND SERIES                                  70

and
                                                n+1

                                          lim 2 = 0.
                                         n n

This  is  incorrect  in  form  because  it  presumes  that  limn  n+1  exists,  which
                                                                   n2
we don't yet know. If we knew that the limit existed to begin with, then this

would be fine. The Squeeze Theorem proves that the limit does in fact exist,

but it must be so stated.

These general theorems will allow us to rigorously explore convergence of

power series in the next chapter without having to appeal directly to the defini-

tion of convergence. However, you should remember that we used the definition

to prove these results and there will be times when we will need to apply the

definition directly. However, before we go into that, let's examine divergence

a bit more closely.

6.3 Divergence

In Theorem 4.2.2 we saw that there is a rearrangment of the alternating Har-
monic series which diverges to  or -. In that section we did not fuss over
any formal notions of divergence. We assumed instead that you are already
familiar with the concept of divergence, probably from taking calculus in the
past.

    However we are now in the process of building precise, formal definitions
for the concepts we will be using so we define the divergence of a sequence as
follows.

Definition 6.3.1 A sequence of real numbers (sn)n=1  diverges if it does not
converge to any a  R.
                                                                                

It may seem unnecessarily pedantic of us to insist on formally stating such

an obvious definition. After all "converge" and "diverge" are opposites in

ordinary English. Why wouldn't they be mathematically opposite too? Why

do we have to go to the trouble of formally defining both of them? Since they

are opposites defining one implicitly defines the other doesn't it?

One way to answer that criticism is to state that in mathematics we always

work from precisely stated definitions and tightly reasoned logical arguments.

But this is just more pedantry. It is a way of saying, "Because we said so"

all dressed up in imposing language. We need to do better than that.

One reason for providing formal definitions of both convergence and diver-

gence is that in mathematics we frequently co-opt words from natural languages

like English and imbue them with mathematical meaning that is only tangen-

tially related to the original English definition. When we take two such words

which happen to be opposites in English and give them mathematical meanings

which are not opposites it can be very confusing, especially at first.

This is what happened with the words "open" and "closed." These are

opposites in English: "not open" is "closed," "not closed" is "open," and there

is nothing which is both open and closed. But recall that an open interval on

the real line, (a, b), is one that does not include either of its endpoints while a

closed interval, [a, b], is one that includes both of them.

These may seem like opposites at first but they are not. To see this observe

that the interval (a, b] is neither open nor closed since it only contains one of

its endpoints. If "open" and "closed" were mathematically opposite then every

interval would be either open or closed.

Open Sets vs. Closed Sets. It is also true that (-, ) is both open and
closed, but an explanation of this would take us too far afield.
CHAPTER 6. CONVERGENCE OF SEQUENCES AND SERIES                     71

    Mathematicians have learned to be extremely careful about this sort of
thing. In the case of convergence and divergence of a series, even though these
words are actually opposites mathematically (every sequence either converges
or diverges and no sequence converges and diverges) it is better to say this
explicitly so there can be no confusion.

    A sequence (an)n=1  can only converge to a real number, a, in one way: by
getting arbitrarily close to a. However there are several ways a sequence might
diverge.

Example 6.3.2 Consider the sequence, (n)n=1  . This clearly diverges by get-
ting larger and larger . . . Ooops! Let's be careful. The sequence 1 - n n=1 1 
gets larger and larger too, but it converges. What we meant to say was that

the terms of the sequence (n)n=1  become arbitrarily large as n increases.
    This is clearly a divergent sequence but it may not be clear how to prove

this formally. Here's one way.

To show divergence we must show that the sequence satisfies the negation

of the definition of convergence. That is, we must show that for every r  R
there is an  > 0 such that for every N  R, there is an n > N with |n - r|  .
So let  = 1, and let r  R be given. Let N = r + 2. Then for every n > N
|n - r| > |(r + 2) - r| = 2 > 1. Therefore the sequence diverges.
                                                                   

This seems to have been rather more work than we should have to do for

such a simple problem. Here's another way which highlights this particular

type of divergence.

First we'll need a new definition:

Definition 6.3.3 A sequence, (an)n=1  , diverges to positive infinity if for
every real number r, there is a real number N such that n > N  an > r.

    A sequence, (an)n=1  , diverges to negative infinity if for every real num-
ber r, there is a real number N such that n > N  an < r.

A sequence is said to diverge to infinity if it diverges to either positive

or negative infinity.                                              

In practice we want to think of |r| as a very large number. This definition

says that a sequence diverges to infinity if it becomes arbitrarily large as n

increases, and similarly for divergence to negative infinity.

Problem 6.3.4 Show that (n)n=1  diverges to infinity. 

Problem 6.3.5 Show that if (an)n=1  diverges to infinity then (an)n=1  diverges.
                                                                                                    

    We will denote divergence to infinity as

                                 lim an = .

                                n

    However, strictly speaking this is an abuse of notation since the symbol 
does not represent a real number. This notation can be very problematic since
it looks so much like the notation we use to denote convergence: lim an = a.

                                                                                                                            n

    Nevertheless, the notation is appropriate because divergence to infinity is
"nice" divergence in the sense that it shares many of the properties of conver-
gence, as the next problem shows.

Problem 6.3.6 Suppose lim an =  and lim bn = .
                       n            n

(a) Show that lim an + bn = 

                             n

(b) Show that lim anbn = 

                             n
CHAPTER 6. CONVERGENCE OF SEQUENCES AND SERIES       72

(c) Is it true that lim an = ? Explain.
                         n bn

                                                                                                    
    Because divergence to positive or negative infinity shares some of the prop-
erties of convergence it is easy to get careless with it. Remember that even
though we write lim an =  this is still a divergent sequence in the sense that

                               n

 lim an does not exist. The symbol  does not represent a real number. This

n

is just a convenient notational shorthand telling us that the sequence diverges
by becoming arbitrarily large.

Problem 6.3.7 Suppose lim an =  and lim bn = - and   R. Prove
                           n                      n
or give a counterexample:

 (a) lim an + bn = 

          n

 (b) lim anbn = -

          n

 (c) lim an = 

          n

 (d) lim bn = -

          n

                                                                                                    
    Finally, a sequence can diverge in other ways as the following problem
displays.
Problem 6.3.8 Show that each of the following sequences diverge.
 (a) ((-1)n)n=1 
 (b) ((-1)n n)n=1 

               1         if n = 2p for some p  N
(c) an = 1               otherwise.

                      n

                                                                                                    
Problem 6.3.9 Suppose that (an)n=1  diverges but not to infinity and that 
is a real number. What conditions on  will guarantee that:

(a) (an)n=1  converges?
(b) (an)n=1  diverges?

                                                     

Problem 6.3.10 Show that if |r| > 1 then (rn)n=1  diverges. Will it diverge
to infinity?
                                                     

6.4 Additional Problems

Problem 6.4.1 Prove that if limn sn = s then limn |sn| = |s|. Prove
that the converse is true when s = 0, but it is not necessarily true otherwise.

                                                                                                    
Problem 6.4.2

 (a) Let (sn) and (tn) be sequences with sn  tn, n. Suppose limn sn = s
       and limn tn = t.
CHAPTER 6. CONVERGENCE OF SEQUENCES AND SERIES                        73

Prove s  t.

Hint.

Assume for contradiction, that s > t and use the definition of convergence

with      =  s-t  to  produce  an  n  with  sn  >  tn.
              2

(b) Prove that if a sequence converges, then its limit is unique. That is, prove
      that if limn sn = s and limn sn = t, then s = t.

                                                                      

Problem 6.4.3 Prove that if the sequence (sn) is bounded then

limn  sn  = 0.                                                        
      n

Problem 6.4.4

(a) Prove that if x = 1, then

                       1 + x + x2 +    + xn = 1 - xn+1 .
                                                         1-x

(b) Use (a) to prove that if |x| < 1, then limn j=0 n xj = 1-x 1 .

                                                                                                    
Problem 6.4.5 Prove

                  lim  a0 + a1n + a2n2 +    + aknk ak
                                                        k= ,
                  n b0 + b1n + b2n +    + bkn2       bk

provided bk = 0. [Notice that since a polynomial only has finitely many roots,

then the denominator will be non-zero when n is sufficiently large.]  

Problem 6.4.6 Prove that if limn sn = s and limn (sn - tn) = 0, then

limn tn = s.                                                          

Problem 6.4.7

(a) Prove that if limn sn = s and s < t, then there exists a real number
      N such that if n > N then sn < t.

(b) Prove that if limn sn = s and r < s, then there exists a real number
      M such that if n > M then r < sn.

                                                                                                    
Problem 6.4.8 Suppose (sn) is a sequence of positive numbers such that

                        lim           sn+1      = L.
                                       sn
                       n

(a) Prove that if L < 1, then limn sn = 0.
      Hint.
      Choose R with L < R < 1. By the previous problem,  N such that if
      n > N , then sns+1 n < R. Let n0 > N be fixed and show sn0+k < Rksn0 .
      Conclude that limk sn0+k = 0 and let n = n0 + k.

(b) Let c be a positive real number. Prove lim cn = 0.
                                                          n n!
                                                                                                   
Chapter 7

A "Tayl" of Three Remain-
ders

7.1 The Integral Form of the Remainder

Now that we have a rigorous definition of the convergence of a sequence, let's
apply this to Taylor series. Recall that the Taylor series of a function f (x)
expanded about the point a is given by

   f (n)(a) (x - a)n = f (a) + f (a) (x - a) + f (a) (x - a)2 +   
           n!                              1!                    2!
  n=0

    When we say that f (x) = n=0 n!  f (x - a) (n)(a) n for a particular value of x,
what we mean is that the sequence of partial sums

                           

n  f (j)(a) (x - a)j 
           j!
  j=0 n=0

= f (a), f (a) + f (a) (x - a), f (a) + f (a) (x - a) + f (a) (x - a)2, . . .
                     1!                          1!                  2!

converges to the number f (x). Note that the index in the summation was
changed to j to allow n to represent the index of the sequence of partial sums.
As intimidating as this may look, bear in mind that for a fixed real number x,
this is still a sequence of real numbers so, that saying f (x) = n=0 n!  f (x - (n)(a)
a)n means that limn j=0 j! n f (x - a) = f (x) (j)(a) j and in the previous
chapter we developed some tools to examine this phenomenon. In particular,
we know that limn j=0 j! n f (x - a) = f (x) (j)(a) j is equivalent to

                                                              

                     n lim f (x) -  f (j)(a) (x - a)j                = 0.
                     n                     j!
                                      j=0

We have seen an example of this already. Problem 6.4.4 of the last chapter

basically  had  you  show  that  the  geometric  series,  1+  x  +  x2 +   x3  +        converges
                                                                      

to 1-1x ,for |x| < 1 by showing that lim 1 -                  n
                                              n 1 - x
                                                                    xj = 0.

                                                              j=0

                                           74
CHAPTER 7. A "TAYL" OF THREE REMAINDERS                                                     75

There is generally not a readily recognizable closed form for the partial

sum for a Taylor series. The geometric series is a special case. Fortunately, for

the issue at hand (convergence of a Taylor series), we don't need to analyze

the series itself. What we need to show is that the difference between the

function and the nth partial sum converges to zero. This difference is called

the remainder (of the Taylor series). (Why?)

While it is true that the remainder is simply

                                                             

                        n f (x) -  f (j)(a) (x - a)j ,
                                            j!

                                         j=0

this form is not easy to work with. Fortunately, a number of alternate versions
of this remainder are available. We will explore these in this chapter.

    Recall the result from Theorem 4.1.9 from Chapter 4,

f (x) = f (a) + f (a) (x - a) + f (a) (x - a)2 +    + f (n)(a) (x - a)n
          1!                        2!                             n!

                                                          1    x
                                                       +
                                                                  f (n+1)(t)(x - t)n dt.
                                                           n! t=a

We can use this by rewriting it as

                                        

        n f (x) -  f (j)(a) (x - a)j = 1                x
                        j!                    n!
          j=0                                              f (n+1)(t)(x - t)n dt.

                                                       t=a

The     expression  1   x    f (n+1)(t)(x  -  t)n  dt  is  called  the  integral  form  of  the
                    n!  t=a
remainder for the Taylor series of f (x), and the Taylor series will converge
                                                           xt=a f (n+1)(t)(x - t)n dt
to f (x) exactly when the sequence limn            1                                    con-
                                                   n!
verges to zero. It turns out that this form of the remainder is often easier to

handle than the original f (x) - j=0 j! n f (x - a) (j)(a) j and we can use it to

obtain some general results.

Theorem 7.1.1 Taylor's series
    If there exists a real number B such that |f (n+1)(t)|  B for all nonnegative

integers n and for all t on an interval containing a and x, then

                     lim 1 x f (n+1)(t)(x - t)n dt = 0
                    n n! t=a

and so                   f (x) = f (n)(a) (x - a)n.
                                          n!

                                      n=0

In order to prove this, it might help to first prove the following.

Lemma 7.1.2 If f and |f | are integrable functions and a  b, then

                             b                b

                                  f (t) dt  |f (t)| dt.

                             t=a              t=a

Problem 7.1.3 Prove Lemma 7.1.2.

Hint.

-|f (t)|  f (t)  |f (t)|.                                                                   
CHAPTER 7. A "TAYL" OF THREE REMAINDERS                                               76

Problem 7.1.4 Prove Theorem 7.1.1.

Hint.
    You might want to use Problem 6.4.8 of Chapter 6. Also there are two

cases to consider: a < x and x < a (the case x = a is trivial). You will find
that this is true in general. This is why we will often indicate that t is between
a and x as in the theorem. In the case x < a, notice that

x                                               a

     f (n+1)(t)(x - t)n dt = (-1)n+1                           f (n+1)(t)(t - x)n dt

t=a                                             t=x

                                     a

                                  =       f (n+1)(t)(t - x)n dt .

                                     t=x

                                                                                                    
Problem 7.1.5 Use Theorem 7.1.1 to prove that for any real number x

 (a) sin x = (-1)nx2n+1
                       (2n + 1)!

                      n=0

 (b) cos x = (-1)nx2n
                        (2n)!

                       n=0

(c) ex  = xn
                  n!

                  n=0

                                                                                                
Problem 7.1.5.c shows that the Taylor series of ex expanded at zero con-

verges to ex for any real number x. Theorem 7.1.1 can be used in a similar

fashion to show that           ex  = ea(x - a)n
                                                 n!

                                        n=0

for any real numbers a and x.

    Recall that Chapter 3 we showed that if we define the function E(x) by the
power series n=0 n!  xn then E(x + y) = E(x)E(y). This, of course, is just the
familiar addition property of integer exponents extended to any real number.

In Chapter 3 we had to assume that defining E(x) as a series was meaningful

because we did not address the convergence of the series in that chapter. Now

that we know the series converges for any real number we see that the definition

                                  f (x) = ex  = xn
                                                        n!

                                                          n=0

is in fact valid.
    Assuming that we can differentiate this series term-by-term it is straight-

forward to show that f (x) = f (x).

Comment. We can, but that proof will have to wait for a few more chapters.

    Along with Taylor's formula this can then be used to show that ea+b = eaeb
more elegantly than the rather cumbersome proof in equation (I.6), as the
following problem shows.

Problem 7.1.6 Recall that if f (x) = ex then f (x) = ex. Use this along with
the Taylor series expansion of ex about a to show that

                                  ea+b = eaeb.

                                                                                      
CHAPTER 7. A "TAYL" OF THREE REMAINDERS                                  77

    Theorem 7.1.1 is a nice "first step" toward a rigorous theory of the con-
vergence of Taylor series, but it is not applicable in all cases. For example,
consider the function f (x) = 1 + x. As we saw in Chapter 3, Problem 3.2.12,
this function's Maclaurin series (the binomial series for (1 + x)1/2)appears to be
converging to the function for x  (-1, 1). While this is, in fact, true, the above
proposition does not apply. If we consider the derivatives of f (t) = (1 + t)1/2,
we obtain:

f (t) = 1 (1 + t) 21 -1
          2

f (t) = 1             1 - 1 (1 + t) 12 -2
           2          2

f (t) = 1             1     1 - 2 (1 + t) 12 -3
           2          2 -1  2

       ..
       .

f (n+1)(t) = 1        1     1              1         (1 + t) 12 -(n+1).
                2     2 -1  2 -2 렁 2 -n

Notice that

             f (n+1)(0) = 1 1 - 1  2- 1 렁 n- 1 .
                      2         2          2         2

    Since this sequence grows without bound as n  , then there is no chance
for us to find a number B to act as a bound for all of the derviatives of f on
any interval containing 0 and x, and so the hypothesis of Theorem 7.1.1 will
never be satisfied. We need a more delicate argument to prove that

1 + x = 1 + 1 1 1 x + 2 2 - 1 1 1 x2 + 2 2 - 1 12 - 2 x3 +   
                   2        2!                3!

is valid for x  (-1, 1). To accomplish this task, we will need to express the
remainder of the Taylor series differently. Fortunately, there are at least two
such alternate forms.

7.2 Lagrange's Form of the Remainder

Joseph-Louis Lagrange provided an alternate form for the remainder in Taylor
series in his 1797 work Thorie des functions analytiques. Lagrange's form of
the remainder is as follows.

Theorem 7.2.1 Lagrange's Form of the Remainder
    Suppose f is a function such that f (n+1)(t) is continuous on an interval

containing a and x. Then

                                   

n f (x) -  f (j)(a) (x - a)j  = f (n+1)(c) (x - a)n+1
                      j!                   (n + 1)!
               j=0

where c is some number between a and x.
Proof. Note first that the result is true when x = a as both sides reduce to 0

(in that case c = x = a.) We will prove the case where a < x; the case x < a

will be an exercise.
CHAPTER 7. A "TAYL" OF THREE REMAINDERS                                                 78

First, we already have

                                     

          n f (x) -  f (j)(a) (x - a)j = 1         x
                         j!                   n!
                  j=0                                 f (n+1)(t)(x - t)n dt

                                                  t=a

so it suffices to show that

               x f (n+1)(t)(x - t)n dt = f (n+1)(c) (x - a)n+1
                                                  n+1
               t=a

for some c with c  [ a, x]. To this end, let

                                M = max f (n+1)(t)

                                          atx

and
                                   m = min f (n+1)(t) .

                                                            atx

Note that for all t  [ a, x], we have m  f (n+1)(t)  M . Since x - t  0, this

gives us

               m (x - t)n  f (n+1)(t)(x - t)n  M (x - t)n                             (III.1)

and so

      x  m (x - t)n dt       x                                   x

                                f (n+1)(t)(x - t)n dt                 M (x - t)n dt.  (III.2)

t=a                      t=a                                     t=a

Computing the outside integrals, we have

          x    (x - t)n dt      x                                     x

       m                             f (n+1)(t)(x - t)n dt  M              (x - t)n dt

          t=a                   t=a                                   t=a

         m (x - a)n+1 x  f (n+1)(t)(x - t)n dt  M (x - a)n+1                          (III.3)
             n+1                                                      n+1
                         t=a

                    m           xt=a f (n+1)(t)(x - t)n dt  M .

                                             (x-a)n+1
                                                 n+1

Since                           xt=a f (n+1)(t)(x - t)n dt

                                             (x-a)n+1
                                                 n+1

is a value that lies between the maximum and minimum of f (n+1) on [ a, x],

then by the Intermediate Value Theorem, there must exist a number c  [ a, x]

with                                 xt=a f (n+1)(t)(x - t)n dt .

                    f (n+1)(c) =                  (x-a)n+1

                                              n+1

This gives us

               x f (n+1)(t)(x - t)n dt = f (n+1)(c) (x - a)n+1.
                                                  n+1
               t=a

And the result follows.                                                                 

Problem 7.2.2 Prove Theorem 7.2.1 for the case where x < a.

Hint.
CHAPTER 7. A "TAYL" OF THREE REMAINDERS                                           79

Note that

    x                                          a

           f (n+1)(t)(x - t)n dt = (-1)n+1           f (n+1)(t)(t - x)n dt.

    t=a                                        t=x

Use the same argument on this integral. It will work out in the end. Really!

You just need to keep track of all of the negatives.                              

This is not Lagrange's proof. He did not use the integral form of the

remainder. However, this is similar to Lagrange's proof in that he also used

the Intermediate Value Theorem (IVT) and Extreme Value Theorem (EVT)

much as we did. In Lagrange's day, these were taken to be obviously true for a

continuous function and we have followed Lagrange's lead by assuming the IVT

and the EVT. However, in mathematics we need to keep our assumptions few

and simple. The IVT and the EVT do not satisfy this need in the sense that

both can be proved from simpler ideas. We will return to this in Chapter 9.

Also, a word of caution about this: Lagrange's form of the remainder is

(n+1)! f (x - a) (n+1)(c) n+1, where c is some number between a and x. The proof does

not indicate what this c might be and, in fact, this c changes as n changes. All

we know is that this c lies between a and x. To illustrate this issue and its

potential dangers, consider the following problem where we have a chance to

compute the value of c for the function f (x) = 1+x 1 .

Problem 7.2.3 This problem investigates the Taylor series representation

                            1 = 1 - x + x2 - x3 +    .
                          1+x

(a) Use the fact that 1+x 1-(-x) = 1-x+x -x +  +(-x) n+1 2 3 n to compute the

remainder         1   -   1 - x + x2 - x3 +    + (-x)n    . Specifically, compute
                1+x
this remainder when x = 1 and conclude that the Taylor series does not

converge   to        1  when  x  =  1.
                   1+x

(b) Compare the remainder in part a with the Lagrange form of the remainder
      to determine what c is when x = 1.

(c) Consider the following argument: If f (x) = 1+x 1 , then

                              (n+1)      (-1)n+1(n + 1)!
                          f (c) =
                                               (1 + c)  n+2

so the Lagrange form of the remainder when x = 1 is given by

                          (-1)n+1(n + 1)!            (-1)n+1
                          (n + 1)!(1 + c)n+2 = (1 + c)n+2

where c  [ 0, 1]. It can be seen in part b that c = 0. Thus 1 + c > 1 and

so by Problem 6.1.7 of Chapter 6, the Lagrange remainder converges to 0

as n  . This argument would suggest that the Taylor series converges

to    1    for  x  =  1.  However,   we  know  from  part    (a)  that  this  is  incorrect.
    1+x
What is wrong with the argument?

                                                                                                    
    Even though there are potential dangers in misusing the Lagrange form of
the remainder, it is a useful form. For example, armed with the Lagrange form
of the remainder, we can prove the following theorem.
CHAPTER 7. A "TAYL" OF THREE REMAINDERS                                                             80

Theorem 7.2.4 The binomial series

                 1 + 1 1 1 x + 2 2 - 1 1 1 x2 + 2 2 - 1 12 - 2 x3 +   
                        2             2!                        3!

converges to 1 + x for x  [0, 1].

Proof. First note that the binomial series is, in fact, the Taylor series for
the function f (x) = 1 + x expanded about a = 0. If we let x be a fixed

number with 0  x  1, then it suffices to show that the Lagrange form of the

remainder converges to 0. With this in mind, notice that

                 f (n+1)(t) = 1           1 - 1    1 - n (1 + t) 12 -(n+1)
                                   2      2                2

and so the Lagrange form of the remainder is

                 f (n+1)(c) 1 n+1 2               12 - 1    12 - n xn+1
                             x=
                                                  (n + 1)!                 (1  +   c)n+    1
                 (n + 1)!                                                                  2

where c is some number between 0 and x. Since 0  x  1 and 1 + c  1, then

we  have      1     1,  and   so
            1+c

                 1 1 0  2 2 - 1    12 - n xn+1                       1
                                    (n + 1)!            (1    +  c)n+   2

                           1     1 - 12    n - 12 xn+1

                    =2              (n + 1)!      (1       +    c)n+    1
                                                                        2

                    = 2 2 2 2 1 1 3 5    2 xn+1 2n-1 1                               1
                                      (n + 1)!                             (1  +  c)n+  2

                     n+1 1  1  3  5      (2n - 1)
                               2 (n + 1)!

                    = 1  3  5      (2n - 1)  1
                       2  4  6      2n  (2n + 2)

                    = 1  3  5      2n - 1  1
                        246                   2n 2n + 2

                     1.
                       2n + 2

Since  limn           1    =  0  =  limn 0,       then     by   the  Squeeze       Theorem,
                   2n+2

            lim f (n+1)(c) xn+1 = 0, so lim                      f (n+1)(c) n+1            = 0.
            n (n + 1)!                            n                          x

                                                                 (n + 1)!

Thus the Taylor series

                 1 + 1 1 1 x + 2 2 - 1 1 1 x2 + 2 2 - 1 12 - 2 x3 +   
                        2             2!                        3!

converges to 1 + x for 0  x  1. 

    Unfortunately, this proof will not work for -1 < x < 0. In this case, the

fact  that  x    c      0  makes 1 + c        1.  Thus       1       1  and    so  the  inequality
                                                           1+c

       2 2 2 2 1 1 3 5    2 2n-1 |x|n+1                       1  1  3  5      (2n - 1)
                                             (1 + c)n+ 21 
                    (n + 1)!                                               2n+1 (n + 1)!
CHAPTER 7. A "TAYL" OF THREE REMAINDERS                                                                    81

may not hold.

Problem 7.2.5 Show that if - 12  x  c  0, then | x 1+c |  1 and modify
                                                                                   

the above proof to show that the binomial series converges to 1 + x for x 

- 12 , 0 .                                                                                                     

    To take care of the case where -1 < x < - 12 , we will use yet another
form of the remainder for Taylor series. However before we tackle that, we

will use the Lagrange form of the remainder to address something mentioned

in Chapter 4. Recall that we noticed that the series representation

                                  1 = 1 - x + x2 - x3 +   
                                1+x

did not work when x = 1, however we noticed that the series obtained by
integrating term by term did seem to converge to the antiderivative of 1+x 1 .
Specifically, we have the Taylor series

                              ln (1 + x) = x - 1 x2 + 1 x3 -    .
                                                    23

Substituting       x    =  1  into  this  provided     the   convergent         series  1-  1  +  1  -  1  +    .
                                                                                            2     3     4
We made the claim that this, in fact, converges to ln 2, but that this was not

obvious. The Lagrange form of the remainder gives us the machinery to prove

this.

Problem 7.2.6

(a) Compute the Lagrange form of the remainder for the Maclaurin series
     for ln (1 + x).

(b) Show that when x = 1, the Lagrange form of the remainder converges to

       0  and  so  the  equation    ln  2  =  1  -  1  +  1  -  1   +        is  actually   correct.
                                                    2     3     4

                                                                                                               

7.3 Cauchy's Form of the Remainder

In his 1823 work, Rsume des leons donnes  l'ecole royale polytechnique sur
le calcul infintsimal, Augustin Cauchy provided another form of the remainder
for Taylor series.

Theorem 7.3.1 Cauchy's Form of the Remainder                                                         interval
    Suppose f is a function such that f (n+1)(t) is continuous on an

containing a and x. Then

                                                 

            n f (x) -  f (j)(a) (x - a)j = f (n+1)(c) (x - c)n(x - a)
                              j!                                n!
                        j=0

where c is some number between a and x.

Problem 7.3.2 Prove Theorem 7.3.1 using an argument similar to the one

used in the proof of Theorem 7.2.1. Don't forget there are two cases to consider.

                                                                                                
Using Cauchy's form of the remainder, we can prove that the binomial

series                                                 12 - 1       12 - 2 x3 +   
                  1 + 1 1 1 x + 2 2 - 1 1 x2 + 2             3!
                  2 2!

converges to 1 + x for x  (-1, 0).
CHAPTER 7. A "TAYL" OF THREE REMAINDERS                                                    82

Comment. Strictly speaking we only need to show this for x  (-1, -1/2).In
problem 7.2.5 we coveredx  (-1/2, 0).

    With this in mind, let x be a fixed number with -1 < x < 0 and consider
that the binomial series is the Maclaurin series for the function f (x) = (1+x) 21 .
As we saw before,

        f (n+1)(t) = 1           1 - 1    1 - n (1 + t) 12 -(n+1) ,
                          2      2          2

so the Cauchy form of the remainder is given by

0  f (n+1)(c) n                          1  12 - 1    12 - n            (x - c)n  1 톥
                     (x - c) (x - 0) =   2

        n!                                                  n!            (1 + c)n+ 2

where c is some number with x  c  0. Thus we have

                     1     12 - 1    12 - n (x - c)nx
            0  n+ 1  2

                               n!           (1 + c) 2

                        1  1 - 12    n - 12 |x - c|n|x|

                 =2            n!           (1           +  c)n+  1
                                                                  2

                 1 1 3 5 = 2 2 2 2    2n-1 2 (c - x)n | x|
                                 n!                      (1 + c)n 1 + c

                 = n+1 1  1  3  5      (2n - 1)  c - x n | x|
                                2 n!                                 
                                                         1+c           1+c

                 = 1  1  3  5      (2n - 1)      c - x n | x|
                        2  2  4  6      2n                   
                                                         1+c           1+c

                     1135                2n - 1 c - x n | x|
                 = 2  2  4  6      2n  1 + c 1 + c

                  c - x n | x| .
                     1+c           1+c

Notice  that     if  -1 < x  c,    then  0 < 1 + x  1 + c.           Thus   0<       1      1
                                                                                   1+c    1+x

and  11+c   11+x . Thus we have

            1        12 - 1    12 - n (x - c)nx                   c - x n | x|
   0        2                                            1                  .

                           n!               n+                       1+c    1+x
                                         (1 + c) 2

Problem 7.3.3 Suppose -1 < x  c  0 and consider the function g(c) =

1+c c-x . Show that on [x, 0], g is increasing and use this to conclude that for
-1 < x  c  0,

                                            c - x  |x|.
                                            1+c
    Use this fact to finish the proof that the binomial series converges to 1 + x

for -1 < x < 0.                                                                            

The proofs of both the Lagrange form and the Cauchy form of the remainder

for Taylor series made use of two crucial facts about continuous functions. First,

we assumed the Extreme Value Theorem: Any continuous function on a closed
CHAPTER 7. A "TAYL" OF THREE REMAINDERS                                 83

Figure 7.3.4 Augustin Cauchy14

    bounded interval assumes its maximum and minimum somewhere on the
interval. Second, we assumed that any continuous function satisfied the Inter-
mediate Value Theorem: If a continuous function takes on two different values,
then it must take on any value between those two values.

    Mathematicians in the late 1700s and early 1800s typically considered these
facts to be intuitively obvious. This was natural since our understanding of
continuity at that time was, solely, intuitive. Intuition is a useful tool, but as
we have seen before it is also unreliable. For example consider the following
function.

f (x) = x sin x1 , if x = 0, .                                          (III.4)
                                   0,  if x = 0

Is this function continuous at 0? Near zero its graph looks like this:

Figure 7.3.5 Formula (III.4) near 0.

but this graph must be taken with a grain of salt as sin  1  oscillates
                                                          x
infinitely often as x nears zero.

14mathshistory.st-andrews.ac.uk/Biographies/Cauchy/
CHAPTER 7. A "TAYL" OF THREE REMAINDERS  84

    No matter what your guess may be, it is clear that it is hard to analyze
such a function armed with only an intuitive notion of continuity. We will
revisit this example in the next chapter.

    As with convergence, continuity is more subtle than it first appears.
    We put convergence on solid ground by providing a completely analytic
definition in the previous chapter. What we need to do in the next chapter is
provide a completely rigorous definition for continuity.

7.4 Additional Problems

Problem 7.4.1 Find the Integral form, Lagrange form, and Cauchy form of
the remainder for Taylor series for the following functions expanded about the
given values of a.

(a) f (x) = ex, a = 0
(b) f (x) = x, a = 1

(c) f (x) = (1 + x), a = 0

(d) f (x) = 1x , a = 3
(e) f (x) = ln x, a = 2

(f)  f (x) = cos x, a =  
                         2

                                         
Chapter 8

Continuity: What It Isn't and
What It Is

8.1 An Analytic Definition of Continuity

Before the invention of calculus, the notion of continuity was treated intuitively
if it was treated at all. At first pass, it seems a very simple idea based solidly
in our experience of the real world. Standing on the bank we see a river flow
past us continuously, not by tiny jerks. Even when the flow might seem at
first to be discontinuous, as when it drops precipitously over a cliff, a closer
examination shows that it really is not. As the water approaches the cliff it
speeds up. When it finally goes over it accelerates very quickly but no matter
how fast it goes it moves continuously, moving from here to there by occupying
every point in between. This is continuous motion. It never disappears over
there and instantaneously reappears over here. That would be discontinuous
motion.

    Similarly, a thrown stone flies continuously (and smoothly) from release
point to landing point, passing through each point in its path.

    But wait.
    If the stone passes through discrete points it must be doing so by teeny
tiny little jerks, mustn't it? Otherwise how would it get from one point to the
next? Is it possible that motion in the real world, much like motion in a movie,
is really composed of tiny jerks from one point to the next but that these tiny
jerks are simply too small and too fast for our senses to detect?
    If so, then the real world is more like the rational number line (Q) from
Chapter 2 than the real number line (R). In that case, motion really consists
of jumping discretely over the "missing" points (like 2) as we move from here
to there. That may seem like a bizarre idea to you -- it does to us as well --
but the idea of continuous motion is equally bizarre. It's just a little harder to
see why.
    The real world will be what it is regardless of what we believe it to be, but
fortunately in mathematics we are not constrained to live in it. So we won't
even try. We will simply postulate that no such jerkiness exists; that all motion
is continuous.
    However we are constrained to live with the logical consequences of our
assumptions, once they are made. These will lead us into some very deep
waters indeed.
    The intuitive treatment of continuity was maintained throughout the 1700's
as it was not generally perceived that a truly rigorous definition was necessary.

                                                  85
CHAPTER 8. CONTINUITY: WHAT IT ISN'T AND WHAT IT IS  86

Consider the following definition given by Euler in 1748.

       A continuous curve is one such that its nature can be expressed
       by a single function of x. If a curve is of such a nature that for
       its various parts . . . different functions of x are required for its
       expression, . . . , then we call such a curve discontinuous.

However, the complexities associated with Fourier series and the types of func-
tions that they represented caused mathematicians in the early 1800s to rethink
their notions of continuity. As we saw in Part II, the graph of the function
defined by the Fourier series

                               4  (-1)k cos ((2k + 1) x)
                                (2k + 1)

                                                k=0

looked like this:

Figure 8.1.1
    This function went against Euler's notion of what a continuous function

should be. Here, an infinite sum of continuous cosine curves provided a sin-
gle expression which resulted in a "discontinuous" curve. But as we've seen
this didn't happen with power series and an intuitive notion of continuity is
inadequate to explain the difference. Even more perplexing is the following
situation. Intuitively, one would think that a continuous curve should have
a tangent line at at least one point. It may have a number of jagged points
to it, but it should be "smooth" somewhere. An example of this would be
f (x) = x2/3. Its graph is given by

Figure 8.1.2
    This function is not differentiable at the origin but it is differentiable every-

where else. One could certainly come up with examples of functions which fail
to be differentiable at any number of points but, intuitively, it would be rea-
sonable
CHAPTER 8. CONTINUITY: WHAT IT ISN'T AND WHAT IT IS         87

Figure 8.1.3 Karl Weierstrass15

    to expect that a continuous function should be differentiable somewhere.
We might conjecture the following:

Conjecture 8.1.4 If f is continuous on an interval I then there is some a  I,
such that f (a) exists.

    Surprisingly, in 1872, Karl Weierstrass showed that the above conjecture is
FALSE. He did this by displaying the counterexample:

                                         

                          f (x) = bn cos(anx).

                                        n=0

    Weierstrass showed that if a is an odd integer, b  (0, 1), and ab > 1 + 32 ,
then f is continuous everywhere, but is nowhere differentiable. Such a function
is somewhat "fractal" in nature, and it is clear that a definition of continuity
relying on intuition is inadequate to study it.

Problem 8.1.5

(a) Given f (x) =         1  n cos (anx), what is the smallest value of a for
                     n=0  2
which f satisfies Weierstrass' criterion to be continuous and nowhere

differentiable.

(b) Let f (x, N ) =  N    1  n cos (13nx) and use a computer algebra system
                     n=0  2
to plot f (x, N ) for N = 0, 1, 2, 3, 4, 10 and x  [0, 1].

(c) Plot f (x, 10) for x  [ 0, c], where c = 0.1, 0.01, 0.001, 0.0001, 0.00001.
     Based upon what you see in parts b and c, why would we describe the
     function to be somewhat "fractal" in nature?

                                                                                                     
    Just as it was important to define convergence with a rigorous definition
without appealing to intuition or geometric representations, it is imperative
that we define continuity in a rigorous fashion not relying on graphs.
    The first appearance of a definition of continuity which did not rely on geom-
etry or intuition was given in 1817 by Bernhard Bolzano in a paper published
in the Proceedings of the Prague Scientific Society entitled Rein analytischer
Beweis des Lehrsatzes dass zwieschen je zwey Werthen, die ein entgegenge-
setztes Resultat gewaehren, wenigstens eine reele Wurzel der Gleichung liege
(Purely Analytic Proof of the Theorem that Between Any Two Values that
Yield Results of Opposite Sign There Will be at Least One Real Root of the
Equation).

  15mathshistory.st-andrews.ac.uk/Biographies/Weierstrass/
CHAPTER 8. CONTINUITY: WHAT IT ISN'T AND WHAT IT IS  88

Figure 8.1.6 Bernhard Bolzano16

    From the title it should be clear that in this paper Bolzano is proving
the Intermediate Value Theorem. To do this he needs a completely analytic
definition of continuity. The substance of Bolzano's idea is that if f is con-
tinuous at a point a then f (x) should be "close to" f (a) whenever x is "close
enough to" a. More precisely, Bolzano said that f is continuous at a provided
|f (x) - f (a)| can be made smaller than any given quantity provided we make
|x - a| sufficiently small.

    The language Bolzano uses is very similar to the language Leibniz used
when he postulated the existence of infinitesimally small numbers. Leibniz
said that infinitesimals are "smaller than any given quantity but not zero."
Bolzano says that "|f (x) - f (a)| can be made smaller than any given quantity
provided we make |x - a| sufficiently small." But Bolzano stops short of saying
that |x - a| is infinitesimally small. Given a, we can choose x so that |x - a|
is smaller than any real number we could name, say b, provided we name b
first, but for any given choice of x, |x - a|, and b are both still real numbers.
Possibly very small real numbers to be sure, but real numbers nonetheless.
Infinitesimals have no place in Bolzano's construction.

    Bolzano's paper was not well known when Cauchy proposed a similar defi-
nition in his Cours d'analyse [1] of 1821 so it is usually Cauchy who is credited
with this definition, but even Cauchy's definition is not quite tight enough
for modern standards. It was Karl Weierstrass in 1859 who finally gave the
modern definition.

Definition 8.1.7 We say that a function f is continuous at a provided that
for any  > 0, there exists a  > 0 such that if |x - a| <  then |f (x)-f (a)| < .

                                                                                                     
    Notice that the definition of continuity of a function is done point-by-point.
A function can certainly be continuous at some points while discontinuous at
others. When we say that f is continuous on an interval, then we mean that
it is continuous at every point of that interval and, in theory, we would need
to use the above definition to check continuity at each individual point.
    Our definition fits the bill in that it does not rely on either intuition or
graphs, but it is this very non-intuitiveness that makes it hard to grasp. It
usually takes some time to become comfortable with this definition, let alone
use it to prove theorems such as the Extreme Value Theorem and Intermediate
Value Theorem. So let's go slowly to develop a feel for it.
    This definition spells out a completely black and white procedure: you
give me a positive number , and I must be able to find a positive number 
which satisfies a certain property. If I can always do that then the function is
continuous at the point of interest.

  16mathshistory.st-andrews.ac.uk/Biographies/Bolzano/
CHAPTER 8. CONTINUITY: WHAT IT ISN'T AND WHAT IT IS                          89

This definition also makes very precise what we mean when we say that

f (x) should be "close to" f (a) whenever x is "close enough to" a. For example,

intuitively we know that f (x) = x2 should be continuous at x = 2. This

means that we should be able to get x2 to within, say,  = .1 of 4 provided

we make x close enough to 2.   Specifically,  we  want  3.9  <  x2  <  4.1.  This
                                                                
happensexactly when 3.9 < x < 4.1. Using the fact that 3.9 < 1.98 and
2.02 < 4.1, then we can see that if we get x to within  = .02 of 2, then
3.9 < 1.98 < x < 2.02 < 4.1 and so x2 will be within .1 of 4. This is very

straightforward. What makes this situation more difficult is that we must be

able to do this for any  > 0.

Notice the similarity between this definition and the definition of conver-

gence of a sequence. Both definitions have the challenge of an  > 0. In the

definition of limn sn = s, we had to get sn to within  of s by making n

large enough. For sequences, the challenge lies in making |sn - s| sufficiently

small. More precisely, given  > 0 we need to decide how large n should be to

guarantee that |sn - s| < .

In our definition of continuity, we still need to make something small

(namely |f (x) - f (a)| < ), only this time, we need to determine how close

x must be to a to ensure this will happen instead of determining how large n

must be.

What makes f continuous at a is the arbitrary nature of  (as long as it

is positive). As  becomes smaller, this forces f (x) to be closer to f (a). That

we can always find a positive distance  to work is what we mean when we say

that we can make f (x) as close to f (a) as we wish, provided we get x close

enough to a. The sequence of pictures below illustrates that the phrase "for

any  > 0, there exists a  > 0 such that if | x - a| <  then |f (x) - f (a)| < "

can be replaced by the equivalent formulation "for any  > 0, there exists a

 > 0 such that if a -  < x < a +  then f (a) -  < f (x) < f (a) + ." This

could also be replaced by the phrase "for any  > 0, there exists a  > 0 such

that if x  (a-, a+) then f (x)  (f (a)-, f (a)+)." All of these equivalent

formulations convey the idea that we can get f (x) to within  of f (a), provided

we make x within  of a, and we will use whichever formulation suits our needs

in a particular application.
CHAPTER 8. CONTINUITY: WHAT IT ISN'T AND WHAT IT IS           90

    The precision of the definition is what allows us to examine continuity
without relying on pictures or vague notions such as "nearness" or "getting
closer to." We will now consider some examples to illustrate this precision.

Example 8.1.8 Use the definition of continuity to show that f (x) = x is

continuous at any point a.                                    

If we were to draw the graph of this line, then you would likely say that

this is obvious. The point behind the definition is that we can back up your

intuition in a rigorous manner.

Proof. Let  > 0. Let  = . If | x - a| < , then

                      |f (x) - f (a)| = | x - a| < 

Thus by the definition, f is continuous at a.                 

Problem 8.1.9 Use the definition of continuity to show that if m and b are
fixed (but unspecified) real numbers then the function

                                 f (x) = mx + b

is continuous at every real number a.                         

Example 8.1.10 Use the definition of continuity to show that f (x) = x2 is

continuous at a = 0.                                          

Proof. Let  > 0. Let  = . If | x - 0| < , then | x| < . Thus

                      x2 - 02 = | x|2 <  2 = .

Thus by the definition, f is continuous at 0.                 

Notice that in these proofs, the challenge of an  > 0 was first given. This

is because the choice of  must depend upon . Also notice that there was

no explanation for our choice of . We just supplied it and showed that it

worked. As long as  > 0, then this is all that is required. In point of fact, the

 we chose in each example was not the only choice that worked; any smaller

 would work as well.

Problem 8.1.11

(a) Given a particular  > 0 in the definition of continuity, show that if a

     particular 0 > 0 satisfies the definition, then any  with 0 <  < 0 will
     also work for this .

(b) Show that if a  can be found to satisfy the conditions of the definition
      of continuity for a particular 0 > 0, then this  will also work for any 
CHAPTER 8. CONTINUITY: WHAT IT ISN'T AND WHAT IT IS                  91

with 0 < 0 < .

                                                                                                    
    It wasn't explicitly stated in the definition but when we say "if |x - a| < 
then |f (x)-f (a)| < ," we should be restricting ourselves to x values which are
in the domain of the function f , otherwise f (x) doesn't make sense. We didn't
put it in the definition because that definition was complicated enough with-
out this technicality. Also in the above examples, the functions were defined
everywhere so this was a moot point. We will continue with the convention
that when we say "if | x - a| <  then |f (x) - f (a)| < ," we will be restricting
ourselves to x values which are in the domain of the function f . This will allow
us to examine continuity of functions not defined for all x without restating
this restriction each time.

Problem 8.1.12 Use the definition of continuity to show that

                         f (x) =            if x  0
                                    x       if x < 0
                                    

                                  - -x

is continuous at a = 0.                                              

Problem 8.1.13 Use the definition of continuity to show that f (x) = x is

continuous at a = 0. How is this problem different from problem 8.1.12? How

is it similar?                                                       

Sometimes the  that will work for a particular  is fairly obvious to see,

especially after you've gained some experience. This is the case in the above

examples (at least after looking back at the proofs). However, the task of

finding a  to work is usually not so obvious and requires some scrapwork.

This scrapwork is vital toward producing a , but again is not part of the

polished proof. This can be seen in the following example.

Example 8.1.14 Use the definition of continuity to prove that f (x) = x is

continuous at a = 1.

SCRAPWORK

As before, the scrapwork for these problems often consists of simply working

backwards. Specifically, given an  > 0, we need to find a  > 0 so that
| x - 1| < , whenever | x - 1| < . We work backwards from what we want,

keeping an eye on the fact that we can control the size of |x - 1|.

                                            | x - 1|
                         ( x - 1) ( x + 1)
| x - 1| = |                                | = x + 1 < | x - 1|.
                             x+1

This seems to suggest that we should make  = . We're now ready for the

formal proof.                                                        

Proof. Let  > 0. Let  = . If | x - 1| < , then | x - 1| < , and so

                                  
                         ( x - 1) ( x + 1)  |x - 1|
x- 1 =|                                     | = x + 1 < |x - 1| < .
                             x+1

Thus by definition, f (x) = x is continuous at 1. 
CHAPTER 8. CONTINUITY: WHAT IT ISN'T AND WHAT IT IS  92

Figure 8.1.15 Paul Halmos17

    Bear in mind that someone reading the formal proof will not have seen the
scrapwork, so the choice of  might seem rather mysterious. However, you are
in no way bound to motivate this choice of  and usually you should not, unless
it is necessary for the formal proof. All you have to do is find this  and show
that it works. Furthermore, to a trained reader, your ideas will come through
when you demonstrate that your choice of  works.

    Now reverse this last statement. As a trained reader, when you read the
proof of a theorem it is your responsibility to find the scrapwork, to see how
the proof works and understand it fully. As the renowned mathematical ex-
positor Paul Halmos (1916-2006) said, "Don't just read it; fight it! Ask your
own questions, look for your own examples, discover your own proofs. Is the
hypothesis necessary? Is the converse true? What happens in the classical
special case? What about the degenerate cases? Where does the proof use the
hypothesis?"

    This is the way to learn mathematics. It is really the only way.

Problem 8.1.16 Use the definition of continuity to show that f (x) = x is

continuous at any positive real number a.            

Problem 8.1.17

(a) Use a unit circle to show that for 0   < 2 , sin    and 1 - cos   
      and conclude |sin |  || and |1 - cos |  || for - 2 <  < 2 .

(b) Use the definition of continuity to prove that f (x) = sin x is continuous
      at any point a.

Hint.
sin x = sin (x - a + a).

                                                                                                    
Problem 8.1.18

 (a) Use the definition of continuity to show that f (x) = ex is continuous at
       a = 0.

(b) Show that f (x) = ex is continuous at any point a.

      Hint.
      Rewrite ex - ea as ea+(x-a) - ea and use what you proved in part a.

                                                                                                   
17mathshistory.st-andrews.ac.uk/Biographies/Halmos/
CHAPTER 8. CONTINUITY: WHAT IT ISN'T AND WHAT IT IS                        93

    In the above problems, we used the definition of continuity to verify our
intuition about the continuity of familiar functions. The advantage of this
analytic definition is that it can be applied when the function is not so intuitive.
Consider, for example, the function given at the end of the last chapter.

                     f (x) = x sin x1 ,      if x = 0 .
                                 0,          if x = 0

Near zero, the graph of f (x) looks like this:

As we mentioned in the previous chapter, since sin                      1  oscillates infinitely
                                                                        x
often as x nears zero this graph must be viewed with a certain amount of

suspicion. However our completely analytic definition of continuity shows that

this function is, in fact, continuous at 0.

Problem 8.1.19 The Topologist's Sine Function. Use the definition of
continuity to show that

                     f (x) = x sin x1 ,      if x = 0
                                 0,          if x = 0

is continuous at 0.                                                        

Even more perplexing is the function defined by

                     D(x) = x, if x is rational 0, if x is irrational.

    To the naked eye, the graph of this function looks like the lines y = 0 and
y = x. Of course, such a graph would not be the graph of a function. Actually,
both of these lines have holes in them. Wherever there is a point on one line
there is a "hole" on the other. Each of these holes is the width of a single
point (that is, their "width" is zero!) so they are invisible to the naked eye
(or even magnified under the most powerful microscope available). This idea
is illustrated in the following graph
CHAPTER 8. CONTINUITY: WHAT IT ISN'T AND WHAT IT IS       94

    Can such a function so "full of holes" actually be continuous anywhere? It
turns out that we can use our definition to show that this function is, in fact,
continuous at 0 and at no other point.

Problem 8.1.20

 (a) Use the definition of continuity to show that the function

             x,  if x is rational
D(x) =           if x is irrational

             0,

      is continuous at 0.

(b) Let a = 0. Use the definition of continuity to show that D is not contin-
      uous at a.

      Hint.
      You might want to break this up into two cases where a is rational or
      irrational. Show that no choice of  > 0 will work for  = | a|. Note that
      Theorem 2.0.11 of Chapter 2 will probably help here.

                                                                                                   

8.2 Sequences and Continuity

There is an alternative way to prove that the function

D(x) = x, if x is rational 0, if x is irrational

is not continuous at a = 0. We will examine this by looking at the relation-
ship between our definitions of convergence and continuity. The two ideas
are actually quite closely connected, as illustrated by the following very useful
theorem.

Theorem 8.2.1 The function f is continuous at a if and only if f satisfies
the following property:

 sequences (xn) , if lim xn = a then lim f (xn) = f (a).
n                             n
CHAPTER 8. CONTINUITY: WHAT IT ISN'T AND WHAT IT IS                      95

    Theorem 8.2.1 says that in order for f to be continuous, it is necessary

and sufficient that any sequence (xn) converging to a must force the sequence
(f (xn)) to converge to f (a). A picture of this situation is below though, as
always, the formal proof will not rely on the diagram.

    This theorem is especially useful for showing that a function f, is not con-
tinuous at a point a; all we need to do is exhibit a sequence (xn) converging
to a such that the sequence limn f (xn) does not converge to f (a). Let's
demonstrate this idea before we tackle the proof of Theorem 8.2.1.

Example 8.2.2 Use Theorem 8.2.1 to prove that

                             f (x) = x|x| ,  if x = 0
                                         0,  if x = 0

is not continuous at 0.                                                  

Proof. First notice that f can be written as

                                             if x > 0
                                      1      if x < 0 .
                             f (x) = -1      if x = 0

                                         0

To show that f is not continuous at 0, all we need to do is create a single

sequence (xn)which converges to 0, but for which the sequence (f (xn)) does

not converge to f (0) = 0. For a function like this one, just about any sequence

will do, but let's use  1  , just because it is an old familiar friend.
                        n

We have lim 1 = 0, but lim f 1 = lim 1 = 1 = 0 = f (0). Thus by
n n                          n n             n

Theorem 8.2.1, f is not continuous at 0.                                 

Problem 8.2.3 Use Theorem 8.2.1 to show that

                             f (x) = x|x| ,  if x = 0
                                         a,  if x = 0

is not continuous at 0, no matter what value a is.                       

Problem 8.2.4 Use Theorem 8.2.1 to show that

                                      x,  if x is rational
                         D(x) =           if x is irrational

                                      0,

is not continuous at a = 0.                                              

Problem 8.2.5 The function T (x) = sin        1  is often called the topologist's
                                              x
sine curve. Whereas sin x has roots at n, n  Z and oscillates infinitely often
CHAPTER 8. CONTINUITY: WHAT IT ISN'T AND WHAT IT IS           96

as x  , T has roots at n1 , n  Z, n = 0, and oscillates infinitely often as
x approaches zero. A rendition of the graph follows.

    Notice that T is not even defined at x = 0. We can extend T to be defined
at 0 by simply choosing a value for T (0) :

                         T (x) = sin x1 ,      if x = 0 .
                                     b,        if x = 0

Use Theorem 8.2.1 to show that T is not continuous at 0, no matter what

value is chosen for b.                                        

Sketch of the Proof of Theorem 8.2.1. We've seen how we can use Theo-

rem 8.2.1, now we need to prove Theorem 8.2.1. The forward direction is

fairly straightforward. So we assume that f is continuous at a and start

with a sequence (xn) which converges to a. What is left to show is that

limn f (xn) = f (a). If you write down the definitions of f being contin-

uous at a, limn xn = a, and limn f (xn) = f (a), you should be able to

get from what you are assuming to what you want to conclude.

To prove the converse, it is convenient to prove its contrapositive. That is, we

want to prove that if f is not continuous at a then we can construct a sequence

(xn) that converges to a but (f (xn))does not converge to f (a). First we need

to recognize what it means for f to not be continuous at a. This says that

somewhere there exists an  > 0, such that no choice of  > 0 will work for

this. That is, for any such , there will exist x, such that | x - a| < , but

|f (x) - f (a)|  . With this in mind, if  = 1, then there will exist an x1 such

that | x1 - a| < 1, but |f (x1) - f (a)|  . Similarly, if  = 12 , then there will
exist an x2 such that | x2 - a| < 12 , but | f (x2) - f (a)|  . If we continue
in this fashion, we will create a sequence (xn) such that | xn - a| < 1n , but
|f (xn) - f (a)|  . This should do the trick.                 

Problem 8.2.6 Turn the ideas of the previous two paragraphs into a formal

proof of Theorem 8.2.1.                                       

Theorem 8.2.1 is a very useful result. It is a bridge between the ideas

of convergence and continuity so it allows us to bring all of the theory we

developed in Chapter 6 to bear on continuity questions. For example consider

the following.

Theorem 8.2.7 Suppose f and g are both continuous at a. Then f + g and
f  g are continuous at a.
CHAPTER 8. CONTINUITY: WHAT IT ISN'T AND WHAT IT IS          97

Proof. We could use the definition of continuity to prove Theorem 8.2.7, but

Theorem 8.2.1 makes our job much easier. For example, to show that f + g

is continuous, consider any sequence (xn) which converges to a. Since f is
continuous at a, then by Theorem 8.2.1, limn f (xn) = f (a). Likewise, since
g is continuous at a, then limn g(xn) = g(a).
By Theorem 6.2.4 of Chapter 6,

        lim (f + g)(xn) = lim (f (xn) + g(xn))
        n                          n

                                   = lim f (xn) + lim g(xn)
                                   n               n

                                   = f (a) + g(a)

                                   = (f + g)(a).

Thus by Theorem 8.2.1, f + g is continuous at a. The proof that f  g is

continuous at a is similar.                                  

Problem 8.2.8 Use Theorem 8.2.1 to show that if f and g are continuous at

a, then f  g is continuous at a.                            

By employing Theorem 8.2.7 a finite number of times, we can see that a

finite sum of continuous functions is continuous. That is, if f1, f2, . . . , fn are
all continuous at a then j=1 n fj is continuous at a. But what about an infinite
sum? Specifically, suppose f1, f2, f3, . . . are all continuous at a. Consider the

following argument.

Let  > 0. Since fj is continuous at a, then there exists j > 0 such that if

| x - a| < j, then |fj(x) - fj(a)| < 2j . Let  =min(1, 2, . . .). If | x - a| < ,
then

                                      

                     fj(x) - fj(a) = (fj(x) - fj(a))

        j=1                  j=1      j=1

                                                    j = .
                      |fj(x) - fj(a)| <
                                                   2
                     j=1                 j=1

Thus by definition, j=1  fj is continuous at a.
This argument seems to say that an infinite sum of continuous functions

must be continuous (provided it converges). However we know that the Fourier

series               4  (-1)k cos ((2k + 1) x)
                      (2k + 1)

                        k=0

is a counterexample to this, as it is an infinite sum of continuous functions
which does not converge to a continuous function. Something fundamental
seems to have gone wrong here. Can you tell what it is?

    This is a question we will spend considerable time addressing in Chapter 10
(in particular, see problem 10.1.1) so if you don't see the difficulty, don't worry;
you will. In the meantime keep this problem tucked away in your consciousness.
It is, as we said, fundamental.

    Theorem 8.2.1 will also handle quotients of continuous functions. There is
however a small detail that needs to be addressed first. Obviously, when we
consider the continuity of f /g at a,we need to assume that g(a) = 0. However,
g may be zero at other values. How do we know that when we choose our
sequence (xn) converging to a that g(xn) is not zero? This would mess up our
idea of using the corresponding theorem for sequences (Theorem 6.2.12 from
Chapter 6). This can be handled with the following lemma.
CHAPTER 8. CONTINUITY: WHAT IT ISN'T AND WHAT IT IS                98

Lemma 8.2.9 If g is continuous at a and g(a) = 0, then there exists  > 0
such that g(x) = 0 for all x  (a - , a + ).

Problem 8.2.10 Prove Lemma 8.2.9.

Hint.
    Consider the case where g(a) > 0. Use the definition with  = 2 g(a) . The

picture is below; make it formal.

For the case g(a) < 0, consider the function -g.                   

A consequence of this lemma is that if we start with a sequence (xn) con-

verging to a, then for n sufficiently large, g(xn) = 0.

Problem 8.2.11 Use Theorem 8.2.1, to prove that if f and g are continuous

at a and g(a) = 0, then f /g is continuous at a.                   

Theorem 8.2.12 Suppose f is continuous at a and g is continuous at f (a).
Then g  f is continuous at a. (Note that (g  f )(x) = g(f (x)).)

Problem 8.2.13 Prove Theorem 8.2.12

(a) Using the definition of continuity.

(b) Using Theorem 8.2.1.

                                                                                                    
    The above theorems allow us to build continuous functions from other con-
tinuous functions. For example, knowing that f (x) = x and g(x) = c are
continuous, we can conclude that any polynomial,

                        p(x) = anxn + an-1xn-1 +    + a1x + a0

is continuous as well. We also know that functions such as f (x) = sin (ex) are
continuous without having to rely on the definition.

Problem 8.2.14 Show that each of the following is a continuous function at
every point in its domain.

(a) Any polynomial.

(b) Any rational function. (A rational function is defined to be a ratio of
      polynomials.)

(c) cos x.

(d) The other trig functions: tan(x), cot(x), sec(x), and csc(x).

                                                                   
CHAPTER 8. CONTINUITY: WHAT IT ISN'T AND WHAT IT IS                     99

Problem 8.2.15 What allows us to conclude that f (x) = sin (ex) is continuous

at any point a without referring back to the definition of continuity?  

Theorem 8.2.1 can also be used to study the convergence of sequences. For

example, since f (x) = ex is continuous at any point and limn n+1 n = 1,
then limn e( nn+1 ) = e. This also illustrates a certain way of thinking about

continuous functions. They are the ones where we can "commute" the function

and a limit of a sequence. Specifically, if f is continuous at a and limn xn =

a, then limn f (xn) = f (a) = f (limn xn).

Problem 8.2.16 Compute the following limits. Be sure to point out how
continuity is involved.

(a) lim sin n
n          2n + 1

(b) lim       n
           n2 + 1
        n

 (c) lim e( sin (1/n))

          n

                                                                                                    
    Having this rigorous formulation of continuity is necessary for proving the
Extreme Value Theorem and the Mean Value Theorem. However there is one
more piece of the puzzle to address before we can prove these theorems.
    We will do this in the next chapter, but before we go on it is time to define a
fundamental concept that was probably one of the first you learned in calculus:
limits.

8.3 The Definition of the Limit of a Function

Since these days the limit concept is generally regarded as the starting point
for calculus, you might think it is a little strange that we've chosen to talk
about continuity first. But historically, the formal definition of a limit came
after the formal definition of continuity. In some ways, the limit concept was
part of a unification of all the ideas of calculus that were studied previously
and, subsequently, it became the basis for all ideas in calculus. For this reason
it is logical to make it the first topic covered in a calculus course.

    To be sure, limits were always lurking in the background. In his attempts to
justify his calculations, Newton used what he called his doctrine of "Ultimate
Ratios." Specifically the ratio h (x+h) = h = 2x + h 2-x2 2xh+h2 becomes the
ultimate ratio 2x at the last instant of time before h - an "evanescent quantity"
- vanishes ([4], p. 33). Similarly Leibniz's "infinitely small" differentials dx and
dy can be seen as an attempt to get "arbitrarily close" to x and y, respectively.
This is the idea at the heart of calculus: to get arbitrarily close to, say, x
without actually reaching it.

    As we saw in Chapter 4, Lagrange tried to avoid the entire issue of "arbi-
trary closesness," both in the limit and differential forms when, in 1797, he
attempted to found calculus on infinite series.

    Although Lagrange's efforts failed, they set the stage for Cauchy to provide
a definition of derivative which in turn relied on his precise formulation of a
limit. Consider the following example: to determine the slope of the tangent
line (derivative) of f (x) = sin x at x = 0. We consider the graph of the
difference quotient D(x) = x sin x .
CHAPTER 8. CONTINUITY: WHAT IT ISN'T AND WHAT IT IS 100

    From the graph, it appears that D(0) = 1 but we must be careful. D(0)
doesn't even exist! Somehow we must convey the idea that D(x) will approach
1 as x approaches 0, even though the function is not defined at 0. Cauchy's
idea was that the limit of D(x) would equal 1 because we can make D(x) differ
from 1 by as little as we wish ([6], p. 158).

    Karl Weierstrass made these ideas precise in his lectures on analysis at the
University of Berlin (1859-60) and provided us with our modern formulation.

Definition 8.3.1 We say lim f (x) = L provided that for each  > 0, there
xa
exists  > 0 such that if 0 < |x - a| <  then |f (x) - L| < .
                                                              

Before we delve into this, notice that it is very similar to the definition of the

continuity of f (x) at x = a. In fact we can readily see that f is continuous at x =

a if and only if lim f (x) = f (a).

                             xa

Comment. This presumes that a is an accumulation point of the domain
of f (Definition 11.0.5). We will discuss accumulation points in Chapter 11.

    There are two differences between this definition and the definition of con-
tinuity and they are related. The first is that we replace the value f (a) with L.
This is because the function may not be defined at a. In a sense the limiting
value L is the value f would have if it were defined and continuous at a. The
second is that we have replaced

                                     |x - a| < 

with
                                         0 < |x - a| < .

    Again, since f needn't be defined at a, we will not even consider what
happens when x = a. This is the only purpose for this change.

    As with the definition of the limit of a sequence, this definition does not
determine what L is, it only verifies that your guess for the value of the limit
is correct.

    Finally, a few comments on the differences and similiarities between this
limit and the limit of a sequence are in order, if for no other reason than
because we use the same notation (lim) for both.

    When we were working with sequences in Chapter 6 and wrote things like
 lim an we were thinking of n as an integer that got bigger and bigger. To

n

put that more mathematically, the limit parameter n was taken from the set
of positive integers, or n  N.

    For both continuity and the limit of a function we write things like lim f (x)

                                                                                                                                   xa

and think of x as a variable that gets arbitrarily close to the number a. Again,
to be more mathematical in our language we would say that the limit parameter
x is taken from the . . . Well, actually, this is interesting isn't it? Do we need
to take x from Q or from R? The requirement in both cases is simply that we
be able to choose x arbitrarily close to a. From Theorem 2.0.11 of Chapter 2
we see that this is possible whether x is rational or not, so it seems either will
CHAPTER 8. CONTINUITY: WHAT IT ISN'T AND WHAT IT IS 101

work. This leads to the pardoxical sounding conclusion that we do not need a
continuum (R) to have continuity. This seems strange.

    Before we look at the above example, let's look at some algebraic examples
to see the definition in use.
Example 8.3.2 Consider the function D(x) = x-1 x2-1 , x = 1. You probably
recognize this as the difference quotient used to compute the derivative of
f (x) = x2 at x = 1, so we strongly suspect that lim x2 - 1 = 2. Just as when

                                                               x1 x - 1
we were dealing with limits of sequences, we should be able to use the definition
to verify this. And as before, we will start with some scrapwork.

    SCRAPWORK
    Let  > 0. We wish to find a  > 0 such that if 0 < |x - 1| <  then
 x-1 x2-1 - 2 < . With this in mind, we perform the following calculations

                           x2 - 1 - 2 = |(x + 1) - 2| = |x - 1| .
                            x-1

    Now we have a handle on  that will work in the definition and we'll give
the formal proof that

                                         lim x2 - 1 = 2.
                                         x1 x - 1

                                                                                                    
Proof. Let  > 0 and let  = . If 0 < |x - 1| < , then

                      x2 - 1 - 2 = |(x + 1) - 2| = |x - 1| <  = .
                       x-1

                                                                                                    
    As in our previous work with sequences and continuity, notice that the
scrapwork is not part of the formal proof (though it was necessary to determine
an appropriate ). Also, notice that 0 < |x - 1| was not really used except to
ensure that x = 1.

Problem 8.3.3 Use the definition of a limit to verify that

                                        lim x2 - a2 = 2a.
                                       xa x - a

                                                                                                    
Problem 8.3.4 Use the definition of a limit to verify each of the following
limits.

 (a) lim x3 - 1 = 3
       x1 x - 1
       Hint.

x3 - 1  = x2 + x + 1 - 3
x-1 -3
         x2 - 1 + |x - 1|
        = (x - 1 + 1)2 - 1 + |x - 1|
        = (x - 1)2 + 2(x - 1) + |x - 1|
         |x - 1|2 + 3 |x - 1| .
CHAPTER 8. CONTINUITY: WHAT IT ISN'T AND WHAT IT IS 102

             
 (b) lim x - 1 = 1/2

       x1 x - 1
       Hint.

                                x-1 - 1 =  1 - 1

                              x-1 2                   x+1 2
                                                      
                                                  2 - ( x + 1)
                                        =
                                                      2 ( x + 1)

                                        =             1-x
                                                            2
                                                  2 (1 + x)

                                           1
                                         2 |x - 1| .

Let's      go  back  to  the  original  problem:  to  show  that  lim  sin x            1.  
                                                                         x
                                                                  x0          =

While rigorous, our definition of continuity is quite cumbersome. We really

need to develop some tools we can use to show continuity rigorously without

having to refer directly to the definition. We have already seen in Theorem 8.2.1

one way to do this. Here is another. The key is the observation we made after

the definition of a limit:

           f is continuous at x = a if and only if lim f (x) = f (a).

                                                                                    xa

    Read another way, we could say that lim f (x) = L provided that if we

                                                                                 xa

redefine f (a) = L (or define f (a) = L in the case where f (a) is not defined)
then f becomes continuous at a. This allows us to use all of the machinery we
proved about continuous functions and limits of sequences.

    For example, the following corollary to Theorem 8.2.1 comes virtually for
free once we've made the observation above.

Corollary 8.3.5 lim f (x) = L if and only if f satisfies the following property:

                                 xa

     sequences (xn), xn = a, if lim xn = a then lim f (xn) = L.
                                        n                   n

Armed with this, we can prove the following familiar limit theorems from

calculus.

Theorem 8.3.6 Suppose lim f (x) = L and lim g(x) = M , then
                              xa                      xa

a lim (f (x) + g(x)) = L + M

    xa

b lim (f (x)  g(x)) = L  M

    xa

c lim f (x) = L/M provided M = 0 and g(x) = 0, for x sufficiently
  xa g(x)
  close to a (but not equal to a).

    We will prove part (a) to give you a feel for this and let you prove parts (b)
and (c).

Proof. Let (xn) be a sequence such that xn = a and lim xn = a. Since
                                                                  n
lim f (x) = L and lim g(x) = M we see that lim f (xn) = L and lim g(xn) =
xa                   xa                               n                       n
M . By Theorem 6.2.4 of Chapter 6, we have lim f (xn) + g(xn) = L + M .
                                                      n
CHAPTER 8. CONTINUITY: WHAT IT ISN'T AND WHAT IT IS 103

Since {xn} was an arbitrary sequence with xn = a and lim xn = a we have

                                                                                                         n

                           lim f (x) + g(x) = L + M .

                           xa

                                                                  

Problem 8.3.7 Prove parts (b) and (c) of Theorem 8.3.6.           

     More in line with our current needs, we have a reformulation of the Squeeze

Theorem.

Theorem 8.3.8 Squeeze Theorem for functions

     Suppose f (x)  g(x)  h(x), for x sufficiently close to a (but not equal to

a). If lim f (x) = L = lim h(x), then lim g(x) = L also.
       xa              xa             xa

Problem 8.3.9 Prove Theorem 8.3.8.

Hint.

     Use Theorem 6.2.15, the Squeeze Theorem for sequences from Chapter 6.

                       sin x 
     Returning to lim      we'll see that the Squeeze Theorem is just what we
              x0 x
need. First notice that since D(x) = sin x/x is an even function, we only need

to focus on x > 0 in our inequalities. Consider the unit circle.

Problem 8.3.10 Use the fact that

              area (OAC) < area ( sector OAC) < area (OAB)

to show that if 0 < x < /2, then cos x < sin x/x < 1. Use the fact that all

of these functions are even to extend the inequality for -/2 < x < 0 and use
                                      sin x
the  Squeeze  Theorem  to  show  lim    x    =  1.                
                                 x0

8.4 The Derivative, An Afterthought

No, the derivative isn't really an afterthought. Along with the integral it is, in
fact, one of the most powerful and useful mathematical objects ever devised
and we've been working very hard to provide a solid, rigorous foundation for
it. In that sense it is a primary focus of our investigations.

    On the other hand, now that we have built up all of the machinery we
need to define and explore the concept of the derivative it will appear rather
CHAPTER 8. CONTINUITY: WHAT IT ISN'T AND WHAT IT IS 104

pedestrian alongside ideas like the convergence of power series, Fourier series,
and the bizarre properties of Q and R.

    You spent an entire semester learning about the properties of the derivative
and how to use them to explore the properties of functions so we will not repeat
that effort here. Instead we will define it formally in terms of the ideas and
techniques we've developed thus far.

Definition 8.4.1 The Derivative. Given a function f (x) defined on an
interval (a, b) we define

                                f (x) = lim f (x + h) - f (x) .
                                         h0            h

                                                                                                     
    There are a few fairly obvious facts about this definition which are never-

theless worth noticing explicitly:

1. The derivative is defined at a point. If it is defined at every point in an
   interval (a, b) then we say that the derivative exists at every point on the
   interval.

2. Since it is defined at a point it is at least theoretically possible for a
   function to be differentiable at a single point in its entire domain.

3. Since it is defined as a limit and not all limits exist, functions are not
   necessarily differentiable.

4. Since it is defined as a limit, Corollary 8.3.5 applies. That is, f (x) exists
   if and only if  sequences (hn), hn = 0, if lim hn = 0 then

                                                                                     n

                                f (x) = lim f (x + hn) - f (x) .
                                            n             hn

      Since lim hn = 0 this could also be written as

                 n

                                f (x) = lim f (x + hn) - f (x) .
                                            hn 0          hn

Theorem 8.4.2 Differentiability Implies Continuity
    If f is differentiable at a point c then f is continuous at c as well.

Problem 8.4.3 Prove Theorem 8.4.2                                                             

As we mentioned, the derivative is an extraordinarily useful mathematical tool

but it is not our intention to learn to use it here. Our purpose here is to define

it rigorously (done) and to show that our formal definition does in fact recover

the useful properties you came to know and love in your calculus course.

     The first such property is known as Fermat's Theorem.

Theorem 8.4.4 Fermat's Theorem. Suppose f is differentiable in some

interval (a, b) containing c. If f (c)  f (x) for every x in (a, b), then f (c) = 0.

Proof. Since f (c) exists we know that if (hn)n=1  converges to zero then the

sequence an = h f(c+hn)-f(c) converges to f (c). The proof consists of showing
that  f (c)        and       n  f (c)       from  which   we  conclude    that  f (c)     0.  We
                0                        0                                             =
                        that

will only show the first part. The second is left as an exercise.

Claim: f (c)  0.                                                                n n=n 1  . Then

Let   n0  be  sufficiently  large  that   1  <    b-c  and    take  (hn)  =            0
                                         n0
CHAPTER 8. CONTINUITY: WHAT IT ISN'T AND WHAT IT IS 105

f  c + 1n  - f (c)    0  and  1  >  0,  so  that
                              n

                    f (c + hn) - f (c)  0, n = n0, n0 + 1, . . .
                             hn

Therefore f (c) = lim f (c + hn) - f (c)  0 also. 
                    hn 0            hn

Problem 8.4.5 Show that f (c)  0 and conclude that f (c) = 0.     

Problem 8.4.6 Show that if f (c)  f (x) for all x in some interval (a, b) then

f (c) = 0 too.                                                    

   Many of the most important properties of the derivative follow from what

is called the Mean Value Theorem (MVT) which we now state.

Theorem 8.4.7 The Mean Value Theorem
    Suppose f  exists for every x  (a, b) and f is continuous on [a, b]. Then

there is a real number c  (a, b) such that

                                      f (c) = f (b) - f (a) .
                                                     b-a

    However, it would be difficult to prove the MVT right now. So we will first
state and prove Rolle's Theorem, which can be seen as a special case of the
MVT. The proof of the MVT will then follow easily.

    Michel Rolle first stated the following theorem in 1691. Given this date
and the nature of the theorem it would be reasonable to suppose that Rolle
was one of the early developers of calculus but this is not so. In fact, Rolle
was disdainful of both Newton and Leibniz's versions of calculus, once deriding
them as a collection of "ingenious fallacies." It is a bit ironic that his theorem
is so fundamental to the modern development of the calculus he ridiculed.

Theorem 8.4.8 Rolle's Theorem
    Suppose f  exists for every x  (a, b), f is continuous on [a, b], and

                                        f (a) = f (b).

   Then there is a real number c  (a, b) such that

                                        f (c) = 0.

Proof.

Comment. Any proof that relies on the Extreme Value Theorem is not com-
plete until the EVT has been proved. We'll get to this in Chapter 9.

Since f is continuous on [a, b] we see, by the Extreme Value Theorem, that f
has both a maximum and a minimum on [a, b]. Denote the maximum by M
and the minimum by m. There are several cases:

           Case 1:       f (a) = f (b) = M = m. In this case f (x) is constant
           Case 2:       (why?). Therefore f (x) = 0 for every x  (a, b).

           Case 3:       f (a) = f (b) = M = m. In this case there is a real number
                         c  (a, b) such that f (c) is a local minimum. By Fermat's
                         Theorem, f (c) = 0.

                         f (a) = f (b) = m = M . In this case there is a real
                         number c  (a, b) such that f (c) is a local maximum. By
                         Fermat's Theorem, f (c) = 0.
CHAPTER 8. CONTINUITY: WHAT IT ISN'T AND WHAT IT IS 106

Case 4:  f (a) = f (b) is neither a maximum nor a minimum. In

         this case there is a real number c1  (a, b) such that f (c1)
         is a local maximum, and a real number c2  (a, b) such
         that f (c2) is a local minimum. By Fermat's Theorem,
         f (c1) = f (c2) = 0.

                                                                                                    
    With Rolle's Theorem in hand we can prove the MVT which is really a
corollary to Rolle's Theorem or, more precisely, it is a generalization of Rolle's
Theorem. To prove it we only need to find the right function to apply Rolle's
Theorem to. The following figure shows a function, f (x), cut by a secant line,
L(x), from (a, f (a)) to (b, f (b)).

    The vertical difference from f (x) to the secant line, indicated by (x) in
the figure should do the trick. You take it from there.

Problem 8.4.9 Prove the Mean Value Theorem.         

The Mean Value Theorem is extraordinarily useful. Almost all of the prop-

erties of the derivative that you used in calculus follow more or less easily from

it. For example the following is true.

Corollary 8.4.10 If f (x) > 0 for every x in the interval (a, b) then for every
c, d  (a, b) where d > c we have

                            f (d) > f (c).

    That is, f is increasing on (a, b).
Proof. Suppose c and d are as described in the corollary. Then by the Mean
Value Theorem there is some number, say   (c, d)  (a, b) such that

                            f () = f (d) - f (c) .
                                           d-c

Since f () > 0 and d - c > 0 we have f (d) - f (c) > 0, or f (d) > f (c). 

Problem 8.4.11 Show that if f (x) < 0 for every x in the interval (a, b) then

f is decreasing on (a, b).                          

Corollary 8.4.12 Suppose f is differentiable on some interval (a, b), f  is
continuous on (a, b), and that f (c) > 0 for some c  (a, b). Then there is an
interval, I  (a, b), containing c such that for every x, y in I where x  y,
f (x)  f (y).

Problem 8.4.13 Prove Corollary 8.4.12.              
CHAPTER 8. CONTINUITY: WHAT IT ISN'T AND WHAT IT IS 107

Problem 8.4.14 Show that if f is differentiable on some interval (a, b) and

that f (c) < 0 for some c  (a, b) then there is an interval, I  (a, b), containing

c such that for every x, y in I where x  y, f (x)  f (y).                    

8.5 Additional Problems

Problem 8.5.1 Use the definition of continuity to prove that the constant

function g(x) = c is continuous at any point a.                              

Problem 8.5.2

(a) Use the definition of continuity to prove that ln x is continuous at 1.

Hint.
You may want to use the fact |ln x| <   - < ln x <  to find a .

(b) Use part (a) to prove that ln x is continuous at any positive real number
      a.

Hint.

ln(x) = ln(x/a) + ln(a). This is a combination of functions which are con-
tinuous at a. Be sure to explain how you know that ln(x/a) is continuous
at a.

                                                                             

Problem 8.5.3 Write a formal definition of the statement f is not contin-

uous at a, and use it to prove that the function f (x) = x if x = 1 is not
                                                                              0 if x = 1

continuous at a = 1.                                                         
Chapter 9

Intermediate and Extreme Val-
ues

9.1 Completeness of the Real Number System

Recall that in deriving the Lagrange and Cauchy forms of the remainder for
Taylor series, we made use of the Extreme Value Theorem (EVT) and Interme-
diate Value Theorem (IVT). In Chapter 8, we produced an analytic definition
of continuity that we can use to prove these theorems. To provide the rest of
the necessary tools we need to explore the make-up of the real number system.
To illustrate what we mean, suppose that we only used the rational number
system. We could still use our definition of continuity and could still consider
continuous functions such as f (x) = x2. Notice that 2 is a value that lies
between f (1) = 1 and f (2) = 4.

The IVT says that somewhere between 1 and 2, f must take on the value

2. That is, there must exist some number c  [1, 2] such that f (c) = 2. You
might say, "Big deal! Everyone knows c = 2 works."
                                                    
However, we are only working with rational numbers and 2 is not rational.

As we saw in Chapter 2 the rational number system has holes in it, whereas

the real number system doesn't. Again, "Big deal! Let's just say that the real

number system contains (square) roots."

108
CHAPTER 9. INTERMEDIATE AND EXTREME VALUES  109

    This sounds reasonable and it actually works for square roots, but consider

the function f (x) = x - cos x. We know this is a continuous function. We also
know that f (0) = -1 and f ( 2 ) = 2 . According to the IVT, there should be
some number c  [ 0, 2 ], where f (c) = 0. The graph is below.

     The situation is not as transparent as before. What would this mysterious
c be where the curve crosses the x axis? Somehow we need to convey the idea
that the real number system is a continuum. That is, it has no "holes" in it.

     How about this? Why don't we just say that it has no holes in it? Some-
times the simple answer works best! But not in this case. How are we going to
formulate a rigorous proof based on this statement? Just like with convergence
and continuity, what we need is a rigorous way to convey this idea that the
real number system does not have any holes, that it is complete.

     We will see that there are several different, but equivalent ways to convey
this notion of completeness. We will explore some of them in this chapter. For
now we adopt the following as our Completeness Axiom for the real number
system.

Axiom 9.1.1 Nested Interval Property of the Real Number System
(NIP). Suppose we have two sequences of real numbers (xn)and (yn) satisfying
the following conditions:

   1. x1  x2  x3  . . . (this says that the sequence, (xn), is non-decreasing)

   2. y1  y2  y3  . . . (this says that the sequence, (yn), is non-increasing)

   3.  n, xn  yn

   4. lim (yn - xn) = 0

           n

    Then there exists a unique number c such that xn  c  yn for all n.
     Geometrically, we have the following situation.

     Notice that we have two sequences (xn) and (yn), one increasing (really
non-decreasing) and one decreasing (non-increasing). These sequences do not
pass each other. In fact, the following is true:
CHAPTER 9. INTERMEDIATE AND EXTREME VALUES                                   110

Problem 9.1.2 Let (xn), (yn) be sequences as in the NIP. Show that for all

n, m  N, xn  ym.                                                                   

    They are also coming together in the sense that limn (yn - xn) = 0.

The NIP says that in this case there is a unique real number c in the middle

of all of this xn  c  yn for all n  N.

    If there was no such c then there would be a hole where these two sequences
come together. The NIP guarantees that there is no such hole. We do not need
to prove this since an axiom is, by definition, a self evident truth. We are taking
it on faith that the real number system obeys this law. The next problem shows
that the completeness property distinguishes the real number system from the
rational number system.

Problem 9.1.3

(a) Find two sequences of rational numbers (xn)and (yn) which satisfy prop-
     erties 1-4 of the NIP and such that there is no rational number c satisfying

     the conclusion of the NIP.

    Hint.
    Consider the decimal expansion of an irrational number.

(b) Find two sequences of rational numbers (xn) and (yn) which satisfy prop-
      erties 1-4 of the NIP and such that there is a rational number c satisfying

      the conclusion of the NIP.

                                                                                                    
    You might find the name "Nested Interval Property" to be somewhat cu-

rious. One way to think about this property is to consider that we have a

sequence of "nested closed intervals" [ x1, y1]  [ x2, y2]  [ x3, y3]     whose
lengths yn - xn are "shrinking to 0." The conclusion is that the intersection
of these intervals is non-empty and, in fact, consists of a single point. That is,

  n=1  [xn, yn] = {c}.
    It appears that the sequences (xn) and (yn) in the NIP converge to c. This

is, in fact, true and can be proven rigorously. In what follows, this will prove

to be a valuable piece of information.

Theorem 9.1.4 Suppose that we have two sequences (xn) and (yn) satisfying
all of the assumptions of the Nested Interval Property. If c is the unique number

such that xn  c  yn for all n, then limn xn = c and limn yn = c.

Problem 9.1.5 Prove Theorem 9.1.4.                                                 

    To illustrate the idea that the NIP "plugs the holes" in the real line, we

will prove the existence of square roots of nonnegative real numbers.

Theorem 9.1.6 Suppose a  R, a  0. There exists a real number c  0 such
that c2 = a. 

    Notice that we can't just say, "Let c = a," since the idea is to show that

this square root exists. In fact, throughout this proof, we cannot really use

a square root symbol as we haven't yet proved that they (square roots) exist.

We will give the idea behind the proof as it illustrates how the NIP is used.

Sketch of Proof. Our strategy is to construct two sequences which will "narrow

in" on the number c that we seek. With that in mind, we need to find a number

x1  such  that  x2   a  and  a  number  y1  such  that  y2    a.  (Remember  that  we

                  1                                       1
          
can't say x1 or y1.) There are many possibilities, but how about x1 = 0 and
CHAPTER 9. INTERMEDIATE AND EXTREME VALUES                                                               111

y1    =  a  +  1?  You  can  check   that   these  will  satisfy      x2     a  y12.      Furthermore

                                                                        1

x1  y1. This is the starting point.

The technique we will employ is often called a bisection technique, and is a

useful way to set ourselves up for applying the NIP. Let m1 be the midpoint

of the interval [ x1, y1].       Then  either   we   have    m2         a   or  m2     a.      In the case

m2                                                              1                  1

   1      a,   we  really  want  m1  to   take  the  place      of  x1  since   it    is  larger   than   x1,

but still represents an underestimate for what would be the square root of a.

This     thinking  prompts     the  following   move.    If  m2         a,  we  will  relabel     things  by

                                                                1

letting x2 = m1 and y2 = y1. The situation looks like this on the number line.

In the other case where a  m21, we will relabel things by letting x2 = x1 and
y2 = m1. The situation looks like this on the number line.

In either case, we've reduced the length of the interval where the square root
lies to half the size it was before. Stated in more specific terms, in either case
we have the same results:

                   x1  x2  y2  y1; x12  a  y12; x22  a  y22

and
                                    y2 - x2 = 1 (y1 - x1) .
                                                  2

Now we play the same game, but instead we start with the interval [x2, y2]. Let

m2    be    the  midpoint  of  [x2, y2].    Then  we  have      m2    a     or  m2         a.  If  m2     a,

                                                                   2               2                  2
we relabel x3 = m2 and y3 = y2. If a  m22, we relabel x3 = x2 and y3 = m2.

In either case, we end up with

      x1  x2  x3  y3  y2  y1; x12  a  y12; x22  a  y22; x32  a  y32

and                                      1                   1

                           y3 - x3 = (y2 - x2) = 2 (y1 - x1) .22

Continuing in this manner, we will produce two sequences, (xn)and (yn) satis-
fying the following conditions:

    1. x1  x2  x3  . . .

    2. y1  y2  y3  . . .

    3.  n, xn  yn

                                         1
    4. lim (yn - xn) = lim n-1 (y1 - x1) = 0
         n                     n 2

    5. These sequences also satisfy the following property:

                                            n, xn2  a  yn2

Properties 1-4 tell us that (xn)and (yn) satisfy all of the conditions of the NIP,

so we can conclude that there must exist a real number c such that xn  c  yn

for all n. At this point, you should be able to use property 5. to show that

c2 = a as desired.                                                                                        
CHAPTER 9. INTERMEDIATE AND EXTREME VALUES                                                      112

Problem 9.1.7 Turn the above outline into a formal proof of Theorem 9.1.6.

                                                                                                    
    The bisection method we employed in the proof of Theorem 9.1.6 is pretty

typical of how we will use the NIP, as taking midpoints ensures that we will

create a sequence of "nested intervals." We will employ this strategy in the

proofs of the IVT and EVT. Deciding how to relabel the endpoints of our

intervals will be determined by what we want to do with these two sequences

of real numbers. This will typically lead to a fifth property, which will be

crucial in proving that the c guaranteed by the NIP does what we want itto
do. Specifically, in the above example, we always wanted our candidate for a

to be in the interval [ xn, yn]. This judicious choice led to the extra Property 5:

  n,   x2    a  yn2 .         In applying the NIP to prove the IVT and EVT, we will

         n

find that properties 1-4 will stay the same. Property 5 is what will change

based on the property we want c to have.

    Before we tackle the IVT and EVT, let's use the NIP to address an in-

teresting question about the Harmonic Series. Recall that the Harmonic Se-

ries,  1  +    1  +  1  +  1  +    ,  grows  without      bound,  that    is,   n=1 n  1 = . The
               2     3     4
question is how slowly does this series grow? For example, how many terms

would it take before the series surpasses 100? 1000? 10000? Leonhard Euler

decided to tackle this problem in the following way. Euler decided to con-

sider the limn 1 + 12 + 13 +    + 1n - ln (n + 1) . This limit is called
Euler's constant and is denoted by . This says that for n large, we have

1+  1  +    1  +렁+       1  ln(n + 1) + .     If      we   could  approximate    ,  then  we  could
    2       3              n
replace     the   inequality         1      1           1         with  the  more  tractable  inequal-
                                 1+  2   +  3  +  +  n    100

ity ln(n + 1) +   0 and solve for n in this. This should tell us roughly how

many terms would need to be added in the Harmonic Series to surpass 100.

Approximating  with a computer is not too bad. We could make n as large

as we wish in 1 + 12 + 13 +    + 1n -ln(1 + n) to make closer approximations
for . The real issue is, HOW DO WE KNOW THAT

                         lim     1 + 1 + 1 +    + 1 - ln (n + 1)
                                            23               n
                        n

ACTUALLY EXISTS?
    You might want to say that obviously it should, but let us point out that

as of the printing of this book (2013), it is not even known if  is rational or
irrational. So, in our opinion the existence of this limit is not so obvious. This
is where the NIP will come into play; we will use it to show that this limit, in
fact, exists. The details are in the following problem.

Problem 9.1.8 The purpose of this problem is to show that

                         lim         1 + 1 + 1 +    + 1 - ln (n + 1)
                                            23               n
                        n

exists.

(a) Let xn =            1 + 12 + 13 +    + 1n - ln (n + 1). Use the following diagram
     to show                                x1  x2  x3    
CHAPTER 9. INTERMEDIATE AND EXTREME VALUES                                                   113

(b) Let zn = ln (n + 1) - 12 + 13 +    + 1 n+1 . Use a similar diagram to
      show that z1  z2  z3    .

(c) Let yn = 1 - zn. Show that (xn) and (yn) satisfy the hypotheses of the
     nested interval property and use the NIP to conclude that there is a real

     number  such that xn    yn for all n.

(d) Conclude that limn 1 + 12 + 13 +    + 1n - ln (n + 1) = .

                                                                                             

Problem 9.1.9 Use the fact that xn    yn for all n to approximate  to

three decimal places.                                                                        

Problem 9.1.10

(a)  Use    the  fact  that  for  large  n,  1+    1  +  1   + 렁 +  1    ln (n + 1) +      to
                                                   2     3            n
     determine approximately how large n must be to make

                                  1 + 1 + 1 +    + 1  100.
                                        23                  n

(b) Suppose we have a supercomputer which can add 10 trillion terms of the
      Harmonic Series per second. Approximately how many Earth Ages (the
      current age of the Earth) would it take for this computer to sum the
      Harmonic Series until it surpasses 100?

                                                                                             

9.2 Proof of the Intermediate Value Theorem

We now have all of the tools we need to prove the Intermediate Value Theorem
(IVT).

Theorem 9.2.1 Intermediate Value Theorem (IVT)

    Suppose f (x) is continuous on [a, b] and v is any real number between f (a)

and f (b). Then there exists a real number c  [ a, b] such that f (c) = v.

Sketch of Proof. We have two cases to consider: f (a)  v  f (b) and f (a) 

v  f (b).

We will look at the case f (a)  v  f (b). Let x1 = a and y1 = b, so we have

x1  y1 and f (x1)  v  f (y1). Let m1 be the midpoint of [ x1, y1] and notice

that we have either f (m1)  v or f (m1)  v. If f (m1)  v , then we relabel

x2 = m1 and y2 = y1. If f (m1)  v , then we relabel x2 = x1 and y2 = m1.

In  either  case,  we  end   up   with   x1    x2        y2      y1,  y2 - x2  =  1  (y1  -  x1),
                                                                                  2
f (x1)  v  f (y1), and f (x2)  v  f (y2).

Now play the same game with the interval [ x2, y2]. If we keep playing this game,

we will generate two sequences (xn) and (yn), satisfying all of the conditions

of the nested interval property. These sequences will also satisfy the following

extra property:  n, f (xn)  v  f (yn). By the NIP, there exists a c such

that xn  c  yn,  n. This should be the c that we seek though this is not

obvious. Specifically, we need to show that f (c) = v. This should be where

the continuity of f at c and the extra property on (xn)and (yn) come into play.

                                                                                             

Problem 9.2.2 Turn the ideas of the previous paragraphs into a formal proof

of the IVT for the case f (a)  v  f (b).                                                     
CHAPTER 9. INTERMEDIATE AND EXTREME VALUES                              114

Problem 9.2.3 We can modify the proof of the case f (a)  v  f (b) into a

proof of the IVT for the case f (a)  v  f (b). However, there is a sneakier

way to prove this case by applying the IVT to the function -f . Do this to

prove the IVT for the case f (a)  v  f (b).                             

Problem 9.2.4 Use the IVT to prove that any polynomial of odd degree must

have a real root.                                                       

9.3 The Bolzano-Weierstrass Theorem

Once we introduced the Nested Interval Property (Axiom 9.1.1), the Inter-
mediate Value Theorem (Theorem 9.2.1) followed pretty readily. The proof of
Extreme Value Theorem (Theorem 9.4.8) takes a bit more work. First we need
to show that a function that satisfies the conditions of the EVT is bounded.

Theorem 9.3.1 A continuous function defined on a closed, bounded interval
must be bounded. That is, let f be a continuous function defined on [ a, b]. Then
there exists a positive real number B such that |f (x)|  B for all x  [ a, b].

    Sketch of Alleged Proof: Let's assume, for contradiction, that there is

no such bound B. This says that for any positive integer n, there must exist

xn  [a, b] such that |f (xn)| > n. (Otherwise n would be a bound for f .) IF
the sequence (xn) converged to something in [ a, b], say c, then we would have
our contradiction. Indeed, we would have limn xn = c. By the continuity of
f at c and Theorem 8.2.1 of Chapter 8, we would have limn f (xn) = f (c).
This would say that the sequence (f (xn)) converges, so by Lemma 6.2.7 of
Chapter 6, it must be bounded. This would provide our contradiction, as we

had |f (xn)| > n, for all positive integers n. QED?
    This would all work well except for one little problem. The way it was

constructed, there is no reason to expect the sequence (xn) to converge to
anything and we can't make such an assumption. That is why we emphasized

the IF above. Fortunately, this idea can be salvaged. While it is true that the

sequence (xn) may not converge, part of it will. We will need the following
definition.

Definition 9.3.2 Let (nk)k=1  be a strictly increasing sequence of positive
integers; that is, n1 < n2 < n3 <    . If (xn)n=1  is a sequence, then (xnk )k=1  =
(xn1 , xn2 , xn3 , . . .) is called a subsequence of (xn).
                                                                        

The idea is that a subsequence of a sequence is a part of the sequence, (xn),

which is itself a sequence. However, it is a little more restrictive. We can choose

any term in our sequence to be part of the subsequence, but once we choose that

term, we can't go backwards. This is where the condition n1 < n2 < n3 <   

comes in. For example, suppose we started our subsequence with the term x100.

We could not choose our next term to be x99. The subscript of the next term

would have to be greater than 100. In fact, the thing about a subsequence is

that it is all in the subscripts; we are really choosing a subsequence (nk) of the

sequence of subscripts (n) in (xn).

Example 9.3.3 Given the sequence (xn), the following are subsequences.

(a) (x2, x4, x6, . . .) = (x2k)k=1 

(b) (x1, x4, x9, . . .) = (xk2 )k=1 

(c) (xn) itself.

                                                                        
CHAPTER 9. INTERMEDIATE AND EXTREME VALUES                  115

Example 9.3.4 The following are NOT subsequences.

 (a) (x1, x1, x1, . . .)
 (b) (x99, x100, x99, . . .)
 (c) (x1, x2, x3)

                                                                                                    
    The subscripts in the examples we have seen so far have a discernable
pattern, but this need not be the case. For example,

              (x2, x5, x12, x14, x23, . . .)

would be a subsequence as long as the subscripts form an increasing sequence
themselves.

Problem 9.3.5 Suppose limn xn = c. Prove that limk xnk = c for any
subsequence (xnk ) of (xn).

Hint.

First prove that nk  k.                                     

A very important theorem about subsequences was introduced by Bernhard

Bolzano and, later, independently proven by Karl Weierstrass. Basically, this

theorem says that any bounded sequence of real numbers has a convergent

subsequence.

Theorem 9.3.6 The Bolzano-Weierstrass Theorem
    Let (xn) be a sequence of real numbers such that xn  [ a, b],  n. Then

there exists c  [ a, b] and a subsequence (xnk ), such that limk xnk = c.

    As an example of this theorem, consider the sequence

              ((-1)n) = (-1, 1, -1, 1, . . .) .

This sequence does not converge, but the subsequence
                                 (-1)2k = (1, 1, 1, . . .)

converges to 1. This is not the only convergent subsequence, as (-1)2k+1 =
(-1, -1, -1, . . .) converges to -1. Notice that if the sequence is unbounded,
then all bets are off; the sequence may have a convergent subsequence or it
may not. The sequences (((-1)n + 1) n) and (n) represent these possibilities
as the first has, for example, (-1)2k+1 + 1 (2k + 1) = (0, 0, 0, . . .) and the
second one has none.

    The Bolzano-Weierstrass Theorem says that no matter how "random" the

sequence (xn) may be, as long as it is bounded then some part of it must
converge. This is very useful when one has some process which produces a

"random" sequence such as what we had in the idea of the alleged proof in

Theorem 9.3.1.
Sketch of a Proof of the Bolzano-Weierstrass Theorem. Suppose we have our

sequence (xn) such that xn  [a, b],  n. To find our c for the subsequence to
converge to we will use the NIP. Since we are already using (xn) as our original
sequence, we will need to use different letters in setting ourselves up for the

NIP. With this in mind, let a1 = a and b1 = b, and notice that xn  [a1, b1] for
infinitely many n. (This is, in fact true for all n, but you'll see why we said

it the way we did.) Let m1 be the midpoint of [a1, b1] and notice that either
xn  [a1, m1] for infinitely many n or xn  [m1, b1] for infinitely many n. If
xn  [a1, m1] for infinitely many n, then we relabel a2 = a1 and b2 = m1. If
xn  [m1, b1] for infinitely many n, then relabel a2 = m1 and b2 = b1. In either
CHAPTER 9. INTERMEDIATE AND EXTREME VALUES                                                       116

case,  we  get   a1      a2     b2    b1,    b2 - a2  =  1  (b1  -  a1),  and   xn     [a2, b2]  for
                                                         2
infinitely many n.

Now we consider the interval [a2, b2] and let m2 be the midpoint of [a2, b2].

Since xn  [a2, b2] for infinitely many n, then either xn  [a2, m2] for infinitely

many n or xn  [m2, b2] for infinitely many n. If xn  [a2, m2] for infinitely

many n, then we relabel a3 = a2 and b3 = m2. If xn  [m2, b2] for infinitely

many n, then we relabel a3 = m2 and b3 = b2. In either case, we get a1 

a2     a3    b3      b2    b1,  b3  - a3  =  1  (b2  - a2)  =  1   (b1  - a1),  and  xn    [a3, b3]
                                             2                 22
for infinitely many n.

If we continue in this manner, we will produce two sequences (ak)and (bk) with

the following properties:

    1. a1  a2  a3    

    2. b1  b2  b3    

    3.  k, ak  bk

                                        1
    4. lim (bk - ak) = lim k-1 (b1 - a1) = 0
       k                     k 2

    5. For each k, xn  [ ak, bk] for infinitely many n

By properties 1-5 and the NIP, there exists a unique c such that c  [ak, bk],

for all k. In particular, c  [a1, b1] = [a, b].

We have our c. Now we need to construct a subsequence converging to it. Since

xn  [a1, b1] for infinitely many n, choose an integer n1 such that xn1  [a1, b1].
Since xn  [a2, b2] for infinitely many n, choose an integer n2 > n1 such that

xn2  [a2, b2]. (Notice that to make a subsequence it is crucial that n2 > n1,
and this is why we needed to insist that xn  [a2, b2] for infinitely many n.)

Continuing in this manner, we should be able to build a subsequence (xnk )
that will converge to c. You can supply the details in the following problem.

                                                                                                 

Problem 9.3.7 Turn the ideas of the above outline into a formal proof of the

Bolzano-Weierstrass Theorem.                                                                     

Problem 9.3.8 Use the Bolzano-Weierstrass Theorem to complete the proof

of Theorem 9.3.1.                                                                                

9.4 The Supremum and the Extreme Value The-
     orem

Theorem 9.3.1 says that a continuous function on a closed, bounded interval
must be bounded. Boundedness, in and of itself, does not ensure the existence
of a maximum or minimum. We must also have a closed, bounded interval. To
illustrate this, consider the continuous function f (x) =tan-1x defined on the
(unbounded) interval (-, ).
CHAPTER 9. INTERMEDIATE AND EXTREME VALUES                                                      117

This     function   is  bounded    between  -       a  nd  2 ,  but  it  does  not  attain  a   maxi-
                                                 2
mum or minimum as the lines y =  2 are horizontal asymptotes. Notice that
if we restricted the domain to a closed, bounded interval then it would attain

its extreme values on that interval (as guaranteed by the EVT).

To find a maximum we need to find the smallest possible upper bound for

the range of the function. This prompts the following definitions.

Definition 9.4.1 Let S  R and let b be a real number. We say that b is an
upper bound of S provided b  x for all x  S.
                                                                                                    

For example, if S = (0, 1), then any b with b  1 would be an upper bound

of S. Furthermore, the fact that b is not an element of the set S is immaterial.

Indeed, if T = [ 0, 1], then any b with b  1 would still be an upper bound of

T . Notice that, in general, if a set has an upper bound, then it has infinitely

many since any number larger than that upper bound would also be an upper

bound. However, there is something special about the smallest upper bound.

Definition 9.4.2 Least Upper Bound Property (LUBP). Let S  R
and let b be a real number. We say that b is the least upper bound of S provided

a b  x for all x  S. (b is an upper bound of S)

b If c  x for all x  S, then c  b. (Any upper bound of S is at least as
   big as b.)

In this case, we also say that b is the supremum of S and we write

                                        b = sup (S) .

                                                                                                     
    Notice that the definition really says that b is the smallest upper bound of

S. Also notice that the second condition can be replaced by its contrapositive

so we can say that b = sup S if and only if

a b  x for all x  S

b If c < b then there exists x  S such that c < x.

    The second condition says that if a number c is less than b, then it can't
be an upper bound, so that b really is the smallest upper bound.

    Also notice that the supremum of the set may or may not be in the set
itself. This is illustrated by the examples above as in both cases, 1 = sup(0, 1)
and 1 = sup[0, 1]. Obviously, a set which is not bounded above such as N =
{1, 2, 3, . . .} cannot have a supremum. However, for non-empty sets which are
bounded above, we have the following.

Theorem 9.4.3 The Least Upper Bound Property (LUBP). Let S be

a non-empty subset of R which is bounded above. Then S has a supremum.

Sketch of Proof. Since S = , then there exists s  S. Since S is bounded

above then it has an upper bound, say b. We will set ourselves up to use

the Nested Interval Property. With this in mind, let x1 = s and y1 = b and

notice that  x  S such that x  x1 (namely, x1 itself) and  x  S, y1  x.

You probably guessed what's coming next: let m1 be the midpoint of [ x1, y1].

Notice that either m1  x,  x  S or  x  S such that x  m1. In the

former case, we relabel, letting x2 = x1 and y2 = m1. In the latter case, we

let x2 = m1 and y2 = y1. In either case, we end up with x1  x2  y2  y1,

y2 - x2  =  1  (y1  - x1),  and    x    S  such  that  x        x2  and  x       S,  y2     x.  If  we
            2
continue this process, we end up with two sequences, (xn)and (yn), satisfying

the following conditions:
CHAPTER 9. INTERMEDIATE AND EXTREME VALUES                      118

1. x1  x2  x3  . . .

2. y1  y2  y3  . . .

3.  n, xn  yn

                                    1
4. lim (yn - xn) = lim n-1 (y1 - x1) = 0
n              n 2

5.  n,  x  S such that x  xn and  x  S, yn  x,

By properties 1-5 and the NIP there exists c such that xn  c  yn,  n. We

will leave it to you to use property 5 to show that c = sup S.  

Problem 9.4.4 Complete the above ideas to provide a formal proof of Theo-

rem 9.4.3.                                                      

Notice that we really used the fact that S was non-empty and bounded

above in the proof of Theorem 9.4.3. This makes sense, since a set which is

not bounded above cannot possibly have a least upper bound. In fact, any real

number is an upper bound of the empty set so that the empty set would not

have a least upper bound.

The following corollary to Theorem 9.4.3 can be very useful.

Corollary 9.4.5 Let (xn) be a bounded, increasing sequence of real numbers.
That is, x1  x2  x3    . Then (xn) converges to some real number c.

Problem 9.4.6 Prove Corollary 9.4.5.

Hint.

Let c = sup{xn| n = 1, 2, 3, . . .}. To show that lim xn = c, let  > 0.Note
                                             n
that c -  is not an upper bound. You take it from here!         

Problem 9.4.7 Consider the following curious expression

                           2 + 2 + 2 + ....

We will use Corollary 9.4.5 to show that this actually converges to some real
number. After we know it converges we can actually compute what it is. Of
course to do so, we need to define things a bit more precisely. With this in
mind consider the following sequence (xn) defined as follows:

                                                     
                                              x1 = 2

                                                   
                                        xn+1 = 2 + xn.

    a Use induction to show that xn < 2 for n = 1, 2, 3, . . ..

    b Use the result from part (a) to show that xn < xn+1 for n = 1, 2, 3, . . . .

    c From Corollary 9.4.5, we have that (xn) must converge to some number
       c. Use the fact that (xn+1) must converge to c as well to compute what
       c must be.

                                                                                                     
    We now have all the tools we need to tackle the Extreme Value Theorem.

Theorem 9.4.8 Extreme Value Theorem (EVT)
    Suppose f is continuous on [ a, b]. Then there exists c, d  [ a, b] such that

f (d)  f (x)  f (c), for all x  [ a, b].
CHAPTER 9. INTERMEDIATE AND EXTREME VALUES                                                    119

Sketch of Proof. We will first show that f attains its maximum. To this end,

recall that Theorem 9.3.1 tells us that f [ a, b] = {f (x)| x  [ a, b]} is a bounded

set. By the LUBP, f [ a, b] must have a least upper bound which we will label

s, so that s = sup f [ a, b]. This says that s  f (x),for all x  [ a, b]. All we

need to do now is find a c  [ a, b] with f (c) = s. With this in mind, notice

that  since  s  =  sup f [ a, b],  then  for  any  positive  integer  n,  s-  1  is  not  an  upper
                                                                              n
bound  of    f [ a, b].  Thus  there  exists                 with s -     1  < f (xn)  s.     Now,
                                              xn    [ a, b]               n

by the Bolzano-Weierstrass Theorem, (xn) has a convergent subsequence (xnk )

converging to some c  [ a, b]. Using the continuity of f at c, you should be

able to show that f (c) = s. To find the minimum of f , find the maximum of

-f .                                                                                          

Problem 9.4.9 Formalize the above ideas into a proof of Theorem 9.4.8. 
    Notice that we used the NIP to prove both the Bolzano-Weierstrass Theo-

rem and the LUBP. This is really unavoidable, as it turns out that all of those
statements are equivalent in the sense that any one of them can be taken as
the completeness axiom for the real number system and the others proved as
theorems. This is not uncommon in mathematics, as people tend to gravitate
toward ideas that suit the particular problem they are working on. In this
case, people realized at some point that they needed some sort of completeness
property for the real number system to prove various theorems. Each individ-
ual's formulation of completeness fit in with his understanding of the problem
at hand. Only in hindsight do we see that they were really talking about the
same concept: the completeness of the real number system. In point of fact,
most modern textbooks use the LUBP as the axiom of completeness and prove
all other formulations as theorems. We will finish this section by showing that
either the Bolzano-Weierstrass Theorem or the LUBP can be used to prove the
NIP. This says that they are all equivalent and that any one of them could be
taken as the completeness axiom.

Problem 9.4.10 Use the Bolzano-Weierstrass Theorem to prove the NIP.
That is, assume that the Bolzano-Weierstrass Theorem holds and suppose we
have two sequences of real numbers, (xn) and (yn), satisfying:

1. x1  x2  x3  . . .

2. y1  y2  y3  . . .

3.  n, xn  yn

4. lim (yn - xn) = 0.

     n

    Prove that there is a real number c such that xn  c  yn, for all n. 
    Since the Bolzano-Weierstrass Theorem and the Nested Interval Property
are equivalent, it follows that the Bolzano-Weierstrass Theorem will not work
for the rational number system.

Problem 9.4.11 Find a bounded sequence of rational numbers such that no

subsequence of it converges to a rational number.                                             

Problem 9.4.12 Use the Least Upper Bound Property to prove the Nested
Interval Property. That is, assume that every non-empty subset of the real
numbers which is bounded above has a least upper bound; and suppose that
we have two sequences of real numbers (xn) and (yn), satisfying:

1. x1  x2  x3  . . .

2. y1  y2  y3  . . .
CHAPTER 9. INTERMEDIATE AND EXTREME VALUES              120

3.  n, xn  yn

4. lim (yn - xn) = 0.

     n

    Prove that there exists a real number c such that xn  c  yn, for all n.
(Again, the c will, of necessity, be unique, but don't worry about that.)

Hint.

Corollary 9.4.5 might work well here.                   

Problem 9.4.13 Since the LUBP is equivalent to the NIP it does not hold for

the rational number system. Demonstrate this by finding a non-empty set of

rational numbers which is bounded above, but whose supremum is an irrational

number.                                                 

We have the machinery in place to clean up a matter that was introduced

in Chapter 2. If you recall (or look back) we introduced the Archimedean

Property of the real number system. This property says that given any two

positive real numbers a, b, there exists a positive integer n with na > b. As we

mentioned in Chapter 2, this was taken to be intuitively obvious. The analogy

we used there was to emptying an ocean b with a teaspoon a provided we are

willing to use it enough times n. The completeness of the real number system

allows us to prove it as a formal theorem.

Theorem 9.4.14 Archimedean Property of R
    Given any positive real numbers a and b, there exists a positive integer n,

such that na > b.

Problem 9.4.15 Prove Theorem 9.4.14.

Hint.

Assume that there are positive real numbers a and b, such that na  b

n  N. Then N would be bounded above by b/a. Let s = sup(N) and
consider s - 1.
                                                        
Given what we've been doing, one might ask if the Archimedean Property

is equivalent to the LUBP (and thus could be taken as an axiom). The answer

lies in the following problem.

Problem 9.4.16 Does Q satisfy the Archimedean Property and what does
this have to do with the question of taking the Archimedean Property as an

axiom of completeness?                                  

9.5 Additional Problems

Problem 9.5.1 Mimic the definitions of an upper bound of a set and the

least upper bound (supremum) of a set to give definitions for a lower bound of

a set and the greatest lower bound (infimum) of a set.

Note: The infimum of a set S is denoted by inf(S).      

Problem 9.5.2 Find the least upper bound (supremum) and greatest lower
bound (infimum) of the following sets of real numbers, if they exist. (If one
does not exist then say so.)

a S = 1 | n = 1, 2, 3, . . .
           n

b T = {r | r is rational and r2 < 2

c (-, 0)  (1, )
CHAPTER 9. INTERMEDIATE AND EXTREME VALUES  121

    d R = (-1)n | n = 1, 2, 3, . . .
                   n

    e (2, 3]  Q
     f The empty set 

                                                                                                    
Problem 9.5.3 Let S  R and let T = {-x| x  S}.

    a Prove that b is an upper bound of S if and only if -b is a lower bound
       of T .

    b Prove that b = sup S if and only if -b = inf T .

                                                                                                    
Chapter 10

Back to Power Series

10.1 Uniform Convergence

We have developed precise analytic definitions of the convergence of a sequence
and continuity of a function and we have used these to prove the EVT and
IVT for a continuous function. We will now draw our attention back to the
question that originally motivated these definitions, "Why are Taylor series well
behaved, but Fourier series are not necessarily?" More precisely, we mentioned
that whenever a power series converges then whatever it converged to was
continuous. Moreover, if we differentiate or integrate these series term by term
then the resulting series will converge to the derivative or integral of the original
series. This was not always the case for Fourier series. For example consider
the function

          4   (-1)k cos ((2k + 1)x)
f (x) =           2k + 1

             k=0

= 4 cos(x) - 1 cos(3x) + 1 (5x) - . . . .
                   3                          5

We have seen that the graph of f is given by

If we consider the following sequence of functions

f1(x) = 4 cos (x)
          

f2(x) = 4 cos (x) - 1 cos (3x)
                      3

                      122
CHAPTER 10. BACK TO POWER SERIES                         123

          4   cos (x) - 1 cos (3x) + 1 cos (5x)
f3(x) =                     3                   5

          ..
          .

we see the sequence of continuous functions (fn) converges to the non-continuous
function f for each real number x. This didn't happen with Taylor series. The
partial sums for a Taylor series were polynomials and hence continuous but
what they converged to was continuous as well.

    The difficulty is quite delicate and it took mathematicians a while to deter-
mine the problem. There are two very subtly different ways that a sequence of
functions can converge: pointwise or uniformly. This distinction was touched
upon by Niels Henrik Abel (1802-1829) in 1826 while studying the domain
of convergence of a power series. However, the necessary formal definitions
were not made explicit until Weierstrass did it in his 1841 paper Zur Theorie
der Potenzreihen (On the Theory of Power Series). This was published in his
collected works in 1894.

    It will be instructive to take a look at an argument that doesn't quite
work before looking at the formal definitions we will need. In 1821 Augustin
Cauchy "proved" that the infinite sum of continuous functions is continuous.
Of course, it is obvious (to us) that this is not true because we've seen several
counterexamples. But Cauchy, who was a first rate mathematician was so
sure of the correctness of his argument that he included it in his textbook on
analysis, Cours d'analyse (1821).

Problem 10.1.1 Find the flaw in the following "proof" that f is also contin-
uous at a.

    Suppose f1, f2, f3, f4 . . . are all continuous at a and that n=1  fn = f .
Let  > 0. Since fn is continuous at a, we can choose n > 0 such that
if |x - a| < n, then |fn(x) - fn(a)| < 2n  . Let  = inf(1, 2, 3, . . .). If
|x - a| <  then

                                        

|f (x) - f (a)| = fn(x) - fn(a)

                            n=1         n=1

                            

                            = (fn(x) - fn(a))

                            n=1

                            

                             |fn(x) - fn(a)|

                            n=1

                                
                             2n

                                n=1

                                  1
                              2n

                                   n=1

                            = .

Thus f is continuous at a.                               

Definition 10.1.2 Let S be a subset of the real number system and let

(fn) = (f1, f2, f3, . . .) be a sequence of functions defined on S. Let f be a

function defined on S as well. We say that (fn) converges to f pointwise

on S provided that for all x  S, the sequence of real numbers (fn(x)) converges

                                        ptwise
to the number f (x). In this case we write fn - f on S.  
CHAPTER 10. BACK TO POWER SERIES                                                124

                                                       ptwise

    Symbolically, we have fn - f on S   x  S,   > 0,  N such that
(n > N  |fn(x) - f (x)| < ).

    This is the type of convergence we have been observing to this point. By

contrast we have the following new definition.

Definition 10.1.3 Let S be a subset of the real number system and let

(fn) = (f1, f2, f3, . . .) be a sequence of functions defined on S. Let f be a

function defined on S as well. We say that (fn) converges to f uniformly

on S provided   > 0,  N such that n > N  |fn(x) - f (x)| <  ,  x  S.

                    unif
In this case we write fn - f on S.                                              

The difference between these two definitions is subtle. In pointwise con-

vergence, we are given a fixed x  S and an  > 0. Then the task is to find

an N that works for that particular x and . In uniform convergence, one is

given  > 0 and must find a single N that works for that particular  but also

simultaneously (uniformly) for all x  S. Clearly uniform convergence implies

pointwise convergence as an N which works uniformly for all x, works for each

individual x also. However the reverse is not true. This will become evident,

but first consider the following example.

Problem 10.1.4 Let 0 < b < 1 and consider the sequence of functions (fn)

                    n                              unif
defined on [0, b] by fn(x) = x . Use the definition to show that fn - 0 on

[0, b].

Hint.

|xn - 0| = xn  bn.                                                              

Uniform convergence is not only dependent on the sequence of functions

but also on the set S. For example, the sequence (fn(x)) = (xn)n=0  of Prob-
lem 10.1.4 does not converge uniformly on [0, 1]. We could use the negation

of the definition to prove this, but instead, it will be a consequence of the

following theorem.

Theorem 10.1.5 Consider a sequence of functions (fn) which are all contin-

                                                                 unif

uous on an interval I. Suppose fn - f on I. Then f must be continuous on
I.
Sketch of Proof. Let a  I and let  > 0. The idea is to use uniform convergence
to replace f with one of the known continuous functions fn. Specifically, by
uncancelling, we can write

         |f (x) - f (a)| = |f (x) - fn(x) + fn(x) - fn(a) + fn(a) - f (a)|
                           |f (x) - fn(x)| + |fn(x) - fn(a)| + |fn(a) - f (a)|

If we choose n large enough, then we can make the first and last terms as small

as we wish, noting that the uniform convergence makes the first term uniformly

small for all x. Once we have a specific n, then we can use the continuity of

fn to find a  > 0 such that the middle term is small whenever x is within 

of a.                                                                           

Problem 10.1.6 Provide a formal proof of Theorem 10.1.5 based on the above

ideas.                                                                          

Problem 10.1.7 Consider the sequence of functions (fn) defined on [0, 1] by
fn(x) = xn. Show that the sequence converges to the function

                                0   if x  [0, 1)
                    f (x) =         if x = 1

                                1

pointwise on [0, 1], but not uniformly on [0, 1].                               
CHAPTER 10. BACK TO POWER SERIES                                              125

Notice that for the Fourier series at the beginning of this chapter,

f (x) = 4 cos  x - 1 cos (3x) + 1 cos (5x) - 1 cos (7x) +   
                2              3      5               7

the convergence cannot be uniform on (-, ), as the function f is not con-
tinuous. This never happens with a power series, since they converge to con-
tinuous functions whenever they converge. We will also see that uniform con-
vergence is what allows us to integrate and differentiate a power series term
by term.

10.2 Uniform Convergence: Integrals and Deriv-
       atives

We saw in the previous section that if (fn) is a sequence of continuous functions
which converges uniformly to f on an interval, then f must be continuous on
the interval as well. This was not necessarily true if the convergence was only
pointwise, as we saw a sequence of continuous functions defined on (-, )
converging pointwise to a Fourier series that was not continuous on the real
line. Uniform convergence guarantees some other nice properties as well.

Theorem 10.2.1                                                                                 unif
Then
                Suppose fn and f are integrable and fn - f on [a, b].

                               b              b

                   lim            fn(x) dx =       f (x) dx.
                n x=a
                                              x=a

Problem 10.2.2 Prove Theorem 10.2.1.

Hint.

For  > 0, we need to make |fn(x) - f (x)| < b-a  , for all x  [a, b].         

Notice that this theorem is not true if the convergence is only pointwise, as

illustrated by the following.

Problem 10.2.3 Consider the sequence of functions (fn) given by

                                n  if x  0, n1 .
                   fn(x) =         otherwise

                                0

                ptwise                             1               1

a Show that fn - 0 on [0, 1], but lim                 fn(x) dx =       0 dx.
                                   n x=0
                                                                  x=0

b Can the convergence be uniform? Explain.

                                                                                                    
    Applying this result to power series we have the following.

Comment. Notice that we must explicitly assume uniform convergence. This
is because we have not yet proved that power series actually do converge uni-
formly.

Corollary 10.2.4 If n=0  anxn converges uniformly to f on an interval con-
taining 0 and x then t=0 x f (t) dt = n=0 n+1  an xn+1 .

Problem 10.2.5 Prove Corollary 10.2.4.
Hint.
CHAPTER 10. BACK TO POWER SERIES                                                               126

Remember that

                                                       N

                                      fn(x) = lim fn(x).
                                                N 
                                 n=0                   n=0

                                                                                                    
    Surprisingly, the issue of term-by-term differentiation depends not on the

uniform convergence of (fn) , but on the uniform convergence of (fn ). More
precisely, we have the following result.

Theorem       10.2.6  Suppose    for  every  n      N  fn  is  differentiable,  f    is  continuous,

                                                                                  n

ptwise               unif                                                                    
fn - f, and fn - g on an interval, I. Then f is differentiable and f = g

on I.

Problem 10.2.7 Prove Theorem 10.2.6.

Hint.
    Let a be an arbitrary fixed point in I and let x  I. By the Fundamental

Theorem of Calculus, we have

                                  x

                                     fn (t) dt = fn(x) - fn(a).

                                 t=a

Take the limit of both sides and differentiate with respect to x.                              

As before, applying this to power series gives the following result.

Corollary 10.2.8 If n=0  anxn converges pointwise to f on an interval con-
taining 0 and x and n=1  annxn-1 converges uniformly on an interval con-
taining 0 and x, then f (x) =  n=1 annxn-1.

Problem 10.2.9 Prove Corollary 10.2.8.                                                         

The above results say that a power series can be differentiated and inte-

grated term-by-term as long as the convergence is uniform. Fortunately it is,

in general, true that when a power series converges the convergence of it and

its integrated and differentiated series is also uniform (almost).

However we do not yet have all of the tools necessary to see this. To build

these tools requires that we return briefly to our study, begun in Chapter 6, of

the convergence of sequences.

10.2.1 Cauchy Sequences

Knowing that a sequence or a series converges and knowing what it converges

to are typically two different matters. For example, we know that n=0 n!  1 and
         1    both  converge.    The  first  converges     to      which  has   meaning  in  other
n=0    n! n!                                                   e,

contexts. We don't know what the second one converges to, other than to say it

converges to          n! n! 1 .  In  fact,  that  question     might  not  have  much    meaning
              n=0
without some other context in which                 1          arises naturally.         Be that as
                                                    n=0 n! n!
it may, we need to look at the convergence of a series (or a sequence for that

matter) without necessarily knowing what it might converge to. We make the

following definition.

Definition 10.2.10 Let (sn) be a sequence of real numbers. We say that

(sn)is a Cauchy sequence if for any  > 0, there exists a real number N such

that if m, n > N, then |sm - sn| < .                                                           

Notice that this definition says that the terms in a Cauchy sequence get

arbitrarily close to each other and that there is no reference to getting close to

any particular fixed real number. Furthermore, you have already seen lots of

examples of Cauchy sequences as illustrated by the following result.
CHAPTER 10. BACK TO POWER SERIES                            127

Theorem 10.2.11 Suppose (sn) is a sequence of real numbers which converges
to s. Then (sn) is a Cauchy sequence.

    Intuitively, this result makes sense. If the terms in a sequence are getting
arbitrarily close to s, then they should be getting arbitrarily close to each other.
This is the basis of the proof.

Comment. But the converse isn't nearly as clear. In fact, it isn't true in the
rational numbers.

Problem 10.2.12 Prove Theorem 10.2.11.

Hint.

|sm - sn| = |sm - s + s - sn|  |sm - s|+|s - sn|.           

So any convergent sequence is automatically Cauchy. For the real num-

ber system, the converse is also true and, in fact, is equivalent to any of our

completeness axioms: the NIP, the Bolzano-Weierstrass Theorem, or the LUB

Property. Thus, this could have been taken as our completeness axiom and

we could have used it to prove the others. One of the most convenient ways

to prove this converse is to use the Bolzano-Weierstrass Theorem. To do that,

we must first show that a Cauchy sequence must be bounded. This result is

reminiscent of the fact that a convergent sequence is bounded (Lemma 6.2.7

of Chapter 6) and the proof is very similar.

Lemma 10.2.13 Suppose (sn) is a Cauchy sequence. Then there exists B > 0
such that |sn|  B for all n.

Problem 10.2.14 Prove Lemma 10.2.13.

Hint.

This is similar to problem 6.2.8 of Chapter 6. There exists N such

that if m, n > N then |sn - sm| < 1. Choose a fixed m > N and let

B = max |s1| , |s2| , . . . , sN , |sm| + 1 .               

Theorem 10.2.15 Cauchy sequences converge

Suppose (sn) is a Cauchy sequence of real numbers. There exists a real

number s such that limn sn = s.

Sketch of Proof. We know that (sn) is bounded, so by the Bolzano-Weierstrass

Theorem, it has a convergent subsequence (snk ) converging to some real number

s. We have |sn - s| = |sn - snk + snk - s|  |sn - snk |+|snk - s|. If we choose

n and nk large enough, we should be able to make each term arbitrarily small.

                                                            

Problem 10.2.16 Provide a formal proof of Theorem 10.2.15.  

    From Theorem 10.2.11 we see that every Cauchy sequence converges in R.
Moreover the proof of this fact depends on the Bolzano-Weierstrass Theorem

which, as we have seen, is equivalent to our completeness axiom, the Nested

Interval Property. What this means is that if there is a Cauchy sequence which

does not converge then the NIP is not true. A natural question to ask is if every

Cauchy sequence converges does the NIP follow? That is, is the convergence

of Cauchy sequences also equivalent to our completeness axiom? The following

theorem shows that the answer is yes.

Theorem 10.2.17 Suppose every Cauchy sequence converges. Then the
Nested Interval Property is true.

Problem 10.2.18 Prove Theorem 10.2.17.

Hint.
    If we start with two sequences (xn) and (yn), satisfying all of the conditions

of the NIP, you should be able to show that these are both Cauchy sequences.
CHAPTER 10. BACK TO POWER SERIES                                             128

                                                                                                    
    Problems 10.2.16 and Problem 10.2.18 tell us that the following are equiv-
alent: the Nested Interval Property, the Bolzano-Weierstrass Theorem, the
Least Upper Bound Property, and the convergence of Cauchy sequences. Thus
any one of these could have been taken as the completeness axiom of the real
number system and then used to prove the each of the others as a theorem
according to the following dependency graph:

    Since we can get from any node on the graph to any other, simply by
following the implications (indicated with arrows), any one of these statements
is logically equivalent to each of the others.

Problem 10.2.19 Since the convergence of Cauchy sequences can be taken

as the completeness axiom for the real number system, it does not hold for the

rational number system. Give an example of a Cauchy sequence of rational

numbers which does not converge to a rational number.                                                           

If we apply the above ideas to series we obtain the following important

result, which will provide the basis for our investigation of power series.

Theorem 10.2.20 Cauchy Criterion. The series     k=0  ak converges if and
only if   > 0,  N such that if m > n > N then |  k=n+1 m ak| < .

Problem 10.2.21 Prove the Cauchy criterion.                                                                     

At this point several of the tests for convergence that you probably learned

in calculus are easily proved. For example:

Problem 10.2.22 The nth Term Test
    Show that if n=1  an converges then lim an = 0. 

                                                                              n

                                                                             

Problem 10.2.23 The Strong Cauchy Criterion. Show that ak

                                                                                                           k=1
                              

converges if and only if lim         ak = 0.
       n
                              k=n+1

Hint.

The hardest part of this problem is recognizing that it is really about the

limit of a sequence as in Chapter 6.                                                                            

You may also recall the Comparison Test from studying series in calculus:

suppose 0  an  bn, if bn converges then an converges. This result

follows from the fact that the partial sums of an form an increasing sequence

which is bounded above by bn. (See Corollary 9.4.5 of Chapter 9.) The

Cauchy Criterion allows us to extend this to the case where the terms an could

be negative as well. This can be seen in the following theorem.

Theorem 10.2.24 Comparison Test                        an also converges.
    Suppose |an|  bn for all n. If bn converges then
CHAPTER 10. BACK TO POWER SERIES                                                   129

Problem 10.2.25 Prove Theorem 10.2.24.

Hint.
    Use the Cauchy criterion with the fact that k=n+1 m ak  k=n+1 m |ak|.
                                                                                                    
    The following definition is of marked importance in the study of series.

Definition 10.2.26 Absolute Convergence

    Given a series an, the series |an| is called the absolute series of an
and if |an| converges then we say that an converges absolutely. 

    The significance of this definition comes from the following result.

Corollary 10.2.27 If an converges absolutely, then an converges.

Problem 10.2.28 Show that Corollary 10.2.27 is a direct consequence of

Theorem 10.2.24.                                                                   

Problem 10.2.29 If n=0  |an| = s, then does it follow that s = |                   n=0  an|?
Justify your answer. What can be said?                                                     

    The converse of Corollary 10.2.27 is not true as evidenced by the series
   n=0 n+1  (-1)n . As we noted in Chapter 4, this series converges to ln 2. However,
its absolute series is the Harmonic Series which diverges. Any such series which

converges, but not absolutely, is said to converge conditionally. Recall also

that in Chapter 4, we showed that we could rearrange the terms of the series
   n=0 n+1  (-1)n to make it converge to any number we wished. We noted further

that all rearrangements of the series n=0 (n+1)2  (-1)n converged to the same value.

The difference between the two series is that the latter converges absolutely

whereas the former does not. Specifically, we have the following result.

Theorem 10.2.30 Suppose an converges absolutely and let s =                        n=0  an.
Then any rearrangement of an must converge to s.

Sketch of Proof. We will first show that this result is true in the case where an 

0. If bn represents a rearrangement of an, then notice that the sequence
of partial sums ( k=0 n bk)n=0  is an increasing sequence which is bounded by s.
By Corollary 9.4.5 of Chapter 9, this sequence must converge to some number

t and t  s. Furthermore an is also a rearrangement of bn. Thus the

result holds for this special case. (Why?) For the general case, notice that

an = 2 |an|+an - 2 |an|-an and that 2 |an|+an and 2 |an|-an are both convergent
series with nonnegative terms. By the special case 2 |bn|+bn = 2 |an|+an and

    2 |bn|-bn = 2 |an|-an . 

Problem 10.2.31 Fill in the details and provide a formal proof of Theo-

rem 10.2.30.                                                                       

10.3 Radius of Convergence of a Power Series

We've developed enough machinery to look at the convergence of power series.
The fundamental result is the following theorem due to Abel.

Theorem 10.3.1 Suppose         ancn  converges        for  some  nonzero     real  number
                        n=0
c. Then       anxn  converges  absolutely  for  al l     such  that          |c|.
         n=0                                          x              |x|  <

    To prove Theorem 10.3.1 first note that by Problem 10.2.22, lim ancn = 0.

                                                                                                                        n

Thus (ancn) is a bounded sequence. Let B be a bound: |ancn|  B. Then

                    |anxn| = ancn  x  B n x n .
                                     c                   c
CHAPTER 10. BACK TO POWER SERIES                                                      130

We can now use the comparison test.

Problem 10.3.2 Prove Theorem 10.3.1.                                                  

Corollary 10.3.3 Suppose n=0  ancn diverges for some real number c. Then
   n=0  anxn diverges for all x such that |x| > |c|.

Problem 10.3.4 Prove Corollary 10.3.3.                                                

As a result of Theorem 10.3.1 and Corollary 10.3.3, we have the following:

either       anxn  converges  absolutely  for  all  x  or  there  exists  some  nonnega-
        n=0
tive real number r such that       anxn   converges    absolutely  when               and
                              n=0                                         |x|   <  r

diverges when |x| > r. In the latter case, we call r the radius of convergence
of the power series n=0  anxn. In the former case, we say that the radius
of convergence of n=0  anxn is . Though we can say that n=0  anxn con-
verges absolutely when |x| < r, we cannot say that the convergence is uniform.

However, we can come close. We can show that the convergence is uniform for

|x|  b < r. To see this we will use the following result

Theorem 10.3.5 The Weierstrass-M Test
    Let (fn)n=1  be a sequence of functions defined on S  R and suppose that

(Mn)n=1  is a sequence of nonnegative real numbers such that

                   |fn(x)|  Mn, x  S, n = 1, 2, 3, . . . .

    If n=1  Mn converges then n=1  fn(x) converges uniformly on S to some
function (which we will denote by f (x)).

Sketch of Proof. Since the crucial feature of the theorem is the function f (x)

that our series converges to, our plan of attack is to first define f (x) and then

show that our series, n=1  fn(x), converges to it uniformly.
First observe that for any x  S, n=1  fn(x) converges by the Comparison
Test (in fact it converges absolutely) to some number we will denote by f (x).

This actually defines the function f (x) for all x  S. It follows that          n=1  fn(x)
converges pointwise to f (x).

Next, let  > 0 be given. Notice that since n=1  Mn converges, say to M ,
then there is a real number, N , such that if n > N , then

                                                       n

                        Mk =         Mk = M - Mk < .

             k=n+1            k=n+1                    k=1

You should be able to use this to show that if n > N , then

                                   n

                    f (x) - fk(x) < , x  S.

                                 k=1

Problem 10.3.6                                                                               
rem 10.3.5.        Use the ideas above to provide a formal proof of Theo-

Problem 10.3.7                                                                               

(a) Referring back to equation (II.1), show that the Fourier series

                     (-1)k sin ((2k + 1)x)
                         (2k + 1)2

                    k=0

converges uniformly on R.

(b) Does its differentiated series converge uniformly on R? Explain.
CHAPTER 10. BACK TO POWER SERIES                                                131

                                                                                                    

Problem 10.3.8 Observe that for all x  [-1, 1] |x|  1. Identify which of
the following series converges pointwise and which converges uniformly on the
interval [-1, 1]. In every case identify the limit function.

       

(a)          xn - xn-1

     n=1

             xn - xn-1
                  n
(b)

        n=1

        (xn-xn-1)
(c) n=1
             n2

                                                                                                
Using the Weierstrass-M test, we can prove the following result.

Theorem 10.3.9 Suppose n=0  anxn has radius of convergence r (where r
could be  as well). Let b be any nonnegative real number with b < r. Then

   n=0  anxn converges uniformly on [-b, b].

Problem 10.3.10 Prove Theorem 10.3.9.

Hint.

We know that n=0  |anbn| converges. This should be all set for the
Weierstrass-M test.
                                                                                     
To finish the story on differentiating and integrating power series, all we

need to do is show that the power series, its integrated series, and its differ-

entiated series all have the same radius of convergence. You might not realize

it, but we already know that the integrated series has a radius of convergence

at least as big as the radius of convergence of the original series. Specifically,

suppose f (x) = n=0  anxnhas a radius of convergence r and let |x| < r. We
know that n=0  anxn converges uniformly on an interval containing 0and x,

and so by Corollary 10.2.4, t=0 x f (t) dt =          n+1 an xn+1 . In other words,
                                                 n=0

the integrated series converges for any x with |x| < r. This says that the radius

of convergence of the integated series must be at least r.

To show that the radii of convergence are the same, all we need to show is

that the radius of convergence of the differentiated series is at least as big as

r as well. Indeed, since the differentiated series of the integrated series is the

original, then this would say that the original series and the integrated series

have the same radii of convergence. Putting the differentiated series into the

role of the original series, the original series is now the integrated series and so

these would have the same radii of convergence as well. With this in mind, we

want to show that if |x| < r, then n=0  annxn-1 converges. The strategy is
to mimic what we did in Theorem 10.3.1, where we essentially compared our

series with a converging geometric series. Only this time we need to start with

the differentiated geometric series.

Problem 10.3.11 Show that             nxn-1   converges     for  |x|  <  1.
                                 n=1

Hint.
We know that k=0 n xk = x-1 xn+1-1 . Differentiate both sides and take the
limit as n approaches infinity.                                                      

Theorem 10.3.12 Suppose               anxn  has  a  radius  of  convergence  r  and  let
                                 n=0
|x| < r. Then n=1  annxn-1 converges.
CHAPTER 10. BACK TO POWER SERIES                                                   132

Problem 10.3.13 Prove Theorem 10.3.12.

Hint.

Let b be a number with |x| < b < r and consider annxn-1 =

anbn  b1  n bx n-1 . You should be able to use the Comparison Test and

Problem 10.3.11.                                                                   

10.4 Boundary Issues and Abel's Theorem

Summarizing our results, we see that any power series anxn has a radius

of convergence r such that anxn converges absolutely when |x| < r and

diverges when |x| > r. Furthermore, the convergence is uniform on any closed

interval [-b, b]  (-r, r) which tells us that whatever the power series converges
to must be a continuous function on (-r, r). Lastly, if f (x) = n=0  anxn for
x  (-r, r), then f (x) = n=1  annxn-1 for x  (-r, r) and t=0 x f (t) dt =

   n=0  an n+1 xn+1 for x  (-r, r).
    Thus power series are very well behaved within their interval of convergence,

and our cavalier approach from Chapter 3 is justified, EXCEPT for one issue.

If you go back to Problem 3.2.10 of Chapter 3, you see that we used the

geometric series to obtain the series, arctan x =  n=0(-1)n 1 2n+1 x2n+1. We
                                                         n=0  (-1) 2n+1 n 1 . Unfortunately, our
substituted  x  =  1  into  this  to  obtain     =
                                              4
integration was only guaranteed on a closed subinterval of the interval (-1, 1)

where the convergence was uniform and we substituted in x = 1. We "danced

on the boundary" in other places as well, including when we said that

       1                                                 j=0 2 n-1 1 - j  (-1)n .
          =                                                  n!           2n + 1
       4              1 - x2 dx = 1 +
                x=0
                                                    n=1

    The fact is that for a power series anxn with radius of convergence r, we
know what happens when |x| < r and when |x| > r. But we've never talked
about what happens when |x| = r. That is because there is no systematic
approach to this boundary problem. For example, consider the three series

                      xn   , xn+1  ,                     xn+2 .
                                      n + 1 (n + 1)(n + 2)
                      n=0 n=0                    n=0

    They are all related in that we started with the geometric series and in-
tegrated twice, thus they all have radius of convergence equal to 1. Their
behavior on the boundary, i.e., when x = 1, is another story. The first series
diverges when x = 1, the third series converges when x = 1. The second
series converges when x = -1 and diverges when x = 1.

    Even with the unpredictability of a power series at the endpoints of its
interval of convergence, the Weierstrass-M test does give us some hope of
uniform convergence.

Problem 10.4.1 Suppose the power series anxn has radius of convergence r
and the series anrn converges absolutely. Then anxn converges uniformly
on [-r, r].

Hint.

For |x|  r, |anxn|  |anrn|.                                                        

Unfortunately, this result doesn't apply to the integrals we mentioned as the

convergence at the endpoints is not absolute. Nonetheless, the integrations we

performed in Chapter 3 are still legitimate. This is due to the following theorem

by Abel which extends uniform convergence to the endpoints of the interval of
CHAPTER 10. BACK TO POWER SERIES                                                133

convergence even if the convergence at an endpoint is only conditional. Abel
did not use the term uniform convergence, as it hadn't been defined yet, but
the ideas involved are his.

Theorem 10.4.2 Abel's Theorem
    Suppose the power series anxn has radius of convergence r and the series
   anrn converges. Then anxn converges uniformly on [0, r].
    The proof of this is not intuitive, but involves a clever technique known as

Abel's Partial Summation Formula.

Lemma 10.4.3 Let

                 a1, a2, . . . , an, b1, b2, . . . , bn

be real numbers and let Am = k=1 m ak. Then

                                                          n-1

       a1b1 + a2b2 +    + anbn = Aj (bj - bj+1 ) + Anbn.

                                                          j=1

Problem 10.4.4 Prove Lemma 10.4.3.                                                  

Hint.                                                                           ... 
    For j > 1, aj = Aj - Aj-1.                                                  Then

Lemma 10.4.5 Abel's Lemma
    Let a1, a2, . . . , an, b1, b2, . . . , bn be real numbers with b1  b2 

bn  0 and let Am = k=1 m ak. Suppose |Am|  B for all m.
| j=1 n aj bj |  B  b1.

Problem 10.4.6 Prove Lemma 10.4.5.                                              

Problem 10.4.7 Prove Theorem 10.4.2.

Hint.

Let  > 0. Since            anrn  converges  then  by   the  Cauchy  Criterion,  there
                 n=0
exists N such that if m > n > N then k=n+1 m akrk < 2 . Let 0  x  r. By
Lemma 10.4.5,

              m               m          xk                 x  n+1  .

                   akxk =         ak rk  r             2    r       2

       k=n+1               k=n+1

Thus for 0  x  r, n > N ,

                                         m                   
                                                           2 < .]
                       ak xk  = lim             ak xk
                                 m
               k=n+1                     k=n+1

Corollary 10.4.8 Suppose the power series                                               
r and the series an (-r)n converges. Then         anxn has radius of convergence

[-r, 0].                                            anxn converges uniformly on

Problem 10.4.9 Prove Corollary 10.4.8.                                                  

Hint.          an (-x)n.
    Consider
Chapter 11

Back to the Real Numbers

As we have seen, when they converge, power series are very well behaved and
Fourier (trigonometric) series are not necessarily. The fact that trigonometric
series were so interesting made them a lightning rod for mathematical study
in the late nineteenth century.

    For example, consider the question of uniqueness. We saw in Chapter 7
that if a function could be represented by a power series, then that series must
be the Taylor series. More precisely, if

                               f (n)(a)

f (x) = an(x - a)n, then an =  n!        .

n=0

    But what can be said about the uniqueness of a trigonometric series? If we
can represent a function f as a general trigonometric series

               

f (x) = (an cos nx + bn sin nx),

              n=0

then must this be the Fourier series with the coefficients as determined by
Fourier?

    For example, if n=0  (an cos nx + bn sin nx) converges to f uniformly
on the interval (0, 1), then because of the uniform convergence, Fourier's term-
by-term integration which we saw in Part II is perfectly legitimate and the
coefficients are, of necessity, the coefficients he computed. However we have
seen that the convergence of a Fourier series need not be uniform. This does
not mean that we cannot integrate term-by-term, but it does say that we can't
be sure that term-by-term integration of a Fourier series will yield the integral
of the associated function.

    This led to a generalization of the integral by Henri Lebesgue in 1905.
Lebesgue's profound work settled the issue of whether or not a bounded point-
wise converging trigonometric series is the Fourier series of a function, but we
will not go in this direction. We will instead focus on work that Georg Cantor
(1845-1918) did in the years just prior. Cantor's work was also profound and
had far reaching implications in modern mathematics. It also leads to some
very weird conclusions.

Comment. 'Weird' does not mean false. It simply means that some of Can-
tor's results can be hard to accept, even after you have seen the proof and
verified its validity.

     134
CHAPTER 11. BACK TO THE REAL NUMBERS                                                                   135

To begin, let's suppress the underlying function and suppose we have

                                                         

          (an cos nx + bn sin nx) = (an cos nx + bn sin nx).

    n=0                                                  n=0

We ask:      If these two series are equal must it be true that an                             =  a    and

                                                                                                    n
bn = bn? We can reformulate this uniqueness question as follows: Suppose

                 

                      ((an   -  a     )  cos  n    x  +  (bn  -  b     )  sin  n  x  )  =  0.
                                   n                                n

                 n=0

If  we  let  cn  =    an  -  a     and   dn      =    bn - bn,   then          the  question   becomes:  If

                               n
n=0  (cn cos nx + dn sin nx) = 0, then will cn = dn = 0? It certainly seems
reasonable to suppose so, but at this point we have enough experience with

infinite sums to know that we need to be very careful about relying on the

intuition we have honed on finite sums.

The answer to this seemingly basic question leads to some very profound

results. In particular, answering this question led the mathematician Cantor

to study the makeup of the real number system. This in turn opened the door

to the modern view of mathematics of the twentieth century. In particular,

Cantor proved the following result in 1871 ([6], p. 305).

Theorem 11.0.1 Cantor
    If the trigonometric series

                           

                               (cn cos nx + dn sin nx) = 0,

                          n=0

    "with the exception of certain values of x," then all of its coefficients vanish.
    In his attempts to nail down precisely which "certain values" could be
exceptional, Cantor was led to examine the nature of subsets of real numbers
and ultimately to give a precise definition of the concept of infinite sets and to
define an arithmetic of "infinite numbers."

Transfinite Numbers. Actually, he called them transfinite numbers be-
cause, by definition, numbers are finite.

    As a first step toward identifying those "certain values," Cantor proved the
following theorem, which we will state but not prove.

Theorem 11.0.2 Cantor, (1870)
    If the trigonometric series

                           

                               (cn cos nx + dn sin nx) = 0,

                          n=0

for all x  R then all of its coefficients vanish.
    He then extended this to the following:

Theorem 11.0.3 Cantor, (1871)
    If the trigonometric series

                           

                               (cn cos nx + dn sin nx) = 0,

                          n=0

for all but finitely many x  R then all of its coefficients vanish.
CHAPTER 11. BACK TO THE REAL NUMBERS                 136

Figure 11.0.4 Georg Cantor18

    Observe that this is not a trivial generalization. Although the exceptional
points are constrained to be finite in number, this number could still be ex-
traordinarily large. That is, even if the series given above differed from zero
on 1010100000 distinct points in the interval (0, 10-10100000 ) the coefficients still
vanish. This remains true even if at each of these 1010100000 points the series
converges to 1010100000 . This is truly remarkable when you think of it this way.

    At this point Cantor became more interested in these exceptional points
than in the Fourier series problem that he'd started with. The next task he
set for himself was to see just how general the set of exceptional points could
be. Following Cantor's lead we make the following definitions.

Definition 11.0.5 Let S  R and let a be a real number. We say that a is
a limit point (or an accumulation point) of S if there is a sequence (an)

with an  S - {a} which converges to a.               

Problem 11.0.6 Let S  R and let a be a real number. Prove that a is a
limit point of S if and only if for every  > 0 the intersection

                   (a - , a + )  S - {a} = .

                                                                                                
The following definition gets to the heart of the matter.

Definition 11.0.7 Let S  R. The set of all limit points of S is called the
derived set of S. The derived set is denoted S.
                                                     

Don't confuse the derived set of a set with the derivative of a function. They

are completely different objects despite the similarity of both the language and

notation. The only thing that they have in common is that they were somehow

"derived" from something else.

Problem 11.0.8 Determine the derived set, S, of each of the following sets.

a S = 1, 1, 1,...
           123

b S = 0, 1 , 1 , 1 , . . .
              123

c S = (0, 1]

  d S = [ 0, 1/2)  (1/2, 1 ]
18mathshistory.st-andrews.ac.uk/Biographies/Cantor/
CHAPTER 11. BACK TO THE REAL NUMBERS                   137

    e S=Q
     f S =R-Q
    g S=Z
    h Any finite set S.

                                                                                                    
Problem 11.0.9 Let S  R.

    a Prove that (S )   S .

    b Give an example where these two sets are equal.

    c Give an example where these two sets are not equal.

                                                                                                    
    The notion of the derived set forms the foundation of Cantor's exceptional
set of values. Specifically, let S again be a set of real numbers and consider
the following sequence of sets:

                                 S   (S)  (S)      .

    Cantor showed that if, at some point, one of these derived sets is empty,
then the uniqueness property still holds. Specifically, we have:

Theorem 11.0.10 Cantor, (1871)
    Let S be a subset of the real numbers with the property that one of its derived

sets is empty. Then if the trigonometric series n=0  (cn cos nx + dn sin nx)
is zero for all x  R - S, then all of the coefficients of the series vanish.

11.1 Infinite Sets

The following theorem follows directly from our previous work with the NIP
and will be very handy later. It basically says that a sequence of nested closed
intervals will still have a non-empty intersection even if their lengths do not
converge to zero as in the NIP.

Theorem 11.1.1 Let ([an, bn])n=1  be a sequence of nested intervals such that
 lim |bn - an| > 0. Then there is at least one c  R such that c  [an, bn] for

n

all n  N.
Proof. By Corollary 9.4.5 of Chapter 9, we know that a bounded increasing

sequence such as (an) converges, say to c. Since an  am  bn for m > n and

lim am = c, then for any fixed n, an  c  bn. This says c  [an, bn] for all
m
n  N.                                                  

Problem 11.1.2 Suppose lim |bn - an| > 0. Show that there are at least

                                                      n

two points, c and d, such that c  [an, bn] and d  [an, bn] for all n  N. 

Our next theorem says that in a certain, very technical sense there are

more real numbers than there are counting numbers [3]. This probably does

not seem terribly significant. After all, there are real numbers which are not

counting numbers. What will make this so startling is that the same cannot

be said about all sets which strictly contain the counting numbers. We will get

into the details of this after the theorem is proved.
CHAPTER 11. BACK TO THE REAL NUMBERS                                  138

Theorem 11.1.3 Cantor, (1874)
    Let S = (sn)n=1  be a sequence of real numbers. There is a real number c,

which is not in S.

Notation. To streamline things, we are abusing notation here as we are letting
S denote both the sequence (which is ordered) and the underlying (unordered)
set of entries in the sequence.
Proof. For the sake of obtaining a contradiction assume that the sequence S
contains every real number; that is, S = R. As usual we will build a sequence
of nested intervals ([xi, yi])i=1  .
Let x1 be the smaller of the first two distinct elements of S, let y1 be the larger
and take [x1, y1] to be the first interval.
Next we assume that [xn-1, yn-1] has been constructed and build [xn, yn] as
follows. Observe that there are infinitely many elements of S in (xn-1, yn-1)
since S = R. Let sm and sk be the first two distinct elements of S such that

                                     sm, sk  (xn-1, yn-1) .

Take xn to be the smaller and yn to be the larger of sm and sk. Then [xn, yn]
is the nth interval.

From the way we constructed them it is clear that

                   [x1, y1]  [x2, y2]  [x3, y3]  . . . .

Therefore by Theorem 11.1.1 there is a real number, say c, such that

                                  c  [xn, yn] for all n  N.
In fact, since x1 < x2 < x3 . . . < y3 < y2 < y1 it is clear that

                                xn < c < yn, n.                       (III.1)

We will show that c is the number we seek. That the inequalities in for-

mula (III.1) are strict will play a crucial role.

To see that c  S we suppose that c  S and derive a contradiction.

So, suppose that c = sp for some p  N. Then only {s1, s2, . . . , sp-1} appear
before sp in the sequence S. Since each xn is taken from S it follows that

only finitely many elements of the sequence (xn) appear before sp = c in the

sequence as well.

Let xl be the last element of (xn) which appears before c = sp in the sequence

and consider xl+1. The way it was constructed, xl+1 was one of the first two

distinct terms in the sequence S strictly between xl and yl, the other being yl+1.

Since xl+1 does not appear before c = sp in the sequence and xl < c < yl, it

follows that either c = xl+1 or c = yl+1. However, this gives us a contradiction

as we know from formula (III.1) that xl+1 < c < yl+1.

Thus c is not an element of S.                                        

So how does this theorem show that there are "more" real numbers than

counting numbers? Before we address that question we need to be very careful

about the meaning of the word 'more' when we're talking about infinite sets.

First let's consider two finite sets, say A = {, , , } and B = {a, b, c, d, e}.

How do we know that B is the bigger set? (It obviously is.) Clearly we can just

count the number of elements in both A and B. Since |A| = 4 and |B| = 5 and

4 < 5 B is clearly bigger. But we're looking for a way to determine the relative

size of two sets without counting them because we have no way of counting

the number of elements of an infinite set. Indeed, it isn't even clear what the

phrase "the number of elements" might mean when applied to the elements of
CHAPTER 11. BACK TO THE REAL NUMBERS                          139

an infinite set.
    When we count the number of elements in a finite set what we're really

doing is matching up the elements of the set with a set of consecutive positive
integers, starting at 1. Thus since

       1
       2
       3
       4

we see that |A| = 4. Moreover, the order of the match-up is unimportant.
Thus since

       2e
       3a
       5b
       4d
       1c

it is clear that the elements of B and the set {1, 2, 3, 4, 5} can be matched up
as well. And it doesn't matter what order either set is in. They both have 5
elements.

    Such a match-up is called a one-to-one correspondence. In general, if two
sets can be put in one-to-one correspondence then they are the same "size." Of
course the word "size" has lots of connotations that will begin to get in the
way when we talk about infinite sets, so instead we will say that the two sets
have the same cardinality. Speaking loosely, this just means that they are the
same size.

    More precisely, if a given set S can be put in one-to-one correspondence
with a finite set of consecutive integers, say {1, 2, 3, . . . , N }, then we say that
the cardinality of the set is N . But this just means that both sets have the
same cardinality. It is this notion of one-to-one correspondence, along with the
next two definitions, which will allow us to compare the sizes (cardinalities) of
infinite sets.

Definition 11.1.4 Any set which can be put into one-to-one correspondence

with N = {1, 2, 3, . . .} is called a countably infinite set. Any set which is
either finite or countably infinite is said to be countable.
                                                              

    Since N is an infinite set, we have no symbol to designate its cardinality
so we have to invent one. The symbol used by Cantor and adopted by math-

ematicians ever since is 0. Thus the cardinality of any countably infinite set

is 0.

Notation.  is the first letter of the Hebrew alphabet and is pronounced
"aleph." 0 is pronounced "aleph null."

    We have already given the following definition informally. We include it
formally here for later reference.

Definition 11.1.5 If two sets can be put into one-to-one correspondence then

they are said to have the same cardinality.                   

With these two definitions in place we can see that Theorem 11.1.3 is noth-

ing less than the statement that the real numbers are not countably infinite.

Since it is certainly not finite, then we say that the set of real numbers is

uncountable and therefore "bigger" than the natural numbers!
CHAPTER 11. BACK TO THE REAL NUMBERS                140

    To see this let us suppose first that each real number appears in the sequence

(sn) n=1 exactly once. In that case the indexing of our sequence is really just a
one-to-one correspondence between the elements of the sequence and N :

                                        1  s1
                                        2  s2
                                        3  s3
                                        4  s4

                                            ..
                                            .

    If some real numbers are repeated in our sequence then all of the real
numbers are a subset of our sequence and will therefore also be countable (see
Problem 11.1.7, part a).

    In either case, every sequence is countable. But our theorem says that no
sequence in R includes all of R. Therefore R is uncountable.

    Most of the sets you have encountered so far in your life have been count-
able.

Problem 11.1.6 Show that each of the following sets is countable.

(a) {2, 3, 4, 5, . . .} = {n}n=2 

(b) {0, 1, 2, 3, . . .} = {n}n=0 

(c)  1, 4, 9, 16, . . . , n2, . . .  =  n2 

                                               n=1

(d) The set of prime numbers

(e) Z

                                                                                                    
    In fact, if we start with a countable set it is rather difficult to use it to
build anything but another countable set.

Problem 11.1.7 Let {A1, A2, A3, . . .} be a collection of countable sets. Show
that each of the following sets is also countable:

(a) Any subset of A1.

(b) A1  A2

 (c) A1  A2  A3

            n

 (d) Ai

          i=1

           

 (e) Ai

          i=1

                                                                                                    
    It seems that no matter what we do the only example of an uncountably
infinite set is R. But wait! Remember the rational numbers? They were similar
to the real numbers in many ways. Perhaps they are uncountably infinite too?
    Alas, no. The rational numbers turn out to be countable too.

Theorem 11.1.8 Show that Q is countable.
Sketch of Proof. First explain how you know that all of the non-negative
CHAPTER 11. BACK TO THE REAL NUMBERS                                                     141

rational numbers are in this list:

                                0, 0, 1, 0, 1, 2, 0, 1, 2, 3,렁.
                                1213214321

However there is clearly some duplication. To handle this, apply part (a) of

Problem 11.1.7. Does this complete the proof or is there more to do?                     

Problem 11.1.9 Prove Theorem 11.1.8.                                                     

     The following corollary says that the cardinality of the real numbers is much

larger than the cardinality of the rational numbers, despite the fact that both

are infinite.

     That is, as a subset of the reals, the rationals can be contained in a sequence

of intervals, the sum of whose lengths can be arbitrarily small. In a sense this

says that a countably infinite set is so small (on the transfinite scale) that it is

"almost" finite.

    Usually we express this idea with the statement, "Q is a set of measure zero
in R". The term "measure" has a precise meaning which we will not pursue.
The following corollary contains the essence of the idea.

Corollary 11.1.10 Let  > 0 be given. There is a collection of intervals in R,
In = [an, bn] such that

                                                                          

                                            Q  In

                                                                        n=1

and

                                                           

                                            (bn - an) < .

                                                         n=1

Problem 11.1.11 Prove Corollary 11.1.10.

Hint.

     If we had only finitely many rationals to deal with this would be easy.

Let  {r1, r2, . . . , rk}  be  these  rational  numbers  and  take  an  =  rn -          and
                                                                                 2(k+1)

bn = rn + 2(k+1)  . Then for all n = 1, . . . , k rn  [an, bn] and

                           k               k     = k < .

                                bn - an =       k+1 k+1

                           n=1             n=1

    The difficulty is, how do we move from the finite to the infinite case?] 
    Notice how this idea hearkens back to the discussion of Leibniz's approach
to the Product Rule (equation (I.1)). He simply tossed aside the expression
dx dy because it was infinitely small compared to either x dy or y dx. Although
this isn't quite the same thing we are discussing here it is similar and it is clear
that Leibniz's insight and intuition were extremely acute. They were moving
him in the right direction, at least.
    All of our efforts to build an uncountable set from a countable one have
come to nothing. In fact many sets that at first "feel" like they should be
uncountable are in fact countable. This makes the uncountability of R all the
more remarkable.

Comment. The failure is in the methods we've used so far. It is possible to
build an uncountable set using just two symbols if we're clever enough, but
this would take us too far away from our main topic.

    However if we start with an uncountable set it is relatively easy to build
others from it.
CHAPTER 11. BACK TO THE REAL NUMBERS        142

Problem 11.1.12

 (a) Let (a, b) and (c, d) be two open intervals of real numbers. Show that
       these two sets have the same cardinality by constructing a one-to-one
       onto function between them.

       Hint.
       A linear function should do the trick.

 (b) Show that any open interval of real numbers has the same cardinality as
       R.
       Hint.
       Consider the interval (-/2, /2).

 (c) Show that (0, 1] and (0, 1) have the same cardinality.

       Hint.
       Note that {1, 1/2, 1/3, . . .} and {1/2, 1/3, . . .} have the same cardinality.

 (d) Show that [0, 1] and (0, 1) have the same cardinality.

                                                                                                    

11.2 Cantor's Theorem and Its Consequences

Figure 11.2.1 Richard Dedekind19

    Once Cantor showed that there were two types of infinity (countable and
uncountable), the following question was natural, "Do all uncountable sets
have the same cardinality?"

    Just like not all "non-dogs" are cats, there is, offhand, no reason to be-
lieve that all uncountable sets should be the same size. However constructing
uncountable sets of different sizes is not as easy as it sounds.

    For example, what about the line segment represented by the interval [0, 1]
and the square represented by the set [0, 1]  [0, 1] = {(x, y)|0  x, y  1}.
Certainly the two dimensional square must be a larger infinite set than the
one dimensional line segment. Remarkably, Cantor showed that these two sets
were the same cardinality. In his 1877 correspondence of this result to his
friend and fellow mathematician, Richard Dedekind, even Cantor remarked, "I
see it, but I don't believe it!"

  19mathshistory.st-andrews.ac.uk/Biographies/Dedekind/
CHAPTER 11. BACK TO THE REAL NUMBERS           143

    The following gives the original idea of Cantor's proof. Cantor devised
the following function f : [0, 1]  [0, 1]  [0, 1]. First, we represent the co-
ordinates of any point (x, y)  [0, 1]  [0, 1] by their decimal representations
x = 0.a1a2a3 . . . and y = 0.b1b2b3 . . .. Even terminating decimals can be writ-
ten this way as we could write 0.5 = 0.5000 . . .. We can then define f (x, y)
by

                  f ((0.a1a2a3 . . . , 0.b1b2b3 . . .)) = 0.a1b1a2b2a3b3 . . . .

    This relatively simple idea has some technical difficulties in it related to
the following result.

Problem 11.2.2 Consider the sequence (0.9, 0.99, 0.999, . . .). Determine that

this sequence converges and, in fact, it converges to 1. This suggests that

0.999 . . . = 1.                               

Similarly, we have 0.04999 . . . = 0.05000 . . ., etc. To make the decimal rep-

resentation of a real number in [0, 1] unique, we must make a consistent choice

of writing a terminating decimal as one that ends in an infinite string of zeros

or an infinite string of nines [with the one exception 0 = 0.000 . . . ]. No matter

which choice we make, we could never make this function onto. For example,

109/1100 = 0.09909090 . . . would have as its pre-image (0.0999 . . . , 0.9000 . . .)

which would be a mix of the two conventions.

Cantor was able to overcome this technicality to demonstrate a one to one

correspondence, but instead we will note that in either convention, the function

is one-to-one, so this says that the set [0, 1]  [0, 1] is the same cardinality as

some (uncountable) subset of R. The fact that this has the same cardinality
as R is something we will come back to. But first we'll try construct an
uncountable set which does not have the same cardinality as R. To address
this issue, Cantor proved the following in 1891.

Theorem 11.2.3 Cantor's Theorem
    Let S be any set. Then there is no one-to-one correspondence between S

and P (S), the set of all subsets of S.

    Since S can be put into one-to-one correspondence with a subset of P (S)
(a  {a}), then this says that P (S) is at least as large as S. In the finite
case |P (S)| is strictly greater than |S| as the following problem shows. It also
demonstrates why P (S) is called the power set of S.

Problem 11.2.4 Prove: If |S| = n, then |P (S)| = 2n.

Hint.
    Let S = {a1, a2, . . . , an}. Consider the following correspondence between

the elements of P (S) and the set T of all n-tuples of yes (Y) or no (N):

                    {}  {N, N, N, . . . , N }
                  {a1}  {Y, N, N, . . . , N }
                  {a2}  {N, Y, N, . . . , N }

                       ..
                       .
                     S  {Y, Y, Y, . . . , Y }

How many elements are in T ?                   

Problem 11.2.5 Prove Cantor's Theorem 11.2.3.

Hint.

Assume for contradiction, that there is a one-to-one correspondence f :

S  P (S). Consider A = {x  S|x  f (x)}. Since f is onto, then there is

a  A such that A = f (a). Is a  A or is a  A?  
CHAPTER 11. BACK TO THE REAL NUMBERS                                  144

    Actually it turns out that R and P (N) have the same cardinality. This can
be seen in a roundabout way using some of the above ideas from Problem 11.2.4.
Specifically, let T be the set of all sequences of zeros or ones (you can use Y s
or N s, if you prefer). Then it is straightforward to see that T and P (N) have
the same cardinality.

    If we consider (0, 1], which has the same cardinality as R, then we can
see that this has the same cardinality as T as well. Specifically, if we think
of the numbers in binary, then every real number in [0, 1] can be written as

   j=1 2j  aj = 0.a1a2a3 . . . where aj  {0, 1}. We have to account for the fact that
binary representations such as 0.0111 . . . and 0.1000 . . . represent the same real
number (say that no representations will end in an infinite string of zeros),
then we can see that [0, 1] has the same cardinality as T - U , where U is the
set of all sequences ending in an infinite string of zeros. It turns out that U
itself is a countable set.

Problem 11.2.6 Let Un = {(a1, a2, a3, . . .) | aj  {0, 1} and an+1 = an+2 =

   = 0}. Show that for each n, Un is finite and use this to conclude that U is

countably infinite.                                                   

    The above problems say that R, T - U , T , and P (N ) all have the same
cardinality. The following two problems show that deleting a countable set

from an uncountable set does not change its cardinality.

Problem 11.2.7 Let S be an infinite set. Prove that S contains a countably

infinite subset.                                                      

Problem 11.2.8 Suppose X is an uncountable set and Y  X is countably

infinite.

(a) Prove that X and X - Y are both uncountable sets.

(b) Prove that X and X - Y have the same cardinality.

Hint.

Let Y = Y0. If X - Y0 is an infinite set, then by the previous problem it
contains a countably infinite set Y1. Likewise if X - (Y0  Y1) is infinite
it also contains an infinite set Y2. Again, if X - (Y0  Y1  Y2) is an
infinite set then it contains an infinite set Y3, etc. For n = 1, 2, 3, . . ., let
fn : Yn-1  Yn be a one-to-one correspondence and define f : X  X-Y
by

                     f (x) = fn(x),   if x  Yn, n = 0, 1, 2, . . . .
                                 x,  if x  X - (n=0Yn)

Show that f is one-to-one and onto.

                                                                                                    
    As we indicated before, Cantor's work on infinite sets had a profound im-
pact on mathematics in the beginning of the twentieth century. For example,
in examining the proof of Cantor's Theorem, the eminent logician Bertrand
Russell devised his famous paradox in 1901. Before this time, a set was naively
thought of as just a collection of objects. Through the work of Cantor and
others, sets were becoming a central object of study in mathematics as many
mathematical concepts were being reformulated in terms of sets. The idea was
that set theory was to be a unifying theme of mathematics. This paradox set
the mathematical world on its ear.
    Russell's Paradox: Consider the set of all sets which are not elements of
CHAPTER 11. BACK TO THE REAL NUMBERS  145

themselves. We call this set D and ask, "Is D  D?" Symbolically, this set is

                                        D = {S | S  S}.

    If D  D, then by definition, D  D. If D  D, then by definition, D  D.
    If you look back at the proof of Cantor's Theorem, this was basically the
idea that gave us the contradiction. To have such a contradiction occurring
at the most basic level of mathematics was scandalous. It forced a number
of mathematicians and logicians to carefully devise the axioms by which sets
could be constructed. To be honest, most mathematicians still approach set
theory from a naive point of view as the sets we typically deal with fall under
the category of what we would call "normal sets." In fact, such an approach is
officially called Naive Set Theory (as opposed to Axiomatic Set Theory). How-
ever, attempts to put set theory and logic on solid footing led to the modern
study of symbolic logic and ultimately the design of computer (machine) logic.
    Another place where Cantor's work had a profound influence in modern
logic comes from something we alluded to before. We showed before that the
unit square [0, 1]  [0, 1] had the same cardinality as an uncountable subset of
R. In fact, Cantor showed that the unit square had the same cardinality as R
itself and was moved to advance the following in 1878.

Conjecture 11.2.9 The Continuum Hypothesis. Every uncountable
subset of R has the same cardinality as R.

    Cantor was unable to prove or disprove this conjecture (along with every
other mathematician). In fact, proving or disproving the Continuum Hypothe-
sis, was one of Hilbert's famous 23 problems presented as a challenge to math-
ematicians at the International Congress of Mathematicians in 1900.

    Since R has the same cardinality as P (N), then the Continuum Hypothesis
was generalized to the:

Conjecture 11.2.10 The Generalized Continuum Hypothesis. Given
an infinite set S, there is no infinite set which has a cardinality strictly between
that of S and its power set P (S).

The Zermelo-Fraenkel Axioms. One of the formal axiomatic approaches
to set theory established by Ernst Zermelo in 1908 and revised by Abraham
Fraenkel in 1921.

    Efforts to prove or disprove this were in vain and with good reason. In
1940, the logician Kurt Gdel showed that the Continuum Hypothesis could
not be disproved from the Zermelo-Fraenkel Axioms of set theory. In 1963,
Paul Cohen showed that the Continuum Hypothesis could not be proved using
the Zermelo-Fraenkel Axioms. In other words, the Zermelo-Fraenkel Axioms
do not contain enough information to decide the truth of the hypothesis.

    We are willing to bet that at this point your head might be swimming a
bit with uncertainty. If so, then know that these are the same feelings that
the mathematical community experienced in the mid twentieth century. In the
past, mathematics was seen as a model of logical certainty. It is disconcerting
to find that there are statements that are "undecidable." In fact, Gdel proved
in 1931 that a consistent finite axiom system that contained the axioms of
arithmetic would always contain undecidable statements which could neither
be proved true nor false with those axioms. Mathematical knowledge would
always be incomplete.
CHAPTER 11. BACK TO THE REAL NUMBERS                146

Figure 11.2.11 Kurt Gdel20

    So by trying to put the foundations of calculus on solid ground, we have
come to a point where we can never obtain mathematical certainty. Does this
mean that we should throw up our hands and concede defeat? Should we
be paralyzed with fear of trying anything? Certainly not! As we mentioned
before, most mathematicians do well by taking a pragmatic approach: using
their mathematics to solve problems that they encounter. In fact, it is typically
the problems that motivate the mathematics. It is true that mathematicians
take chances that don't always pan out, but they still take these chances,
often with success. Even when the successes lead to more questions, as they
typically do, tackling those questions usually leads to a deeper understanding.
At the very least, our incomplete understanding means we will always have
more questions to answer, more problems to solve.

    What else could a mathematician ask for?

20mathshistory.st-andrews.ac.uk/Biographies/Godel/
Chapter 12

Epilogues

12.1 On the Nature of Numbers: A Dialogue
       (with Apologies to Galileo)

Interlocuters:Salviati, Sagredo, and Simplicio; Three Friends of Galileo Galilei
    Setting: Three friends meet in a garden for lunch in Renassaince Italy.

Prior to their meal they discuss the book How We Got From There to Here:
A Story of Real Analysis. How they obtained a copy is not clear.

    Salviati: My good sirs. I have read this very strange volume as I hope you
have?

    Sagredo: I have and I also found it very strange.
    Simplicio: Very strange indeed; at once silly and mystifying.
    Salviati: Silly? How so?
    Simplicio: These authors begin their tome with the question, "What is a
number?" This is an unusually silly question, don't you think? Numbers are
numbers. Everyone knows what they are.
    Sagredo: I thought so as well until I reached the last chapter. But now I
am not so certain. What about this quantity 0? If this counts the positive
integers, isn't it a number? If not, then how can it count anything? If so, then
what number is it? These questions plague me 'til I scarcely believe I know
anything anymore.
    Simplicio: Of course 0 is not a number! It is simply a new name for the
infinite, and infinity is not a number.
    Sagredo: But isn't 0 the cardinality of the set of natural numbers, N, in
just the same way that the cardinality of the set S = {Salviati, Sagredo, Simplicio}
is 3? If 3 is a number, then why isn't 0?
    Simplicio: Ah, my friend, like our authors you are simply playing with
words. You count the elements in the set S = {Salviati, Sagredo, Simplicio} ;
you see plainly that the number of elements it contains is 3 and then you
change your language. Rather than saying that the number of elements in S
is 3 you say that the cardinality is 3. But clearly "cardinality" and "number
of elements" mean the same thing.
    Similarly you use the symbol N to denote the set of positive integers. With
your new word and symbol you make the statement "the cardinality (number
of elements) of N is 0." This statement has the same grammatical form as the
statement "the number of elements (cardinality) of S is three." Since three is
a number you conclude that 0 is also a number.
    But this is simply nonsense dressed up to sound sensible. If we unwind
our notation and language, your statement is simply, "The number of positive

                                                 147
CHAPTER 12. EPILOGUES  148

integers is infinite." This is obviously nonsense because infinity is not a number.
    Even if we take infinity as an undefined term and try to define it by your

statement this is still nonsense since you are using the word "number" to
define a new "number" called infinity. This definition is circular. Thus it is no
definition at all. It is nonsense.

    Salviati: Your reasoning on this certainly seems sound.
    Simplicio: Thank you.
    Salviati: However, there are a couple of small points I would like to exam-
ine more closely if you will indulge me?
    Simplicio: Of course. What troubles you?
    Salviati: You've said that we cannot use the word "number" to define
numbers because this would be circular reasoning. I entirely agree, but I am
not sure this is what our authors are doing.
    Consider the set {1, 2, 3}. Do you agree that it contains three elements?
    Simplicio: Obviously.
    Sagredo: Ah! I see your point! That there are three elements does not
depend on what those elements are. Any set with three elements has three ele-
ments regardless of the nature of the elements. Thus saying that the set {1, 2, 3}
contains three elements does not define the word "number" in a circular manner
because it is irrelevant that the number 3 is one of the elements of the set. Thus
to say that three is the cardinality of the set {1, 2, 3} has the same meaning as
saying that there are three elements in the set {Salviati, Sagredo, Simplicio}.
In both cases the number "3" is the name that we give to the totality of the
elements of each set.
    Salviati: Precisely. In exactly the same way 0 is the symbol we use to
denote the totality of the set of positive integers.
    Thus 0 is a number in the same sense that '3' is a number, is it not?
    Simplicio: I see that we can say in a meaningful way that three is the
cardinality of any set with . . . well, . . . with three elements (it becomes very
difficult to talk about these things) but this is simply a tautology! It is a way
of saying that a set which has three elements has three elements!
    This means only that we have counted them and we had to stop at three.
In order to do this we must have numbers first. Which, of course, we do. As I
said, everyone knows what numbers are.
    Sagredo: I must confess, my friend, that I become more confused as we
speak. I am no longer certain that I really know what a number is. Since you
seem to have retained your certainty can you clear this up for me? Can you
tell me what a number is?
    Simplicio: Certainly. A number is what we have just been discussing. It
is what you have when you stop counting. For example, three is the totality
(to use your phrase) of the elements of the sets {Salviati, Sagredo, Simplicio}
or {1, 2, 3} because when I count the elements in either set I have to stop at
three. Nothing less, nothing more. Thus three is a number.
    Salviati: But this definition only confuses me! Surely you will allow that
fractions are numbers? What is counted when we end with, say 4/5 or 1/5?
    Simplicio: This is simplicity itself. 4/5 is the number we get when we
have divided something into 5 equal pieces and we have counted four of these
fifths. This is four-fifths. You see? Even the language we use naturally bends
itself to our purpose.
    Salviati: But what of one-fifth? In order to count one fifth we must first
divide something into fifths. To do this we must know what one-fifth is, musn't
we? We seem to be using the word "number" to define itself again. Have we
not come full circle and gotten nowhere?
CHAPTER 12. EPILOGUES  149

    Simplicio: I confess this had not occurred to me before. But your objec-
tion is easily answered. To count one-fifth we simply divide our "something"
into tenths. Then we count two of them. Since two-tenths is the same as
one-fifth the problem is solved. Do you see?

    Sagredo: I see your point but it will not suffice at all! It merely replaces
the question, "What is one-fifth?" with, "What is one-tenth?" Nor will it do
to say that one-tenth is merely two-twentieths. This simply shifts the question
back another level.

    Archimedes said, "Give me a place to stand and a lever long enough and I
will move the earth." But of course he never moved the earth because he had
nowhere to stand. We seem to find ourselves in Archimedes' predicament: We
have no place to stand.

    Simplicio: I confess I don't see a way to answer this right now. However
I'm sure an answer can be found if we only think hard enough. In the meantime
I cannot accept that 0 is a number. It is, as I said before, infinity and infinity
is not a number! We may as well believe in fairies and leprechauns if we call
infinity a number.

    Sagredo: But again we've come full circle. We cannot say definitively that
0 is or is not a number until we can state with confidence what a number
is. And even if wecould find solid ground on which to solve the problem of
fractions, what of 2? Or ? Certainly these are numbers but I see no way to
count to either of them.

    Simplicio: Alas! I am beset by demons! I am bewitched! I no longer
believe what I know to be true!

    Salviati: Perhaps things are not quite as bad as that. Let us consider
further. You said earlier that we all know what numbers are, and I agree. But
perhaps your statement needs to be more precisely formulated. Suppose we
say instead that we all know what numbers need to be? Or that we know what
we want numbers to be?

    Even if we cannot say with certainly what numbers are surely we can say
what we want and need for them to be. Do you agree?

    Sagredo: I do.
    Simplicio: And so do I.
    Salviati: Then let us invent numbers anew, as if we've never seen them
before, always keeping in mind those properties we need for numbers to have.
If we take this as a starting point then the question we need to address is,
"What do we need numbers to be?"
    Sagredo: This is obvious! We need to be able to add them and we need
to be able to multiply them together, and the result should also be a number.
    Simplicio: And subtract and divide too, of course.
    Sagredo: I am not so sure we actually need these. Could we not define
"subtract two from three" to be "add negative two to three" and thus dispense
with subtraction and division?
    Simplicio: I suppose we can but I see no advantage in doing so. Why not
simply have subtraction and division as we've always known them?
    Sagredo: The advantage is parsimony. Two arithmetic operations are
easier to keep track of than four. I suggest we go forward with only addition
and multiplication for now. If we find we need subtraction or division we can
consider them later.
    Simplicio: Agreed. And I now see another advantage. Obviously addition
and multiplication must not depend on order. That is, if x and y are numbers
then x + y must be equal to y + x and xy must be equal to yx. This is not true
for subtraction, for 3 - 2 does not equal 2 - 3. But if we define subtraction as
CHAPTER 12. EPILOGUES  150

you suggest then this symmetry is preserved:

                                     x + (-y) = (-y) + x.

    Sagredo: Excellent! Another property we will require of numbers occurs
to me now. When adding or multiplying more than two numbers it should not
matter where we begin. That is, if x, y and z are numbers it should be true
that

                                  (x + y) + z = x + (y + z)

and
                                     (x  y)  z = x  (y  z).

    Simplicio: Yes! We have it! Any objects which combine in these precise
ways can be called numbers.

    Salviati: Certainly these properties are necessary, but I don't think they
are yet sufficient to our purpose. For example, the number 1 is unique in
that it is the only number which, when multiplying another number leaves it
unchanged.

    For example: 1  3 = 3. Or, in general, if x is a number then 1  x = x.
    Sagredo: Yes. Indeed. It occurs to me that the number zero plays a similar
role for addition: 0 + x = x.
    Salviati: It does not seem to me that addition and multiplication, as we
have defined them, force 1 or 0 into existence so I believe we will have to
postulate their existence independently.
    Sagredo: Is this everything then? Is this all we require of numbers?
    Simplicio: I don't think we are quite done yet. How shall we get division?
    Sagredo: In the same way that we defined subtraction to be the addition
of a negative number, can we not define division to be multiplication by a
reciprocal? For example, 3 divided by 2 can be considered 3 multiplied by 1/2,
can it not?
    Salviati: I think it can. But observe that every number will need to have
a corresponding negative so that we can subtract any amount. And again
nothing we've discussed so far forces these negative numbers into existence so
we will have to postulate their existence separately.
    Simplicio: And in the same way every number will need a reciprocal so
that we can divide by any amount.
    Sagredo: Every number that is, except zero.
    Simplicio: Yes, this is true. Strange is it not, that of them all only this
one number needs no reciprocal? Shall we also postulate that zero has no
reciprocal?
    Salviati: I don't see why we should. Possibly 0 is the reciprocal of zero.
Or possibly not. But I see no need to concern ourselves with things we do not
need.
    Simplicio: Is this everything then? Have we discovered all that we need
for numbers to be?
    Salviati: I believe there is only one property missing. We have postulated
addition and we have postulated multiplication and we have described the
numbers zero and one which play similar roles for addition and multiplication
respectively. But we have not described how addition and multiplication work
together.
    That is, we need a rule of distribution: If x, y and z are all numbers then

                                   x  (y + z) = x  y + x  z.

    With this in place I believe we have everything we need.
CHAPTER 12. EPILOGUES                                                   151

    Simplicio: Indeed. We can also see from this that 0 cannot be a number
since, in the first place, it cannot be added to another number and in the
second, even if it could be added to a number the result is surely not also a
number.

    Salviati: My dear Simplicio, I fear you have missed the point entirely!
Our axioms do not declare what a number is, only how it behaves with respect
to addition and multiplication with other numbers. Thus it is a mistake to
presume that "numbers" are only those objects that we have always believed
them to be. In fact, it now occurs to me that "addition" and "multiplication"
also needn't be seen as the operations we have always believed them to be.

    For example suppose we have three objects, {a, b, c} and suppose that we
define "addition" and "multiplication" by the following tables:

+abc                                 abc
a abc                               aaaa
b bca                               babc
c cab                               cacb

    I submit that our set along with these definitions satisfy all of our axioms
and thus a, b and c qualify to be called "numbers."

    Simplicio: This cannot be! There is no zero, no one!
    Sagredo: But there is. Do you not see that a plays the role of zero -- if
you add it to any number you get that number back. Similarly b plays the role
of one.
    Simplicio: This is astonishing! If a, b and c can be numbers then I am less
sure than ever that I know what numbers are! Why, if we replace a, b, and c
with Simplicio Sagredo, and Salviati then we become numbers ourselves!
    Salviati: Perhaps we will have to be content with knowing how numbers
behave rather than knowing what they are.
    However I confess that I have a certain affection for the numbers I grew up
with. Let us call those the "real" numbers. Any other set of numbers, such as
our {a, b, c} above we will call a field of numbers, since they seem to provide
us with new ground to explore. Or perhaps just a number field?
    As we have been discussing this I have been writing down our axioms. They
are these. Numbers are any objects which satisfy all of the following properties:
    AXIOMS OF NUMBERS

      Axiom I:     They can be combined by two operations, denoted ""
Definition of      and "+".

  Operations

Axiom II:          If x, y and z are numbers then x + y is also a number.
  Closure          x  y is also a number.

        Axiom II:  x+y=y+x x톦=y톥
Commutativity

   Axiom III:      (x + y) + z = x + (y + z) (x  y)  z = x  (y  z)
Associativity

Axiom IV:          There is a number, denoted 0, such that for any number,
  Additive         x,
   Identity
                                                 x + 0 = x.

      Axiom V:     There is a number, denoted 1, such that for any number,
Multiplicative     x,

         Identity                                 1  x = x.
CHAPTER 12. EPILOGUES                                                 152

     Axiom VI:      Given any number, x, there is a number, denoted -x,
        Additive    with the property that
         Inverses
                                                x + (-x) = 0.
    Axiom VII:      Given any number, x = 0, there is a number, denoted
Multiplicative      x-1, with the property that

          Inverse                                x  x-1 = 1.
                    If x, y and z are numbers then
  Axiom VIII:
               The                       x  (y + z) = x  y + x  z.

   Distributive
       Property

    Sagredo: My friend, this is a thing of surpassing beauty! All seems clear
to me now. Numbers are any group of objects which satisfy our axioms. That
is, a number is anything that acts like a number.

    Salviati: Yes this seems to be true.
    Simplicio: But wait! We have not settled the question: Is 0 a number
or not?
    Salviati: If everything we have just done is valid then 0 could be a number.
And so could 1,2 . . . if we can find a way to define addition and multiplication
on the set {0, 1, 2 . . .} in a manner that agrees with our axioms.
    Sagredo: An arithmetic of infinities! This is a very strange idea. Can such
a thing be made sensible?
    Simplicio: Not, I think, before lunch. Shall we retire to our meal?

12.1.1 Additional Problems

Problem 12.1.1 Show that 0 = 1.

Hint.

Show that if x = 0, then 0  x = x.                                   

Problem 12.1.2 Consider the set of ordered pairs of integers: {(x, y)|x, y  Z},
and define addition and multiplication as follows:

 Addition:.
   (a, b) + (c, d) = (ad + bc, bd)

 Multiplication:.
   (a, b)  (c, d) = (ac, bd).

(a) If we add the convention that

                                    (ab, ad) = (b, d)

      show that this set with these operations forms a number field.
(b) Which number field is this?

Problem 12.1.3 Consider the set of ordered pairs of real                       
{(x, y)|x, y  R}, and define addition and multiplication as follows:  numbers,

    Addition:.
       (a, b) + (c, d) = (a + c, b + d)
CHAPTER 12. EPILOGUES  153

   Multiplication:.
     (a, b)  (c, d) = (ac - bd, ad + bc).

(a) Show that this set with these operations forms a number field.
(b) Which number field is this?

                                                                                                   

12.2 Building the Real Numbers

Contrary to the title of this section we will not be rigorously building the
real numbers here. Instead our goal is to show why such a build is logically
necessary, and to give a sense of some of the ways this has been accomplished
in the past. This may seem odd given our uniform emphasis on mathematical
rigor, especially in the third part of the text, but there are very good reasons
for this.

    One is simple practicality. The fact is that rigorously building the real
numbers and then showing that they have the required properties is extraordi-
narily detailed work, even for mathematics. If we want to keep this text to a
manageable size (we do), we simply don't have the room.

    The second reason is that there is, as far as we know, very little for you to
gain by it. When we are done we will have the real numbers. The same real
numbers you have been using all of your life. They have the same properties,
and quirks, they've always had. To be sure, they will not have lost any of their
charm; They will be the same delightful mix of the mundane and the bizarre,
and they are still well worth exploring and getting to know better. But nothing
we do in the course of building them up logically from simpler ideas will help
with that exploration.

    A reasonable question then, is, "Why bother?" If the process is overwhelm-
ingly, tediously detailed (it is) and gives us nothing new for our efforts, why
do it at all?

Andrew Wiles. The man who proved Fermat's Last Theorem21.

    Andrew Wiles22 has compared doing mathematics has been compared to
entering a dark room. At first you are lost. The layout of the room and
furniture are unknown so you fumble about for a bit and slowly get a sense
of your immediate environs, perhaps a vague notion of the organization of
the room as a whole. Eventually, after much, often tedious exploration, you
become quite comfortable in your room. But always there will be dark corners;
hidden areas you have not yet explored. Such a dark area may hide anything;
the latches to unopened doors you didn't know were there; a clamp whose
presence explains why you couldn't move that little desk in the corner; even
the light switch that would allow you to illuminate an area more clearly than
you would have imagined possible.

    But, and this is the point, there is no way to know what you will find there
until you walk into that dark corner and begin exploring. Perhaps nothing.
But perhaps something wonderful.

    This is what happened in the late nineteenth century. The real numbers had
been used since the Pythagoreans learned that 2 was irrational. But really,
most calculuations were (and still are) done with just the rational numbers.
Moreover, since Q forms a "set of measure zero," it is clear that most of the

  21mathshistory.st-andrews.ac.uk/HistTopics/Fermat's_last_theorem/
  22mathshistory.st-andrews.ac.uk/Biographies/Wiles/
CHAPTER 12. EPILOGUES  154

real numbers had gone completely unused. The set of real numbers was thus
one of those "dark corners" of mathematics. It had to be explored.

Measure Zero Sets. See Corollary 11.1.10 of Chapter 11.

     "But even if that is true," you might ask, ``I have no interest in the logical
foundations of the real numbers, especially if such knowledge won't tell me
anything I don't already know. Why do I need to know all of the details of
constructing R from Q?

     The answer to this is very simple: You don't.
     That's the other reason we're not covering all of the details of this material.
We will explain enough to light up, dimly perhaps, this little corner of math-
ematics. Later, should you need (or want) to come back to this and explore
further you will have a foundation to start with. Nothing more.
     Until the nineteenth century the geometry of Euclid, as given in his book
The Elements, was universally regarded as the touchstone of mathematical
perfection. This belief was so deeply embedded in Western culture that as
recently as 1923, Edna St. Vincent Millay opened one of the poems in her
book The Harp Weaver and Other Poems with the line "Euclid alone has
looked on beauty bare."
     Euclid begins his book by stating 5 simple axioms and proceeds, step by
logical step, to build up his geometry. Although far from actual perfection,
his methods are clean, precise and efficient -- he arrives at the Pythagorean
Theorem in only 47 steps (theorems) -- and even today Euclid's Elements still
sets a very high standard of mathematical exposition and parsimony.
     The goal of starting with what is clear and simple and proceeding logically,
rigorously, to what is complex is still a guiding principle of all mathematics for
a variety of reasons. In the late nineteenth century, this principle was brought
to bear on the real numbers. That is, some properties of the real numbers that
at first seem simple and intuitively clear turn out on closer examination, as we
have seen, to be rather counter-intuitive. This alone is not really a problem.
We can have counter-intuitive properties in our mathematics -- indeed, this is
a big part of what makes mathematics interesting -- as long as we arrive at
them logically, starting from simple assumptions the same way Euclid did.
     Having arrived at a view of the real numbers which is comparable to that of
our nineteenth century colleagues, it should now be clear that the real numbers
and their properties must be built up from simpler concepts as suggested by
our Italian friends in the previous section.
     In addition to those properties we have discovered so far, both Q and R
share another property which will be useful. We have used it throughout this
text but have not heretofore made it explicit. They are both linearly ordered.
We will now make this property explicit.

Definition 12.2.1 Linear Ordering. A number field is said to be linearly
ordered if there is a relation, denoted "<", on the elements of the field which
satisfies all of the following for all x, y, and z in the field.

    1. For all numbers x and y in the field, exactly one of the following holds:

         (a) x < y

         (b) x = y

         (c) y < x

    2. If x < y, then x + z < y + z for all z in the field.

    3. If x < y, and 0 < z, then x  z < y  z.
CHAPTER 12. EPILOGUES                                           155

   4. If x < y and y < z then x < z.

                                                                                                     
    Any number field with such a relation is called a linearly ordered number
field and as the following problem shows, not every number field is linearly
ordered.

Problem 12.2.2

 (a) Prove that the following must hold in any linearly ordered number field.

         (a) 0 < x if and only if -x < 0.

        (b) If x < y and z < 0 then y  z < x  z.

         (c) For all x = 0, 0 < x2.

        (d) 0 < 1.

 (b) Show that the set of complex numbers (C) is not a linearly ordered field.

                                                                                                     
    In a thorough, rigorous presentation we would now assume the existence
of the natural numbers (N), and their properties and use these to define the
integers, (Z). We would then use the integers to define the rational numbers,
(Q). We could then show that the rationals satisfy the field axioms worked out
in the previous section, and that they are linearly ordered.
    Then -- at last -- we would use Q to define the real numbers (R), show that
these also satisfy the field axioms and also have the other properties we expect:
Continuity, the Nested Interval Property, the Least Upper Bound Property,
the Bolzano-Weierstrass Theorem, the convergence of all Cauchy sequences,
and linear ordering.
    We would start with the natural numbers because they seem to be simple
enough that we can simply assume their properties. As Leopold Kronecker
(1823-1891) said: "God made the natural numbers, all else is the work of
man."
    Unfortunately this is rather a lot to fit into this epilogue so we will have to
abbreviate the process rather severely.
    We will assume the existence and properties of the rational numbers. Build-
ing Q from the integers is not especially hard and it is easy to show that they
satisfy the axioms worked out by Salviati, Sagredo and Simplicio in the previ-
ous section. But the level of detail required for rigor quickly becomes onerous.
    Even starting at this fairly advanced position in the chain of logic there is
still a considerable level of detail needed to complete the process. Therefore
our exposition will necessarily be incomplete.
    Rather than display, in full rigor, how the real numbers can be built up
from the rationals we will show, in fairly broad terms, three ways this has been
done in the past. We will give references later in case you'd like to follow up
and learn more.

12.2.1 The Decimal Expansion

This is by far the most straightforward method we will examine. Since we begin

with Q, we already have some numbers whose decimal expansion is infinite. For
example,  1  = 0.333 . . ..  We also know that if x  Q then expressing x as a
          3
decimal gives either a finite or a repeating infinite decimal.

    More simply, we can say that Q consists of the set of all decimal expressions
which eventually repeat. (If it eventually repeats zeros then it is what we've
CHAPTER 12. EPILOGUES                                              156

called a finite decimal.)
    We then define the real numbers to be the set of all infinite decimals, re-

peating or not.
    It may feel as if all we have to do is define addition and multiplication in the

obvious fashion and we are finished. This set with these definitions obviously
satisfy all of the field axioms worked out by our Italian friends in the previous
section. Moreover it seems clear that all of our equivalent completeness axioms
are satisfied.

    However, things are not quite as clear cut as they seem.
    The primary difficulty in this approach is that the decimal representation of
the real numbers is so familiar that everything we need to show seems obvious.
But stop and think for a moment. Is it really obvious how to define addition
and multiplication of infinite decimals? Consider the addition algorithm we
were all taught in grade school. That algorithm requires that we line up two
numbers at their decimal points:

                                         d1d2.d3d4
                                       + 12.34.

    We then begin adding in the rightmost column and proceed to the left. But
if our decimals are infinite we can't get started because there is no rightmost
column!

    A similar problem occurs with multiplication.
    So our first problem is to define addition and multiplication in R in a
manner that re-captures addition and multiplication in Q.
    This is not a trivial task.
    One way to proceed is to recognize that the decimal notation we've used
all of our lives is really shorthand for the sum of an infinite series. That is, if
x = 0.d1d2d3 . . . where 0  di  9 for all i  N then

                                       x=   di    i.

                                           i=1  10

Addition is now apparently easy to define: If x =                i=1 10i  di and y =

i=1 10i  i then          ei

                 x+y =                 10  i where ei = di + i.

                        i=1

    But there is a problem. Suppose for some j  N, ej = dj + j > 10. In that
case our sum does not satisfy the condition 0  ej  9 so it is not even clear
that the expression j=1 10j  ej represents a real number. That is, we may not
have the closure property of a number field. We will have to define some sort
of "carrying" operation to handle this.

Problem 12.2.3 Define addition on infinite decimals in a manner that is
closed.

Hint.

Find an appropriate "carry" operation for our definition.          

A similar difficulty arises when we try to define multiplication. Once we

have a notion of carrying in place, we could define multiplication as just the

multiplication of series. Specifically, we could define

                                       a1 a2             b1 b2
(0.a1a2a3 . . .)  (0.b1b2b3 . . .) =  + 2 +렁         + 2 +렁
                                       10 10             10 10
CHAPTER 12. EPILOGUES                                157

                                     = 102 a1b1 + 103 a1b2 + a2b1 + 104 a1b3 + a2b2 + a3b1 +    .

    We could then convert this to a "proper" decimal using our carrying oper-
ation.

    Again the devil is in the details to show that such algebraic operations
satisfy everything we want them to. Even then, we need to worry about linearly
ordering these numbers and our completeness axiom.

    Another way of looking at this is to think of an infinite decimal representa-
tion as a (Cauchy) sequence of finite decimal approximations. Since we know
how to add and multiply finite decimal representations, we can just add and
multiply the individual terms in the sequences. Of course, there is no reason
to restrict ourselves to only these specific types of Cauchy sequences, as we see
in our next approach.

12.2.2 Cauchy Sequences

As we've seen, Georg Cantor began his career studying Fourier series and

quickly moved on to more foundational matters in the theory of infinite sets.

But he did not lose his fascination with real analysis when he moved on.

Like many mathematicians of his time, he realized the need to build R from Q.
He and his friend and mentor Richard Dedekind (who's approach we will see

in the next section) both found different ways to build R from Q.
    Cantor started with Cauchy sequences in Q.
    That is, we consider the set of all Cauchy sequences of rational numbers.

We would like to define each such sequence to be a real number  . The goal
should be clear. If (sn)n=1 is asequence in Q which converges to 2 then we
will call (sn) the real number 2.

This probably seems a bit startling at first. There are a lot of numbers in

(sn) (countably infinitely many, to be precise) and we are proposing putting

all of them into a big bag, tying it up in a ribbon, and calling the whole thing
  2. It seems a very odd thing to propose, but recall from the discussion in

the previous section that we left the concept of "number" undefined. Thus if

we can take any set of objects and define addition and multiplication in such

a way that the field axioms are satisfied, then those objects are legitimately

numbers. To show that they are, in fact, the real numbers we will also need

the completeness property.

A bag full of rational numbers works as well as anything if we can define

addition and multiplication appropriately.

    Our immediate problem though is not addition or multiplication but unique-
ness. If we take one sequence (sn) which converges to 2 and defineit to be

  2, what will we do with all of the other sequences that converge to 2?

Also, we have to be careful not to refer to any real numbers, like the square

root of two for example, as we define the real numbers. This would be a circular

-- and thus useless -- definition. Obviously though, we can refer to rational

numbers, since these are the tools we'll be using.

    The solution is clear. We take all sequences of rational numbers that con-
verge to 2, throw them into our bag and call that 2. Our bag is getting

pretty full now.            

But we need to do this without using 2 because it is a real number. The

following two definitions satisfy all of our needs.

Definition 12.2.4 Let x = (sn)n=1  and y = (n)n=1  be Cauchy sequences
in Q. x and y are said to be equivalent if they satisfy the following property:
For every  > 0,   Q, there is a rational number N such that for all n > N ,
CHAPTER 12. EPILOGUES                                             158

n  N,                      |sn - n| < .

We will denote equivalence by writing, x  y.                      

Problem 12.2.5 Show that:

(a) x  x

(b) x  y  y  x

(c) x  y and y  z  x  z

                                                                  

Definition 12.2.6 Every set of all equivalent Cauchy sequences defines a real

number.                                                           

A very nice feature of Cantor's method is that it is very clear how addition

and multiplication should be defined.

Definition 12.2.7 If

          x = { (sn)k=1  | (sn)k=1  is Cauchy in Q }

and
                        y = { (n)k=1  | (n)k=1  is Cauchy in Q }

then we define the following:

    Addition:.

       x + y = { (tn)k=1  | tk = sk + k, (sn)  x, and (n)  y}

    Multiplication:.

       x  y = { (tn)k=1  | tk = skk, (sn)  x, and (n)  y}

                                                                                                     
    The notation used in Definition 12.2.6 can be difficult to read at first, but
basically it says that addition and multiplication are done component-wise.
However since x and y consist of all equivalent sequences we have to take
every possible choice of (sn)  x and (n)  y, form the sum (product) (sn +
n) n=1 ((snn) n=1) and then show that all such sums (products) are equivalent.
Otherwise addition (multiplication) is not well-defined: It would depend on
which sequence we choose to represent x and y.

Problem 12.2.8 Let x and y be real numbers (that is, let them be sets of
equivalent Cauchy sequences in Q). If (sn) and (tn) are in x and (n) and (n)
are in y then

                                (sn + n)n=1   (n + tn)n=1  .

                                                                                                     
Theorem 12.2.9 Let 0 be the set of Cauchy sequences in Q which are all
equivalent to the sequence (0, 0, 0, . . .). Then

                                            0 + x = x.
Proof. From Problem 12.2.8 it is clear that in forming 0 + x we can choose
any sequence in 0 to represent 0 and any sequence in x to represent x. (This
is because any other choice will yield a sequence equivalent to 0 + x.)
Thus we choose (0, 0, 0, . . .) to represent 0 and any element of x, say
CHAPTER 12. EPILOGUES                                         159

(x1, x2, . . .), to represent x. Then

(0, 0, 0, . . .) + (x1, x2, x3, . . .) = (x1, x2, x3, . . .)
                                      = x.

Since any other sequences taken from 0 and x respectively, will yield a sum
equivalent to x (see Problem 12.2.5) we conclude that

                                       0 + x = x.

Problem 12.2.10                                                                             
that               Identify the set of equivalent Cauchy sequences, 1, such

                                    1  x = x.

                                                                                                    
Problem 12.2.11 Let x, y, and z be real numbers (equivalent sets of Cauchy
sequences). Show that with addition and multiplication defined as above we
have:

(a) x + y = y + x

(b) (x + y) + z = x + (y + z)

(c) x  y = y  x

(d) (x  y)  z = x  (y  z)

(e) x  (y + z) = x  y + x  z

                                                                                                    
Comment. We will not address this issue here, but you should give some
thought to how this might be accomplished.

    Once the existence of additive and multiplicative inverses is established
the collection of all sets of equivalent Cauchy sequences, with addition and
multiplication defined as above satisfy all of the field axioms. It is clear that
they form a number field and thus deserve to be called numbers.

    However this does not necessarily show that they form R. We also need
to show that they are complete in the sense of Chapter 9. It is perhaps not
too surprising that when we build the real numbers using equivalent Cauchy
sequences the most natural completeness property we can show is that if a
sequence of real numbers is Cauchy then it converges.

    However we are not in a position to show that Cauchy sequences in R
converge. To do this we would first need to show that these sets of equivalence
classes of Cauchy sequences (real numbers) are linearly ordered.

    Unfortunately showing the linear ordering, while not especially hard, is
time consuming. So we will again invoke the prerogatives of the teacher and
brush all of the difficulties aside with the assertion that it is straightforward
to show that the real numbers as we have constructed them in this section are
linearly ordered and are complete. If you would like to see this construction in
full rigor we recommend the book, The Number System by H. A. Thurston [16].

Comment. Thurston first builds R as we've indicated in this section. Then
as a final remark he shows that the real numbers must be exactly the infinite
decimals we saw in the previous section.
CHAPTER 12. EPILOGUES  160

12.2.3 Dedekind Cuts

An advantage of building the reals via Cauchy sequences in the previous section
is that once we've identified equivalent sequences with real numbers it is very
clear how addition and multiplication should be defined.

    On the other hand, before we can even start to understand that construc-
tion, we need a fairly strong sense of what it means for a sequence to converge
and enough experience with sequences to be comfortable with the notion of
a Cauchy sequence. Thus a good deal of high level mathematics must be
mastered before we can even begin.

    The method of "Dedekind cuts" first developed by Richard Dedekind (though
he just called them "cuts") in his 1872 book, Continuity and the Irrational
Numbers shares the advantage of the Cauchy sequence method in that, once
the candidates for the real numbers have been identified, it is very clear how
addition and multiplication should be defined. It is also straightforward to
show that most of the field axioms are satisfied.

Comment. "Clear" does not mean "easy to do" as we will see.

    In addition, Dedekind's method also has the advantage that very little
mathematical knowledge is required to get started. This is intentional. In the
preface to the first edition of his book, Dedekind states:

       This memoir can be understood by anyone possessing what is usu-
       ally called common sense; no technical philosophic, or mathemati-
       cal, knowledge is in the least degree required. (quoted in [5])

While he may have overstated his case a bit, it is clear that his intention was
to argue from very simple first principles just as Euclid did.

    His starting point was the observation we made in Chapter 2: The rational
number line is full of holes. More precisely we can "cut" the rational line in
two distinct ways:

   1. We can pick a rational number, r. This choice divides all other rational
       numbers into two classes: Those greater than r and those less than r.

   2. We can pick one of the holes in the rational number line. In this case
       all of the rationals fall into two classes: Those greater than the hole and
       those less.

    But to speak of rational numbers as less than or greater than something
that is not there is utter nonsense. We'll need a better (that is, a rigorous)
definition.

    As before we will develop an overall sense of this construction rather than
a fully detailed presentation, as the latter would be far too long to include.

    Our presentation will closely follow that of Edmund Landau's in his classic
1951 text Foundations of Analysis [7]. We do this so that if you choose to
pursue this construction in more detail you will be able to follow Landau's
presentation more easily.

Definition 12.2.12 Dedekind Cut
    A set of positive rational numbers is called a cut if

Comment. Take special notice that we are not using the negative rational
numbers or zero to build our cuts. The reason for this will become clear shortly.

Property I  It contains a positive rational number but does not con-
            tain all positive rational numbers.
CHAPTER 12. EPILOGUES                                161

 Property II  Every positive rational number in the set is less than
Property III  every positive rational number not in the set.

              There is no element of the set which is greater than every
              other element of the set.

                                                                                                     
    Given their intended audiences, Dedekind and Landau shied away from us-
ing too much notation. However, we will include the following for those who
are more comfortable with the symbolism as it may help provide more perspec-
tive. Specifically the properties defining a Dedekind cut  can be written as
follows.

 Property I    =  and Q+ -  = .
Property II   If x   and y  Q+ - , then x < y. (Alternatively, if
              x   and y < x, then y  .)

Property III If x  , then  z   such that x < z.

    Properties I-III really say that Dedekind cuts are bounded open intervals
of rational numbers starting at 0. For example, (0, 3)  Q+ is a Dedekind cut
(which will eventually be the real number 3). Likewise, x|x2 < 2  Q+ is a

                                                                          
Dedekind cut (which will eventually be the real number 2). Notice that care
must be taken not to actually refer to irrational numbers in the properties as
the purpose is to construct them from rational numbers, but it might help to
ground you to anticipate what will happen.

    Take particular notice of the following three facts:

    1. Very little mathematical knowledge is required to understand this defin-
       ition. We need to know what a set is, we need to know what a rational
       number is, and we need to know that given two positive rational numbers
       either they are equal or one is greater.

2. The language Landau uses is very precise. This is necessary in order to
   avoid such nonsense as trying to compare something with nothing like
   we did a couple of paragraphs up.

3. We are only using the positive rational numbers for our construction. The
   reason for this will become clear shortly. As a practical matter for now,
   this means that the cuts we have just defined will (eventually) correspond
   to the positive real numbers.

Definition 12.2.13 Let  and  be cuts. Then we say that  is less than ,
and write

                                               <

if there is a rational number in  which is not in .  

Note that, in light of what we said prior to Definition 12.2.13 (which is taken

directly from Landau), we notice the following.

Theorem 12.2.14 Let  and  be cuts. Then  <  if and only if   .

Problem 12.2.15 Prove Theorem 12.2.14 and use this to conclude that if 
and  are cuts then exactly one of the following is true:

1.  = .

2.  < .

3.  < .

                                                     
CHAPTER 12. EPILOGUES  162

    We will need first to define addition and multiplication for our cuts and
eventually these will need to be extended to R (once the non-positive reals
have also been constructed). It will be necessary to show that the extended
definitions satisfy the field axioms. As you can see there is a lot to do.

    As we did with Cauchy sequences and with infinite decimals, we will stop
well short of the full construction. If you are interested in exploring the details
of Dedekind's construction, Landau's book [7] is very thorough and was written
with the explicit intention that it would be accessible to students. In his
"Preface for the Teacher" he says

        I hope that I have written this book, after a preparation stretching
        over decades, in such a way that a normal student can read it in
        two days.

This may be stretching things. Give yourself at least a week and make sure
you have nothing else to do that week.

    Addition and multiplication are defined in the obvious way.

Definition 12.2.16 Addition on cuts
    Let  and  be cuts. We will denote the set {x + y|x  , y  } by  + .
                                                                                                     

Definition 12.2.17 Multiplication on cuts
    Let  and  be cuts. We will denote the set {xy|x  , y  } by  or .
                                                                                                     
    If we are to have a hope that these objects will serve as our real numbers we

must have closure with respect to addition and multiplication. We will show
closure with respect to addition.

Theorem 12.2.18 Closure with Respect to Addition
    If  and  are cuts then  +  is a cut.

Proof. We need to show that the set  +  satisfies all three of the properties
of a cut.

Property I  Let x be any rational number in  and let x1 be a rational
            number not in . Then by Property II x < x1.
            Let y be any rational number in  and let y1 be a rational
            number not in . Then by Property II y < y1.
            Thus since x + y represents a generic element of  + 

            and x + y < x1 + y1, it follows that x1 + y1   + .
CHAPTER 12. EPILOGUES                                            163

Property II   We will show that the contrapositive of Property II is

              true: If x   +  and y < x then y   + .

              First, let x   + . Then there are x   and x  
                                                        y
              such that y  < x = x + x.  Therefore  x +x   < 1,  so that

                                                y
                                     x x + x < x
              and

                           x y < x.
                                 x + x

              Therefore x      y    and x         y         . There-
              fore         x +x               x +x

              y = x            y  + x        y         + .
                           x + x         x + x

Property III  Let z  +. We need to find w > z, w  +. Observe
              that for some x   and y  

                                  z = x + y.

              Since  is a cut, there is a rational number x1   such
              that x1 > x. Take w = x1 + y   + . Then

                           w = x1 + y > x + y = z.

                                                                                                     
Problem 12.2.19 Show that if  and  are cuts then    is also a cut. 

    At this point we have built our cuts and we have defined addition and
multiplication for cuts. However, as observed earlier the cuts we have will
(very soon) correspond only to the positive real numbers. This may appear to
be a problem but it really isn't because the non-positive real numbers can be
defined in terms of the positives, that is, in terms of our cuts. We quote from
Landau [7]:

       These cuts will henceforth be called the "positive numbers;" . . .

       We create a new number 0 (to be read "zero"), distinct from the
       positive numbers.

       We also create numbers which are distinct from the positive num-
       bers as well as distinct from zero, and which we will call negative
       numbers, in such a way that to each  (I.e. to each positive number)
       we assign a negative number denoted by - (- to be read "minus").
       In this, - and - will be considered as the same number (as equal)
       if and only if  and  are the same number.

       The totality consisting of all positive numbers, of 0, and of all
       negative numbers, will be called the real numbers.

Of course it is not nearly enough to simply postulate the existence of the
non-negative real numbers.

    All we have so far is a set of objects we're calling the real numbers. For
some of them (the positive reals -- that is, the cuts) we have defined addition
and multiplication. These definitions will eventually turn out to correspond to
the addition and multiplication we are familiar with.
CHAPTER 12. EPILOGUES                            164

    However we do not have either operation for our entire set of proposed real
numbers. Before we do this we need first to define the absolute value of a real
number. This is a concept you are very familiar with and you have probably
seen the following definition: Let   R. Then

                                      if   0,
                    || =              if  < 0.

                              -

    Unfortunately we cannot use this definition because we do not yet have a
linear ordering on R so the statement   0 is meaningless. Indeed, it will
be our definition of absolute value that orders the real numbers. We must be
careful.

    Notice that by definition a negative real number is denoted with the dash
('-') in front. That is  is positive while - is negative. Thus if A is any real
number then one of the following is true:

1. A =  for some   R (A is positive)
2. A = - for some   R (A is negative)
3. A = 0.

We define absolute value as follows:

Definition 12.2.20  Let A  R as above. Then

                       
                        if A = 

                    |A| = 0           if A = 0
                                      if A = -.

                                                                                                     
    With this definition in place it is possible to show that R is linearly ordered.
We will not do this explicitly. Instead we will simply assume that the symbols
"<" ">," and "=" have been defined and have all of the properties we have
learned to expect from them.
    We now extend our definitions of addition and multiplication from the
positive real numbers (cuts) to all of them. Curiously, multiplication is the
simpler of the two.

Definition 12.2.21 Multiplication

Let ,   R. Then


- || || if  > 0,  < 0 or  < 0,  > 0,

   =  || ||           if  < 0,  < 0,
                  0    if  = 0 or  = 0.

                                                                                                     
    Notice that the case where  and  are both positive was already handled
by Definition 12.2.17 because in that case they are both cuts.
    Next we define addition.

Definition 12.2.22 Addition
CHAPTER 12. EPILOGUES                                          165

Let ,   R. Then                      if  < 0,  < 0
                                     if  > 0,  < 0, || > ||
                        -(|| + ||)   if  > 0,  < 0, || = ||
                         || - ||     if  > 0,  < 0, || < || .
                         0           if  < 0,  > 0
                                     if  = 0
              +  = -(|| - ||)        if  = 0
                          + 
                         
                                   

                                                                                                     
Comment. Notice also that the fifth case refers to the addition as defined in
the second case.

    But wait! In the second and fourth cases of our definition we've actually
defined addition in terms of subtraction. But we haven't defined subtraction
yet! Oops!

    This is handled with the definition below, but it illuminates very clearly
the care that must be taken in these constructions. The real numbers are so
familiar to us that it is extraordinarily easy to make unjustified assumptions.

    Since the subtractions in the second and fourth cases above are done with
positive numbers we only need to give meaning to the subtraction of cuts.

Definition 12.2.23 If ,  and  are cuts then the expression

                       - =

is defined to mean

                        =  + .

                                                                                                     
    Of course, there is the detail of showing that there is such a cut . (We
warned you of the tediousness of all this.) Landau goes through the details
of showing that such a cut exists. We will present an alternative by defining
the cut  -  directly (assuming  < ). To motivate this definition, consider
something we are familiar with: 3 - 2 = 1. In terms of cuts, we want to
say that the open interval from 0 to 3 "minus" the open interval from 0 to 2
should give us the open interval from 0 to 1. Taking elements from (0, 3) and
subtracting elements from (0, 2) won't do it as we would have differences such
as 2.9 - .9 = 2 which is not in the cut (0, 1). A moment's thought tells us
that what we need to do is take all the elements from (0, 3) and subtract all
the elements from (2, ), restricting ourselves only to those which are positive
rational numbers. This prompts the following definition.

Definition 12.2.24 Let  and  be cuts with  < . Define  -  as follows:

                     -  = {x - y|x   and y  }  Q+.

                                                                                                     
    To show that, in fact,  + ( - ) = , the following technical lemma will
be helpful.

Lemma 12.2.25 Let  be a cut, y and z be positive rational numbers not in
 with y < z, and let  > 0 be any rational number. Then there exist positive
rational numbers r and s with r  , and s  , such that s < z, and s - r < .
CHAPTER 12. EPILOGUES                 166

Problem 12.2.26 Prove Lemma 12.2.25.

Hint.

    Since  is a cut there exists r1  . Let s1 = y  . We know that
r1 < s1 < z. Consider the midpoint s1+r1 2 . If this is in  then relabel it as r2
and relabel s1 as s2. If it is not in  then relabel it as s2 and relabel r1 as r2,

etc.                                  

Problem 12.2.27 Let  and  be cuts with  < . Prove that  +(-) = .

Hint.
    It is pretty straightforward to show that  + ( - )  . To show that

   + ( - ), we let x  . Since  < , we have y   with y  . We
can assume without loss of generality that x < y. (Why?) Choose z   with
y < z. By the Lemma 12.2.25, there exists positive rational numbers r and s
with r  , s  , s < z, and s - r < z - x. Show that x < r + (z - s). 

    We will end by saying that no matter how you construct the real number
system, there is really only one. More precisely we have the following theorem
which we state without proof.

Comment. In fact, not proving this result seems to be standard in real
analysis references. Most often it is simply stated as we do here.

Theorem 12.2.28 Any complete, linearly ordered field is isomorphic to R.
    Two linearly ordered number fields are said to be isomorphic if there is a

one-to-one, onto mapping between them (such a mapping is called a bijection)

which preserves addition, multiplication, and order. More precisely, if F1 and
F2 are both linearly ordered fields, x, y  F1 and  : F1  F2 is the mapping
then

1. (x + y) = (x) + (y)

2. (x  y) = (x)  (y)

3. x < y  (x) < (y).

    Remember that we warned you that these constructions were fraught with
technical details that are not necessarily illuminating. Nonetheless, at this
point, you have everything you need to show that the set of all real numbers as
defined above is linearly ordered and satisfies the Least Upper Bound property.

    But we will stop here in order, to paraphrase Descartes, to leave for you
the joy of further discovery.
Bibliography

[1] Robert E. Bradley and C. Edward Sandifer. Cauchy's Cours d'analyse:
       An Annotated Translation. Sources and Studies in the History of Mathe-
       matics and Physical Sciences. Springer, 2009.

[2] William Dunham. Journey Through Genius. Penguin Books, 1990.
[3] John Franks. Cantor's other proofs that R is uncountable. Mathematics

       Magazine, 83(4):283--289, October 2010.
[4] Judith Grabiner. The Origins of Cauchy's Rigorous Calculus. MIT Press,

       Cambridge MA, 1981.
[5] Stephen Hawking, editor. God Created the Integers: The Mathemati-

       cal Breakthroughs that Changed History. Running Press, Philadelphia,
       London, 2005.
[6] H. Jahnke, editor. A History of Analysis. AMS Publications, Providence
       RI, 2003.
[7] Edmund Landau. Foundations of Analysis. Chelsea Publishing Company,
       New York, NY, 1966. Translated by F. Steinhardt, Columbia University.
[8] Thomas Levenson. Newton and the Counterfeiter. Houghton Mifflin
       Harcourt, 2009.
[9] Reviel Netz and William Noel. The Archimedes Codex. Da Capo Press,
       2007.
[10] Isaac Newton. Sir Isaac Newton's Two Treatises of the Quadrature of
       Curves and Analysis by Equation of an Infinite Number of Terms, Ex-
       plained. Society for the Encouragement of Learning, 1745. Translated
       from Latin by John Stewart, A. M. Professor of Mathematicks in the
       Marishal College and University of Aberdeen.
[11] J. J. O'Connor and E. F. Robertson. The Brachistochrone Problem.
       http://www-gap.dcs.st-and.ac.uk", MacTutor History of Mathematics archive
[12] Abraham Robinson. Non-standard analysis. North-Holland Pub. Co.,
       1974.
[13] Lucio Russo, translated by Silvio Levy. The Forgotten Revolution: How
       Science Was Born in 300 BC and Why It Had to Be Reborn. Springer,
       1996.
[14] C. Edward Sandifer. The Early Mathematics of Leonard Euler. Spec-
       trum, 2007. ISBN 10: 0883855593, ISBN 13: 978-0883855591.
[15] Dirk Struik, editor. Source Book in Mathematics, 1200-1800. Harvard
       University Press, Cambridge, MA, 1969.

                                                 167
CHAPTER 12. EPILOGUES  168

[16] H. A. Thurston. The Number System. Blackie and Son Limited, London,
       Glassgow, 1956.
Index

Q                                                   first series expansion, 35
      Q is countable, 141                           second series expansion, 35
      creating irrationals from               sin x
            rationals, 17                           as a power series, 38
      has measure zero in R, 141                    orthogonality of, 54
      is countable, 140                             Taylor's series for, 44
      is it possible to have two
            rational numbers, a and              2 + 2 + 2 + ...
            b, such that ab is
            irrational, 17                     value of, 118
      rational numbers exist                    2
            between rational
            numbers, 11                             is irrational, 11

R                                              meaning of, 13
      addition of Cauchy sequences,             x
            158
      any complete, linearly ordered                is continuous at zero, 91
            field is isomorphic to, 166
      as Cauchy sequences                     b is an upper bound of S  R if
         identify the multiplicative                      and only if -b is a lower
            identity, 159
      irrational numbers, 17                           bound of -S, 121
      is uncountable
         Cantor's first proof, 138            ex
      ordering Dedekind cuts, 161
      real numbers exist between                    ea+b = eaeb, 76
            real numbers, 14
      the number 1 as a Cauchy                      definition of e, 31
            sequence, 159
                                                    Taylor's series for, 44
cos(nx)
      orthogonality of, 58                    g(c)  =  c-x  is  increasing  on  [x, 0],
                                                       1+c
cos x                                                  82
      Taylor's series for, 44
                                              xn

      divergence to, 71                             converges pointwise on [0, 1],
      negative infinity
         divergence to, 71                             124
      positive infinity
         divergence to, 71                    xn converges pointwise on [0, 1],

                                                       124

                                              Abel, Niels Henrik
                                                    Abel's Lemma, 133
                                                    Abel's Theorem, 133

                                              absolute value, 63
                                              Archimedean Property, 14, 120

                                                    and Q, 120

                                              Bernoulli Jacob, 24
                                              Bernoulli, Johann, 23

                                                    Bernoulli's challenge, 23
                                                    portrait of, 24
                                              Binomial Series, the

                                         169
INDEX                                                                             170

g(c)   =   c-x  is  increasing,  82            Cauchy's flawed proof that
           1+c                                       the limit of continuous
converges on the interval                            functions is continuous,
                                                     123
       [0, 1], 80
                                               definition of, 88
squaring the, 34                               Extreme Value Theorem

Bolzano, Bernhard, 87, 88, 115                      (EVT) and, 88
                                               formal definition of
portrait of, 88
                                                     discontinuity, 107
Bolzano-Weierstrass Theorem, 115               Heaviside's function is not

Bolzano-Weierstrass Theorem                          continuous at zero, 95
                                               implied by differentiability,
       (BWT), 116
                                                     104
implies that a continuous                      Intermediate Value Theorem

       functions on a closed set                     and, 88
                                               via limits, 94, 96
       is bounded, 116                   continuous functions
                                               continuous function on a
implies the NIP, 119
                                                     closed, bounded interval
Brachistochrone problem, the, 23,                    is bounded, 114
                                               on a closed set, and the
       28                                            Bolzano-Weierstrass
                                                     Theorem, 116
Bernoulli's solution, 24                       sum of continuous functions is
                                                     continuous, 96
Cantor, Georg, 134, 135, 137, 139,             the composition of continuous
            144, 145, 157                            functions is continuous,
                                                     98
      and the modern view of                   uniform convergence and, 124
            mathematics, 135                   uniform limit of continuous
                                                     functions is continuous,
      Cantor's Theorem, 143                          124
      first proof that R is              Continuum Hypothesis
                                               generalized, 145
            uncountable, 138                   original, 145
      fourth theorem on the              convergence
                                               of a sequence, 65
            uniqueness of Fourier                 convergence to zero drill,
            series, 137                              63, 64
      portrait of, 136                            implies Cauchy sequence,
      uniqueness of Fourier series                   127
         first theorem on, 135                 of a series
         second theorem, 135                      absolute, 129
         third theorem on, 135                    absolute convergence
      unit interval and unit square                  implies convergence, 129
            have equal cardinalty,                pointwise, 123
            142                                pointwise convergence, 125
Cardano, Girolomo, 5                           pointwise vs. uniform
cardinality                                          convergence, 125
      countable sets, 139                      the radius of convergence of a
      equal cardinality, 139                         power series, 130
Cauchy, Augustin, 81, 88, 99                   uniform convergence, 125
      Cauchy's flawed proof that         countable sets
            the limit of continuous            countable union of finite sets,
            functions is continuous,
            123
      portrait of, 83
common denominators, 11
continuity, 88
       x is continuous at zero, 91
      f (x) = mx + b is continuous
            everywhere, 90
      Bolzano-Weierstrass Theorem
            implies a continuous
            function on a closed set is
            bounded, 116
INDEX                                                                           171

            144                                 if a prime divides a product
      defintion of, 139                            of two numbers then it
                                                   divides one of the factors,
Dedekind, Richard, 142, 157,                       8
            160-162
                                                if a prime divides an
      Dedekind cuts, 160                           arbitrary product then it
         absolute value, 164                       divides one of the factors,
         addition of, 162, 164                     8
         as sets, 165
         closure of, 162               Fermat's Theorem, 104
         definition of, 160                  if f (a) is a maximum then
         multiplication of, 163, 164               f (a) = 0, 105
         multiplication of positive          if f (a) is a minimum then
            cuts, 162                              f (a) = 0, 105
         order properties, 161
         ordering of, 161              fields
         subtraction of, 165                 any complete, linearly ordered
                                                   field is isomorphic to R.,
      portrait of, 142                             166
derived sets, 137
differentiation                        Fourier Series, 57
                                             Cantor's first theorem on
      f (a) > 0 implies f is                       uniqueness, 135
            increasing nearby, 106           Cantor's second theorem on
                                                   uniqueness, 135
      f (a) < 0 implies f is                 Cantor's third theorem on
            decreasing nearby, 107                 uniqueness, 135
                                             computing the coefficients, 58
      definition of the derivative,          cosine series
            104                                 the Fourier cosine series of
                                                   f (x) = x - 12 , 58
      differentiability implies              divergent Fourier series
            continuity, 104                        example, 57
                                             sine series of an odd function,
      if f  < 0 on an interval then f              54
            is decreasing, 106
                                       Fourier, Jean Baptiste Joseph,
      of the pointwise limit of                    52-54, 134
            functions, 126
                                             portrait of, 52
      power rule with fractional
            exponents, 22              Greatest Lower Bound Property
                                                  (GLBP)
      term by term differentiation
            of power series, 126             definition of, 120
                                       Gdel, Kurt
divergence
      divergence to infinity implies         portrait of, 146
            divergence, 71
      of a sequence, 70                Halmos, Paul, 92
      of a series                            portrait of, 92
         nth term test, 128
                                       Heat Equation, the, 53
Euler, Leonhard, 112                         fundamental solutions of, 53
      portrait of, 36                        parameter k must be less
                                                   than zero, 53
Extreme Value Theorem (EVT),                 solving for (x), 53
            79, 118, 119
                                       Intermediate Value Theorem
      continuity and, 88                          (IVT), 79, 113
      Rolle's Theorem, and, 105
                                             a polynomial with odd degree
Fermat's Little Theorem, 8                         must have a root, 114
      problems leading to
         if p is prime then p divides        continuity and, 88
             kp , 8
INDEX                                                                            172

      the case f (a)  v  f (b), 114           endpoints, 110
      the case f (a)  v  f (b), 113           implies the existence of square

Lagrange's form of the remainder,                   roots of integers, 112
            77                                implies the LUBP, 118
                                              square roots of integers, and,
      x < a, 78
Lagrange, Joseph-Louis, 15, 21, 77,                 112
                                              weak form, 137
            79, 99                      Newton, Isaac, 19, 20, 24, 33, 99,
      portrait of, 42
Least Upper Bound Property                          105
                                              foundation of calculus, 15
           (LUBP), 117, 121                   portrait of, 21
      doesn't hold in Q, 120            number field
      identifying suprema and                 linearly ordered, 154

            infima, 120                 orthogonality
      implies the Nested Interval             of cos nx, 58
                                              of sin nx, 54
            Property (NIP), 119
Lebesgue, Henri, 134                    pointwise convergence, 124
Leibniz, Gottfried Wilhelm, 2, 19,            derivative and, 126

            27, 33, 45, 88, 99, 105,    polynomials
            141                               infinite, 28
      and infinitesimals, 14                  with odd degree must have a
      differentiation rules, 19                     root, 114
      first calculus publication, 18
      portrait of, 18                   power series
limit, 100                                    a power series diverges
      limn b( n1 ) = 1 if b > 0, 66                 outside it's radius of
      lim f (x) = f (a) implies f (x)               convergence, 130
                                              converge inside radius of
        xa                                          convergence, 129
                                              converge uniformly on their
            is continuous, 96                       interval of convergence,
      accumulation point, 136                       131
      accumulation points, 136                definition of, 28
      definition of non-existence, 64         drills, 38
      identifing the theorems used            for ax expanded about 0, 38
                                              of sin(x), expanded about a,
            in a limit, 69                          38
      of a constant sequence, 66              term by term derivative of,
      of a constant times a                         126, 131
                                              term by term integral of, 125
            sequence, 68                      the radius of convergence, 130
      of interval endpoints in the
                                        Quadratic Formula
            NIP, 110                          second proof, 6
      of ratios of polynomials, 73
      products of, 67                   Riemann, Bernhard, 46
      properties of, 102, 103           Rolle's Theorem, 105
      quotients of, 68                  Russell's Paradox, 144
      Squeeze Theorem for
                                        sequences
            Sequences, 69                     all subsequences of a
      termwise sums of, 67                          convergent sequence
      verify limit laws from calculus,              converge, 115

            103

Maclaurin series drills, 38
Mean Value Theorem, the, 105,

            106
measure zero, 141, 153

      Q has measure zero in R, 141

Nested Interval Property (NIP)
INDEX                                                                           173

      bounded and non-decreasing,               Cauchy Criterion, 128
            118                              Comparison Test, 128
                                             Geometric series
      Cauchy sequences, 126
         addition and multiplication            (0.9) converges to 1, 143
            of, 158                             alternating, 38
         addition of is well defined,           derivation of the series
            158
         Cauchy's remainder, 81                    representation of
         convergence of, 127                       ln(1 + x) from, 38
         convergence of is equivalent           differentiating, 36
            to the NIP, 127                     naive derivation, 29
         don't always converge in Q,         graph the square root series,
            128                                    35
         equivalent, 157                     Harmonic Series, 46, 129
         real numbers as Cauchy                 slow divergence of, 112
            sequences, 158                   rearrangements, 129
         zero as a Cauchy sequence,          Taylor's series, 75
            158                                 f (n) < B,  n  N 
                                                   Taylor series converges,
      constant multiples of, 68                    76
      constant sequences, 66                    Cauchy Remainder, 81
      convergence, 65, 127                      expansion of ex, sin x, and
      convergence of                               cos x, 44
                                                used to approximate ln 2, 46
         convergent sequences are            term by term integration of,
            bounded, 68                            38
                                       sets
      convergence to zero, 62                accumulation points, 136
      divergence of, 70                      countably infinite subsets, 144
      divergence to , 71                     derived sets, 136
      find a bounded sequence of       square roots exist, 110
                                       Squeeze Theorem
            rational numbers such            for functions, 103
            that no subsequence
            converges to a rational    Taylor's Formula, 40
            number, 119                      use to obtain the general
      subsequences, 114                            binomial series, 44
      termwise product of, 68
      termwise quotient of, 69         Taylor's Theorem, 44, 45
      termwise sums of, 67             Taylor, Brook
      the sequence of positive
            integers diverges to             portrait of, 41
            infinity, 71               Topologist's sine function
series
      nth term test, 128                     is continuous at zero, 93
      absolute convergence of          Triangle Inequalities, 66
         rearrangements, 129
         vs. the absolute value of a         for Integrals, 75
            series, 129                      Reverse Triangle Inequalitiy,
      Alternating Harmonic Series
         rearrangements of, 46                     67
      Binomial Series, the, 80               Triangle Inequality, 66
         Binomial Series is a Taylor
            series, 81                 uniform convergence, 124
      Cauchy Criterion, 128                  continuous functions and, 124
         Strong Cauchy criterion,            integration and, 125
            128
      Cauchy sequences                 Upper Bound, 117

                                       Weierstrass, Karl, 87, 100, 115
                                             portrait of, 87

                                       Weierstrass-M Test, 130
