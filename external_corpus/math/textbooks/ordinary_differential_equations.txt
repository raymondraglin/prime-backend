ORDINARY
DIFFERENTIAL
EQUATIONS

Stephen Wiggins
University of Bristol
      University of Bristol
Ordinary Differential Equations

                  Stephen Wiggins
This text is disseminated via the Open Education Resource (OER) LibreTexts Project
(https://LibreTexts.org) and like the thousands of other texts available within this powerful platform, it is
freely available for reading, printing, and "consuming."
The LibreTexts mission is to bring together students, faculty, and scholars in a collaborative effort to
provide and accessible, and comprehensive platform that empowers our community to develop, curate,
adapt, and adopt openly licensed resources and technologies; through these efforts we can reduce the
financial burden born from traditional educational resource costs, ensuring education is more accessible
for students and communities worldwide.
Most, but not all, pages in the library have licenses that may allow individuals to make changes, save,
and print this book. Carefully consult the applicable license(s) before pursuing such effects. Instructors
can adopt existing LibreTexts texts or Remix them to quickly build course-specific resources to meet the
needs of their students. Unlike traditional textbooks, LibreTexts' web based origins allow powerful
integration of advanced features and new technologies to support learning.

LibreTexts is the adaptable, user-friendly non-profit open education resource platform that educators
trust for creating, customizing, and sharing accessible, interactive textbooks, adaptive homework, and
ancillary materials. We collaborate with individuals and organizations to champion open education
initiatives, support institutional publishing programs, drive curriculum development projects, and more.
The LibreTexts libraries are Powered by NICE CXone Expert and was supported by the Department of
Education Open Textbook Pilot Project, the California Education Learning Lab, the UC Davis Office of
the Provost, the UC Davis Library, the California State University Affordable Learning Solutions
Program, and Merlot. This material is based upon work supported by the National Science Foundation
under Grant No. 1246120, 1525057, and 1413739.
Any opinions, findings, and conclusions or recommendations expressed in this material are those of the
author(s) and do not necessarily reflect the views of the National Science Foundation nor the US
Department of Education.
Have questions or comments? For information about adoptions or adaptions contact
info@LibreTexts.org or visit our main website at https://LibreTexts.org.

                                        This text was compiled on 06/04/2025
TABLE OF CONTENTS

  Licensing

 1: Getting Started - The Language of ODEs

       1.1: Problem Set

 2: Special Structure and Solutions of ODEs

       2.1: Problem Set

 3: Behavior Near Trajectories and Invariant Sets - Stability

       3.1: Problem Set

 4: Behavior Near Trajectories - Linearization

       4.1: Problem Set

 5: Behavior Near Equilbria - Linearization

       5.1: Problem Set

 6: Stable and Unstable Manifolds of Equilibria

       6.1: Problem Set

 7: Lyapunov's Method and the LaSalle Invariance Principle

       7.1: Lyapunov's Method and the LaSalle Invariance Principle
       7.2: Problem Set

 8: Bifurcation of Equilibria I

       8.1: Bifurcation of Equilibria I
       8.2: Problem Set

 9: Bifurcation of Equilibria II

       9.1: Bifurcation of Equilibria II
       9.2: Problem Set

 10: Center Manifold Theory

       10.1: Center Manifold Theory
       10.E: Center Manifold Theory (Exericses)

 11: Appendices

       11.1: A- Jacobians, Inverses of Matrices, and Eigenvalues
       11.2: B- Integration of Some Basic Linear ODEs
       11.3: Finding Lyapunov Functions
       11.4: D- Center Manifolds Depending on Parameters
       11.5: E- Dynamics of Hamilton's Equations

1  https://math.libretexts.org/@go/page/26185
Index
F: A Brief Introduction to the Characteristics of Chaos
Index
Glossary
Detailed Licensing

2                                                        https://math.libretexts.org/@go/page/26185
Licensing

A detailed breakdown of this resource's licensing can be found in Back Matter/Detailed Licensing.

1  https://math.libretexts.org/@go/page/115475
1: Getting Started - The Language of ODEs

This is a course about ordinary differential equations (ODEs). So we begin by defining what we mean by this term.

 DEFINITION 1: ORDINARY DIFFERENTIAL EQUATIONS

  An ordinary differential equation (ODE) is an equation for a function of one variable that involves (`'ordinary") derivatives of
  the function (and, possibly, known functions of the same variable).

We give several examples below.

1. dt2 d + 2x 2 x = 0
           2
2. 2 d x - x dtdx - x + x3 = sin(t)
dt
3. dt2 d2x - (1 - x2) dxdt + x = 0
4. 3 d + 3f f 2 d + 2f (1 - ( 2 d ) ) = 0 2f 2
d   d                                d

5. dx4 d + x dx2 + x = 0 4y 2 d2y 5

ODEs can be succinctly written by adopting a more compact notation for the derivatives. We rewrite the examples above with this
shorthand notation.

1. x¨ + 2x = 0
2. x¨ - xx - x + x3 = sin(t)
3. x¨ - (1 - x2)x + x = 0
4. f  + f f  + (1 - (f )2) = 0
5. y + x2y + x5 = 0

Characterizing ODEs

Now that we have defined the notion of an ODE, we will need to develop some additional concepts in order to more deeply
describe the structure of ODEs. The notions of "structure" are important since we will see that they play a key role in how we
understand the nature of the behavior of solutions of ODEs.

DEFINITION 2: DEPENDENT VARIABLE
 The value of the function, e.g for example 1, x(t).

DEFINITION 3: INDEPENDENT VARIABLE
 The argument of the function, e.g for example 1, t.

We summarize a list of the dependent and independent variables in the five examples of ODEs given above.

                                     Table 1.1: Identifying the independent and dependent variables for several examples.

       Example                                        Dependent variable  Independent Variable

                       1                                         x                                                         t

                       2                                         x                                                         t

                       3                                         x                                                         t

                       4                                         f                                                         

                       5                                         y                                                         x

The notion of ''order'' is an important characteristic of ODEs.

                                                                 1.1      https://math.libretexts.org/@go/page/24130
DEFINITION 4: ORDER OF AN ODE
 The number associated with the largest derivative of the dependent variable in the ODE.

We give the order of each of the ODEs in the five examples above.

   Table 1.2: Identifying the order of the ODE for several examples.

Example                                                                                   Order

1                                                                                         Second Order

2                                                                                         Second Order

3                                                                                         Second Order

4                                                                                         Third Order

5                                                                                         Fourth Order

Distinguishing between the independent and dependent variables enables us to define the notion of autonomous and
nonautonomous ODEs.

 DEFINITION 5: AUTONOMOUS and NONAUTONOMOUS

  An ODE is said to be autonomous if none of the coefficients (i.e. functions) multiplying the dependent variable, or any of its
  derivatives, depend explicitly on the independent variable, and also if no terms not depending on the dependent variable or any
  of it derivatives depend explicitly on the independent variable. Otherwise, it is said to be nonautonomous.

Or, more succinctly, an ODE is autonomous if the independent variable does not explicitly appear in the equation. Otherwise, it is
nonautonomous. We apply this definition to the five examples above, and summarize the results in the table below.

Table 1.3: Identifying autonomous and nonautonomous ODEs for several examples.

Example                                                                                   Order

1                                                                                         autonomous

2                                                                                         nonautonomous

3                                                                                         autonomous

4                                                                                         autonomous

5                                                                                         nonautonomous

All scalar ODEs, i.e. the value of the dependent variable is a scalar, can be written as first order equations where the new dependent
variable is a vector having the same dimension as the order of the ODE. This is done by constructing a vector whose components
consist of the dependent variable and all of its derivatives below the highest order. This vector is the new dependent variable. We
illustrate this for the five examples above.

    x = v ,

  v = -w2x, (x, v)  R × R .

    x = v ,

  v = xv + x - x3 + sin(t), (x, v)  R × R .

    x = v ,

  v = (1 - x2)v - x, (x, v)  R × R .

    f' = v,

    f'' = u,

         1.2                                                                              https://math.libretexts.org/@go/page/24130
f  = -f f  - (1 - (f )2)
or
f' = v,
v' = f'' = u,

u = f  = -f u - (1 - u2)

or

f                 v       
 v  =                     , (f, v, u)  R × R × R
                  u
 u   -fu - (1 - u2) 

y' = w,

y'' = v,

y''' = u,
y = -x2 y - x5

or

y' = w,

w' = y'' = v,

v' = y''' = u,
u = y = -x2v - x5

or

 y   w 
 w   v 
   =                 , (y, w, v, u)  R × R × R × R
    v          u     
 u   -x2v - x5 

Therefore without loss of generality, the general form of the ODE that we will study can be expressed as a first order vector ODE:

                                x = f (x), x(t0)  x0, x  Rn, autonomous,                                                            (1.1)
                              x = f (x, t), x(t0)  x0, x  Rn, nonautonomous,                                                        (1.2)

where x(t0)  x0 , is referred to as the initial condition.

This first order vector form of ODEs allows us to discuss many properties of ODEs in a way that is independent of the order of the
ODE. It also lends itself to a natural geometrical description of the solutions of ODEs that we will see shortly.

A key characteristic of ODEs is whether or not they are linear or nonlinear.

Definition: LINEAR AND NONLINEAR ODES

An ODE is said to be linear if it is a linear function of the dependent variable. If it is not linear, it is said to be nonlinear.

Note that the independent variable does not play a role in whether or not the ODE is linear or nonlinear.

                          Table 1.4: Identifying linear and nonlinear ODEs for several examples.

                     Example                                                  Order

                     1                                                        linear

                     2                                                        nonlinear

                     3                                                        nonlinear

                     4                                                        nonlinear

                                                            1.3                                   https://math.libretexts.org/@go/page/24130
                      Example                                                           Order
                          5                                                             linear

When written as a first order vector equation the (vector) space of dependent variables is referred to as the phase space of the ODE.
The ODE then has the geometric interpretation as a vector field on phase space. The structure of phase space, e.g. its dimension and
geometry, can have a significant influence on the nature of solutions of ODEs. We will encounter ODEs defined on different types
of phase space, and of different dimensions. Some examples are given in the following lists.

1-dimension

1. R -the real line,
2. I  R -an interval on the real line,

3. S1 -the circle.

`'Solving" One dimensional Autonomous ODEs. Formally (we will explain what that means shortly) an expression for the solution

of a one dimensional autonomous ODE can be obtained by integration. We explain how this is done, and what it means. Let P

denote one of the one dimensional phase spaces described above. We consider the autonomous vector field defined on P as follows:

                                           x = dxdt = f(x), x(t0) = x0, x  P                    (1.3)

This is an example of a one dimensional separable ODE which can be written as follows:

                                           = x  (t) dx t dt = t - t0.                           (1.4)
                                           x(t0) f (x) t0

If we can compute the integral on the left hand side of (1.4), then it may be possible to solve for x(t). However, we know that not

all functions    1    can be integrated. This is what we mean by we can `'formally" solve for the solution for this example. We may

               f (x)
not be able to represent the solution in a form that is useful.

The higher dimensional phase spaces that we will consider will be constructed as Cartesian products of these three basic one
dimensional phase spaces.

2-dimensions

1. R2 = R × R -the plane,
2. T2 = S × S -the two torus,
3. C = I × S - the (finite) cylinder,
4. C = R × S -the (infinite) cylinder.

In many applications of ODEs the independent variable has the interpretation of time, which is why the variable t is often used to
denote the independent variable. Dynamics is the study of how systems change in time. When written as a first order system ODEs
are often referred to as dynamical systems, and ODEs are said to generate vector fields on phase space. For this reason the phrases

ODE and vector field t end to be used synonomously.

Existence of Solutions

Several natural questions arise when analyzing an ODE. ''Does the ODE have a solution?'' ''Are solutions unique?'' (And what does
''unique'' mean?) The standard way of treating this in an ODE course is to ''prove a big theorem'' about existence and uniqueness.
Rather, than do that (you can find the proof in hundreds of books, as well as in many sites on the internet), we will consider some
examples that illustrate the main issues concerning what these questions mean, and afterwards we will describe sufficient
conditions for an ODE to have a unique solution (and then consider what ''uniqueness'' means).

First, do ODEs have solutions? Not necessarily, as the following example shows.

Example 1.1: An example of an ODE that has no solutions

Consider the following ODE defined on R:

                                          x2 + x2 + t2 = -1, x  R.

                                                                 1.4                    https://math.libretexts.org/@go/page/24130
This ODE has no solutions since the left hand side is nonnegative and the right hand side is strictly negative.

Then you can ask the question- ''if the ODE has solutions, are they unique?'' Again, the answer is ''not necessarily'', as the
following example shows.

Example 1.2: An example illustrating the meaning of uniqueness

                                  x = ax, x  Rn,

where a is an arbitrary constant. The solution is given by

                                                            x(t) = ceat.                                                    (1.5)

So we see that there are an infinite number of solutions, depending upon the choice of the constant c. So what could
uniqueness of solutions mean? If we evaluate the solution in Equation 1.5 at t = 0 we see that

                                                            x(0) = c

Substituting this into the solution in Equation 1.5, the solution has the form:

                                  x(t) = x(0)eat.                                                                           (1.6)

From the form of Equation 1.6 we can see exactly what "uniquess of solutions" means. For a given initial condition, there is
exactly one solution of the ODE satisfying that initial condition.

Example 1.8

An example of an ODE with non-unique solutions. Consider the following ODE defined on R:

                                        x = 3x 3 , 2 x(0) = 0, x  R.

It is easy to see that a solution satisfying x(0) = 0 is x = 0. However, one can verify directly by substituting into the equation
that the following is also a solution satisfying x(0) = 0:

                                  x(t) = { (t - a)3 0, ,                  ta}

                                                                          t>a

for any a > 0 . Hence, in this example, there are an infinite number of solutions satisfying the same initial condition. This
example illustrates precisely what we mean by uniqueness. Given an initial condition, only one (`'uniqueness") solution
satisfies the initial condition at the chosen initial time.

There is another question that comes up. If we have a unique solution does it exist for all time? Not necessarily, as the following
example shows.

Example 1.9: An example of an ODE with unique solutions that exists only for a finite time

Consider the following ODE on R:

                                  x = x2, x(0) = x0, x  R

We can easily integrate this equation (it is separable) to obtain the following solution satisfying the initial condition:
                                                            x(t) = x0 .
                                                                     1 - x0t

The solution becomes infinite, or ''does not exist'' or ''blows up'' at t. This x0 is what ''does not exist'' means. So the solution
only exists for a finite time, and this ''time of existence'' depends on the initial condition.

                                                            1.5                           https://math.libretexts.org/@go/page/24130
Uniqueness of Solutions

These three examples contain the essence of the ''existence issues'' for ODEs that will concern us. They are the ''standard examples''
that can be found in many textbooks. Now we will state the standard ''existence and uniqueness'' theorem for ODEs. The statement
is an example of the power and flexibility of expressing a general ODE as a first order vector equation. The statement is valid for
any (finite) dimension.

We consider the general vector field on Rn

                         x = f(x, t), x(t0) = x0, x  R.                  (1.7)

It is important to be aware that for the general result we are going to state it does not matter whether or not the ODE is autonomous
or nonautonomous.

We define the domain of the vector field. Let U  Rn be an open set and let I  R be an interval. Then we express that the n-
dimensional vector field is defined on this domain as follows:

                                                f : U  Rn ,

                                               (x, t)  f(x, t)

We need a definition to describe the `'regularity" of the vector field.

DEFINITION 7: C R FUNCTION
We say that f(x,t) is C r on U × I  Rn × R if it is r times differentiable and each derivative is a continuous function (on the

 same domain). If r = 0, f (x, t) is just said to be continuous.

Now we can state sufficient conditions for (1.13) to have a unique solution. We suppose that f(x,t) is C r, r  1 . We choose any
point (x0, t0)  U × I . Then there exists a unique solution of (1.13) satisfying this initial condition. We denote this solution by
x(t, t0, x0), and reflect in the notation that it satisfies the initial condition by x(t0, t0, x0) = x0 . This unique solution exists for a
time interval centered at the initial time t0, denoted by (t0 - , t0 + ) , for some  > 0 . Moreover, this solution, x(t, t0, x0), is a
C r function of t, t0, x0. Note that from Example 9  may depend on x0. This also explains how a solution `'fails to exist"-it

becomes unbounded (`'blow up") in a finite time.

Finally, we remark that existence and uniqueness of ODEs is the mathematical manifestation of determinism. If the initial condition
is specified (with 100% accuracy), then the past and the future is uniquely determined. The key phrase here is ''100% accuracy''.
Numbers cannot be specified with 100% accuracy. There will always be some imprecision in the specification of the initial
condition. Chaotic dynamical systems are deterministic dynamical systems having the property that imprecisions in the initial
conditions may be magnified by the dynamical evolution, leading to seemingly random behavior (even though the system is
completely deterministic).

This page titled 1: Getting Started - The Language of ODEs is shared under a CC BY 4.0 license and was authored, remixed, and/or curated by

Stephen Wiggins via  source content that was edited to the style and standards of the LibreTexts platform.

                         1.6                                             https://math.libretexts.org/@go/page/24130
1.1: Problem Set

 1. For each of the ODEs below, write it as a first order system, state the dependent and independent variables, state any parameters
    in the ODE (i.e. unspecified constants) and state whether it is linear or nonlinear, and autonomous or nonautonomous,

(a)

¨ +  + sin = F cos(t) ,   S1 .

(b)

¨ +  +  = F cos(t) ,   S1 .

(c)

d3y 2 dy
dx3 + x y dx + y = 0 , x  R .  1

(d)

x¨ + x + x - x3 =  ,

¨ + sin = 0 , (x, )  R1 × S1

(e)

  ¨ +  + sin = x ,

 x¨ - x + x3 = 0 , (, x)  R1 × S1

2. Consider the vector field:

x = 3x 32 , x(0)  0, x  R.

Does this vector field have unique solutions?

3. Consider the vector field:

 x = -x + x2 , x(0) = x0, x  R.

Determine the time interval of existence of all solutions as a function of the initial condition, x0.

4. Consider the vector field:

 x = a(t)x + b(t) , x  R.

Determine sufficient conditions on the coefficients a(t) and b(t) for which the solutions will exist for all time. Do the results
depend on the initial condition?

This page titled 1.1: Problem Set is shared under a CC BY 4.0 license and was authored, remixed, and/or curated by Stephen Wiggins via  source

content that was edited to the style and standards of the LibreTexts platform.

                                               1.1.1  https://math.libretexts.org/@go/page/24131
2: Special Structure and Solutions of ODEs

A consistent theme throughout all of ODEs is that ''special structure'' of the equations can reveal insight into the nature of the
solutions. Here we look at a very basic and important property of autonomous equations:

"Time shifts of solutions of autonomous ODEs are also solutions of the ODE (but with a
different initial condition)".

Now we will show how to see this.

Throughout this course we will assume that existence and uniqueness of solutions holds on a domain and time interval sufficient
for our arguments and calculations.

We start by establishing the setting. We consider an autonomous vector field defined on Rn:

                                            x = f (x) x(0) = x0, x  Rn,                                                (2.1)

with solution denoted by:

                                               x(t, 0, x0), x(0, 0, x0) = x0.                                          (2.2)

Here we are taking the initial time to be t0 = 0 . We will see, shortly, that for autonomous equations this can be done without loss

of generality. Now we choose s  R (s  0 , which is to be regarded as a fixed constant). We must show the following:

                                               x(t + s) = f (x(t + s))                                                 (2.3)

This is what we mean by the phrase time shifts of solutions are solutions. This relation follows immediately from the chain rule
calculation:

                                            d  d d(t + s)                 d                                            (2.4)
                                            dt = d(t + s) dt = d(t + s) .

Finally, we need to determine the initial condition for the time shifted solution. For the original solution we have:

                                               x(t, 0, x0), x(0, 0, x0) = x0,                                          (2.5)

and for the time shifted solution we have:

                                               x(t + s, 0, x0), x(s, 0, x0).                                           (2.6)

It is for this reason that, without loss of generality, for autonomous vector fields we can take the initial time to be t0 = 0 . This
allows us to simplify the arguments in the notation for solutions of autonomous vector fields, i.e., x(t, 0, x0)  x(t, x0) with
x(0, 0, x0) = x(0, x0) = x0.

Example 2.5: The time-shift property of autonomous vector fields

Consider the following one dimensional autonomous vector field:

                                     x = x, x(0) = x0, x  R,   R.

The solution is given by:

                                               x(t, 0, x0) = x(t, x0) = et x0.

The time shifted solution is given by:

                                               x(t + s, x0) = e(t+s) x0.

We see that it is a solution of the ODE with the following calculations:

                                            d x(t + s, x0) = e(t+s) x0 = x(t + s, x0),
                                            dt

with initial condition:

                                               2.1                                           https://math.libretexts.org/@go/page/24137
                                                             x(s, x0) = es x0.

In summary, we see that the solutions of autonomous vector fields satisfy the following three properties:

1. x(0, x0) = x0
2. x(t, x0) is C r in x0
3. x(t + s, x0) = x(t, x(s, x0))

Property one just reflects the notation we have adopted. Property 2 is a statement of the properties arising from existence and
uniqueness of solutions. Property 3 uses two characteristics of solutions. One is the ''time shift'' property for autonomous vector
fields that we have proven. The other is ''uniqueness of solutions'' since the left hand side and the right hand side of Property 3
satisfy the same initial condition at t = 0.

These three properties are the defining properties of a flow, i.e., a . In other words, we view the solutions as defining a map of
points in phase space. The group property arises from property 3, i.e., the time-shift property. In order to emphasize this ''map of
phase space'' property we introduce a general notation for the flow as follows

                                               x(t, x0)  t()

where the ''·'' in the argument of t() reflects the fact that the flow is a function on the phase space. With this notation the three

properties of a flow are written as follows:

1. 0() is the identity map.
2. t() is C r for each t.
3. t+s () = t  s()

We often use the phrase ''the flow generated by the (autonomous) vector field''. Autonomous is in parentheses as it is understood
that when we are considering flows then we are considering the solutions of autonomous vector fields. This is because
nonautonomous vector fields do not necessarily satisfy the time-shift property, as we now show with an example.

Example 2.6: An example of a nonautonomous vector field not having the time-shift property

Consider the following one dimensional vector field on R:

                                           x = tx\), \)x(0) = x0, x  R\), \(  R.

This vector field is separable and the solution is easily found to be:

                                                           x(t, 0, x0) = x0e 2 t2 .

The time shifted `'solution" is given by:

                                                      x(t  +  s,  0,  x0  )  =  x0   e     (t+s)2  .

                                                                                        2

We show that this does not satisfy the vector field with the following calculation:

                                           d   x  (t  +  s,  0,  x0  )  =  x0  e     (t+s)2  (t    +  s)  .
                                           dt
                                                                                  2

                                                            tx(t + s, 0, x0) .

Perhaps a more simple example illustrating that nonautonomous vector fields do not satisfy the time-shift property is the following.

 Example 2.7

  Consider the following one dimensional nonautonomous vector field:

                                               x = et , x  R.

  The solution is given by:

                                                 x(t) = et .

  It is easy to verify that the time-shifted function:

                                                                        2.2                                  https://math.libretexts.org/@go/page/24137
                                    x(t + s) = et+s ,

does not satisfy the equation.

In the study of ODEs certain types of solutions have achieved a level of prominence largely based on their significance in
applications. They are

    equilibrium solutions,
    periodic solutions,
    heteroclinic solutions,
    homoclinic solutions.

We define each of these.

DEFINITION 8: EQUILIBRIUM           that is a solution of the ODE , i.e.

A point in phase space x = x¯ = Rn

 f (x¯) = 0, f (x¯, t) = 0 ,

 is called an equilibrium point.

For example, x = 0 is an equilibrium point for the following autonomous and nonautonomous one dimensional vector fields,
respectively,

  x = x , x  R,
  x = tx , x  R.

A periodic solution is simply a solution that is periodic in time. Its definition is the same for both autonomous and nonautonomous
vector fields.

 DEFINITION 9: PERIODIC SolutionS

  A solution x(t, t0, x0) is periodic if there exists a T > 0 such that

  x(t, t0, x0) = x(t + T , t0, x0)

Homoclinic and heteroclinic solutions are important in a variety of applications. Their definition is not so simple as the definitions
of equilibrium and periodic solutions since they can be defined and generalized to many different settings. We will only consider
these special solutions for autonomous vector fields, and solutions homoclinic or heteroclinic to equilibrium solutions.

DEFINITION 10 (HOMOCLINIC AND HETEROCLINIC SolutionS)

Suppose x¯1 and x¯2 are equilibrium points of an autonomous vector field, i.e.                                  (2.7)
                                                   f (x¯1) = 0 , f (x¯2) = 0 .

A trajectory x(t, t0, x0) is said to be heteroclinic to x¯1 and x¯2 if
                                                  limt x(t, t0, x0) = x¯2,
                                                 limt- x(t, t0, x0) = x¯1,

If x¯1 = x¯2 the trajectory is said to be homoclinic to x¯1 = x¯2 .

Example 2.8: equilibrium points and heteroclinic orbits                                                         (2.8)

Consider the following one dimensional autonomous vector field on R:
                                       x = x - x3 = x(1 - x2), x  R.

 This vector field has three equilibrium points at x = 0, ±1.

 In Fig. 2.1 we show the graph of the vector field (2.12) in panel a) and the phase line dynamics in panel b).

                                    2.3                                         https://math.libretexts.org/@go/page/24137
The solid black dots in panel b) correspond to the equilibrium points and these, in turn, correspond to the zeros of the vector
field shown in panel a). Between its zeros, the vector field has a fixed sign (i.e. positive or negative), corresponding to  being
either increasing or decreasing. This is indicated by the direction of the arrows in panel b).

Figure 2.1: a) Graph of the vector field. b) The phase space.

Our discussion about trajectories, as well as this example, brings us to a point where it is natural to introduce the important notion

 of an invariant set. While this is a general idea that applies to both autonomous and nonautonomous systems, in this course we will

only discuss this notion in the context of autonomous systems. Accordingly, let t() denote the flow generated by an autonomous

vector field.

DEFINITION 11: INVARIANT SET            x  M  t(x)  M          (2.9)

A set M  Rn is said to be invariant if

t.

In other words, a set is invariant (with respect to a flow) if you start in the set, and remain in the set, forever.

If you think about it, it should be clear that invariant sets are sets of trajectories. Any single trajectory is an invariant set. The entire
phase space is an invariant set. The most interesting cases are those ''in between''. Also, it should be clear that the union of any two
invariant sets is also an invariant set (just apply the definition of invariant set to the union of two, or more, invariant sets).

There are certain situations where we will be interested in sets that are invariant only for positive time-positive invariant sets.

 DEFINITION 12: POSITIVE INVARIANT SET

 A set M  Rn is said to be positive invariant if
 x  M  t(x)  M t > 0 .

There is a similar notion of negative invariant sets, but the generalization of this from the definition of positive invariant sets should
be obvious, so we will not write out the details.

                                        2.4                    https://math.libretexts.org/@go/page/24137
Concerning example 8, the three equilibrium points are invariant sets, as well as the closed intervals [-1, 0] and [0, 1]. Are there
other invariant sets?

This page titled 2: Special Structure and Solutions of ODEs is shared under a CC BY 4.0 license and was authored, remixed, and/or curated by

Stephen Wiggins via  source content that was edited to the style and standards of the LibreTexts platform.

2.5  https://math.libretexts.org/@go/page/24137
2.1: Problem Set

 EXERCISE 2.1.1

  Consider an autonomous vector field on the plane having a hyperbolic fixed point with a homoclinic orbit connecting the
  hyperbolic fixed point, as illustrated in Fig. 1. We assume that existence and uniqueness of solutions holds. Can a trajectory
  starting at any point on the homoclinic orbit reach the hyperbolic fixed point in a finite time? (You must justify your answer.)

EXERCISE 2.1.2

Can an autonomous vector field on R that has no equilibrium points have periodic orbits? We assume that existence and

 uniqueness of solutions holds.(You must justify your answer.)

EXERCISE 2.1.3

Can a nonautonomous vector field on R that has no equilibrium points have periodic orbits? We assume that existence and

 uniqueness of solutions holds.(You must justify your answer.)

EXERCISE 2.1.4
 Can an autonomous vector field on the circle that has no equilibrium points have periodic orbits? We assume that existence and
 uniqueness of solutions holds. (You must justify your answer.)

EXERCISE 2.1.5
 Consider the following autonomous vector field on the plane:

                                                            x = -y ,

                                           y = x , (x, y)  R2,

2.1.1  https://math.libretexts.org/@go/page/24138
where  > 0
    Show that the flow generated by this vector field is given by:

                ( x(t) ) = ( cost                                       -sint ) ( x0 )
                y(t)                                   sint             cost        y0

Show that the flow obeys the time shift property.
Give the initial condition for the time shifted flow.

EXERCISE 2.1.6

Consider the following autonomous vector field on the plane:
                                                                        x = y ,

                                          y = x , (x, y)  R2 ,

    Show that the flow generated by this vector field is given by:

                ( x(t) ) = ( cosht                                      sinht ) ( x0 )
                y(t)                                   sinht
                                                                        cosht y0

Show that the flow obeys the time shift property.
Give the initial condition for the time shifted flow.

EXERCISE 2.1.7

 Show that the time shift property for autonomous vector fields implies that trajectories cannot ''cross each other'', i.e. intersect,
 in phase space.

EXERCISE 2.1.8
 Show that the union of two invariant sets is an invariant set.

EXERCISE 2.1.9
 Show that the intersection of two invariant sets is an invariant set.

EXERCISE 2.1.10
 Show that the complement of a positive invariant set is a negative invariant set.

This page titled 2.1: Problem Set is shared under a CC BY 4.0 license and was authored, remixed, and/or curated by Stephen Wiggins via  source

content that was edited to the style and standards of the LibreTexts platform.

                                                                 2.1.2                  https://math.libretexts.org/@go/page/24138
3: Behavior Near Trajectories and Invariant Sets - Stability

Consider the general nonautonomous vector field in n dimensions:

                                                           x = f (x, t), x  Rn,              (3.1)

and let x¯(t, t0, x0) be a solution of this vector field.

Many questions in ODEs concern understanding the behavior of neighboring solutions near a given, chosen solution. We will
develop the general framework for considering such questions by transforming Equation 3.1 to a form that allows us to explicitly
consider these issues.

We consider the following (time dependent) transformation of variables:

                                                           x = y + x¯(t, t0, x0).            (3.2)

We wish to express Equation 3.1 in terms of the y variables. It is important to understand what this will mean in terms of Equation
3.2. For y small it means that x is near the solution of interest, x¯(t, t0, x0). In other words, expressing the vector field in terms of y
will provide us with an explicit form of the vector field for studying the behavior near x¯(t, t0, x0). Towards this end, we begin by
transforming Equation 3.1 using Equation 3.2 as follows:

    x = y + x¯ = f (x, t) = f (y + x¯, t),                                                   (3.3)

or

                                                           y = f (y + x¯, t) - x¯ ,

    = f (y + x¯, t) - f (x¯, t)  g(y, t), g(0, t) = 0.                                       (3.4)

Hence, we have shown that solutions of Equation 3.1 near x¯(t, t0, x0) are equivalent to solutions of Equation 3.4 near y = 0.

The first question we want to ask related to the behavior near x¯(t, t0, x0) is whether or not this solution is stable? However, first we
need to mathematically define what is meant by this term ''stable''. Now we should know that, without loss of generality, we can
discuss this question in terms of the zero solution of Equation 3.4.

We begin by defining the notion of ''Lyapunov stability'' (or just "stability").

Definition 13 (LYAPUNOV STABILITY)
 y = 0 is said to be Lyapunov stable at t0 if given  > 0 there exists a  = (t0, ) such that

                                                           |y(t0)| <   |y(t)| < , t > t0     (3.5)
    If a solution is not Lyapunov stable, then it is said to be unstable.

Definition 14 (UNSTABLE)
 If y = 0 is not Lyapunov stable, then it is said to be unstable.
 Then we have the notion of asymptotic stability.

Definition 15 (ASYMPTOTIC STABILITY)                                                         (3.6)

 y = 0 is said to be asymptotically stable at t0 if:
  1. it is Lyapunov stable at t0,
  2. there exists  = (t0) > 0 such that:

                                                           |y(t0)| <   limt |y(t)| = 0

We have several comments about these Definitions.

    Roughly speaking, a Lyapunov stable solution means that if you start close to that solution, you stay close-forever. Asymptotic
    stability not only means that you start close and stay close forever, but that you actually get "closer and closer" to the solution.
    Stability is an infinite time concept.

                                                                   3.1                       https://math.libretexts.org/@go/page/24144
If the ODE is autonomous, then the quantity  = (t0, ) can be chosen to be independent of t0.

The Definitions of stability do not tell us how to prove that a solution is stable (or unstable). We will learn two techniques for
analyzing this question-linearization and Lyapunov's (second) method.

t Why is Lyapunov stability included in the Definition of asymptotic stability? Because it is possible to construct examples where

nearby solutions do get closer and closer to the given solution as  , but in the process there are intermediate intervals of
time where nearby solutions make "large excursions" away from the given solution.

"Stability" is a notion that applies to a "neighborhood" of a trajectory. At this point we want to formalize various notions related to

distance and neighborhoods in phase space. For simplicity in expressing these ideas we will take as our phase space Rn. Points in

this phase space are denoted x  Rn , x  (x1, . . . , xn). The norm, or length, of x, denoted |x| is defined as:

  |x| =                                                                  --n---
                                                    x x x  x ---------------                                          (3.7)
                                                                                 2.
                                                     21 + 22 +  + 2n =
                                                                             i
                                                                         i=1

 The distance between two points in x, y  Rn is defined as:                                                           (3.8)
                                                                                           -----------------------
                      d(x, y)  |x - y| = (x1 - y1)2 +  + (xn - yn)2

\[= \sqrt{\sum_{i=1}^{n} \sqrt{(x_{i}-y_{i})^2}. \label{3.7}\]

Distance between points in Rn should be somewhat familiar, but now we introduce a new concept, the distance between a point
and a set. Consider a set M, M  Rn , let p  Rn . Then the distance from p to M is defined as follows:
dist(p, M)  infxM |p -x|.
                                                                                                                      (3.9)

We remark that it follows from the Definition that if p  M , then dist(p, M) = 0.

We have previously defined the notion of an invariant set. Roughly speaking, invariant sets are comprised of trajectories. We now

have the background to discuss the notion of . Recall, that the notion of invariant set was only developed for autonomous vector

fields. So we consider an autonomous vector field:

                                                    x = f(x), x  Rn,                                                  (3.10)

and denote the flow generated by this vector field by t(). Let M be a closed invariant set (in many applications we may also
require M to be bounded) and let U M denoted a neighborhood of M.

The definition of Lyapunov stability of an invariant set is as follows.

DEFINITION 16 (LYAPUNOV STABILITY OF M)

M is said to be Lyapunov stable if for any neighborhood U  M, x  U  t(x)  U, t > 0 .

 Similarly, we have the following definition of asymptotic stability of an invariant set.

 DEFINITION 17 (ASYMPTOTIC STABILITY OF M)
  M is said to be asymptotically stable if
    1. it is Lyapunov stable,

  2. there exists a neighborhood U  M such that x  U, dist((tx), M)  0 as t  .

In the dynamical systems approach to ordinary differential equations some alternative terminology is typically used.
 DEFINITION 18 (ATTRACTING SET)
  If M is asymptotically stable it is said to be an attracting set.

The significance of attracting sets is that they are the "observable" regions in phase space since they are regions to which
trajectories evolve in time. The set of points that evolve towards a specific attracting set is referred to as the basin of attraction for
that invariant set.

                                                                3.2                                                 https://math.libretexts.org/@go/page/24144
DEFINITION 19 (BASIN OF ATTRACTION)

Let B  Rn denote the set of all points, x  B  Rn such that
                                     dist(t(x), M)  0 as t  

 Then call B is called the basin of attraction of M.

We now consider an example that allows us to explicitly explore these ideas.

Example 3.9

Consider the following autonomous vector field on the plane:                  (3.11)

x = x ,
                                  y = y2(1 - y2)  f(y), (x, y)  R2.

First, it is useful to note that the x and y components of Equation 3.11 are independent. Consequently, this may seem like a
trivial example. However, we will see that such examples provide a great deal of insight, especially since they allow for simple
computations of many of the mathematical ideas.

In Fig. 3.1 we illustrate the flow of the x and y components of Equation 3.11 separately.

The two dimensional vector field Equation 3.11 has equilibrium points at: (x, y) = (0, 0), (0, 1), (0, -1).

In this example it is easy to identify three invariant horizontal lines (examples of invariant sets). Since y = 0 implies that

y = 0 , this implies that the x axis is invariant. Since y = 1 implies that y = 0 , this implies that the line y = 1 is invariant. Since
y = -1 implies that y = 0 , which implies that the line y = -1 is invariant. This is illustrated in Fig. 3.2. Below we provide

some additional invariant sets for (3.10). It is instructive to understand why they are invariant, and whether or not there are
other invariant sets.

             3.3                                                              https://math.libretexts.org/@go/page/24144
Figure 3.1: a) The phase "line" of the x component of (3.10). b) The graph of f(y) (top) and the phase "line" of the y component
of (3.10) directly below.

3.4  https://math.libretexts.org/@go/page/24144
                                          Figure 3.2: Phase plane of (3.10). The black dots indicate equilibrium points.
                                                          Additional invariant sets for (3.10).

                                               {(x, y)| - < x < 0, - < y < 1 },
                                                {(x, y)| 0 < x < , - < y < 1 },
                                                 {(x, y)| - < x < 0, 1 < y < 0 },

                                                  {(x, y)| 0 < x < , 1 < y < 0 },
                                                 {(x, y)| - < x < 0, 0 < y < 1 },
                                                  {(x, y)| 0 < x < , 0 < y < 1 },
                                                {(x, y)| - < x < 0, 1 < y <  },
                                                 {(x, y)| 0 < x < , 1 < y <  }.

This page titled 3: Behavior Near Trajectories and Invariant Sets - Stability is shared under a CC BY 4.0 license and was authored, remixed,

and/or curated by Stephen Wiggins via  source content that was edited to the style and standards of the LibreTexts platform.

3.5  https://math.libretexts.org/@go/page/24144
3.1: Problem Set

EXERCISE 3.1.1

Consider the following autonomous vector field on R:                                                 (3.1.1)

                                x = x - x3, x  R.

Compute all equilibria and determine their stability, i.e., are they Lyapunov stable, asymptotically stable, or unstable?
Compute the flow generated by (3.11) and verify the stability results for the equilibria directly from the flow.

EXERCISE 3.1.2

Consider an autonomous vector field on Rn:

                                                   x = f(x), x  Rn.                                  (3.1.2)

Suppose M  Rn is a bounded, invariant set for (3.12). Let t() denote the flow generated by (3.12). Suppose
p  Rn, p  M . Is it possible for
                                                   t(p)  M ,

for some finite t?

EXERCISE 3.1.3

Consider the following vector field on the plane:

x = x - x3 ,

                                                   y = -y, (x, y)  R2.                               (3.1.3)

1. Determine 0-dimensional, 1-dimensional, and 2-dimensional invariant sets.
2. Determine the attracting sets and their basins of attraction.
3. Describe the heteroclinic orbits and compute analytical expressions for the heteroclinic orbits.
4. Does the vector field have periodic orbits?
5. Sketch the phase portrait.6

This page titled 3.1: Problem Set is shared under a CC BY 4.0 license and was authored, remixed, and/or curated by Stephen Wiggins via  source

content that was edited to the style and standards of the LibreTexts platform.

                                                   3.1.1                https://math.libretexts.org/@go/page/24145
4: Behavior Near Trajectories - Linearization

Now we are going to discuss a method for analyzing stability that utilizes linearization about the object whose stability is of
interest. For now, the "objects of interest" are specific solutions of a vector field.The structure of the solutions of linear, constant
coefficient systems is covered in many ODE textbooks. My favorite is the book of Hirsch et al.1. It covers all of the linear algebra
needed for analyzing linear ODEs that you probably did not cover in your linear algebra course. The book by Arnold is also very
good, but the presentation is more compact, with fewer examples.

We begin by considering a general nonautonomous vector field:

                              x = f(x, t), x  Rn,                                                      (4.1)

and we suppose that

                              x¯(t, t0, x0),                                                           (4.2)

is the solution of Equation 4.1 for which we wish to determine its stability properties. As when we introduced the definitions of
stability, we proceed by localizing the vector field about the solution of interest. We do this by introducing the change of
coordinates

                                               x = y + x¯

for Equation 4.1 as follows:

                              x = y + x¯ = f(y + x¯, t) ,

or

                              y = f(y + x¯, t)x¯ ,

                              = f(y + x¯, t)f(x¯, t),                                                  (4.3)

where we omit the arguments of x¯(t, t0, x0) for the sake of a less cumbersome notation. Next we Taylor expand f(y + x¯, t) in y
about the solution x¯, but we will only require the leading order terms explicitly

                              f(y + x¯, t) = f(x¯, t) + Df(x¯, t)y + O(|y|2),                          (4.4)

where Df denotes the derivative (i.e. Jacobian matrix) of the vector valued function f and O(|y|2) denotes higher order terms in

the Taylor expansion that we will not need in explicit form. Substituting this into Equation 4.4 gives:

                                        y = f(y + x¯, t) - f(x¯, t) ,
                                 = f(x¯, t) + Df(x¯, t)y + O(|y|2)f(x¯, t) ,

                              = Df(x¯, t)y + O(|y|2).                                                  (4.5)

Keep in mind that we are interested in the behavior of solutions near x¯(t, t0, x0), i.e., for y small. Therefore, in that situation it
seems reasonable that neglecting the O(|y|2) in Equation 4.5 would be an approximation that would provide us with the particular

information that we seek. For example, would it provide sufficient information for us to deter- mine stability? In particular,

                                             y = Df(x¯, t)y,                                           (4.6)
is referred to as the linearization of the vector field x = f(x, t) about the solution x¯(t, t0, x0).

Before we answer the question as to whether or not Equation 4.1 provides an adequate approximation to solutions of Equation 4.5
for y "small", we will first study linear vector fields on their own.

Linear vector fields can also be classified as nonautonomous or autonomous. Nonautonomous linear vector fields are obtained by
linearizing a nonautonomous vector field about a solution (and retaining only the linear terms). They have the general form:

                              y = A(t)y, y(0) = y0,                                                    (4.7)

where

                              A(t)  Df(x¯(t, t0, x0), t)                                               (4.8)

                                                               4.1                                     https://math.libretexts.org/@go/page/24151
is a n × n matrix. They can also be obtained by linearizing an autonomous vector field about a time-dependent solution.

An autonomous linear vector field is obtained by linearizing an autonomous vector field about an equilibrium point. More

precisely, let x = f (x) denote an autonomous vector field and let x = x0 denote an equilibrium point, i.e. f (x0) = 0 . The

linearized autonomous vector field about this equilibrium point has the form:

                                                    y = Df (x0)y, y(0) = y0,                                                 (4.9)

or

                                                    y = Ay, y(0) = y0,                                                       (4.10)

where A  Df(x0) is a n × n matrix of real numbers. This is significant because (4.10) can be solved using techniques of linear

algebra, but Equation 4.7, generally, cannot be solved in this manner. Hence, we will now describe the general solution of (4.10).

The general solution of Equation 4.10 is given by:

                                                        y(t) = eAty0.                                                        (4.11)

In order to verify that this is the solution, we merely need to substitute into the right hand side and the left hand side of (4.10) and

show that equality holds. However, first we need to explain what eAt is, i.e. the exponential of the n × n matrix A (by examining
Equation 4.11 it should be clear that if Equation 4.11 is to make sense mathematically, then eAt must be a ntimesn matrix).

Just like the exponential of a scalar, the exponential of a matrix is defined through the exponential series as follows:

     eAt  I + At + 1 A2t2 +    + 1 Antn +  ,          2!               n!

                                                        = n 1 Aiti,                                                          (4.12)
                                                          i=0 i!

where I denotes the n × n identity matrix. But we still must answer the question, "does this exponential series involving products

of matrices make mathematical sense"? Certainly we can compute products of matrices and multiply them by scalars. But we have
to give meaning to an infinite sum of such mathematical objects. We do this by defining the norm of a matrix and then considering
the convergence of the series in norm. When this is done the "convergence problem" is exactly the same as that of the exponential
of a scalar. Therefore the exponential series for a matrix converges absolutely for all t, and therefore it can be differentiated with
respect to t term-by-term, and the resulted series of derivatives also converges absolutely.

Next we need to argue that Equation 4.11 is a solution of Equation 4.10. If we differentiate the series (4.12) term by term, we
obtain that:

                                                    d eAt = AeAt = eAtA,                                                     (4.13)
                                                    dt

where we have used the fact that the matrices A and eAt commute (this is easy to deduce from the fact that A commutes with any

power of A) It then follows from this calculation that:

                                      y = d eAty0 = AeAty0 = Ay.                                                             (4.14)
                                          dt

Therefore the general problem of solving (4.10) is equivalent to computing eAt, and we now turn our attention to this task.

First, suppose that A is a diagonal matrix, say

                                                       1 0  0 
                                                    A= 0 2  0                      
                                                      0 0  0                                                                 (4.15)

                                                       0 0  n 

Then it is easy to see by substituting A into the exponential series (4.12) that:

                                                          4.2                        https://math.libretexts.org/@go/page/24151
                                       e1t 0  0 
                                      At  0 e2t  0 
                                      e = 0 0  0                                         (4.16)

                                          0 0  et 

Therefore our strategy will be to transform coordinates so that in the new coordinates A becomes diagonal (or as "close as

possible" to diagonal, which we will explain shortly). Then eAt will be easily computable in these coordinates. Once that is

accomplished, then we use the inverse of the transformation to transform the solution back to the original coordinate system.

Now we make these ideas precise. We let                                                  (4.17)

                                         y = T u, u  Rn, y  Rn,
where T is a n × n matrix whose precise properties will be developed in the following.

This is a typical approach in ODEs. We propose a general coordinate transformation of the ODE, and then we construct it in a way
that gives the properties of the ODE that we desire. Substituting (4.17) into (4.10) gives:

                                      y = T u = Ay = AT u,                               (4.18)

T will be constructed in a way that makes it invertible, so that we have:

                                      u = T -1 AT u, u(0) = T -1 y(0).                   (4.19)

To simplify the notation we let:

                                           T = T -1 AT ,                                 (4.20)

or

                                           A = T -1 T .                                  (4.21)

Substituting (4.21) into the series for the matrix exponential (4.12) gives:

                                           eAt = eT T -1t ,

                         = 1 + T T -1 t + ( 1 2! T T -1 )2t2 +  + n ( 1! T T -1 )ntn +   (4.22)

Now note that for any positive integer n we have:

                           (T T -1 )n = (T T -1 )(T T -1 )  (T T -1 )(T T -1 )

                                 

                                                                            nfactors

                                                               = T nT -1                 (4.23)

Substituting this into (4.22) gives:   eAt =
                                        = T (
                                                                1 (T T -1 )ntn ,

                                                               n=0 n!
                                                                 1 n n -1

                                                               n=0 n!  t )T ,

                                           = T etT -1                                    (4.24)

or

                                           eAt = T etT -1                                (4.25)

Now we arrive at our main result. If T is constructed so that

                                            = T -1 AT                                    (4.26)

is diagonal, then it follows from (4.16) and (4.25) that eAt can always be computed. So the ODE problem of solving (4.10)
becomes a problem in linear algebra. But can a general n × n matrix A always be diagonalized? If you have had a course in linear

algebra, you know that the answer to this question is "no". There is a theory of the (real) that will apply here. However, that would
take us into too great a diversion for this course. Instead, we will consider the three standard cases for 2 × 2 matrices. That will

                                                               4.3                       https://math.libretexts.org/@go/page/24151
suffice for introducing the the main ideas without getting bogged down in linear algebra. Nevertheless, it cannot be avoided
entirely. You will need to be able to compute eigenvalues and eigenvectors of 2 × 2 matrices, and understand their meaning.
The three cases of 2 × 2 matrices that we will consider are characterized by their eigenvalues:

    two real eigenvalues, diagonalizable A,
    two identical eigenvalues, nondiagonalizable A,
    a complex conjugate pair of eigenvalues.
In the table below we summarize the form that these matrices can be transformed in to (referred to as the of A) and the resulting
exponential of this canonical form.

Once the transformation to  has been carried out, we will use these results to deduce e .

This page titled 4: Behavior Near Trajectories - Linearization is shared under a CC BY 4.0 license and was authored, remixed, and/or curated by

Stephen Wiggins via  source content that was edited to the style and standards of the LibreTexts platform.

4.4  https://math.libretexts.org/@go/page/24151
4.1: Problem Set

 EXERCISE 4.1.1

 Suppose  is n × n matrix and T is a n × n invertible matrix. use mathematical induction to show that:
                                         (T -1 T )k = T -1 kT ,

 for all natural numbers k, i.e., k = 1, 2, 3, ...

EXERCISE 4.1.2

Suppose A is a n × n matrix. Use the exponential series to give an argument that:
 d eAt = AeAt .

 dt

(You are allowed to use eA(t+h) = eAteAh without proof, as well as the fact that A and eAt commute, without proof.)

EXERCISE 4.1.3

 Consider the following linear autonomous vector field:

x = Ax, x(0) = x0, x  Rn ,
where A is a n × n matrix of real numbers.

     Show that the solutions of this vector field exist for all time.

   Show that the solutions are infinitely differentiable with respect to the initial condition, x0.

EXERCISE 4.1.4

Consider the following linear autonomous vector field on the plane:

( x1 ) = ( 0 1 ) ( x1 )
x2  0 0 x2

(a) Describe the invariant sets.

(b) Sketch the phase portrait.

(c) Is the origin stable or unstable? Why?

EXERCISE 4.1.5

Consider the following linear autonomous vector field on the plane:

( x ) = ( 0 0 ) ( x1 )y
    0 0 x2

(a) Describe the invariant sets.

(b) Sketch the phase portrait.

(c) Is the origin stable or unstable? Why?

This page titled 4.1: Problem Set is shared under a CC BY 4.0 license and was authored, remixed, and/or curated by Stephen Wiggins via  source

content that was edited to the style and standards of the LibreTexts platform.

                                            4.1.1                                                    https://math.libretexts.org/@go/page/24152
5: Behavior Near Equilbria - Linearization                                                             (5.1)

Now we will consider several examples for solving, and understanding, the nature of the solutions, of

                                            x = Ax, x  R2.

For all of the examples, the method for solving the system is the same.

Steps for Solving

   Step 1. Compute the eigenvalues of A.
   Step 2. Compute the eigenvectors of A.
   Step 3. Use the eigenvectors of A to form the transformation matrix T .
   Step 4. Compute  = T -1 AT .
   Step 5. Compute eAt = T et T -1 .

Once we have computed eAt we have the solution of Equation 5.1 through any initial condition y0 since y(t), y(0) = y0 , is given
by y(t) = eAty0 .

Example 5.10

We consider the following linear, autonomous ODE:

                                        ( x1 ) = ( 2        1 ) ( x1 )                                 (5.2)
                                        x2             1    2 x2

where

                                                       A=(2 1),                                        (5.3)

                                                                  12

Step 1. Compute the eigenvalues of A.

The eigenvalues of A, denote by , are given by the solutions of the characteristic polynomial:

                                        det ( 2 -      1 2 -  ) = (2 - )2 - 1 = 0

                                                    1

                                                       = 2 - 4 + 3 = 0,                                (5.4)

or

                 1 ------

1,2 = 2 ± 2 16 - 12 = 3, 1

Step 2. Compute the eigenvectors of A.

For each eigenvalue, we compute the corresponding eigenvector. The eigenvector corresponding to the eigenvalue 3 is found
by solving:

                                        ( 2 1 ) ( x1 ) = 3 ( x1 ) ,                                    (5.5)
                                        1 2 x2                           x2

or

                                                       2x1 + x2 = 3x1,                                 (5.6)

                                                       x1 + 2x2 = 3x2.                                 (5.7)

Both of these equations yield the same equation since the two equations are dependent:                 (5.8)

                                              x2 = x1

                                                       .

Therefore we take as the eigenvector corresponding to the eigenvalue 3:

                                                       5.1                                      https://math.libretexts.org/@go/page/24158
                                                         (1)                                     (5.9)

                                                            1

Next we compute the eigenvector corresponding to the eigenvalue 1. This is given by a solution to the following equations:

                                                  ( 2 1 ) ( x1 ) = ( x1 ) ,                      (5.10)
                                                  1 2 x2               x2

or

                                                      2x1 + x2 = x1,                             (5.11)

                                                      x1 + 2x2 = x2.                             (5.12)

Both of these equations yield the same equation:

                                                      x2 = -x1.                                  (5.13)

Therefore we take as the eigenvector corresponding to the eigenvalue:

                                               (1)                                               (5.14)

                                                                                             -1

Step 3. Use the eigenvectors of A to form the transformation matrix T.

For the columns of T we take the eigenvectors corresponding the the eigenvalues 1 and 3:

                                                      T =( 1 1)                                  (5.15)

                                                                 -1 1

with the inverse given by:

                                                  T -1 = 1 ( 1 -1 )                              (5.16)

                                                              21 1

Step 4. Compute  = T -1 AT .

We have:

                              T -1AT = 1 ( 1 -1 ) ( 2 1 ) ( 1 1 )
                                                  21 1          12         -1 1

                                                  = 21 ( 1 -1 ) ( 1 3 )
                                                      11        -1 3

                                                      =(1 0)                                     (5.17)
                                                                                                 (5.18)
                                                             03
                                                                                                 (5.19)
Therefore, in the u1 - u2 coordinates (5.2) becomes:

                                                  ( u1 ) = ( 1 0 ) ( u1 )
                                                  u2      0 3 u2

In the u1 - u2 coordinates it is easy to see that the origin is an unstable equilibrium point.
Step 5. Compute eAt = T et T -1 .

We have:

                              eAT = 12 ( 1 -1         1 ) ( et   e3t0 ) ( 11  -1 )
                                                      1   0
                                                                               1

                              = 21 ( 1                   1 et -et
                                                          ) ( 3t 3t )
                                                      -1 1 e e

                              = ( 21 et + e3t -et + e3t et + e3t -et + e3t )

                                                         5.2                                     https://math.libretexts.org/@go/page/24158
We see that the origin is also unstable in the original x1 - x2 coordinates. It is referred to as a source, and this is characterized

by the fact that all of the eigenvalues of A have positive real part. The phase portrait is illustrated in Fig. 5.1.

                                         Figure 5.1: Phase plane of Equation 5.19. The origin is unstable-a source.

We remark this it is possible to infer the behavior of eAt as t   from the behavior of et as t   since T does not

depend on t.

Example 5.11

We consider the following linear, autonomous ODE:

                                        ( x1 ) = ( -1   -1 ) ( x1 )                                        (5.20)
                                        x2         9    -1 x2

where

                                                   A = ( -1 -1 ) ,                                         (5.21)

                                                               9 -1

Step 1. Compute the eigenvalues of A.

The eigenvalues of A, denote by , are given by the solutions of the characteristic polynomial:

              det ( -1 -                           -1 -1 -  ) = (-1 - )2 + 9 = 0 ,

                            9

                                                   = 2 + 2 + 10 = 0,                                       (5.22)

or

                                                                                        1 -----

                                   1,2 = -1 ± 2 4 - 40 = -1 ± 3i

The eigenvectors are complex, so we know it is not diagonalizable over the real numbers. What this means is that we cannot
find real eigenvectors so that it can be transformed to a form where there are real numbers on the diagonal, and zeros in the off
diagonal entries. The best we can do is to transform it to a form where the real parts of the eigenvalue are on the diagonal, and
the imaginary parts are on the off diagonal locations, but the off diagonal elements differ by a minus sign.

Step 2. Compute the eigenvectors of A.

The eigenvector of A corresponding to the eigenvector -1 - 3i is the solution of the following equations:

                                        ( -1 -1 ) ( x1 ) = (-1 - 3i) ( x1 ) ,                              (5.23)
                                        9 -1 x2                       x2

or

                                                   5.3                                          https://math.libretexts.org/@go/page/24158
                                            -x1 - x2 = -x1 - 3ix2,         (5.24)
                                            9x1 - x2 = -x2 - 3ix2.         (5.25)

A solution to these equations is given by:

                                       ( 13i ) = ( 10 ) + i ( 03 )

Step 3. Use the eigenvectors of A to form the transformation matrix T.

For the first column of T we take the real part of the eigenvector corresponding to the eigenvalue -1 - 3i , and for the second

column we take the complex part of the eigenvector:

                                            T =(1 0)                       (5.26)

                                                       03

with the inverse given by:

                                            T -1 = ( 1 1 0 )               (5.27)
                                                           03

Step 4. Compute  = T -1 AT .

                              T -1AT = 21 ( 1 1 0 ) ( -1 -1 ) ( 1 0 )
                                            03               9 -1      03

                                            =(1       10 ) ( -1  -3 )
                                                   0
                                                      39         -3

                                            = ( -1 -3 )                    (5.28)

                                                    3 -1

With  in this form, we know from the previous chapter that:  -sin3t )      (5.29)
                                                             cos3t
                                     et = e-t ( cos3t
                                               sin3t

Then we have:

                                          eAt = T etT -1 .
From this expression we can conclude that eAt  0 as t  . Hence the origin is asymptotically stable. It is referred to as a

sink and it is characterized by the real parts of the eigenvalues of A being negative. The phase plane is sketched in Fig. 5.2.

                                                      5.4                  https://math.libretexts.org/@go/page/24158
                                  Figure 5.2: Phase plane of Equation 5.20. The origin is a sink.

Example 5.12

We consider the following linear, autonomous ODE:

                                           ( x1 ) = ( -1    1 ) ( x1 )                                        (5.30)
                                                   x2  1    1 x2                                              (5.31)

where                                                                                                         (5.32)

                                                       A = ( -1 1 ) ,                                         (5.33)
                                                                                                              (5.34)
                                                                   11                                         (5.35)

Step 1. Compute the eigenvalues of A.

The eigenvalues of A, denote by , are given by the solutions of the characteristic polynomial:

                                  det ( -1 -       1 1 -  ) = (-1 - )(1 - ) - 1 = 0

                                                1

                                                       = 2 - 2 = 0,

which are

                                                                       -

                                                       1,2 = ±2

Step 2. Compute the eigenvectors of A.

                                                                             -

The eigenvector corresponding to the eigenvalue 2 is given by the solution of the following equations:

                                        (  -1 1 x1                        - x1
                                                       ) ( ) = 2 ( ) ,
                                           1 1 x2                                                         x2

or

                                                                                                       -

                                         -x1 + x2 = 2x1,

                                                                                                     -

                                          x1 + x2 = 2x2.

A solution is given by:                                                   -
corresponding to the eigenvector
                                                       x2 = (1 + 2)x1 .

                                                       5.5                                                    https://math.libretexts.org/@go/page/24158
                                             ( - 1 )

                                                                                         1 + 2
                                                                                -

The eigenvector corresponding to the eigenvalue -2 is given by the solution to the following equations:

                                               (  -1 1 x1                     - x1
                                                       ) ( ) = -2 ( ) ,                                     (5.36)
                                                  1 1 x2                            x2
                                                                                                            (5.37)
or                                                                                                          (5.38)

                                                                               -

                                                      -x1 + x2 = -2x1,

                                                                            -

                                                      x1 + x2 = -2x2.

A solution is given by:

                                                                         -

                                                      x2 = (1 - 2)x1 .

corresponding to the eigenvector:

                                                              ( - 1 )

                                                                 1 - 2

Step 3. Use the eigenvectors of A to form the transformation matrix T.

For the columns of T we take the eigenvectors corresponding the the eigenvalues 2 and -2: -        -

                                                  T = ( - 1                  - 1 )                          (5.39)

                                                             1 + 2         1 - 2

with the inverse given by:

                                                                              -
                                                              1          1 - 2      -1 )
                                               T -1 = - - ( -                                               (5.40)
                                                              22 -1 - 2              1

Step 4. Compute  = T -1 AT .We have:

                            T -1AT = - 1 (                 -     -1 ) ( -1       1 ) ( - 1           - 1 )
                                                  1 - 2
                                                                                                   1 - 2
                                                       -
                                   22 -1 - 2                       1       1     1      1 + 2

                                                           -                     -              -
                                   =- 1 (         1 - 2            -1         2          -2
                                                                         )( -
                                                              -                             -)
                                      22 -1 - 2                       1    2 + 2
                                                                                        2 - 2

                                                                -      - 0 )                                (5.41)
                                                             2
                                                                      -2
                                                      =(

                                                               0

Therefore, in the u1 - u2 coordinates (5.30) becomes:

                                                  u1               -        - 0 ) ( u1 )
                                               ( )=(             2         -2 u2
                                                                                                            (5.42)
                                                  u2
                                                                   0

The phase portrait of (5.42) is shown in 5.3.

It is easy to see that the origin is unstable for (5.42). In fig. 5.3 we see that the origin has the structure of a saddle point, and we
want to explore this idea further.

                                                                                                                                       -

In the u1 - u2 coordinates the span of the eigenvector corresponding to the eigenvalue 2 is given by u2 = 0, i.e. the u1 axis.

                                                                                                   -

The span of the eigenvector corresponding to the eigenvalue -2 is given by u1 = 0, i.e. the u2 axis. Moreover, we can see
from the form of (5.42) that these coordinate axes are invariant. The u1 axis is referred to as the unstable subspace, denoted by
Eu, and the u2 axis is referred to as the stable subspace, denoted by Es. In other words, the unstable subspace is the span of

the eigenvector corresponding to the eigenvalue with positive real part and the stable subspace is the span of the eigenvector
corresponding to the eigenvalue having negative real part. The stable and unstable subspaces are invariant subspaces with
respect to the flow generated by (5.42).

                                                                      5.6                             https://math.libretexts.org/@go/page/24158
                                                           Figure 5.3: Phase portrait of Equation 5.42.

The stable and unstable subspaces correspond to the coordinate axes in the coordinate system given by the eigenvectors. Next

we want to understand how they would appear in the original x1 - x2 coordinates. This is accomplished by transforming them

to the original coordinates using the transformation matrix (Equation 5.39).

We first transform the unstable subspace from the u1 - u2 coordinates to the x1 - x2 coordinates. In the u1 - u2 coordinates
points on the unstable subspace have coordinates (u1, 0). Acting on these points with T gives:

    T ( u1 ) = ( - 1  - 1 ) ( u1 ) = ( x1 ) ,                                                                (5.43)
    0  1 + 2          1 - 2  0  x2

which gives the following relation between points on the unstable subspace in the u1 - u2 coordinates to points in the x1 - x2

coordinates:

           u1 = x1,                                                                                          (5.44)
                                                                                                             (5.45)
                 -

       (1 + 2)u1 = x2,

or

                 -                                                                                           (5.46)

       (1 + 2)x1 = x2.

This is the equation for the unstable subspace in the x1 - x2 coordinates, which we illustrate in Fig. 5.4.

                      5.7                      https://math.libretexts.org/@go/page/24158
                                                Figure 5.4: The unstable subspace in the original coordinates.

Next we transform the stable subspace from the u1 - u2 coordinates to the x1 - x2 coordinates. In the u1 - u2 coordinates
points on the stable subspace have coordinates (0, u2). Acting on these points with T gives:

    T ( 0 ) = ( - 1  - 1 ) ( 0 ) = ( x1 ) ,                                                                     (5.47)
    u2  1 + 2        1 - 2  u2  x2

which gives the following relation between points on the stable subspace in the u1 - u2 coordinates to points in the x1 - x2

coordinates:

            u2 = x1,                                                                                            (5.48)
                                                                                                                (5.49)
                  -

        (1 - 2)u2 = x2,

or

                  -                                                                                             (5.50)

        (1 - 2)x1 = x2.

This is the equation for the stable subspace in the x1 - x2 coordinates, which we illustrate in Fig. 5.5.

                                                  Figure 5.5: The stable subspace in the original coordinates.
In Fig. 5.6 we illustrate both the stable and the unstable subspaces in the original coordinates.

                     5.8                                                                                        https://math.libretexts.org/@go/page/24158
Figure 5.6: The stable and unstable subspaces in the original coordinates.

Now we want to discuss some general results from these three examples.

For all three examples, the real parts of the eigenvalues of A were nonzero, and stability of the origin was determined by the sign of
the real parts of the eigenvalues, e.g., for example 10 the origin was unstable (the real parts of the eigenvalues of A were positive),
for example 11 the origin was stable (the real parts of the eigenvalues of A were negative), and for example 12 the origin was
unstable (A had one positive eigenvalue and one negative eigenvalue). This is generally true for all linear, autonomous vector
fields. We state this more formally.

Consider a linear, autonomous vector field on Rn:

                                                   y = Ay, y(0) = y0, y  Rn.                                       (5.51)

Then if A has no eigenvalues having zero real parts, the stability of the origin is determined by the real parts of the eigenvalues of
A. If all of the real parts of the eigenvalues are strictly less than zero, then the origin is asymptotically stable. If at least one of the
eigenvalues of A has real part strictly larger than zero, then the origin is unstable.

There is a term applied to this terminology that permeates all of dynamical systems theory.

DEFINITION 20: HYPERBOLIC EQUILIBRIUM POINT

The origin of Equation 5.51 is said to be if none of the real parts of the eigenvalues of A have zero real parts.

It follows that hyperbolic equilibria of linear, autonomous vector fields on Rn can be either sinks, sources, or saddles. The key

point is that the eigenvalues of A all have nonzero real parts.

If we restrict ourselves to two dimensions, it is possible to make a (short) list of all of the distinct canonical forms for A. These are
given by the following six 2 × 2 matrices.

The first is a diagonal matrix with real, nonzero eigenvalues ,   0 , i.e. the origin is a hyperbolic fixed point:

                                                   (  0 0  )                                                       (5.52)

In this case the orgin can be a sink if both eigenvalues are negative, a source if both eigenvalues are positive, and a saddle if the
eigenvalues have opposite sign.

The next situation corresponds to complex eigenvalues, with the real part, , and imaginary part, , both being nonzero. In this
case the equilibrium point is hyperbolic, and  sink for  < 0, and a source for  > 0. The sign of  does not influence stability:

                                                   5.9                                       https://math.libretexts.org/@go/page/24158
(  )                                                                                                            (5.53)
  -

Next we consider the case when the eigenvalues are real, identical, and nonzero, but the matrix is nondiagonalizable, i.e. two

eigenvectors cannot be found. In this case the origin is hyperbolic for   0 , and is a sink for  < 0 and a source for  > 0 :

(  1 0  )                                                                                                       (5.54)

Next we consider some cases corresponding to the origin being nonhyperbolic that would have been possible to include in the
discussion of earlier cases, but it is more instructive to explicitly point out these cases separately.

We first consider the case where A is diagonalizable with one nonzero real eigenvalue and one zero eigenvalue:

( 0)                                                                                                            (5.55)

   00

We consider the case where the two eigenvalues are purely imaginary, ±ib. In this case the origin is ref!erred to as a center.

( 0  - 0 )                                                                                                      (5.56)

For completeness, we consider the case where both eigenvalues are zero and A is diagonal.

(0 0)                                                                                                           (5.57)

   00

Finally, we want to expand on the discussion related to the geometrical aspects of Example 12. Recall that for that example the
span of the eigenvector corresponding to the eigenvalue with negative real part was an invariant subspace, referred to as the stable

subspace. Trajectories with initial conditions in the stable subspace decayed to zero at an exponential rate as t  + . The stable
invariant subspace was denoted by Es. Similarly, the span of the eigenvector corresponding to the eigenvalue with positive real

part was an invariant subspace, referred to as the unstable subspace. Trajectories with initial conditions in the unstable subspace

decayed to zero at an exponential rate as t  - . The unstable invariant subspace was denoted by Eu.

We can easily see that Equation 5.52 has this behavior when  and  have opposite signs. If  and  are both negative, then the
span of the eigenvectors corresponding to these two eigenvalues is R2, and the entire phase space is the stable subspace. Similarly,
if  and  are both positive, then the span of the eigenvectors corresponding to these two eigenvalues is R2, and the entire phase

space is the unstable subspace.

The case Equation 5.53 is similar. For that case there is not a pair of real eigenvectors corresponding to each of the complex
eigenvalues. The vectors that transform the original matrix to this canonical form are referred to as generalized eigenvectors. If

 < 0 the span of the generalized eigenvectors is R2, and the entire phase space is the stable subspace. Similarly, if  > 0 the span
of the generalized eigenvectors is R2, and the entire phase space is the unstable subspace. The situation is similar for (5.54). For
 < 0 the entire phase space is the stable subspace, for  > 0 the entire phase space is the unstable subspace.

The case in Equation 5.55 is different. The span of the eigenvector corresponding to  is the stable subspace for  < 0 , and the
unstable subspace for  > 0 The space of the eigenvector corresponding to the zero eigenvalue is referred to as the center

subspace.

For the case (5.56) there are not two real eigenvectors leading to the resulting canonical form. Rather, there are two generalized
eigenvectors associated with this pair of complex eigenvalues having zero real part. The span of these two eigenvectors is a two

dimensional center subspace corresponding to R2. An equilibrium point with purely imaginary eigenvalues is referred to as a

center.

Finally, the case in Equation 5.57 is included for completeness. It is the zero vector field where R2 is the center subspace.

We can characterize stability of the origin in terms of the stable, unstable, and center subspaces. The origin is asymptotically stable

if Eu =  and Ec = .The origin is unstable if Eu   .

This page titled 5: Behavior Near Equilbria - Linearization is shared under a CC BY 4.0 license and was authored, remixed, and/or curated by

Stephen Wiggins via  source content that was edited to the style and standards of the LibreTexts platform.

5.10                                                                                       https://math.libretexts.org/@go/page/24158
5.1: Problem Set

 EXERCISE 5.1.1

 Suppose A is a n × n matrix of real numbers. Show that if  is an eigenvalue of A with eigenvector e, then ¯ is an eigenvalue
 of A with eigenvector e¯.

EXERCISE 5.1.2

Consider the matrices:

A1 = ( 0 - w w0 ) , A2 = ( 0 w -w 0 ) , w > 0

Sketch the trajectories of the associated linear autonomous ordinary differential equations:

( x1 ) = Ai ( x1 ) , i = 1, 2
x2  x2

EXERCISE 5.1.3
 Consider the matrices:

A = ( -1 -1 )

            -9 -1

 (a) Show that the eigenvalues and eigenvectors are given by:

-1 - 3i : ( 13i ) = ( 10 ) + i ( 03 )
-1 + 3i : ( 1 -3i ) = ( 10 ) - i ( 03 )

 (b) Consider the four matrices:

T1 = ( 1 0 )

             0 -3

T2 = ( 1 0 )

             03

T3 = ( 0 1 )

             -3 0

T4 = ( 0 1 )

             30

Compute i = Ti-1 ATi, i = 1 ... 4 .

 (c) Discuss the form of T in terms of the eigenvectors of A.

EXERCISE 5.1.4

Consider the following two dimensional linear autonomous vector field:

( x1 ) = ( -2   1 ) ( x1 ) , (x1(0), x2(0)) = (x10, x20).
x2  -5          2 x2

Show that the origin is Lyapunov stable. Compute and sketch the trajectories.

                                                               5.1.1                          https://math.libretexts.org/@go/page/24159
EXERCISE 5.1.5

Consider the following two dimensional linear autonomous vector field:

( x1 ) = ( 1        2 ) ( x1 ) , (x1(0), x2(0)) = (x10, x20).
x2               2  1 x2

Show that the origin is a saddle. Compute the stable and unstable subspaces of the origin in the original coordinates, i.e. the

x1 - x2 coordinates. Sketch the trajectories in the phase plane.

EXERCISE 5.1.6

Compute eA , where

A = (  1 0  )

Hint. Write

A = (  0 0  ) + ( 0 1 0 0 )
        
          S             N

Then

A  S + N , and NS = SN.
Use the binomial expansion fo compute (S + N )n , n  1 ,

(S  +  N  )n  =     n   (  n  )  S  kN  n-k  ,
                           k
                 k=0

where

k( n ) = n!
              k!(n-k)!

and substitute the results into the exponential series.

This page titled 5.1: Problem Set is shared under a CC BY 4.0 license and was authored, remixed, and/or curated by Stephen Wiggins via  source

content that was edited to the style and standards of the LibreTexts platform.

                                                               5.1.2    https://math.libretexts.org/@go/page/24159
6: Stable and Unstable Manifolds of Equilibria

For hyperbolic equilibria of autonomous vector fields, the linearization captures the local behavior near the equilibria for the
nonlinear vector field. We describe the results justifying this statement in the context of two dimensional autonomous systems.

We consider a C r, r  1 two dimensional autonomous vector field of the following form:
                                             x = f(x, y),

                                       y = g(x, y), (x, y)  R2.                                                    (6.1)

Let t() denote the flow generated by (6.1). Suppose (x0, y0) is a hyperbolic equilibrium point of this vector field, i.e. the two

eigenvalues of the Jacobian matrix:

                                          f                   fy (x0, y0) 

                                        x (x0, y0)            y (x0, y0) g
                                        g
                                        x (x0, y0)

(x0, y0) is a source for the linearized vector field,
(x0, y0) is a sink for the linearized vector field,
(x0, y0) is a saddle for the linearized vector field.

We consider each case individually.

  In this case (x0, y0) is a source for (6.1). More precisely, there exists a neighborhood U of (x0, y0) such that for any p  U ,
  t(p) leaves U as t increases.
  In this case (x0, y0) is a sink for (6.1). More precisely, there exists a neighborhood S of (x0, y0) such that for any p  S , t(p)
  approaches (x0, y0) at an exponential rate as t increases. In this case (x0, y0) is an example of an attracting set and its basin of

    attraction is given by:

  B  t0 t(S).

    For the case of hyperbolic saddle points, the saddle point structure is still retained near the equilibrium point for nonlinear
    systems. We now explain precisely what this means. In order to do this we will need to examine (6.1) more closely. In
    particular, we will need to transform (6.1) to a coordinate system that "localizes" the behavior near the equilibrium point and
    specifically displays the structure of the linear part. We have already done this several times in examining the behavior near
    specific solutions, so we will not repeat those details.

Transforming locally near (x0, y0) in this manner, we can express (6.1) in the following form:

                                   (  ) = ( - 0 ) (  ) + ( u(, )) , ,  > 0, (, )  R2, (6.2)
                                       0   v(, )

where the Jacobian at the origin,

                                                         ( - 0 0  ) ,                                              (6.3)

reflects the hyperbolic nature of the equilibrium point. The linearization of (6.1) about the origin is given by:

                                       (  ) = ( - 0 ) (  ) , (6.4)
                                                         0 

                                       It is easy to see for the linearized system that                            (6.5)

                                                Es = (, )| = 0,

is the invariant stable subspace and

                                                         Eu = (, )| = 0,                                           (6.6)

is the invariant unstable subspace.

                                                         6.1                                    https://math.libretexts.org/@go/page/24165
We now state how this saddle point structure is inherited by the nonlinear system by stating the results of the stable and unstable
manifold theorem for hyperbolic equilibria for two dimensional nonautonomous vector fields.

First, we consider two intervals of the coordinate axes containing the origin as follows:

                                                                I  - <  < ,                                                (6.7)

and

                                           I  - <  < ,                                                                     (6.8)
for some small  > 0 . A neighborhood of the origin is constructed by taking the cartesian product of these two intervals:

                                                      B  {(, )  R2|(, )  I × I},                                           (6.9)

and it is illustrated in Fig. 6.1. The stable and unstable manifold theorem for hyperbolic equilibrium points of autonomous vector
fields states the following.

There exists a C r curve, given by the graph of a function of the  variables:

                                                                 = S(),   I,                                               (6.10)

This curve has three important properties.

It passes through the origin, i.e. S(0) = 0.

It  is  tangent  to  Es  at  the  origin,  i.e.,  dS (0) = 0 .
                                                  d

It is locally invariant in the sense that any trajectory starting on the curve approaches the origin at an exponential rate as t  ,
and it leaves B as t  - .

Moreover, the curve satisfying these three properties is unique. For these reasons, this curve is referred to as the local stable
manifold of the origin, and it is denoted by:

                                  Wlsoc((0, 0)) = {(, )  B| = S()}.                                                        (6.11)
Similarly, there exists another C r curve, given by the graph of a function of the  variables:                             (6.12)

                                            = U(),   I,

This curve has three important properties.

It passes through the origin, i.e. U(0) = 0.

It  is  tangent  to  Eu  at  the  origin,  i.e.,  dU  (0) = 0.
                                                  d

It is locally invariant in the sense that any trajectory starting on the curve approaches the origin at an exponential rate as t  - ,
and it leaves B as t  .

For these reasons, this curve is referred to as the local unstable manifold of the origin, and it is denoted by:

                                                      Wluoc((0, 0)) = {(, )  B| = S()}.                                    (6.13)

The curve satisfying these three properties is unique.

                                                                6.2                             https://math.libretexts.org/@go/page/24165
              Figure 6.1: The neighborhood of the origin, B , showing the local stable and unstable manifolds

These local stable and unstable manifolds are the "seeds" for the global stable and unstable manifolds that are defined as follows:

               W s((0, 0))  t(Wlsoc((0, 0))),                                                                  (6.14)
                              t0

and

               W u((0, 0))  t(Wluoc((0, 0))),                                                                  (6.15)
                              t0

Now we will consider a series of examples showing how these ideas are used.

Example 6.13

We consider the following autonomous, nonlinear vector field on the plane:                                     (6.16)

                                              x = x ,
                                       y = y + x2, (x, y)  R2.

This vector field has an equilibrium point at the origin, (x, y) = (0, 0). The Jacobian of the vector field evaluated at the origin is
given by:

              (1 0 ).                                                                                          (6.17)

                 0 -1

From this calculation we can conclude that the origin is a hyperbolic saddle point. Moreover, the x-axis is the unstable
subspace for the linearized vector field and the y axis is the stable subspace for the linearized vector field.

Next we consider the nonlinear vector field (6.16). By inspection, we see that the y axis (i.e. x = 0) is the global stable
manifold for the origin. We next consider the unstable manifold. Dividing the second equation by the first equation in (6.16)
gives:

              y = dy = - y + x.                                                                                (6.18)
              x dx x

              6.3                                                            https://math.libretexts.org/@go/page/24165
This is a linear nonautonomous equation. A solution of this equation passing through the origin is given by:  (6.19)

                                    y = x , 2
                                                                                                 3

It is also tangent to the unstable subspace at the origin. It is the global unstable manifold.

We examine this statement further. It is easy to compute the flow generated by (6.16). The x-component can be solved and

substituted into the y component to yield a first order linear nonautonomous equation. Hence, the flow generated by (6.16) is

given by:

                                        x(t, x0) = x0et ,
                                        y(t, t0) = (y0 - x ) 20 e-t + x20 e2t,
                                                             3            3                                   (6.20)

The global unstable manifold of the origin is the set of initial conditions having the property that the trajectories through these
initial conditions approach the origin at an exponential rate as t  - . On examining the two components of (6.20), we see
that the x component approaches zero as t  - for any choice of x0. However, the y component will only approach zero as
t  - if y0 and x0 are chosen such that
                                                             y0 = x , 20
                                                                  3                                           (6.21)

Hence (6.21) is the global unstable manifold of the origin.

Example 6.14

Consider the following nonlinear autonomous vector field on the plane:                                        (6.22)

                                   x = x - x3 ,
                                y = -y, (x, y)  R2.

Note that the x and y components evolve independently.

The equilibrium points and the Jacobians associated with their linearizations are given as follows:

                                        (x, y) = (0, 0); ( 1 0 ) ; saddle                                     (6.23)
                                                                   0 -1

                                        (x, y) = (±1, 0); ( -2 0 ) ; sinks                                    (6.24)
                                                                        0 -1

We now compute the global stable and unstable manifolds of these equilibria. We begin with the saddle point at the origin.

                                        W s((0, 0)) = {(x, y)|x = 0}

              W u((0, 0)) = {(x, y)| - 1 < x < 1, y = 0}                                                      (6.25)

For the sinks the global stable manifold is synonomous with the basin of attraction for the sink.

               (1, 0) : W s((1, 0)) = {(x, y)|x > 0}                                                          (6.26)
              (-1, 0) : W s((-1, 0)) = {(x, y)|x < 0}                                                         (6.27)

                                                             6.4                                   https://math.libretexts.org/@go/page/24165
              Figure 6.2: Invariant manifold structure of (6.22). The black dots indicate equilibrium points.

Example 6.15

In this example we consider the following nonlinear autonomous vector field on the plane:                      (6.28)

                                    x = -x ,                                                                   (6.29)
                              y = y2(1 - y2), (x, y)  R2.                                                      (6.30)

Note that the x and y components evolve independently.

The equilibrium points and the Jacobians associated with their linearizations are given as follows:

                                (x, y) = (0, 0), (0, ±1)

                         (x, y) = (0, 0); ( -1 0 ) ; nothyperbolic
                                                                                           00

              (x, y) = (0, 1); ( -1 0 ) ; sink                                                                 (6.31)
                                           0 -2

              (x, y) = (0, -1); ( -1 0 ) ; saddle                                                              (6.32)
                                              02

We now compute the global invariant manifold structure for each of the equilibria, beginning with (0, 0).      (6.33)

                              W s((0, 0)) = {(x, y)|y = 0}
                         W u((0, 0)) = {(x, y)| - 1 < y < 1, x = 0}

The x-axis is clearly the global stable manifold for this equilibrium point. The segment on the y-axis between -1 and 1 is
invariant, but it does not correspond to a hyperbolic direction. It is referred to as the center manifold of the origin, and we will
learn much more about invariant manifolds associated with nonhyperbolic directions later.

The equilibrium point (0, 1) is a sink. Its global stable manifold (basin of attraction) is given by:          (6.34)

                              W s((0, 1)) = {(x, y)|y > 0}

              6.5                                                                                      https://math.libretexts.org/@go/page/24165
The equilibrium point (0, -1) is a saddle point with global stable and unstable manifolds given by:                             (6.35)

                                    W s((0, -1)) = {(x, y)|y = -1}
                              W u((0, -1)) = {(x, y)| -  < y < 0, x = 0}

                               Figure 6.3: Invariant manifold structure of (6.28). The black dots indicate equilibrium points.  (6.36)
                                                                                                                                (6.37)
Example 6.16
 In this example we consider the following nonlinear autonomous vector field on the plane:                                      (6.38)
                                                                                                                                (6.39)
                                               x = y ,                                                                          (6.40)
                                   y = x - x3 - y, (x, y)  R2,  > 0,
where  > 0 is to be viewed as a parameter. The equilibrium points are given by:

                                        (x, y) = (0, 0), (±1, 0).

We want to classify the linearized stability of the equilibria. The Jacobian of the vector field is given by:

                                          A = ( 1 - 3x2 0 1 - ) ,

and the eigenvalues of the Jacobian are:

                                           ± = -  ± 1    --2---------2
                                                       22
                                                          + 4 - 12x .

We evaluate this expression for the eigenvalues at each of the equilibria to determine their linearized stability.

                                           (0, 0); ± = -  ± 1      --2---
                                                                 22
                                                                    +4

                                          6.6                              https://math.libretexts.org/@go/page/24165
Note that

                  2 +4 > 2

therefore the eigenvalues are always real and of opposite sign. This implies that (0, 0) is a saddle.

                              1                                  -----                                 (6.41)

                  (±1, 0); ± = - 2 ± 2                              2

                                                                  -8

First, note that

                  2 -8 < 2 .

This implies that these two fixed points are always sinks. However, there are two subcases.

2 - 8 < 0 : The eigenvalues have a nonzero imaginary part.
2 - 8 > 0 : The eigenvalues are purely real.

In fig. 6.4 we sketch the local invariant manifold structure for these two cases.

        Figure 6.4: Local invariant manifold structure of (6.36). The black dots indicate equilibrium points. (a) 2 - 8 < 0 , (b)
         2 - 8 > 0

In fig. 6.5 we sketch the global invariant manifold structure for the two cases. In the coming lectures we will learn how we can
justify this figure. However, note the role that the stable manifold of the saddle plays in defining the basins of attractions of the
two sinks.

                                                            6.7                              https://math.libretexts.org/@go/page/24165
           Figure 6.5: A sketch of the global invariant manifold structure of (6.36). The black dots indicate equilibrium points. (a)
           2 - 8 < 0 , (b) 2 - 8 > 0

This page titled 6: Stable and Unstable Manifolds of Equilibria is shared under a CC BY 4.0 license and was authored, remixed, and/or curated by

Stephen Wiggins via  source content that was edited to the style and standards of the LibreTexts platform.

6.8  https://math.libretexts.org/@go/page/24165
6.1: Problem Set

 EXERCISE 6.1.1

 Consider the C r, r  1 , autonomous vector field on R2:
                                                 x = f(x),

  with flow

                                                   t (),
 and let x = x¯ denote a hyperbolic saddle type equilibrium point for this vector field. We denote the local stable and unstable

  manifolds of this equilibrium point by:

                                              Wlsoc(x¯), Wluoc(x¯),
 respectively. The global stable and unstable manifolds of x¯ are defined by:

                                          W s(x¯)  t0 t(Wlsoc(x¯)) ,
                                         W u(x¯)  t0 t(Wlsoc(x¯)) ,
 (a) Show that W s(x¯) and W u(x¯) are invariant set.
 (b) Suppose that p  W s(x¯) , show that t(p)  x¯ at an exponential rate as t  .
 (c) Suppose that p  W u(x¯), show that t(p)  x¯ at an exponential rate as t  - .

 EXERCISE 6.1.2

 Consider the C r, r  1, autonomous vector field on R2 having a hyperbolic saddle point. Can its stable and unstable manifolds

  intersect at an isolated point (which is not a fixed point of the vector field) as shown in figure 2?

6.1.1  https://math.libretexts.org/@go/page/24166
EXERCISE 6.1.3

Consider the following autonomous vector field on the plane:
x = x ,

                                                 y = y + xn+1,                               (6.1.1)

where  < 0,  > 0,  is a real number, and n is a positive integer.

1. Show that the origin is a hyperbolic saddle point.
2. Compute and sketch the stable and unstable subspaces of the origin.
3. Show that the stable and unstable subspaces are invariant under the linearized dynamics.
4. Show the the flow generated by this vector field is given by:

x(t, x0) = x0et ,

                t x0n+1  x0n+1                   (n+1)t
y(t, x0 , y0 ) = e (y0 - (n+1)- ) + ( (n+1)- )e

5. Compute the global stable and unstable manifolds of the origin from the flow.
6. Show that the global stable and unstable manifolds that you have computed are invariant.
7. Sketch the global stable and unstable manifolds and discuss how they depend on g and n.

                                                         6.1.2                               https://math.libretexts.org/@go/page/24166
 EXERCISE 6.1.4

 Suppose x = f(x), x  Rn is a C r vector field having a hyperbolic fixed point, x = x0 , with a homoclinic orbit. Describe the
 homoclinic orbit in terms of the stable and unstable manifolds of x0.

 EXERCISE 6.1.5

 Suppose x = f(x), x  Rn is a C r vector field having hyperbolic fixed points, x = x0 and x1, with a heteroclinic orbit
 connecting x0 and x1. Describe the heteroclinic orbit in terms of the stable and unstable manifolds of x0 and x1.

This page titled 6.1: Problem Set is shared under a CC BY 4.0 license and was authored, remixed, and/or curated by Stephen Wiggins via  source

content that was edited to the style and standards of the LibreTexts platform.

6.1.3  https://math.libretexts.org/@go/page/24166
CHAPTER OVERVIEW

7: Lyapunov's Method and the LaSalle Invariance Principle

  7.1: Lyapunov's Method and the LaSalle Invariance Principle
  7.2: Problem Set
This page titled 7: Lyapunov's Method and the LaSalle Invariance Principle is shared under a CC BY 4.0 license and was authored, remixed,
and/or curated by Stephen Wiggins via  source content that was edited to the style and standards of the LibreTexts platform.

                                                                                              1
7.1: Lyapunov's Method and the LaSalle Invariance Principle

We will next learn a method for determining stability of equilibria which may be applied when stability information obtained from
the linearization of the ODE is not sufficient for determining stability information for the nonlinear ODE. The book by LaSalle is
an excellent supplement to this lecture. This is Lyapunov's method (or Lyapunov's second method, or the method of Lyapunov
functions). We begin by describing the framework for the method in the setting that we will use.

We consider a general C r, r  1 autonomous ODE

                                                      x = f(x), x  R,                        (7.1.1)

having an equilibrium point at x = x¯ , i.e.,

                                                              f(x¯) = 0,                     (7.1.2)
                                                             V : Rn  R ,
For a scalar valued function defined on Rn

                                                             x  V (x),                       (7.1.3)

we define the derivative of Equation 7.1.3 along trajectories of Equation 7.1.1 by:

                                               d   V  (x  )  =  V(x  )  =  V  (x  )    x  ,
                                               dt

                                                             V (x)  f(x),                    (7.1.4)

We can now state Lyapunov's theorem on stability of the equilibrium point x = x¯ .

Theorem 1

Consider the following C r(r  1) autonomous vector field on Rn:

                                                      x = f(x), x  Rn,                       (7.1.5)

                                                                     .

Let x = x¯ be a fixed point of Equation 7.1.5 and let V : U  R be a C 1 function defined in some neighborhood U of x¯ such

that:

1. V (x¯) = 0 and V (x) > 0 if x  x¯ .
2. V (x)  0 in U - x¯

Then x¯ is Lyapunov stable. Moreover, if

                                                      V(x) < 0 in U - x¯

then x¯ is asymptotically stable.

The function V (x) is referred to as a Lyapunov function. We now consider an example.

Example 7.1.17

Consider this function

                                                                x = y ,

                                               y = -x - x2y, (x, y)  R2,                     (7.1.6)

where  is a parameter. It is clear that (x, y) = (0, 0) is an equilibrium point of Equation 7.1.6 and we want to determine the

nature of its stability.

We begin by linearizing Equation 7.1.6 about this equilibrium point. The matrix associated with this linearization is given by:

                                                      A=( 0 1),                              (7.1.7)

                                                                 -1 0

                                                                7.1.1                        https://math.libretexts.org/@go/page/24172
and its eigenvalues are ±i. Hence, the origin is not hyperbolic and therefore the information provided by the linearization of

(7.6) about (x, y) = (0, 0)does not provide information about stability of (x, y) = (0, 0) for the nonlinear system (Equation
7.1.6).

Therefore we will attempt to apply Lyapunov's method to determine stability of the origin.

We take as a Lyapunov function:

                                          V (x, y) = ( 1 x2 + y2).                                                                                    (7.1.8)
                                                                                                                                                   2  (7.1.9)

Note that V(0,0) = 0 and V(x,y) > 0 in any neighborhood of the origin.
Moreover, we have:

                                      V(x, y) = V x + V y
                                                x y

                                              = xy + y(-x - x2y)
                                             = -x2y2
                                               0, for > 0.

Hence, it follows from Theorem 1 that the origin is Lyapunov stable.

Next we will introduce the LaSalle invariance principle. Rather than focus on the particular question of stability of an equilibrium

solution as in Lyapunov's method, the LaSalle invariance principle gives conditions that describe the behavior as t   of all

solutions of an autonomous ODE.

We begin with an autonomous ODE defined on Rn:

                                                        M x = f(x), x  Rn,                                                                            (7.1.10)

where f(x) is C r, (r  1). Let t() denote the flow generated by Equation 7.1.10 and let  Rn denote a positive invariant set

that is compact (i.e. closed and bounded in this setting). Suppose we have a scalar valued function

                                                        V : Rn  R,                                                                                    (7.1.11)

such that                                               V(x)  0 in M,                                                                                 (7.1.12)
(Note the "less than or equal to" in this inequality.)

M Let
                                                        E = {x  |V(x) = 0},                                                                           (7.1.13)

and

     M = {the union of all trajectories that start in E and remain in E for allt  0}                                                                  (7.1.14)

Now we can state LaSalle's invariance principle.

M Theorem 2: LaSalle's Invariance Principle

For all x  , t(x)  M as t  .

We will now consider an example.

Example 7.1.18

Consider the following vector field on R2 : x = y ,

                                                        x = y ,

                                                        7.1.2                                                                                         https://math.libretexts.org/@go/page/24172
                                    y = x - x3 - y, (x, y)  R2,  > 0.                                                (7.1.15)
This vector field has three equilibrium points-a saddle point at (x, y) = (0, 0) and two sinks at (x, y) = (±1, 0).

Consider the function

                                         V (x, y) = y - + , 2 x2 x4                                                  (7.1.16)

                                                           224

and its level sets:

                                                         V (x, y) = C.

We compute the derivative of V along trajectories of (7.15):

                                         V(x, y) = V x + V y,
                                                              x      y

                                         = (-x + x3)y + y(x - x3 - y) .

                                                         = -y2,                                                      (7.1.17)

from which it follows that

                       M V(x, y)  0 on V (x, y) = C .                                                                , containing the

Therefore, for C sufficiently large, the corresponding level set of V bounds a compact positive invariant set,
three equilibrium points of Equation 7.1.15.

Next we determine the nature of the set

                                         E = {(x, y)  M|V(x, y) = 0}.                                                (7.1.18)
                                                                                                                     (7.1.19)
Using Equation 7.1.17we see that:        E = {(x, y)  M|y = 0  M}.

The only points in E that remain in E for all time are:

                         M M = {(±1, 0), (0, 0)}.                                                                                   (7.1.20)

Therefore it follows from Theorem 2 that given any initial condition in  , the trajectory starting at that initial condition

approaches one of the three equilibrium points as t  .

Autonomous Vector Fields on the Plane; Bendixson's Criterion and the Index Theorem

Now we will consider some useful results that apply to vector fields on the plane.

First we will consider a simple, and easy to apply, criterion that rules out the existence of periodic orbits for autonomous vector
fields on the plane (e.g., it is not valid for vector fields on the two torus).

We consider a C r, r  1 vector field on the plane of the following form:
                                                x = f(x, y),

                                         y = g(x, y), (x, y)  R2                                                     (7.1.21)

The following criterion due to Bendixson provides a simple, computable condition that rules out the existence of periodic orbits in

certain regions of R2.

Theorem 3: Bendixson's Critterion

If on a simply connected region D  R2 the expression

                                         f (x, y) + g (x, y),                                                        (7.1.22)
                                         x                       y

is not identically zero and does not change sign then (7.21) has no periodic orbits lying entirely in D.

                                                              7.1.3                 https://math.libretexts.org/@go/page/24172
Example 7.1.19                                                                             (7.1.23)
                                                                                           (7.1.24)
 We consider the following nonlinear autonomous vector field on the plane:

                                           x = y  f(x, y),
                               y = x - x3 - y  g(x, y), (x, y)  R2, d > 0.

 Computing (7.22) gives:

                                            f + g = -,
                                            x y
Therefore this vector field has no periodic orbits for   0 .

Example 7.1.20                                                                             (7.1.25)
                                                                                           (7.1.26)
 We consider the following linear autonomous vector field on the plane:

                                        x = ax + by  f(x, y) ,
                              y = cx + dy  g(x, y), (x, y)  R2, a, b, c, d  R

 Computing (7.22) gives:

                                           f + g = a + d,
                                           x y
Therefore for a + d  0 this vector field has no periodic orbits.

Next we will consider the index theorem. If periodic orbits exist, it provides conditions on the number of fixed points, and their
stability, that are contained in the region bounded by the periodic orbit.

 Theorem 4: index theorem

  Inside any periodic orbit there must be at least one fixed point. If there is only one, then it must be a sink, source, or center. If
  all the fixed points inside the periodic orbit are hyperbolic, then there must be an odd number, 2n + 1, of which n are saddles,
  and n + 1 are either sinks or sources.

Example 7.1.21                                                                             (7.1.27)
                                                                                           (7.1.28)
 We consider the following nonlinear autonomous vector field on the plane:                 (7.1.29)

                                           x = y  f(x, y),
                               y = x - x3 - y + x2y  g(x, y), (x, y)  R2,
where  > 0 . The equilibrium points are given by:

                                                                (x, y) = (0, 0), (±1, 0).
 The Jacobian of the vector field, denoted by A, is given by:

A = ( 1 - 3x2 + 2xy - + x2 0 1 ) ,

Using the general expression for the eigenvalues for a 2 × 2 matrix A:

                               trA 1 -----2-------

                                   1,2 = 2 ± 2 (trA) - 4detA ,

we obtain the following expression for the eigenvalues of the Jacobian

 1,2 = - + x ± 2 1------------------------
2  2
                  (- + x2)2 + 4(1 - 3x2 + 2xy)

                    7.1.4                                                                  https://math.libretexts.org/@go/page/24172
If we substitute the locations of the equilibria into this expression we obtain the following values for the eigenvalues of the
Jacobian of the vector field evaluated at the respective equilibria:

 (0, 0)1,2 = -  ± 1      --2---                                                                (7.1.30)
                       22
                          +4

 (±1, 0)1,2 = - + 1 ± 1-----------                                                             (7.1.31)
2         2
                       (- + 1)2 - 8

From these expressions we conclude that (0, 0) is a saddle for all values of  and (±1, 0) are

sinks for  > 1

centers for  = 1

sources for 0 <  < 1

Now we will use Bendixson's criterion and the index theorem to determine regions in the phase plane where periodic orbits
may exist. For this example (7.22) is given by:

- + x2.                                                                                        (7.1.32)

Hence the two vertical lines x = - and x =  divide the phase plane into three regions where periodic orbits cannot exist

entirely in one of these regions (or else Bendixson's criterion would be violated). There are two cases to be considered for the

location of these vertical lines with respect to the equilibria:  > 1 and 0 <  < 1 .

In Fig. 7.1 we show three possibilities (they do not exhaust all possible cases) for the existence of periodic orbits that would

satisfy Bendixson's criterion in the case  > 1 . However, (b) is not possible because it violates the index theorem.

   7.1.5                                                                                       https://math.libretexts.org/@go/page/24172
        Figure 7.1: The case  > 1. Possibilities for periodic orbits that satisfy Bendixson's criterion. However, (b) is not possible
        because it violates the index theorem.

In Fig. 7.2 we show three possibilities (they do not exhaust all possible cases) for the existence of periodic orbits that would
satisfy Bendixson's criterion in the case 0 <  < 1 . However, (e) is not possible because it violates the index theorem.

7.1.6  https://math.libretexts.org/@go/page/24172
           Figure 7.2: The case 0 <  < 1 . Possibilities for periodic orbits that satisfy Bendixson's criterion. However, (e) is not possible
           because it violates the index theorem.

This page titled 7.1: Lyapunov's Method and the LaSalle Invariance Principle is shared under a CC BY 4.0 license and was authored, remixed,

and/or curated by Stephen Wiggins via  source content that was edited to the style and standards of the LibreTexts platform.

7.1.7  https://math.libretexts.org/@go/page/24172
7.2: Problem Set

 EXERCISE 7.2.1

  Consider the following autonomous vector field on the plane:

                                                              x = y ,

                                     y = x - x3 - y,   0, (x, y)  R2 .

  Use Lyapunov's method to show that the equilibria (x, y) = (±1, 0) are Lyapunov stable for  = 0 and asymptotically stable
  for  > 0 .

EXERCISE 7.2.2

 Consider the following autonomous vector field on the plane:

                                                             x = y ,

                                     y = -x - x2y,  > 0, (x, y)  R2 .

 Use the LaSalle invariance principle to show that
                                                                      (x, y) = (0, 0),

 is asymptotically stable.

EXERCISE 7.2.3

 Consider the following autonomous vector field on the plane:

                                                             x = y ,

                                   y = x - x3 - x2y, a > 0, (x, y)  R2 ,

 use the LaSalle invariance principle to describe the fate of all trajectories as t  .

EXERCISE 7.2.4

 Consider the following autonomous vector field on the plane:

                                                             x = y ,

                                      y = x - x3 + xy, (x, y)  R2 ,

 where  is a real parameter. Determine the equilibria and discuss their linearized stability as a function of .

EXERCISE 7.2.5

Consider the following autonomous vector field on the plane:

x = ax + by ,

                         y = cx + dy, (x, y)  R2,                                                                (7.2.1)

where a, b, c, d  R. In the questions below you are asked to give conditions on the constants a, b, c, and d so that particular

dynamical phenomena are satisfied. You do not have to give all possible conditions on the constants in order for the dynamical
condition to be satisfied. One condition will be sufficient, but you must justify your answer.

Give conditions on a, b, c, d for which the vector field has no periodic orbits.
Give conditions on a, b, c, d for which all of the orbits are peri- odic.
Using

V (x, y) = 12 (x2 + y2)

                         7.2.1                                                          https://math.libretexts.org/@go/page/24173
as a Lyapunov function, give conditions on a, b, c, d for which (x, y) = (0, 0) is asymptotically stable.

Give conditions on a, b, c, d for which x = 0 is the stable man- ifold of (x,y) = (0,0) and y = 0 is the unstable manifold of (x,
y) = (0, 0).

EXERCISE 7.2.6

Consider the following autonomous vector field on the plane:      (7.2.2)

x = y ,

                                        y = x - x2y, (x, y)  R2.

Determine the linearized stability of (x, y) = (0, 0).
Describe the invariant manifold structure for the linearization of (7.35) about (x, y) = (0, 0).

Using V (x, y) = 12 (x2 + y2) as a Lyapunov function, what can you conclude about the stability of the origin? Does this

agree with the linearized stability result obtained above? Why or why not?

Using the LaSalle invariance principle, determine the fate of a trajectory starting at an arbitrary initial condition as t  ?

What does this result allow you to conclude about stability of (x, y) = (0, 0)?

This page titled 7.2: Problem Set is shared under a CC BY 4.0 license and was authored, remixed, and/or curated by Stephen Wiggins via  source

content that was edited to the style and standards of the LibreTexts platform.

                7.2.2                                             https://math.libretexts.org/@go/page/24173
CHAPTER OVERVIEW

8: Bifurcation of Equilibria I

  8.1: Bifurcation of Equilibria I
  8.2: Problem Set
This page titled 8: Bifurcation of Equilibria I is shared under a CC BY 4.0 license and was authored, remixed, and/or curated by Stephen Wiggins
via  source content that was edited to the style and standards of the LibreTexts platform.

                                                                                              1
8.1: Bifurcation of Equilibria I

We will now study the topic of bifurcation of equilibria of autonomous vector fields, or `'what happens as an equilibrium point
loses hyperbolicity as a parameter is varied?" We will study this question through a series of examples, and then consider what the
examples teach us about the `'general situation" (and what this might be).

ExAMPLE 8.1.22: The Saddle-Node Bifurcation

Consider the following nonlinear, autonomous vector field on R2:

                                                            x = x2 ,

                                             y = y, (x, y)  R2,              (8.1.1)

where  is a (real) parameter. The equilibrium points of (8.1) are given by:

(x, y) = (-, 0), (--, 0).                                                    (8.1.2)

It is easy to see that there are no equilibrium points for  < 0 , one equilibrium point for  = 0 , and two equilibrium points for
 >0.

The Jacobian of the vector field evaluated at each equilibrium point is given by:

  - -2-                                                               0 ),   (8.1.3)

(, 0) : (                                                             -1

                       0

from which it follows that the equilibria are hyperbolic and asymptotically stable for  > 0 , and nonhyperbolic for  = 0 .

(--, 0) : ( 2- 0 )                                                           (8.1.4)

                        01

from which it follows that the equilibria are hyperbolic saddle points for  > 0 , and nonhyperbolic for  = 0 . We emphasize
again that there are no equilibrium points for  < 0 .

As a result of the "structure" of (8.1) we can easily represent the behavior of the equilibria as a function of  in a bifurcation

diagram. That is, since the x and y components of (8.1) are "decoupled", and the change in the number and stability of

equilibria us completely captured by the x coordinates, we can plot the x component of the vector field as a function of , as

we show in Fig. 8.1.

       Figure 8.1: Bifurcation diagram for (8.1) in the  - x plane. The curve of equilibria is given by  = x . The dashed line

        denotes the part of the curve corresponding to unstable equilibria, and the solid line denotes the part of the curve corresponding
        to stable equilibria.

In Fig. 8.2 we illustrate the bifurcation of equilibria for (8.1) in the x - y plane.

                                             8.1.1                           https://math.libretexts.org/@go/page/24179
              Figure 8.2: Bifurcation diagram for (8.1) in the x - y plane for  < 0,  = 0, and  > 0. Compare with Fig. 8.1.

This type of bifurcation is referred to as a saddle-node bifurcation (occasionally it may also be referred to as a fold bifurcation
or tangent bifurcation, but these terms are used less frequently).

The key characteristic of the saddle-node bifurcation is the following. As a parameter () is varied, the number of equilibria

change from zero to two, and the change occurs at a parameter value corresponding to the two equilibria coalescing into one
nonhyperbolic equilibrium.

 is called the bifurcation parameter and  = 0 is called the bifurcation point.

8.1.2  https://math.libretexts.org/@go/page/24179
Example 8.1.23 (The Transcritical Bifurcation)

Consider the following nonlinear, autonomous vector field on R2:

                                                         x = x - x2 ,

                                                y = y, (x, y)  R2,                 (8.1.5)

where  is a (real) parameter. The equilibrium points of (8.5) are given by:

                                                (x, y) = (0, 0), (, 0).            (8.1.6)

The Jacobian of the vector field evaluated at each equilibrium point is given by:

                                                (0, 0) (  0 )                      (8.1.7)

                                                             0 -1

                                                (, 0) ( - 0 )                      (8.1.8)

                                                              01

from which it follows that (0, 0) is asymptotically stable for  < 0 , and a hyperbolic saddle for  > 0 , and (, 0) is a
hyperbolic saddle for  < 0 and asymptotically stable for  > 0 . These two lines of fixed points cross at  = 0 , at which there

is only one, nonhyperbolic fixed point.

In Fig. 8.3 we show the bifurcation diagram for (8.5) in the  - x plane.

       Figure 8.3: Bifurcation diagram for (8.5) in the  - x plane. The curves of equilibria are given by  = x and x = 0. The

        dashed line denotes unstable equilibria, and the solid line denotes stable equilibria.

In Fig. 8.4 we illustrate the bifurcation of equilibria for (8.5) in the x - y plane for  < 0 ,  = 0 , and  > 0 .

                                                8.1.3                              https://math.libretexts.org/@go/page/24179
              Figure 8.4: Bifurcation diagram for (8.1) in the x - y plane for  < 0,  = 0, and  > 0. Compare with Fig. 8.3.

This type of bifurcation is referred to as a transcritical bifurcation.

The key characteristic of the transcritical bifurcation is the following. As a parameter () is varied, the number of equilibria

change from two to one, and back to two, and the change in number of equilibria occurs at a parameter value corresponding to
the two equilibria coalescing into one nonhyperbolic equilibrium.

ExAMPLE 8.1.24 (The (Supercritical) Pitchfork Bifurcation).             (8.1.9)

Consider the following nonlinear, autonomous vector field on R2:

                                                          x = x - x3 ,

                                           y = -y, (x, y)  R2,

8.1.4                                                                   https://math.libretexts.org/@go/page/24179
where  is a (real) parameter. The equilibrium points of (8.9) are given by:

(x, y) = (0, 0), (-, 0), (--, 0)                                                   (8.1.10)

The Jacobian of the vector field evaluated at each equilibrium point is given by:

(0, 0) (  0 )                                                                      (8.1.11)

             01

-(±, 0) ( -2 0   )                                                                 (8.1.12)

          01

from which it follows that (0, 0) is asymptotically stable for  < 0 , and a hyperbolic saddle for  > 0 , and (±-, 0) are
asymptotically stable for  > 0 , and do not exist for  < 0 . These two curves of fixed points pass through zero at  = 0 , at

which there is only one, nonhyperbolic fixed point.

In Fig. 8.5 we show the bifurcation diagram for (8.9) in the  - x plane.

       Figure 8.5: Bifurcation diagram for (8.9) in the  - x plane. The curves of equilibria are given by  = x2 , and x = 0. The

        dashed line denotes unstable equilibria, and the solid line denotes stable equilibria.

In Fig. 8.6 we illustrate the bifurcation of equilibria for (8.9) in the x - y plane for  < 0 ,  = 0 , and  > 0 .

   8.1.5                                                                           https://math.libretexts.org/@go/page/24179
Figure 8.6: Bifurcation diagram for (8.9) in the x - y plane for  < 0,  = 0, and  > 0. Compare with Fig. 8.5.

Example 8.1.25 (The (Subcritical) Pitchfork Bifurcation)                                                       (8.1.13)
                                                                                                               (8.1.14)
Consider the following nonlinear, autonomous vector field on R2:

 x = x + x3 ,

                                            y = y, (x, y)  R2,

 where  is a (real) parameter. The equilibrium points of (8.9) are given by:
                                               (x, y) = (0, 0), (-, 0), (--, 0).

 The Jacobian of the vector field evaluated at each equilibrium point is given by:

8.1.6                                                                               https://math.libretexts.org/@go/page/24179
(0, 0) (  0 )         (8.1.15)

             0 -1

-(±, 0) ( -2 0     )  (8.1.16)

          01

from which it follows that (0, 0) is asymptotically stable for  < 0 , and a hyperbolic saddle for  > 0 , and (±-, 0) are
hyperbolic saddles for  < 0 , and do not exist for  > 0 . These two curves of fixed points pass through zero at  = 0 , at
which there is only one, nonhyperbolic fixed point. In Fig. 8.7 we show the bifurcation diagram for (8.13) in the  - x plane.

       Figure 8.7: Bifurcation diagram for (8.13) in the  - x plane. The curves of equilibria are given by  = x2 , and x = 0. The

        dashed curves denotes unstable equilibria, and the solid line denotes stable equilibria.

In Fig. 8.8 we illustrate the bifurcation of equilibria for (8.9) in the x - y plane for  < 0 ,  = 0 , and  > 0 .

   8.1.7              https://math.libretexts.org/@go/page/24179
                Figure 8.8: Bifurcation diagram for (8.13) in the x - y plane for  < 0,  = 0, and  > 0. Compare with Fig. 8.7.

We note that the phrase supercritical pitchfork bifurcation is also referred to as a soft loss of stability and the phrase subcritical
pitchfork bifurcation is referred to as a hard loss of stability. What this means is the following. In the supercritical pitchfork

bifurcation as  goes from negative to positive the equilibrium point loses stability, but as  increases past zero the trajectories near

the origin are bounded in how far away from the origin they can move. In the subcritical pitchfork bifurcation the origin loses

stability as  increases from negative to positive, but trajectories near the unstable equilibrium can become unbounded.

It is natural to ask the question,

"what is common about these three examples of bifurcations of fixed points of one dimensional autonomous vector fields?" We
note the following.

    A necessary (but not sufficient) for bifurcation of a fixed point is nonhyperbolicity of the fixed point.

8.1.8  https://math.libretexts.org/@go/page/24179
    The "nature" of the bifurcation (e.g. numbers and stability of fixed points that are created or destroyed) is determined by the
    form of the nonlinearity.

But we could go further and ask what is in common about these examples that could lead to a definition of the bifurcation of a
fixed point for autonomous vector fields? From the common features we give the following definition.

 Definition 21 (Bifurcation of a fixed point of a one dimensional autonomous vector field)

  We consider a one dimensional autonomous vector field depending on a parameter, . We assume that at a certain parameter

  value it has a fixed point that is not hyperbolic. We say that a bifurcation occurs at that "nonhyperbolic parameter value" if for

   in a neighborhood of that parameter value the number of fixed points and their stability changes.

Finally, we finish the discussion of bifurcations of a fixed point of one dimensional autonomous vector fields with an example
showing that a nonhyperbolic fixed point may not bifurcate as a parameter is varied, i.e. non-hyperbolicity is a necessary, but not
sufficient, condition for bifurcation.

Example 8.1.26

We consider the one dimensional autonomous vector field:

                                                         x =  - x3, x  R,  (8.1.17)

where  is a parameter. This vector field has a nonhyperbolic fixed point at x = 0 for  = 0 . The curve of fixed points in the
 - x plane is given by  = x3 , and the Jacobian of the vector field is -3x2, which is strictly negative at all fixed points,

except the nonhyperbolic fixed point at the origin.

In fig. 8.9 we plot the fixed points as a function of .

                                                               Figure 8.9: Bifurcation diagram for (8.17).

  We see that there is no change in the number or stability of the fixed points for  > 0 and  < 0 . Hence, no bifurcation.

This page titled 8.1: Bifurcation of Equilibria I is shared under a CC BY 4.0 license and was authored, remixed, and/or curated by Stephen

Wiggins via  source content that was edited to the style and standards of the LibreTexts platform.

                                                          8.1.9            https://math.libretexts.org/@go/page/24179
8.2: Problem Set

Exercise 8.2.1

  Consider the following autonomous vector fields on the plane depending on a scalar parameter . Verify that each vector field

 has a fixed point at (x, y) = (0, 0) for  = 0 . Determine the linearized stability of this fixed point. Determine the nature (i.e.

  stability and number) of the fixed points for  in a neighborhood of zero. (In other words, carry out a bifurcation analysis.)
  Sketch the flow in a neighborhood of each fixed point for values of  corresponding to changes in stability and/or numbers of
  fixed points.

  1. x =  +10x2 ,
    y = x -5y .

  2. x = x +10x2 ,
    y = x -2y .

  3. x = x + x5 ,
    y = y .

This page titled 8.2: Problem Set is shared under a CC BY 4.0 license and was authored, remixed, and/or curated by Stephen Wiggins via  source

content that was edited to the style and standards of the LibreTexts platform.

8.2.1  https://math.libretexts.org/@go/page/24180
CHAPTER OVERVIEW

9: Bifurcation of Equilibria II

  9.1: Bifurcation of Equilibria II
  9.2: Problem Set
This page titled 9: Bifurcation of Equilibria II is shared under a CC BY 4.0 license and was authored, remixed, and/or curated by Stephen
Wiggins via  source content that was edited to the style and standards of the LibreTexts platform.

                                                                                              1
9.1: Bifurcation of Equilibria II

We have examined fixed points of one dimensional autonomous vector fields where the matrix associated with the linearization of
the vector field about the fixed point has a zero eigenvalue. It is natural to ask "are there more complicated bifurcations, and what
makes them complicated?"

If one thinks about the examples considered so far, there are two possibilities that could complicate the situation. One is that there
could be more than one eigenvalue of the linearization about the fixed point with zero real part (which would necessarily require a
consideration of higher dimensional vector fields), and the other would be more complicated nonlinearity (or a combination of
these two). Understanding how dimensionality and nonlinearity contribute to the "complexity" of a bifurcation (and what that
might mean) is a very interesting topic, but beyond the scope of this course. This is generally a topic explored in graduate level
courses on dynamical systems theory that emphasize bifurcation theory. Here we are mainly just introducing the basic ideas and
issues with examples that one might encounter in applications. Towards that end we will consider an example of a bifurcation that
is very important in applications-the Poincaré-Andronov-Hopf bifurcation (or just Hopf bifurcation as it is more commonly
referred to). This is a bifurcation of a fixed point of an autonomous vector field where the fixed point is nonhyperbolic as a result of
the Jacobian having a pair of purely imaginary eigenvalues, ±iw, w  0. Therefore this type of bifurcation requires (at least two
dimensions), and it is not characterize by a change in the number of fixed points, but by the creation of time dependent periodic
solutions. We will analyze this situation by considering a specific example. References solely devoted to the Hopf bifurcation are
the books of Marsden and McCracken and Hassard, Kazarinoff, and Wan.

We consider the following nonlinear autonomous vector field on the plane:

                                   x = x - wy + (ax - by)(x2 + y2)                                    (9.1.1)

                                   y = wx + y + (bx + ay)(x2 + y2), (x, y)  R2,                       (9.1.2)

where we consider a, b, w as fixed constants and  as a variable parameter. The origin, (x, y) = (0, 0) is a fixed point, and we want
to consider its stability. The matrix associated with the linearization about the origin is given by:

                                   (  -w ) ,                                                          (9.1.3)

                                     w

and its eigenvalues are given by:

                                   1,2 =  ± iw.                                                       (9.1.4)

Hence, as a function of  the origin has the following stability properties:

                                     < 0  sink                                                        (9.1.5)
                                   =0     center
                                          source

                                      >0

The origin is not hyperbolic at  = 0 , and there is a change in stability as  changes sign. We want to analyze the behavior near the
origin, both in phase space and in parameter space, in more detail.

Towards this end we transform Equation 9.1.2 to polar coordinates using the standard relationship between Cartesian and polar
coordinates:

                                                           x = r cos , y = r sin                      (9.1.6)
Differentiating these two expressions with respect to t, and substituting into Equation 9.1.2 gives:

x = r cos  - r sin  = r cos  - wr sin  + (ar cos  - br sin )r2,                                       (9.1.7)

                                 y = r cos  + r sin  = wr cos  - r sin  + (br cos  - ar sin )r2,      (9.1.8)
from which we obtain the following equations for r and

                                   r = r + ar3,                                                       (9.1.9)

                                   r = wr + br3,                                                      (9.1.10)

                                   9.1.1                                                              https://math.libretexts.org/@go/page/24186
where Equation 9.1.9 is obtained by multiplying Eqution 9.1.7 by cos  and Equation 9.1.8 by sin  and adding the two results,
and Equation 9.1.10 is obtained by multiplying Equation 9.1.7 by sin and (9.7) by cos and adding the two results. Dividing
both equations by r gives the two equations that we will analyze:

                                                                    r = r + ar3,                         (9.1.11)

                                                                     = w + br2,                          (9.1.12)

Note that Equation 9.1.11 has the form of the pitchfork bifurcation that we studied earlier. However, it is very important to realize
that we are dealing with the equations in polar coordinates and to understand what they reveal to us about the dynamics in the
original cartesian coordinates. To begin with, we must keep in mind that r  0 .

Note that Equation 9.1.11is independent of , i.e. it is a one dimensional, autonomous ODE which we rewrite below:

                                                          r = r + ar3 = r( + ar2).                       (9.1.13)

The fixed points of this equation are:

                                                           r = 0, r =      ----                          (9.1.14)

                                                                             +
                                                                           -a r .

(Keep in mind that r  0 .) Substituting r+ into Equations 9.1.11and 9.1.12gives:

                                                             r+ = r+ + ar+3 = 0 ,

                                                                 = wr + b(-  ),                          (9.1.15)
                                                                                a

The    component  can  easily  be  solved  (using  r+  =  -     ),  after  which  we  obtain:
                                                             a

                                                          (t) = (w - b ) + (0).                          (9.1.16)
                                                                         a

Therefore r does not change in time at r = r+ and q evolves linearly in time. But  is an angular coordinate. This implies that
r = r+ corresponds to a periodic orbit.

Using this information, we analyze the behavior of Equation 9.1.13by constructing the bifurcation diagram. There are two cases to
consider: a > 0 and a < 0 .

In figure 9.1 we sketch the zeros of (9.12) as a function of  for a > 0. We see that a periodic orbit bifurcates from the
nonhyperbolic fixed point at  = 0 . The periodic orbit is unstable and exists for  < 0 . In Fig. 9.2 we illustrate the dynamics in the
x - y phase plane.

In figure 9.3 we sketch the zeros of Equation 9.1.13 as a function of  for a < 0. We see that a periodic orbit bifurcates from the
nonhyperbolic fixed point at  = 0 . The periodic orbit is stable in this case and exists for  > 0 . In Fig. 9.4 we illustrate the
dynamics in the x - y phase plane.

                                           Figure 9.1: The zeros of (9.12) as a function of  for a > 0.

                                                                           9.1.2                         https://math.libretexts.org/@go/page/24186
Figure 9.2: Phase plane as a function of  in the x - y plane for a > 0.

9.1.3                                                                    https://math.libretexts.org/@go/page/24186
Figure 9.3: The zeros of (9.12) as a function of  for a < 0.

9.1.4                                                         https://math.libretexts.org/@go/page/24186
                            Figure 9.4: Phase plane as a function of  in the x - y plane for a < 0.

In this example we have seen that a nonhyperbolic fixed point of a two dimensional autonomous vector field, where the

nonhyperbolicity arises from the fact that the linearization at the fixed point has a pair of pure imaginary eigenvalues, ±iw, can

lead to the creation of periodic orbits as a parameter is varied. This is an example of what is generally called the Hopf bifurcation.
It is the first example we have seen of the bifurcation of an equilibrium solution resulting in time-dependent solutions.

At this point it is useful to summarize the nature of the conditions resulting in a Hopf bifurcation for a two dimensional,
autonomous vector field. To begin with, we need a fixed point where the Jacobian associated with the linearization about the fixed
point has a pair of pure imaginary eigenvalues. This is a necessary condition for the Hopf bifurcation. Now just as for bifurcations
of fixed points of one dimensional autonomous vector fields (e.g., the saddle-node, transcritical, and pitchfork bifurcations that we
have studied in the previous chapter) the nature of the bifurcation for parameter values in a neighbourhood of the bifurcation
parameter is determined by the form of the nonlinearity of the vector field. We developed the idea of the Hopf bifurcation in the

9.1.5  https://math.libretexts.org/@go/page/24186
context of Equation 9.1.2, and in that example the stability of the bifurcating periodic orbit was given by the sign of the coefficient
a (stable for a < 0, unstable for a > 0). In the example the âA YâA Z stability coefficient âA Z âA Z was evident from the simple
structure of the nonlinearity. In more complicated examples, i.e. more complicated nonlinear terms, the determination of the
stability coefficient is more âA YâA Z algebraically intensive âA Z âA Z .Explicit expressions for the stability coefficient are given
in many dynamical systems texts. For example, it is given in Guckenheimer and Holmes and Wiggins. The complete details of the
calculation of the stability coefficient are carried out in 7. Problem 2 at the end of this chapter explores the nature of the Hopf
bifurcation, e.g. the number and stability of bifurcating periodic orbits, for different forms of nonlinearity.

Next, we return to the examples of bifurcations of fixed points in one dimensional vector fields and give two examples of one
dimensional vector fields where more than one of the bifurcations we discussed earlier can occur.

Example 9.1.27

Consider the following one dimensional autonomous vector field depending on a parameter :

                                        x =  - x + , x  2 x3 R.                               (9.1.17)
                                                                                              (9.1.18)
                                                                                          23

The fixed points of this vector field are given by:

                               = x2 - x3 ,

                                      23

and are plotted in Fig. 9.5.

                                             Figure 9.5: Fixed points of (9.16) plotted in the  - x plane.

The curve plotted is where the vector field is zero. Hence, it is positive to the right of the curve and negative to the left of the
curve. From this we conclude the stability of the fixed points as shown in the figure.

There are two saddle node bifurcations that occur where dx d (x) = 0. These are located at

                              (x, ) = (0, 0), (1, 1 ).                                        (9.1.19)

                                                            6

                              9.1.6                                                           https://math.libretexts.org/@go/page/24186
Example 9.1.28

Consider the following one dimensional autonomous vector field depending on a parameter :
                                                      x = x - 2x3 + 3x4 ,

                                                     = x( - x2 + x3 ),                          (9.1.20)
                                                                                                (9.1.21)
                                                                    23                          (9.1.22)

The fixed points of this vector field are given by:

                                                          = x2 - x3 ,

                                                                                            23

and

                                                     x = 0,

and are plotted in the  - x plane in Fig. 9.6.

                                           Figure 9.6: Fixed points of (9.19) plotted in the  - x plane.
  In this example we see that there is a pitchfork bifurcation and a saddle-node bifurcation.

This page titled 9.1: Bifurcation of Equilibria II is shared under a CC BY 4.0 license and was authored, remixed, and/or curated by Stephen

Wiggins via  source content that was edited to the style and standards of the LibreTexts platform.

                                                     9.1.7                                      https://math.libretexts.org/@go/page/24186
9.2: Problem Set

 Exercise 9.2.1

  Consider the following autonomous vector field on the plane:
  x = x - 3y - x(x2 + y2)3 ,
  y = 3x + y - y(x2 + y2)3 ,
  where  is a parameter. Analyze possible bifurcations at (x,y) = (0, 0) for  in a neighborhood of zero. (Hint: use polar
  coordinates.)

 Exercise 9.2.2
  These exercises are from the book of Marsden and McCracken8. Consider the following vector fields expressed in polar

 coordinates, i.e. (r, )  R+ × S 1 , depending on a parameter . Analyze the stability of the origin and the stability of all

  bifurcating periodic orbits as a function of .
  (a)
  r = -r(r - )2 ,
   = 1 .
  (b)
  r = r( - r2)(2 - r2)2 ,
   = 1 .
  (c)
  r = r(r + )(r - ) ,
   = 1 .
  (d)
  r = r(r2 - ) ,
   = 1 .
  (e)
  r = -2r(r + )2(r - )2 ,
   = 1 .

 Exercise 9.2.3

  Consider the following vector field:

 x = x - 2x3 + x5 4 , x  R

  where  is a parameter. Classify all bifurcations of equilibria and, in the process of doing this, determine all equilibria and their
  stability type.

This page titled 9.2: Problem Set is shared under a CC BY 4.0 license and was authored, remixed, and/or curated by Stephen Wiggins via  source

content that was edited to the style and standards of the LibreTexts platform.

9.2.1  https://math.libretexts.org/@go/page/24187
CHAPTER OVERVIEW

10: Center Manifold Theory

  10.1: Center Manifold Theory
  10.E: Center Manifold Theory (Exericses)
This page titled 10: Center Manifold Theory is shared under a CC BY 4.0 license and was authored, remixed, and/or curated by Stephen Wiggins
via  source content that was edited to the style and standards of the LibreTexts platform.

                                                                                              1
10.1: Center Manifold Theory

This chapter is about center manifolds, dimensional reduction, and stability of fixed points of autonomous vector fields. We begin
with a motivational example.

Example 10.1.29

Consider the following linear, autonomous vector field on Rc × Rs :
                                              x = Ax ,

                 y = By, (x, y)  R × R,                                                                                    (10.1.1)

where A is a c × c matrix of real numbers having eigenvalues with zero real part and B is a s × s matrix of real numbers

having eigenvalues with negative real part. Suppose we are interested in stability of the nonhyperbolic fixed point

(x, y) = (0, 0). Then that question is determined by the nature of stability of x = 0 in the lower dimensional vector field:

                 x = Ax, x  Rc.                                                                                            (10.1.2)

This follows from the nature of the eigenvalues of B, and the properties that x and y are decoupled in (10) and that it is linear.

More precisely, the solution of (10) is given by:

                 ( x(t, x0) y(t, y0) ) = ( eAtx0 eBty0 ) .                                                                 (10.1.3)

From the assumption of the real parts of the eigenvalues of B having negative real parts, it follows that:

                 lim eBty0 = 0.                                                                                            (10.1.4)

                 t

In fact, 0 is approached at an exponential rate in time. Therefore it follows that stability, or asymptotic stability, or instability of
x = 0 for (10.2) implies stability, or asymptotic stability, or instability of (x, y) = (0, 0) for (10).

It is natural to ask if such a dimensional reduction procedure holds for nonlinear systems. This might seem unlikely since, in
general, nonlinear systems are coupled and the superposition principle of linear systems does not hold. However, we will see that
this is not the case.

Invariant manifolds lead to a form of decoupling that results in a dimensional reduction procedure that gives, essentially, the same
result as is obtained for this motivational linear example.This is the topic of center manifold theory that we now develop.

We begin by describing the set-up. It is important to realize that when applying these results to a vector field, it must be in the
following form.

                 x = Ax + f(x, y) ,

                 y = By + g(x, y), (x, y)  Rc × Rs,                                                                        (10.1.5)

where the matrices A and B are have the following properties:

A. c × c matrix of real numbers having eigenvalues with zero real parts,
B. s × s matrix of real numbers having eigenvalues with negative real parts,
and f and g are nonlinear functions. That is, they are of order two or higher in x and y, which is expressed in the following

properties:

                                        f (0, 0) = 0, Df (0, 0) = 0                                                        (10.1.6)
                                        g(0, 0) = 0, Dg(0, 0) = 0,                                                         (10.1.7)
and they are C r, r as large as required (we will explain what this means when we explicitly use this property later on).

With this set-up (x, y) = (0, 0) is a fixed point for (10.4) and we are interested in its stability properties.

The linearization of (10.4) about the fixed point is given by:

                                                                x = Ax ,

                                                                10.1.1    https://math.libretexts.org/@go/page/24193
                                          y = By, (x, y)  Rc × Rs,                    (10.1.8)

The fixed point is nonhyperbolic. It has a c dimensional invariant center subspace and a s dimensional invariant stable subspace
given by:

                                          Ec = (x, y)  Rc × Rs|y = 0,                 (10.1.9)

                                          Es = (x, y)  Rc × Rs|x = 0,                 (10.1.10)

respectively.

For the nonlinear system (Equation 10.1.5) there is a s dimensional, C r passing through the origin and tangent to Es at the origin.
Moreover, trajectories in the local stable manifold inherit their behavior from trajectories in Es under the linearized dynamics in

the sense that they approach the origin at an exponential rate in time.

Similarly, there is a c dimensional C r local center manifold that passes through the origin and is tangent to Ec are the origin.

Hence, the center manifold has the form:

                             W c(0) = (x, y)  Rc × Rs|y = h(x), h(0) = 0, Dh(0) = 0,  (10.1.11)

which is valid in a neighborhood of the origin, i.e. for |x| sufficiently small.

We illustrate the geometry in Fig. 10.1.

The application of the center manifold theory for analyzing the behavior of trajectories near the origin is based on three theorems:

    existence of the center manifold and the vector field restricted to the center manifold,
    stability of the origin restricted to the center manifold and its relation to the stability of the origin in the full dimensional phase
    space,
    obtaining an approximation to the center manifold.

Theorem 5: Existence and Restricted Dynamics

There exists a C r center manifold of (x, y) = (0, 0) for Equation 10.1.5. The dynamics of Equation 10.1.5 restricted to the

center manifold is given by:

                                          u = Au + f(u, h(u)), u  Rc,                 (10.1.12)

for |u| sufficiently small.

A natural question that arises from the statement of this theorem is "why did we use the variable'`u' when it would seem that 'x'
would be the more natural variable to use in this situation"? Understanding the answer to this question will provide some insight

                                              10.1.2                                  https://math.libretexts.org/@go/page/24193
and understanding to the nature of solutions near the origin and the geometry of the invariant manifolds near the origin. The answer
is that "x and y are already used as variables for describing the coordinate axes in Equation 10.1.5. We do not want to confuse a
point in the center manifold with a point on the coordinate axis. A point on the center manifold is denoted by (x, h(x)). The
coordinate u denotes a parametric representation of points along the center manifold. Moreover, we will want to compare
trajectories of Equation 10.1.12 with trajectories in Equation 10.1.5. This will be confusing if x is used to denote a point on the
center manifold. However, when computing the center manifold and when considering the (i.e. Equation 10.1.12) it is traditional to
use the coordinate 'x', i.e. the coordinate describing the points in the center subspace. This does not cause ambiguities since we can
name a coordinate anything we want. However, it would cause ambiguities when comparing trajectores in Equation 10.1.5 with
trajectories in Equation 10.1.12, as we do in the next theorem.

Theorem 6

i. Suppose that the zero solution of Equation 10.1.12is stable (asymptotically stable) (unstable), then the zero solution of
   (10.4) is also stable (asymptotically stable) (unstable).

ii. Suppose that the zero solution of Equation 10.1.12is stable. Then if (x(t), y(t)) is a solution of (10.4) with (x(0), y(0))

  sufficiently small, then there is a solution u(t) of Equation 10.1.12such that as t  

                                        x(t) = u(t) + O(e-t) ,

                                        y(t) = h(u(t)) + O(e-t),                                             (10.1.13)

where  > 0 is a constant.

Part i) of this theorem says that stability properties of the origin in the center manifold imply the same stability properties of the
origin in the full dimensional equations. Part ii) gives much more precise results for the case that the origin is stable. It says that
trajectories starting at initial conditions sufficiently close to the origin asymptotically approach a trajectory in the center manifold.

Now we would like to compute the center manifold so that we can use these theorems in specific examples. In general, it is not
possible to compute the center manifold. However, it is possible to approximate it to "sufficiently high accuracy" so that we can
verify the stability results of Theorem 6 can be confirmed. We will show how this can be done. The idea is to derive an equation
that the center manifold must satisfy, and then develop an approximate solution to that equation.

We develop this approach step-by-step.

The center manifold is realized as the graph of a function,

                                        y = h(x), x  Rc, y  Rs,                                              (10.1.14)

i.e. any point (xc, yc), sufficiently close to the origin, that is in the center manifold satisfies yc = h(x - c) . In addition, the center
manifold passes through the origin (h(0) = 0) and is tangent to the center sub- space at the origin (Dh(0) = 0 ).

Invariance of the center manifold implies that the graph of the function h(x) must also be invariant with respect to the dynamics
generated by Equation 10.1.5. Differentiating Equation 10.1.14 with respect to time shows that (x, y) at any point on the center

manifold satisfies

                                                             y = Dh(x)x.                                     (10.1.15)

This is just the analytical manifestation of the fact that invariance of a surface with respect to a vector field implies that the vector
field must be tangent to the surface.

We will now use these properties to derive an equation that must be satisfied by the local center manifold.

The starting point is to recall that any point on the local center manifold obeys the dynamics generated by . Substituting y = h(x)
into gives:

                                          x = Ax + f(x, h(x)),                                               (10.1.16)
                                 y = Bh(x) + g(x, h(x)), (x, y)  Rc × Rs.                                    (10.1.17)
Substituting and into the invariance condition y = Dh(x)x gives:

                                        Bh(x) + g(x, h(x)) = Dh(x)(Ax + f(x, h(x))),                         (10.1.18)

                                                             10.1.3                   https://math.libretexts.org/@go/page/24193
or

                         Dh(x)(Ax + f(x, h(x))) - Bh(x) - g(x, h(x))  N (h(x) = 0.                         (10.1.19)

This is an equation for h(x). By construction, the solution implies invariance of the graph of h(x), and we seek a solution
satisfying the additional conditions h(0) = 0 and Dh(0) = 0 . The basic result on approximation of the center manifold is given by

the following theorem.

Theorem 7: Approximation

Let  : Rc  Rs be a C 1 mapping with
(0) = 0, D(0) = 0 ,

 such that

N ((x)) = O(|x|q) as x  0,

for some q > 1. Then

|h(x) - (x)| = O(|x|q) as x  0.

The theorem states that if we can find an approximate solution of Equation 10.1.19 to a specified degree of accuracy, then that
approximate solution is actually an approximation to the local center manifold, to the same degree of accuracy.

We now consider some examples showing how these results are applied.

Example 10.1.30

    We consider the following autonomous vector field on the plane:

                                                 x = x2y - x5 ,

                                                 y = -y + x2, (x, y)  R2,                                  (10.1.20)

                                                 .

    or, in matrix form:

                                     ( x ) = ( 0 0 ) ( x ) + ( 2 x2y - x5 ) ,                              (10.1.21)
                                              y  0 -1 y              x

    We are interested in determining the natural of the stability of (x, y) = (0, 0). The Jacobian associated with the linearization
    about this fixed point is:

                                                 ( 0 0 ),                                                  (10.1.22)
                                                                                                           (10.1.23)
                                                                                             0 -1

    which is nonhyperbolic, and therefore the linearization does not suffice to determine stability.
    The vector field is in the form of Equation 10.1.5

                                              x = Ax + f(x, y) ,
                                       y = By + g(x, y), (x, y)  R × R,

    where

                                  A = 0, B = 1, f(x, y) = x2y - x5, g(x, y) = x2.

    We assume a center manifold of the form:

                                                 y = h(x) = ax2 + bx3 + O(x4),                             (10.1.24)

    which satisfies h(0) = 0 ("passes through the origin") and Dh(0) = 0 (tangent to Ec at the origin). A center manifold of this
    type will require the vector field to be at least C 3 (hence, the meaning of the phrase C r, r as large as necessary).

    Substituting this expression into the equation for the center manifold (10.17) (using (10.21)) gives:

                                                 10.1.4                                               https://math.libretexts.org/@go/page/24193
(2ax + 3b2 + O(x3))(ax4 + bx5 + O(x6) - x5) + ax2 + bx3 + O(x4) - x2 = 0.                                                    (10.1.25)

In order for this equation to be satisfied the coefficients on each power of x must be zero. Through third order this gives:

x2 : a - 1 = 0  a = 1 ,

x3 : b = 0.                                                                                                                  (10.1.26)

Substituting these values into (10.22) gives the following expression for the center manifold through third order:

y = x2 + O(x4).                                                                                                              (10.1.27)

Therefore the vector field restricted to the center manifold is given by:

x = x4 + O(x5).                                                                                                              (10.1.28)

Hence, for x sufficiently small, x is positive for x  0, and therefore the origin is unstable. We illustrate the flow near the
origin in Fig. 10.2.

Figure 10.2: The flow near the origin for (10.19).

Example 10.1.31                                                                                                              (10.1.29)
                                                                                                                             (10.1.30)
 We consider the following autonomous vector field on the plane:
                                                                   x = xy,

                                          y = y + x3, (x, y)  R2,

 Therefore the vector field restricted to the center manifold is given by:

                                         x = x4 + O(x5).

 Since x is positive for x sufficiently small. the origin is unstable. We illustrate the flow near the origin in Fig. 10.3.

10.1.5                                                                     https://math.libretexts.org/@go/page/24193
                                                           Figure 10.3: The flow near the origin for (10.27).

This page titled 10.1: Center Manifold Theory is shared under a CC BY 4.0 license and was authored, remixed, and/or curated by Stephen

Wiggins via  source content that was edited to the style and standards of the LibreTexts platform.

10.1.6  https://math.libretexts.org/@go/page/24193
10.E: Center Manifold Theory (Exericses)

Exercise 10.E. 1

  Consider the following autonomous vector field on the plane:

 x = x2y - x3 ,
 y = y + x3, (x, y)  R2 .

  Determine the stability of (x, y) = (0, 0) using center manifold theory.

Exercise 10.E. 2

Consider the following autonomous vector field on the plane:                                                     "blow up in finite

x = x2 ,
y = y + x2, (x, y)  R2 .
Determine the stability of (x, y) = (0, 0) using center manifold theory. Does the fact that solutions of x = x2

time" influence your answer (why or why not)?

Exercise 10.E. 3

 Consider the following autonomous vector field on the plane:

x = x + y2 ,
y = -2x2 + 2xy2, (x, y)  R2 .
Show that y = x2 is an invariant manifold. Show that there is a trajectory connecting (0, 0) to (1, 1), i.e. a heteroclinic

 trajectory.

Exercise 10.E. 4

Consider the following autonomous vector field on R3:
x = y ,
y = -x - x2y ,
z = -z + xz2, (x, y, z)  R3

 Determine the stability of (x, y, z) = (0, 0, 0) using center manifold theory.

Exercise 10.E. 5

Consider the following autonomous vector field on R3:
x = y ,
y = -x - x2y + zxy ,
z = -z + xz2, (x, y, z)  R3 .

 Determine the stability of (x, y, z) = (0, 0, 0) using center manifold theory.

Exercise 10.E. 6

Consider the following autonomous vector field on R3:
x = y ,
y = -x + zy2 ,

                                                       10.E.1                    https://math.libretexts.org/@go/page/24194
 z = -z + xz2, (x, y, z)  R3 .

  Determine the stability of (x, y, z) = (0, 0, 0) using center manifold theory.

This page titled 10.E: Center Manifold Theory (Exericses) is shared under a CC BY 4.0 license and was authored, remixed, and/or curated by

Stephen Wiggins via  source content that was edited to the style and standards of the LibreTexts platform.

10.E.2  https://math.libretexts.org/@go/page/24194
CHAPTER OVERVIEW

11: Appendices

  11.1: A- Jacobians, Inverses of Matrices, and Eigenvalues
  11.2: B- Integration of Some Basic Linear ODEs
  11.3: Finding Lyapunov Functions
  11.4: D- Center Manifolds Depending on Parameters
  11.5: E- Dynamics of Hamilton's Equations
11: Appendices is shared under a not declared license and was authored, remixed, and/or curated by LibreTexts.

                                                                                              1
11.1: A- Jacobians, Inverses of Matrices, and Eigenvalues

In this appendix we collect together some results on Jacobians and inverses and eigenvalues of 2 × 2 matrices that are used
repeatedly in the material.

First, we consider the Taylor expansion of a vector valued function of two variables, denoted as follows:

                                          H(x, y) = ( f(x, y) g(x, y) ) , (x, y)  R2,                                 (11.1.1)

More precisely, we will need to Taylor expand such functions through second order:

                                  H(x0 + h, y0 + k) = H(x0, y0) + DH(x0, y0) ( h ) + O(2).                            (11.1.2)
                                                                         k

The Taylor expansion of a scalar valued function of one variable should be familiar to most students at this level. Possibly there is
less familiarity with the Taylor expansion of a vector valued function of a vector variable. However, to compute this we just Taylor
expand each component of the function (which is a scalar valued function of a vector variable) in each variable, holding the other
variable fixed for the expansion in that particular variable, and then we gather the results for each component into matrix form.

Carrying this procedure out for the f(x, y)component of Equation 11.1.1gives:

f(x0 + h, y0 + k)                 = f(x0, y0 + k) + f (x0, y0 + k)h + O(h2)                                           (11.1.3)
                                                            x                                                         (11.1.4)
                                  = f(x0, y0) + f (x0, y0)k + O(k2) + f (x0, y0)h + O(hk) + O(h2).
                                                         y                   x

The same procedure can be applied to g(x, y). Recombining the terms back into the vector expression for Equation 11.1.1gives:

                                          f(x0, y0)  x (x0, y0)f             fy (x0, y0)  h

H(x0 + h, y0 + k) = (                                       ) + g                       ( ) + O(2),                   (11.1.5)
                                          g(x0, y0)  x (x0, y0)              y (x0, y0)  kg

Hence, the Jacobian of Equation 11.1.1at (x0, y0) is:

                                             f                 fy (x0, y0) 

                                           x (x0, y0)           g            ,                                        (11.1.6)
                                           g                   y (x0, y0) 
                                           x (x0, y0)

which is a 2 × 2 matrix of real numbers.

We will need to compute the inverse of such matrices, as well as its eigenvalues.

We denote a general 2 × 2 matrix of real numbers:

                                          A = ( a b ) , a, b, c, d  R.                                                (11.1.7)
                                               cd

It is easy to verify that the inverse of A is given by:

                                          A-1 = 1 ( d -b ) .                                                          (11.1.8)
                                                ad -bc -c a

Let I denote the 2 × 2 identity matrix. Then the eigenvalues of A are the solutions of the characteristic equation:

                                                            det(A - I) = 0.                                             (11.1.9)
                                                                                                                      (11.1.10)
where "det" is notation for the determinant of the matrix. This is a quadratic equation in  which has two solutions:

                                           1,2 = trA ± 1       ------------
                                                       22
                                                               (trA)2 - 4detA,

where we have used the notation:

                                                               11.1.1                        https://math.libretexts.org/@go/page/24207
                         trA  traceA = a+d , detA  determinantA = ad -bc .

This page titled 11.1: A- Jacobians, Inverses of Matrices, and Eigenvalues is shared under a CC BY 4.0 license and was authored, remixed, and/or

curated by Stephen Wiggins via  source content that was edited to the style and standards of the LibreTexts platform.

11.1.2  https://math.libretexts.org/@go/page/24207
11.2: B- Integration of Some Basic Linear ODEs

In this appendix we collect together a few common ideas related to solving, explicitly, linear inhomogeneous differential equations.
Our discussion is organized around a series of examples.

Example 11.2.32

Consider the one dimensional, autonomous linear vector field:

                                                          x = ax, x, a  R.                                                                (11.2.1)

We often solve problems in mathematics by transforming them into simpler problems that we already know how to solve.
Towards this end, we introduce the following (time-dependent) transformation of variables:

                                                          x = ueat.                                                                       (11.2.2)

Differentiating this expression with respect to t, and using (B.1), gives the following ODE for u:

                                                          u = 0,                                                                          (11.2.3)

which is trivial to integrate, and gives:

                                                          u(t) = u(0),                                                                    (11.2.4)

and it is easy to see from (36) that:

                                                          u(0) = x(0).                                                                    (11.2.5)

Using (36), as well as (B.4) and (B.5), it follows that:

                                                 x(t)eat = u(t) = u(0) = x(0),                                                            (11.2.6)

or

                                                          x(t) = x(0)eat.                                                                 (11.2.7)

Example 11.2.33

Consider the following linear inhomogeneous nonautonomous ODE (due to the presence of the term b(t)):

                                                 x = ax + b(t), a, x  R,                                                                  (11.2.8)

where b(t) is a scalar valued function of t, whose precise properties we will consider a bit later. We will use exactly the same
strategy and change of coordinates as in the previous example:

                                                          x = ueat.                                                                       (11.2.9)

Differentiating this expression with respect to t, and using (B.8), gives:

                                                          u = e-at b(t).                                                                  (11.2.10)

(Compare with (B.3).) Integrating (B.10) gives:

                                                                                                      t                                   (11.2.11)

                                                 u(t) = u(0) +  e-at b(t)dt.

                                                                                                                                       0

Now using (B.9) (with the consequence u(0) = x(0)) with (B.11) gives:

                                                                                 t                                                        (11.2.12)

                                           x(t) = x(0)eat + eat  e-at b(t)dt.

                                                                                            0

Finally, we return to the necessary properties of b(t) in order for this unique solution of (B.8) to "make sense". Upon inspection
of (B.12) it is clear that all that is required is for the integrals involving b(t) to be well-defined. Continuity is a sufficient
condition.

                                                          11.2.1                                                                          https://math.libretexts.org/@go/page/24214
Example 11.2.34

Consider the one dimensional, nonautonomous linear vector field:

                                                       x = a(t)x, x  R,                                            (11.2.13)

where a(t) is a scalar valued function of t whose precise properties will be considered later. The similarity between (B.1) and
(B.13) should be evident. We introduce the following (time-dependent) transformation of variables (compare with (36)):

                                                             x = ue0 . t a(t)dt                                    (11.2.14)
Differentiating this expression with respect to t, and substituting (B.13) into the result gives:

                                                   x = ue0 + t a(t)dt ua(t)e0t a(t)dt ,

                                                       = ue0 + t a(t)dt a(t)x,                                     (11.2.15)

which reduces to:

                                                       u = 0.                                                      (11.2.16)

Integrating this expression, and using (B.14), gives:

                                    u(t) = u(0) = x(0) = x(t)e- 0 , t a(t)dt                                       (11.2.17)

or

                                                       x(t) = x(0)e0 . t a(t)dt                                    (11.2.18)

As in the previous example, all that is required for the solution to be well-defined is for the integrals involving a(t) to exist.
Continuity is a sufficient condition.

Example 11.2.35

Consider the one dimensional inhomogeneous nonautonomous linear vector field:

                                    x = a(t)x + b(t), x  R,                                                        (11.2.19)

where a(t), b(t) are scalar valued functions whose required properties will be considered at the end of this example. We make
the same transformation as (B.14):

                                                       x = ue0 , t a(t)dt                                          (11.2.20)

from which we obtain:

                                                       u = b(t)e- 0 . t a(t)dt                                     (11.2.21)

Integrating this expression gives:

                                    t u(t) = u(0) +  b(t )e-  t 0 a(t)dt dt .                                      (11.2.22)

                                                                       0

Using gives:

                                    x(t)e- 0t a(t)dt t = x(0) +  b(t )e- 0t a(t)dt dt .                            (11.2.23)

                                                                                               0

or

                                    x(t) = x(0)e0t a(t)dt + e0t a(t)dt t  b(t )e- 0t a(t)dt dt .                   (11.2.24)

                                                                                                                0

As in the previous examples, that all that is required is for the integrals involving a(t) and b(t) to be well-defined. Continuity is
a sufficient condition.

                                                       11.2.2                                                      https://math.libretexts.org/@go/page/24214
The previous examples were all one dimensional. Now we will consider two n dimensional examples.               (11.2.25)
                                                                                                               (11.2.26)
 Example 11.2.36                                                                                               (11.2.27)
                                                                                                               (11.2.28)
  Consider the n dimensional autonomous linear vector field:                                                   (11.2.29)
                                                                                                               (11.2.30)
                                            x = Ax, x  Rn,
 where A is a n × n matrix of real numbers. We make the following transformation of variables (compare with):

                                               x = eAtu.

  Differentiating this expression with respect to t, and using (B.25), gives:

                                                u = 0.

  Integrating this expression gives:

                                              u(t) = u(0).

  Using (B.26) with (B.28) gives:

                                      u(t) = e-At x(t) = u(0) = x(0),

  from which it follows that:

                                            x(t) = eAtx(0).

Example 11.2.37

Consider the n dimensional inhomogeneous nonautonomous linear vector field:

                             x = Ax + g(t), x  Rn,                                                             (11.2.31)

where g(t) is a vector valued function of t whose required properties will be considered later on. We use the same
transformation as in the previous example:

                             x = eAtu.                                                                         (11.2.32)

Differentiating this expression with respect to t, and using (B.31), gives:

                             u = eAtg(t),                                                                      (11.2.33)

from which it follows that:

                             u(t) = u(0) +  t e-At g(t)dt,                                                     (11.2.34)

                                                                0

or, using (B.32)

                             x(t) = eAtx(0) + eAt  t e-At g(t)dt.                                              (11.2.35)

                                                                               0

This page titled 11.2: B- Integration of Some Basic Linear ODEs is shared under a CC BY 4.0 license and was authored, remixed, and/or curated

by Stephen Wiggins via  source content that was edited to the style and standards of the LibreTexts platform.

                             11.2.3                                               https://math.libretexts.org/@go/page/24214
11.3: Finding Lyapunov Functions

Lyapunov's method and the LaSalle invariance principle are very powerful techniques, but the obvious question always arises,
"how do I find the Lyapunov function? The unfortunate answer is that given an arbitrary ODE there is no general method to find a
Lyapunov function appropriate for a given ODE for the application of these methods.

In general, to determine a Lyapunov function appropriate for a given ODE the ODE must have a structure that lends itself to the
construction of the Lyapunov function. Therefore the next question is "what is this structure?" If the ODE arises from physical
modeling there may be an "energy function" that is "almost conserved". What this means is that when certain terms of the ODE are
neglected the resulting ODE has a conserved quantity, i.e. a scalar valued function whose time derivative along trajectories is zero,
and this conserved quantity may be a candidate to for a Lyapunov function. If that sounds vague it is because the construction of
Lyapunov functions often requires a bit of "mathematical artistry". We will consider this procedure with some examples. Energy
methods are important techniques for understanding stability issues in science and engineering; see, for example see the book by
Langhaar and the article by Maschke.

To begin, we consider Newton's equations for the motion of a particle of mass m under a conservative force in one dimension:

                                                             mx¨ = - d (x), x  R,              (11.3.1)
                                                                    dx

Writing this as a first order system gives:

                                            x = y                                              (11.3.2)
                                            y = - 1 d (x).                                     (11.3.3)

                                                  m dx

It is easy to see that the time derivative of the following function is zero

                                                                 E = my + ( 2 x),              (11.3.4)

                                                                            2

since

                                               E = myy + d (x)x                                (11.3.5)
                                                          dx                                   (11.3.6)

                                                 = -y d (x) + y d (x) = 0.
                                                      dx dx

In terms of dynamics, the function in Equation 11.3.4has the interpretation as the conserved kinetic energy associated with (C.1).

Now we will consider several examples. In all cases we will simplify matters by taking m = 1.

Example 11.3.38

Consider the following autonomous vector field on R2:

                                                                         x = y ,

                                               y = -x - y,   0, (x, y)  R2.                    (11.3.7)
                                                                                               (11.3.8)
For  = 0 Equtation 11.3.7has the form of (C.1):                                                (11.3.9)

                                                                         x = y ,

                                                                 y = x, (x, y)  R2.

with

                                                                 E = y + . 2 x2

                                                                          22

It  is  easy  to  verify  that  dE  =0  along  trajectories  of  (C.6).
                                dt

Now we differentiate E along trajectories of (C.5) and obtain:

                                                                         11.3.1                https://math.libretexts.org/@go/page/24221
                                                                 dE = -y2.               (11.3.10)
                                                                 dt

(C.6) has only one equilibrium point located at the origin. E is clearly positive everywhere, except for the origin, where it is
zero. Using E as a Lyapunov function we can conclude that the origin is Lyapunov stable. If we use E to apply the LaSalle
invariance principle, we can conclude that the origin is asymptotically stable. Of course, in this case we can linearize and

conclude that the origin is a hyperbolic sink for  > 0 .

Example 11.3.39

Consider the following autonomous vector field on R2:

                                                                          x = y ,

                                               doty = x - x3 - y,   0, (x, y)  R2.       (11.3.11)

For  = 0 , Equation 11.3.11has the form of (C.1):

                                                                          x = y ,

                                                             doty = x - x3, (x, y)  R2.  (11.3.12)

with

                                                                 E = y - + . 2 x2 x4     (11.3.13)

                                                                          224

It  is  easy  to  verify  that  dE  =0  along  trajectories  of  (C.10).
                                dt

The question now is how we will use E to apply Lyapunov's method or the LaSalle invariance principle? (C.9) has three

equilibrium points, a hyperbolic saddle at the origin for   0 and hyperbolic sinks at (x, y) = (±1, 0) for  > 0 and centers for
 = 0 . So linearization gives us complete information for  > 0 . For  = 0 linearization is sufficient to allow is to conclude
that the origin is a saddle. The equilibria (x, y) = (±1, 0) are Lyapunov stable for  = 0 , but an argument involving the

function E would be necessary in order to conclude this. Linearization allows us to conclude that the equilibria (x, y) = (±1, 0)

are asymptotically stable for  > 0 .

The function E can be used to apply the LaSalle invariance principle to conclude that for  > 0 all trajectories approach one of
the three equilibria as t  .

This page titled 11.3: Finding Lyapunov Functions is shared under a CC BY 4.0 license and was authored, remixed, and/or curated by Stephen

Wiggins via  source content that was edited to the style and standards of the LibreTexts platform.

                                                                          11.3.2         https://math.libretexts.org/@go/page/24221
11.4: D- Center Manifolds Depending on Parameters

In this appendix we describe the situation of center manifolds that depend on a parameter. The theoretical framework plays an
important role in bifurcation theory.

As when we developed the theory earlier, we begin by describing the set-up. As before, it is important to realize that when applying
these results to a vector field, it must be in the following form.

                                                x = Ax + f(x, y, ),

y = By + g(x, y, ), (x, y, )  Rc × Rs × Rp,                                                       (11.4.1)

where   Rp is a vector of parameters and the matrices A and B are have the following properties:

1. A c × c matrix of real numbers

  having eigenvalues with zero real parts,

2. B s × s matrix of real numbers

  having eigenvalues with negative real parts,

and f and g are nonlinear functions. That is, they are of order two or higher in x, y and , which is expressed in the following

properties:

                                                f (0, 0, 0) = 0, Df (0, 0, 0) = 0,

                                                g(0, 0, 0) = 0, Dg(0, 0, 0) = 0,                  (11.4.2)

and they are C r, r as large as is required to compute an adequate approximation the center manifold. With this set-up
(x, y, ) = (0, 0, 0) is a fixed point for (D.1) and we are interested in its stability properties.

The conceptual "trick" that reveals the nature of the parameter dependence of center manifolds is to include the parameter  as a

new dependent variable:

          x = Ax + f(x, y, ),                                                                     (11.4.3)
                 = 0 ,

y = By + g(x, y, ), (x, y, )  Rc × Rs × Rp,

The linearization of (D.3) about the fixed point is given by:                                     (11.4.4)

                                               x = Ax ,
                                                 = 0 ,
                                     y = By, (x, y, )  Rc × Rs × Rp.

Even after increasing the dimension of the phase space by p dimensions by including the parameters as new dependent variables,

the fixed point (x, y, ) = (0, 0, 0) remains a nonhyperbolic fixed point. It has a c + p dimensional invariant center subspace and a

s dimensional invariant stable subspace given by:

  Ec = {(x, y, )  Rc × Rs × Rp, |y = 0},                                                          (11.4.5)
Es = {(x, y, )  Rc × Rs × Rp, |x = 0,  = 0},                                                      (11.4.6)

respectively.

It should be clear that center manifold theory, as we have already developed, applies to (D.3). Including the parameters,  as

additional dependent variables has the effect of increasing the dimension of the "center variables", but there is also an important

consequence. Since  are now dependent variables they enter in to the determination of the nonlinear terms in the equations. In

particular, terms of the form

                                               xi mj ykn,
now are interpreted as nonlinear terms, when  + m + n > 1 , for nonnegative integers , m, n. We will see this in the example

below.

                                                11.4.1                              https://math.libretexts.org/@go/page/24228
Now we consider the information that center manifold theory provides us near the origin of (D.3).

1. In a neighborhood of the origin there exists a C r center manifold that is represented as the graph of a function over the center
  variables, h(x, ), it passes through the origin (h(0, 0) = 0) and is tangent to the center subspace at the origin (Dh(0, 0) = 0)

 2. All solutions sufficiently close to the origin are attracted to a trajectory in the center manifold at an exponential rate.
 3. The center manifold can be approximated by a power series expansion.

It is significant that the center manifold is defined in a neighborhood of the origin in both the x and  coordinates since  = 0 is a

bifurcation value. This means that all bifurcating solutions are contained in the center manifold. This is why, for example, that
without loss of generality bifurcations from a single zero eigenvalue can be described by a parametrized family of one dimensional
vector fields.

Example 11.4.40

We now consider an example which was exercise 1b from Problem Set 8.                                                                     (11.4.7)
                                                                                                                                         (11.4.8)
                                             x = x + 10x2 ,
                                                  = 0 ,

                                      y = x - 2y, (x, y)  R2,   R.
The Jacobian associated with the linearization about (x, , y) = (0, 0, 0) is given by:

                                    0 0 0 
                                    0 0 0 .
                                     1 0 -2 

It is easy to check that the eigenvalues of this matrix are 0, 0, and 2 (as we would have expected). Each of these eigenvalues
has an eigenvector. It is easily checked that two eigenvectors corresponding to the eigenvalue 0 are given by:

                                                           2                                                                             (11.4.9)
                                                           0.
                                                           1

                                                           0                                                                             (11.4.10)
                                                           1.
                                                           0

and an eigenvector corresponding to the eigenvalue -2 is given by:

                                                           0                                                                             (11.4.11)
                                                           0.
                                                           1

From these eigenvectors we form the transformation matrix

                    2 0 0                                                                                                                (11.4.12)
                 T =0 1 0.

                    1 0 1

with inverse

                                       21 0 0                                                                                            (11.4.13)
                                 T -1 =  0 1 0  .

                                      -1 0 1
                                                                                                                                      2

The transformation matrix, T, defines the following transformation of the dependent variables of (D.7):

                                                           11.4.2                       https://math.libretexts.org/@go/page/24228
                       x   u  2 0 0  u   2u                                                                                                           (11.4.14)
                      =T =0 1 0=                                                                                                                      (11.4.15)
                      y v 1 0 1v u+v                                                                                                                  (11.4.16)
                                                                                                                                                      (11.4.17)
It then follows that the transformed vector field has the form:

     u   0 0 0   u    (2u) + 10(2u)2 
       =  0 0 0     + T -1                                       
     v   0 0 -2   v          0                                   

                             0

or

                   u   0 0 0   u   21 0 0   2u + 40u2 
                     =  0 0 0     + 0 1 0   0 
                   v   0 0 -2   v   - 1 0 1   0 

                                                                                                                                                   2

or

                                                       u = u + 20u2 ,
                                                             = 0 ,

    v = -2v - u - 20u2.

This page titled 11.4: D- Center Manifolds Depending on Parameters is shared under a CC BY 4.0 license and was authored, remixed, and/or

curated by Stephen Wiggins via  source content that was edited to the style and standards of the LibreTexts platform.

    11.4.3                                                         https://math.libretexts.org/@go/page/24228
11.5: E- Dynamics of Hamilton's Equations

In this appendix we give a brief introduction to some of the characteristics and results associated with Hamiltonian differential
equations (or, Hamilton's equations or Hamiltonian vector fields). The Hamiltonian formulation of Newton's equations reveals a
great deal of structure about dynamics and it also gives rise to a large amount of deep mathematics that is the focus of much
contemporary research. .

Our purpose here is not to derive Hamilton's equations from Newton's equations. Discussions of that can be found in many
textbooks on mechanics (although it is often considered `'advanced mechanics"). For example, a classical exposition of this topic
can be found in the classic book of Landau, and more modern expositions can be found in Abraham and Marsden and Arnold.
Rather, our approach is to start with Hamilton's equations and to understand some simple aspects and consequences of the special
structure associated with Hamilton's equations. Towards this end, our starting point will be Hamilton's equations. Keeping with the
simple approach throughout these lectures, our discussion of Hamilton's equations will be for two dimensional systems.

We begin with a scalar valued function defined on R2

H = H(q, p), (q, p)  R2.                                                                                              (11.5.1)

This function is referred to as the Hamiltonian. From the Hamiltonian, Hamilton's equations take the following form:  (11.5.2)

                                              q = H (q, p),

                                                                       p

                                        p = H (q, p), (q, p)  R2.
                                             q

The form of Hamilton's equations implies that the Hamiltonian is constant on trajectories. This can be seen from the following
calculation:

dH = H q + H p
dt q                                                    p

= H H - H H = 0.                                                                                                      (11.5.3)
  q p p q

Furthermore, this calculation implies that the level sets of the Hamiltonian are invariant manifolds. We denote the level set of the
Hamiltonian as:

HE = {(q, p)  R2|H(q, p) = E}                                                                                         (11.5.4)

In general, the level set is a curve (or possibly an equilibrium point). Hence, in the two dimensional case, the trajectories of
Hamilton's equations are given by the level sets of the Hamiltonian.

The Jacobian of the Hamiltonian vector field (E.2), denoted J, is given by:

       2H                                                  2 H                                                        (11.5.5)

                qp                                         p2

J(p, q) =  2                                             2   ,
      - H                                               - H 

                                                     2    pq

                 q

at an arbitrary point (q, p)  R2 . Note that the trace of J(q, p), denoted trJ(q, p), is zero. This implies that the eigenvalues of J(q, p),
denoted by 1,2, are given by:

     ---------                                                                                                        (11.5.6)

1,2 = ± -detJ(q, p),

where detJ(q, p) denotes the determinant of J(q, p). Therefore, if (q0, p0) is an equilibrium point of (E.1) and detJ(q0, p0) = 0 ,
then the equilibrium point is a center for detJ(q0, p0) > 0 and a saddle for detJ(q0, p0) < 0 .

Next we describe some examples of two dimensional, linear autonomous Hamiltonian vector fields.

11.5.1                                                                       https://math.libretexts.org/@go/page/24235
Example 11.5.41 (The Hamiltonian Saddle )

We consider the Hamiltonian:

                              H(q, p) =  (p2 - q2) =  (p - q)(p + q), (q, p)  R2,  (11.5.7)
                                          2           2                            (11.5.8)

with  > 0 . From this Hamiltonian, we derive Hamilton's equations:

                                             q  =  H  (q,  p)  =    p  ,
                                                   p

                                             p = H (q, p) = q,
                                                p

or in matrix form:

                                             ( q ) = ( 0  ) ( q ) .                (11.5.9)
                                              p  0 p

The origin is a fixed point, and the eigenvalues associated with the linearization are given by ±. Hence, the origin is a saddle

point.The value of the Hamiltonian at the origin is zero. We also see from (E.7) that the Hamiltonian is zero on the lines

p - q = 0 and p + q = 0. These are the unstable and stable manifolds of the origin, respectively. The phase portrait is

illustrated in Fig. E.1.

    Figure E.1: The phase portrait of the linear Hamiltonian saddle. The stable manifold of the origin is given by p - q = 0 and the

        unstable manifold of the origin is given by p + q = 0.
The flow generated by this vector field is given in Chapter 2, Problem Set 2, problem 6.

Example 11.5.42 (The Hamiltonian Center)

We consider the Hamiltonian:  H(q, p) =  (p2 + q2), (q, p)  R2,
                                               2
                                                                                   (11.5.10)

                                                   11.5.2                          https://math.libretexts.org/@go/page/24235
with  > 0 . From this Hamiltonian, we derive Hamilton's equations:

                       q  =  H  (q,  p)  =                          p  ,
                             p

                    p = H (q, p) = -q,                                           (11.5.11)
                       p                                                         (11.5.12)

or in matrix form:

                    ( q ) = ( 0  ) ( q ) .p
                             - 0 p

The level sets of the Hamiltonian are circles, and are illustrated in Fig. E.2.

                                                  Figure E.2: The phase portrait for the linear Hamiltonian center.
  The flow generated by this vector field is given in Chapter 2, Problem Set 2, problem 5.

We will now consider two examples of bifurcation of equilibria in two dimensional Hamiltonian systems. Bifurcation associated
with one zero eigenvalue (as we studied in Chapter 8) is not possible since, following (E.6), if there is one zero eigenvalue the other
eigenvalue must also be zero. We will consider examples of the Hamiltonian saddle-node and Hamiltonian pitchfork bifurcations.
Discussions of the Hamiltonian versions of these bifurcations can also be found in Golubitsky et al.

                             11.5.3                                              https://math.libretexts.org/@go/page/24235
Example 11.5.43 (Hamiltonian saddle-node bifurcation)

We consider the Hamiltonian:

                                     H(q, p) = p - 2 q + q ), ( 3 q, p)  R2,                                        (11.5.13)
                                           2                3                                                       (11.5.14)

where   is considered to be a parameter that can be varied. From this Hamiltonian, we derive Hamilton's equations:

                                        q  =  H        (q,  p)  =  p  ,
                                              p

                                     p = - H (q, p) =  - q2,
                                          p

The fixed points for (E.14) are:

                                                            -                                                       (11.5.15)

                                        (q, p) = (±, 0),

from which it follows that there are no fixed points for  < 0 , one fixed point for  = 0 , and two fixed points for  > 0 . This

is the scenario for a saddle-node bifurcation.

Next we examine stability of the fixed points. The Jacobian of (E.14) is given by:

                                           ( 0 1 -2q 0 ) .                                                          (11.5.16)

The eigenvalues of this matrix are:

                                             ----

                                        1,2 = ± -2q .

Hence (q, p) = (-, 0) is a saddle, (q, p) = (, 0) is a center, and (q, p) = (0, 0) has two zero eigenvalues. The phase--

portraits are shown in Fig. E.3.

                                              11.5.4                                https://math.libretexts.org/@go/page/24235
Figure E.3: The phase portraits for the Hamiltonian saddle-node bifurcation.

Example 11.5.44 (Hamiltonian pitchfork bifurcation)

We consider the Hamiltonian:

                                  H(q, p) = p - 2  q + ), ( 2 q4 q, p)  R2,                                        (11.5.17)
                                  2                     24                                                         (11.5.18)

where  is considered to be a parameter that can be varied. From this Hamiltonian, we derive Hamilton's equations:

                                  q  =               H  (q,  p)  =  p  ,
                                                     p

                                  p = - H (q, p) = q - q3,
                                       p

The fixed points for (E.18) are:

                                                     11.5.5                  https://math.libretexts.org/@go/page/24235
                                     (q, p) = (0, 0), (±p, 0),                 (11.5.19)

from which it follows that there is one fixed point for  < 0 , one fixed point for  = 0 , and three fixed points for  > 0 . This
is the scenario for a pitchfork bifurcation.

Next we examine stability of the fixed points. The Jacobian of (E.18) is given by:

                                     (  - 3q2 0 10 ) .                         (11.5.20)

The eigenvalues of this matrix are:

                                          ------
                                                                            2

                                     1,2 = ±  - 3q .

Hence (q, p) = (0, 0) is a center for  < 0 , a saddle for  > 0 and has two zero eigenvalues for  = 0 . The fixed points
(q, p) = (p, 0) are centers for  > 0 . The phase portraits are shown in Fig. E.4.

Figure E.4: The phase portraits for the Hamiltonian pitchfork bifurcation.

                                     11.5.6                                    https://math.libretexts.org/@go/page/24235
We remark that, with a bit of thought, it should be clear that in two dimensions there is no analog of the Hopf bifurcation for
Hamiltonian vector fields similar to to the situation we analyzed earlier in the non-Hamiltonian context. There is a situation that is
referred to as the Hamiltonian Hopf bifurcation, but this notion requires at least four dimensions, see Van Der Meer.

In Hamiltonian systems a natural bifurcation parameter is the value of the level set of the Hamiltonian, or the "energy". From this
point of view perhaps a more natural candidate for a Hopf bifurcation in a Hamiltonian system is described by the Lyapunov
subcenter theorem, see Kelley. The setting for this theorem also requires at least four dimensions, but the associated phenomena
occur quite often in applications.

This page titled 11.5: E- Dynamics of Hamilton's Equations is shared under a CC BY 4.0 license and was authored, remixed, and/or curated by

Stephen Wiggins via  source content that was edited to the style and standards of the LibreTexts platform.

11.5.7  https://math.libretexts.org/@go/page/24235
Index                                                 D                                                     Lyapunov Stability

A                                                     dimensional reduction                                    3: Behavior Near Trajectories and Invariant Sets -
                                                                                                            Stability
autonomous differential equation                         10.1: Center Manifold Theory
                                                                                                            Lyapunov's method
   1: Getting Started - The Language of ODEs          H
                                                                                                               7.1: Lyapunov's Method and the LaSalle Invariance
autonomous vector fields                              Hopf bifurcation                                      Principle

   10.1: Center Manifold Theory                          9.1: Bifurcation of Equilibria II                  N

B                                                     I                                                     nonautonomous differential equation

Bendixson's Criterion                                 index theorem                                            1: Getting Started - The Language of ODEs

   7.1: Lyapunov's Method and the LaSalle Invariance     7.1: Lyapunov's Method and the LaSalle Invariance  nonhyperbolic points
Principle                                             Principle
                                                                                                               10.1: Center Manifold Theory
C                                                     L
                                                                                                            S
Center Manifold Theory                                LaSalle Invariance Principle
                                                                                                            saddle
   10.1: Center Manifold Theory                          7.1: Lyapunov's Method and the LaSalle Invariance
                                                      Principle                                                6: Stable and Unstable Manifolds of Equilibria
center manifolds
                                                      Lyapunov function                                     sink
   10.1: Center Manifold Theory
                                                         11.3: Finding Lyapunov Functions                      6: Stable and Unstable Manifolds of Equilibria

                                                                                                            stability of fixed points

                                                                                                               10.1: Center Manifold Theory

                                                      1                                                     https://math.libretexts.org/@go/page/38033
F: A Brief Introduction to the Characteristics of Chaos

In this appendix we will describe some aspects of the phenomenon of chaos as it arises in ODEs. Chaos is one of those notable
topics that crosses disciplinary boundaries in mathematics, science, and engineering and captures the intrigue and curiousity of the
general public. Numerous popularizations and histories of the topic, from different points of view, have been written; see, for
example the books by Lorenz, Diacu and Holmes, Stewart, and Gleick.

Our goal here is to introduce some of the key characteristics of chaos based on notions that we have already developed so as to
frame possible future directions of studies that the student might wish to pursue. Our discussion will be in the setting of a flow
generated by an autonomous vector field.

The phrase "chaotic behavior" calls to mind a form of randomness and unpredictability. But keep in mind, we are working in the
setting of -our ODE satisfies the criteria for existence and uniqueness of solutions. Therefore specifying the initial condition
exactly implies that the future evolution is uniquely determined, i.e. there is no "randomness or unpredictability". The key here is
the word "exactly". Chaotic systems have an intrinsic property in their dynamics that can result in slight perturbations of the initial
conditions leading to behavior, over time, that is unlike the behavior of the trajectory though the original initial condition. Often it
is said that a chaotic system exhibits sensitive dependence on initial conditions. Now this is a lot of words for a mathematics
course. Just like when we studied stability, we will give a mathematical definition of sensitive dependence on initial conditions, and
then consider the meaning of the definition in the context of specific examples.

As mentioned above, we consider an autonomous, C r, r  1 vector field on Rn:

                                                    x = f(x), x  Rn,                                                         (F.1)

and we denote the flow generated by the vector field by t(), and we assume that it exists for all time. We let   Rn denote an

invariant set for the flow. Then we have the following definition.

Definition 22 (Sensitive dependence on initial conditions)

The flow t() is said to have sensitive dependence on initial conditions on  if there exists  > 0 such that, for any x   and
any neighborhood U of x there exists y  U and t > 0 such that |t(x) - t(y)| >  .

Now we consider an example and analyze whether or not sensitive dependence on initial conditions is present in the example.

Example F . 45

Consider the autonomous linear vector field on R2:
x = x ,

                                                    y = y. (x, y)  R2                               (F.2)

with ,  > 0 . This is just a standard "saddle point". The origin is a fixed point of saddle type with its stable manifold given by

the y axis (i.e. x = 0) and its unstable manifold given by the x axis (i.e. y = 0). The flow generated by this vector field is given
by:

                t(x0, y0) = (x0et , y0et).                                                          (F.3)

Following the definition, sensitive dependence on initial conditions is de- fined with respect to invariant sets. Therefore we
must identify the invariant sets for which we want to determine whether or not they possess the property of sensitive
dependence on initial condition.

The simplest invariant set is the fixed point at the origin. However, that invariant set clearly does not exhibit sensitive
dependence on initial conditions.

Then we have the one dimensional stable (y axis) and unstable manifolds (x axis). We can consider the issue of sensitive
dependence on initial conditions on these invariant sets. The stable and unstable manifolds divide the plane into four quadrants.
Each of these is an invariant set (with a segment of the stable and unstable manifold forming part of their boundary), and the
entire plane (i.e. the entire phase space) is also an invariant set.

We consider the unstable manifold, y = 0. The flow restricted to the unstable manifold is given by

                                                    F.1                       https://math.libretexts.org/@go/page/24242
                                                      t(x0, 0) = x0et , 0)                       (F.4)

It should be clear the the unstable manifold is an invariant set that exhibits sensitive dependence on initial conditions. Choose

an arbitrary point on the unstable manifold, x¯1. Consider another point arbitrarily close to x¯1, x¯2. Now consider any  > 0 . We

have

                |t(x¯1, 0) - |t(x¯2, 0)| = |x¯1 - x¯2|et                                         (F.5)

Now since |x¯1 - x¯2| is a fixed constant, we can clearly find a t > 0 such that

                                                       |x¯1 - x¯2|et > .                         (F.6)

Therefore the invariant unstable manifold exhibits sensitive dependence on initial conditions. Of course, this is not surprising
because of the elt term in the expression for the flow since this term implies exponential growth in time of the x component of
the flow.

The stable manifold, x = 0, does not exhibit sensitive dependence on initial conditions since the restriction to the stable
manifold is given by:

                                                      t(0, y0) = (0, y0et),                      (F.7)

which implies that neighboring points actually get closer together as t increases.

Moreover, the term et implies that the four quadrants separated by the stable and unstable manifolds of the origin also exhibit

sensitive dependence on initial conditions.

Of course, we would not consider a linear autonomous ODE on the plane having a hyperbolic saddle point to be a chaotic
dynamical system, even though it exhibits sensitive dependence on initial conditions. Therefore there must be something more to
"chaos", and we will explore this through more examples.

Before we consider the next example we point out two features of this example that we will consider in the context of other
examples.

 1. The invariant sets that we considered (with the exception of the fixed point at the origin) were unbounded. This was a feature of
    the linear nature of the vector field.

 2. The "separation of trajectories" occurred at an exponential rate.Therewas no requirement on the "rate of separation" in the
    definition of sensitive dependence on initial conditions.

 3. Related to these two points is the fact that trajectories continue to separate for all time, i.e. they never again "get closer" to each
    other.

Example F . 46

Consider the autonomous vector field on the cylinder:

                                                       r = 0 ,

                 = r, (r, q)  R+ × S1,                                                           (F.8)

The flow generated by this vector field is given by:

                t(r0, q0) = (r0, r0t + 0),                                                       (F.9)

Note that r is constant in time. This implies that any annulus is an invariant set. In particular, choose any r1 < r2 . Then the

annulus

                A  {(r, q)  R+ × S1|r1  r  r2,   S1}                                             (F.10)

is a bounded invariant set.

Now choose initial conditions in A, (r1, 1), (r2, 2), with r1  r1 < r2  r2 . Then we have that:

                               |t(r1, 1)t(r2, 2)| = |(r1, r1t + 1) - (r2, r2t + 2)| ,
                                        = (r1 - r2, (r1 - r2)t + (1 - 2)) .

                                                       F.2                          https://math.libretexts.org/@go/page/24242
Hence we see that the distance between trajectories will grow linearly in time, and therefore trajectories exhibit sensitive

dependence on initial conditions. However, the distance between trajectories will not grow unboundedly (as in the previous

example). This is because  is on the circle. Trajectories will move apart (in , but their r values will remain constant) and then

come close, then move apart, etc. Nevertheless, this is not an example of a chaotic dynamical system.

Example F . 47

Consider the following autonomous vector field defined on the two dimensional torus (i.e. each variable is an angular
variable):

                                                     q1 = 1 ,

                                                   q2 = 2, (1, 2)  S 1 × S 1                                               (F.11)

This vector field is an example that is considered in many dynamical systems courses where it is shown that if             1  is an
                                                                                                                           2
irrational number, then the trajectory through any initial condition "densely fills out the torus". This means that given any point

on the torus any trajectory will get arbitrarily close to that point at some time of its evolution, and this "close approach" will

happen infinitely often. This is the classic example of an ergodic system, and this fact is proven in many textbooks, e.g. Arnold
                                                                                      1
or Wiggins. This behavior is very different from the previous examples. For the case  2  an irrational number, the natural

invariant set to consider is the entire phase space (which is bounded).

Next we consider the issue of sensitive dependence on initial conditions. The flow generated by this vector field is given by:

                                         t(1, 2) = (1t + 1, 2t + 2).                                                       (F.12)

We choose two initial conditions, (1, 2), (1, 2). Then we have
                       |t(1, 2) - t(1, 2)| = |(1t + 1, 2t + 2) - (1t + 1, 2t + 2| ,
                                               = |(1 - 1, 2 - 2)| ,

and therefore trajectories always maintain the same distance from each other during the course of their evolution.

Sometimes it is said that chaotic systems contain an infinite number of unstable periodic orbits. We consider an example.

Example F . 48

Consider the following two dimensional autonomous vector field on the cylinder:

                                                   r  =  si  n(     )    ,
                                                                 r

                                                   q = r, (r, q)  R+ × S1 .

Equilibrium points of the r component of this vector field correspond to periodic orbits. These equilibrium points are given by

                                                   r = 1n , n = 0, 1, 2, 3, . . . .                                        (F.13)

Stability of the periodic orbits can be determined by computing the Jacobian of the r component of the equation and evaluating
it on the periodic orbit. This is given by:

                                                   -      cos(      ,
                                                      r2         r

and evaluating this on the periodic orbits gives;

                                                   - n2  (-1)n

Therefore all of these periodic orbits are hyperbolic and stable for n even and unstable for n odd. This is an example of a two
dimensional autonomous vector field that contains an infinite number of unstable hyperbolic periodic orbits in a bounded
region, yet it is not chaotic.

Now we consider what we have learned from these four examples. In example 45 we identified invariant sets on which the
trajectories exhibited sensitive dependence on initial conditions (i.e. trajectories separated at an exponential rate), but those

                                                          F.3                            https://math.libretexts.org/@go/page/24242
invariant sets were unbounded, and the trajectories also became unbounded. This illustrates why boundedness is part of the
definition of invariant set in the context of chaotic systems.

In example 46 we identified an invariant set, A, on which all trajectories were bounded and they exhibited sensitive dependence on

initial conditions, although they only separated linearly in time. However, the r coordinates of all trajectories remained constant,

indicating that trajectories were constrained to lie on circles ("invariant circles") within A.

                          1
In example 47, for 2 an irrational number, every trajectory densely fills out the entire phase space, the torus (which is bounded).
However, the trajectories did not exhibit sensitive dependence on initial conditions.

Finally, in example 48 we gave an example having an infinite number of unstable hyperbolic orbits in a bounded region of the
phase space. We did not explicitly examine the issue of sensitive dependence on initial conditions for this example.

So what characteristics would we require of a chaotic invariant set? A combination of examples 45 and 47 would capture many
manifestations of "chaotic invariant sets":

 1. the invariant set is bounded,
 2. every trajectory comes arbitrarily close to every point in the invariant set during the course of its evolution in time, and
 3. every trajectory has sensitive dependence on initial condition.

While simple to state, developing a unique mathematical framework that makes these three criteria mathematically rigorous, and
provides a way to verify them in particular examples, is not so straight-forward.

Property 1 is fairly straightforward, once we have identified a candidate invariant set (which can be very difficult in explicit
ODEs). If the phase space is equipped with a norm, then we have a way of verifying whether or not the invariant set is bounded.

Property 2 is very difficult to verify, as well as to develop a universally accepted definition amongst mathematicians as to what it
means for "every trajectory to come arbitrarily close to every point in phase space during the course of its evolution". Its definition
is understood within the context of recurrence properties of trajectories. Those can be studied from either the topological point of
view (see Akin). or from the point of view of ergodic theory (see Katok and Hasselblatt or Brin and Stuck). The settings for both of
these points of view utilize different mathematical structures (topology in the former case, measure theory in the latter case). A
book that describes how both of these points of view are used in the application of mixing is Sturman et al.

Verifying that Property 3 holds for all trajectories is also not straight-forward. What "all" means is different in the topological
setting ("open set", Baire category) and the ergodic theoretic setting (sets of "full measure"). What "sensitive dependence on initial
conditions" means is also different in each setting. The definition we gave above was more in the spirit of the topological point of
view (no specific "rate of separation" was given) and the ergodic theoretic framework focuses

on Lyapunov exponents ("Lyapunov's first method") and exponential rate of separation of trajectories.

Therefore we have not succeeded in giving a specific example of an ODE whose trajectories behave in a chaotic fashion. We have
been able to describe some of the issues, but the details will be left for other courses (which could be either courses in dynamical
systems theory or ergodic theory or, ideally, a bit of both). But we have illustrated just how difficult it can be to formulate
mathematically precise definitions that can be verified in specific examples.

All of our examples above were two dimensional, autonomous vector fields. The type of dynamics that can be exhibited by such
systems is very limited, according to the Poincaré-Bendixson theorem (see Hirsch et al. or Wiggins). There are a number of
variations of this theorem, so we will leave the exploration of this theorem to the interested student.

F.4  https://math.libretexts.org/@go/page/24242
Index                                                 D                                                     Lyapunov Stability

A                                                     dimensional reduction                                    3: Behavior Near Trajectories and Invariant Sets -
                                                                                                            Stability
autonomous differential equation                         10.1: Center Manifold Theory
                                                                                                            Lyapunov's method
   1: Getting Started - The Language of ODEs          H
                                                                                                               7.1: Lyapunov's Method and the LaSalle Invariance
autonomous vector fields                              Hopf bifurcation                                      Principle

   10.1: Center Manifold Theory                          9.1: Bifurcation of Equilibria II                  N

B                                                     I                                                     nonautonomous differential equation

Bendixson's Criterion                                 index theorem                                            1: Getting Started - The Language of ODEs

   7.1: Lyapunov's Method and the LaSalle Invariance     7.1: Lyapunov's Method and the LaSalle Invariance  nonhyperbolic points
Principle                                             Principle
                                                                                                               10.1: Center Manifold Theory
C                                                     L
                                                                                                            S
Center Manifold Theory                                LaSalle Invariance Principle
                                                                                                            saddle
   10.1: Center Manifold Theory                          7.1: Lyapunov's Method and the LaSalle Invariance
                                                      Principle                                                6: Stable and Unstable Manifolds of Equilibria
center manifolds
                                                      Lyapunov function                                     sink
   10.1: Center Manifold Theory
                                                         11.3: Finding Lyapunov Functions                      6: Stable and Unstable Manifolds of Equilibria

                                                                                                            stability of fixed points

                                                                                                               10.1: Center Manifold Theory
Detailed Licensing                                                 8: Bifurcation of Equilibria I - CC BY 4.0

Overview                                                               8.1: Bifurcation of Equilibria I - CC BY 4.0
                                                                       8.2: Problem Set - CC BY 4.0
Title: Ordinary Differential Equations (Wiggins)
                                                                   9: Bifurcation of Equilibria II - CC BY 4.0
Webpages: 42
                                                                       9.1: Bifurcation of Equilibria II - CC BY 4.0
All licenses found:                                                    9.2: Problem Set - CC BY 4.0

    CC BY 4.0: 76.2% (32 pages)                                    10: Center Manifold Theory - CC BY 4.0
    Undeclared: 23.8% (10 pages)
                                                                       10.1: Center Manifold Theory - CC BY 4.0
By Page                                                                10.E: Center Manifold Theory (Exericses) - CC BY
                                                                       4.0
    Ordinary Differential Equations (Wiggins) - CC BY 4.0
        Front Matter - Undeclared                                  11: Appendices - Undeclared
            TitlePage - Undeclared
            InfoPage - Undeclared                                      11.1: A- Jacobians, Inverses of Matrices, and
            Table of Contents - Undeclared                             Eigenvalues - CC BY 4.0
            Licensing - Undeclared                                     11.2: B- Integration of Some Basic Linear ODEs - CC
        1: Getting Started - The Language of ODEs - CC BY 4.0          BY 4.0
            1.1: Problem Set - CC BY 4.0                               11.3: Finding Lyapunov Functions - CC BY 4.0
        2: Special Structure and Solutions of ODEs - CC BY 4.0         11.4: D- Center Manifolds Depending on Parameters
            2.1: Problem Set - CC BY 4.0                               - CC BY 4.0
        3: Behavior Near Trajectories and Invariant Sets -             11.5: E- Dynamics of Hamilton's Equations - CC BY
        Stability - CC BY 4.0                                          4.0
            3.1: Problem Set - CC BY 4.0
        4: Behavior Near Trajectories - Linearization - CC BY      Back Matter - CC BY 4.0
        4.0
            4.1: Problem Set - CC BY 4.0                               Index - Undeclared
        5: Behavior Near Equilbria - Linearization - CC BY 4.0         F: A Brief Introduction to the Characteristics of
            5.1: Problem Set - CC BY 4.0                               Chaos - CC BY 4.0
        6: Stable and Unstable Manifolds of Equilibria - CC BY         Index - Undeclared
        4.0                                                            Glossary - Undeclared
            6.1: Problem Set - CC BY 4.0                               Detailed Licensing - Undeclared
        7: Lyapunov's Method and the LaSalle Invariance
        Principle - CC BY 4.0
            7.1: Lyapunov's Method and the LaSalle Invariance
            Principle - CC BY 4.0
            7.2: Problem Set - CC BY 4.0

                                                                1  https://math.libretexts.org/@go/page/115476
