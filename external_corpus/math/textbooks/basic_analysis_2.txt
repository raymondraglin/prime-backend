            Basic Analysis II

Introduction to Real Analysis, Volume II

                                                       by Jií Lebl
                                                     May 23, 2025
                                                       (version 6.2)
2

Typeset in LATEX.
Copyright ©2012-2025 Jií Lebl

This work is dual licensed under the Creative Commons Attribution-Noncommercial-Share
Alike 4.0 International License and the Creative Commons Attribution-Share Alike 4.0 Inter-
national License. To view a copy of these licenses, visit https://creativecommons.org/
licenses/by-nc-sa/4.0/ or https://creativecommons.org/licenses/by-sa/4.0/ or
send a letter to Creative Commons PO Box 1866, Mountain View, CA 94042, USA.
You can use, print, duplicate, and share this book as much as you want. You can base your
own notes on it and reuse parts if you keep the license the same. You can assume the
license is either CC-BY-NC-SA or CC-BY-SA, whichever is compatible with what you wish
to do. Your derivative work must use at least one of the licenses. Derivative works must be
prominently marked as such.
During the writing of these notes, the author was in part supported by NSF grant
DMS-1362337.
The date is the main identifier of version. The major version / edition number is raised
only if there have been substantial changes. From 6th edition onwards, both volumes share
the same version number.
See https://www.jirka.org/ra/ for more information (including contact information,
possible updates, and errata).
The LATEX source for the book is available for possible modification and customization at
github: https://github.com/jirilebl/ra
Contents

Introduction                                      5

8 Several Variables and Partial Derivatives       7

8.1 Vector spaces, linear mappings, and convexity . . . . . . . . . . . . . . . . . 7

8.2 Analysis with vector spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

8.3 The derivative . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

8.4 Continuity and the derivative . . . . . . . . . . . . . . . . . . . . . . . . . . . 46

8.5 Inverse and implicit function theorems . . . . . . . . . . . . . . . . . . . . . . 51

8.6 Higher order derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60

9 One-dimensional Integrals in Several Variables  65

9.1 Differentiation under the integral . . . . . . . . . . . . . . . . . . . . . . . . . 65

9.2 Path integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71

9.3 Path independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83

10 Multivariable Integral                         91

10.1 Riemann integral over rectangles . . . . . . . . . . . . . . . . . . . . . . . . . 91

10.2 Iterated integrals and Fubini theorem . . . . . . . . . . . . . . . . . . . . . . 103

10.3 Outer measure and null sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108

10.4 The set of Riemann integrable functions . . . . . . . . . . . . . . . . . . . . . 117

10.5 Jordan measurable sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123

10.6 Green's theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128

10.7 Change of variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134

11 Functions as Limits                            139

11.1 Complex numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139

11.2 Swapping limits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144

11.3 Power series and analytic functions . . . . . . . . . . . . . . . . . . . . . . . . 153

11.4 Complex exponential and trigonometric functions . . . . . . . . . . . . . . . 163

11.5 Maximum principle and the fundamental theorem of algebra . . . . . . . . 169

11.6 Equicontinuity and the Arzelà-Ascoli theorem . . . . . . . . . . . . . . . . . 172

11.7 The Stone-Weierstrass theorem . . . . . . . . . . . . . . . . . . . . . . . . . . 179

11.8 Fourier series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
4                 CONTENTS
Further Reading              209
Index                        211
List of Notation             215
Introduction

About this book

This second volume of "Basic Analysis" is meant to be a seamless continuation. The
chapter numbers start where the first volume left off. The book started with my notes for a
second-semester undergraduate analysis at University of Wisconsin--Madison in 2012,
which I taught more or less with Rudin's book. Some of the material and some of the
proofs are similar to Rudin, though I try to provide more detail and context. In 2016, I
taught a second-semester undergraduate analysis at Oklahoma State University, modifying
and cleaning up the notes, this time using them as the main text. I have since taught the
course several more times, adding chapter 11 (originally written for the Wisconsin course),
and making many other smaller improvments.

    I plan to eventually add a few more topics. I will try to preserve the numbering in
subsequent editions as always. The new topics planned would add chapters onto the end
of the book, or add sections to end of existing chapters, and I will try as hard as possible to
leave exercise numbers unchanged.

    For the most part, this second volume depends on the non-optional parts of volume I,
while some of the optional parts are also used. Higher order derivatives (but not Taylor's
theorem itself) are used in 8.6, 9.3, 10.6. Exponentials, logarithms, and improper integrals
are used in a few examples and exercises, and they are heavily used in chapter 11.

    An alternate plan for a two-semester course is that some bits of the first volume, such
as metric spaces, are covered in the second semester, while some of the optional topics of
volume I are covered in the first semester. Leaving metric spaces for the second semester
makes the second semester the "multivariable" part of the course.

    Several possibilities for things to cover after metric spaces, depending on time are:

1) 8.1-8.5, 10.1-10.5, 10.7 (multivariable calculus, focus on multivariable integral).

2) Chapter 8, chapter 9, 10.1 and 10.2 (multivariable calculus, focus on path integrals).

3) Chapters 8, 9, and 10 (multivariable calculus, path integrals, multivariable integrals).

4) Chapters 8, (maybe 9), and 11 (multivariable differential calculus, some advanced
    analysis).

5) Chapter 8, chapter 9, 11.1, 11.6, 11.7 (a simpler variation of the above).
6  INTRODUCTION
Chapter 8

Several Variables and Partial Derivatives

8.1 Vector spaces, linear mappings, and convexity

Note: 3 lectures

8.1.1 Vector spaces

The euclidean space  has already made an appearance in the metric space chapter. In
this chapter, we extend the differential calculus we created for one variable to several
variables. The key idea in differential calculus is to approximate differentiable functions by
linear functions (approximating the graph by a straight line). In several variables, we must
introduce a little bit of linear algebra before we can move on. We start with vector spaces
and linear mappings of vector spaces.

    It is common to use ì or the bold v for elements of , especially in the applied sciences.
We use just plain old , which is common in mathematics. That is,    is a vector, which
means  = (1, 2, . . . , ) is an -tuple of real numbers. At times, it is useful to write and
treat vectors as column vectors, that is, -by-1 matrices:

                                                                                                   1
                                                                                                   2

                                           = (1, 2, . . . , ) = ..
                                                                         .

                                                                                                  

We do so when convenient. We call real numbers scalars to distinguish them from vectors.
    We often think of vectors as a direction and a magnitude and draw the vector as an

arrow. The vector (1, 2, . . . , ) is represented by an arrow from the origin to the point
(1, 2, . . . , ). When we think of vectors as arrows, they are not based at the origin
necessarily; a vector is simply the direction and the magnitude, and it does not know where
it starts. There is a natural algebraic structure when thinking of vectors as arrows. We can
add vectors as arrows by following one vector and then the other. And we can take scalar
multiples of vectors as arrows by rescaling the magnitude. See Figure 8.1.

     Subscripts are used for many purposes, so sometimes we may have several vectors that may also be
identified by subscript, such as a finite or infinite sequence of vectors 1, 2, . . ..
8  CHAPTER 8. SEVERAL VARIABLES AND PARTIAL DERIVATIVES

                             2 (1, 2) 
                                                                  +   2

                                            1

   Figure 8.1: Vector as an arrow in 2, and the meaning of addition and scalar multiplication.

    A vector may also represent a point in . Usually, we think of    as a point if
we are thinking of  as a metric space, and we think of it as an arrow if we think of the
so-called vector space structure on  (addition and scalar multiplication). Let us define the
abstract notion of a vector space, as there are many other vector spaces than just .

Definition 8.1.1. Let  be a set together with the operations of addition, + :  ×   ,
and multiplication, · :  ×   , (we usually write  instead of  · ). The set  is called

a vector space (or a real vector space) if the following conditions are satisfied:

   (i) (Addition is associative)  If , ,   , then  + ( + ) = ( + ) + .

   (ii) (Addition is commutative) If ,   , then  +  =  + .

   (iii) (Additive identity)      There is a 0   such that  + 0 =  for all   .

   (iv) (Additive inverse)        For each   , there is a -  , such that  +(-) = 0.

   (v) (Distributive law)         If   , and ,   , then ( + ) =  + .

   (vi) (Distributive law)        If ,   , and   , then ( + ) =  + .

(vii) (Multiplication is associative) If ,   , and   , then () = ().

(viii) (Multiplicative identity)  1 =  for all   .

Elements of a vector space are usually called vectors, even if they are not elements of 
(vectors in the "traditional" sense). If    is a subset that is a vector space itself using
the same operations, then  is called a subspace or a vector subspace of .

    Multiplication by scalars works as one would expect. For example, 2 = (1 + 1) =
1 + 1 =  + , similarly 3 =  +  + , and so on. One particular fact we often use is that
0 = 0, where the zero on the left is 0   and the zero on the right is 0  . To see this,
start with 0 = (0 + 0) = 0 + 0, and add -(0) to both sides to obtain 0 = 0. Similarly,
- = (-1), which follows by (-1) +  = (-1) + 1 = (-1 + 1) = 0 = 0. Such algebraic

facts which follow quickly from the definition will be taken for granted from now on.

Example 8.1.2: The set  is a vector space; the zero vector is (0, . . . , 0), addition and
multiplication by a scalar is done componentwise: If   ,  = (1, 2, . . . , )  , and
 = (1, 2, . . . , )  , then

    +  (1, 2, . . . , ) + (1, 2, . . . , ) = (1 + 1, 2 + 2, . . . ,  + ),
    (1, 2, . . . , ) = (1, 2, . . . , ).
8.1. VECTOR SPACES, LINEAR MAPPINGS, AND CONVEXITY  9

    We will mostly deal with "finite-dimensional" vector spaces that can be regarded as
subsets of , but other vector spaces are useful in analysis. It is better to think of even
such simpler vector spaces abstractly abstract notion rather than as .

Example 8.1.3: A trivial example of a vector space is  {0}. The operations are defined
in the obvious way: 0 + 0 0 and 0 0. A zero vector must always exist, so all vector
spaces are nonempty sets, and this  is the smallest possible vector space.

Example 8.1.4: The space ([0, 1], ) of continuous functions on the interval [0, 1] is a
vector space. For two functions  and  in ([0, 1], ) and   , we make the obvious
definitions of  +  and   :

                           (  + )()  () + (), (  )()   () .

The 0 is the function that is identically zero. We leave it as an exercise to check that all the
vector space conditions are satisfied. The space 1([0, 1], ) of continuously differentiable
functions is a subspace of ([0, 1], ).

Example 8.1.5: The space of polynomials 0 + 1 + 22 + · · · +  (of arbitrary degree )
is a vector space, denoted by [] (coefficients are real and the variable is ). The operations
are defined in the same way as for functions above. Suppose there are two polynomials,
one of degree  and one of degree . Assume    for simplicity. Then

  (0 + 1 + 22 + · · · + ) + (0 + 1 + 22 + · · · + ) =
               (0 + 0) + (1 + 1) + (2 + 2)2 + · · · + ( + ) + +1+1 + · · · + 

and
             (0 + 1 + 22 + · · · + ) = (0) + (1) + (2)2 + · · · + ().

Despite what it looks like, [] is not equivalent to  for any . In particular, it is not
"finite-dimensional." We will make this notion precise in just a little bit. One can make a
finite-dimensional vector subspace by restricting the degree. For example, if P is the set
of polynomials of degree  or less, then P is a finite-dimensional vector space, and we
could identify it with +1.

    Above, the variable  is really just a formal placeholder. By setting  equal to a real
number, we obtain a function. So the space [] can be thought of as a subspace of (, ).
If we restrict the range of  to [0, 1], [] can be identified with a subspace of ([0, 1], ).

Proposition 8.1.6. For    to be a vector subspace of a vector space , we only need to check:
1) 0  .
2)  is closed under addition: If ,   , then  +   .
3)  is closed under scalar multiplication: If    and   , then   .
10  CHAPTER 8. SEVERAL VARIABLES AND PARTIAL DERIVATIVES

    Items 2) and 3) ensure that addition and scalar multiplication are indeed defined on .
Item 1) is required to fulfill item (iii) from the definition of vector space. Existence of
additive inverse -, item (iv), follows because - = (-1) and item 3) says that -   if
  . All other properties are certain equalities that are already satisfied in  and thus
must be satisfied in a subset.

    It is possible to use other fields than  in the definition (for example, it is common to
use the complex numbers ), but let us stick with the real numbers.

8.1.2 Linear combinations and dimension

Definition 8.1.7. Suppose  is a vector space, 1, 2, . . . ,    are vectors, and
1, 2, . . . ,    are scalars. Then

                                           11 + 22 + · · · + 

is called a linear combination of the vectors 1, 2, . . . , .
    For a subset   , let span(), or the span of , be the set of all linear combinations of

all finite subsets of . We say  spans span(). By convention, define span() {0}.
Example 8.1.8: Let  (1, 1)  2. Then

    span() = (, )  2 :    .

That is, span() is the line through the origin and the point (1, 1).
Example 8.1.9: Let  (1, 1), (0, 1)  2. Then

    span() = 2,

as every point (, )  2 can be written as a linear combination

    (, ) = (1, 1) + ( - )(0, 1).

Example 8.1.10: Let  {1, , 2, 3, . . .}  [] and             {1, 2, 4, 6, . . .}  []. The
span of  is all polynomials,

                                              span() = [].

The span of  is the set of polynomials with even powers of  only.

    Suppose we have two linear combinations of vectors from . One linear combination
uses the vectors {1, 2, . . . , }, and the other uses {1, 2, . . . , }. We can write their
sum using vectors from the union {1, 2, . . . , }  {1, 2, . . . , }:

(11 + 22 + · · · +  ) + (11 + 22 + · · · +  )
                                          = 11 + 22 + · · · +  + 11 + 22 + · · · + .

  If you want a very funky vector space over a different field,  itself is a vector space over the field .
8.1. VECTOR SPACES, LINEAR MAPPINGS, AND CONVEXITY  11

So the sum is also a linear combination of vectors from . Similarly, a scalar multiple of a
linear combination of vectors from  is a linear combination of vectors from :

(11 + 22 + · · · + ) = 11 + 22 + · · · + .

Finally, 0  span(); if  is nonempty, 0 = 0 for some   . We have proved the following
proposition.

Proposition 8.1.11. Let  be a vector space and    is a subset. Then the set span() is a
vector subspace of .

    Every linear combination of elements in a subspace is an element of that subspace. So
span() is the smallest subspace that contains . In particular, if  is already a vector
subspace, then span() = .

Definition 8.1.12. A set of vectors {1, 2, . . . , }   is linearly independent if the equation

11 + 22 + · · · +  = 0                              (8.1)

has only the trivial solution 1 = 2 = · · · =  = 0. By convention,  is linearly independent.

A set that is not linearly independent is linearly dependent. A linearly independent set of
vectors    such that span() =  is called a basis of . We generally consider the basis
as not just a set, but as an ordered -tuple: 1, 2, . . . , .

    Suppose  is largest integer for which  contains a set of  linearly independent vectors.
We then say  is the dimension of , and we write dim  . If  contains a set of 
linearly independent vectors for arbitrarily large , we say  is infinite-dimensional and
write dim  . For the trivial vector space {0}, we define dim {0} 0.

    A subset of a linearly independent set is clearly linearly independent. So if a set contains
 linearly independent vectors, it also contains a set of  linearly independent vectors for
all   . Moreover, if a set does not have  + 1 linearly independent vectors, no set of
more than  + 1 vectors is linearly independent. So  is of dimension is  if there is a set of
 linearly independent vectors, but no set of  + 1 vectors is linearly independent.

    No element of a linearly independent set can be zero, and a set with one nonzero
element is always linearly independent. In particular, {0} is the only vector space of

dimension 0. Every other vector space has a positive dimension or is infinite-dimensional.
As the empty set is linearly independent, it is a basis of {0}.

    As an example, the set  of the two vectors in Example 8.1.9 is a basis of 2, and
so dim 2  2. We will see in a moment that every vector subspace of  has a finite
dimension, and that dimension is less than or equal to . So every set of 3 vectors in 2 is
linearly dependent, and dim 2 = 2.

    If a set is linearly dependent, then one of the vectors is a linear combination of the
others. In (8.1), if   0, then we solve for :

                     = -1  1 + · · · + --1  -1 + -+1  +1 + · · · + -   .

     For an infinite set   , we say  is linearly independent if every finite subset of  is linearly

independent in the sense given. However, this situation only comes up in infinitely many dimensions.
12  CHAPTER 8. SEVERAL VARIABLES AND PARTIAL DERIVATIVES

The vector  has at least two different representations as linear combinations of the vectors
{1, 2, . . . , }. The one above and  itself. For instance, the set (0, 1), (2, 3), (5, 0) in
2 is linearly dependent:

    3(0, 1) - (2, 3) + 2(1, 0) = 0, so (2, 3) = 3(0, 1) + 2(1, 0).

Proposition 8.1.13. Suppose a vector space  has a basis  = {1, 2, . . . , }. Then every
   has a unique representation of the form

                                                                               

                                                    =  

                                                                             =1

for some scalars 1, 2, . . . , .

Proof. As  is the span of , every    is a linear combination of elements of . Suppose

        

     =   =  .

    =1  =1

Then 
                                                   ( - ) = 0.

                                                                =1

By linear independence of the basis,  =  for all , and so the representation is unique. 

    For , we define the standard basis of :

    1 (1, 0, 0, . . . , 0), 2 (0, 1, 0, . . . , 0), . . . ,  (0, 0, 0, . . . , 1).

We use the same letters  for any , and which space  we are working in is understood
from context. A direct computation shows that {1, 2, . . . , } really is a basis of ; it
spans  and is linearly independent. In fact,

                                                                                              

                                        = (1, 2, . . . , ) =  .

                                                                                            =1

Proposition 8.1.14. Let  be a vector space and  a nonnegative integer.
   (i) If  is spanned by  vectors, then dim   .
  (ii) If  is a linearly independent set and    \ span(), then   {} is linearly independent.

 (iii) dim  =  if and only if  has a basis of  vectors. In particular, dim  = .
 (iv) If    is a vector subspace and dim  = , then dim   .

  (v) If dim  =  and a set  of  vectors spans , then  is linearly independent.
 (vi) If dim  =  and a set  of  vectors is linearly independent, then there is a set  of  - 

       vectors such that    is a basis of .
8.1. VECTOR SPACES, LINEAR MAPPINGS, AND CONVEXITY  13

    In particular, the last item says that if dim  =  and  is a set of  linearly independent
vectors, then  spans . Another thing to note is that item (iii) implies that every basis of a
finite dimensional vector space has the same number of elements.

Proof. All statements hold trivially when  = 0, so assume   1.
    We start with (i). Suppose  {1, 2, . . . , } spans , and  {1, 2, . . . , } is a

linearly independent subset of . We wish to show that   . As  spans , write

                                                                              

                                                 1 = ,1 ,

                                                                            =1

for some numbers 1,1, 2,1, . . . , ,1. One of the ,1 is nonzero, otherwise 1 would be
zero. Without loss of generality, suppose 1,1  0. Solve

                                           1 = 1    1 - ,1  .
                                                  1,1 =2 1,1

In particular, {1, 2, . . . , } spans , since 1 can be obtained from {1, 2, . . . , }.
Therefore, there are some numbers for some numbers 1,2, 2,2, . . . , ,2, such that

                                                                                      

                                            2 = 1,21 + ,2 .

                                                                                    =2

As  is linearly independent--and so {1, 2} is linearly independent--one of the ,2 for
  2 must be nonzero. Without loss of generality suppose 2,2  0. Solve

                                     2 = 1   2 - 1,2    1 - ,2  .
                                            2,2 2,2 =3 2,2

In particular, {1, 2, 3, . . . , } spans .
    We continue this procedure. If  < , we are done. Suppose   . After  steps, we

obtain that {1, 2, . . . , } spans . Any other vector  in  is a linear combination of
{1, 2, . . . , } and hence cannot be in  as  is linearly independent. So  = .

    We continue with (ii). Suppose  = {1, 2, . . . , } is linearly independent, does not
span , and    \ span(). Suppose 11 + 22 + · · · +   + +1 = 0 for some scalars
1, 2, . . . , +1. If +1  0, then  would be a linear combination of , so +1 = 0. Then,
as  is linearly independent, 1 = 2 = · · · =  = 0. So   {} is linearly independent.

    We move to (iii). If dim  = , then there must exist some linearly independent set
 of  vectors, and  must span , otherwise we could choose a larger set of linearly
independent vectors via (ii). So we have a basis of  vectors. On the other hand, if we have
a basis of  vectors, the dimension is at least  as a basis is linearly independent. A basis
also spans , and so by (i) we know that dimension is at most . Hence the dimension of
 must equal . The "in particular" follows by noting that {1, 2, . . . , } is a basis of .
14  CHAPTER 8. SEVERAL VARIABLES AND PARTIAL DERIVATIVES

    To see (iv), suppose    is a vector subspace, where dim  = . As  cannot contain

 + 1 linearly independent vectors, neither can .

    For (v), suppose  is a set of  vectors that is linearly dependent and spans . We will

show that  > . One of the vectors is a linear combination of the others. If we remove it

from , we obtain a set of  - 1 vectors that still span . Hence  = dim    - 1 by (i).

    For (vi) suppose  = {1, 2, . . . , } is a linearly independent set. First,    by

definition of dimension. If  = , the set  must span  as in the proof of (iii), otherwise

we could add another vector to . If  < ,  cannot span  by (iii). So find  not in the

span of . Via (ii), the set   {} is a linearly independent set of  + 1 elements. Therefore,

we repeat this procedure  -  times to find a set of  linearly independent vectors. Again,

they must span , otherwise we could add yet another vector.  

8.1.3 Linear mappings

When   , a function  :    is often called a mapping or a map rather than a function.

Definition 8.1.15. A map  :    of vector spaces  and  is linear (we also say  is a
linear transformation or a linear operator) if for all    and all ,   ,

                         () = () and ( + ) = () + ().

We usually write  instead of () if  is linear. If  is one-to-one and onto, then we say
 is invertible, and we denote the inverse by -1. If  :    is linear, then we say  is a
linear operator on .

    We write ( , ) for the set of linear maps from  to , and () for the set of linear
operators on . If    and ,   ( , ), define the maps  and  +  by

                              ()() , ( + )()  + .

If   (, ) and   ( , ), define the map  :    as the composition   ,

                                                  ().

Finally, denote by   () the identity: the linear operator such that  =  for all .

Proposition 8.1.16. Let , , and  be vector spaces.
   (i) If   ( , ), then 0 = 0.
  (ii) If ,   ( , ), then  +   ( , ).

 (iii) If   ( , ) and   , then   ( , ).
 (iv) If   (, ) and   ( , ), then   ( , ).
  (v) If   ( , ) is invertible, then -1  (, ).

    In particular, ( , ) is a vector space, where 0  ( , ) is the linear map that takes
everything to 0. As () is not only a vector space, but also admits a product (composition),
it is called an algebra.
8.1. VECTOR SPACES, LINEAR MAPPINGS, AND CONVEXITY                                   15

Proof. We leave the first four items as a quick exercise, Exercise 8.1.20. Let us prove the last
item. Let    and   . As  is onto, then there is an    such that  = . As it is
also one-to-one, -1() =  for all   . So

-1( ) = -1() = -1 () =  = -1().

Similarly, let 1, 2   and 1, 2   be such that 1 = 1 and 2 = 2, then
     -1(1 + 2) = -1(1 + 2) = -1 (1 + 2) = 1 + 2 = -1(1) + -1(2). 

Proposition 8.1.17. If   ( , ) is linear, then it is completely determined by its values on a
basis of . Furthermore, if  is a basis of , then every function  :    extends to a linear
function  on .

    We only prove this proposition for finite-dimensional spaces, as we do not need
infinite-dimensional spaces.

Proof. Let {1, 2, . . . , } be a basis of , and let                              . Every    has a unique
representation

                                                                               

                                                    =  

                                                                             =1

for some numbers 1, 2, . . . , . By linearity,

                                                                                 

                       =   =   =  .

                      =1               =1                                        =1

The "furthermore" follows by setting   (), and then for  =                           
the extension as ()   
                                                                                     =1  , defining
                      =1  .  The function is well-defined by uniqueness of the

representation of . We leave it to the reader to check that  is linear.              

    For a linear map, it is sufficient to check injectivity at the origin. That is, if the only  such
that  = 0 is  = 0, then  is one-to-one, because if  = , then ( - ) = 0. For this
reason, one often studies the nullspace of , that is, {   :  = 0}. For finite-dimensional
vector spaces (and only in finitely many dimensions) we have the following special case of
the so-called rank-nullity theorem from linear algebra.

Proposition 8.1.18. If  is a finite-dimensional vector space and   (), then  is one-to-one
if and only if it is onto.

Proof. Let {1, 2, . . . , } be a basis for . First suppose  is one-to-one. Let 1, 2, . . . , 

be scalars such that                       

                      0 =   =   .

                        =1                 =1

     For infinite-dimensional spaces, the proof is essentially the same, but a little trickier to write. Moreover,
we haven't even defined what a basis is for infinite-dimensional spaces.
16  CHAPTER 8. SEVERAL VARIABLES AND PARTIAL DERIVATIVES

As  is one-to-one, the only vector that is taken to 0 is 0 itself. Hence,

                     

            0 =  ,

                   =1

and so  = 0 for all  as {1, 2, . . . , } is a basis. So {1, 2, . . . , } is linearly
independent. By Proposition 8.1.14 and the fact that the dimension is , we conclude

{1, 2, . . . , } spans . Consequently,  is onto, as any    can be written as

                                          

         =   =   .

        =1                                =1

    For the other direction, suppose  is onto. Suppose that for some 1, 2, . . . , ,

                                        

        0 =    =  .

          =1                            =1

As  is determined by the action on the basis, {1, 2, . . . , } spans . So by

Proposition 8.1.14, the set is linearly independent, and  = 0 for all . In other words, if

 = 0, then  = 0. Thus,  is one-to-one.                                                

    We leave the proof of the next proposition as an exercise.

Proposition 8.1.19. If  and  are finite-dimensional vector spaces, then ( , ) is also finite-
dimensional.

    We can identify a finite-dimensional vector space  of dimension  with , provided
we fix a basis {1, 2, . . . , } in . That is, we define a bective linear map   ( , ) by
 , where {1, 2, . . . , } is the standard basis in . We have the correspondence

              

             (1, 2, . . . , )   .             

    =1

8.1.4 Convexity

A subset  of a vector space is convex if whenever ,   , the line segment from  to 
lies in . That is, if the convex combination (1 - ) +   is in  for all   [0, 1]. We write
[, ] for this line segment. See Figure 8.2.

    In , convex sets are precisely the intervals, which are also precisely the connected sets.
In two or more dimensions there are lots of nonconvex connected sets. For example, the
set 2 \ {0} is connected, but not convex--for any   2 \ {0} where  -, we find
(1/2) + (1/2) = 0, which is not in the set. Balls (in the standard metric) in  are convex. It
is a useful enough result to state as a proposition, but we leave its proof as an exercise.

Proposition 8.1.20. Let    and  > 0. The ball (, )   is convex.
8.1. VECTOR SPACES, LINEAR MAPPINGS, AND CONVEXITY                   17

                                                    
                                (1 - ) +  

Figure 8.2: Convexity.

Example 8.1.21: A convex combination is, in particular, a linear combination. So every
vector subspace  of a vector space  is convex.

Example 8.1.22: Let ([0, 1], ) be the vector space of continuous real-valued functions on
. Let   ([0, 1], ) be the set of those  such that

                      1
                              ()   1 and  ()  0 for all   [0, 1].

                                  0

Then  is convex. Take   [0, 1], and note that if  ,   , then (1 - )  () +  ()  0 for
all . Furthermore,

1                                 1                   1

   (1 - )  () +  ()  = (1 - )  ()  +  ()   1.

0                                 0                   0

Note that  is not a vector subspace of ([0, 1], ). The function  ()  1 is in , but 2 
and -  is not.

Proposition 8.1.23. The intersection of two convex sets is convex. In fact, if {} is an
arbitrary collection of convex sets in a vector space, then

                                  is convex.

                              

Proof. If ,   , then ,    for all   , and hence if   [0, 1], then (1 - ) +    
for all   . Therefore, (1 - ) +     and  is convex.
                                                                     

    A useful construction using intersections of convex sets is the convex hull. Given a
subset  of a vector space , define the convex hull of  as the intersection of all convex
sets containing :

   co()                 {   :   , and  is convex}.

That is, the convex hull is the smallest convex set containing . By Proposition 8.1.23, the
intersection of convex sets is convex. Hence the convex hull is convex.
18  CHAPTER 8. SEVERAL VARIABLES AND PARTIAL DERIVATIVES

Example 8.1.24: The convex hull of {0, 1} in  is [0, 1]. Proof: A convex set containing 0
and 1 must contain [0, 1], so [0, 1]  co({0, 1}). The set [0, 1] is convex and contains {0, 1},
so co({0, 1})  [0, 1].

    Linear mappings preserve convex sets. So in some sense, convex sets are the right sort
of sets when considering linear mappings or changes of coordinates.

Proposition 8.1.25. Let  ,  be vector spaces,   ( , ), and let    be convex. Then
() is convex.

Proof. Take two points ,   (). Pick ,    such that  =  and  = . As  is
convex, then (1 - ) +    for all   [0, 1], so

    (1 - ) +  = (1 - ) +  =  (1 - ) +   ().                                                     

8.1.5 Exercises

Exercise 8.1.1: Show that in  (with the standard euclidean metric), for every    and every  > 0, the
ball (, ) is convex.

Exercise 8.1.2: Verify that  is a vector space.

Exercise 8.1.3: Let  be a vector space. Prove that a finite set of vectors {1, 2, . . . , }   is linearly
independent if and only if for every  = 1, 2, . . . , 

    span {1, . . . , -1, +1, . . . , }  span {1, 2, . . . , } .

That is, the span of the set with one vector removed is strictly smaller.
Exercise 8.1.4: Show that the set   ([0, 1], ) of those functions such that 0  1  = 0 is a vector subspace.
Compare Exercise 8.1.16.

Exercise 8.1.5 (Challenging): Prove ([0, 1], ) is an infinite-dimensional vector space where the operations
are defined in the obvious way:  =  +  and  =   are defined as ()  () + () and ()   ().
Hint: For the dimension, think of functions that are only nonzero on the interval (1/+1, 1/).

Exercise 8.1.6: Let  : [0, 1]2   be continuous. Show that  : ([0, 1], )  ([0, 1], ) defined by

      ()  1
               (, )  () 

            0

is a linear operator. That is, first show that  is well-defined by showing that   is continuous whenever  is,
and then showing that  is linear.

Exercise 8.1.7: Let P be the vector space of polynomials in one variable of degree  or less. Show that P is
a vector space of dimension  + 1.

Exercise 8.1.8: Let [] be the vector space of polynomials in one variable . Let  : []  [] be the
derivative operator (derivative in ). Show that  is a linear operator.
8.1. VECTOR SPACES, LINEAR MAPPINGS, AND CONVEXITY                                             19

Exercise 8.1.9: Let us show that Proposition 8.1.18 only works in finite dimensions. Take the space of
polynomials [] and define the operator  : []  [] by  () (). Show that  is linear and

one-to-one, but show that it is not onto.

Exercise 8.1.10: Finish the proof of Proposition 8.1.17 in the finite-dimensional case. That is, suppose
{1, 2, . . . } is a basis of , {1, 2, . . . }  , and define a function

                             ()                         

                                      ,  if  =   .

                                 =1                    =1

Prove that  :    is linear.

Exercise 8.1.11: Prove Proposition 8.1.19. Hint: A linear transformation is determined by its action on
a basis. So given two bases {1, . . . , } and {1, . . . , } for  and  respectively, consider the linear
operators  that send   = , and   = 0 if   .

Exercise 8.1.12 (Easy): Suppose  and  are vector spaces and   ( , ) is a linear operator.
 a) Show that the nullspace  {   :  = 0} is a vector space.
 b) Show that the range  {   :  =  for some   } is a vector space.

Exercise 8.1.13 (Easy): Show by example that a union of convex sets need not be convex.

Exercise 8.1.14: Compute the convex hull of the set of 3 points (0, 0), (0, 1), (1, 1) in 2.

Exercise 8.1.15: Show that the set (, )  2 :  > 2 is a convex set.
Exercise 8.1.16: Show that the set   ([0, 1], ) of those functions such that 0  1  = 1 is a convex set,
but not a vector subspace. Compare Exercise 8.1.4.

Exercise 8.1.17: Show that every convex set in  is connected using the standard topology on .

Exercise 8.1.18: Suppose   2 is a convex set such that the only point of the form (, 0) in  is the point
(0, 0). Further suppose that (0, 1)   and (1, 1)  . Show that if (, )   and   0, then  > 0.

Exercise 8.1.19: Prove that an arbitrary intersection of vector subspaces is a vector subspace. That is, if 

is a vector space and {} is an arbitrary collection of vector subspaces of , then          is a vector
subspace of .

Exercise 8.1.20 (Easy): Finish the proof of Proposition 8.1.16, that is, prove the first four items of the
proposition.
20                CHAPTER 8. SEVERAL VARIABLES AND PARTIAL DERIVATIVES

8.2 Analysis with vector spaces

Note: 3 lectures

8.2.1 Norms

Let us start measuring the size of vectors and hence distance.

Definition 8.2.1. If  is a vector space, then we say a function · :    is a norm if
   (i)   0, with  = 0 if and only if  = 0.
  (ii)  = ||  for all    and   .

 (iii)  +    +  for all ,    (triangle inequality).

A vector space equipped with a norm is called a normed vector space.

    Given a norm (any norm) on a vector space , define a distance (, )  - , which
makes  into a metric space (exercise). So what you know about metric spaces applies to
normed vector spaces. Before defining the standard norm on , we define the standard
scalar dot product on . For  = (1, 2, . . . , )   and  = (1, 2, . . . , )   define

                  ·   

                          .

                     =1

Dot product is linear in each variable separately--in more fancy language, it is bilinear.
That is, if  is fixed, the map    ·  is a linear map from  to . Similarly, if  is fixed,
   ·  is linear. It is symmetric in the sense that  ·  =  · . Define the euclidean norm as

                           ·  = (1)2 + (2)2 + · · · + ()2.

We will normally write , only in the rare instance when it is necessary to emphasize
that we are talking about the euclidean norm will we write  . Unless otherwise stated,
if we talk about  as a normed vector space, we mean the standard euclidean norm. It is
easy to see that the euclidean norm satisfies (i) and (ii). To prove that (iii) holds, the key
inequality is the so-called Cauchy-Schwarz inequality we saw before. As this inequality
is so important, we state and prove a slightly stronger version using the notation of this
chapter.

Theorem 8.2.2 (Cauchy-Schwarz inequality). Let ,   , then

                                      | · |    =  ·   · ,

with equality if and only if  =  or  =  for some   .
8.2. ANALYSIS WITH VECTOR SPACES                                  21

Proof. If  = 0 or  = 0, then the theorem holds trivially. So assume   0 and   0.
    If  is a scalar multiple of , that is,  =  for some   , then the theorem holds

with equality:

| · | = | · | = || | · | = || 2 =   =  .

Fixing  and ,  +  2 is a quadratic polynomial as a function of :

 +  2 = ( +  ) · ( +  ) =  ·  +  ·   +   ·  +   ·   = 2 + 2( · ) + 22.

If  is not a scalar multiple of , then  +  2 > 0 for all . So the polynomial  +  2 is
never zero. Elementary algebra says that the discriminant must be negative:

    4( · )2 - 422 < 0.

In other words, ( · )2 < 22.                                      

Item (iii), the triangle inequality in , follows from:

 + 2 =  ·  +  ·  + 2( · )  2 + 2 + 2   =  +  2.

    The distance (, )  -  is the standard distance (standard metric) on  that
we used when we talked about metric spaces.

Definition 8.2.3. Let   ( , ). Define

   sup  :    with  = 1 .

The number  (possibly ) is called the operator norm. We will see below that it is indeed
a norm on ( , ) for finite-dimensional spaces. Again, when necessary to emphasize
which norm we are talking about, we may write it as (,).

For example, if  = 1 with norm  = ||, elements of () are multiplication

by scalars,   , and we identify    with the corresponding element of (). If

 = || = 1, then || = ||, so the operator norm of  is ||.

                                                          
By linearity,   =  for all nonzero   . The vector  is of norm 1. Therefore,

                         = sup  :    with  = 1 = sup  .
                                                                                 

                                                                                                              0

This implies, assuming    to avoid a technicality when  = 0, that for every   ,

                                .

Conversely, if one shows    for all , then   .
22  CHAPTER 8. SEVERAL VARIABLES AND PARTIAL DERIVATIVES

    It is not hard to see from the definition that  = 0 if and only if  = 0, where  = 0
means that  takes every vector to the zero vector. It is also not difficult to compute the

operator norm of the identity operator:

     = sup  = sup  = 1.
       
    0       0

The operator norm is not always so easy to compute using the definition alone, nor is it
easy to read off the form of the operator. Consider 2 and the operator   (2) that
takes (, ) to ( + , 2). Unit norm vectors can be written as ±, ± 1 - 2 for   [0, 1]
(or perhaps cos(), sin() ). One then maximizes

               2
    (, ) =   ± 1 - 2 + 42

to find  = 3 + 5. More generally, one often does two steps. For instance, consider
the operator    ([0, 1], ),  taking a continuous  to  (0). If    = 1 (the uniform
norm), then clearly |  (0)|  1, so |  |  1, meaning   1. To prove it is equal to 1, note
that the constant function 1 has norm 1, so 1 = 1, meaning   1. So  = 1.

    The operator norm is not always a norm on ( , ), in particular,  is not always
finite for   ( , ). We prove below that  is finite when  is finite-dimensional. The
operator norm being finite is equivalent to  being continuous. For infinite-dimensional

spaces, neither statement needs to be true. For an example, consider the vector space of
continuously differentiable functions on [0, 2] using the uniform norm. The functions
  sin() have norm 1, but their derivatives have norm . So differentiation, which is a

linear operator valued in the space of continuous functions, has infinite operator norm on

this space. We will stick to finite-dimensional spaces.
    Given a finite-dimensional vector space , we often think of , although if we have a

norm on , the norm might not be the standard euclidean norm. In the exercises, you can
prove that every norm on  is "equivalent" to the euclidean norm in that the topology it

generates is the same. For simplicity, we only prove the following proposition for euclidean

spaces, and the proof for general finite-dimensional spaces is left as an exercise.

Proposition 8.2.4. Let  and  be normed vector spaces,   ( , ), and  is finite-dimensional.
Then  < , and  is uniformly continuous (Lipschitz with constant ).

Proof. As we said we only prove the proposition for euclidean spaces, so suppose that
 =  and the norm is the standard euclidean norm. The general case is left as an exercise.

    Let {1, 2, . . . , } be the standard basis of . Write   , with  = 1, as

                                                                              

                                                   =  .

                                                                             =1

Since  ·  = 0 whenever    and  ·  = 1, we have  =  · . By Cauchy-Schwarz,

                                        || = | · |    = 1.
8.2. ANALYSIS WITH VECTOR SPACES                                             23

Then                                  

                  =                ||   .

                     =1           =1  =1

The right-hand side does not depend on . We found a finite upper bound for 
independent of , so  < .

    Take normed vector spaces  and , and   ( , ) with  < . For ,   ,

                  -  = ( - )    - .

As  < , then the inequality above says that  is Lipschitz with constant . 

Proposition 8.2.5. Let , , and  be finite-dimensional normed vector spaces.
   (i) If ,   ( , ) and   , then

                  +    + ,  = || .

     In particular, the operator norm is a norm on the vector space ( , ).
(ii) If   ( , ) and   (, ), then

                            .

Proof. First, since all the spaces are finite-dimensional, then all the operator norms are

finite, and the statements make sense to begin with.
    For (i), let    be arbitrary. Then

( + ) =  +    +     +   =  +  .

So  +    + . Similarly,

                 () = ||   ||  .

Thus   || . Next,

                     ||  =    .

Hence ||   .

For (ii), write

                        .                                                    

    A norm defines a metric, giving a metric space topology on ( , ) for finite-dimensional
vector spaces. So, we can talk about open/closed sets, continuity, convergence, etc.

     If we strike the "In particular" part and interpret the algebra with infinite operator norms properly,
namely decree that 0 times  is 0, then this result also holds for infinite-dimensional spaces.
24                     CHAPTER 8. SEVERAL VARIABLES AND PARTIAL DERIVATIVES

Proposition 8.2.6. Let  be a finite-dimensional normed vector space. Let ()  () be the
set of invertible linear operators.

    (i) If   (),   (), and
                                                    -  < -1 1  , (8.2)

       then   (), that is,  is invertible. In particular, () is open.
  (ii)   -1 is a continuous function on ().

    We illustrate this proposition on a simple example. Consider  = 1, where linear

operators are just numbers  and the operator norm of  is ||. The operator  is invertible
(-1
     =  1/)  whenever      0.  The  condition  |  -  |  <    1   indeed  implies  that    is  not  zero.
                                                           |-1|
Moreover,   1/ is a continuous function. When the dimension is 2 or higher, there are

other noninvertible operators than just zero, and things are a bit more difficult.

Proof. Let us prove (i). We know something about -1 and  - ; they are linear operators.
So apply them to a vector: -1( - ) =  - -1.

Therefore,

                          = -1( - ) + -1
                               -1  -   + -1 .

Assume   0 and so   0. Using (8.2), we obtain

                                     <  + -1 .

Thus   0 for all   0, and consequently   0 for all   0. So  is one-to-one; if

 = , then ( - ) = 0, so  = . As  is a one-to-one linear mapping from  to ,

which is finite-dimensional, it is also onto by Proposition 8.1.18. Therefore,  is invertible.

It follows that, in particular, () is open.

    Let us prove (ii). We must show that the inverse is continuous. Fix a   (). Let

  be near    , specifically  -      <      1      Then (8.2) is satisfied and        is invertible.  A

                                       2-1 .
similar computation as above (using -1 instead of ) gives

        -1  -1  -  -1 + -1   1 -1 + -1 ,
                                                                        2

or
                                            -1  2-1 .

So -1  2-1.            -1( - )-1 = -1(-1 - ) = -1 - -1,
    Now

    () is called the general linear group, that is where the acronym GL comes from.
8.2. ANALYSIS WITH VECTOR SPACES                                                25

and

      -1 - -1 = -1( - )-1  -1  -  -1  2-12 - .

Therefore, as  tends to , -1 - -1 tends to 0, and so the inverse operation is a

continuous function at .                                                        

8.2.2 Matrices

Once we fix a basis in a finite-dimensional vector space , we can represent a vector of  as an
-tuple of numbers--a vector in . Same can be done with ( , ), bringing us to matrices,

which are a convenient way to represent finite-dimensional linear transformations. Suppose
{1, 2, . . . , } and {1, 2, . . . , } are bases for vector spaces  and  respectively. A
linear operator is determined by its values on the basis. Given   ( , ),  is an
element of . Define the numbers , via

                                                                                (8.3)

                                  = , ,

                                             =1

and write them as a matrix, which we, by slight abuse of notation, also call ,

                                1,1 1,2 · · · 1,     
                                2,1 2,2 · · · 2, 
                            =    ..  ..   ...    ..     .
                               .       .         .   

                               ,1 ,2 · · · ,         

We sometimes write  as [,]. We say  is an -by- matrix. The th column of the matrix
contains precisely the coefficients that represent  in terms of the basis {1, 2, . . . , }.
Given the numbers ,, then via the formula (8.3), we find the corresponding linear
operator, as it is determined by the action on a basis. Hence, once we fix bases on  and ,

we have a one-to-one correspondence between ( , ) and the -by- matrices. When

                                              

                                      =  ,

                                            =1

then                                                        

       =   =                             ,      =              ,  ,

                =1             =1    =1             =1     =1

which gives rise to the familiar rule for matrix multiplication, thinking of  as a column
vector, that is, an -by-1 matrix. More generally, if  is an -by- matrix with entries ,,
then the matrix for  =  is an -by- matrix whose (, )th entry , is

                                             

                                 , = , ,.

                                            =1
26                      CHAPTER 8. SEVERAL VARIABLES AND PARTIAL DERIVATIVES

A way to remember it is if you order the indices as we do--row, column--and put the
elements in the same order as the matrices, then the "middle index" is "summed-out."

    There is a one-to-one correspondence between matrices and linear operators in ( , ),
once we fix bases in  and . If we choose different bases, we get different matrices. This
is an important distinction. The operator  acts on elements of , while the matrix is
something that works with -tuples of numbers, that is, vectors of . By convention, we
use standard bases in  unless otherwise specified, and we identify ( , ) with the
set of -by- matrices.

    A linear mapping changing one basis to another is represented by a square matrix in
which the columns represent vectors of the second basis in terms of the first basis. We call
such a linear mapping a change of basis. So for two choices of a basis in an -dimensional
vector space, there is a linear mapping (a change of basis) taking one basis to the other, and
this corresponds to an -by- matrix which does the corresponding operation on .

    Suppose  = ,  = , and all the bases are just the standard bases. Using the
Cauchy-Schwarz inequality, with  = (1, 2, . . . , )  , compute

                         2                                          

    2 =                     ,              (   ,  )2      (  )2  =         (,)2 2.

                    =1  =1       =1    =1             =1            =1 =1

In other words, we have a bound on the operator norm (note that equality rarely happens)

                                         

                                                 (   ,  )2 .

                                         =1 =1

The right hand side is the euclidean norm on , the space of all the entries of the matrix.
If the entries go to zero, then  goes to zero. Conversely,

                                                      

                               (,)2 = 2  2 = 2.

                        =1 =1        =1               =1

So if the operator norm of  goes to zero, so do the entries. In particular, if  is fixed and 
is changing, then the entries of  go to the entries of  if and only if  goes to  in operator
norm ( -  goes to zero). We have proved:

Proposition 8.2.7. The topology (the set of open sets) on ( , ) is the same whether we
consider ( , ) as a metric space using the operator norm, or the euclidean metric of .

    In particular, let  be a metric space and let  : ( , )   identify an operator with
the -tuple of entries of the corresponding matrix. Then  :   ( , ) is continuous if and
only if    :    is continuous. Similarly for  : ( , )   and   -1 :   .
8.2. ANALYSIS WITH VECTOR SPACES                                                       27

8.2.3 Determinants

A certain number can be assigned to square matrices that measures how the corresponding

linear mapping stretches space. In particular, this number, called the determinant, can be

used to test for invertibility of a matrix.
    Define the symbol sgn() (read "sign of ") for a number  by

                    sgn()                 -1 if  < 0,
                                          
                                          0 if  = 0, 1 if  > 0.

A permutation  = (1, 2, . . . , ) is a reordering of (1, 2, . . . , ). Define

                    sgn() = sgn(1, . . . , )            sgn( - ).                      (8.4)

                                                    <

Here stands for multiplication, similarly to how stands for summation.

    Every permutation can be obtained by a sequence of transpositions (switchings of two

elements). A permutation is even (resp. odd) if it takes an even (resp. odd) number of
transpositions to get from (1, 2, . . . , ) to . For instance, (2, 4, 3, 1) is two transpositions
away from (1, 2, 3, 4) and is therefore even: (1, 2, 3, 4)  (2, 1, 3, 4)  (2, 4, 3, 1). Being
even or odd is well-defined: sgn() is 1 if  is even and -1 if  is odd (exercise). This fact
follows since applying a transposition changes the sign and sgn(1, 2, . . . , ) = 1.

    Let  be the set of all permutations on  elements (the symmetric group). Let  = [,]
be a square -by- matrix. Define the determinant of  as

                                                    

                    det()                   sgn() , .

                                                    =1

Proposition 8.2.8.

(i) det() = 1.

(ii) For every  = 1, 2, . . . , , the function   det [1 2 · · · ] is linear.

(iii) If two columns of a matrix are interchanged, then the determinant changes sign.

(iv) If two columns of  are equal, then det() = 0.

(v) If a column is zero, then det() = 0.

(vi)   det() is a continuous function on ().

(vii) det       =  - , and det [] = .
           

    In fact, the determinant is the unique function that satisfies (i), (ii), and (iii), but we
digress. By (ii), we mean that if we fix all the vectors 1, . . . ,  except for , and let
,    be two vectors, and ,    be scalars, then

det [1 · · · -1 ( + ) +1 · · · ] =
     det [1 · · · -1  +1 · · · ] +  det [1 · · · -1  +1 · · · ] .
28                           CHAPTER 8. SEVERAL VARIABLES AND PARTIAL DERIVATIVES

Proof. We go through the proof quickly, as you have likely seen it before. Item (i) is trivial.

For (ii), note that each term in the definition of the determinant contains exactly one

factor from each column. Item (iii) follows as switching two columns is switching the two

corresponding numbers in every element in . Hence, all the signs are changed. Item (iv)

follows because if two columns are equal, and we switch them, we get the same matrix

back. So item (iii) says the determinant must be 0. Item (v) follows because the product in

each term in the definition includes one element from the zero column. Item (vi) follows

as det is a polynomial in the entries of the matrix and hence continuous (as a function of

the entries of the matrix). A function defined on matrices is continuous in the operator

norm if and only if it is continuous as a function of the entries (Proposition 8.2.7). Finally,

item (vii) is a direct computation.                                                                             

    The determinant tells us about areas and volumes, and how they change. For example,

in the 1-by-1 case, a matrix is just a number, and the determinant is exactly this number. It
says how the linear mapping "stretches" the space. Similarly, suppose   (2) is a linear

transformation. It can be checked directly that the area of the image of the unit square
 [0, 1]2 is |det()|, see Figure 8.3 for an example. This works with arbitrary figures,

not just the unit square: The absolute value of the determinant tells us the stretch in the

area. The sign of the determinant tells us if the image is flipped (changes orientation)
or not. In 3 it tells us about the 3-dimensional volume, and in  dimensions about the
-dimensional volume. We claim this without proof.

                             1                                1
                             00 1                             00 1 2
                                                            -1

  Figure 8.3: Image of the unit square [0, 1]2 via the matrix             11   . The image is a square of side
                                                                         -1 1

    2, thus of area 2, and the determinant of the matrix is 2.

Proposition 8.2.9. If  and  are -by- matrices, then det() = det() det(). Furthermore,
                                                                         det(-1)
    is  invertible  if  and  only  if  det()    0  and  in  this  case,           =     1

                                                                                     det() .

Proof. Let 1, 2, . . . ,  be the columns of . Then

                                        = [1 2 · · · ].

That is, the columns of  are 1, 2, . . . , .
8.2. ANALYSIS WITH VECTOR SPACES                                                        29

Let , denote the elements of  and  the columns of . By linearity of the determinant,

                                                                                  

det() = det [1            2     ···    ]       = det   =1  ,1 2 · · ·             
                                                      

                                                                                  

           = ,1 det [ 2 · · · ]

              =1

           =              1,12,2 · · ·  , det [1 2 · · ·  ]

              1 1,2,...,

           =               1,12,2 · · ·  , sgn(1, 2, . . . , ) det [1 2 · · · ] .

              (1,2,..., )

In the last equality, we sum over the elements of  instead of all -tuples for integers
between 1 and , because when two columns in the determinant are the same, then the

determinant is zero. Reordering the columns to the original ordering to obtains the sgn.
    The conclusion that det() = det() det() follows by recognizing that the expression

in parentheses above is the determinant of . We obtain this by plugging in  = . The
expression we get for the determinant of  has rows and columns swapped, so as a bonus,

we have also just proved that the determinant of a matrix and its transpose are equal.
    Let us prove the "Furthermore." If  is invertible, then -1 = . Consequently

det(-1) det() = det(-1) = det() = 1. If  is not invertible, then it is not one-to-one,
and so  takes some nonzero vector to zero. In other words, the columns of  are linearly

dependent. Suppose                        

                                               = 0,

                                       =1

where not all  are equal to 0. Without loss of generality, suppose 1  0. Take

                                       1 0 0 · · · 0
                                       2 1 0 · · · 0
                                 3 0 1 · · · 0 .
                                        .  .. ... ... . . . ... 
                                        0 0 · · · 1
                                                         

Using the definition of the determinant (there is only a single permutation  for which

=1  ,  is  nonzero)   we  find  det()  =  1    0.  Then  det()    =  det() det()  =  1  det().

The first column of  is zero, and hence det() = 0. We conclude det() = 0.               

Proposition 8.2.10. Determinant is independent of the basis: If  and  are -by- matrices and

 is invertible, then            det() = det(-1).
30      CHAPTER 8. SEVERAL VARIABLES AND PARTIAL DERIVATIVES

Proof.  det(-1) = det(-1) det() det() =     1   det()  det()  =  det().  
                                         det()

    If in one basis  is the matrix representing a linear operator, then for another basis we
can find a matrix  such that the matrix -1 takes us to the first basis, applies  in the
first basis, and takes us back to the basis we started with. Let  be a finite-dimensional
vector space. Let   ( , ) take a basis {1, . . . , } to the standard basis {1, . . . , }
and let   ( , ) take another basis {1, . . . , } to the standard basis. Let   () be
a linear operator and let a matrix  represent the operator in the basis {1, . . . , }. Then
 would be such that we have the following diagram:

         -1 

                                         -1

                                                -1

                                         -1

          

The two s on the bottom row represent  in the first basis, and the s on top represent
 in the second basis.

    If we compute the determinant of the matrix , we obtain the same determinant if we
use any other basis; in the other basis the matrix would be -1. Consequently,

        det : ()  

is a well-defined function without the need to fix a basis. That is, det is defined on (),
not just on matrices.

    There are three types of so-called elementary matrices. Let 1, 2, . . . ,  be the standard
basis on  as usual. First, for  = 1, 2, . . . ,  and   ,   0, define the first type of an
elementary matrix, an -by- matrix  by

                                               if   ,
                                                         if  = .

Given any -by- matrix  the matrix  is the same matrix as  except with the th
row multiplied by . It is an easy computation (exercise) that det() = .

    Next, for ,  with    and   , define the second type of an elementary matrix  by

                                             if   ,

           +  if  = .

Given any -by- matrix  the matrix  is the same matrix as  except with  times
the th row added to the th row. It is an easy computation (exercise) that det() = 1.

     This is a so-called commutative diagram. Following arrows in any way should end up with the same
result.
8.2. ANALYSIS WITH VECTOR SPACES                               31

    Finally, for  and  with   , define the third type of an elementary matrix  by

                                               if    and   ,
                                              
                                       if  = ,  if  = .
Given any -by- matrix  the matrix  is the same matrix with th and th rows
swapped. It is an easy computation (exercise) that det() = -1.

Proposition 8.2.11. Let  be an -by- invertible matrix. Then there exists a finite sequence of
elementary matrices 1, 2, . . . ,  such that

                                                  = 12 · · ·  ,

and
                                    det() = det(1) det(2) · · · det().

    The proof is left as an exercise. The proposition says we can compute the determinant
via elementary row operations. We do not have to factor the matrix into a product of
elementary matrices completely. It is sufficient to do row operations until we find an upper
triangular matrix, that is, a matrix [,] where , = 0 if  > . Computing determinant of
such a matrix is not difficult (exercise).

    Factorization into elementary matrices (or variations on elementary matrices) is useful
in proofs involving an arbitrary linear operator, by reducing to a proof for an elementary
matrix, similarly as the computation of the determinant.

8.2.4 Exercises

Exercise 8.2.1: For a vector space  with a norm ·, show that (, )  -  makes  a metric space.
Exercise 8.2.2 (Easy): Show that for square matrices  and , det() = det().
Exercise 8.2.3: For   , define

                                 max |1|, |2|, . . . , || ,

sometimes called the sup or the max norm.

a) Show that · is a norm on  (defining a different distance).

b) What is the unit ball (0, 1) in this norm?

Exercise 8.2.4: For   , define                  

                                           1      ||,

                                               =1

sometimes called the 1-norm (or 1 norm).

a) Show that ·1 is a norm on  (defining a different distance, sometimes called the taxicab distance).

b) What is the unit ball (0, 1) in this norm? Think about what it is in 2 and 3. Hint: It is, for example,
    a convex hull of a finite number of points.
32                    CHAPTER 8. SEVERAL VARIABLES AND PARTIAL DERIVATIVES

Exercise 8.2.5: Using the euclidean norm on 2, compute the operator norm of the operators in (2) given
by the matrices:

a) 1 0    b) 0 1      c) 1 1    d) 0 1

      02        -1 0        01         00

Exercise 8.2.6: Using the standard euclidean norm , show:

a) Suppose   (, ) is defined for    by   for a vector   . Then the operator norm
   (,) =  . (That is, the operator norm of  is the euclidean norm of .)

b) Suppose   ( , ) is defined for    by     ·  for a vector   . Then the operator norm
    ( ,) =  .

Exercise 8.2.7: Suppose  = (1, 2, . . . , ) is a permutation of (1, 2, . . . , ).
 a) Show that we can make a finite number of transpositions (switching of two elements) to get to (1, 2, . . . , ).
 b) Using the definition (8.4) show that  is even if sgn() = 1 and  is odd if sgn() = -1. In particular,

     showing that being odd or even is well-defined.

Exercise 8.2.8: Verify the computation of the determinant for the three types of elementary matrices.

Exercise 8.2.9: Prove Proposition 8.2.11.

Exercise 8.2.10:

 a) Suppose  = [,] is an -by- diagonal matrix, that is, , = 0 whenever   . Show that
     det() = 1,12,2 · · · ,.

 b) Suppose  is a diagonalizable matrix. That is, there exists a matrix  such that -1 =  for a
     diagonal matrix  = [,]. Show that det() = 1,12,2 · · · ,.

Exercise 8.2.11: Take the vector space of polynomials [] and let   ([]) be differentiation (we
proved in an earlier exercise that  is a linear operator). Given () = 0 + 1 + · · · +   [] define
 sup || :  = 0, 1, 2, . . . ,  .

 a) Show that · is a norm on [].
 b) Prove  = . Hint: Consider the polynomials  as  tends to infinity.

Exercise 8.2.12: We finish the proof of Proposition 8.2.4. Let  be a finite-dimensional normed vector space
with basis {1, 2, . . . , }. Denote by · the norm on , by · the standard euclidean norm on ,
and by ·(,) the operator norm.
 a) Define  :   ,

                                      (1, 2, . . . , ) 11 + 22 + · · · +   .

     Show  is continuous.
b) Show that there exist numbers  and  such that if  = (1, 2, . . . , )   with  = 1, then

       11 + 22 + · · · +    .

 c) Show that there exists a number  such that if 11 + 22 + · · · +   = 1, then ||  .

d) Use part c) to show that if  is a finite-dimensional vector space and   ( , ), then (,) < .
8.2. ANALYSIS WITH VECTOR SPACES                                                            33

Exercise 8.2.13: Let  be a finite-dimensional vector space with basis {1, 2, . . . , }.
 a) Let · be a norm on ,  = (1, 2, . . . , )  , and · the standard euclidean norm on .

    Prove that there exist numbers ,  > 0 such that for all   ,

                         11 + 22 + · · · +     .

   Hint: See the previous exercise.

b) Use part a) to show that if ·1 and ·2 are two norms on , then there exist numbers ,  > 0
   (perhaps different from above) such that for all   ,

                                    1  2  1.

 c) Show that    is open in the metric defined by  -  1 if and only if  is open in the metric defined
     by  -  2. So convergence of sequences and continuity of functions is the same in either norm.

Exercise 8.2.14: Let  be an upper triangular matrix. Find a formula for the determinant of  in terms of
the diagonal entries, and prove that your formula works.

Exercise 8.2.15: Given an -by- matrix , prove that |det()|   (the norm on  is the operator
norm). Hint: One way to do it is to first prove it in the case  = 1, which means that all columns are of
norm 1 or less, then prove that this means that |det()|  1 using linearity.

Exercise 8.2.16: Consider Proposition 8.2.6 where  =  (for all ) using the euclidean norm.

a)  Prove that the estimate  -   <     1  is the best possible:  For every     (), find a  where
                                    -1
    equality is satisfied and  is not invertible. Hint: Difficulty is that -1 is not always 1. Prove

    that a vector 1 can be completed to a basis {1, . . . , } such that 1 ·  = 0 for   2. For the right 1,

    make it so that ( - ) = 0 for   2.

b)  For every fixed    (), let M denote the set of matrices  such that  -  <      1  Prove that

    while every   M is invertible, -1 is unbounded as a function of  on M.     -1 .

    Let  be an -by- matrix. A    (possibly complex even for a real matrix) is an eigenvalue of
 if there is a nonzero (possibly complex) vector    such that  =  (the multiplication by
complex vectors is the same as for real vectors; if  =  +  for real vectors  and , and  is a real
matrix, then  =  + ). The number

                       () sup || :  is an eigenvalue of 

is the spectral radius of . Here || is the complex modulus. We state without proof that at least one
eigenvalue always exists, and there are no more than  distinct eigenvalues of . You can therefore
assume that 0  () < . The exercises below hold for complex matrices, but feel free to assume

they are real matrices.

Exercise 8.2.17: Let ,  be -by- matrices, where  is invertible. Prove that  is an eigenvalue of , if
and only if it is an eigenvalue of -1. Then prove that (-1) = (). In particular,  is a well-defined
function on () for every finite-dimensional vector space .
34  CHAPTER 8. SEVERAL VARIABLES AND PARTIAL DERIVATIVES

Exercise 8.2.18: Let  be an -by- matrix .
 a) Prove ()  . (See above for definition of .)
 b) For every   , prove ()  1/.
 c) Suppose lim  = 0 (limit in the operator norm). Prove that () < 1.

                     

Exercise 8.2.19: We say a set    is symmetric if    implies -  .
 a) Let · be any given norm on . Show that the closed unit ball (0, 1) (using the metric induced by

     this norm) is a compact symmetric convex set.
 b) (Challenging) Let    be a compact, but note symmetric convex set and 0  . Show that

                                               inf  :  > 0 and   
                                                                               

    is a norm on , and  = (0, 1) (the closed unit ball) in the metric induced by this norm.

Hint: Feel free to the result of Exercise 8.2.13 part c). In particular, whether a set is "compact" is independent
of the norm.
8.3. THE DERIVATIVE                                                        35

8.3 The derivative

Note: 2-3 lectures

8.3.1 The derivative

For a function  :   , we defined the derivative at  as

                          lim  ( + ) -  () .
                          0    

In other words, there is a number  (the derivative of  at ) such that

lim  ( + ) -  () -  = lim  ( + ) -  () -   = lim |  ( + ) -  () -  | = 0.
0                     0                       0                        ||

    Multiplying by  is a linear map in one dimension:    . Namely, we think of
  (1, 1), which is the best linear approximation of how  changes near . We use this

interpretation to extend differentiation to more variables.

Definition 8.3.1. Let    be open and  :    a function. We say  is differentiable
at    if there exists an   ( , ) such that

                      lim   ( + ) -  () -  = 0.
                      0      
                        

We will show momentarily that , if it exists, is unique. We write   () , or  () ,
and we say  is the derivative of  at . When  is differentiable at every   , we say
simply that  is differentiable. See Figure 8.4 for an illustration.

    For a differentiable function, the derivative of  is a function from  to ( , ).
Compare to the one-dimensional case, where the derivative is a function from  to ,
but we really want to think of  here as (1, 1). As in one dimension, the idea is that
a differentiable mapping is "infinitesimally close" to a linear mapping, and this linear
mapping is the derivative.

    Notice the norms in the definition. The norm in the numerator is on , and the norm
in the denominator is on  where  lives. Normally it is understood that    from
context (the formula makes no sense otherwise). We will not explicitly say so from now on.
Let us prove, as promised, that the derivative is unique.

Proposition 8.3.2. Let    be an open subset and  :    a function. Suppose   
and there exist ,   ( , ) such that

   lim   ( + ) -  () -  = 0 and  lim   ( + ) -  () -  = 0.
   0                             0                      

Then  = .
36                   CHAPTER 8. SEVERAL VARIABLES AND PARTIAL DERIVATIVES

                                            = ( 1, 2)

                                                                        

                                                                                                     2

                                                             

                                                    1

Figure 8.4: Illustration of a derivative for a function  : 2  . The vector  is shown in the
12-plane based at (1, 2), and the vector   1 is shown along the  direction.

Proof. Suppose   ,   0. Compute

          ( - )  = -  ( + ) -  () -  +  ( + ) -  () -  
                            ( + ) -  () -   +   ( + ) -  () -   .

So  (-)  0 as   0. Given  > 0, for all nonzero  in some -ball around the origin
we have

                                      > ( - )  = ( - )   .

For  any  given        with    =  1,  if    =  (/2) ,  then    <    and     =  .  So
                                                                         
( - ) < . Taking the supremum over all  with  = 1, we get the operator norm

 -   . As  > 0 was arbitrary,  -  = 0, or in other words  = .                                           

Example 8.3.3: If  () =  for a linear mapping , then  () = :

            ( + ) -  () -   = ( + ) -  -   = 0  = 0.

Example 8.3.4: Let  : 2  2 be defined by

           (, ) = 1(, ), 2(, ) (1 +  + 2 + 2, 2 + 3 +  ).

Let us show that  is differentiable at the origin and compute the derivative directly using
the definition. If the derivative exists, it is in (2, 2), so it can be represented by a 2-by-2
8.3. THE DERIVATIVE                                                                37

matrix    . Suppose  = (1, 2). We need the following expression to go to zero.
        

  (1, 2) -  (0, 0) - ( 1 +  2,  1 + 2) =
                      (1, 2)

                                          22                                    2
                     (1 - )1 + (2 - )2 + 1 + (2 - )1 + (3 - )2 + 1 2
                                                                                   .

                                          2 + 2

                                            12

If we choose  = 1,  = 2,  = 2,  = 3, the expression becomes

                     4 + 22              2 + 2

                       1 12                12

                                  = |1|            = |1|.
                      2 + 2              2 + 2

                         12                12

This expression does indeed go to zero as   0. The function  is differentiable at the
origin and the derivative  (0) is represented by the matrix
                                                             12 .

                                                             23

Proposition 8.3.5. Let    be open and  :    be differentiable at   . Then  is

continuous at .

Proof. Another way to write the differentiability of  at  is to consider

                     ()  ( + ) -  () -  ().

The function  is differentiable at  if  () goes to zero as   0, so () itself goes to zero.
The mapping    () is a linear mapping between finite-dimensional spaces, hence
continuous and  ()  0 as   0. Thus,  ( + ) must go to  () as   0. That is, 

is continuous at .                                                                    

    Differentiation is a linear operator on the space of differentiable functions.
Proposition 8.3.6. Suppose    is open,  :    and  :    are differentiable at
  , and   . Then the functions  +  and   are differentiable at ,

                     (  + )() =  () + (), and (  )() =   ().

Proof. Let   ,   0. Then

 ( + ) + ( + ) -  () + () -  () + () 

                     

                       ( + ) -  () -  () + ( + ) - () - () ,
                                                                          

and   ( + ) -   () -   ()  ( + )) -  () -  ()                                The result
                                   = ||  .                                              

The limits as  goes to zero of the right-hand sides are zero by hypothesis.
follows.
38  CHAPTER 8. SEVERAL VARIABLES AND PARTIAL DERIVATIVES

    If   ( , ) and   ( , ) are linear maps, then they are their own derivative.
The composition   ( , ) is also its own derivative, and so the derivative of the com-
position is the composition of the derivatives. As differentiable maps are "infinitesimally
close" to linear maps, they have the same property:

Theorem 8.3.7 (Chain rule). Let    and    be open sets,  :    be differentiable
at   ,  ()  , and let  :    be differentiable at  (). Then  :    defined by

                            ()   ()

is differentiable at , and

                            () =   ()  ().

    Without the points where things are evaluated, we write  = (   ) =   . The
derivative of the composition    is the composition of the derivatives of  and  : If
 () =  and   () = , then () = , just as for linear maps.

Proof. Let   () and              () . Take a nonzero    and write    (),
  ( + ) -  (). Let          ()  ( + ) -  () - .

Then () =  -  or  =  - (), and  ( + ) =  + . We look at the quantity we
need to go to zero:

    ( + ) - () -   =   ( + ) -   () -  

                                     = ( + ) - () -   - ()  
                                      ( + ) - () -   +  () 
                                     = ( + ) - () -    ( + ) -  ()   +  ()  .

First,  is a constant and  is differentiable at , so the term   () goes to 0. Next,
because  is continuous at ,  goes to 0 as  goes to 0. Thus  (+)-()- goes to 0,
because  is differentiable at . Finally,

  ( + ) -  ()     ( + ) -  () -   +      ( + ) -  () -   + .

As  is differentiable at , for small enough , the quantity    (+)-  ()- is bounded.
Hence, the term    (+)-  () stays bounded as  goes to 0. Therefore,  (+)-()-
goes to zero, and () = , which is what was claimed.
                                                                               
8.3. THE DERIVATIVE                                                        39

8.3.2 Partial derivatives

There is another way to generalize the derivative from one dimension. We hold all but one
variable constant and take the regular one-variable derivative.

Definition 8.3.8. Let  :    be a function on an open set   . If the following limit
exists, we write

   ()  lim  (1, . . . , -1,  + , +1, . . . , ) -  () = lim  ( + ) -  () .
       0                                                              0

We call    () the partial derivative of  with respect to . See Figure 8.5. Here  is a
number, not a vector.

    For a mapping  :   , we write  = ( 1, 2, . . . , ), where  are real-valued

                                                                                                                    

functions. We then take partial derivatives of the components,  .

                                            = ( 1, 2)

                                                   slope = 2 ( 1, 2)

                                                                                         2

                                           ( 1, 2)

                                     1

Figure 8.5: Illustration of a partial derivative for a function  : 2  . The 2-plane where 1
is fixed is marked in dotted line, and the slope of the tangent line in the 2-plane is 2   (1, 2).

    Partial derivatives are easier to compute with all the machinery of calculus, and they
provide a way to compute the derivative of a function.

Proposition 8.3.9. Let    be open and let  :    be differentiable at   . Then
all the partial derivatives at  exist and, in terms of the standard bases of  and ,  () is

represented by the matrix    1                           1 

                                             1
                            1 () 2 () . . .  ()
                             2                           2         
                                             2
                            1 () 2 () . . .  () .
                            ..                  ... . . . ..       
                           .                                .
                                                          
                                    (    )  2 ()   ...        (    )
                                 1                                 
40                   CHAPTER 8. SEVERAL VARIABLES AND PARTIAL DERIVATIVES

    In other words,                              ()  =   ()  .
                                                          =1 

If  =           =  (1,  2,  .  .  .  ,    ),  then
       =1

                         ()  =     ()  =     ()  .
                                        =1 =1          =1 =1  

Proof. Fix a  and note that for nonzero ,

               ( + ) -  ()  -  ()                      =  ( + ) -  () -  () 
                                                                             

                                                       =   ( + ) -  () -  () .
                                                                          

As  goes to 0, the right-hand side goes to zero by differentiability of  . Hence,

                                        lim  ( + ) -  () =  () .
                                        0

The limit is in . Represent  in components  = ( 1, 2, . . . , ). Taking a limit in 
is the same as taking the limit in each component separately. So for every , the partial

derivative                                () = lim ( + ) - ()

                                                    0  

exists and is equal to the th component of  () , which is the th column of  (), and

we are done.                                                                       

    The converse of the proposition is not true. Just because the partial derivatives exist, does
not mean that the function is differentiable. See the exercises. However, when the partial
derivatives are continuous, we will prove that the converse holds. One of the consequences
of the proposition above is that if  is differentiable on , then   :   ( , ) is a

                                                                            

continuous function if and only if all the  are continuous functions.

8.3.3 Gradients, curves, and directional derivatives

Let    be open and  :    a differentiable function. We define the gradient as

                                                ()        () .
                                                       =1 

The gradient gives a way to represent the action of the derivative as a dot product:
 ()  =   () · .
8.3. THE DERIVATIVE                                                                                           41

Suppose  : (, )     is differentiable. Such a function and its image is

sometimes called a curve, or a differentiable curve. Write  = (1, 2, . . . , ). For the
purposes of computation, we identify (1) and  as we did when we defined the
derivative in one variable. We also identify (1, ) with . We treat  () both as an
operator in (1, ) and the vector         1 (       2 (                            in . Using Proposition 8.3.9,
                                              ),        ),  .  .  .  ,      (  )
                                                                          
if    is  () acting as a vector, then     (for   1 = ) is  () acting as an
operator in (1, ). We often use this slight abuse of notation when dealing with curves.
The vector  () is called a tangent vector. See Figure 8.6.

                                  ()  ()                                    ()

                            ()                     (, )

Figure 8.6: Differentiable curve and its derivative as a vector (for clarity assuming  defined on
[, ]). The tangent vector  () points along the curve.

Suppose  (, )   and let

                                        ()  () .

The function  is differentiable. Treating () as a number,

            () =   ()    () =   ()   () =    .
                                        =1                               =1  

For convenience, we often leave out the points where we are evaluating, such as above on

the far right-hand side. With the notation of the gradient and the dot product the equation

becomes                     () = (  ) () ·  () =   ·  .

    We use this idea to define derivatives in a specific direction. A direction is simply a
vector pointing in that direction. Pick a vector    such that  = 1, and fix   . We
define the directional derivative as

              ()              ( + ) = lim  ( + ) -  () ,                          
                             =0                             0

where  the  notation    =0  represents  the   derivative             evaluated      at    =  0.  When    =    is  a
                      
standard basis vector, we find    =   . For this reason, sometimes the notation    is
used instead of   .

Define  by

                                        ()  + .
42  CHAPTER 8. SEVERAL VARIABLES AND PARTIAL DERIVATIVES

Then  () =  for all . Let us see what happens to  when we travel along :
                    () =  =0  ( + ) = (  ) (0) ·  (0) = (  )() · .

In fact, this computation holds whenever  is any curve such that (0) =  and  (0) = .
    Suppose (  )()  0. By the Cauchy-Schwarz inequality,

                                            |  ()|  (  )().

Equality is achieved when  is a scalar multiple of (  )(). That is, when
                                                  = (  )() (  )() ,

we get   () = (  )(). The gradient points in the direction in which the function
grows fastest, in other words, in the direction in which   () is maximal.

8.3.4 The Jacobian

Definition 8.3.10. Let    and  :    be a differentiable mapping. Define the
Jacobian determinant, or simply the Jacobian, of  at  as

                                               () det  () .

Sometimes  is written as  ( 1, 2, . . . , ) .
                          (1, 2, . . . , )

    This last piece of notation may seem somewhat confusing, but it is quite useful when
we need to specify the exact variables and function components used, as we will do, for
example, in the implicit function theorem.

    The Jacobian determinant  is a real-valued function, and when  = 1 it is simply the
derivative. From the chain rule and the fact that det() = det() det(), it follows that:

                           () =  () ().

    The determinant of a linear mapping tells us what happens to area/volume under
the mapping. Similarly, the Jacobian determinant measures how much a differentiable
mapping stretches things locally, and if it flips orientation. In particular, if the Jacobian
determinant is non-zero than we would assume that locally the mapping is invertible (and
we would be correct as we will later see).

     Named after the Italian mathematician Carl Gustav Jacob Jacobi (1804-1851).
     The matrix from Proposition 8.3.9 representing  () is called the Jacobian matrix, or sometimes
confusingly also called just "the Jacobian."
8.3. THE DERIVATIVE                                                                         43

8.3.5 Exercises

Exercise 8.3.1: Suppose  : (-1, 1)   and  : (-1, 1)   are two differentiable curves such that
(0) = (0) and  (0) = (0). Suppose  :    is a differentiable function. Show that

                              =0 () =  =0 () .

Exercise 8.3.2: Let  : 2   be given by  (, )  2 + 2, see Figure 8.7. Show that  is not
differentiable at the origin.

Figure 8.7: Graph of 2 + 2.

Exercise 8.3.3: Using only the definition of the derivative, show that the following  : 2  2 are
differentiable at the origin and find their derivative.

 a)  (, ) (1 +  +  , ),
 b)  (, )  - 10,  ,
 c)  (, ) ( +  + 1)2, ( -  + 2)2 .

Exercise 8.3.4: Suppose  :    and  :    are differentiable functions. Using only the definition
of the derivative, show that  : 2  2 defined by (, )  (), () is a differentiable function, and
find the derivative, at all points (, ).

Exercise 8.3.5: Define a function  : 2   by (see Figure 8.8)

                      (, )                    if (, )  (0, 0),
                             2+2              if (, ) = (0, 0).

                             0

a) Show that the partial derivatives    and    exist at all points (including the origin).
b) Show that  is not continuous at the origin (and hence not differentiable).
44  CHAPTER 8. SEVERAL VARIABLES AND PARTIAL DERIVATIVES

Figure 8.8: Graph of 2+2  .

Exercise 8.3.6: Define a function  : 2   by (see Figure 8.9)

                               (, )    2  if (, )  (0, 0),
                                     2+2  if (, ) = (0, 0).

                                     0

a) Show that the partial derivatives    and    exist at all points.
b) Show that for all   2 with  = 1, the directional derivative   exists at all points.
c) Show that  is continuous at the origin.
d) Show that  is not differentiable at the origin.

Figure 8.9: Graph of 2+2 2 .
8.3. THE DERIVATIVE                                                45

Exercise 8.3.7: Suppose  :    is one-to-one, onto, differentiable at all points, and such that  -1 is
also differentiable at all points.
 a) Show that  () is invertible at all points  and compute (  -1)  () . Hint: Consider  =  -1  () .
b) Let  :    be a function differentiable at    and such that () = . Suppose  () =  for

    some   . Show () =  -1  () where  is the Jacobian determinant.

Exercise 8.3.8: Suppose  : 2   is differentiable and such that  (, ) = 0 if and only if  = 0 and
such that   (0, 0) = (0, 1). Prove that  (, ) > 0 whenever  > 0, and  (, ) < 0 whenever  < 0.

    As for functions of one variable,  :    has a relative maximum at    if there exists a
 > 0 such that  ()   () for all   (, )  . Similarly for relative minimum.

Exercise 8.3.9: Suppose    is open and  :    is differentiable. Suppose  has a relative maximum
at   . Show that  () = 0, that is, the zero mapping in ( , ). Namely,  is a critical point of  .

Exercise 8.3.10: Suppose  : 2   is differentiable and  (, ) = 0 whenever 2 + 2 = 1. Prove that
there exists at least one point (0, 0) such that    (0, 0) =    (0, 0) = 0.

Exercise 8.3.11: Define  (, ) ( - 2)(22 - ). The graph of  is called the Peano surface.
 a) Show that (0, 0) is a critical point, that is,  (0, 0) = 0, the zero linear map in (2, ).
 b) Show that for every direction the restriction of  to a line through the origin in that direction has a

     relative maximum at the origin. In other words, for every (, ) such that 2 + 2 = 1, the function
     ()  (,  ), has a relative maximum at  = 0.
    Hint: While not necessary §4.3 of volume I makes this part easier.
 c) Show that  does not have a relative maximum at (0, 0).

Exercise 8.3.12: Suppose  :    is differentiable and   () = 1 for all  (that is, we have a curve in
the unit sphere). Show that  () ·  () = 0 (treating  () as a vector) for all .

Exercise 8.3.13: Define  : 2  2 by  (, )       ,  + () for some differentiable function  of one
variable. Show  is differentiable and find  .

Exercise 8.3.14: Suppose    is open,   , and  :   ,  :   ,  :    are functions
such that  () = () = (),  and  are differentiable at ,  () = (), and

                      ()  ()  () for all   

Show that  is differentiable at  and () =  () = ().

Exercise 8.3.15: Prove a version of mean value theorem for functions of several variables. That is, suppose
   is open,  :    differentiable, ,   , and the segment [, ]  . Prove that there exists an

  [, ] such that   () · ( - ) =  () -  ().

Named after the Italian mathematician Giuseppe Peano (1858-1932).
46                    CHAPTER 8. SEVERAL VARIABLES AND PARTIAL DERIVATIVES

8.4 Continuity and the derivative

Note: 1-2 lectures

8.4.1 Bounding the derivative

Let us prove a "mean value theorem" for vector-valued functions.
Lemma 8.4.1. If  : [, ]   is differentiable on (, ) and continuous on [, ], then there
exists a 0  (, ) such that

                                      () - ()  ( - )(0).

Proof. By the mean value theorem on the scalar-valued function   () - () · (),
where the dot is the dot product, we obtain a 0  (, ) such that

                    () - ()2 = () - () · () - ()
                                        = () - () · () - () - () · ()
                                        = ( - ) () - () · (0),

where we treat  as a vector in  by the abuse of notation we mentioned in the previous
section. If we think of () as a vector, then by Exercise 8.2.6, ()(,) = () .
That is, the euclidean norm of the vector is the same as the operator norm of ().

    By the Cauchy-Schwarz inequality

    () - ()2 = ( - ) () - () · (0)  ( - )() - () (0). 

    Recall that a set  is convex if whenever ,   , the line segment from  to  lies in .

Proposition 8.4.2. Let    be a convex open set,  :    be a differentiable function,

and an  be such that    ()   for all   .

Then  is Lipschitz with constant , that is,

                        () -  ()   -  for all ,   .

Proof. Fix  and  in  and note that (1 - ) +    for all   [0, 1] by convexity. Next

                              (1 - ) +   =   (1 - ) +   ( - ).
By Lemma 8.4.1, there is some 0  (0, 1) such that

      () -  ()                  (1 - ) + 
                       =0

                         (1 - 0) + 0  -    - .                  
8.4. CONTINUITY AND THE DERIVATIVE                                        47

Example 8.4.3: If  is not convex the proposition is not true: Consider the set
                           (, ) : 0.5 < 2 + 2 < 2 \ (, 0) :  < 0 .

For (, )  , let  (, ) be the angle that the line from the origin to (, ) makes with the
positive  axis. We even have a formula for  :

                                      (, ) = 2 arctan  .
                                                               + 2 + 2

Think a spiral staircase with room in the middle. See Figure 8.10.

(, )
   =  (, )

Figure 8.10: A non-Lipschitz function with uniformly bounded derivative.

    The function is differentiable, and the derivative is bounded on , which is not hard
to see. Now think of what happens near where the negative -axis cuts the annulus
in half. As we approach this cut from positive ,  (, ) approaches . From negative
,  (, ) approaches -. So for small  > 0, |  (-1, ) -  (-1, -)| approaches 2, but
(-1, ) - (-1, -) = 2, which is arbitrarily small. The conclusion of the proposition
does not hold for this nonconvex .

    Let us solve the differential equation   = 0.

Corollary 8.4.4. If    is open and connected,  :    is differentiable, and  () = 0
for all   , then  is constant.

Proof. For any given   , there is a ball (, )  . The ball (, ) is convex. Since
  ()  0 for all   (, ), then by the proposition,   () -  ()  0 -  = 0. So
 () =  () for all   (, ). Therefore,  -1() is open for all   .

    Suppose 0   is such that  -1(0)  . As  is also continuous, the two sets

 =  -1(0),  =  -1  \ {0}

are open and disjoint, and further  =   . As  is nonempty and  is connected,
then  = . So  () = 0 for all   .
                                                                          
48                   CHAPTER 8. SEVERAL VARIABLES AND PARTIAL DERIVATIVES

8.4.2 Continuously differentiable functions

Definition 8.4.5. Let    be open. We say  :    is continuously differentiable, or
1(), if  is differentiable and   :   ( , ) is continuous.

Proposition 8.4.6. Let    be open and  :   . The function  is continuously

                                                                                   

differentiable if and only if the partial derivatives  exist for all  and  and are continuous.

    Without continuity the theorem does not hold. Just because partial derivatives exist
does not mean that  is differentiable, in fact,  may not even be continuous. See the
exercises for the last section and also for this section.

Proof. We proved that if  is differentiable, then the partial derivatives exist. The par-
tial derivatives are the entries of the matrix representing  (). If   :   ( , )

is continuous, then the entries are continuous, and hence the partial derivatives are

continuous.

    To prove the opposite direction, suppose the partial derivatives exist and are continuous.
Fix   . If we show that  () exists we are done, because the entries of the matrix
representing  () are the partial derivatives and if the entries are continuous functions,
the matrix-valued function   is continuous.

    We do induction on dimension. First, the conclusion is true when  = 1 (exercise, note
that  is vector-valued). In this case,  () is essentially the derivative of chapter 4. Suppose
the conclusion is true for -1. That is, if we restrict to the first  - 1 variables, the function
is differentiable. When taking the partial derivatives in 1 through -1, it does not matter
if we consider  or  restricted to the set where  is fixed. In the following, by a slight
abuse of notation, we think of -1 as a subset of , that is, the set in  where  = 0. In
other words, we identify the vectors (1, 2, . . . , -1) and (1, 2, . . . , -1, 0).

    Fix    and let

      1                    1              1                      1                  1 
     1 () . . .  ()                      1 () . . . -1 ()                           ()
      ... . . . ...  ,                    ... . . .                 ..            .     ..     
                                                                      .      ,                    .
                                                                                     
             (    )  ...        (    )           (    )  ...            (    )            (    )
          1                                   1                     -1                         

Let  > 0 be given. By the induction hypothesis, there is a  > 0 such that for every
  -1 with  < , we have

                                       ( + ) -  () -  < .
                                                  

By continuity of the partial derivatives, suppose  is small enough so that

                                           ( + ) -    () < 

for all  and all    with  < .
8.4. CONTINUITY AND THE DERIVATIVE                                      49

    Suppose  =  +  is a vector in , where   -1,   , such that  < . Then
   < . Note that  =  + .

    ( + ) -  () -  =   ( +  + ) -  ( + ) -  +  ( + ) -  () - 
                                    ( +  + ) -  ( + ) -  +   ( + ) -  () - 
                                    ( +  + ) -  ( + ) -  + .

As all the partial derivatives exist, by the mean value theorem, for each  there is some
  [0, ] (or [, 0] if  < 0), such that

                          ( +  + ) - ( + ) =     ( +  +  ).
We have  +     < , and so we can finish the estimate

            ( + ) -  () -     ( +  + ) -  ( + ) -  + 
                                               ( +  +  ) -    2 () + 
                                                =1  
                                            || + 
                                         ( + 1). 

    A common application is to prove that a certain function is differentiable. For example,
we can show that all polynomials are differentiable, and in fact continuously differentiable,
by computing the partial derivatives.

Corollary 8.4.7. A polynomial  :    in several variables

                                                       1 2  
(1, 2, . . . , ) =                   1,2,..., 1 2 · · · 

                    0 1+2+···+

is continuously differentiable.
Proof. Consider the partial derivative of  in the  variable. Write  as

              
                                                  

() = (1, . . . , -1)  ,

             =0

where  are polynomials in one less variable. Then

  -1
     () = (1, . . . , -1)  ,

 =1

which is again a polynomial. So the partial derivatives of polynomials exist and are

again polynomials. By the continuity of algebraic operations, polynomials are continuous

functions. Therefore  is continuously differentiable.                   
50               CHAPTER 8. SEVERAL VARIABLES AND PARTIAL DERIVATIVES

8.4.3 Exercises

Exercise 8.4.1: Define  : 2   as

                  (, )  (2 + 2) sin (2 + 2)-1  if (, )  (0, 0),
                        0                      if (, ) = (0, 0).

Show that  is differentiable at the origin, but that it is not continuously differentiable.
Note: Feel free to use what you know about sine and cosine from calculus.

Exercise 8.4.2: Let  : 2   be the function from Exercise 8.3.5, that is,

                         (, )          if (, )  (0, 0),
                                  2+2  if (, ) = (0, 0).

                                  0

Compute the partial derivatives    and    at all points and show that these are not continuous functions.

Exercise 8.4.3: Let (0, 1)  2 be the unit ball, that is, the set given by 2 + 2 < 1. Suppose
 : (0, 1)   is a differentiable function such that |  (0, 0)|  1, and     1 and     1 for all points
in (0, 1).
 a) Find an    such that   (, )   for all (, )  (0, 1).
 b) Find a    such that |  (, )|   for all (, )  (0, 1).

Exercise 8.4.4: Define  : [0, 2]  2 by () = sin(), cos() . Compute () for all . Compute
() for all . Notice that () is never zero, yet (0) = (2), therefore, Rolle's theorem is not true in
more than one dimension.
Exercise 8.4.5: Let  : 2   be a function such that    and    exist at all points and there exists an
   such that      and      at all points. Show that  is continuous.

Exercise 8.4.6: Let  : 2   be a function and   , such that for every (, )  2, the function
()  (, ) is differentiable and |()|   for all .
 a) Show that  is continuous at (0, 0).
 b) Find an example of such an  that is discontinuous at every other point of 2.

    Hint: Think back to how we constructed a nowhere continuous function on [0, 1].

Exercise 8.4.7: Suppose  :  \    is a rational function, that is,  :    and  :    are
polynomials,  is not identically zero,  = -1(0), and  =  . Show that  is continuously differentiable.
Exercise 8.4.8: Suppose  :    and  :    are two differentiable functions such that  () =
() for all   . Prove that if  (0) = (0), then  () = () for all   .

Exercise 8.4.9: Prove the base case in Proposition 8.4.6. That is, prove that if  = 1 and "the partials exist
and are continuous," then the function is continuously differentiable. Note that  is vector-valued.

Exercise 8.4.10: Suppose that    is open,  :    is differentiable, there is an  such that
  ()   for all   , and    is a compact set. Prove that there exists an  (where   ),
such that for all ,    we have   () -  ()   - . Compare to Proposition 8.4.2.
8.5. INVERSE AND IMPLICIT FUNCTION THEOREMS                      51

8.5 Inverse and implicit function theorems

Note: 2-3 lectures

    Intuitively, if a function is continuously differentiable, then it locally "behaves like" the
derivative (which is a linear function). The idea of the inverse function theorem is that if
a function is continuously differentiable and the derivative is invertible, the function is
(locally) invertible.

Theorem 8.5.1 (Inverse function theorem). Let    be an open set and let  :    be
a continuously differentiable function. Suppose    and  () is invertible (that is,  ()  0).
Then there exist open sets  ,    such that     ,  () = , and  | is one-to-one.
Hence a function  :    exists such that () (  | )-1(). Furthermore,  is continuously

differentiable and    () =  () -1, for all    ,  =  ().

See Figure 8.11.

                                                          =  ()
                                                          ()
                      
                                                           

                    

                      

Figure 8.11: Setup of the inverse function theorem in .

    To prove the theorem, we use the contraction mapping principle from chapter 7, where
we used it to prove Picard's theorem. Recall that a mapping  :    between metric
spaces ( , ) and (, ) is a contraction if there exists a  < 1 such that

                              (),  ()   (, ) for all ,   .

The contraction mapping principle says that if  :    is a contraction and  is a
complete metric space, then there exists a unique fixed point, that is, there exists a unique
   such that  () = .

Proof. Write  =  (). As   is continuous, there is an open ball  centered at  such that

                                  -  () < 2-1 1  for all    .

Consequently, the derivative  () is invertible for all    by Proposition 8.2.6.
52  CHAPTER 8. SEVERAL VARIABLES AND PARTIAL DERIVATIVES

    Given   , define  :    by

                           ()  + -1  -  () .

As -1 is one-to-one, () =  ( is a fixed point) if only if  -  () = 0, or in other words
 () = . Using the chain rule we obtain

                    () =  - -1  () = -1  -  () .

So for   , we have         ()  -1  -  () < 1/2.

As  is a ball, it is convex. Hence

    (1) - (2)  1 1 - 2              for all 1, 2   .
                                2

In other words,  is a contraction defined on , though we so far do not know what

is the range of . We cannot yet apply the fixed point theorem, but we can say that
 has at most one fixed point in : If (1) = 1 and (2) = 2, then 1 - 2 =
(1) - (2)  12 1 - 2, so 1 = 2. That is, there exists at most one    such
that  () = , and so  | is one-to-one.

    Let   () and let  :    be the inverse of  | . We need to show that  is
open. Take a 0  . There is a unique 0   such that  (0) = 0. Let  > 0 be small
enough such that the closed ball (0, )   (such  > 0 exists as  is open).

    Suppose  is such that            - 0 < 2-1   .

If we show that   , then we have shown that  is open. If 1  (0, ), then

    (1) - 0  (1) - (0) + (0) - 0

                        1 1 - 0 + -1( - 0)
                          2

                        1  + -1  - 0
                          2

                       < 12  + -1 2-1   = .

So  takes (0, ) into (0, )  (0, ). It is a contraction on (0, ) and (0, ) is
complete (closed subset of  is complete). Apply the contraction mapping principle to
obtain a fixed point , i.e. () = . That is,  () = , and    (0, )   () = .
Therefore,  is open.

    Next we need to show that  is continuously differentiable and compute its derivative.
First, let us show that it is differentiable. Let    and   ,   0, such that  +   .
Because  | is a one-to-one and onto mapping of  onto , there are unique    and
8.5. INVERSE AND IMPLICIT FUNCTION THEOREMS                         53

                                                          
                                                           +
+  
                                                                
                            
                                                      

Figure 8.12: Proving that  is differentiable.

  ,   0 and  +   , such that  () =  and  ( + ) =  + . In other words,
() =  and ( + ) =  + . See Figure 8.12.

    We can still squeeze some information from the fact that  is a contraction.

                    ( + ) - () =  + -1  () -  ( + ) =  - -1.

So 1 
                     - -1 = ( + ) - ()  2  +  -  = 2 .

By the inverse triangle inequality,  - -1  12 . So

                                         2-1  2-1 .

In particular, as  goes to 0, so does .                  () -1, which is what we think the
    As   , then  () is invertible. Let 

derivative of  at  is. Then

( + ) - () -   =  -  

   =  -   ( + ) -  ()  

   =   ( + ) -  () -  () 
                                                        
       ( + ) -  () -  ()
                                                          
    2 -1   ( + ) -  () -  () .
                                                          

As  goes to 0, so does . So the right-hand side goes to 0 as  is differentiable, and hence
the left-hand side also goes to 0. And  is precisely what we wanted () to be.

    We have  is differentiable, let us show it is 1(). The function  :    is
continuous (it is differentiable),   is a continuous function from  to (), and   -1
is a continuous function on the set of invertible operators. As () =   () -1 is the

composition of these three continuous functions, it is continuous.  
54  CHAPTER 8. SEVERAL VARIABLES AND PARTIAL DERIVATIVES

Corollary 8.5.2. Suppose    is open and  :    is a continuously differentiable
mapping such that  () is invertible for all   . Then for every open set   , the set  ()
is open (  is said to be an open mapping).

Proof. Without loss of generality, suppose  = . For each    (), pick    -1()

(there could be more than one such point), then by the inverse function theorem there is a

neighborhood of  in  that maps onto a neighborhood of . Hence  () is open.  

Example 8.5.3: The theorem, and the corollary, is not true if  () is not invertible for
some . For example, the map  (, ) (,  ), maps 2 onto the set 2 \ (0, ) :   0 ,
which is neither open nor closed. In fact,  -1(0, 0) = (0, ) :    . This bad behavior
only occurs on the -axis, everywhere else the function is locally invertible. If we avoid the
-axis,  is even one-to-one.

Example 8.5.4: Just because  () is invertible everywhere does not mean that  is one-to-
one. It is "locally" one-to-one, but perhaps not "globally." Consider  : 2 \ (0, 0) 

2 \ (0, 0) defined by  (, ) (2 - 2, 2 ). It is left to the reader to verify the following
statements. The map  is differentiable and the derivative is invertible. On the other hand,
 is 2-to-1 globally: For every (, ) that is not the origin, there are exactly two solutions to
2 - 2 =  and 2  =  (  is also onto). Notice that once you show that there is at least
one solution, replacing  and  with - and - we obtain another solution.

    The invertibility of the derivative is not a necessary condition, just sufficient, for having
a continuous inverse and for being an open mapping. For example, the function  () 3

is an open mapping from  to  and is globally one-to-one with a continuous inverse,
although the inverse is not differentiable at  = 0.

    As a side note, there is a related famous, and as yet unsolved, problem called the
Jacobian conjecture. If  :    is polynomial (each component is a polynomial) and
 (the Jacobian determinant) is a nonzero constant, does  have a polynomial inverse?
The inverse function theorem gives a local 1 inverse, but can one always find a global

polynomial inverse is the question.

8.5.1 Implicit function theorem

The inverse function theorem is a special case of the implicit function theorem, which we
prove next. Although somewhat ironically we prove the implicit function theorem using
the inverse function theorem. In the inverse function theorem we showed that the equation
 -  () = 0 is solvable for  in terms of  if the derivative with respect to  is invertible,
that is, if  () is invertible. Then there is (locally) a function  such that  -  () = 0.

    In general, the equation  (, ) = 0 is not solvable for  in terms of  in every case. For
instance, there is generally no solution when  (, ) does not actually depend on . For a
more interesting example, notice that 2 + 2 - 1 = 0 defines the unit circle, and we can
locally solve for  in terms of  when 1) we are near a point on the unit circle and 2) we are
not at a point where the circle has a vertical tangency, that is, where    = 0.
8.5. INVERSE AND IMPLICIT FUNCTION THEOREMS                        55

    We fix some notation. Let (, )  + denote the coordinates (1, . . . ,  , 1, . . . , ).
We can then write a linear map   (+ , ) as  = [ ] so that (, ) =  + ,
where   ( , ) and   (). First, the linear version of the theorem.

Proposition 8.5.5. Let  = [ ]  (+ , ) and suppose  is invertible.  If  =
-()-1, then

                                        0 = (, ) =   + .

Furthermore,  =  is the unique    such that (, ) = 0.

    The proof is immediate: We solve and obtain  = . Another way to solve is to
"complete the basis," that is, add rows to the matrix until we have an invertible matrix: The
operator in (+) given by (, )  (,   +  ) is invertible, and the map  can be
read off from the inverse. Let us show that the same can be done for 1 functions.

Theorem 8.5.6 (Implicit function theorem). Let   + be an open set and let  :   
be a 1() mapping. Let (, )   be a point such that  (, ) = 0 and such that

                                            ( 1, . . . , ) (, )  0.
                                            (1, . . . , )

Then there exists an open set    with   , an open set    with   , where
 ×   , and a 1() map  :   , with () = , and for all   , the point ()
is the unique point in  such that

                                                   , () = 0.
Furthermore, if  = [ ] =  (, ), then

() = -()-1 .

    The condition (1,...,) ( 1,..., ) (, ) = det()  0 simply means that  is invertible. If
 =  = 1, the condition is    (, )  0, and  and  are open intervals. See Figure 8.13.

  (, )



                                              × 

  
        (, ) = 0

Figure 8.13: Implicit function theorem for  (, ) = 2 + 2 - 1 in  = 2 and (, ) in the first
quadrant.
56  CHAPTER 8. SEVERAL VARIABLES AND PARTIAL DERIVATIVES

Proof. Define  :   + by (, ) ,  (, ) . It is clear that  is 1, and we want
to show that its derivative at (, ) is invertible. Let us compute the derivative. The quotient

      ( + ,  + ) -  (, ) -   -  
                        (, )

goes to zero as (, ) = 2 + 2 goes to zero. But then so does

  ( + ,  + ) - (, ) - (,   +  )
                          (, )
                                                 =  ,  ( + ,  + ) -  (, ) - (,   +  )
                                                                               (, )
                                                 =   ( + ,  + ) -  (, ) -   -  .
                                                                         (, )

So the derivative of  at (, ) takes (, ) to (,   +  ). In block matrix form, it is

    0

    . If (,   +  ) = (0, 0), then  = 0, and so   = 0. As  is one-to-one,  = 0.
Thus (, ) is one-to-one, and hence invertible. We apply the inverse function theorem.

    That is, there exists an open set   + with (, ) = (, 0)  , and a 1 mapping
 :   +, such that  (, ) = (, ) for all (, )  ,  is one-to-one, and () is
open. Write  = (1, 2) (the first  and the next  components of ). Then

     1(, ), 2(, ) = 1(, ),  1(, ), 2(, ) = (, ).

So  = 1(, ) and  1(, ), 2(, ) =  , 2(, ) = . Plugging in  = 0, we obtain

     , 2(, 0) = 0.

As the set () is open and (, )  (), there exist some open sets  and  such that
 ×   () with    and   . Take     : 2(, 0)   . The function
that takes  to 2(, 0) is continuous and therefore  is open. Define  :    by
() 2(, 0), which is the  in the theorem. The fact that () is the unique point in 
follows because  ×   () and  is one-to-one.

    Next, differentiate
                                                    , ()

at , which is the zero map, so its derivative is zero. Using the chain rule,

    0 =  , () =   +  ()

for all   , and we obtain the desired derivative for .       
8.5. INVERSE AND IMPLICIT FUNCTION THEOREMS                57

In other words, in the context of the theorem, we have  equations in  +  unknowns:

                        1(1, . . . ,  , 1, . . . , ) = 0,
                        2(1, . . . ,  , 1, . . . , ) = 0,

                                         ...

                        (1, . . . ,  , 1, . . . , ) = 0.

The theorem guarantees a solution if  = ( 1, 2, . . . , ) is a 1 map (the components are
1: partial derivatives in all variables exist and are continuous) and the matrix

                          1  1     1 
                         1 2 . . .  
                                        
                          2  2     2 
                         1 2 . . .  
                         .. .. . . . .. 
                        . .         .
                                  
                                ... 
                         1 2     

is invertible at (, ).

Example 8.5.7: Consider the set given by 2 + 2 - ( + 1)3 = -1 and  +   +  = 3 near
the point (0, 0, 0). It is the zero set of the mapping

             (, , ) = 2 + 2 - ( + 1)3 + 1,  +   +  - 3 ,

whose derivative is      2 2 -3( + 1)2

                        =                    .
                          

The matrix              0 0 2(0) -3(0 + 1)2 = 1 1 0 -3

is invertible. Hence near (0, 0, 0), we can solve for  and  as 1 functions of  such that for

 near 0,    2 + ()2 - () + 1 3 = -1,   +  () +  () = 3.

In other words, near the origin the set of solutions is a smooth curve in 3 that goes
through the origin. The theorem does not tell us how to find () and () explicitly, it just

tells us they exist.

    An interesting, and sometimes useful, observation from the proof is that we solved the
equation  , () =  for all  in some neighborhood of 0, not just  = 0.

Remark 8.5.8. There are versions of the theorem for arbitrarily many derivatives: If  has 
continuous derivatives (see the next section), then the solution has  continuous derivatives
as well.
58                      CHAPTER 8. SEVERAL VARIABLES AND PARTIAL DERIVATIVES

8.5.2 Exercises

Exercise 8.5.1: Let  (, )  2 : 2 + 2 = 1 .

a) Solve for  in terms of  near (0, 1) (that is, find the function  from the implicit function theorem for a
   neighborhood of the point (, ) = (0, 1)).

b) Solve for  in terms of  near (0, -1).

c) Solve for  in terms of  near (-1, 0).

Exercise 8.5.2: Define  : 2  2 by  (, )           ,  + () for some continuously differentiable
function  of one variable.

a) Show that  is one-to-one and onto.

b) Compute  . (Make sure to argue why   exists.)

c) Show that   is invertible at all points, and compute its inverse.

Exercise 8.5.3: Define  : 2  2 \ (0, 0) by  (, )  cos(),  sin() .

a) Show that  is onto.

b) Show that   is invertible at all points.

c) Show that  is not one-to-one, in fact for every (, )  2 \ (0, 0) , there exist infinitely many different
   points (, )  2 such that  (, ) = (, ).

Therefore, invertible derivative at every point does not mean that  is invertible globally.
Note: Feel free to use what you know about sine and cosine from calculus.

Exercise 8.5.4: Find a map  :    that is one-to-one, onto, continuously differentiable, but  (0) = 0.
Hint: Generalize  () = 3 from one to  dimensions.

Exercise 8.5.5: Consider 2 +  +  = 0 in 3. Find an equation (, ) = 0, such that if (0, 0)  0
and 2 + 0 + 0 = 0 for some   , then for points near (0, 0) there exist exactly two distinct
continuously differentiable functions 1(, ) and 2(, ) such that  = 1(, ) and  = 2(, ) solve
2 +  +  = 0. Do you recognize the expression  from algebra?

Exercise 8.5.6: Suppose  : (, )  2 is continuously differentiable and the first component (the 
component) of   () is not equal to 0 for all   (, ). Prove that there exists an open interval    and a
continuously differentiable function  :    such that (, )   (, ) if and only if    and  = ().
In other words, the set  (, ) is a graph of .

Exercise 8.5.7: Define  : 2  2

                         (, )                 2 sin(1/) + /2,   if   0,
                                             (0, )              if  = 0.

a) Show that  is differentiable everywhere.
b) Show that  (0, 0) is invertible.
c) Show that  is not one-to-one in every neighborhood of the origin (it is not locally invertible, that is, the

    inverse function theorem does not work).
d) Show that  is not continuously differentiable.

Note: Feel free to use what you know about sine and cosine from calculus.
8.5. INVERSE AND IMPLICIT FUNCTION THEOREMS                  59

Exercise 8.5.8 (Polar coordinates): Define a mapping (, )  cos(),  sin() .
 a) Show that  is continuously differentiable (for all (, )  2).
 b) Compute (0, ) for all .
 c) Show that if   0, then (, ) is invertible, therefore an inverse of  exists locally as long as   0.
d) Show that  : 2  2 is onto, and for each point (, )  2, the set -1(, ) is infinite.
 e) Show that  : 2  2 is not an open mapping. Note that |(0,)× is an open mapping via

     Corollary 8.5.2. Hint: Where does a small open rectangle such as (-, ) × (-, ) go?
 f) Show that |(0,)×[0,2) is one-to-one and onto 2 \ (0, 0) .

Note: Feel free to use what you know about sine and cosine from calculus.

Exercise 8.5.9: Let  (, )  2 :  > 0}, and for (, )   define

(, )  2 + 2 - 1  -2
      2 + 2 + 2 + 1, 2 + 2 + 2 + 1 .

Prove that  is a bective mapping from  to (0, 1), it is continuously differentiable on , and its inverse is
also continuously differentiable.

Exercise 8.5.10: Suppose   2 is open and  :    is a 1 function such that   (, )  0 for all
(, )  . Show that every level set is a 1 smooth curve. That is, for every (, )  , there exists a 1
function  : (-, )  2 with  (0)  0 such that  () is constant for all   (-, ).

Exercise 8.5.11: Suppose   2 is open and  :    is a 1 function such that   (, )  0 for all
(, )  . Show that for every (, ) there exists a neighborhood  of (, ) an open set   2, a bective
1 function with a 1 inverse  :    such that the level sets of    are horizontal lines in , that is,
the set given by (   )(, ) =  for a constant  is a set of the form (, 0)  2 :   , (, 0)   ,
where 0 is fixed. That is, the level curves can be locally "straightened."
60  CHAPTER 8. SEVERAL VARIABLES AND PARTIAL DERIVATIVES

8.6 Higher order derivatives

Note: less than 1 lecture, optional, see also the optional §4.3 of volume I
    Let    be an open set and  :    a function. Denote our coordinates by

 = (1, 2, . . . , )  . Suppose    exists everywhere in , then it is also a function
   :   . Therefore, it makes sense to talk about its partial derivatives. We denote the
partial derivative of    with respect to  by

                               2      
                                     .

If  =  , then we write 2 2  for simplicity.                    Suppose 1, 2, . . . ,  are integers

                                                             

    We define higher order derivatives inductively.

between 1 and , and suppose  -1 

                             -1 -2 · · · 1

exists and is differentiable in the variable  , then the partial derivative with respect to

that variable is denoted by

                                    -1  -1 -2 ···1
     -1 · · · 1                                           .

                                            

Such a derivative is called a partial derivative of order .
    Sometimes the notation   is used for  2  . This notation swaps the order in which

we write the derivatives, which may be important.

Definition 8.6.1. Suppose    is an open set and  :    is a function. We say  is
-times continuously differentiable function, or a  function, if all partial derivatives of all
orders up to and including order  exist and are continuous.

    So a continuously differentiable, or 1, function is one where all first order partial

derivatives exist and are continuous, which agrees with our previous definition due to
Proposition 8.4.6. We could have required only that the th order partial derivatives exist

and are continuous, as the existence of lower order partial derivatives is clearly necessary
to even define th order partial derivatives, and these lower order partial derivatives are

continuous as they are (continuously) differentiable functions.

    When the partial derivatives are continuous, we can swap their order.

Proposition 8.6.2. Suppose    is open and  :    is a 2 function, and  and  are
two integers from 1 to . Then

                                                 2  = 2  .
                                                
8.6. HIGHER ORDER DERIVATIVES                     61

Proof. Fix a   , and let  and  be the standard basis vectors. Pick two positive
numbers  and  small enough so that  + 0 + 0   whenever 0 < 0   and
0 < 0  . Any small enough  and  work as  is open and so contains a small open ball
(or a box if you wish) around .

    Use the mean value theorem on the function

                    ( +  + ) -  ( + ),

on the interval [0, ] to find a 0  (0, ) such that
  ( +  + ) -  ( + ) -  ( +  ) +  ()  =    ( +  + 0) -    ( + 0).

Similarly, there exists a number 0  (0, ) such that

         ( +   + 0) -    ( + 0) = 2  ( + 0 + 0).
                                

In other words,

(, )   ( +  + ) -  ( + ) -  ( +  ) +  () 2   ( + 0 + 0).
                                      =
                                           

                                + 0 + 0
                                  +  + 
                   +              +  + 0
                  + 0

                                             +  

Figure 8.14: Using the mean value theorem to estimate a second order partial derivative by a
certain difference quotient.

    See Figure 8.14. The 0 and 0 depend on  and , but 0 < 0 <  and 0 < 0 < .
Let the domain of the function  be the set (0, ) × (0, ) for some small  > 0. As
(, )  (0, ) × (0, ) goes to (0, 0), the point (0, 0) also goes to (0, 0). By continuity of the

second partial derivatives,

                   lim (, ) = 2  ().
                 ( , )(0,0)     

    Now reverse the roles of  and  (and  and ). Start with the function    ( +  +
) -  ( +  ) find an 1  (0, ) such that

 ( +  + ) -  ( +  ) -  ( + ) +  ()  =    ( + 1 + ) -    ( + 1 ).
62                      CHAPTER 8. SEVERAL VARIABLES AND PARTIAL DERIVATIVES

Find a 1  (0, ) such that

              ( + 1 +  ) -    ( + 1 ) = 2  ( + 1 + 1).
                                                         

So (, ) =  2  ( + 1 + 1) for the same  as above. As before,

                                           lim (, ) = 2  ().
                           ( , )(0,0)                    

Therefore, the two partial derivatives are equal.                            

    The proposition does not hold if the derivatives are not continuous. See Exercise 8.6.2.
Notice also that we did not really need a 2 function, we only needed the two second order

partial derivatives involved to be continuous functions.

8.6.1 Exercises

Exercise 8.6.1: Suppose  :    is a 2 function for some open    and   . Use the proof of

Proposition 8.6.2 to find an expression in terms of just the values of  (analogue of the difference quotient for
the first derivative), whose limit is   2  ().

Exercise 8.6.2: Define

                            (, )            (2-2)       if (, )  (0, 0),
                                              2+2       if (, ) = (0, 0).

                                           0

Show that

a) The first order partial derivatives exist and are continuous.
b) The partial derivatives  2  and  2  exist, but are not continuous at (0, 0), and  2  (0, 0)   2  (0, 0).

Exercise 8.6.3: Let  :    be a  function for some open    and   . Suppose 1, 2, . . . , 
are integers between 1 and , and  = (1, 2, . . . , ) is a permutation of (1, 2, . . . , ). Prove

                             () =   ().
                         -1 · · · 1         -1 · · · 1

Exercise 8.6.4: Suppose  : 2   is a  function such that (0, ) = (0, ) for all ,    and
(, ) = (,  + 2) for all ,   . Let (, )  cos(),  sin() from Exercise 8.5.8. Show that
a function  : 2  , given (, )  -1(, ) is well-defined (notice that -1(, ) can only be
defined locally), and when restricted to 2 \ {0} it is a  function.

Note: Feel free to use what you know about sine and cosine from calculus.

Exercise 8.6.5: Suppose  : 2   is a 2 function. For all (, )  2, compute

           lim           ( + , ) +  ( - , ) +  (,  + ) +  (,  - ) - 4  (, )

           0                                         2

in terms of the partial derivatives of  .
8.6. HIGHER ORDER DERIVATIVES                                        63

Exercise 8.6.6: Suppose  : 2   is a function such that all first and second order partial derivatives
exist. Furthermore, suppose that all second order partial derivatives are bounded functions. Prove that  is

continuously differentiable.

Exercise 8.6.7: Follow the strategy below to prove the following simple version of the second derivative test
for functions defined on 2 (using (, ) as coordinates): Suppose  : 2   is a twice continuously
differentiable function with a critical point at the origin,  (0, 0) = 0. If

2   (0, 0) > 0 and             2   2                  2  2
                                 2 (0, 0) 2 (0, 0) -    (0, 0) > 0,
2                                                     

then  has a (strict) local minimum at (0, 0). Use the following technique: First suppose without loss of
generality that  (0, 0) = 0. Then prove:
a) There exists an   (2) such that  =    is such that  2  (0, 0) = 0, and 2 2  (0, 0) = 2 2  (0, 0) = 1.

b) For every  > 0, there exists a  > 0 such that (, ) - 2 - 2 < (2 + 2) for all (, ) 
      (0, 0),  .
    Hint: You can use Taylor's theorem in one variable.

 c) This means that , and therefore  , has a strict local minimum at (0, 0).

Note: You must avoid the temptation to just apply the one variable second derivative test along lines through
the origin, see Exercise 8.3.11.
64  CHAPTER 8. SEVERAL VARIABLES AND PARTIAL DERIVATIVES
Chapter 9

One-dimensional Integrals in Several
Variables

9.1 Differentiation under the integral

Note: less than 1 lecture

Let  (, ) be a function of two variables, and define

                                          

                           ()                (, ) .

If  is continuous on the compact rectangle [, ] × [, ], then Proposition 7.5.12 from

volume I says that  is continuous on [, ].

Suppose  is differentiable in . When can we "differentiate under the integral"? That

is, when is it true that  is differentiable and its derivative is

                            ?   
                            () =             (, ) .
                                           

Differentiation is a limit and therefore we are really asking when do the two limiting

operations of integration and differentiation commute. This is not always possible and

some extra hypothesis is necessary. The first question we would face is the integrability of
   , but the formula above can fail even if    is integrable as a function of  for every fixed .

    We prove a simple, but perhaps the most useful version of this kind of result.

Theorem 9.1.1 (Leibniz integral rule). Suppose  : [, ] × [, ]   is a continuous function,
such that    exists for all (, )  [, ] × [, ] and is continuous. Define  : [, ]   by

                                          

                           ()                (, ) .

Then  is continuously differentiable and

                              () =   (, ) .
                                        
66            CHAPTER 9. ONE-DIMENSIONAL INTEGRALS IN SEVERAL VARIABLES

    The hypotheses on  and    can be weakened, see e.g. Exercise 9.1.8, but not dropped
outright. The main point in the proof requires that    exists and is continuous for all  up
to the endpoints, but we only need a small interval in the  direction. In applications, we
often make [, ] a small interval around the point where we need to differentiate.

Proof. Fix   [, ] and let  > 0 be given. As    is continuous on [, ] × [, ] it is
uniformly continuous. In particular, there exists  > 0 such that whenever 1  [, ] with
1 -  <  and all   [, ], we have

                                     (, 1) -   (, ) < .

    Suppose  is such that  +   [, ] and || < . Fix  for a moment and apply the
mean value theorem to find a 1 between  and  +  such that

                                       (,  + ) -  (, )  =   (, 1).

As 1 -   || < ,

                     (,  + ) -  (, )  -   (, ) =   (, 1) -   (, ) < .

The argument worked for every   [, ] (different 1 may have been used). Thus, as a
function of 

     (,  + ) -  (, )  converges uniformly to     (, ) as   0.

We defined uniform convergence for sequences although the idea is the same. You
                                                         {   }
may  replace    with  a  sequence  of  nonzero  numbers              converging  to  0  such  that
                                                                 =1
 +   [, ] and let   .

    Consider the difference quotient of ,

    ( + ) - () =     (,  + )  -     (, )    =  (,  + ) -  (, ) .     
                                                         

Uniform convergence implies the limit can be taken underneath the integral. So

     lim ( + ) - ()   = lim  (,  + ) -  (, )    =   (, ) .
     0                    0                                           

Then  is continuous on [, ] by Proposition 7.5.12 from volume I mentioned above. 
9.1. DIFFERENTIATION UNDER THE INTEGRAL                                                                                67

Example 9.1.2: Let                          1
Then                                 () = sin(2 - 2) .

                                                    0

                                          1
                                  () = -2 cos(2 - 2) .

                                                0

Example 9.1.3: Consider                             1  - 1 .
                                                    0 ln()

The function under the integral extends to be continuous on [0, 1], and hence the integral

exists, see Exercise 9.1.1. Trouble is finding it. We introduce a parameter  and define a

function:                                   1 ()  - 1 .
                                                      0 ln()

The  function   -1   also   extends    to  a  continuous       function  of    and      for  (,  )    [0,  1]  ×  [0,  1]
               ln()
(also part of the exercise). See Figure 9.1.

Figure  9.1:   The  graph     =   -1   on  [0, 1]  ×  [0, 1].
                                 ln()

    Hence,  is a continuous function on [0, 1] and (0) = 0. For every  > 0, the 
derivative of the integrand, , is continuous on [0, 1] × [, 1]. Therefore, for  > 0, we may

differentiate under the integral sign,

                              1 () = ln()  =   1  = 1 .
                                       0 ln()                                     +1
                                                                   0

We   need  to  figure  out  (1)  given     that    ()  =        1  and  (0)    =  0.  Elementary    calculus      says
               1                                               +1

that (1) = 0  ()  = ln(2). Thus,

                                               1  - 1  = ln(2).
                                               0 ln()
68                CHAPTER 9. ONE-DIMENSIONAL INTEGRALS IN SEVERAL VARIABLES

9.1.1 Exercises

Exercise 9.1.1: Prove the two statements that were asserted in Example 9.1.3:

a)  Prove  -1     extends to a continuous function of [0, 1].  That is, there exists a continuous function on [0, 1]
           ln()
                  -1
    that  equals  ln()  on  (0,  1).

b)  Prove   -1    extends to a continuous function on [0, 1] × [0, 1].
           ln()

Exercise 9.1.2: Suppose  :    is continuous and  :    is continuously differentiable and
compactly supported. That is, there exists some  > 0, such that () = 0 whenever ||  . Define

                                           

                                       ()          -  ()( - ) .

Show that  is differentiable.

Exercise 9.1.3: Suppose  :    is infinitely differentiable (derivatives of all orders exist) and  (0) = 0.
Show that there exists an infinitely differentiable function  :    such that  () =  (). Show also
that if  (0)  0, then (0)  0.

                                     

Hint: Write  () = 0  ()  and then rewrite the integral to go from 0 to 1.

Exercise 9.1.4: Compute 0  1  . Derive the formula for 0  1    not using integration by parts, but
by differentiation underneath the integral.

Exercise 9.1.5: Let    be open and suppose  (, 1, 2, . . . , ) is a continuous function defined on
[0, 1] ×   +1. Suppose 1   , 2   , . . . ,    exist and are continuous on [0, 1] × . Prove that  :   
defined by

                                                          1

                                 (1, 2, . . . , )         (, 1, 2, . . . , ) 

                                                      0

is continuously differentiable.

                                               z
                                  y

                                                                            x

Figure 9.2: The graph  = (2+2)2 3 on [0, 1] × [0, 1].
9.1. DIFFERENTIATION UNDER THE INTEGRAL                                                              69

Exercise 9.1.6: Work out the following counterexample: Let

                        (, )       3                if   0 or   0,
                                   2 2 2            if  = 0 and  = 0.

                                     ( + )

                                  0
                                  

See Figure 9.2.
 a) Prove that for every fixed , the function    (, ) is Riemann integrable on [0, 1], and

                                  1 
                              ()                  (, )  = 2 .
                                                            2 + 2
                                  0

Therefore, () exists and its derivative is the continuous function

                       () =   1  (, )  =                    1 - 2
                                   0                                 2.
                                                            2(2 + 1)

b) Prove    exists at all  and  and compute it.

c) Show that for all               1   (, ) 
                                   0 

exists, but                         1 (0)    (, 0) .
                                             0 

Exercise 9.1.7: Work out the following counterexample: Let

                        (, )       sin                if (, )  (0, 0),
                                  0              2+2  if (, ) = (0, 0).

a) Prove  is continuous on all of 2. Therefore the following function is well-defined for every   :

                                                 1

                                  ()                   (, ) .

                                                 0

b) Prove    exists for all (, ), but is not continuous at (0, 0).
 c) Show that 0   1   (, 0)  does not exist even if we take improper integrals, that is, that the limit

     lim+    1   (, 0)  does not exist.

       0

Note: Feel free to use what you know about sine and cosine from calculus.
70                CHAPTER 9. ONE-DIMENSIONAL INTEGRALS IN SEVERAL VARIABLES

Exercise 9.1.8: Strengthen the Leibniz integral rule in the following way. Suppose  : (, ) × (, )  
is a bounded continuous function, such that    exists for all (, )  (, ) × (, ) and is continuous and
bounded. Define  : (, )   by  

                                                  ()          (, ) .

Then  is continuously differentiable and

                                                 () =   (, ) .
                                                           

Hint: See also Exercise 7.5.18 and Theorem 6.2.10 from volume I.

Exercise 9.1.9:   Suppose  :     is continuously differentiable,              :        2    is continuous,          exists and
                                                                                                                 

is continuous at all points. Show that

                                        (, )                       
                                                           () + (, ) 

                                                                         0

is  continuously  differentiable,  and  that  it  is  the  solution  of  the  partial  differential  equation    =  ,  with  the
                                                                                                               
initial condition (, 0) = () for all   .
9.2. PATH INTEGRALS                                                                                    71

9.2 Path integrals

Note: 2-3 lectures

9.2.1 Piecewise smooth paths

Let  : [, ]   be a function and write  = (1, 2, . . . , ). Suppose  is continuously

differentiable, meaning it is differentiable and the derivative is continuous. In other words,
there exists a continuous function   : [, ]   such that for every   [, ], we have
lim || (+)-()- ()  = 0. We treat  () either as a linear operator (an  × 1 matrix) or
0
a vector,  () =      1 (  ),   2 (  ),  .  .  .  ,      (  )
                                                              .  Equivalently,    is  a  continuously  differentiable
                                                      
function on [, ] for every  = 1, 2, . . . , . By Exercise 8.2.6, the operator norm of the
operator  () equals the euclidean norm of the corresponding vector, which allows us to
write  () without any confusion.

Definition 9.2.1. A continuously differentiable function  : [, ]   is called a smooth
path or a continuously differentiable path if  is continuously differentiable and  ()  0 for
all   [, ].

    The function  : [, ]   is called a piecewise smooth path or a piecewise continuously
differentiable path if there exist finitely many points 0 =  < 1 < 2 < · · · <  =  such that
the restriction |[-1,] is smooth path for every  = 1, 2, . . . , .

    A path  is a closed path if () = (), that is, the path starts and ends in the same point.
A path  is a simple path if either 1)  is a one-to-one function, or 2) |[,) is one-to-one and
() = () ( is a simple closed path).

Example 9.2.2: Let  : [0, 4]  2 be defined by

                                    ()                  (, 0)      if   [0, 1],
                                                        (1,  - 1)  if   (1, 2],

                                                        (3 - , 1) if   (2, 3],
                                                        (0, 4 - ) if   (3, 4].

    The path  is the unit square traversed counterclockwise. See Figure 9.3. It is a piecewise
smooth path. For example, |[1,2]() = (1,  - 1) and so (|[1,2])() = (0, 1)  0. Similarly
for the other 3 sides. Notice that (|[1,2])(1) = (0, 1), (|[0,1])(1) = (1, 0), but  (1) does not
exist. At the corners  is not differentiable. The path  is a simple closed path, as |[0,4) is
one-to-one and (0) = (4).

    The definition of a piecewise smooth path as we have given it implies continuity
(exercise). For general functions, many authors also allow finitely many discontinuities,
when they use the term piecewise smooth, and so one may say that we defined a piecewise

     The word "smooth" can sometimes mean "infinitely differentiable" in the literature.
72  CHAPTER 9. ONE-DIMENSIONAL INTEGRALS IN SEVERAL VARIABLES

    =3                                             =2

    =4      =0                                     =1

Figure 9.3: The path  traversing the unit square.

smooth path to be a continuous piecewise smooth function. While one may get by with smooth
paths, for computations, the simplest paths to write down are often piecewise smooth.

    Generally, we are interested in the direct image  [, ] , rather than the specific
parametrization, although that is also important to some degree. When we informally talk
about a path or a curve, we often mean the set  [, ] , depending on context.

Example 9.2.3: The condition  ()  0 means that the image  [, ] has no "corners"
where  is smooth. Consider

                                           () (0, 2 (2, 0) if  < 0, ) if   0.

See Figure 9.4. It is left for the reader to check that  is continuously differentiable, yet the
image () = (, )  2 : (, ) = (, 0) or (, ) = (0, ) for some   0 has a "corner"
at the origin. And that is because  (0) = (0, 0). More complicated examples with, say,
infinitely many corners exist, see the exercises.

    = -1

    = -1/2

    =0          = 1/2                              =1

Figure 9.4: "Smooth" path with a corner if we allow zero derivative. The points corresponding
to several values of  are marked with dots.

    The condition  ()  0 even at the endpoints of every subinterval guarantees not only
no corners, but also that the path ends nicely, that is, it can extend a little bit past the
endpoints if needed. Again, see the exercises.
9.2. PATH INTEGRALS  73

Example 9.2.4: A graph of a continuously differentiable function  : [, ]   is a smooth
path. Define  : [, ]  2 by

                                                 () ,  () .

Then  () = 1,  () , which is never zero, and  [, ] is the graph of  .
    There are other ways of parametrizing the path. That is, there are different paths with

the same image. The function   (1 - ) + , takes the interval [0, 1] to [, ]. Define
 : [0, 1]  2 by

                                  () (1 - ) + ,  ((1 - ) + ) .

Then () =  - , ( - )  ((1 - ) + ) , which is never zero. As sets,  [0, 1] =
 [, ] = (, )  2 :   [, ] and  () =  , which is just the graph of  .

    The last example leads us to a definition.

Definition 9.2.5. Let  : [, ]   be a smooth path and  : [, ]  [, ] a continuously
differentiable bective function such that ()  0 for all   [, ]. Then the composition
   is called a smooth reparametrization of .

    Let  be a piecewise smooth path, and  a piecewise smooth bective function
with nonzero one-sided limits of . The composition    is called a piecewise smooth
reparametrization of .

    If  is strictly increasing, then  is said to preserve orientation. If  does not preserve
orientation, then  is said to reverse orientation.

    A reparametrization is another path for the same set. That is, (  ) [, ] =  [, ] .
    The conditions on the piecewise smooth  mean that there is some partition 0 =  <

                                                                                                                                                     

1 < 2 < · · · <  = , such that |[-1,] is continuously differentiable and |[-1,] ()  0
for all   [-1, ]. Since  is bective, it is either strictly increasing or strictly decreasing.
So either |[-1,] () > 0 for all  or |[-1,] () < 0 for all .
Proposition 9.2.6. If  : [, ]   is a piecewise smooth path, and    : [, ]   is a
piecewise smooth reparametrization, then    is a piecewise smooth path.

Proof. Assume that  preserves orientation, that is,  is strictly increasing. If  : [, ] 
[, ] gives a piecewise smooth reparametrization, then for some partition 0 =  < 1 <
2 < · · · <  = , the restriction |[-1,] is continuously differentiable with a positive
derivative.

    Let 0 =  < 1 < 2 < · · · <  =  be the partition from the definition of piecewise
smooth for  together with the points {(0), (1), (2), . . . , ( )}. Let  -1().
Then 0 =  < 1 < 2 < · · · <  =  is a partition that includes (is a refinement of) the
{0, 1, . . . ,  }. If   [-1, ], then ()  [-1, ] since (-1) = -1, () = , and
 is strictly increasing. Also |[-1,] is continuously differentiable, and |[-1,] is also
continuously differentiable. Then

                                (  )|[-1,]() = |[-1,] |[-1,]() .
74  CHAPTER 9. ONE-DIMENSIONAL INTEGRALS IN SEVERAL VARIABLES

The function (  )|[-1,] is therefore continuously differentiable and by the chain rule
                       (  )|[-1,] () = |[-1,]  () |[-1,] ()  0.

Consequently,    is a piecewise smooth path. The proof for orientation reversing  is

left as an exercise.                                                                  

    If two paths are simple and their images are the same, it is left as an exercise that there
exists a reparametrization. Here is where our assumption that  is never zero is important.

9.2.2 Path integral of a one-form

Definition 9.2.7. Let (1, 2, . . . , )   be our coordinates. Given  real-valued
continuous functions 1, 2, . . . ,  defined on a set   , we define a one-form to be an
object of the form

                                     = 1 1 + 2 2 + · · · +  .

We could represent  as a continuous function from  to , although it is better to think
of it as a different object.

Example 9.2.8:            (, ) 2 + 2 -  + 2 + 2  

is a one-form defined on 2 \ {(0, 0)}.

Definition 9.2.9. Let  : [, ]   be a smooth path and let

                           = 1 1 + 2 2 + · · · +  

be a one-form defined on the direct image  [, ] . Write  = (1, 2, . . . , ). Define:

                      
                            1 () 1() + 2 () 2() + · · · +  () () 
       
                        
      

                           
                =
                               () () .
                      
                          =1

To remember the definition note that  is (), so  becomes () .
    If  is piecewise smooth, take the corresponding partition 0 =  < 1 < 2 < . . . <  = ,

and assume the partition is minimal in the sense that  is not differentiable at 1, 2, . . . , -1.
As each |[-1,] is a smooth path, define

                                                            

                          |[0 ,1 ]  +   |[1 ,2 ]   + ··· +  |[-1, ]  .

                      
9.2. PATH INTEGRALS                                                         75

    The notation makes sense from the formula you remember from calculus, let us state it
somewhat informally: If () = (), then  = () .

    Paths can be cut up or concatenated. The proof is a direct application of the additivity

of the Riemann integral, and is left as an exercise. The proposition justifies why we defined

the integral over a piecewise smooth path in the way we did, and it justifies that we may as

well have taken any partition not just the minimal one in the definition.

Proposition 9.2.10. Let  : [, ]   be a piecewise smooth path, and   (, ). Define the
piecewise smooth paths  |[,] and  |[,]. Let  be a one-form defined on  [, ] .

Then                                

                                      =  + .

                                                 

Example 9.2.11: Let the one-form  and the path  : [0, 2]  2 be defined by
                  (, ) 2 + 2 -  + 2 + 2  , () cos(), sin() .

Then

                2         - sin()                  cos()

         =                  2        2 - sin() +    2             2 cos() 
               0 cos() + sin()                     cos() + sin()
                2

            =       1  = 2.

               0

Next, parametrize the same curve as  : [0, 1]  2 defined by ()    cos(2), sin(2) ,
that is,  is a smooth reparametrization of . Then

                       1           - sin(2)

                    =                   2          2 -2 sin(2)
                             cos(2) + sin(2)
                       0

                             + 2 2 cos(2) 2 cos(2) 
                                  cos(2) + sin(2)

                      1
                    = 2  = 2.

                          0

Finally, reparametrize with  : [0, 2]  2 as () cos(-), sin(-) . Then

          2         - sin(-)                       cos(-)

      =                 2            2 sin(-) +      2            2 - cos(-) 
         0 cos(-) + sin(-)                       cos(-) + sin(-)
          2

      = (-1)  = -2.

         0

The path  is an orientation preserving reparametrization of , and the integrals are the
same. The path  is an orientation reversing reparametrization of  and the integral is
minus the original. See Figure 9.5.
76  CHAPTER 9. ONE-DIMENSIONAL INTEGRALS IN SEVERAL VARIABLES

                               ( /2) =  (1/4) =  (3 /2)
                                                     ( /4) =
                                                              (1/8) =  (7 /4)

    ( ) = (1/2) = ( )                            (0) = (0) = (2 )
                                                 (2 ) = (1) = (0)

                                         (3 /2) = (3/4) = ( /2)

Figure 9.5: A circular path reparametrized in two different ways. The arrow indicates the
orientation of  and . The path  traverses the circle in the opposite direction.

    The previous example is not a fluke. The path integral does not depend on the
parametrization of the curve, the only thing that matters is the direction in which the curve
is traversed.

Proposition 9.2.12. Let  : [, ]   be a piecewise smooth path and    : [, ]   a
piecewise smooth reparametrization. Suppose  is a one-form defined on the set  [, ] . Then

                                        if  preserves orientation,
                            =           if  reverses orientation.
                                 
                               - 

Proof. Assume first that  and  are both smooth. Write  = 1 1 + 2 2 + · · · +  .
Suppose that  is orientation preserving. Use the change of variables formula for the

Riemann integral:

                               

                      =            () () 

                              =1

                                         () () 
                      =
                                    ()
                            
                              =1
                                                              
                      =        
                                        (  )()  =                      .
                                    ()

                              =1

If  is orientation reversing, it swaps the order of the limits on the integral and introduces

a minus sign. The details, along with finishing the proof for piecewise smooth paths, is left

as Exercise 9.2.4.                                                                         
9.2. PATH INTEGRALS                                                                  77

    Due to this proposition (and the exercises), if    is the image of a simple piecewise
smooth path  [, ] , then as long as we somehow indicate the orientation, that is, the
direction in which we traverse the curve, we can write

                                                     

                                                           ,

                                                                              

without mentioning the specific . Furthermore, for a simple closed path, it does not even
matter where we start the parametrization. See the exercises.

    Recall that simple means that  is one-to-one except perhaps at the endpoints, in
particular it is one-to-one when restricted to [, ). We may relax the condition that the
path is simple a little bit. For example, it is enough to suppose that  : [, ]   is
one-to-one except at finitely many points. See Exercise 9.2.14. But we cannot remove the
condition completely as is illustrated by the following example.

Example 9.2.13: Take  : [0, 2]  2 given by () cos(), sin() , and  : [0, 2]  2
by () cos(2), sin(2) . Notice that  [0, 2] =  [0, 2] ; we travel around the same
curve, the unit circle. But  goes around the unit circle once in the counter clockwise
direction, and  goes around the unit circle twice (in the same direction). See Figure 9.6.

                          ( /2) = ( /4) = (5 /4)
                                                    ( /4) = ( /8) = (9 /8)

  ( ) = ( /2) = (3 /2)             (0) = (0) = ( )
                                   (2 ) = (2 )

                                         (3 /2) = (3 /4) = (7 /4)
Figure 9.6: Circular path traversed once by  : [0, 2]  2 and twice by  : [0, 2]  2.

Compute               2
      

  -  +   =                - sin()  - sin() + cos() cos()  = 2,

                     0
                      2

  -  +   =                - sin(2) -2 sin(2) + cos() 2 cos()  = 4.

                     0

It is sometimes convenient to define a path integral over  : [, ]   that is not a

path. Define               
                                      () () 
                        
                             =1
                       
78  CHAPTER 9. ONE-DIMENSIONAL INTEGRALS IN SEVERAL VARIABLES

for every continuously differentiable . A case that comes up naturally is when  is
constant. Then  () = 0 for all , and  [, ] is a single point, which we regard as a
"curve" of length zero. Then,   = 0 for every .

9.2.3 Path integral of a function

Next, we integrate a function against the so-called arc-length measure . The geometric
picture we have in mind is the area under the graph of the function over a path. Imagine a
fence erected over  with height given by the function and the integral is the area of the
fence. See Figure 9.7.

Figure 9.7: A path  : [, ]  2 in the  -plane (bold curve), and a function  =  (, )
graphed above it in the  direction. The integral is the shaded area depicted.

Definition 9.2.14. Suppose  : [, ]   is a smooth path, and  is a continuous function

defined on the image  [, ] . Then define

                                        
                                               ()  () .
                   
                                          
                

To emphasize the variables we may use                            
                                         
                                                                      .
                                                () ()
                                                                   
                                                             

    The definition for a piecewise smooth path is similar as before and is left to the reader.

    The path integral of a function is also independent of the parametrization, and in this

case, the orientation does not matter.

Proposition 9.2.15. Let  : [, ]   be a piecewise smooth path and    : [, ]  

a piecewise smooth reparametrization. Suppose  is a continuous function defined on the set

 [, ] . Then                                                   

                                          =  .

                                                               
9.2. PATH INTEGRALS                                                             79

Proof. Suppose  is orientation preserving and that  and  are both smooth. Then

                               

                         =          ()  () 
                                     ()   () () 
                                     ()   () () 
                                    (  )() (  )() 
                            =
                                

                            =
                                

                            =
                              

                            =       .

If  is orientation reversing it swaps the order of the limits on the integral, but you also
have to introduce a minus sign in order to take  inside the norm. The details, along with
finishing the proof for piecewise smooth paths is left to the reader as Exercise 9.2.5. 

    As before, due to this proposition (and the exercises), if  is simple, it does not matter
which parametrization we use. Therefore, if  =  [, ] , we can simply write

                                                   

                                                           .

                                                                            

In this case we do not need to worry about orientation, either way we get the same integral.

Example 9.2.16: Let  (, ) . Let   2 be half of the unit circle for   0. We wish

to compute                          

                                          .

                                       

Parametrize the curve  via  : [-/2, /2]  2 defined as ()    cos(), sin() . Then
 () = - sin(), cos() , and

                      /2                                     /2

    =   =              -/2  cos()   - sin() 2 + cos() 2  =  -/2  cos()  = 2.

            

Definition 9.2.17. Suppose    is parametrized by a simple piecewise smooth path
 : [, ]  , that is,  [, ] = . We define the length by

                                             

                                ()        = .

                                             

If  is smooth,                         
                                () =  () .

                                             
80  CHAPTER 9. ONE-DIMENSIONAL INTEGRALS IN SEVERAL VARIABLES

This may be a good time to mention that it is common to write    ()  even if the
path is only piecewise smooth. That is because  () is defined and continuous at all but
finitely many points and is bounded, and so the integral exists.

Example 9.2.18: Let ,    be two points and write [, ] as the straight line segment
between the two points  and . Parametrize [, ] by () (1 - ) +   for  running
between 0 and 1. See Figure 9.8. Then  () =  - , and therefore

                                   1

                      [, ] =  [,]   =  -   =  - .

                                   0

The length of [, ] is the standard euclidean distance between  and , justifying the name.

                                                    =1
                                      [, ]

                                   =0

Figure 9.8: Straight path between  and  parametrized by (1 - ) +  .

    A simple piecewise smooth path  : [0, ]   is said to be an arc-length parametrization
if for all   [0, ], we have

                                                   [0, ] = .

If  is smooth, then                   

                         =  =   [0, ] =  () 

                     0                0

for all . The fundamental theorem of calculus then says that  () = 1 for all . Similarly
for piecewise smooth , we get  () = 1 for all  where the derivative exists. Think of
such a parametrization as moving around your curve at speed 1. If  : [0, ]   is an

                                                                                                                                            

arclength parametrization, it is common to use  as the variable as    = 0  () .

9.2.4 Exercises

Exercise 9.2.1: Show that if  : [, ]   is a piecewise smooth path as we defined it, then  is a
continuous function.

Exercise 9.2.2: Finish the proof of Proposition 9.2.6 for orientation reversing reparametrizations.
9.2. PATH INTEGRALS  81

Exercise 9.2.3: Prove Proposition 9.2.10.

Exercise 9.2.4: Finish the proof of Proposition 9.2.12 for
 a) orientation reversing reparametrizations, and
 b) piecewise smooth paths and reparametrizations.

Exercise 9.2.5: Finish the proof of Proposition 9.2.15 for
 a) orientation reversing reparametrizations, and
 b) piecewise smooth paths and reparametrizations.

Exercise 9.2.6: Suppose  : [, ]   is a piecewise smooth path, and  is a continuous function defined
on the image  [, ] . Provide a definition of   .

Exercise 9.2.7: Directly using the definitions compute:
 a) The arc-length of the unit square from Example 9.2.2 using the given parametrization.
 b) The arc-length of the unit circle using the parametrization  : [0, 1]  2, () cos(2), sin(2) .
 c) The arc-length of the unit circle using the parametrization  : [0, 2]  2, () cos(), sin() .
Note: Feel free to use what you know about sine and cosine from calculus.

Exercise 9.2.8: Suppose  : [0, 1]   is a smooth path, and  is a one-form defined on the image  [, ] .
For   [0, 1], let  : [0, ]   be defined as simply the restriction of  to [0, ]. Show that the function
()   is a continuously differentiable function on [0, 1].
Exercise 9.2.9: Suppose  : [, ]   is a smooth path. Show that there exists an  > 0 and a smooth
function  : ( - ,  + )   with () = () for all   [, ] and  ()  0 for all   ( - ,  + ).
That is, prove that a smooth path extends some small distance past the end points.

Exercise 9.2.10: Suppose  : [, ]   and  : [, ]   are piecewise smooth paths such that
  [, ] =  [, ] . Show that there exist finitely many points {1, 2, . . . , }  , such that the
sets -1 {1, 2, . . . , } and -1 {1, 2, . . . , } are partitions of [, ] and [, ] such that on every
subinterval the paths are smooth (that is, they are partitions as in the definition of piecewise smooth path).

Exercise 9.2.11:
 a) Suppose  : [, ]   and  : [, ]   are two smooth paths that are one-to-one and  [, ] =

      [, ] . Then there exists a smooth reparametrization  : [, ]  [, ] such that  =   .
     Hint 1: It is not hard to show  exists. The trick is to prove it is continuously differentiable with a nonzero
     derivative. Apply the implicit function theorem though it may at first seem the dimensions are wrong.
     Hint 2: Worry about derivative of  in (, ) first.
 b) Prove the same thing as part a, but now for simple closed paths with the further assumption that
     () = () = () = ().
 c) Prove parts a) and b) but for piecewise smooth paths, obtaining piecewise smooth reparametrizations.
     Hint: The trick is to find two partitions such that when restricted to a subinterval of the partition both
     paths have the same image and are smooth, see the exercise above.
82              CHAPTER 9. ONE-DIMENSIONAL INTEGRALS IN SEVERAL VARIABLES

Exercise 9.2.12: Suppose  : [, ]   and  : [, ]   are piecewise smooth paths with () = ().
Let  : [, ]   be defined by

                                               () () if   [, ],
                                                           () if   (, ].

Show that  is a piecewise smooth path, and that if  is a one-form defined on the curve given by , then

                                                

                                                  =  + .

                                                                      

Exercise 9.2.13: Suppose  : [, ]   and  : [, ]   are two simple closed piecewise smooth
paths. That is, () = () and () = () and the restrictions |[,) and |[,) are one-to-one. Suppose
 =  [, ] =  [, ] and  is a one-form defined on   . Show that either

                                                                               

                                      = , or                                = - .

                                                                               

                               
In particular, the notation   makes sense if we indicate the direction in which the integral is evaluated.
Hint: See previous three exercises.

Exercise 9.2.14: Suppose  : [, ]   and  : [, ]   are two piecewise smooth paths which are

one-to-one except at finitely many points. That is, there exist finite sets   [, ] and   [, ] such that

|[,]\ and |[,]\ are one-to-one. Suppose  =  [, ] =  [, ] and  is a one-form defined on
  . Show that either
                                                                               

                                      = , or                                = - .

                                                                               

In particular, the notation   makes sense if we indicate the direction in which the integral is evaluated.
Hint: Same hint as the last exercise.

Exercise 9.2.15: Define  : [0, 1]  2 by () 3 sin(1/),  32 sin(1/) -  cos(1/) 2 for   0 and

(0) = (0, 0). Show that

a)  is continuously differentiable on [0, 1].

b)  Show  that  there  exists  an  infinite  sequence  { }        in  [0,  1]  converging  to  0,  such  that    ( )  =  (0,  0).

                                                              =1

c) Show that the points () lie on the line  = 0 and such that the -coordinate of () alternates between

    positive and negative (if they do not alternate you only found a subsequence, you need to find them all).

d) Show that there is no piecewise smooth  whose image equals  [0, 1] . Hint: Look at part c) and show
    that  must be zero where it reaches the origin.

e) (Computer) If you know a plotting software that allows you to plot parametric curves, make a plot of
   the curve, but only for  in the range [0, 0.1] otherwise you will not see the behavior. In particular, you
   should notice that  [0, 1] has infinitely many "corners" near the origin.

Note: Feel free to use what you know about sine and cosine from calculus.
9.3. PATH INDEPENDENCE                                              83

9.3 Path independence

Note: 2 lectures

9.3.1 Path independent integrals

Let    be a set and  a one-form defined on . The integral of  is said to be path
independent if for every pair of points ,    and every pair of piecewise smooth paths
 : [, ]   and  : [, ]   such that () = () =  and () = () = , we have

                                  

                                            = .

                                             

In this case, we simply write              
                                       
                                              = .
                                    
                                                  

Not every one-form gives a path independent integral. Most do not.

Example 9.3.1: Let  : [0, 1]  2 be the path () (, 0) going from (0, 0) to (1, 0). Let
 : [0, 1]  2 be the path () , (1 - ) also going between the same points. Then

                               1               1

                      = 2()1()  = 0(1)  = 0,
                               0               0
                               1               1
                      = 2()1()  = (1 - )(1)  = 1 .
                               0               0       6

The integral of   is not path independent. In particular, (0,0)  (1,0)   does not make sense.

Definition 9.3.2. Let    be an open set and  :    a continuously differentiable
function. The one-form

                          1 1 +     2 2 + · · · +    

is called the total derivative of  .
    An open set    is said to be path connected if for every two points  and  in ,

there exists a piecewise smooth path starting at  and ending at .

    We leave as an exercise that every connected open set is path connected.

     Normally only a continuous path is used in this definition, but for open sets the two definitions are
equivalent. See the exercises.
84  CHAPTER 9. ONE-DIMENSIONAL INTEGRALS IN SEVERAL VARIABLES

Proposition 9.3.3. Let    be a path connected open set and  a one-form defined on . Then

   

   is path independent (for all ,   ) if and only if there exists a continuously differentiable
 :    such that  =  .

    In fact, if such an  exists, then for every pair of points ,   

                                           
                                                    =  () -  ().

                                                                

    In other words, if we fix   , then  () =  +     for some constant .

Proof. First suppose that the integral is path independent. Pick   . Since  is path
connected, there exists a path from  to every   . Define

                                

                         ()       .

Write  = 1 1 + 2 2 + · · · +  . We wish to show that for every  = 1, 2, . . . , ,
the partial derivative    exists and is equal to .

    Let  be an arbitrary standard basis vector, and  a nonzero real number. Compute

       ( + ) -  () = 1   +             = 1  + ,
                                          
                                - 
                                     

which follows by Proposition 9.2.10 and path independence as   +  =     +   + ,
because we pick a path from  to  +  that also happens to pass through , and then we
cut this path in two, see Figure 9.9.

                                   + 
                        

                                  

Figure 9.9: Using path independence in computing the partial derivative.

    Since  is open, suppose  is so small so that all points of distance || or less from 

are in . As the integral is path independent, pick the simplest path possible from  to
 + , that is, ()  +   for   [0, 1]. The path is in . Notice  () =  has only
one nonzero component and that is the th component, which is . Therefore,

    1  +  = 1   = 1  1  1 ( +  )  = ( +  ) .
          0
                                       0
9.3. PATH INDEPENDENCE                                                   85

We wish to take the limit as   0. The function  is continuous at . Given  > 0,
suppose  is small enough so that () - () <  whenever  -   ||. Thus,
( +  ) - () <  for all   [0, 1], and we estimate

              1                     1

                  ( +  )  - () =       ( +  ) - ()   .

              0                     0

That is,                    lim  ( + ) -  () = ().

                            0    

All partials of  exist and are equal to , which are continuous functions. Thus,  is
continuously differentiable, and furthermore  = .

    For the other direction, suppose a continuously differentiable  exists such that  = .

Take a smooth path  : [, ]   such that () =  and () = . Then

                                                         
               =      () 1 () +     () 2 () + · · · +    () () 
                   1             2                     
                =   () 

                   

              =  () -  ().

The value of the integral only depends on  and , not the path taken. Therefore the

integral is path independent. We leave checking this fact for a piecewise smooth path as

an exercise.                                                             

Path independence can be stated more neatly in terms of integrals over closed paths.

Proposition 9.3.4. Let    be a path connected open set and  a one-form defined on .

Then  =  for some continuously differentiable  :    if and only if
                

                =0    for every piecewise smooth closed path  : [, ]  .

              

Proof. Suppose  =  and let  be a piecewise smooth closed path. Since () = () for

a closed path, the previous proposition says
                                     

                                            =  () -  () = 0.

                                                        

                                                                                 
    Now suppose that for every piecewise smooth closed path ,   = 0. Let ,  be two
points in  and let  : [0, 1]   and  : [0, 1]   be two piecewise smooth paths with
(0) = (0) =  and (1) = (1) = . See Figure 9.10.
    Define  : [0, 2]   by

                            () ()      if   [0, 1],

                               (2 - ) if   (1, 2].
86  CHAPTER 9. ONE-DIMENSIONAL INTEGRALS IN SEVERAL VARIABLES

                                                       
                                                  

Figure 9.10: Two paths from  to .

This path is piecewise smooth. This is due to the fact that |[0,1]() = () and |[1,2]() =
(2 - ) (note especially (1) = (1) = (2 - 1)). It is also closed as (0) = (0) = (0) = (2).

So             

              0 =  =  - .

                                                                   

This follows first by Proposition 9.2.10, and then noticing that the second part is  traveled

backwards so that we get minus the  integral. Thus the integral of  on  is path

independent.                                                         

    However one states path independence, it is often a difficult criterion to check, you have
to check something "for all paths." There is a local criterion, a differential equation, that
guarantees path independence, or in other words it guarantees an antiderivative  whose
total derivative is the given one-form . Since the criterion is local, we generally only find
the function  locally. We can find the antiderivative in every so-called simply connected
domain, which informally is a connected open set where every path between two points
can be "continuously deformed" into any other path between those two points. But to
make matters simple, we prove the result for so-called star-shaped domains, which is often
good enough. As a bonus the proof in the star-shaped case constructs the antiderivative
explicitly. As balls are star-shaped we then have the result locally.

Definition 9.3.5. Let    be an open set and   . We say  is a star-shaped domain
with respect to  if for every other point   , the line segment [, ] is in , that is, if
(1 - ) +    for all   [0, 1]. If we say simply star-shaped, then  is star-shaped with
respect to some   . See Figure 9.11.

                                                         
                                                                 

Figure 9.11: A star-shaped domain with respect to .
9.3. PATH INDEPENDENCE                                 87

    Notice the difference between star-shaped and convex. Convex implies star-shaped, but
a star-shaped domain need not be convex.
Theorem 9.3.6 (Poincaré lemma). Let    be a star-shaped domain and  a continuously
differentiable one-form defined on . That is, if

                                     = 1 1 + 2 2 + · · · +   ,

then 1, 2, . . . ,  are continuously differentiable functions. Suppose that for every  and 

                                                     =   ,

then there exists a twice continuously differentiable function  :    such that  = .
    The condition on the derivatives of  is precisely the condition that the second partial

derivatives commute. That is, if  = , and  is twice continuously differentiable, then

                                       = 2  = 2  =  .
                                         

The condition is clearly necessary. The Poincaré lemma says that it is sufficient for a
star-shaped .

Proof. Suppose  is a star-shaped domain with respect to  = (1, 2, . . . , )  . Given
 = (1, 2, . . . , )  , define the path  : [0, 1]   as () (1-)+, so  () = -.

Let   1 

 ()        =                        (1 - ) +  ( - ) .

                        0 =1

We differentiate in  under the integral, which is allowed as everything, including the
partials, is continuous:

   1 () =   (1 - ) +  ( - ) +  (1 - ) +  
 0 =1 
 1  
=             (1 - ) +  ( - ) +  (1 - ) +  

      0  =1
 1 =   (1 - ) +  
0 

= ().

That is precisely what we wanted.                      
88    CHAPTER 9. ONE-DIMENSIONAL INTEGRALS IN SEVERAL VARIABLES

Example 9.3.7: Without some hypothesis on , the theorem is not true. On 2 \ {(0, 0)}, let
                                    (, ) 2 + 2 -  + 2 + 2  

Then                  -        2 - 2                       

                                             
                      2 + 2 = (2 + 2)2 =  2 + 2 .

However, there is no  : 2\{(0, 0)}   such that   = . In Example 9.2.11, we integrated
from (1, 0) to (1, 0) along the unit circle counterclockwise, that is, () = cos(), sin() for
  [0, 2], and we found the integral to be 2. We would have gotten 0 if the integral was
path independent, or in other words if there would exist an  such that  = .

9.3.2 Vector fields

A common object to integrate is a so-called vector field.

Definition 9.3.8. Let    be a set. A continuous function  :    is called a vector

field. Write  = (1, 2, . . . , ).
    Given a smooth path  : [, ]   with  [, ]   we define the path integral of

the vectorfield  as            
                                     () ·  () ,
                           · 
                                 
                         

where the dot in the definition is the standard dot product. The definition for a piecewise
smooth path is, again, done by integrating over each smooth interval and adding the
results.

    Unraveling the definition, we find that

                         

                        ·  = 1 1 + 2 2 + · · · +  .
                         

What we know about integration of one-forms carries over to the integration of vector
fields. For example, path independence for integration of vector fields is simply that

                                                  
                                                           · 

                                                                          

is path independent if and only if  =   , that is,  is the gradient of a function. The
function  is then called a potential for .

    A vector field  whose path integrals are path independent is called a conservative vector
field. The rationale for the naming is that such vector fields arise in physical systems where
a certain quantity, the energy, is conserved.
9.3. PATH INDEPENDENCE  89

9.3.3 Exercises

Exercise 9.3.1: Find an  : 2   such that   = 2+2  + 2+2 .

Exercise 9.3.2: Find an 2 : 2   such that there exists a continuously differentiable  : 2   for
which   =   + 2 .

Exercise 9.3.3: Finish the proof of Proposition 9.3.3, that is, we only proved the second direction for a smooth
path, not a piecewise smooth path.

Exercise 9.3.4: Show that if    is a star-shaped domain, then it is path connected.

Exercise 9.3.5: Show that  2 \ {(, )  2 :   0,  = 0} is a star-shaped domain and find all
points (0, 0)   such that  is star-shaped with respect to (0, 0).

Exercise 9.3.6: Suppose 1 and 2 are two open sets in  with 1  2 nonempty and path connected.
Suppose there exists an 1 : 1   and 2 : 2  , both twice continuously differentiable such that
 1 =  2 on 1  2. Then there exists a twice differentiable function  : 1  2   such that  =  1
on 1 and  =  2 on 2.

Exercise 9.3.7 (Hard): Let  : [, ]   be a simple nonclosed piecewise smooth path (so  is one-to-one).
Suppose  is a continuously differentiable one-form defined on some open set  with  [, ]   and
  =   for all  and . Prove that there exists an open set  with  [, ]     and a twice
continuously differentiable function  :    such that  = .
Hint 1:  [, ] is compact.
Hint 2: Show that you can cover the curve by finitely many balls in sequence so that the th ball only intersects
the ( - 1)th ball.
Hint 3: See previous exercise.

Exercise 9.3.8:

 a) Show that a connected open set    is path connected. Hint: Start with a point   , and let
        is the set of points that are reachable by a path from . Show that  and  \  are both open,
     and since  is nonempty (  ) it must be that  = .

b) Prove the converse, that is, an open path connected set    is connected. Hint: For contradiction
     assume there exist two open and disjoint nonempty open sets and then assume there is a piecewise smooth
    (and therefore continuous) path between a point in one to a point in the other.

Exercise 9.3.9: Usually path connectedness is defined using continuous paths rather than piecewise smooth
paths. Prove that for open subsets of  the definitions are equivalent, in other words prove:
Suppose    is open and for every ,   , there exists a continuous function  : [, ]   such that

() =  and () = . Then  is path connected, that is, there is a piecewise smooth path in  from  to .

If the definition of "path connected" is as in the next exercise, "open" would not be needed for this part.
90  CHAPTER 9. ONE-DIMENSIONAL INTEGRALS IN SEVERAL VARIABLES

Exercise 9.3.10 (Hard): Take

                              (, ) 2 + 2 -  + 2 + 2  

defined on 2 \ {(0, 0)}. Let  : [, ]  2 \ {(0, 0)} be a closed piecewise smooth path. Let   {(, ) 
2 :   0 and  = 0}. Suppose    [, ] is a finite set of  points. Prove that

                                                      

                                                              = 2

                                                                                 

for some integer  with | |  .
H int 1: First prove that for a path  that starts and end on  but does not intersect it otherwise, you find that
   is -2, 0, or 2.
Hint 2: You proved above that 2 \  is star-shaped.
Note: The number  is called the winding number it measures how many times does  wind around the

origin in the clockwise direction.
Chapter 10

Multivariable Integral

10.1 Riemann integral over rectangles

Note: 2-3 lectures
    As in chapter 5, we define the Riemann integral using the Darboux upper and lower

integrals. The ideas in this section are very similar to integration in one dimension. The
complication is mostly notational. The differences between one and several dimensions
will grow more pronounced in the sections following.

10.1.1 Rectangles and partitions

Definition 10.1.1. Let (1, 2, . . . , ) and (1, 2, . . . , ) be such that    for all .
The set [1, 1] × [2, 2] × · · · × [ , ] is called a closed rectangle. It is sometimes useful
to allow  = , and we think of [ , ] = {}. If  <  for all , then the set
(1, 1) × (2, 2) × · · · × ( , ) is called an open rectangle.

    For an open or closed rectangle  [1, 1] × [2, 2] × · · · × [ , ]   or 
(1, 1) × (2, 2) × · · · × ( , )  , we define the -dimensional volume by

                                  () (1 - 1)(2 - 2) · · · ( - ).
    A partition  of the closed rectangle  = [1, 1] × [2, 2] × · · · × [ , ] is given
by partitions 1, 2, . . . ,  of the intervals [1, 1], [2, 2], . . . , [ , ]. We write  =
(1, 2, . . . , ). That is, for every  = 1, 2, . . . ,  there is an integer  and a finite set of
numbers  = {,0, ,1, ,2, . . . , , } such that

                            = ,0 < ,1 < ,2 < · · · < ,-1 < , =  .
Picking a set of  integers 1, 2, . . . ,  where   {1, 2, . . . , } we get the subrectangle

                        [1,1-1 , 1,1 ] × [2,2-1 , 2,2 ] × · · · × [,-1 , , ].
92                          CHAPTER 10. MULTIVARIABLE INTEGRAL

    2,3

                1           2         3

    2,2

                6           5         4

    2,1

    2,0         7           8         9

           1,0         1,1     1,2                           1,3

Figure 10.1: Example partition of a rectangle in 2. The order of the subrectangles is not
important.

We order the subrectangles somehow and we say {1, 2, . . . ,  } are the subrectangles
corresponding to the partition  of , or more simply, subrectangles of . In other words,

we subdivided the original rectangle into many smaller subrectangles. See Figure 10.1.
    Let    be a closed rectangle and let  :    be a bounded function. Let  be a

partition of  with  subrectangles 1, 2, . . . ,  . Define

     inf    () :    ,           sup  () :    ,
    (,  )
                                      

               (),             (,  )        (  ).

           =1                         =1

We call (,  ) the lower Darboux sum and (,  ) the upper Darboux sum.

    To see the relationship to the  notation from the one-variable definition, note that
when

                     = [1,1-1 , 1,1 ] × [2,2-1 , 2,2 ] × · · · × [,-1 , , ],

then

    () = (1,1 - 1,1-1)(2,2 - 2,2-1) · · · (, - ,-1) = 1,1 2,2 · · · , .

It is not difficult to see (left to reader) that the subrectangles of  cover our original , and
their volumes sum to that of . That is,

                                                          

            = ,             and () = ().

                   =1                                    =1

    The indexing in the definition may be complicated, but fortunately we do not need to
go back directly to the definition often.
10.1. RIEMANN INTEGRAL OVER RECTANGLES                              93

Proposition 10.1.2. Suppose    is a closed rectangle and  :    is a bounded function.
Let ,    be such that for all   , we have    ()  . Then for every partition  of ,

                                  ()  (,  )  (,  )   ().            
Proof. Let  be a partition of . For all , we have       . Also
(). Therefore,                                                      =1 () =

                                    

 () =  () =  ()   () 

  =1  =1                            =1

                                        

        ()   () =  () =  (). 

      =1                            =1  =1

10.1.2 Upper and lower integrals

By Proposition 10.1.2, the sets of upper and lower Darboux sums are bounded sets, and
we can take their infima and suprema. As in one variable, we define the lower and upper
integrals.

Definition 10.1.3. Let  :    be a bounded function on a closed rectangle   .
Define

                                        
    sup (,  ) :  a partition of  ,          inf (,  ) :  a partition of  .

                                          

          
We call the lower Darboux integral and the upper Darboux integral.

    To refine a partition of a rectangle, we refine the underlying partitions of intervals.

Definition 10.1.4. Let    be a closed rectangle. Let  = (1, 2, . . . , ) and  =
(1, 2, . . . , ) be partitions of . We say  a refinement of  if, as sets,    for all
 = 1, 2, . . . , .

    If  is a refinement of , then subrectangles of  are unions of subrectangles of .
Simply put, in a refinement, we take the subrectangles of , and we cut them into smaller
subrectangles and call that . See Figure 10.2.

Proposition 10.1.5. Suppose    is a closed rectangle,  is a partition of , and  is a
refinement of . If  :    is bounded, then

                           (,  )  (,  ) and (,  )  (,  ).
94                                    CHAPTER 10. MULTIVARIABLE INTEGRAL

              ~2,4 2,3

                              1    2          3       4 5

              ~2,3 2,2 ~  2,2 18 12 13                6 7
                              19 14           15      8 9
              ~2,1 2,1
                              20 16           17      10 11

              ~2,0 2,0           1,1                  1,2      1,3
                        1,0      ~1,1 ~1,2            ~1,3 ~1,4 ~1,5
                        ~1,0

Figure 10.2: Example refinement of the partition from Figure 10.1. New "cuts" are marked in
dashed lines. The exact order of the new subrectangles does not matter.

Proof. We prove the first inequality, and the second follows similarly. Let 1, 2, . . . , 
be the subrectangles of  and 1, 2, . . . ,  be the subrectangles of . Let  be the set
of all indices  such that   . For example, in figures 10.1 and 10.2, 4 = {6, 7, 8, 9} as
4 = 6  7  8  9. Then,

                         = ,          () = ().

                                                      

    Let   inf  () :    , and          inf  () :  as usual. If   , then   .
Then

                                                           

(,  ) = () =                  ()                      () = () = (,  ). 

          =1            =1            =1                   =1

    The key point of this next proposition is that the lower Darboux integral is less than or
equal to the upper Darboux integral.

Proposition 10.1.6. Let    be a closed rectangle and  :    a bounded function. Let
,    be such that for all   , we have    ()  . Then

                                 

                         ()       ().                                 (10.1)
                                            

Proof. For every partition , via Proposition 10.1.2,

               ()  (,  )  (,  )   ().

Taking supremum of (,  ) and infimum of (,  ) over all partitions , we obtain the
first and the last inequality in (10.1).
10.1. RIEMANN INTEGRAL OVER RECTANGLES                                     95

    The key inequality in (10.1) is the middle one. Let  = (1, 2, . . . , ) and  =
(1, 2, . . . , ) be partitions of . Define  = (1, 2, . . . , ) by letting     for
every . Then  is a partition of , and  is a refinement of  and also a refinement of .

By Proposition 10.1.5, (,  )  (,  ) and (,  )  (,  ). Therefore,

                    (,  )  (,  )  (,  )  (,  ).

In other words, for two arbitrary partitions  and , we have (,  )  (,  ). Via
Proposition 1.2.7 from volume I, we obtain

           sup (,  ) :  a partition of   inf (,  ) :  a partition of  .

             
In other words,      .                                                     

10.1.3 The Riemann integral

We have all we need to define the Riemann integral in -dimensions over rectangles. As in
one dimension, the Riemann integral is only defined on a certain class of functions, called
the Riemann integrable functions.

Definition 10.1.7. Let    be a closed rectangle and  :    a bounded function

such that                         

                             ()  =  () .
                                  

Then  is said to be Riemann integrable, and we sometimes say simply integrable. We denote
the set of Riemann integrable functions on  by R(). For   R() define the Riemann

integral                        

                                   = .
                                        

When the variable    needs to be emphasized, we write

                                                        
                                                             () .
              () ,         (1, . . . , ) 1 · · ·  , or
                                                          
                        

If   2, then we usually say area instead of volume, and we write
                                                 

                                                         () .

                                                                         

    Proposition 10.1.6 immediately implies the following proposition.

Proposition 10.1.8. Let  :    be a Riemann integrable function on a closed rectangle
  . Let ,    be such that    ()   for all   . Then

                                                    
                                           ()     ().

                                                                              
96                                 CHAPTER 10. MULTIVARIABLE INTEGRAL

Example 10.1.9: A constant function is Riemann integrable. Proof: Suppose  () =  for

all   . Then                   

                          ()       ().
                                   

                                             
So  is integrable, and furthermore   =  ().

    The proofs of linearity and monotonicity are almost completely identical to the proofs
from one variable. We leave the next two propositions as exercises.

Proposition 10.1.10 (Linearity). Let    be a closed rectangle and let  and  be in R()
and   .

    (i)   is in R() and                        

                                    =  .
                                               

(ii)  +  is in R() and             

                               ( + ) =  + .
                                                 

Proposition 10.1.11 (Monotonicity). Let    be a closed rectangle, let  and  be in R(),
and suppose  ()  () for all   . Then

                               

                                   .
                                   

    Checking for integrability using the definition often involves the following technique,
as in the single variable case.

Proposition 10.1.12. Let    be a closed rectangle and  :    a bounded function. Then
  R() if and only if for every  > 0, there exists a partition  of  such that

                           (,  ) - (,  ) < .

Proof. First, if  is integrable, then the supremum of (,  ) and infimum of (,  ) over
all partitions  and  are equal and hence the infimum of (,  ) - (,  ) is zero. Taking

a common refinement  of  and  we find (,  ) - (,  )  (,  ) - (,  ). Hence
the infimum of (,  ) - (,  ) over all partitions  is zero, and so for every  > 0, there
must be some partition  such that (,  ) - (,  ) < .

    For the other direction, given an  > 0 find  such that (,  ) - (,  ) < .

                         

                            -   (,  ) - (,  ) < .
                           

                                                   
As      and the above holds for every  > 0, we conclude   =   and   R(). 
10.1. RIEMANN INTEGRAL OVER RECTANGLES                                 97

    Suppose  :    is a function and    is a closed rectangle. If the restriction  | is
integrable, then for simplicity we say  is integrable on , or   R(), and we write

                                              

                                                 |.

Proposition 10.1.13. Let    be a closed rectangle. If  :    is integrable and    is a
closed rectangle, then  is integrable on .

Proof. Given  > 0, find a partition  = (1, . . . , ) of  such that (,  ) - (,  ) < . By
making a refinement of  if necessary, assume that the endpoints of  are in . That is, if

 = [1, 1] × [2, 2] × · · · × [ , ], then  ,   . Let  = (1, . . . , ) be the partition of

 given by  =   [ , ]. Subrectangles of  are subrectangles of , that is,  is a union
of subrectangles of . Divide the subrectangles of  into two collections: Let 1, 2 . . . , 

be the subrectangles of  that are also subrectangles of  and let +1, . . . ,  be the rest.
See Figure 10.3. Let  and  be the infimum and supremum of  on  as usual. Then,

                                                        

 > (,  ) - (,  ) = ( - )() +                                 ( - )()

                                  =1                    =+1



 ( - )() = (,  |) - (,  |).

=1

Therefore,  | is integrable.                                                                        

    2,3

                              12           1         2       5

    2,2

                              11           3         4       6

    2,1

    2,0                       10           9         8       7
                                                1,2 1,3           1,4
         1,0                      1,1

Figure 10.3: A partition of a large rectangle , that also gives a partition of a smaller rectangle

(shaded and outlined)   . The subrectangles 1, 2, 3, 4 are the subrectangles of
 = {1,1, 1,2, 1,3}, {2,1, 2,2, 2,3} .
98                                                  CHAPTER 10. MULTIVARIABLE INTEGRAL

10.1.4 Integrals of continuous functions

Although we will prove a more general result later, it is useful to start with integrability
of continuous functions. To do so, we wish to measure the fineness of partitions. In one
variable, we measure the length of a subinterval. In several variables, we measure the sides
of a subrectangle. We say a rectangle  = [1, 1] × [2, 2] × · · · × [ , ] has longest side at
most  if  -    for all  = 1, 2, . . . , .
Proposition 10.1.14. If a rectangle    has longest side at most , then for all ,   ,

                                                 -    .

Proof.

                           -  = (1 - 1)2 + (2 - 2)2 + · · · + ( - )2

                                  (1 - 1)2 + (2 - 2)2+ · · · + ( - )2
                                 2 + 2 + · · · + 2 =  .                                         

Theorem 10.1.15. Let    be a closed rectangle. If  :    is continuous, then   R().

Proof. The proof is analogous to the one-variable proof with some complications. The set
 is a closed and bounded subset of , and hence compact. So  is uniformly continuous

by Theorem 7.5.11 from volume I. Let  > 0 be given. Find a  > 0 such that  -  < 
                                  
implies    |  ()  -   ()|    <
                                () .
       Let  be a partition of , where the longest side of every subrectangle is strictly less than
                                                                                            
    .  If  ,         for  a  subrectangle    of  ,  then,  by  the  proposition,    -    <      = .

Therefore,                       () -  ()  |  () -  ()| <  () .

As  is continuous on , which is compact,  attains a maximum and a minimum on this
subrectangle. Let  be a point where  attains the maximum and  be a point where 

attains the minimum. Then  () =  and  () =  in the notation from the definition

of the integral. Thus,                 -  =  () -  () <  () .

And so

                                                                    

                     (,  ) - (,  ) = () - ()

                                             =1                     =1

                                             

                                           = ( - )()

                                             =1

                                           <        

                                                       () = .
                                             () =1

Proposition 10.1.12 then says that   R().                                                       
10.1. RIEMANN INTEGRAL OVER RECTANGLES                                                           99

10.1.5 Integration of functions with compact support

Let    be an open set and  :    be a function. The support of  is the set

           supp(  ) {   :  ()  0},

where the closure is with respect to the subspace topology on . Taking the closure with

respect to the subspace topology is the same as {   :  ()  0}  , where the closure
is with respect to the ambient euclidean space . In particular, supp(  )  . The support
is the closure (in ) of the set of points where the function is nonzero. Its complement in
 is open. If    and  is not in the support of  , then  is constantly zero in a whole
neighborhood of .

    A function  is said to have compact support if supp(  ) is a compact set.

Example 10.1.16: The function  : 2  , defined by

     (, )  -(2 + 2 - 1)2                if 2 + 2  1,
           0                            else,

is continuous and its support is the closed unit disc (0, 1) = (, ) : 2 + 2  1 , which
is a compact set, so  has compact support. Note that the function is zero on the entire
-axis and on the unit circle, but all such points that lie in the closed unit disc are still
within the support as they are in the closure of points where  is nonzero. See Figure 10.4.

Figure 10.4: Function with compact support (left), the support is the closed unit disc (right).

    If   , then you must be careful to take the closure in . Consider the following
two examples.

Example 10.1.17: Let (0, 1)  2 be the unit disc. The function  : (0, 1)  , defined

by

     (, )  0              if 2 + 2 > 1/2,

           1/2 - 2 + 2 if 2 + 2  1/2,
100                        CHAPTER 10. MULTIVARIABLE INTEGRAL

is continuous on (0, 1) and its support is the smaller closed ball (0, 1/2). As that is a
compact set,  has compact support.

    The function  : (0, 1)  , defined by

                   (, )    0 if   0,
                            if  > 0,

is continuous on (0, 1), but its support is the set (, )  (0, 1) :   0 . In particular,
 is not compactly supported.

    We really only need to consider the case when  = . In light of Exercise 10.1.1,
which says every continuous function on an open    with compact support can be
extended to a continuous function with compact support on , considering  =  is not
an oversimplification.

Example 10.1.18: The continuous function  : (0, 1)   given by  (, )                1

                                                                       sin 1-2-2
does not have compact support; as  is not constantly zero on any neighborhood of every

point in (0, 1), the support is the entire disc (0, 1). The function does not extend as

above to a continuous function on 2. In fact, it is not difficult to show that  cannot be

extended in any way whatsoever to be continuous on all of 2 (the boundary of the disc is

the problem).

Proposition 10.1.19. Suppose  :    is a continuous function with compact support. If 

and  are closed rectangles such that supp(  )   and supp(  )  , then

                     

                            = .
                                       

Proof. As  is continuous,it is automatically integrable on the rectangles , , and   .
Then Exercise 10.1.7 says   =   =   .                                  

Because of this proposition, when  :    has compact support and is integrable

on a rectangle  containing the support, we write

                                                      

                      or                                .

                                                        
For example, if  is continuous and of compact support, then   exists.

10.1.6 Exercises

Exercise 10.1.1: Suppose    is open and  :    is continuous and of compact support. Show that
the function  :   , where

                                                 ()  () if    ,
                                                            0 otherwise,

is continuous.
10.1. RIEMANN INTEGRAL OVER RECTANGLES                                            101

Exercise 10.1.2: Prove Proposition 10.1.10.

Exercise 10.1.3: Suppose  is a closed rectangle with the length of one of the sides equal to 0. For every
bounded function  :   , show that   R() and   = 0.

Exercise 10.1.4: Suppose  is a closed rectangle with the length of one of the sides equal to 0, and suppose 
is a closed rectanglewith   . If  :    is a bounded function such that  () = 0 for    \ , show
that   R() and   = 0.

Exercise 10.1.5: Suppose  :    is such that  () 0 if   0 and  (0) 1. Show that  is
integrable on  [-1, 1] × [-1, 1] × · · · × [-1, 1] directly using the definition, and find   .

Exercise 10.1.6: Suppose  is a closed rectangle and  :    is a bounded function such that () = 0 if
   (the boundary of ). Let  be a closed rectangle. Show that   R() and

                                                        
                                                                = 0.

                                                                                   

Hint: Write  as a sum of functions as in Exercise 10.1.4.

Exercise 10.1.7: Suppose  and  are two closed rectangles with   . Suppose  :    is in R()
and  () = 0 for    \ . Show that   R() and

                                                 

                                                = .
                                                 

Do this in the following steps.

a) First do the proof assuming that furthermore  () = 0 whenever    \ .

b) Write  () = () + () where () = 0 whenever    \ , and () is zero except perhaps on .
Then show   =   = 0 (see Exercise 10.1.6).
  
c) Show   =   .

Exercise 10.1.8: Suppose    and    are two rectangles such that  =    is a rectangle,
and    is rectangle with one of the sides having length 0 (that is, (  ) = 0). Let  :    be a
function such that   R() and   R(). Show that   R() and

                                  

                                              =  + .
                                                   

Hint: See previous exercise.

Exercise 10.1.9: Prove a stronger version of Proposition 10.1.19. Suppose  :    is a function with
compact support but not necessarily continuous. Prove that if  is a closed rectangle such that supp(  )  
and  is integrable on , then for every other closed rectangle  with supp(  )  , the function  is integrable
on  and   =   . Hint: See Exercise 10.1.7.

Exercise 10.1.10: Suppose  and  are closed rectangles of . D efine  :    as  ()   1 if   ,
and  () 0 otherwise. Prove  is integrable on  and compute   . Hint: Consider   .
102                                                        CHAPTER 10. MULTIVARIABLE INTEGRAL

Exercise 10.1.11: Let  [0, 1] × [0, 1]  2.                 1 if  = ,
 a) Suppose  :    is defined by                            0 else.

                                                     (, )

                                             
     Show that   R() and compute   .
b) Suppose  :    is defined by

                             (, )                          1 if    or   ,
                                                           0 else.

Show that   R().

Exercise 10.1.12: Suppose  is a closed rectangle, and suppose  are closed rectangles such that    and
  +1 for all . Suppose  :    is bounded and   R() for all . Show that   R() and

                                                           

                            lim  =  .
                                                           

Exercise 10.1.13: Suppose  : [-1, 1] × [-1, 1]   is a Riemann integrable function such  () = -  (-).

Using the definition prove  

                                                  = 0.

                                   [-1,1]×[-1,1]
10.2. ITERATED INTEGRALS AND FUBINI THEOREM                             103

10.2 Iterated integrals and Fubini theorem

Note: 1-2 lectures

    The Riemann integral in several variables is hard to compute via the definition. For one-
dimensional Riemann integral, we have the fundamental theorem of calculus, which allows
computing many integrals without having to appeal to the definition of the integral. We
will rewrite a Riemann integral in several variables into several one-dimensional Riemann
integrals by iterating. However, if  : [0, 1]2   is a Riemann integrable function, it is not
immediately clear if the three expressions

                     1 1                               1 1

      ,                           (, )  , and                   (, )  

[0,1]2                  00                            00

are equal, or if the last two are even well-defined.

Example 10.2.1: Define

                         (, )    1 if  = 1/2 and   ,
                                 0 otherwise.

Then  is Riemann integrable on   [0, 1]2 and    = 0. Moreover, 0 0  1  1  (, )   = 0.
However,                         1

                                  (1/2, ) 

                                                                     0

does not exist, so strictly speaking, 0 0  1  1  (, )   does not make sense. See Figure 10.5.

                               2 1 1
  Figure 10.5: Left: [0, 1] with the line  = /2 marked dotted and 0  (, )  marked as gray

                                                                        1
  solid line for a generic . Center: Similar picture but 0  (, )  marked for some   1/2.
  Right: The three different rectangles in the partition used to integrate  in different grays.

    Proof: We start with integrability of  . Consider the partition of [0, 1]2 where the
partition in the  direction is {0, 1/2 - , 1/2 + , 1} and in the  direction {0, 1}. The
corresponding subrectangles are

 1 [0, 1/2 - ] × [0, 1], 2 [1/2 - , 1/2 + ] × [0, 1], 3 [1/2 + , 1] × [0, 1].
104                                       CHAPTER 10. MULTIVARIABLE INTEGRAL

We have 1 = 1 = 0, 2 = 0, 2 = 1, and 3 = 3 = 0. Therefore,

     (,  ) = 1(1) + 2(2) + 3(3) = 0(1/2 - ) + 0(2) + 0(1/2 - ) = 0,

and

     (,  ) = 1(1) + 2(2) + 3(3) = 0(1/2 - ) + 1(2) + 0(1/2 - ) = 2.

The upper and lower sums are arbitrarily close and the lower sum is always zero, so the
function is integrable and   = 0.

    For every fixed , the function that takes  to  (, ) is zero except perhaps at a
                                                                          1

single point  = 1/2. Such a function is integrable and 0  (, )  = 0. Therefore,
 1 1
 0 0  (, )   = 0. However, if  = 1/2, the function that takes  to  (1/2, ) is the
nonintegrable function that is 1 on the rationals and 0 on the irrationals. See Example 5.1.4
from volume I.

    We solve this problem of undefined inside integrals by using the upper and lower
integrals, which are always defined for any bounded function.

    Split the coordinates of + into two parts: Write the coordinates on + =  × 
as (, ) where    and   . For a function  (, ), write

                                     ()  (, )

when  is fixed and we want a function of . Write
                                                  ()  (, )

when  is fixed and we want a function of .

Theorem 10.2.2 (Fubini version A). Let ×  × be a closed rectangle and  : ×  
be integrable. The functions  :    and  :    defined by

                                                               

                           ()         and ()                       

are integrable on  and             

                                     = =    ×               .
                                        

In other words,

                                              

                 ×      =           (, )   =                     (, )  .

                                                               

If  is integrable for all , for example when  is continuous, we obtain the more familiar
                                        

                               ×     =       (, )  .

Named after the Italian mathematician Guido Fubini (1879-1943).
10.2. ITERATED INTEGRALS AND FUBINI THEOREM                                                                                                                    105

Proof. A partition of  ×  is a concatenation of a partition of  and a partition of .
                                                                  (, )                                                      1     2                 
That  is,      write   a   partition   of      ×         as                       =    (1       ,  2  ,  .  .  .  ,      ,     ,     ,  .  .  .  ,      ),  where
                                            (1     2                                                                                                  
  =  (1, 2, . . . , )                  =        ,     ,  .  .  .  ,       )                                                       
                              and                                            are     partitions                of        and         respectively.             Let
                                                                          1 ,     2 ,                                                                       .
1,  2,   .  .  .  ,    be  the  subrectangles       of         and                     .  .  .  ,        be       the    subrectangles              of         The
                              )                                                                      
subrectangles        of   (,      are       ×       where            1                    and      1                     .
                                                  

    Let

                                               ,                     inf  (, ).
                                                               (,) ×

Notice   that        (    ×  )    =   (  ) ( )        and            hence

                                

                      (, ),                                                  )                                           (

                                     =             ,     (              ×         =                               ,            )  ().

                                       =1 =1                                           =1          =1

Define

                                       ()                inf  (, ) = inf ().
                                                                                       

For   , we have ,  (), and therefore,

                                                                                                               

                             , ()                           (     )    (       )  =  (,            )                    = ().
                                                                             
                                                                                                                     
                       =1                      =1

The inequality holds for all   , and so

                                                

                                                   , ()  inf ().

                                                                           
                                               =1

We obtain                                               

                                 (, ),                               inf () () = (, ).

                                                       =1            

    Similarly,  (, ),  )  (, ), and the proof of this inequality is left as an exercise.
Putting the two inequalities together with the fact that ()  () for all ,

                        (, ),   (, )  (, )  (, )   (, ),  .

Since  is integrable, it must be that  is integrable as
                           (, ) - (, )   (, ),  -  (, ),  ,

and we can make the right-hand side arbitrarily small. As for any partition we have
 (, ),   (, )   (, ),  , we have   = ×  .

    Likewise,
                     (, ),   (, )  (, )  (, )   (, ),  ,
106                                         CHAPTER 10. MULTIVARIABLE INTEGRAL

and hence      (, ) - (, )   (, ),  -  (, ),  .

A s  is integrable, so is . Moreover,  (, ),          (, )   (, ),        implies
   = ×  .                                                                        

    We can also do the iterated integration in the opposite order. The proof of this version is
almost identical to version A (or follows quickly from version A). We leave it as an exercise.

Theorem 10.2.3 (Fubini version B). Let ×   × be a closed rectangle and  : ×  
be integrable. The functions  :    and  :    defined by

                                                          

                           ()           and ()              

are integrable on  and             

                                        = =   ×      .
                                          

That is,                                         

             ×          =           (, )   =               (, )  .

                                                        

Next suppose  and   are integrable. For example, suppose  is continuous. By

putting the two versions together we obtain the familiar

                                                 

                        ×  =           (, )   =            (, )  .

   Often the Fubini theorem is stated in two dimensions for a continuous function
 :    on a rectangle  = [, ] × [, ]. Then the Fubini theorem states that

                                                

                        =           (, )   =              (, )  .

                                                     

The Fubini theorem is commonly thought of as the theorem that allows us to swap the

order of iterated integrals, although there are many variations on Fubini, and we have seen

but two of them.
    Repeatedly applying Fubini theorem gets us the following corollary: Let  [1, 1] ×

[2, 2] × · · · × [ , ]   be a closed rectangle and let  :    be continuous. Then

                         1  2  

             =                   ···       (1, 2, . . . , )  -1 · · · 1.

                        1 2             

    We may switch the order of integration to any order we please. We may relax the
continuity requirement by making sure that all the intermediate functions are integrable,
or by using upper or lower integrals appropriately.
10.2. ITERATED INTEGRALS AND FUBINI THEOREM                                               107

10.2.1 Exercises

Exercise 10.2.1: Compute 0 -1  1  1    in a simple way.

Exercise 10.2.2: Prove the assertion  (, ),   (, ) from the proof of Theorem 10.2.2.

Exercise 10.2.3 (Easy): Prove Theorem 10.2.3.

Exercise 10.2.4: Let  [, ] × [, ] and  (, ) is an integrable function on  such that for every

fixed , the function that takes  to  (, ) is zero except at finitely many points. Show
                                                        

                                                  = 0.

                                               

Exercise 10.2.5: Let  [, ] × [, ] and  (, ) ()() for continuous functions  : [, ]  

and  : [, ]  . Prove                                  
                          =                                .

                                                        

Exercise 10.2.6: Compute (using calculus)

 1  1 2 - 2                                                1  1 2 - 2
0 0 (2 + 2)2   and                                        0 0 (2 + 2)2  .

You will need to interpret the integrals as improper, that is, the limit of   1 as   0+.

Exercise 10.2.7: Suppose  (, ) () where  : [, ]   is Riemann integrable. Show that  is

Riemann integrable for every  = [, ] × [, ] and

                                                      

                                              = ( - ) .
                                                        

Exercise 10.2.8: Define  : [-1, 1] × [0, 1]   by

                       (, )                        if   ,
                                                  0 else.

a) Show 0 -1  1  1  (, )   exists, but -1 0  1  1  (, )   does not.
b) Compute -1 0  1  1  (, )   and -1 0  1  1  (, )  .

c) Show  is not Riemann integrable on [-1, 1] × [0, 1] (use Fubini).

Exercise 10.2.9: Define  : [0, 1] × [0, 1]   by

 (, )                 1/ if   ,   , and  = / in lowest terms,
                      0 else.

 a) Show  is Riemann integrable on [0, 1] × [0, 1].

b) Find 0  1  (, )  and 0  1  (, )  for all   [0, 1], and show they are unequal for all   .
 c) Show 0 0  1  1  (, )   exists, but 0 0  1  1  (, )   does not.
Note: By Fubini, 0 0  1  1  (, )   and 0 0  1  1  (, )   do exist and equal the integral of  on .
108                                                 CHAPTER 10. MULTIVARIABLE INTEGRAL

10.3 Outer measure and null sets

Note: 2 lectures

10.3.1 Outer measure and null sets

Before we characterize all Riemann integrable functions, we need to make a slight detour.
We introduce a way of measuring the size of sets in .

Definition 10.3.1. Define the outer measure of a set    as

                                       ()                  

                                                    inf (),

                                                           =1

where  the     infimum  is   taken  over  all  sequences        {   }       of  open  rectangles  such    that

           ,                                                            =1                .
       =1
               and  we  are  allowing  both    the  sum  and    the   infimum   to    be     See  Figure  10.6.
In particular,  is of measure zero or a null set if () = 0.

                                       2

                                    1             

                                               3

Figure 10.6: Outer measure construction, in this case   1  2  3  · · · , so () 
(1) + (2) + (3) + · · · .

    An immediate consequence (Exercise 10.3.2) of the definition is that if   , then
()  (). It is also not difficult to show (Exercise 10.3.13) that we obtain the same
number () if we also allow both finite and infinite sequences of rectangles in the

definition. It is not enough, however, to allow only finite sequences.
    The theory of measures on  is a very complicated subject. We will only require

measure-zero sets and so we focus on these. A set  is of measure zero if for every  > 0,
                                                    {   }
there  exists  a  sequence  of  open  rectangles                such  that
                                                            =1

                                                                                                  (10.2)

                                   and                              () < .

                                        =1                      =1

If  is of measure zero and   , then  is of measure zero. We can use the same exact
rectangles.
10.3. OUTER MEASURE AND NULL SETS                                                                                    109

    It is sometimes more convenient to use balls instead of rectangles. Furthermore, we can
choose balls no bigger than a fixed radius.

Proposition 10.3.2. Let  > 0 be given. A set    is of measure zero if and only if for every
                                                       {} ,
  >  0,  there  exists  a  sequence   of  open  balls              where  the  radius  of    is    <  ,   and  such  that
                                                               =1

                                                                         

                                          and                                   <  .

                                               =1                            

                                                                         =1

  Note    that  the     "volume"      of        is  proportional   to    .

                                                                          

Proof. If  is a closed cube (rectangle with all sides equal) of side , then  is contained in a
                                                                                                                    
closed ball of radius   by Proposition 10.1.14, and hence in an open ball of radius 2  .

  Suppose  is a rectangle of positive volume. Let  > 0 be a number less than the smallest
                               
side of  and such that 2   < . If each side of  is an integer multiple of , then  is
                                                                                                   
contained in a union of closed cubes 1, 2, . . . ,  of side  such that                             =1     ()   =     ().

So suppose the sides of  are not integer multiples of . Consider a side of length ( + ),

for an integer  and 0   < 1. As  is less than the smallest side,   1, and so ( + )  2 .

Increasing this side to 2 , and similarly increasing every side of , we obtain a new larger
rectangle of volume at most 2 times larger, whose sides are multiples of . See Figure 10.7.

Thus  is contained in a union of closed cubes 1, 2, . . . ,  of side  such that

                                                  

                                                     ()  2().

                                                 =1

                                                  = 2

                                                                       

                                                     2  = 4

  Figure 10.7: Covering a rectangle by cubes of total size at most 2().

  So     suppose     that      is  a  null  set     and  there  exist    open   rectangles    {   }       whose  union

                                                                                    {   }             =1            
                                                                                                      
contains        and  such    that  (10.2)   is  true.    Choose    closed    cubes          =1  with      of  side    as
                                                 {   }
above    that   cover   all  the  rectangles                 and   so  that
                                                         =1

                                                                       

                                        =           ()  2                () < 2.

                                                                   =1

                                   =1           =1
110                                            CHAPTER 10. MULTIVARIABLE INTEGRAL

Covering each  with a ball  of radius  = 2  < , we obtain

                                                          
                                     
                                    = (2 )  < (4 ) .

                               =1          =1

As           and (4)  can be arbitrarily small, the forward direction

follows.                                                                           < ,

    For the other direction, suppose  is covered by balls  of radii , such that  =1 

as in the statement of the proposition. Each  is contained in an open cube  of side 2.
    ()       (2  )     2   .
So        =         =          Therefore,
                             

                                                          

                       and                         ()         2 < 2 .            

                                                                    

                       =1                      =1         =1

    The definition of outer measure (not just null sets) could have been done with open
balls as well. We leave this generalization to the reader.

10.3.2 Examples and basic properties

Example 10.3.3: The set    of points with rational coordinates is of measure zero.
    Proof: The set  is countable, so write it as a sequence 1, 2, . . .. For each , find an

open rectangle  with    and () < 2-. Then

                                                              

                          and                         () < 2- = .

                                  =1             =1         =1

    The example points to a more general result.
Proposition 10.3.4. A countable union of measure zero sets is of measure zero.

Proof. Suppose                                     

                                            = ,

                                                   =1

where  are all measure zero sets. Let  > 0 be given. For each , there exists a sequence
of open rectangles {,}=1 such that

                                                       

                         , and                            (,) < 2- .

                                =1                    =1

Then                                           

                                                      ,.

                                               =1 =1
10.3. OUTER MEASURE AND NULL SETS                                                            111

All (,) are nonnegative, so the sum over all  and  can be done by summing first over
the  and then over the , see Exercise 2.6.15 in volume I. In particular, as

                                             

                                   (,) < 2-  = .                                             

                            =1 =1            =1

The next example is not just interesting, it will be useful later.

Example 10.3.5: Suppose   ,  = 1, 2, . . . , , and   . Then              {   :  = } is
of measure zero. Note that if   2, then  is uncountable.

    Proof: First fix    and consider

                      :  =  and ||   for all    .

Given any  > 0 define the open rectangle
                    :  -  <  <  +  and || <  + 1 for all    .

Clearly,   . Furthermore,

                               () = 2 2( + 1) -1.

As  is fixed, () can be arbitrarily small by picking  small enough. So  is of measure
zero.

    Next 
                                                     = 

                                                                                =1

and a countable union of measure zero sets is of measure zero.

Example 10.3.6: If  < , then  [, ] =  - .
    Proof: In , open rectangles are open intervals. Since [, ]  ( - ,  + ) for all  > 0,

we have  [, ]   - .
    The other inequality is harder. Suppose ( , ) =1  are open intervals such that

                                              

                               [, ]  (, ).

                                              =1

We wish to bound         -  )  from  below.  Since  [,  ]  is  compact,  finitely  many  of  the

                  =1 ( 
open intervals still cover [, ]. As throwing out some of the intervals only makes the

sum smaller, we only need to consider the finite number of intervals covering [, ]. If

( , )  ( , ), then we throw out ( , ) as well. The intervals that are left have distinct
                                                                                   
left endpoints, and whenever  <  < , then  < . Therefore, [, ] 
                                                                                   =1( , )
for some , and we assume that the intervals are sorted such that 1 < 2 < · · · < . As
112                                                    CHAPTER 10. MULTIVARIABLE INTEGRAL

(2, 2) is not contained in (1, 1), since  > 2 for all  > 2, and since the intervals must
contain every point in [, ], we find that 2 < 1, or in other words 1 < 2 < 1 < 2.
Similarly  < +1 <  < +1 for all . Furthermore, 1 <  and  > . See Figure 10.8 for
a sample configuration. As  -  > +1 - , we obtain

                                     -1

                         ( - )  (+1 - ) + ( - ) =  - 1 >  - .

                     =1              =1

So  [, ]   - .

                     1  2 1 3 4 2 3                                             4

Figure 10.8: Open intervals covering [, ] which satisfy  < +1 <  < +1 for all .

Proposition 10.3.7. Suppose    is a compact set of measure zero. Then for every  > 0,
there exist finitely many open rectangles 1, 2, . . . ,  such that

                           1  2  · · ·   and                      

                                                                     () < .

                                                                 =1

Moreover, for every  > 0 and every  > 0, there exist finitely many open balls 1, 2, . . . ,  of
radii 1, 2, . . . ,  <  such that

                                                                     

                           1  2  · · ·   and                                <  .

                                                                         

                                                                     =1

Proof.  As   is  of  measure  zero,  there  exists  a  sequence  of  open  rectangles  {    }     such that

                                                                                              =1

                                                           

                                 and                           () < .

                                      =1                   =1

By compactness, there are finitely many of these rectangles that still contain . That is,
there is some  such that   1  2  · · ·  . Hence

                                                       

                                         ()  () < .

                                     =1                =1

The proof that we can choose balls instead of rectangles is left as an exercise.                  
10.3. OUTER MEASURE AND NULL SETS                                              113

Example 10.3.8: So that the reader is not under the impression that there are only few

measure zero sets and that these sets are uncomplicated, here is an uncountable, compact,
measure zero subset of [0, 1], which contains no intervals. Any   [0, 1] can be expanded

in ternary:                  

                      = 3- ,              where  = 0, 1, or 2.

                            =1

See §1.5 in volume I, in particular Exercise 1.5.4. Define the Cantor set  as

                                                    

                [0, 1] :  = 3- , where  = 0 or  = 2 for all  .

                                                   =1

That is,  is in  if it has a ternary expansion in only 0s and 2s. If  has two expansions, as
long as one of them does not have any 1s, then  is in . Define 0 [0, 1] and

                                         

   [0, 1] :  = 3- , where  = 0 or  = 2 for all  = 1, 2, . . . ,  .

                                        =1

Clearly,                                        

                                        = .

                                               =1

See Figure 10.9.
    We leave as an exercise to prove:

(i) Each  is a finite union of closed intervals. It is obtained by taking -1, and from
    each closed interval removing the "middle third."

(ii) Each  is closed, and so  is closed.

              2
(iii)  () = 1 - =1 3+1 .

(iv) Hence, () = 0.

(v) The set  is in one-to-one correspondence with [0, 1], in other words,  is uncountable.

      0
      1
      2
      3
      4

Figure 10.9: Cantor set construction.
114                              CHAPTER 10. MULTIVARIABLE INTEGRAL

10.3.3 Images of null sets under differentiable functions

Before we look at images of measure zero sets, let us see what a continuously differentiable
function does to a ball.
Lemma 10.3.9. Suppose    is an open set,    is an open (resp. closed) ball of radius at
most ,  :    is continuously differentiable, and suppose   ()   for all   . Then
 ()  , where  is an open (resp. closed) ball of radius at most .

Proof. Suppose  is open. As the ball  is convex, Proposition 8.4.2 says that   () -  () 
 -  for all ,   . So if  -  < , then   () -  () < . In other words, if

 = (, ), then  ()    (),  . If  is closed, then (, ) = . As  is continuous,

 () =  (, )   (, )    (),  , as  ()   () for any set .                                         

    The image of a measure zero set using a continuous map is not necessarily a measure
zero set, although this takes some work to show (see the exercises). However, if the
mapping is continuously differentiable, then it cannot "stretch" the set that much.

Proposition 10.3.10. Suppose    is open and  :    is continuously differentiable. If
   is a measure zero set, then  () is measure zero.

Proof. We prove the proposition for a compact  and leave the general case as an exercise.
Suppose  is compact and of measure zero. First, we will replace  by a smaller open set to
make   () bounded. At each point    pick an open ball (, ) such that the closed
ball (, )  . By compactness, we only need to take finitely many points 1, 2, . . . , 
to cover  with the balls ( ,  ). Define

                                               

                       (,  ),                      ( ,  ).

                   =1                          =1

We have       . The set , being a finite union of compact sets, is compact. The
function that takes  to   () is continuous, and therefore there exists an  > 0 such
that   ()   for all   . So without loss of generality, we may replace  by  and
from now on suppose that   ()   for all   .

At each   , take the maximum radius  such that (, )   (we may assume
  ). Let                                                                  {   }
           inf  .  We  want  to  show  that    >  0.  Take   a  sequence                 in    so
                                                                                 =1
that   . As  is compact, we can pick the sequence to be convergent to some   .

Once  -  < 2 , then  > 2 by the triangle inequality. Thus,  > 0.
    Given  > 0, there exist balls 1, 2, . . . ,  of radii 1, 2, . . . ,  < /2 such that

                                                      

             1  2  · · ·   and                               <  .

                                                          

                                                      =1
10.3. OUTER MEASURE AND NULL SETS                                                                                 115

We can assume that each ball contains a point of  and so the balls are contained in .
         1 ,  2 ,              
Suppose            .  .  .  ,      are  the  balls  of  radius   1 ,   2 ,  .  .  .  ,      from  Lemma  10.3.9,  such
                                 
that   ()         for       all  .  Then,

              ()   (1)   (2)  · · ·   ()     and () <  . 
                       1  2  · · ·  
                                                                                        =1

10.3.4 Exercises

Exercise 10.3.1: Finish the proof of Proposition 10.3.7: Show that you can use balls instead of rectangles.

Exercise 10.3.2: If   , then ()  ().

Exercise 10.3.3: Suppose    is a set such that for every  > 0, there exists a set  such that   
and ()  . Prove that  is a measure zero set.
Exercise 10.3.4: Show that if    is a closed rectangle, then () = ().
Exercise 10.3.5: The closure of a measure zero set can be quite large. Find an example set    that is of
measure zero, but whose closure  = .

Exercise 10.3.6: Prove the general case of Proposition 10.3.10 without using compactness:

 a) Mimic the proof to prove that the proposition holds if  is relatively compact; a set    is relatively
     compact if the closure of  in the subspace topology on  is compact, or in other words if there exists a
     compact set  with    and   .
    Hint: The bound on the size of the derivative still holds, but you need to use countably many balls in the
     second part of the proof. Be careful as the closure of  need no longer be measure zero.

 b) Now prove it for every null set .
    Hint: First show that {   :  -   1/ for all    and   } is compact for every  > 0.

Exercise 10.3.7: Let    be an open set and let  :    be a continuously differentiable function.
Let  (, )   ×  :  =  () be the graph of  . Show that  is of measure zero.

Exercise 10.3.8: Given a closed rectangle   , show that for every  > 0, there exists a number  > 0
and finitely many open cubes 1, 2, . . . ,  of side  such that   1  2  · · ·   and

                                                                       

                                                       ()  () + .

                                                                      =1

Exercise 10.3.9: Show that there exists a number  = (, , ) depending only on ,  and  such the
following holds: Given (, )   and  > 0, there exist  open balls 1, 2, . . . ,  of radius at most 
such that (, )  1  2  · · ·  . Note that you can find  that only depends on  and the ratio /.

Exercise 10.3.10 (Challenging): Prove the statements of Example 10.3.8. That is, prove:

a) Each  is a finite union of closed intervals, and so  is closed.

                       2
b)  ( ) = 1 - =1 3+1 .

c) () = 0.

d) The set  is in one-to-one correspondence with [0, 1].
116                                           CHAPTER 10. MULTIVARIABLE INTEGRAL

Exercise 10.3.11: Prove that the Cantor set of Example 10.3.8 contains no interval. That is, whenever  < ,
there exists a point    such that  <  < .
Note a consequence of this statement. While every open set in  is a countable disjoint union of intervals, a

closed set (despite being the complement of an open set) need not be a countable union of intervals or points.

Exercise 10.3.12 (Challenging): Let us construct the so-called Cantor function or the Devil's staircase.

Let  be the Cantor set and let  be as in Example 10.3.8. Write   [0, 1] in ternary representation
 = =1 3-. If   1 for all , then let           
                                              2  for  all  .  Otherwise,  let    be  the  smallest  integer  such

that  = 1. Let      if    <  ,       1, and      0 if  > . Define
                 2

                                     ()        

                                                   2- .

                                              =1

a) Prove that  is continuous and increasing (see Figure 10.9).
b) Prove that for   ,  is differentiable at  and () = 0. (Notice that  exists and is zero except for a

    set of measure zero, yet the function manages to climb from 0 to 1.)

c) Define  : [0, 1]  [0, 2] by () () + . Show that  is continuous, strictly increasing, and
    bective.

d) Prove that while () = 0,  ()  0. That is, continuous functions need not take measure zero
    sets to measure zero sets. Hint:  ([0, 1] \ ) = 1, but  [0, 2] = 2.

                             1

                             0.5

                             0                   0.5

                                  0                                1

Figure 10.10: Cantor function or Devil's staircase (the function  from the exercise).

Exercise 10.3.13: Prove that we obtain the same outer measure if we allow both finite and infinite sequences in
the definition. That is, define () inf  () where the infimum is taken over all countable (finite or
infinite) sets of open rectangles {} such that    . Prove that for every   , () = ().
Exercise 10.3.14: Prove that for any two subsets ,   , we have (  )  () + ().

Exercise 10.3.15: Suppose ,    are such that () = 0. Prove that (  ) = ().

Exercise 10.3.16 (Challenging): Suppose 1, 2, . . . ,  are pairwise disjoint open rectangles. Prove that
(1  2  · · ·  ) = (1) + (2) + · · · + (). Hint: Some of the exercises above may prove
very useful.
10.4. THE SET OF RIEMANN INTEGRABLE FUNCTIONS                  117

10.4 The set of Riemann integrable functions

Note: 1 lecture

10.4.1 Oscillation and continuity

Consider    and  :   . Instead of just saying that  is or is not continuous at a
point   , we want to quantify how discontinuous is  at . For every  > 0, define the
oscillation of  on the -ball in subspace topology, (, ) =  (, )  , as

(  , , )         sup  () - inf  () = sup  (1) -  (2) .
                                  ( ,)
                  ( ,)                  1 , 2  ( ,)

That is, (  , , ) is the length of the smallest interval that contains the image  (, ) .
The definition makes sense for unbounded functions, where the oscillation can be ,
although we will mainly consider bounded functions. Clearly (  , , )  0 and (  , , ) 
(  , , ) whenever  < . Therefore, the limit as   0 from the right exists, and we
define the oscillation of  at  as

                 (  , )          lim (  , , ) = inf (  , , ).
                                 0+     >0

    We will prove that function is continuous at  if and only if (  , ) = 0. Fox example,
if  :    is the Dirichlet function where  () = 1 if    and  () = 0 otherwise,
then (  , ) = 1 for every , as any interval contains both rational and irrational numbers.
Accordingly,  is not continuous at any . For another example, which is perhaps the origin
of the terminology, let  :    be given by () = sin(1/) for   0 and (0) = 0, see
Figure 10.11. Then at the discontinuity at  = 0, we find (, 0) = 2, as in any neighborhood
of 0, the function takes both values 1 and -1. For all   0, the function is continuous and
so, as we will see, (, ) = 0.

Figure 10.11: Graph of sin(1/).
118                                     CHAPTER 10. MULTIVARIABLE INTEGRAL

Proposition 10.4.1. A function  :    is continuous at    if and only if (  , ) = 0.

Proof. First suppose that  is continuous at   . Given  > 0, there exists a  > 0 such
that for   (, ), we have |  () -  ()| < . Therefore, if 1, 2  (, ), then

      (1) -  (2) =  (1) -  () -  (2) -  () <  +  = 2.

Take the supremum over 1 and 2 to find

     (  , , ) = sup  (1) -  (2)  2.

                          1 , 2  ( ,)

As (,  )  (  , , )  2, and  > 0 was arbitrary, (,  ) = 0.
    On the other hand, suppose (,  ) = 0. Given  > 0, find a  > 0 such that (  , , ) < .

If   (, ), then

     |  () -  ()|  sup  (1) -  (2) = (  , , ) < .                                    

                1 , 2  ( ,)

Proposition 10.4.2. Let    be closed,  :   , and  > 0. The set    : (  , )  
is closed.

Proof. Equivalently, we want to show that     : (  , ) <  is open in the
subspace topology. Consider   . As inf>0 (  , , ) < , find a  > 0 such that

                (  , , ) < .

Take any   (, /2). Notice that (, /2)  (, ). Therefore,

(  , , /2) = sup  (1) -  (2)  sup  (1) -  (2) = (  , , ) < .
     1,2 (,/2)                          1 , 2  ( ,)

So (  , ) <  as well. As this is true for all   (, /2), we get that  is open in the
subspace topology, and  \  is closed as claimed.
                                                                                     

10.4.2 The set of Riemann integrable functions

We have seen that continuous functions are Riemann integrable, but we also know that
certain kinds of discontinuities are allowed. It turns out that as long as the discontinuities
happen on a set of measure zero, the function is integrable, and vice versa.

Theorem 10.4.3 (Riemann-Lebesgue or Lebesgue-Vitali). Let    be a closed rectangle
and  :    bounded. Then  is Riemann integrable if and only if the set of discontinuities of
 is of measure zero.

     Giuseppe Vitali (1875-1932) was an Italian mathematician. Note also that the name Riemann-Lebesgue
often refers to a result like Exercise 5.2.18 from volume I.
10.4. THE SET OF RIEMANN INTEGRABLE FUNCTIONS                                   119

Proof. Let    be the set of discontinuities of  , that is,  =    : (  , ) > 0 .
Suppose  is a measure zero set: () = 0. The trick to proving that  is integrable is to

isolate the bad set into a small set of subrectangles of a partition. A partition has finitely
many subrectangles, so we need compactness. If  were closed, then it would be compact
and we could cover it by finitely many small rectangles. Unfortunately,  itself is not closed

in general, but the following set is. Given  > 0, define

    : (  , )   .

By Proposition 10.4.2,  is closed, and as it is also a subset of the bounded ,  is compact.
Moreover,    and  is of measure zero, so  is of measure zero. Via Proposition 10.3.7,
                                                         
finitely many open rectangles 1, 2, . . . ,  cover  and
                                                         =1 () < .
The set   \ (1  · · ·  ) is closed, bounded, and so compact. As (  , ) <  for
all   , for each   , there is a  > 0 such that (  , , ) < , so there exists a small

closed rectangle   (, ) with  in the interior of , such that

sup  () - inf  () < .
                


The interiors of the rectangles  cover . As  is compact, finitely many such rectangles
1, 2, . . . ,  cover . Construct a partition  out of the endpoints of the rectangles
1, 2, . . . ,  and 1, 2, . . . ,  (ignoring those that are outside the endpoints of ). The
subrectangles 1, 2, . . . ,  of  are such that every  is contained in  for some  or
the closure of  for some  . Order the rectangles so that 1, 2, . . . ,  are those that are
contained in some  , and +1, +2, . . . ,  are the rest. See Figure 10.12. So

                                                         

    ()  () and                          ()  ( ) < .

=1              =+1                                       =1

The second estimate holds because the  that are subsets of  give a partition of  and
hence their volumes sum to ( ). Let  and  be the inf and sup of  over  as usual.
If    for some  , then  -  < . Let    be such that |  ()|   for all   , so
 -   2 over all rectangles. Then

                                     

(,  ) - (,  ) = ( - )()

                                    =1

                                        

= ( - )() +                                              ( - )()

=1                                      =+1

                    

<  () +                                 2 ()

=1                =+1

<  () + 2 =  () + 2 .

We can make the right-hand side as small as we want, and hence  is integrable.
120                                                          CHAPTER 10. MULTIVARIABLE INTEGRAL

Figure 10.12: A rectangle  with  marked as thick black line, and the  as shaded rectangles.
The partition is given by the dotted lines. Note how the  partition the  .

    For the other direction, suppose  is Riemann integrable on . Let  be the set of
discontinuities of  again. Consider the sequence of sets

                                   1/ =    : (  , )  1/ .

Fix a   . Given an  > 0, find a partition  with subrectangles 1, 2, . . . ,  such that

                                                                               

                              (,  ) - (,  ) = ( - )() < .

                                                                              =1

Suppose 1, 2, . . . ,  are ordered so that the interiors of 1, 2, . . . ,  intersect 1/,
                             +1 ,    +2 ,  .  .  .  ,                         1/ .       
while  the  interiors  of                                are  disjoint  from        Let       denote  the  interior
                                                                                            
of .   Suppose              and consider                        1/ .
                                                                        Let      > 0 be small enough so that
                                                            
(, )  . As   1/, we get (  , , )  (  , )  1/, which, along with (, )  ,
implies  -   1/. Then

                         1
             > ( - )()  ( - )()   ().
                       =1                                =1                         =1

In other words, =1  () < . Let  be the set of all boundaries of all the subrectangles of
. The set  is of measure zero (it can be covered by finitely many sets from Example 10.3.5).

We find

                                   1/               1    2      ···           .

                                                                        

As  can also be covered by open rectangles arbitrarily small volume, 1/ must be of
measure zero. As 

                                                    = 1/

                                                                              =1

and a countable union of measure zero sets is of measure zero,  is of measure zero. 
10.4. THE SET OF RIEMANN INTEGRABLE FUNCTIONS               121

Corollary 10.4.4. Let    be a closed rectangle. Let R() be the set of Riemann integrable
functions on . Then

   (i) R() is a real algebra: If  ,   R() and   , then    R(),  +   R() and
           R().

  (ii) If  ,   R() and

                         () max  (), () , () min  (), () ,

      then ,   R().
(iii) If   R(), then |  |  R(), where |  |() |  ()|.
(iv) If    is another closed rectangle,    and    are open sets such that   

      and   ,  :    is continuously differentiable, bective, -1 is continuously
      differentiable, ()  , and   R(), then the composition    is Riemann integrable
      on .

  The proof is contained in the exercises.

10.4.3 Exercises

Exercise 10.4.1: Suppose  : (, ) × (, )   is a bounded continuous function. Show that the integral
of  over  = [, ] × [, ] makes sense and is uniquely defined. That is, set  to be anything (bounded) on
the boundary of  and compute the integral, showing that the values on the boundary are irrelevant.

Exercise 10.4.2: Suppose    is a closed rectangle. Show that R(), the set of Riemann integrable
functions, is an algebra. That is, show that if  ,   R() and   , then    R(),  +   R(), and
   R().

Exercise 10.4.3: Suppose    is a closed rectangle and  :    is a bounded function which is zero
except on a closed set    of measure zero. Show that   exists and compute it.

Exercise 10.4.4: Suppose    is a closed rectangle and  :    and  :    are two Riem ann
integrable functions. Suppose  =  except for a closed set    of measure zero. Show that   =  .

Exercise 10.4.5: Suppose    is a closed rectangle and  :    is a bounded function.
 a) Suppose there exists a closed set    of measure zero such that  |\ is continuous. Then   R().
 b) Find an example where    is a set of measure zero (not closed) such that  |\ is continuous and

       R().

Exercise 10.4.6: Suppose    is a closed rectangle and  :    and  :    are Riemann
integrable. Show that

() max  (), ()           and () min  (), ()

are Riemann integrable.
122                                               CHAPTER 10. MULTIVARIABLE INTEGRAL

Exercise 10.4.7: Suppose    is a closed rectangle and  :    is Riemann integrable. Show that |  |

is Riemann integrable. Hint: Define +() max  (), 0 and -() max -  (), 0 , and then write
|  | in terms of + and -.

Exercise 10.4.8:
 a) Suppose    and    are closed rectangles,    and    are open sets such

     that    and   ,  :    is continuously differentiable, bective, -1 is continuously
     differentiable, ()  , and   R(), then the composition    is Riemann integrable on .
 b) Find a counterexample when  is not one-to-one. Hint: Try (, ) (, 0) and  =  = [0, 1] × [0, 1].

Exercise 10.4.9: Suppose  : [0, 1]2   is defined by

      (, )               1        if  ,        and    =     and    =    in  lowest  terms,
                                                                      

                         0 else.

Show that   R [0, 1]2 .

Exercise 10.4.10: Compute the oscillation   , (, ) for all (, )  2 for the function

                                   (, )               if (, )  (0, 0),
                                             2+2      if (, ) = (0, 0).

                                             0

Exercise 10.4.11: Consider the popcorn function  : [0, 1]  ,
                                 1  ()  if    and  =  in lowest terms,
                                              0 else.

Compute (  , ) for all   [0, 1].

Exercise 10.4.12: Suppose  : [, ]   and  : [, ]   are Riemann integrable.                   Show that
 : [, ] × [, ]   defined by (, )  ()() is Riemann integrable and

                                                            
                                               =                 .

                                    [ , ]×[ , ]               

Exercise 10.4.13: Let    be a closed rectangle and  :    a Riemann integrable function such that
 ()  0 for all   . Show that if   = 0, then there is a measure zero set    such that  () = 0 for
all    \  (one says "  = 0 almost everywhere"). Note: This exercise in particular implies the rather
subtle statement: If  () > 0 for all   , then   > 0.
10.5. JORDAN MEASURABLE SETS                                                          123

10.5 Jordan measurable sets

Note: 1-1.5 lecture

10.5.1 Volume and Jordan measurable sets

Given a set   , its characteristic function or indicator function  :    is defined by

                       ()          1 if   ,
                                   0 if   .

A bounded set  is Jordan measurable if for some closed rectangle  such that   ,
the function  is Riemann integrable, that is,   R(). Take two closed rectangles 
and  with    and   , then    is a closed rectangle also containing . By
Proposition 10.1.13 and Exercise 10.1.7,   R(  ) and so   R(). Thus

                                     

                        =  =              .
                              

We define the -dimensional volume of the bounded Jordan measurable set  as

                                   

                               ()     ,

                                   

where  is any closed rectangle containing .

Proposition 10.5.1. A bounded set    is Jordan measurable if and only if the boundary 
is a measure zero set.

Proof. Suppose  is a closed rectangle such that  is contained in the interior of . If   ,

then for every  > 0, the sets   (, ) (where  is 1) and the sets ( \ )  (, ) (where

 is 0) are both nonempty. So  is not continuous at . If  is either in the interior of  or

in the complement of the closure , then  is either identically 1 or identically 0 in a whole

neighborhood of  and hence  is continuous at . Therefore, the set of discontinuities of

 is precisely the boundary . The proposition follows.                                 

Proposition 10.5.2. Suppose  and  are bounded Jordan measurable sets. Then
   (i) The closure  is Jordan measurable.
  (ii) The interior  is Jordan measurable.

 (iii)    is Jordan measurable.
 (iv)    is Jordan measurable.
  (v)  \  is Jordan measurable.

     Named after the French mathematician Marie Ennemond Camille Jordan (1838-1922).
124                                                              CHAPTER 10. MULTIVARIABLE INTEGRAL

    The proof of the proposition is left as an exercise. Next, the volume that we defined
above coincides with the outer measure we defined previously.

Proposition 10.5.3. If    is Jordan measurable, then () = ().

Proof. Given  > 0, let  be a closed rectangle that contains . Let  be a partition of 

such that                                                                                

     (, )   +  = () +  and (, )   -  = () - .
                                                                                         

Let 1, 2, . . . ,  be all the subrectangles of  such that  is not identically zero on each
. That is, there is some point    such that    (i.e. () = 1). Let  be an open
rectangle such that    and () < () + /. Notice that    . Then

                                                                  

                                (, ) = () >                          () -   () - .

                                                     =1          =1

As (, )  () + , then () -   () + , or in other words ()  ().
          1 ,  2 ,              
.    Let            .  .  .  ,      be  all  the  subrectangles  of     such     that    is  identically  one  on  each
                                  
     In other words, these are the subrectangles contained in .                              The interiors of the

subrectangles                are    disjoint  and   ( )      =    ( ).  Via    Exercise  10.3.16,

                                                          

                                                                 
                                                    = ().
                                                                             

                                                   =1            =1

Hence

     ()                                                          () =            () = (,  )  () - .
                                                      
                                                          =             
                                                  
                                                             =1           =1
                             =1               =1

Therefore ()  () as well.                                                                                          

10.5.2 Integration over Jordan measurable sets

In  there is really only one reasonable type of set to integrate over--an interval. In 
there are many kinds of very reasonable sets. The ones that work with the Riemann integral
are the Jordan measurable sets.

Definition 10.5.4. Let    be a bounded Jordan measurable set. A bounded function
 :    is said to be Riemann integrable on , or   R(), if for a closed rectangle  such
that   , the function  :    defined by

                                           ()  () if   ,
                                                     0 otherwise,
10.5. JORDAN MEASURABLE SETS                                               125

is in R(). We write                              

                                                   .

As above, Proposition 10.1.13 and Exercise 10.1.7 imply that the actual rectangle  used

is irrelevant and the integral is well-defined.
    When  is defined on a larger set and we wish to integrate over , then we apply the

definition to the restriction  |. As the restriction can be defined by the product  , and
the product of Riemann integrable functions is Riemann integrable,  | is automatically
Riemann integrable. In particular, if  :    for a closed rectangle , and    is a

Jordan measurable subset, then                 

                                       =  .
                                               

Proposition 10.5.5. If    is a bounded Jordan measurable set and  :    is a bounded

continuous function, then  is integrable on .

Proof. Define the function  as above for some closed rectangle  with   . If    \ ,

then  is identically zero in a neighborhood of . Similarly, if  is in the interior of , then

 =  on a neighborhood of  and  is continuous at . Therefore,  is only ever possibly

discontinuous at , which is a set of measure zero, and we are finished.    

    We say some property for almost every  or almost everywhere if it holds for all  except
on a set of measure zero. For example, we say  :    and  :    are equal almost
everywhere if there exists a measure zero set    such that  () = () for all    \ .

Many of the standard properties of the integral carry over easily since we are really

integrating over a rectangle. Furthermore, we can make some of the statements to be almost

everywhere. Proofs of the following three propositions left as exercises.

Proposition 10.5.6. Suppose    is a bounded Jordan measurable set and  :    and

 :    are Riemann integrable on , and   . Then
                                  
(i) If  = 0 almost everywhere, then   = 0.
                                  
(ii) If  =  almost everywhere, then   =  .
                                                        
(iii)  +  is Riemann integrable on  and (  + ) =   +  .
                                                   
(iv)   is Riemann integrable on  and    =    .
                                          
(v) If  ()  () for almost every , then     .

We also have additivity.

Proposition 10.5.7. Suppose    and    are disjoint bounded Jordan measurable sets

and  :      is such that the restrictions  | and  | are Riemann integrable on  and 
respectively. Then  is Riemann integrable on    and

                                               

                                     =  + .
                                                      
126         CHAPTER 10. MULTIVARIABLE INTEGRAL

    Finally, to integrate over non-rectangular regions using Fubini's theorem, the typical
way is to cut the region into simpler pieces that can be described by two graphs. We state
the theorem in the plane, but similar statements can be made in more variables. The proof
is again left as an exercise.

Proposition 10.5.8. Let  : [, ]   and  : [, ]   be continuous functions and such
that for all   (, ),  () < (). Let

      (, )  2 :  <  <  and  () <  < () .

See Figure 10.13. Then  is Jordan measurable, and if  :    is Riemann integrable on ,

then     ()

       =          (, )  .

             ()

             = ()

                                              = ()

  Figure 10.13: Region between two graphs.

10.5.3 Images of Jordan measurable subsets

Finally, images of Jordan measurable sets are Jordan measurable under nice enough
mappings. For simplicity, we assume that the Jacobian determinant never vanishes.
Proposition 10.5.9. Suppose    is open and    is a compact Jordan measurable set.
Suppose  :    is a one-to-one continuously differentiable mapping such that the Jacobian
determinant  is never zero on . Then () is bounded and Jordan measurable.
Proof. Let  (). By Lemma 7.5.5 from volume I, the set  is also compact and so
closed and bounded. We claim   (). Suppose the claim is proved. As  is Jordan
measurable, then  is measure zero. Then () is measure zero by Proposition 10.3.10.
As   (), then  is Jordan measurable.

    It is therefore left to prove the claim. As  is closed,   . Suppose   , then
there must exist an    such that () = , and by hypothesis ()  0. We use the
inverse function theorem (Theorem 8.5.1). We find a neighborhood    of  and an
10.5. JORDAN MEASURABLE SETS                                                                                      127

open set  such that the restriction  | is a one-to-one and onto function from  to 
with a continuously differentiable inverse. In particular, () =   . As   , there
                         {   }
exists  a  sequence                  in       with  lim          =         and         .  As  |  is   invertible  and
                                 =1                                                       {   }
in   particular     has  a  continuous        inverse,  there    exists    a    sequence              in    such  that
                                                                                                  =1
() =  and lim  = . Since    = (), clearly   . Since   , we
conclude that   . The claim is proved,   ().
                                                                                                                  

10.5.4 Exercises

Exercise 10.5.1: Prove Proposition 10.5.2.

Exercise 10.5.2: Prove that a bounded convex set is Jordan measurable. Hint: Induction on dimension.

Exercise 10.5.3: Prove Proposition 10.5.8. That is,
 a) Show that  is Jordan measurable.
b) Prove that    =   ()    () (, )  .

Exercise 10.5.4: Let us construct an example of a non-Jordan measurable open set. Start in one dimension.
     {   }
Let             be  an  enumeration  of  all  rational  numbers  in  (0,   1).  Let ( , ) be open intervals such that
            =1
( , )  (0, 1) for all ,   ( , ), and                       -  )      1/2.                 
                                                                 <         Now  let
                                                    =1 (                                  =1( , ).

a) Show the open intervals ( , ) as above actually exist.

b) Prove  = [0, 1] \ .

c) Prove  is not of measure zero, and therefore  is not Jordan measurable.

d) Show that              × (0, 2)  (0, 1) × (1, 2) is a connected bounded open set in 2 that is not Jordan
    measurable.

Exercise 10.5.5: Suppose    is a closed measure zero set.

 a) If  is bounded, prove that  is Jordan measurable.
b) If    is bounded and Jordan measurable, prove that  \  is Jordan measurable.
 c) Construct a bounded Jordan measurable    and a bounded    of measure zero, such that

     neither  nor  \  is Jordan measurable.

Exercise 10.5.6: Suppose    is open and    is compact. Find a compact Jordan measurable set 
such that    and    ( is in the interior of ).

Exercise 10.5.7: Prove a version of Corollary 10.4.4, replacing all closed rectangles with closed and bounded
Jordan measurable sets.

Exercise 10.5.8: Prove Proposition 10.5.6.

Exercise 10.5.9: Prove Proposition 10.5.7.
128                          CHAPTER 10. MULTIVARIABLE INTEGRAL

10.6 Green's theorem

Note: 1 lecture, requires chapter 9

    One of the most important theorems in the calculus of several variables is the so-called
generalized Stokes' theorem, a generalization of the fundamental theorem of calculus. The
two-dimensional version is called Green's theorem. We will state the theorem in general,
but we will only prove a special, but important, case.

Definition 10.6.1. Let   2 be a bounded connected open set. Suppose the boundary
 is a disjoint union of (the images of) finitely many simple closed piecewise smooth
paths such that every    is in the closure of 2 \ . Then  is called a bounded domain
with piecewise smooth boundary in 2.

    The condition about points outside the closure says that locally  separates 2 into
an "inside" and an "outside." The condition prevents  from being just a "cut" inside
. As we travel along the path in a certain orientation, there is a well-defined left and a
right, and either  is on the left and the complement of  is on the right, or vice versa.
The orientation on  is the direction in which we travel along the paths. We can switch
orientation if needed by reparametrizing the path.

Definition 10.6.2. Let   2 be a bounded domain with piecewise smooth boundary, let
 be oriented , and let  : [, ]  2 be a parametrization of  giving the orientation.
Write () = (), () . If the vector () -(), () points into the domain, that is,
() + () is in  for all small enough  > 0, then  is positively oriented. See Figure 10.14.
Otherwise it is negatively oriented.

                           
     () = -(), ()

        () = (), ()

Figure 10.14: Positively oriented domain (left), and a positively oriented domain with a hole
(right).

    The vector () turns  () counterclockwise by 90, that is, to the left. When we travel
along a positively oriented boundary in the direction of its orientation, the domain is "on
our left." For example, if  is a bounded domain with "no holes," that is,  is connected,
then the positive orientation means we are traveling counterclockwise around . If we
do have "holes," then we travel around them clockwise.

     Named after the British mathematical physicist George Green (1793-1841).
10.6. GREEN'S THEOREM                                                                    129

Proposition 10.6.3. Let   2 be a bounded domain with piecewise smooth boundary. Then 
is Jordan measurable.

Proof. We must show that  is a null set. As  is a finite union of piecewise smooth

paths, which are finite unions of smooth paths, we need only show that a smooth path in
2 is a null set. Let  : [, ]  2 be a smooth path. It is enough to show that  (, ) is
a null set, as adding the points () and (), to a null set still results in a null set. Define

            : (, ) × (-1, 1)  2, as  (, ) ().

The set (, ) × {0} is a null set in 2 and  (, ) =  (, ) × {0} . By Proposition 10.3.10,
 (, ) is a null set in 2 and so  [, ] is a null set, and so finally  is a null set. 

Theorem 10.6.4 (Green). Suppose   2 is a bounded domain with piecewise smooth boundary

with the boundary positively oriented. Suppose  and  are continuously differentiable functions

defined on some open set that contains the closure . Then

               +   =  -  .
                                

We stated Green's theorem in general, although we will only prove a special version of

it. That is, we will only prove it for a special kind of domain. The general version follows

from the special case by application of further geometry, and cutting up the general domain

into smaller domains on which to apply the special case.

Let   2 be a bounded domain with piecewise smooth boundary. We say  is of type

I if there exist numbers  < , and continuous functions  : [, ]   and  : [, ]  ,

such that

            (, )  2 :  <  <  and  () <  < () .

Similarly,  is of type II if there exist numbers  < , and continuous functions  : [, ]  
and  : [, ]  , such that

            (, )  2 :  <  <  and () <  < () .

Finally,   2 is of type III if it is both of type I and type II. See Figure 10.15.
    Common domains to apply Green's theorem to are rectangles and discs, and these are

type III domains. We will only prove Green's theorem for type III domains.

Proof of Green's theorem for  of type III. Let  , , ,  be the functions defined  above.  Using
Proposition 10.5.8,  is Jordan measurable and as  is of type I, then

            -      () = -  (, )  
                          ()  
                         

                       = - ,  () +  , () 
                             

                       =  , ()  -  ,  () .
                                
130                                       CHAPTER 10. MULTIVARIABLE INTEGRAL

          type I                        type II           type III

Figure 10.15: Domain types for Green's theorem.

We integrate   along the boundary. The one-form   integrates to zero along the

straight vertical lines in the boundary. Therefore, it is only integrated along the top and
along the bottom. As a parameter,  runs from left to right. If we use the parametrizations
that take  to ,  () and to , () , we recognize path integrals above. However, the

second path integral is in the wrong direction; the top should be going right to left. After

switching this orientation, we find

                =  , ()  +  ,  ()  = -  .
                                                           

Similarly,  is also of type II. The form   integrates to zero along horizontal lines. So

     () =    (, )   =  , () -  , ()                                       
         ()                                                          =  .

                                                                                

Putting the two computations together, we obtain

           +   =   +   = -   +   =  -  . 
                                                              

    Let us see how one can use the simple version of Green's (type III domains only) for a
more complex path.

                                     -            
Example 10.6.5: Suppose (, ) = 2+2 , (, ) = 2+2 . If we think of (, ) as a vector,
so that we have a so-called vector field, (, ) is called the vortex vector field, as it gives the

velocity of particles traveling in a vortex around the origin. Variations on this vector field

come up often in applications. Suppose that  is a path that goes counterclockwise around

a rectangle whose interior contains the origin. We claim

                   - 
                                 + 2 2  = 2.
                   2 + 2                   +

    First we draw a circle  of radius  > 0 centered at the origin such that the entire circle
is within  and oriented clockwise. Consider  to be the domain between  and . See
10.6. GREEN'S THEOREM                                                                                       131

Figure 10.16. The integral around  is the integral around  plus the integral around .

Now  is not a domain of type III, so we cannot just apply the version of Green's theorem

we actually proved. However, if we cut the box along the axis as shown in the figure with

dashed lines, the four resulting domains, let us call them 1, 2, 3, 4, are of type III.
The dashed lines are oriented in opposite directions for the two  that share them, and so

when we integrate along both, the integrals cancel. That is,



          +   =

                                                                                    

                      +   +   +   +   +   +   +  .
              1                          2                       3                       4

Now we are allowed to apply Green's theorem to every . We leave it to the reader to
                                         
verify  that  outside  of  the  origin,     -    =  0.  So  for  each  ,  we  find
                                               

                                                         -            
                                                                    = 0 = 0,
                                  +   =             
                                                                          

        
and so    +   = 0. As  is  together with , we find
                                                                 

                         +   +   +   =   +   = 0.
                                                                 
                                            

So the integral around  is minus the integral around . The integral around  is easy to
                                                               -                    
compute   as  on    we  have    2  +  2  =  2,  so  (,  )   =  2   and    (,  )  =       We  leave  it  to  the
                                                                                    2 .
reader to compute 
                                                  - 
                                    +   = 2  + 2  = -2.

The claim follows.

                                              2                    3

                                                        0                 

                                              1                    4

  Figure 10.16: Changing the box integral to an integral around a small circle around the origin.
  The domain  is the entire shaded area between the circle and the box.

                                                                   
    We remark that if  would not contain the origin,    +   = 0, as we could just
apply Green's to . So this integral can detect whether the origin is inside  or not.
132                                    CHAPTER 10. MULTIVARIABLE INTEGRAL

    As a second example, we illustrate the usefulness of Green's theorem on a fundamental
result about harmonic functions.

Example 10.6.6: Suppose   2 is open and  :    is harmonic, that is,  is twice

continuously differentiable and satisfies the Laplace equation, 2 2  + 2 2  = 0. Harmonic

functions are, for instance, the steady state heat distribution, or the electric potential

between charges. We will prove one of the most fundamental properties of these functions.

    Let  (, ) be a disc such that its closure  = (, )  . Write  = (0, 0). We
orient  positively. See Exercise 10.6.1. Then via Green's and differentiation under the

integral,

                       2  2 
           0= 1           2+ 2
           2   
           =1          -    +   
           2  2   
           =1          -   0 +  cos(), 0 +  sin()
           2 0                                            - sin()

           =               +   0 +  cos(), 0 +  sin()  cos() 
               1  2

                           0 +  cos(), 0 +  sin()  .
               2 0

               1  2
Let () 2 0  0 +  cos(), 0 +  sin()  for   0 (small enough). The function
is continuous at  = 0 (exercise), and we have just proved that () = 0 for all  > 0.

Therefore, (0) = () for all  > 0, and

           () = (0) = 1  2  0 + 0 cos(), 0 + 0 sin()  =  (0, 0).
                             2 0

We proved the mean value property of harmonic functions:

            (0, 0) = 1  2  0 +  cos(), 0 +  sin()  = 1   .
               2 0                                        2 

That is, for a harmonic function, the value at  = (0, 0) equals the average of its values
over a circle of any radius  centered at (0, 0), provided  is small enough so that the
entire closed disc fits within .

10.6.1 Exercises

Exercise 10.6.1: Prove that a disc (, )  2 is a type III domain, and prove that the orientation given by
the parametrization () = 0 +  cos(), 0 +  sin() where  = (0, 0) is the positive orientation of the
boundary (, ).
Note: Feel free to use what you know about sine and cosine from calculus.
10.6. GREEN'S THEOREM                                                                         133

Exercise 10.6.2: Prove that a convex bounded domain with piecewise smooth boundary is a type III domain.

Exercise 10.6.3: Suppose   2 is a bounded domain with piecewise smooth boundary of type III and

suppose that   2 is an open set such that   . Suppose  :    is a twice continuously
                                       
differentiable function. Prove that    +   = 0.

Exercise 10.6.4: For a disc (, )  2, orient the boundary (, ) positively.
               

a) Compute  ( , )    - .

b) Compute            .
            (,) - 
c) Compute            + .
            (,) 2         2

Exercise 10.6.5: Using Green's theorem show that the area of a triangle with vertices (1, 1), (2, 2),
(3, 3) is 12 |1 2 + 2 3 + 3 1 - 12 - 23 - 31|. Hint: See previous exercise.

Exercise 10.6.6: Using the mean value property prove the maximum principle for harmonic functions:
Suppose   2 is a connected open set and  :    is harmonic. Prove that if  attains a maximum at
  , then  is constant.

Exercise 10.6.7: Let  (, ) ln 2 + 2.

a) Show  is harmonic where defined.

b) Show lim  (, ) = -.
    ( ,  )0
                                                                 
c)  Using a circle   of radius   around the origin, compute   1       . What happens as   0?
                                                             2     

d) Why can't you use Green's theorem?
134                                          CHAPTER 10. MULTIVARIABLE INTEGRAL

10.7 Change of variables

Note: 1 lecture

In one variable, we have the familiar change of variables

                         () ()  =             ()

                                             ()    () .

The analogue in higher dimensions is quite a bit more complicated. The first complication

is orientation. If we use the definition of integral from this chapter, then we do not have
                   
the notion of  versus  . We are simply integrating over an interval [, ]. With this
notation, the change of variables becomes

                                             

                   [,]   () |()|  =           ([ , ])   () .

In this section we will obtain the several-variable analogue of this form.
    Let us remark the role of |()| in the formula. The integral measures volumes in

general, so in one dimension it measures length. Notice that |()| scales the  and so it
scales the lengths. If our  is linear, that is, () = , then () =  and the length of the
interval ([, ]) is simply ||( - ). That is because ([, ]) is either [, ] or [, ].
This property holds in higher dimension with || replaced by the absolute value of the

determinant.

Proposition 10.7.1. Suppose    is a rectangle and  :    is linear. Then () is
Jordan measurable and  () = |det()| ().

Proof. It is enough to prove for elementary matrices. The proof is left as an exercise. 

    Let us prove that absolute value of the Jacobian determinant |()| = det () is the
replacement of |()| for multiple dimensions in the change of variables formula. The
following theorem holds in more generality, but this statement is sufficient for many uses.

Theorem 10.7.2. Suppose    is open,    is a compact Jordan measurable set, and
 :    is a one-to-one continuously differentiable mapping, such that  is never zero on .
Suppose  : ()   is Riemann integrable. Then    is Riemann integrable on  and

                                           

                     ()   ()  =  () |()| .
                                           

    The set () is Jordan measurable by Proposition 10.5.9, so the left-hand side does
make sense. That the right-hand side makes sense follows by Corollary 10.4.4 (actually
Exercise 10.5.7).
10.7. CHANGE OF VARIABLES                                                                                    135

Proof. The set  can be covered by finitely many closed rectangles 1, 2, . . . , , whose
interiors do not overlap such that each    (Exercise 10.7.2). Proving the theorem for
   instead of  is enough. Define  () 0 for all   (). The new  is still Riemann
integrable since () is Jordan measurable. We can now replace the integrals over  with
integrals over the whole rectangle. We therefore assume that  is equal to a rectangle .

    Let  > 0 be given. For every   , let

                                      : () - () < /2 .

By Exercise 10.7.3,  is open. As    for every , it is an open cover. By the Lebesgue

covering lemma (Lemma 7.4.10 from volume I), there exists a  > 0 such that for every

  , there is an  such that (, )  . In other words, if  is a rectangle of maximum
                                                                     (, )    
side    length  less    than        and       ,  then                            .    By triangle inequality,

() - () <  for all ,   .

    Let 1, 2, . . . ,  be subrectangles partitioning  such that the maximum side of every
                     .                                                                                  2 ,
    is  less  than      We    also  make   sure  that   the  minimum       side  length  is  at  least       which
                      
we can do if  is sufficiently small relative to the sides of  (Exercise 10.7.4).
    Consider some  and some fixed   . First suppose  = 0, (0) = 0, and (0) = .
For any given   , apply the fundamental theorem of calculus to the function   ( )
                    1
to  find  ()    =       ( )   .     As   the  side  of       is  at  most    ,  then         .   So
                     0

                    1                            1                                    1
                        ( ) -    ( ) -     ( ) -    .
() -  =

                    0                            0                                    0

Therefore, ()  , where  is a rectangle obtained from  by extending by  on all
sides. See Figure 10.17.

                                                                                         
                      ()                                                                 ()
                                                                                         
                                                 

                                                                        2
                                        = 0 = ()

                                                                     

                                              1                  

Figure 10.17: Image of  under  lies inside . A sample point    (on the boundary of 
in fact) is marked and () must lie within with a radius of  (also marked).
136                               CHAPTER 10. MULTIVARIABLE INTEGRAL

If the sides of  are 1, 2, . . . , , then () = 12 · · · . Recall   2 . Thus,

     () = (1 + 2)(2 + 2) · · · ( + 2)       
                          
      (1 + 4  1)(2 + 4  2) · · · ( + 4   )
                                                                       
     = 1(1 + 4  ) 2(1 + 4  ) · · · (1 + 4  ) = () (1 + 4  ) .

In other words,          ()  ()  () (1 + 4 ).

Next, suppose  (0) is not necessarily the identity. Write  =    where (0) = .

By Proposition 10.7.1,  () = |det()| (), and hence
                                  ()  |det()| () (1 + 4 )
                                              = |(0)| () (1 + 4 ).

Translation does not change volume, and therefore for every , and   , including
when   0 and ()  0, we find

                                  ()  |()| () (1 + 4 ).

Write  as  = + - - for two nonnegative Riemann integrable functions + and -:

                      +() max  (), 0 , -() max -  (), 0 .

So, if we prove the theorem for a nonnegative  , we obtain the theorem for arbitrary  .
Therefore, suppose that  ()  0 for all   .

    For a small enough  > 0, we have

                             

      +  () |()|   sup  () |()| ()
                             =1 

                                  sup   ()    |()| ()

                                  

                              =1

                                  sup  ()  ()                       1
                                                                    
                                                                    (1 + 4  )

                             =1 ()

                                         ()                         1
                                                                    
                                              (1 + 4  )
                             =1 ()        

                          =       1            () .
                                  
                             (1 + 4  ) ()

The last equality follows because the overlaps of the rectangles are their boundaries, which

are of measure zero, and hence the image of their boundaries is also measure zero. Let 

go to zero to find                      

                           () |()|      ()   () .
10.7. CHANGE OF VARIABLES                                                           137

By adding this result for several rectangles covering an  we obtain the result for an
arbitrary bounded Jordan measurable   , and nonnegative integrable function  :

                                                               

                                            () |()|            ()             () .

    Recall that -1 exists and -1 () = . Also, 1 = -1 =  -1()                        -1() for
  (). So

                                     

                 ()   ()  =            ()         -1()         | -1() | |-1()| 
                                                                            
                                     

                                        -1 (  ())      () |()|  =  () |()| .

                                                                                

The conclusion of the theorem holds for all nonnegative  and as we mentioned above,

it thus holds for all Riemann integrable  .                                         

10.7.1 Exercises

Exercise 10.7.1: Prove Proposition 10.7.1.

Exercise 10.7.2: Suppose    is open and    is a compact Jordan measurable set. Show that there

exist finitely many closed rectangles 1, 2, . . . ,  such that   ,   1  2  · · ·  , and the
                                                      
interiors  are  mutually  disjoint,  that  is,            =    whenever      .
                                                        

Exercise 10.7.3: Suppose    is open,   , and  :    is a continuously differentiable mapping.
For every  > 0, show that

                                            : () - () < /2

is an open set.

Exercise 10.7.4: Suppose    is a closed rectangle. Show that if  > 0 is sufficiently small relative to
the sides of , then  can be partitioned into subrectangles where each side of every subrectangle is between
 and .

 2

Exercise 10.7.5: Prove the following version of the theorem: Suppose  :    is a Riemann integrable
compactly supported function. Suppose    is the support of  ,  is a compact set, and
 :    is a function that when restricted to a neighborhood  of  is one-to-one and

continuously differentiable, () =  and  is never zero on  (in the formula assume () = 0 if 
not differentiable at , that is, when   ). Then

                                                          

                                                 ()  =  () |()| .
                                                             
138        CHAPTER 10. MULTIVARIABLE INTEGRAL

Exercise 10.7.6: Prove the following version of the theorem: Suppose    is an open bounded Jordan
measurable set,  :    is a one-to-one continuously differentiable mapping such that  is

never zero on , and such that () is bounded and Jordan measurable (it is also open). Suppose

 : ()   is Riemann integrable. Then    is Riemann integrable on  and

         

     ()   ()  =  () |()| .
         

Hint: Write  as an increasing union of compact Jordan measurable sets, then apply the theorem of the section
to those. Then prove that you can take the limit.
Chapter 11

Functions as Limits

11.1 Complex numbers

Note: half a lecture

11.1.1 The complex plane

In this chapter we consider approximation of functions, or in other words functions as
limits of sequences and series. We will extend some results we already saw to a somewhat
more general setting, and we will look at some completely new results. In particular, we
consider complex-valued functions. We gave complex numbers as examples before, but let
us start from scratch and properly define the complex number field.

    A complex number is just a pair (, )  2 on which we define multiplication (see
below). We call the set the complex numbers and denote it by . We identify    with
(, 0)  . The -axis is then called the real axis and the -axis is called the imaginary axis.
As  is just the plane, we also call the set  the complex plane.

    Define:

                (, ) + (, ) ( + ,  + ), (, )(, ) ( - ,  + ).

Under the identification above, we have 0 = (0, 0) and 1 = (1, 0). These two operations
make the plane into a field (exercise). We write a complex number (, ) as  + , where
we define

                                                     (0, 1).
Notice that 2 = (0, 1)(0, 1) = (0 - 1, 0 + 0) = -1. That is,  is a solution to the polynomial
equation

                                                    2 + 1 = 0.
From now on, we will not use the notation (, ) and use only  + . See Figure 11.1.

     Note that engineers use  instead of .
140                                          CHAPTER 11. FUNCTIONS AS LIMITS

                                          + or ( , )

                                          1

Figure 11.1: The points 1, , ,  , and  +   in the complex plane.

    We generally use , , , ,  for real values and , , ,  for complex values, although
that is not a hard and fast rule. In particular,  is often used as a third real variable in 3.

Definition 11.1.1. Suppose  =  +  . We call  the real part of , and we call  the imaginary
part of . We write

                                          Re  , Im  .

Define complex conjugate as

                                      ¯  - ,

and define modulus as

                                      ||  2 + 2.

    Modulus is the complex analogue of the absolute value and has similar properties. For
example, || = || || (exercise). The complex conjugate is a reflection of the plane across
the real axis. The real numbers are precisely those numbers for which the imaginary part
 = 0. In particular, they are precisely those numbers which satisfy the equation

                                                        = ¯.

As  is really 2, we let the metric on  be the standard euclidean metric on 2. In

particular,

             || = (, 0), and also | - | = (, ).

So the topology on  is the same exact topology as the standard topology on 2 with the
euclidean metric, and || is equal to the euclidean norm on 2. Importantly, since 2 is
a complete metric space, then so is . As || is the euclidean norm on 2, we have the

triangle inequality of both flavors:

             | + |  || + || and || - ||  | - |.

The complex conjugate and the modulus are even more intimately related:
                              ||2 = 2 + 2 = ( +  )( -  ) = ¯.

Remark 11.1.2. There is no natural ordering on the complex numbers. In particular, no
ordering that makes the complex numbers into an ordered field. Ordering is one of the
things we lose when we go from real to complex numbers.
11.1. COMPLEX NUMBERS                                                                        141

11.1.2 Complex numbers and limits

Algebraic operations with complex numbers are continuous because convergence in 2 is

the same as convergence for each component, and we already know that the real algebraic
operations are continuous. For example, write  =  +   and  =  +  , and
suppose that lim  =  =  +   and lim  =  =  +  . Let us show

                                          lim  = .

                                          

First,

                           = (  - ) + ( +  ).

The topology on  is the same as on 2, and so   ,   ,   , and   .

Hence,

        lim (  - ) =  -  and                                lim ( +  ) =  + .
                                                            

As ( - ) + ( + ) = ,

                                          lim  = .

                                          

Similarly the modulus and the complex conjugate are continuous functions. We leave

the remainder of the proof of the following proposition as an exercise.

Proposition  11.1.3.  Suppose  {   }=1 ,  {  }         are  sequences  of  complex  numbers  converging  to

 and  respectively. Then                           =1

(i) lim  +  =  + .

      

(ii) lim  = .

        

(iii) Assuming   0 for all  and   0, lim  =  .
                                                        

(iv) lim || = ||.

        

(v) lim ¯ = ¯.

       

As we have seen above, convergence in  is the same as convergence in 2. In particular,

a sequence in  converges if and only if the real and imaginary parts converge. Therefore,
feel free to apply everything you have learned about convergence in 2, as well as applying

results about real numbers to the real and imaginary parts.
                                                                 {   }
We also need convergence of complex series.                 Let              be a sequence of complex
                                                                         =1
numbers. The series 

                                                       

                                               =1

converges if the limit of partial sums converges, that is, if

                                             

                                  lim                       exists.

                                  
                                          =1

A series converges absolutely if          |  converges.

                                  =1|
142                                                      CHAPTER 11. FUNCTIONS AS LIMITS

We say a series is Cauchy if the sequence of partial sums is Cauchy. The following

two propositions have essentially the same proofs as for real series and we leave them as

exercises.

Proposition 11.1.4. The complex series                   is  Cauchy  if  for  every     >  0,    there  exists  an

                                               =1 
   such that for every    and every  > , we have

                                                         < .

                                                     

                                               =+1

Proposition 11.1.5. If a complex series               converges  absolutely,    then  it  converges.
                                               =1

The series               is  a  real  series.  All  the  convergence     tests  (ratio    test,  root   test,  etc.)

                =1 |  |
that talk about absolute convergence work with the numbers ||, that is, they are really

talking about convergence of series of nonnegative real numbers. You can directly apply

these tests them without needing to reprove anything for complex series.

11.1.3 Complex-valued functions

When we deal with complex-valued functions  :   , what we often do is to write

 =  +   for real-valued functions  :    and  :   .

Suppose we wish to integrate  : [, ]  . We write  =  +   for real-valued 

and . We say that  is Riemann integrable if  and  are Riemann integrable, and in this

case we define                                               
                                            
                                                        +  .
                                                              

We make the same definition for every other type of integral (improper, multivariable, etc.).
    Similarly when we differentiate, write  : [, ]   as  =  +  . Thinking of  as

2, we say that  is differentiable if  and  are differentiable. For a function valued in 2,
the derivative is represented by a vector in 2. Now a vector in 2 is a complex number.

In other words, we write the derivative as

                                       () () +  ().

The linear operator representing the derivative is the multiplication by the complex number
 (), so nothing is lost in this identification.

11.1.4 Exercises

Exercise 11.1.1: Check that  is a field.
Exercise 11.1.2: Prove that for ,   , we have || = || ||.
Exercise 11.1.3: Finish the proof of Proposition 11.1.3.
11.1. COMPLEX NUMBERS                                                                                 143

Exercise 11.1.4: Prove Proposition 11.1.4.

Exercise 11.1.5: Prove Proposition 11.1.5.

                                                                                     -

Exercise 11.1.6: Given  +   define the matrix   . Prove:
 a) The action of this matrix on a vector (, ) is the same as the action of multiplying ( +  )( + ).

b) Multiplying two such matrices is the same multiplying the underlying complex numbers and then finding

   the corresponding matrix for the product. In other words, the field  can be identified with a subset of the

   2-by-2 matrices.

                             -

c) The matrix   has eigenvalues  +   and  -  . Recall that  is an eigenvalue of a matrix  if
     -  (a complex matrix in our case) is not invertible, that is, if it has linearly dependent rows: one row

    is a (complex) multiple of the other.

Exercise 11.1.7:  Prove the Bolzano-Weierstrass theorem for complex sequences.  Suppose  {   }        is  a

                                                                                                 =1
bounded sequence of complex numbers, that is, there exists an  such that ||   for all . Prove that
                                        }
there  exists  a  subsequence  {              that  converges  to  some      .
                                          =1

Exercise 11.1.8:

a) Prove that there is no simple mean value theorem for complex-valued functions: Find a differentiable
   function  : [0, 1]   such that  (0) =  (1) = 0, but  ()  0 for all   [0, 1].

b) However, there is a weaker form of the mean value theorem as there is for vector-valued functions. Prove: If
     : [, ]   is continuous and differentiable in (, ), and for some , |  ()|   for all   (, ),
    then |  () -  ()|  | - |.

Exercise 11.1.9: Prove that there is no simple mean value theorem for integrals for complex-valued functions:
                                                             1

Find a continuous function  : [0, 1]   such that 0  = 0 but  ()  0 for all   [0, 1].
144                                                         CHAPTER 11. FUNCTIONS AS LIMITS

11.2 Swapping limits

Note: 2 lectures

11.2.1 Continuity

Let  us  get  back  to  swapping     limits  and   expand   on  chapter  6  of  volume  I.  Let  { }        be

                                                                                                        =1
a sequence of functions  :    for a set  and a metric space . Let  :    be a
function and for every   , suppose

                                              () = lim ().

                                                         

We   say   the  sequence  { }        converges  pointwise   to  .

                                 =1
     For  = , a series of functions converges pointwise to  if for every   , we have

                                                                   

                                      () = lim         () =          ().

                                                   =1           =1

     The question is: If  are all continuous, is  continuous? Differentiable? Integrable?

What are the derivatives or integrals of  ? For example, for continuity of the pointwise
                                             } ,
limit  of  a  sequence  of  functions  {           we  are  asking   if
                                               =1

                                                                ?

                                     lim lim () = lim lim ().
                            0                           0

A priori, we do not even know if both sides exist, let alone if they equal each other.

Example 11.2.1: The functions  :   ,

                                                () 1 + 2 1 ,
are continuous and converge pointwise to the discontinuous function

                                           () 1 if  = 0,
                                                     0 else.

     So pointwise convergence is not enough to preserve continuity (nor even boundedness).
                                                                                                  { }
For that, we need uniform convergence.                 Let  :    be functions.              Then
                                                                                                         =1
converges uniformly to  if for every  > 0, there exists an  such that for all    and all

  , we have

                                              (),  () < .
11.2. SWAPPING LIMITS                                                                                            145

    A series              of  complex-valued                      functions  converges   uniformly  if  the  sequence       of
               =1
partial sums converges uniformly, that is, if for every  > 0, there exists an  such that for

all    and all   ,

                                                                

                                                                  () -  () < .

                                        =1

    The simplest property preserved by uniform convergence is boundedness. We leave
the proof of the following proposition as an exercise. It is almost identical to the proof for
real-valued functions.

Proposition 11.2.2. Let  be a set and (, ) a metric space. If  :    are bounded functions
and converge uniformly to  :   , then  is bounded.

    If  is a set and (, ) is a metric space, then a sequence  :    is uniformly Cauchy
if for every  > 0, there is an  such that for all ,    and all   , we have

                                               (), () < .

The notion is the same as for real-valued functions. The proof of the following proposition
is again essentially the same as in that setting and is left as an exercise.

Proposition 11.2.3. Let  be a set, (, ) be a metric space, and  :    be functions.
    { }                                       { }                                                                { }
If             converges  uniformly,    then                      is  uniformly  Cauchy.  Conversely,        if             is
           =1                                        =1                          }                                      =1
uniformly      Cauchy  and  ( ,  )  is  Cauchy-complete,              then   {           converges  uniformly.
                                                                                     =1

    For  :   , we write

                                                                  sup|  ()|.

                                                                  

We call · the supremum norm or uniform norm, and the subscript denotes the set over
which the supremum is taken. Then a sequence of functions  :    converges
uniformly to  :    if and only if

                                               lim   -   = 0.

                                                             

    The supremum norm satisfies the triangle inequality: For every   ,

                            |  () + ()|  |  ()| + |()|     +  .

Take a supremum on the left to get

                                          +      +  .

    For a compact metric space , the uniform norm is a norm on the vector space ( , ).
We leave it as an exercise. While we will not need it, ( , ) is in fact a complex vector

space, that is, in the definition of a vector space we can replace  with . Convergence in
the metric space ( , ) is uniform convergence.

    We will study a couple of types of series of functions, and a useful test for uniform
convergence of a series is the so-called Weierstrass -test.
146                                                    CHAPTER 11. FUNCTIONS AS LIMITS

Theorem 11.2.4 (Weierstrass -test). Let  be a set. Suppose  :    are functions and
 > 0 numbers such that

                                                               

               | ()|   for all    , and                             converges.

                                                               =1

Then                           

                                   ()     converges uniformly.

                              =1

Another way to state the theorem is to say that if                          converges,  then   

                                                               =1                              =1 

converges uniformly. Note that the converse of this theorem is not true. Applying the
            
theorem to        ( )|,  we  see  that  this  series   also  converges  uniformly.      So the series
            =1|

converges both absolutely and uniformly.

Proof. Suppose           converges.  Given      >  0,  we  have  that  the  partial  sums  of  
                =1
are Cauchy so there is an  such that for all ,    with   , we have                             =1 

                                           

                                                < .

                                        =+1

We estimate a Cauchy difference of the partial sums of the functions

                                                             

                              ()              | ()|               < .

                         =+1            =+1                =+1

The series converges by Proposition 11.1.4. The convergence is uniform, as  does not
depend on . Indeed, for all   ,

                                                       

                              () - ()                        ()  .                             

                         =1          =1                =+1

Example 11.2.5: The series                 sin()
                                          =1 2

converges uniformly on . See Figure 11.2. This series is a Fourier series, and we will see
                                                                                               1
more of these in a later section. Proof: The series converges uniformly because                =1 2

converges and

                                        2 sin()  21 .
11.2. SWAPPING LIMITS                                                                                              147

                                        1

                                     0.5

                                        0

                                  -0.5

                                     -1

                                              -      -2         0        2

                                        sin()

Figure 11.2: Plot of =1 2 including the first 8 partial sums in various shades of gray.

Example 11.2.6: The series                                 

                                                          =0 !

converges uniformly on every bounded interval. This series is a power series that we will

study shortly. Proof: Take the interval [-, ]   (every bounded interval is contained in
                                                                                  
some [-, ]). The series =0 ! converges by the ratio test, so =0 ! converges uniformly
on [-, ] as
                                                      !   ! .

Now we would love to say something about the limit. For example, is it continuous?

Proposition 11.2.7. Let ( , ) and (, ) be metric spaces, and suppose (, ) is Cauchy-
                                                                                      {   }
complete.    Suppose   :                   converge  uniformly  to    :       .  Let              be  a  sequence  in  
                                                                                              =1
and  lim . Suppose
                                                  lim ()
                                                          

exists  for  all  .   Then  {   }       converges    and

                                    =1

                                                 lim  () = lim .
                                                                

In other words,

                                        lim lim () = lim lim ().
                                                           

Proof.  First     we  show  that  {     }     converges.  As    {    }      converges  uniformly      it  is  uniformly

                                          =1                           =1
Cauchy. Let  > 0 be given. There is an  such that for all ,   , we have

                                         (), () <  for all .

Note that ( , )    , () +  (), ()                                             +        (),               and take the
limit as    to find

                                                 ( , )  .
148                                                          CHAPTER 11. FUNCTIONS AS LIMITS

Hence  {   }       is  Cauchy  and    converges   since    is  complete.  Write      lim .

               =1
  Find a    such that

                                              (),  () < /3

for all   . Assume  is large enough so that

                                                  ( , ) < /3.

Find an    such that for   ,

                                              (),  < /3.

Then for   ,

        (),     (), () +  (),  +   ,  < /3 + /3 + /3 = . 

  We obtain an immediate corollary about continuity. If  are all continuous then
      ()                  {   }                                      () and so we do not require
  =              and  so              converges automatically to
                                  =1
completeness of .

Corollary 11.2.8. Let  and  be metric spaces. If  :    are continuous functions such
      { }                                         
that             converges  uniformly  to    :      ,  then    is  continuous.
             =1

    The converse is not true. Just because the limit is continuous does not mean that the
convergence is uniform. For example:  : (0, 1)   defined by ()  converge to
the zero function, but not uniformly. However, if we add extra conditions on the sequence,

we can obtain a partial converse such as Dini's theorem, see Exercise 6.2.10 from volume I.
    In Exercise 11.2.3 the reader is asked to prove that for a compact , ( , ) is a

normed vector space with the uniform norm, and hence a metric space. We have just
shown that ( , ) is Cauchy-complete: Proposition 11.2.3 says that a Cauchy sequence in
( , ) converges uniformly to some function, and Corollary 11.2.8 shows that the limit is
continuous and hence in ( , ).

Corollary 11.2.9. Let ( , ) be a compact metric space. Then ( , ) is a Cauchy-complete

metric space.

Example 11.2.10: By Example 11.2.5 the Fourier series

                                                     sin()
                                                   =1 2

converges uniformly and hence is continuous by Corollary 11.2.8 (as is visible in Figure 11.2).

11.2.2 Integration

Proposition      11.2.11.   Suppose    :  [, ]        are  Riemann  integrable  and  suppose  that  { }

converges uniformly to  : [, ]  . Then  is Riemann integrable and                                          =1

                                                           

                                                   = lim .
                                                     
11.2. SWAPPING LIMITS                                                                    149

    Since the integral of a complex-valued function is just the integral of the real and
imaginary parts separately, the proof follows directly by the results of chapter 6 of volume I.
We leave the details as an exercise.

Corollary 11.2.12. Suppose  : [, ]   are Riemann integrable and suppose that

                                            

                                                ()

                                           =1

converges uniformly. Then the series is Riemann integrable on [, ] and

                                                  

                                  =1   ()  =     =1         () 

Example 11.2.13: Let us show how to integrate a Fourier series.

                        cos()                 cos()                  sin()
                                        =           2  =
                     0 =1 2                =1 0                   =1      3

The swapping of integral and sum is possible because of uniform convergence, which we
have proved before using the Weierstrass -test (Theorem 11.2.4).

    We remark that we can swap integrals and limits under far less stringent hypotheses,
but for that we would need a stronger integral than the Riemann integral. E.g. the Lebesgue
integral.

11.2.3 Differentiation

Recall that a complex-valued function  : [, ]  , where  () = () +  (), is
differentiable, if  and  are differentiable and the derivative is

                                              () = () +  ().

    The proof of the following theorem is to apply the corresponding theorem for real
functions to  and , and is left as an exercise.

Theorem 11.2.14. Let    be a bounded interval and let  :    be continuously differ-
                              {   }                                       and suppose { ()}=1
entiable functions.  Suppose           converges uniformly to  :       ,
                                   =1            { }
is a convergent sequence for some       .  Then             converges  uniformly  to  a  continuously
differentiable function  :   , and   = .                =1

    Uniform limits of the functions themselves are not enough, and can make matters even
worse. In §11.7 we will prove that continuous functions are uniform limits of polynomials,
yet as the following example demonstrates, a continuous function need not be differentiable
anywhere.
150                                                       CHAPTER 11. FUNCTIONS AS LIMITS

Example 11.2.15: There exist continuous nowhere differentiable functions. Such functions
are often called Weierstrass functions, although this particular one, essentially due to Takagi,
is a different example than what Weierstrass gave. Define

                                       () || for   [-1, 1].

Extend  to all of  by making it 2-periodic: Decree that () = ( + 2). The function
 :    is continuous, in fact, |() - ()|  | - | (why?). See Figure 11.3.

                      1

                      0  -8 -6 -4 -2 0

                                                                2  4         6    8

Figure 11.3: The 2-periodic function .

                3

    As =0 4 converges and |()|  1 for all , by the -test (Theorem 11.2.4),
                                              () 3  (4 )
                                                      =0 4

converges uniformly and hence is continuous. See Figure 11.4.

                      3

                      2

                      1

                      0

                         0                             1                          2

Figure 11.4: Plot of the nowhere differentiable function  .

We claim  :    is nowhere differentiable. Fix , and we will show  is not

differentiable at . Define                      ± 1 4- ,
                                                         2

where  the  sign  is  chosen  so  that  there  is  no  integer  between  4   and  4(  +  )  =  4   ±  1.

                                                                                                      2

Tei Takagi (1875-1960) was a Japanese mathematician.
11.2. SWAPPING LIMITS                                                                           151

    We want to look at the difference quotient

                 ( + ) -  ()  =             3   4( + )              - (4 ) .
                         =0
                                            4                 

Fix  for a moment. Consider the expression inside the series:

                                4( + ) - (4 ) .
                                                   

If  > , then 4  is an even integer. As  is 2-periodic we get that  = 0.
    As there is no integer between 4(+) = 4 ±1/2 and 4 , then on this interval () =

± +  for some integer  . In particular,  4( + ) - (4 ) = |4  ± 1/2 - 4 | = 1/2.

Therefore,                          4( + ) - (4 ) 

                        || =                1 -                   =4 .
                                            ±( /2)4

Similarly, suppose  < . Since |() - ()|  | - |,

                || = 1 -  4  1 - = 4   ± (1/2)4- - (4 ) ±(1/2)4-  .
                                   ±( /2)4                    ±( /2)4

    And so

         ( + ) -  ()               3    =            3  
                                   4 =0              4
                          =

                               =0

                                             3  -1  - 3  
                                                 4 =0 4

                                             3 - 3 = 3 -1 - 3 - 1 = 3 + 1 .
                                                      =0 3 - 1 2

As      ,   we  have      0,  but  3 +1  goes  to  infinity.  So    cannot  be  differentiable  at  .
                                     2

11.2.4 Exercises

Exercise 11.2.1: Prove Proposition 11.2.2.

Exercise 11.2.2: Prove Proposition 11.2.3.

Exercise 11.2.3: Suppose ( , ) is a compact metric space. Prove that the uniform norm · is a norm on
the vector space of continuous complex-valued functions ( , ).
152                                                               CHAPTER 11. FUNCTIONS AS LIMITS

Exercise 11.2.4:

a) Prove that () 2- sin(2 ) converge uniformly to zero, but there exists a dense set    such
   that lim () = 1 for all   .

b) Prove that         2-  sin(2  )  converges   uniformly  to  a  continuous  function,  and  there  exists  a  dense  set
                  =1
        where the derivatives of the partial sums do not converge.

Exercise 11.2.5: Prove that   1   [,] +   [,] is a norm on the vector space of continuously
differentiable complex-valued functions 1 [, ],  .

Exercise 11.2.6: Prove Theorem 11.2.14.

Exercise 11.2.7: Prove Proposition 11.2.11 by reducing to the real result.

Exercise 11.2.8: Work through the following counterexample to the converse of the Weierstrass -test
(Theorem 11.2.4). Define  : [0, 1]   by

                                                1 if 1 <  < 1 ,
                                         ( )               +1     

                                                0 else.

Prove that          converges  uniformly,  but  
            =1
                                                =1 [0,1] does not converge.

Exercise 11.2.9: Suppose  : [0, 1]   are monotone increasing functions and suppose that                         
                                    
                                    =1                                                                          =1 

converges pointwise. Prove that            converges  uniformly.

Exercise 11.2.10: Prove that                      

                                                     -

                                                =1

converges for all  > 0 to a differentiable function.
11.3. POWER SERIES AND ANALYTIC FUNCTIONS                                                                153

11.3 Power series and analytic functions

Note: 2-3 lectures

11.3.1 Analytic functions

A (complex) power series is a series of the form

                                                    

                                                       ( - )

                                                   =0

for  , ,   . We say the series converges if the series converges for some   .
    Let    be an open set and  :    a function. Suppose that for every    there

exists a  > 0 and a power series convergent to the function

                                                         

                                             () = ( - )

                                                        =0

for all   (, ). Then we say  is an analytic function. Similarly, given an interval
(, )  , we say that  : (, )   is analytic or perhaps real-analytic if for each point
  (, ) there is a power series around  that converges in some ( - ,  + ) for some
 > 0. As we will sometimes talk about real and sometimes about complex power series,
we will use  to denote a complex number and  a real number. We will always mention

which case we are working with.

    An analytic function has different expansions around different points. Moreover,

convergence does not automatically happen on the entire domain of the function. For
example, if || < 1, then

                                                    1 =   .
                                                  1 -  =0

While the left-hand side exists on all of   1, the right-hand side happens to converge only

if  ||  <  1.  See  a  graph  of  a  small  piece  of   1  in  Figure  11.5.  We  cannot  graph  the  function
                                                       1-
itself, we can only graph its real or imaginary parts for lack of dimensions in our universe.

11.3.2 Convergence of power series

We proved several results for power series of a real variable in §2.6 of volume I. For the
                                                                                                            |
most part the convergence properties of power series deal with the series                             |  -
                                                                                          =0 |  |
and so we have already proved many results about complex power series. In particular, we

computed the so-called radius of convergence of a power series.
154                                                       CHAPTER 11. FUNCTIONS AS LIMITS

Figure 11.5:  Graphs of the real and imaginary parts of     =  +           1  in the square [-0.8, 0.8]2.
                                                                          1-
The singularity at  = 1 is marked with a vertical dashed line.

Proposition 11.3.1. Let      (  -  )        be  a  power  series.  There  exists  a      [0,  ]  such  that
                         =0

(i) If  = 0, then the series diverges.

(ii) If  = , then the series converges absolutely for all   .

(iii) If 0 <  < , then the series converges absolutely on (, ), and diverges when | - | > .

Furthermore, if 0 <  < , then the series converges uniformly on the closed ball (, ).

    The number  is the radius of convergence. See Figure 11.6. The radius of convergence
gives a disc around  where the series converges. A power series is convergent if  > 0.

                                    series                 series
                                 converges         does not converge

                                         
                                     

Figure 11.6: Radius of convergence.

Proof. We use the real version of this proposition, Proposition 2.6.10 in volume I. Let
                                               lim sup  ||.

                                                                           
11.3. POWER SERIES AND ANALYTIC FUNCTIONS                                                                       155

If  = 0, then                 |  -  |  converges       for     all  .   If  = , then                      |  -  |
                                                                                          |
                   =0 |  |                                 1/ and                      -         =0 |  |
converges only at  = . Otherwise, let                                       =0|| |
                                                                                             converges    when

| - | < , and diverges (in fact the terms of the series do not go to zero) when | - | > .

To prove the "Furthermore," suppose 0 <  <  and   (, ). Then consider the

partial sums

                                                                            
                               ( - )  ||| - |  || .
                                                                                                                   

                           =0                   =0                      =0

If          (   -  )    converges   for     some    ,  then
     =0

                                                

                                                   ( - )

                                                =0

converges absolutely whenever | - | < | - |. Conversely, if the series diverges at ,
then it must diverge at  whenever | - | > | - |. Hence, to show that the radius of

convergence is at least some number, we simply need to show convergence at some point

by any method we know.

Example 11.3.2:    We list some series we already know:

                                                has radius of convergence 1.

                            

                        =0                      has radius of convergence .
                                                has radius of convergence 0.
                         1 
                        =0 !

                         

                            

                        =0

Example 11.3.3:    Note the difference between          1      and its power series.        Let us expand        1
                                                       1-                                                       1-
as power series around a point   1. Let                     1  ,  then
                                                           1-

              1 =  =    ( - )  =                                                 1        ( - ).
            1 -  1 - ( - ) =0 =0                                            (1 - )+1

The series         (  -    )  converges     if  and  only  if  the  series  on  the  right-hand  side    converges
and         =0

                                    lim sup  || = || = 1 .
                                                                    |1 - |

The radius of convergence of the power series is |1 - |, that is, the distance from 1 to .

The  function   1  has  a  power    series  representation        around    every        1  and  so  is  analytic  in
               1-
 \ {1}. The domain of the function is bigger than the region of convergence of the power

series representing the function at any point.

    It turns out that if a function has a power series representation converging to the
function on some ball, then it has a power series representation at every point in the ball.
We will prove this result later.
156                                                     CHAPTER 11. FUNCTIONS AS LIMITS

11.3.3 Properties of analytic functions

Proposition 11.3.4. If                           

                                    ()               ( - )

                                                 =0

is convergent in (, ) for some  > 0, then  : (, )   is continuous. In particular, analytic

functions are continuous.

Proof. For 0  (, ), pick  <  such that 0  (, ). On (, ) the partial sums
(which are continuous) converge uniformly, and so the limit  |(,) is continuous. Any
sequence converging to 0 has some tail that is completely in the open ball (, ), hence 

is continuous at 0.                                                                                

    In Corollary 6.2.13 of volume I, we proved that we can differentiate real power series
term by term. That is, we proved that if

                                                     ( - )

                                    ()

                                                 =0

converges for real  in an interval around   , then we can differentiate term by term
and obtain a series

                                                        
                      () = ( - )-1 = ( + 1)+1( - )

                              =1                        =0

with the same radius of convergence. We only proved this theorem when  is real, however,
for complex , we write  =  + , and as  and  are real

                                                            
                            ( - ) = ( - ) +  ( - ).

                        =0                 =0               =0

We apply the theorem to the real and imaginary part.
    By iterating this theorem, we find that an analytic function is infinitely differentiable:

                                                        
 ()() = ( - 1) · · · ( -  + 1)( - )- = ( +  )( +  - 1) · · · ( + 1)+ ( - ).

     =                                                  =0

In particular,                              ()() =  !  .

                                                                                                   (11.1)

The coefficients are uniquely determined by the derivatives of the function, and vice versa.

   On the other hand, just because we have an infinitely differentiable function doesn't
mean that the numbers  obtained by  = !  ()(0) give a convergent power series. There is
a  theorem,  which  we  will  not  prove,  that  given  an  arbitrary  sequence  {    } ,   there  exists

                                                                                        =1
11.3. POWER SERIES AND ANALYTIC FUNCTIONS                                       157

an infinitely differentiable function  such that  = !  ()(0) . Moreover, even if the obtained
series converges, it may not converge to the function we started with. For an example, see
Exercise 5.4.11 in volume I: The function

                              () -1/ if  > 0,
                                         0    if   0,

is infinitely differentiable, and all derivatives at the origin are zero. So its series at the
origin would be just the zero series, and while that series converges, it does not converge
to  for  > 0.

    We can apply an affine transformation    +  that converts a power series at  to a
series at the origin. That is, if

                                         we consider                         

       () = ( - ),                                        ( + ) = .

                  =0                                                        =0

Therefore, it is usually sufficient to prove results about power series at the origin. From
now on, we often assume  = 0 for simplicity.

11.3.4 Power series as analytic functions

We need a theorem on swapping limits of series, that is, Fubini's theorem for sums. For
real series this was Exercise 2.6.15 in volume I, but we have a slicker argument now.

Theorem 11.3.5 (Fubini for sums). Let {,}=1,=1 be a double sequence of complex numbers
and suppose that for every  the series

                                              converges

                                   |,|

                               =1

and furthermore that          

                                       |,|    converges.

                             =1 =1

Then                                          

                                         , =          , ,

                             =1 =1            =1 =1

where all the series involved converge.

Proof. Let  be the set {1/ :   }  {0}, and treat it as a metric space with the metric
inherited from . Define the sequence of functions  :    by

                                                           

                       (1/)              , and (0)              , .

                             =1                           =1
158                         CHAPTER 11. FUNCTIONS AS LIMITS

As the series converges, each  is continuous at 0 (since 0 is the only cluster point, they are
continuous at every point of , but we don't need that). For all   , we have

                                                                                  

                                               | ()|  |,|.

                                                                                =1

As  |,| converges (and does not depend on ), we know that

                                                                           

                                                          ()

                                                                         =1

converges uniformly on . Define

                 ()       

                             (),

                         =1

which is, therefore, a continuous function at 0. So

                 

            , =      (0) = (0) = lim (1/)

     =1 =1       =1                           

                                                       

            = lim (1/) = lim                                  ,
                                                     
                     =1                                =1 =1

                                                     

            = lim           , =                             , .                     
                 
                     =1 =1                           =1 =1

    Now we prove that once we have a series converging to a function in some interval, we
can expand the function around every point.

Theorem 11.3.6 (Taylor's theorem for real-analytic functions). Let

                  ()      

                             

                         =0

be a power series converging in (-, ) for some  > 0. Given any   (-, ), and  such that
| - | <  - ||, we have

                                            () =  ()() ( - ) .
                                                   =0 !

    The power series at  could of course converge in a larger interval, but the one above is
guaranteed. It is the largest symmetric interval about  that fits in (-, ).
11.3. POWER SERIES AND ANALYTIC FUNCTIONS                                                               159

Proof. Given  and  as in the theorem, write

                                

                    () =  ( - ) +  

                                      =0

                                                        -( - ).

                                    = 
                                      =0 =0 

Define ,       -  if                 and 0 if        >  .  Then
            

                                                                                                        (11.2)

                                   () =               ,( - ).

                                          =0 =0

Let us show that the double sum converges absolutely.

                   ,( - )                                  -( - )
            =0 =0
                                          =
                                                 =0 =0 

                                                               ||-| - |

                                          = ||
                                                 =0 =0 

                                               

                                          = || | - | + || ,

                                                 =0

and this series converges as long as (| - | + ||) <  or in other words if | - | <  - ||.

    Using Theorem 11.3.5, swap the order of summation in (11.2), and the following series
converges when | - | <  - ||:

                                    ,( - ) =                                ( - ).

             () =                                                 ,

                   =0 =0                                  =0  =0

The formula in terms of derivatives at  follows by differentiating the series to obtain

(11.1).                                                                                                  

    Note that if a series converges for real   ( - ,  + ) it also converges for all complex
numbers in (, ). We have the following corollary, which says that functions defined by

power series are analytic.

Corollary 11.3.7. For every   , if        (         -  )   converges  to    ()  in  (,  )  and       (,  ),
                                      =0
                                      (   -      )                         ()       (,   -  |   -  |).
then there exists a power series  =0                that  converges  to        in

Proof. Without loss of generality assume that  = 0. We can rotate to assume that  is real,
                                                                                ¯
but since that is harder to picture, let us do it explicitly. Let 
                                                                                || . Notice that

                                          |1/| = || = 1.
160                                                       CHAPTER 11. FUNCTIONS AS LIMITS

Therefore the series   (/)               =    -               converges     to    (/) in (0, ).            When

                      =0                     =0
 =  is real we apply Theorem 11.3.6 at || and get a series that converges to  (/) on

(||,  - ||). That is, there is a convergent series

                                                     

                                      (/) =   - ||  .

                                                    =0

Using  = ||, we find

                                                                                 
          () =  (/) = ( - ||) =    - ||/  =  ( - ) ,

                            =0                        =0                        =0

and this series converges for all  such that  - || <  - || or | - | <  - ||.                               

We proved above that a convergent power series is an analytic function where it

converges.  We  have  also  shown    before  that      1  is  analytic  outside  of    =    1.
                                                      1-
Note that just because a real analytic function is analytic on the entire real line it does

not necessarily mean that it has a power series representation that converges everywhere.

For example, the function

                                              () = 1 + 2 1

happens to be real analytic function on  (exercise). A power series around the origin
converging to  has a radius of convergence of exactly 1. Can you see why? (exercise)

11.3.5 Identity theorem for analytic functions

Lemma 11.3.8. Suppose  () =                    is  a  convergent  power  series  and   {   }        is  a  sequence
                                     =0
                                                                                               =1
of nonzero complex numbers converging to 0, such that  () = 0 for all . Then  = 0 for

every .

Proof. By continuity we know  (0) = 0 so 0 = 0. Suppose there exists some nonzero .
Let  be the smallest  such that   0. Then

                                                                        

                       () =   =   - =  + .

                                =              =                        =0

Write () =      =0 +          (this  series  converges    in  on  the   same  set  as   ).      is  continuous

and (0) =   0. Thus there exists some  > 0 such that ()  0 for all   (0, ). As
 () =  (), the only point in (0, ) where  () = 0 is when  = 0, but this contradicts

the assumption that  () = 0 for all .                                                                      

    Recall that in a metric space , a cluster point (or sometimes limit point) of a set  is a
point    such that (, ) \ {} contains points of  for all  > 0.
11.3. POWER SERIES AND ANALYTIC FUNCTIONS                                                                  161

Theorem 11.3.9 (Identity theorem). Let    be open and connected. If  :    and
 :    are analytic functions that are equal on a set   , and  has a cluster point in ,
then  () = () for all   .

    In most common applications of this theorem  is an open set or perhaps a curve.

Proof. Without loss of generality suppose  is the set of all points    such that () =  ().
Note that  must be closed as  and  are continuous.

    Suppose  has a cluster point. Without loss of generality assume that 0 is this cluster

point. Near 0, we have the expansions

                                                                        

                           () =   and () =   ,

                                       =0                               =0

which converge in some ball (0, ). Therefore the series

                                                                 

                                0 =  () - () = ( - )

                                                                =0

converges in (0, ). As 0 is a cluster point of , there is a sequence of nonzero points
{   }
            such  that    () -  ()     =   0.  Hence,   by  the  lemma  above    =    for  all  .   Therefore,
        =1
(0, )  .

Thus the set of cluster points of  is open. The set of cluster points of  is also closed: A

limit of cluster points of  is in  as it is closed, and it is clearly a cluster point of . As  is

connected, the set of cluster points of  is equal to , or in other words  = .                              

    By restricting our attention to real , we obtain the same theorem for connected open
subsets of , which are just open intervals.

11.3.6 Exercises

Exercise 11.3.1: Let                                 1

                                                            if  = ,

                                          ,            -2- if  < ,
                                                     0 if  > .

Compute (or show the limit doesn't exist):                                                          
                          

a) |,| for all , b) |,| for all , c)                             |,|, d)              ,,        e)         ,.

=1                        =1                                =1 =1              =1 =1                =1 =1

Hint: Fubini for sums does not apply, in fact, answers to d) and e) are different.

Exercise 11.3.2: Let  ()          1    Prove   that

                                1+2 .

a)  is analytic function on all of  by finding a power series for  at every   ,

b) the radius of convergence of the power series for  at the origin is 1.
162                                                                  CHAPTER 11. FUNCTIONS AS LIMITS

Exercise 11.3.3: Suppose  :    is analytic. Show that for each , there are at most finitely many zeros
of  in (0, ), that is,  -1(0)  (0, ) is finite for each .

Exercise 11.3.4: Suppose    is open and connected, 0  , and  :    is analytic. Treating  as a
function of a real  at the origin, suppose  ()(0) = 0 for all . Show that  () = 0 for all   .

Exercise 11.3.5: Suppose    is open and connected, 0  , and  :    is analytic. For real  and

, let ()  () and ()           -  ( ). Show that  and  are infinitely differentiable at the origin and
(0) = (0).

Exercise 11.3.6: Suppose a function  is analytic in some neighborhood of the origin, and that there exists an
 such that |  ()(0)|   for all . Prove that the series of  at the origin converges for all   .

Exercise 11.3.7: Suppose  ()                       with  a  radius   of  convergence  1.  Suppose   (0)  =  0,  but    is  not
                                        =0                                                                  
the zero function. Show that there exists a    and a convergent power series ()                             =0   
                                                                                                                       with

radius of convergence 1 such that  () =  () for all   (0, 1), and (0)  0.

Exercise 11.3.8: Suppose    is open and connected. Suppose that  :    is analytic,     
and  () = 0 for all     . Show that  () = 0 for all   .

Exercise 11.3.9: For    and  = 0, 1, 2, 3 . . ., define

                                             ( - 1) · · · ( - )  ! .

a) Show that the series                              ()  
                                                            =0 

     converges whenever || < 1. In fact, prove that for  = 0, 1, 2, 3, . . . the radius of convergence is , and
     for all other  the radius of convergence is 1.

b) Show that for   , || < 1, we have

                                            (1 + )  () =   (),

     meaning that  () = (1 + ).

Exercise 11.3.10: Suppose  :    is analytic and suppose that for some open interval (, )  ,  is
real valued on (, ). Show that  is real-valued on .

Exercise 11.3.11: Let  (0, 1) be the unit disc. Suppose  :    is analytic with power series

=0     .  Suppose  ||      1  for  all  .   Prove  that  for  all        ,  we  have  |  ()|      1

                                                                                                1-|| .
11.4. COMPLEX EXPONENTIAL AND TRIGONOMETRIC FUNCTIONS  163

11.4 Complex exponential and trigonometric functions

Note: 1 lecture

11.4.1 The complex exponential

Let 
                                                () 1!  .

                                                                                =0

This series converges for all   , and so by Corollary 11.3.7,  is analytic on . We notice
that (0) = 1, and that for  =   , ()  . Keeping  real, direct computation shows

                                                 () = ().

In §5.4 of volume I (or by Picard's theorem), we proved that the unique function satisfying
 =  and (0) = 1 is the exponential. In other words, for   ,  = ().

    For complex numbers , we define

                                               () = 1  .
                                                              =0 !

On the real line this new definition agrees with our previous one. See Figure 11.7. Notice
that in the  direction (the real direction) the graph behaves like the real exponential, and
in the  direction (the imaginary direction) the graph oscillates.

Figure 11.7: Graphs of the real part (left) and imaginary part (right) of the complex exponential
 = +. The -axis goes from -4 to 4, the -axis goes from -6 to 6, and the vertical axis goes
from -4  -54.6 to 4  54.6. The plot of the real exponential ( = 0) is marked in a bold line.
164                                            CHAPTER 11. FUNCTIONS AS LIMITS

Proposition 11.4.1 (Law of exponents). Let ,    be complex numbers. Then

                                 + =   .

Proof. We already know that the equality + =    holds for all real numbers  and .
For every fixed   , consider the expressions as functions of  and apply the identity
theorem (Theorem 11.3.9) to get that + =    for all   . Fixing an arbitrary   , we
get + =    for all   . Again by the identity theorem + =   for all   . 

    A simple consequence of the proposition is that   0 for all   , as  - = - = 1.
This computation means that ()-1 = -. Combining that fact with the law of exponents

gives                         () =  for all   .

A yet more complicated consequence is that we can compute the power series for the
exponential at any point   :

                                =    -  =   ( - ) .
                                                =0 !

11.4.2 Trigonometric functions and 

We can now finally define sine and cosine by the equation

                              + =  cos() +  sin() .

In fact, we define sine and cosine for all complex :

                   cos()  and sin  + - ()   - - .
                              2                                  2

    Let us use our definition to prove common properties of sine and cosine. In the process,
we also define the number .

Proposition 11.4.2. The sine and cosine functions have the following properties:

(i) For all   ,     = cos() +  sin() (Euler's formula).

(ii) cos(0) = 1, sin(0) = 0.

(iii) For all   ,

                              cos(-) = cos(),  sin(-) = - sin().

(iv) For all   ,

                    cos() = (-1) 2 ,                             (-1) 2+1.
                              =0 (2)!
                                               sin() =
                                                      =0 (2 + 1)!
11.4. COMPLEX EXPONENTIAL AND TRIGONOMETRIC FUNCTIONS               165

(v) For all         cos() = Re() and sin() = Im().

(vi) For all   ,       cos() 2 + sin() 2 = 1.

(vii) For all   ,

                       |sin()|  1, |cos()|  1.

(viii) For all   ,

                     cos() = - sin() and  sin() = cos().

(ix) For all   0,

                          sin()  .

(x) There exists an  > 0 such that cos() = 0. We define

                        2 inf{ > 0 : cos() = 0}.

(xi) For all   ,       2 = 1 and  +2 =   .

(xii) Sine and cosine are 2-periodic and not periodic with any smaller period. That is, 2 is the
      smallest number such that for all   ,

                    sin( + 2) = sin() and cos( + 2) = cos().

(xiii) The function    is a bective map from [0, 2) onto the set of    such that || = 1.
    The proposition immediately implies that sin() and cos() are real whenever  is real.

Proof. The first three items follow directly from the definition. The computation of the
power series for both is left as an exercise. As complex conjugate is a continuous function,
the definition of  implies  = ¯ . If  is real,

                           = -.

                    -- -                                      
Thus for real , cos() = 2 = 2 = Re( ) and similarly sin() = Im( ).
For real , we compute

1 =  - =   = ||2 = cos() +  sin() 2 = cos() 2 + sin() 2.

A slightly more complicated computation shows this fact for complex numbers, see
Exercise 11.4.6. In particular, is  is unimodular for real ; the values lie on the unit circle.
A square of a real number is always nonnegative:

                                         sin() 2 = 1 - cos() 2  1.
166                                           CHAPTER 11. FUNCTIONS AS LIMITS

So |sin()|  1 and similarly |cos()|  1.

    We leave the computation of the derivatives to the reader as exercises. Let us prove that
sin()   for   0. Consider  ()  - sin() and differentiate:

                                  () =   - sin() = 1 - cos()  0,

for all    as |cos()|  1. In other words,  is increasing and  (0) = 0. So  must be
nonnegative when   0 and hence, sin()  .

    Next, we claim there exists a positive  such that cos() = 0. As cos(0) = 1 > 0,
cos() > 0 for  near 0. Namely, there is some  > 0, such that cos() > 0 on [0, ). Then
sin() is strictly increasing on [0, ). As sin(0) = 0, then sin() > 0 for   (0, ). Take
  (0, ). By the mean value theorem, there is a   (, ) such that

        2  cos() - cos() = sin()( - )  sin()( - ).

As   (0, ), then sin() > 0 and so

                                     2 sin() + .

Hence there is some largest  such that cos() > 0 in [0, ), and let  be the largest such

number. By continuity, cos() = 0. In fact,  is the smallest positive  such that cos() = 0.

As mentioned,  is defined to be 2.
    As cos(/2) = 0, then sin(/2) 2 = 1. As sin is positive on (0, /2), we have sin(/2) = 1.

Hence,                              /2 = ,

and by the law of exponents,

                                = -1,  2 = 1.

So 2 = 1 = 0. The law of exponents also says

                               +2 =    2 =  

for all   . Immediately, we also obtain cos( + 2) = cos() and sin( + 2) = sin(). So
sin and cos are 2-periodic.

    We claim that sin and cos are not periodic with a smaller period. It would suffice to
show that if  = 1 for the smallest positive , then  = 2. Let  be the smallest positive 
such that  = 1. Of course,   2. By the law of exponents,

                                    /4 4 = 1.

If /4 =  + , then

                   ( + )4 = 4 - 622 + 4 +  4(2 - 2) = 1.
11.4. COMPLEX EXPONENTIAL AND TRIGONOMETRIC FUNCTIONS                       167

Then either  = 0 or 2 = 2. As /4  /2, then  = cos(/4)  0 and  = sin(/4) > 0. If

2 = 2, then 4 - 622 + 4 = -44 < 0 and in particular not equal to 1. Therefore  = 0,
in which case /4 = /2. Hence 2 is the smallest period we could choose for  and so also

for cos and sin.

    Finally, we wish to show that  is one-to-one and onto from the set [0, 2) to the set
of    such that || = 1. Suppose  =  and  > . Then (-) = 1, meaning  - 

is a multiple of 2 and hence only one of them can live in [0, 2). To show onto, pick

(, )  2 such that 2 + 2 = 1. Suppose first that ,   0. By the intermediate value
theorem, there must exist an   [0, /2] such that cos() = , and hence 2 = sin() 2.

As  and sin() are nonnegative,  = sin(). Since - sin() is the derivative of cos() and

cos(-) = cos(), then sin() < 0 for   [-/2, 0). Using the same reasoning, we obtain

that if  > 0 and   0, we can find an  in [-/2, 0), and by periodicity,   [3/2, 2) such
that cos() =  and sin() = . Multiplying by -1 is the same as multiplying by  or -.

So we can always assume that   0 (details are left as exercise).            

11.4.3 The unit circle and polar coordinates

The arclength of a curve parametrized by  : [, ]   is given by
                                                
                                                       | ()| .

                                                                       

We  have  that    parametrizes    the  circle  for    in  [0,  2).  As      = , the circumference
                                                                        
of the circle (the arclength) is

                                   2   ||  =           2

                                                               1  = 2.

                                  0                   0

    More generally,  parametrizes the circle by arclength. That is,  measures the arclength

on a circle of radius 1 by the angle in radians. So the definitions of sin and cos given above

agree with the standard geometric definitions.
    All the points on the unit circle can be achieved by  for some . Therefore, we can

write a complex number    (in so-called polar coordinates) as

                                                = 

for some   0 and   . The  is, of course, not unique as  or  + 2 gives the same
number. The law of exponents  + =    leads to a useful formula for powers and
products of complex numbers in polar coordinates:

                              (  ) =    , (  )(  ) =    (+).
168                                                      CHAPTER 11. FUNCTIONS AS LIMITS

11.4.4 Exercises

Exercise 11.4.1: Derive the power series for sin() and cos() at the origin.

Exercise 11.4.2:  Using the power series, show that for real , we have    sin()  =  cos()   and    cos()  =
                                                                                                 
- sin().

Exercise 11.4.3: Finish the proof of the argument that    from [0, 2) is onto the unit circle. In
particular, assume that we get all points of the form (, ) where 2 + 2 = 1 and   0. By multiplying by
 or -, show that we get everything, that is, even points where  < 0.

Exercise 11.4.4: Show that the exponential is onto  \ {0}, and in fact, that for every nonzero , there are
infinitely many    such that  = .

Exercise 11.4.5: Prove that for every   0 and every  > 0, there exists a    with || <  such that
1/ = .

Exercise 11.4.6: We showed cos() 2 + sin() 2 = 1 for all   . Prove that cos() 2 + sin() 2 = 1
for all   .

Exercise 11.4.7: Prove the trigonometric identities sin( + ) = sin() cos() + cos() sin() and
cos( + ) = cos() cos() - sin() sin() for all ,   .

Exercise 11.4.8: Define sinc()  sin()                    1. Show that sinc is analytic and compute
its power series at zero.
                                   for   0 and sinc(0)

    Define the hyperbolic sine and hyperbolic cosine by

                          sinh()       - - ,     cosh()                  + - .
                                          2                                 2

Exercise 11.4.9: Derive the power series at the origin for the hyperbolic sine and cosine.

Exercise 11.4.10: Show

a) sinh(0) = 0, cosh(0) = 1.

b)  For    ,      sinh()  =   cosh()  and    cosh()      = sinh().
                                           

c) cosh() > 0 for all    and show that sinh() is strictly increasing and bective from  to .

d) cosh() 2 = 1 + sinh() 2 for all .

Exercise 11.4.11: Define tan() cos() sin() as usual.
 a) Show that for   (-/2, /2) both sin and tan are strictly increasing, and hence sin-1 and tan-1 exist

     when we restrict to that interval.

b) Show that sin-1 and tan-1 are differentiable and that  sin-1() =  11-2 and  tan-1() = 1+12 .

c) Using the finite geometric sum formula show

                                                1  (-1)   = 2+1
                                tan-1() =                =0 2 + 1
                                           0 1+

     converges for all -1    1 (including the end points). Hint: Integrate the finite sum, not the series.

d) Use this to show that          1 - 1 + 1  - · · · = (-1) =  .

                                      35             =0 2 + 1 4
11.5. MAXIMUM PRINCIPLE AND THE FUNDAMENTAL THEOREM OF ALGEBRA169

11.5 Maximum principle and the fundamental theorem of
        algebra

Note: half a lecture, optional

    In this section we study the local behavior of polynomials, and analytic functions in
general, and the growth of polynomials as  goes to infinity. As an application we prove
the fundamental theorem of algebra: Any nonconstant polynomial has a complex root.

Lemma 11.5.1. Let  > 0, let () be a nonconstant complex polynomial, or more generally a
nonconstant power series converging in (0, ), and suppose (0)  0. Then there exists a
  (0, ) such that |()| < |(0)|.

Proof. We prove this lemma for a polynomial and leave the general case as Exercise 11.5.1.
Without loss of generality assume that 0 = 0 and (0) = 1. Write

                                 () = 1 +   + +1+1 + · · · +   ,

where   0. Pick  such that   = -||, which we can do by the discussion on
trigonometric functions. Suppose  > 0 is small enough such that 1 - || > 0. We have

                         (  ) = 1 -  || +  +1+1 (+1) + · · · +     .

So
    (  ) -  +1+1 (+1) + · · · +      (  ) -  +1+1 (+1) - · · · -    
                                                          = 1 - || = 1 - ||.

In other words,

                     (  )  1 -   || -  +1 (+1) + · · · + --1   .

For small enough , the expression in the parentheses is positive as || > 0. Hence,
() < 1 = (0).
                                                                              

    What the lemma says is that the only minima the modulus of analytic functions has
are precisely at the zeros. It is sometimes called the minimum modulus principle. If  is
analytic and nonzero at a point, then 1/ is analytic near that point. Applying the lemma
and the identity theorem, one obtains the maximum modulus principle, or sometimes just the
maximum principle.

Theorem 11.5.2 (Maximum modulus principle). If    is open and connected,  :   
is analytic, and |  ()| attains a relative maximum at 0  , then  is constant.

    The details of the proof is left as Exercise 11.5.2.
170                                           CHAPTER 11. FUNCTIONS AS LIMITS

Remark 11.5.3. The lemma (and the maximum principle) does not hold if we restrict to the
real numbers. For example, 2 + 1 has a minimum at  = 0, but no zero there. There is
a  arbitrarily close to 0 such that |2 + 1| < 1, but this  is necessarily not real. Letting
 =  for small  > 0 works.

    The moral of the story is that if (0) = 1, then very close to 0, the series (or polynomial)
looks like 1 + , and 1 +  has no minimum at the origin. All the higher powers of 
are too small to make a difference. For polynomials, we find similar behavior at infinity.

Lemma 11.5.4. Let () be a nonconstant complex polynomial. Then for an  > 0, there exists
an  > 0 such that |()|   whenever ||  .

Proof. Write () = 0 + 1 + · · · +  and suppose that   1 and   0. Suppose
||   (so also ||-1  -1). We estimate:

     |()|  | | - |0| - |1| - · · · - |-1-1|
             = || || - |0| ||- - |1| ||-+1 - · · · - |-1| ||-1
               || - |0|- - |1|1- - · · · - |-1|-1 .

Then the expression in parentheses is eventually positive for large enough . In particular,
for large enough  we get that this expression is greater than 2 || , and so

     |()|   || .
                     2

Therefore, we can pick  large enough to be bigger than a given .  

    This second lemma does not generalize to analytic functions, even those defined on the
entire plane . The function cos() is a counterexample. We had to look at the term with

the largest degree, and we only have such a term for a polynomial. In fact, something that

we will not prove is that an analytic function defined on all of  satisfying the conclusion

of the lemma must be a polynomial.
    The moral of the story here is that for very large || (far away from the origin) a

polynomial of degree  really looks like a constant multiple of .

Theorem 11.5.5 (Fundamental theorem of algebra). Let () be a nonconstant complex
polynomial, then there exists a 0   such that (0) = 0.

Proof. Let  inf |()| :    . Find an  such that for all  with ||  , we

have |()|   + 1. Therefore, every  with |()| close to  must be in the closed ball

(0, ) =    : ||   . As |()| is a continuous real-valued function, it achieves its

minimum on the compact set (0, ) (closed and bounded) and this minimum must be .

So there is a 0  (0, ) such that |(0)| = . As that is a minimum of |()| on , then
by the first lemma above, we have |(0)| = 0.
                                                                  

    The fundamental theorem also does not generalize to analytic functions. The exponential
 is an analytic function on  with no zeros.
11.5. MAXIMUM PRINCIPLE AND THE FUNDAMENTAL THEOREM OF ALGEBRA171

11.5.1 Exercises

Exercise 11.5.1: Prove Lemma 11.5.1 for an analytic function. That is, suppose that () is a nonconstant
power series converging in (0, ).

Exercise 11.5.2: Use Lemma 11.5.1 for analytic functions to prove Theorem 11.5.2.

Exercise 11.5.3: Let    be open and 0  . Suppose  :    is analytic and  (0) = 0. Show that
there exists an  > 0 such that either  ()  0 for all  with 0 < || <  or  () = 0 for all   (0, ). In
other words, zeros of analytic functions are isolated. Of course, same holds for polynomials.

    A rational function is a function  () () () where  and  are polynomials and  is not identically
zero. A point 0   where  (0) = 0 (and therefore (0) = 0) is called a zero. A point 0   is
called an singularity of  if (0) = 0. As all zeros are isolated and so all singularities of rational
functions are isolated and so are called an isolated singularity. An isolated singularity is called
removable if lim0  () exists. An isolated singularity is called a pole if lim0|  ()| = . We say
 has pole at  if

                                                       lim |  ()| = ,

                                                                           

that is, if for every  > 0 there exists an  > 0 such that |  ()| >  for all  with || > .

Exercise 11.5.4: Show that a rational function which is not identically zero has at most finitely many zeros
and singularities. In fact, show that if  is a polynomial of degree  > 0 it has at most  zeros.
Hint: If 0 is a zero of , then without loss of generality assume 0 = 0. Then use induction.

Exercise 11.5.5: Prove that if 0 is a removable singularity of a rational function  ()  () () , then there
exist polynomials  and  such that (0)  0 and  () = () () .
Hint: Without loss of generality assume 0 = 0.

Exercise 11.5.6: Given a rational function  with an isolated singularity at 0, show that 0 is either
removable or a pole.
Hint: See the previous exercise.

Exercise 11.5.7: Let  be a rational function and    is the set of the singularities of  . Prove that  is
equal to a polynomial on  \  if and only if  has a pole at infinity and all the singularities are removable.

Hint: See previous exercises.
172                                                                 CHAPTER 11. FUNCTIONS AS LIMITS

11.6 Equicontinuity and the Arzelà-Ascoli theorem

Note: 2 lectures

    We would like an analogue of Bolzano-Weierstrass. Something to the tune of "every
bounded sequence of functions (with some property) has a convergent subsequence."
Matters are not as simple even for continuous functions. Not every bounded sequence in
the metric space  [0, 1],  has a convergent subsequence.

Definition 11.6.1. Let  be a set. Let  :    be functions in a sequence. We say that
{   }                                                                                 
            is  pointwise     bounded  if  for  every          ,   there  is  an            such    that
        =1

                                           | ()|   for all   .

We   say    that  {   }       is  uniformly     bounded  if  there  is  an            such  that

                          =1

                                  | ()|   for all    and all   .

    If  is a compact metric space, then a sequence in ( , ) is uniformly bounded if it is
bounded as a set in the metric space ( , ) using the uniform norm.

Example 11.6.2: There exist sequences of continuous functions on [0, 1] that are uniformly

bounded but contain no subsequence converging even pointwise. Let us state without
proof that () sin(2) is one such sequence. Below we will show that there must
always exist a subsequence converging at countably many points, but [0, 1] is uncountable.

Example 11.6.3: The sequence ()  of continuous functions on [0, 1] is uniformly
bounded, but contains no subsequence that converges uniformly, although the sequence
converges pointwise (to a discontinuous function).

Example 11.6.4:      The      sequence       {   }       of  functions  in      [0, 1],     given by ()       3
                                                                                                            1+ 4  2
                                                     =1
converges pointwise to the zero function (obvious at  = 0, and for  > 0, we have

  3         1 ).     As for each    ,  {     ()}         converges to 0,    it  is  bounded  so  {    }     is pointwise
1+ 4  2
                                                    =1                                                  =1
bounded.

    Via calculus, we find that the maximum of  on [0, 1] occurs at the critical point  = 1/2:

                                                 [0,1] =  (1/2) = /2.

So lim [0,1] = , and this sequence is not uniformly bounded.

    When the domain is countable, we can locate a subsequence converging at least
pointwise. The proof uses a very common and useful diagonal argument.

Proposition 11.6.5. Let  be a countable set and  :    give a pointwise bounded sequence
                        {   }
of  functions.    Then              has   a  subsequence     that  converges    pointwise.
                                =1
11.6. EQUICONTINUITY AND THE ARZELÀ-ASCOLI THEOREM                                                                                                                                     173

Proof. Let 1, 2, 3, . . . be an enumeration of the elements of . The sequence { (1)}=1
is  bounded                and       hence        we     have          a   subsequence                        of  {  } ,         which         we  denote               by   {  1, }=1,
                                          )}
                                                                                                                          =1
such     that         {  1        (          =1   converges.                  Next            {  1       (       )}      is  bounded           and      so      {  1         }     has  a
                             ,         1                                                            ,         2                                                         ,  
                                                                                                                    =1                                                         =1
subsequence                     {  2        }     such          that       {  2       (       )}            converges.           Note        that    {  2       (       )}         is  still
                                       ,                                         ,         2                                                               ,         1
                                              =1                                                 =1                                                                        =1
convergent.
                                                                                         } ,                                                                                 } ,
    In      general,              we        have  a     sequence              {  ,                     which         is  a   subsequence           of   {     -1,                  such
                             )}                                                            =1                                                  }                               =1
that     {            (                     converges             for         =       1, 2, . . . , .             We     let     {                   be       a    subsequence
                 ,              =1                                            )}                                                      +1  ,      =1
    {            }                                {               (
of          ,              such           that           +1  ,           +1      =1        converges                 (and    hence    it       converges             for     all       for
                   =1
 = 1, 2, . . . ,  + 1). Rinse and repeat.

    If  is finite, we are done as the process stops at some point. If  is countably infinite,
                                                  , }=1.                                                                                                                   } .
we  pick         the     sequence           {                        This     is      a  subsequence                 of     the  original      sequence              {                 For
                                                                                                                                                                                =1
every , the tail { ,}= is a subsequence of { ,}=1 and hence for any  the sequence
{ ,()}=1 converges.                                                                                                                                                                     

    For larger than countable sets, we need the functions of the sequence to be related.
When we look at continuous functions, the concept we need is equicontinuity.

Definition 11.6.6. Let ( , ) be a metric space. A set  of functions  :    is uniformly
equicontinuous if for every  > 0, there is a  > 0 such that if ,    with (, ) < , we

have
                                    |  () -  ()| <  for all   .

    Notice that functions in a uniformly equicontinuous sequence are all uniformly contin-

uous. It is not hard to show that a finite set of uniformly continuous functions is uniformly
equicontinuous. The definition is really interesting if  is infinite.

    Just as for continuity, one can define equicontinuity at a point. That is,  is equicontinuous
at    if for every  > 0, there is a  > 0 such that for    with (, ) < , we have
|  () -  ()| <  for all   . We will only deal with compact  here, and one can prove
(exercise) that for a compact metric space , if  is equicontinuous at every   , then it

is uniformly equicontinuous. For simplicity we stick to uniform equicontinuity.

Proposition                  11.6.7.        Suppose             (, )          is      a    compact               metric     space,             ( , ),                   and     { }
                                                         }
converges           uniformly,              then     {                 is  uniformly             equicontinuous.                                                                       =1
                                                             =1

Proof.      Let            >    0 be given.             As      {     }             converges               uniformly,        there       is   an                such that for

all                                                                       =1

                                                        | () -  ()| < /3 for all   .

As  is compact, every continuous function is uniformly continuous. So { 1, 2, . . . ,  } is

a finite set of uniformly continuous functions. And so, as we mentioned above, the set is

uniformly equicontinuous. Hence there is a  > 0 such that

                                                                         | () - ()| < /3 < 

whenever (, ) <  and 1    .
    Take  > . For (, ) < , we have

| () - ()|  | () -  ()| + |  () -  ()| + |  () - ()| < /3 + /3 + /3 = . 
174                                                             CHAPTER 11. FUNCTIONS AS LIMITS

Proposition 11.6.8. A compact metric space ( , ) contains a countable dense subset, that is,
there exists a countable    such that  = .

Proof. For each    there are finitely many balls of radius 1/ that cover  (as  is
compact). That is, for every , there exists a finite set of points ,1, ,2, . . . , , such that

                                                     

                                             = (, , 1/).

                                                    =1

Let                                             The set       is countable as it is          a countable union of

                =1{,1, ,2, . . . , , }.
finite sets. For every    and every  > 0, there exists an  such that 1/ <  and an

,   such that

                                      (, , 1/)  (, , ).

Hence   , so  = , and  is dense.                                                                                                 

    We are now ready for the main result of this section, the Arzelà-Ascoli theorem about
existence of convergent subsequences.

Theorem         11.6.9  (Arzelà-Ascoli).    Let  (, )     be   a  compact          metric  space,  and    let  { }               be

                                                                                                                      =1
pointwise bounded and uniformly equicontinuous sequence of functions   ( , ). Then
{   }                                    {   }
            is  uniformly  bounded  and              contains  a  uniformly        convergent  subsequence.
        =1                                       =1

    Basically, a uniformly equicontinuous sequence in the metric space ( , ) that is

pointwise bounded is bounded (in ( , )) and furthermore contains a convergent

subsequence in ( , ).                                                                                              }

    As  we      mentioned  before,  as      is  compact,    it  is  enough         to  just  assume     that   {       =1        is

equicontinuous as uniform equicontinuity is automatic via an exercise.

Proof. We first show that the sequence is uniformly bounded. By uniform equicontinuity,
there is a  > 0 such that for all    and all   ,

                                         (, )         -1  ( (), 1)              .

                                                     

The space  is compact, so there exist 1, 2, . . . ,  such that

                                                          

                                                 = (, ).

                                                        =1

As  {   }       is  pointwise  bounded      there  exist  1,    2,  .  .  .  ,     such      that  for    =  1,  2,  .  .  .  ,  ,

            =1

                                         | ()|   for all .

    Named after the Italian mathematicians Cesare Arzelà (1847-1912), and Giulio Ascoli (1843-1896).
11.6. EQUICONTINUITY AND THE ARZELÀ-ASCOLI THEOREM                                                                                      175

Let  1 + max{1, 2, . . . , }. Given any   , there is a  such that   ( , ).
                                                             -1      ( (), 1)
Therefore, for all , we have                                                        , or in other words
                                                             

                                                            | () - ()| < 1.

By the reverse triangle inequality,

                                                     | ()| < 1 + | ()|  1 +   .

As       was      arbitrary,            {    }       is  uniformly       bounded.

                                                 =1
       Next, pick a countable dense subset   . By Proposition 11.6.5, we find a subsequence
         }                                                                                                                       { }
{              that        converges           pointwise    on    .      Write            for  simplicity.    The    sequence
           =1                                                                                                                            =1

is uniformly equicontinuous. Let  > 0 be given, then there exists a  > 0 such that for all

   and all   ,                                                                -1

                                                            (, )                  ((), /3)     .

By density of  and because  is fixed, every    is in (, ) for some   . By
compactness of , there is a finite subset {1, 2, . . . , }   such that

                                                                               

                                                                      = (, ).

                                                                             =1

As     {1  ,   2  ,  .  .  .  ,      }  is  a  finite  set  and   {      }     converges  pointwise       on  ,  there   exists  a  single

 such that for all ,   ,                                                   =1

                                            |() - ()| < /3 for all  = 1, 2, . . . , .

       Let    be arbitrary. There is some  such that   ( , ) and so for all   ,

                                                            | () -  ()| < /3.

So for ,   ,

               |() - ()|  |() - ()| + |() - ()| + |() - ()|
                                   < /3 + /3 + /3 = .

Hence,        { }                is  uniformly         Cauchy.       By  completeness     of   ,  it  is  uniformly  convergent.           

                      =1

Corollary 11.6.10. Let ( , ) be a compact metric space. Let   ( , ) be a closed, bounded
and uniformly equicontinuous set. Then  is compact.

    The theorem says that  is sequentially compact and that means compact in a metric
space. Recall that the closed unit ball in  [0, 1],  , and therefore also in  [0, 1],  , is

not compact. Hence it cannot be a uniformly equicontinuous set.

Corollary            11.6.11.           Suppose        { }        is  a  sequence   of  differentiable    functions  on  [, ],   {      }
                                                                                                                                        =1
                                                              =1
is uniformly bounded, and there is an 0  [, ] such that { (0)}=1 is bounded. Then there
exists   a    uniformly              convergent        subsequence       {    } .

                                                                                =1
176                                                 CHAPTER 11. FUNCTIONS AS LIMITS

Proof.  The trick is to use the mean value theorem.  If    is  the     uniform    bound  on  {     } ,

then by the mean value theorem for every                                                             =1

                    | () - ()|  | - | for all ,   .

All the  are Lipschitz with the same constant and hence the sequence is uniformly

equicontinuous.
    Suppose | (0)|  0 for all . For all   [, ],

        | ()|  | (0)| + | () - (0)|  0 + | - 0|  0 + ( - ).

So  {   }       is  uniformly  bounded.  We  apply  Arzelà-Ascoli  to  find  the  subsequence.     

            =1

    A classic application of the corollary above to Arzelà-Ascoli in the theory of differential
equations is to prove the Peano existence theorem, that is, the existence of solutions to
ordinary differential equations. See Exercise 11.6.11 below.

    Another application of Arzelà-Ascoli using the same idea as the corollary above is the
following. Take a continuous  : [0, 1] × [0, 1]  . For every    [0, 1],  define

                                 ()          1
                                                   () (, ) .

                                               0

In exercises to earlier sections you have shown that  is a linear operator on  [0, 1],  .
Via Arzelà-Ascoli, we also find (exercise) that the image of the unit ball of functions

                     (0, 1) =     [0, 1],  :   [0,1] < 1

has compact closure, usually called relatively compact. Such an operator is called a compact
operator. And they are very useful. Generally operators defined by integration tend to be
compact.

11.6.1 Exercises

Exercise 11.6.1: Let  : [-1, 1]   be given by ()  1+()2 . Prove that the sequence is uniformly
bounded, converges pointwise to 0, yet there is no subsequence that converges uniformly. Which hypothesis of
Arzelà-Ascoli is not satisfied? Prove your assertion.

Exercise 11.6.2: Define  :    by () 1 (-)2+1 . Prove that this sequence is uniformly bounded,
uniformly equicontinuous, the sequence converges pointwise to zero, yet there is no subsequence that converges
uniformly. Which hypothesis of Arzelà-Ascoli is not satisfied? Prove your assertion.

Exercise 11.6.3: Let ( , ) be a compact metric space,  > 0, 0 <   1, and suppose  :    are
functions such as () - ()  (, ) for all ,    and   . Suppose also that there is a point
   such that () = 0 for all . Show that there exists a uniformly convergent subsequence converging
to an  :    that also satisfies  () = 0 and  () -  ()  (, ).
11.6. EQUICONTINUITY AND THE ARZELÀ-ASCOLI THEOREM                                                                                      177

Exercise 11.6.4:       Let  :  [0, 1],                [0, 1],  be the operator given by

                                                         ()           
                                                                             () .

                                                                        0

(That  is linear and that   is continuous follows from linearity of the integral and the fundamental theorem
of calculus.)

a) Show that  takes the unit ball centered at 0 in  [0, 1],  into a relatively compact set (a set with
   compact closure). That is,  is a compact operator.

   Hint: See Exercise 7.4.20 in volume I.

b) Let    [0, 1],  the closed unit ball, prove that the image () is not closed (though it is relatively
    compact).

Exercise 11.6.5: Given    [0, 1] × [0, 1],  , define the operator  :  [0, 1],    [0, 1],  by

                                                      ()       1
                                                                     () (, ) .

                                                                 0

Show that  takes the unit ball centered at 0 in  [0, 1],  into a relatively compact set (a set with compact
closure). That is,  is a compact operator.

Hint: See Exercise 7.4.20 in volume I.
Note: That  is a well-defined linear operator was proved in Exercise 8.1.6.

Exercise 11.6.6: Suppose 1   is the unit circle, that is, the set where || = 1. Suppose the continuous
functions  : 1   are uniformly bounded. Let  : [0, 1]  1 be a parametrization of 1, and (, )
a continuous function on (0, 1) × 1 (here (0, 1)   is the closed unit ball). Define the functions
 : (0, 1)   by the path integral (see §9.2)

                                                     

                                            () := () (, ) ().

                                                                               

Show   that  {  }        has  a  uniformly       convergent   subsequence.

                     =1

Exercise 11.6.7:       Suppose   (  ,  )     is  a  compact   metric  space,   {   }       a  uniformly       equicontinuous      sequence  of

                 ( ,   ).                 {   }                                        =1

functions    in              Suppose              =1  converges  pointwise.       Show     that  it  converges    uniformly.

Exercise     11.6.8:   Suppose      that    { }        is  a  uniformly     equicontinuous           uniformly    bounded         sequence  of

                                                   =1
2-periodic functions  :   . Show that there is a uniformly convergent subsequence.

Exercise 11.6.9:       Show   that  for  a  compact    metric  space  ,     a  sequence    {   }       that   is  equicontinuous  at    every

   is uniformly equicontinuous.                                                                    =1

Exercise 11.6.10: Define  : [0, 1]   by () (2+), which gives a uniformly equicontinuous

uniformly bounded sequence. Prove a stronger conclusion than that of Arzelà-Ascoli for this sequence. Let
                                                                                                                      { }
   be given, and define ()                        (2+).       Show    that  there  exists     a  subsequence      of              converging
                                                                                                                             =1
uniformly to .
                                                                                                 { }
Hint:  Feel  free  to  use  the  Kronecker          density   theorem:      The   sequence                    is  dense  in  the  unit  circle.
                                                                                                          =1

Named after the German mathematician Leopold Kronecker (1823-1891).
178                                                                CHAPTER 11. FUNCTIONS AS LIMITS

Exercise 11.6.11: Prove the Peano existence theorem (note the weaker hypotheses than Picard, but also the
lack of uniqueness in this theorem):

    Theorem: Suppose  :  ×    is a continuous function where ,    are closed bounded
intervals, let  and  be their interiors, and let (0, 0)   × . Then there exists an  > 0 and a
differentiable function  : [0 - , 0 + ]    , such that

                                       () =  ,  () and  (0) = 0.

    Use the following outline:

a) We wish to define the Picard iterates, that is, set 0() 0, and

                                     +1()                    
                                                        0 +  , () .

                                                                   0

    Prove that there exists an  > 0 such that  : [0 - , 0 + ]   is well-defined for all . Hint:  is

     bounded (why?).

b)  Show  that  { }        is  equicontinuous    and  bounded,     in  fact  it  is  Lipschitz  with  a  uniform   Lipschitz
                                                                                                                   }
                       =1                                                                                 {  
                                                                                                                     =1
     constant.  Arzelà-Ascoli  then  says  that  there  exists  a  uniformly     convergent  subsequence                 .

c) Prove  ,  () =1  converges uniformly on [0 - , 0 + ]. Hint:  is uniformly continuous
    (why?).

d) Finish the proof of the theorem by taking the limit under the integral and applying the fundamental
    theorem of calculus.
11.7. THE STONE-WEIERSTRASS THEOREM                                                                               179

11.7 The Stone-Weierstrass theorem

Note: 3 lectures

11.7.1 Weierstrass approximation

Perhaps surprisingly, even a very badly behaved continuous function is a uniform limit
of polynomials. We cannot really get any "nicer" functions than polynomials. The idea
of the proof is a very common approximation or "smoothing" idea (convolution with an
approximate delta function) that has applications far beyond pure mathematics.

Theorem 11.7.1 (Weierstrass approximation theorem). If  : [, ]   is continuous, then
                             {   }
there   exists  a  sequence              of  polynomials  converging  to    uniformly      on  [, ].  Furthermore,
                                     =1
if  is real-valued, we can find  with real coefficients.

Proof. For   [0, 1], define

                             ()  ( - ) +  -  () -   () -  () .

If  we  prove      the  theorem  for     and  find  the  sequence  {   }       for  ,  it  is  proved  for    as  we

                                                                           =1
simply composed with an invertible affine function and added an affine function to  : We

reverse the process and apply that to our , to obtain polynomials approximating  . The
function  is defined on [0, 1] and (0) = (1) = 0. For simplicity, assume that  is defined

on  by letting () 0 if  < 0 or  > 1. This extended  is continuous.

    Define                       1                       -1

                         (1 - 2)  , () (1 - 2) .
                                 -1
The choice of  is so that -1  1 ()  = 1. See Figure 11.8.

                        6

                        5

                        4

                        3

                        2

                        1

                        0  -1

                                                             0                             1

    Figure 11.8: Plot of the approximate delta functions  on [-1, 1] for  = 5, 10, 15, 20, . . . , 100
    with higher  in lighter shade.
180                        CHAPTER 11. FUNCTIONS AS LIMITS

    The functions  are peaks around 0 (ignoring what happens outside of [-1, 1]) that
get narrower and taller as  increases, while the area underneath is always 1. A classic

approximation idea is to do a convolution integral with peaks like this: For for   [0, 1], let

            1                     
                 ()( - ) 
        ()                 =      -  ()( - )  .
              0

The idea of this convolution is that we do a "weighted average" of the function  around
the point  using  as the weight. See Figure 11.9.

                           6
                           5
                           4
                           3
                           2
                           1
                           0
                         -1 0.5 1

Figure 11.9: For  = 0.3, the plot of 100( - ) (light gray peak centered at ), some continuous
function () (the jagged line) and the product ()100( - ) (the bold line).

    As  is a narrow peak, the integral mostly sees the values of  that are close to  and

it does the weighted average of them. When the peak gets narrower, we compute this
average closer to  and we expect the result to get closer to the value of (). Really, we are
approximating what is called a delta function (don't worry if you have not heard of this
concept), and functions like  are often called approximate delta functions. We could do

this with any set of polynomials that look like narrower and narrower peaks near zero.
These just happen to be the simplest ones. We only need this behavior on [-1, 1] as the
convolution sees nothing further than this as  is zero outside [0, 1].

    Because  is a polynomial, we write

            ( - ) = 0() + 1()  + · · · + 2() 2 ,

where () are polynomials in , and hence integrable functions. So

        1

     () = ()( - ) 

        0            1                        1                                                2 .
                          ()1()    +··· +
        1                                                                                     )  =
                       0                              0
     =     ()0()  +                                                              ()2() 

        0                                                                        " -   ()( -

The delta function is not actually a function, it is a "thing" that should give                     ()."
11.7. THE STONE-WEIERSTRASS THEOREM                                                                 181

In other words,  is a polynomial in . If () is real-valued, then the functions ()()

are real-valued and  has real coefficients, proving the "furthermore" part of the theorem.
                                    {   }
We   still  need  to  prove  that               converges  to  .  We  start  with  estimating  the  size
                                            =1
of . For   [0, 1], we have that 1 -   1 - 2. We estimate

                      -1 2  1                   2  1 
                       = (1 -  )  = 2 (1 -  ) 
                                -1              0
                                                1
                                                 2 (1 - )  = 2 .
                                                                      +1
                                                0

So     +1     .   Let us see how small          is if we ignore some small interval around
        2
the origin, where the peak is. Given any  > 0,  < 1, we have that for all  such that

  ||  1,                        ()  (1 - 2)  (1 - 2) ,

because  is increasing on [-1, 0] and decreasing on [0, 1]. By the ratio test, (1 - 2)

goes to 0 as  goes to infinity.

The function  is even, () = (-), and  is zero outside of [0, 1]. So for   [0, 1],

              1                      1-                               1

     () = ()( - )  =                            ( + )(-)  = ( + )() .
                                    -                                 -1
              0

Let  > 0 be given. As [-1, 2] is compact and  is continuous on [-1, 2], we have that  is
uniformly continuous. Pick 0 <  < 1 such that if | - | <  (and ,   [-1, 2]), then

                                               |() - ()| <  .
                                                                    2

Let  be such that |()|   for all . Let  be such that for all   ,

                                              4(1 - 2) <  .
                                                                     2

Note that -1  1 ()  = 1 and ()  0 on [-1, 1]. So for    and every   [0, 1],

                         1                                 1

     |() - ()| = ( + )()  - () () 
                         -1                                -1
                         1

                      =   1 -1  ( + ) - () () 

                       |( + ) - ()|() 
                         -1
                          -                                       

                      = |( + ) - ()|()                         + |( + ) - ()|() 

                            -1                                       -

                          1

                         + |( + ) - ()|() 

                               

Do note that the functions  depend on , so the coefficients of  change as  changes.
182                                          CHAPTER 11. FUNCTIONS AS LIMITS

                   -  2 ()  +     1 ()  + 2 () 
                                -1         2 -   
                   2(1 - 2)(1 - ) +  + 2(1 - 2)(1 - )
                                      2
                  < 4(1 - 2) + 2 < . 

    A convolution often inherits some property of the functions we are convolving. In our
case the convolution  inherited the property of being a polynomial from . The same
idea of the proof is often used to get other properties. If  or  is infinitely differentiable,
so is . If  or  is a solution to a linear differential equation, so is . Etc.

    Let us note an immediate application of the Weierstrass theorem. We have already seen

that countable dense subsets can be very useful.

Corollary 11.7.2. The metric spaces  [, ],  and  [, ],  each contain a countable dense
subset.

Proof. Without loss of generality, consider only  [, ],  (why?). Real polynomials
are dense in  [, ],  by Weierstrass. If we show that every real polynomial can be

approximated by polynomials with rational coefficients, we are done. Indeed, there are

only countably many rational numbers and so there are only countably many polynomials

with rational coefficients (a countable union of countable sets is countable).
    Further without loss of generality, suppose [, ] = [0, 1]. Let

                                     ()   

                                              

                                         =0

be a polynomial of degree  where   .         Given  > 0, pick    such that
|  -  |      
         <        Then  if  we  let      
            +1 .

                                     ()       ,

                                         =0

we have

      |() - ()| = ( - )  | - |      | - | <  = . 
                           =0 =0 =0 =0  + 1

Remark 11.7.3. While we will not prove so, the corollary above implies that  [, ],  has
the same cardinality as , which may be a bit surprising. The set of all functions [, ]  
has cardinality strictly greater than the cardinality of , it has the cardinality of the power
set of . So the set of continuous functions is a very tiny subset of the set of all functions.

    Warning! The fact that every continuous function  : [-1, 1]   (or any interval [, ])
can be uniformly approximated by polynomials

                                                                           

                                                           

                                                                         =0
11.7. THE STONE-WEIERSTRASS THEOREM                                                                        183

does not mean that every continuous  is analytic, that is, equal to a power series

                                              

                                                  .

                                             =0

An analytic function is infinitely differentiable, however, the function || is continuous and

near the origin approximable by polynomials, and so provides a counterexample.

The key distinction is that the polynomials coming from the Weierstrass theorem are not

the partial sums of a power series. For each one, the coefficients  above can be completely
                                                                           {        } .
different--they  do     not  need  to  come  from     a  single  sequence       
                                                                                      =1

    Interestingly, to generalize Weierstrass, we will only need to use it to approximate the
absolute value function by polynomials without a constant term.

Corollary  11.7.4.  Let  [- ,  ]   be  an  interval.  Then  there  is  a  sequence  of  real  polynomials  {   }

that converges uniformly to || on [-, ] and such that (0) = 0 for all .                                            =1

Proof. As  () || is continuous and real-valued on [-, ], the Weierstrass theorem
                                             {   }
gives  a  sequence  of  real  polynomials                that  converges   to       uniformly  on  [-, ].  Let
                                                     =1

                                           () () - (0).

Obviously (0) = 0.
    Given  > 0, let  be such that for   , we have () - || < /2 for all   [-, ].

In particular, |(0)| < /2. Then for   ,

          () - || = () - (0) - ||  () - || + |(0)| < /2 + /2 = .                                           

    Generalizing the corollary, we can make the polynomials from the Weierstrass theorem
be equal to our target function at one point, not just for ||, but that's the one we will need.

It is also possible (see Exercise 11.7.14) to make the polynomials equal at finitely many

points by subtracting not a constant but a properly crafted polynomial.

11.7.2 Stone-Weierstrass approximation

We want to abstract away what is not really necessary and prove a general version of the
Weierstrass theorem, the Stone-Weierstrass theorem. Polynomials are dense in the space
of continuous functions on a compact interval. What other kind of families of functions
are also dense? And if the domain is an arbitrary metric space, then we no longer have
polynomials to begin with.

     Named after the American mathematician Marshall Harvey Stone (1903-1989), and the German
mathematician Karl Theodor Wilhelm Weierstrass (1815-1897).
184  CHAPTER 11. FUNCTIONS AS LIMITS

Definition 11.7.5. A set A of complex-valued functions  :    is said to be an algebra
(sometimes complex algebra or algebra over ) if for all  ,   A and   , we have

   (i)  +   A.

  (ii)    A.

 (iii)    A.

A real algebra or an algebra over  is a set of real-valued functions that satisfies the three
properties above for   .

    We are interested in the case when  is a compact metric space. Then ( , ) and
( , ) are metric spaces. Given a set A  ( , ), the set of all uniform limits is the
metric space closure A. When we talk about closure of an algebra from now on we mean
the closure in ( , ) as a metric space. Same for ( , ).

    The set P of all polynomials is an algebra in  [, ],  , and we have shown that its
closure P =  [, ],  . That is, it is dense. That is the sort of result that we wish to prove.

    We leave the following proposition as an exercise.

Proposition 11.7.6. Suppose  is a compact metric space. If A  ( , ) is an algebra, then the
closure A is also an algebra. Similarly for a real algebra in ( , ).

    We distill the properties of polynomials that are sufficient for an approximation theorem.

Definition 11.7.7. Let A be a set of complex-valued functions defined on a set .

   (i) A separates points if for every ,    with   , there is an   A such that
         ()   ().

  (ii) A vanishes at no point if for every    there is an   A such that  ()  0.

Example 11.7.8: Given any    (or   ), the set P of polynomials in one variable
separates points and vanishes at no point on . That is, 1  P, so it vanishes at no point.
And for ,   ,   , take  () . Then  () =    =  (). So P separates points.

Example 11.7.9: The set of functions of the form

                                                                                 

                                            () = 0 +  cos()

                                                                               =1

is an algebra, which follows by the identity cos() cos() = 2 cos((+)) + 2 cos((-)) . The
algebra vanishes at no point as it contains a constant function. It does not separate points
if the domain is an interval [-, ], as  (-) =  () for all . It does separate points if the
domain is [0, ]; cos() is one-to-one on [0, ].

Example 11.7.10: The set P of real polynomials with no constant term is an algebra that
vanishes at the origin. Clearly, any function in the closure of P also vanishes at the origin,
so the closure of P cannot be  [0, 1],  .

    Similarly, the set of constant functions is an algebra that does not separate points.
Uniform limits of constants are constants, so we also do not obtain all continuous functions.
11.7. THE STONE-WEIERSTRASS THEOREM                                               185

    It is interesting that these two properties, "vanishes at no point" and "separates points,"
are sufficient to obtain approximation of any real-valued continuous function. Before we
prove this theorem, we note that such an algebra can interpolate a finite number of values
exactly. We will state this result only for two points as that is all that we will require.

Proposition 11.7.11. Suppose Ais an algebra of complex-valued functions on a set  that separates
points and vanishes at no point. Suppose ,  are distinct points of , and ,   . Then there is
an   A such that

                                             () = ,  () = .

If A is a real algebra, the conclusion holds for ,   .

Proof. There must exist an , ,   A such that ()  (), ()  0, ()  0. Let

                - ()  +   - () 
                      () - () () () - () ()

               =    - () ()() - ()() +   - () ()() - ()() .

We are not dividing by zero (clear from the first formula). Also by the first formula,  () = 

and  () = . By the second formula,   A (as A is an algebra).                      

Theorem 11.7.12 (Stone-Weierstrass, real version). Let  be a compact metric space and A a
real algebra of real-valued continuous functions on , such that A separates points and vanishes
at no point. Then the closure A = ( , ).

    The proof is divided into several claims.
Claim 1: If   A, then |  |  A.

Proof. The function  is bounded (continuous on a compact set), so there is an  such that
|  ()|   for all   . Let  > 0 be given. By the corollary to the Weierstrass theorem,
there exists a real polynomial 1 + 22 + · · · +   (vanishing at  = 0) such that

                                         

                              || -  < 

                                       =1

for all   [-, ]. Because A is an algebra and because there is no constant term in the

polynomial,                   

                                     A.

                              =1

As |  ()|  , then for all   

                             

             |  ()| -   ()  < .

                            =1

So |  | is in the closure of A, which is itself closed. In other words, |  |  A.  
186                                 CHAPTER 11. FUNCTIONS AS LIMITS

Claim 2: If   A and   A, then max(  , )  A and min(  , )  A, where
     max(  , ) () max  (), () , and min(  , ) () min  (), () .

Proof. Write:

     max(  , ) =  +  + |  - | , and min(  , ) =  +  - |  - | .
               2                 2               2                    2

As A is an algebra we are done.                                                    

    By induction, the claim is also true for the minimum or maximum of a finite collection
of functions.

Claim 3: Given   ( , ),   , and  > 0, there exists a   A with () =  () and

                  () >  () -  for all   .

Proof. Fix  , , and . By Proposition 11.7.11, for every   , find an   A such that

                  () =  (), () =  ().

As  and  are continuous, the function  -  is continuous, and the set
                           : () >  () -  = ( -  )-1 (-, )

is open (it is the inverse image of an open set by a continuous function). Furthermore
  . So the sets  cover . The space  is compact, so there exist finitely many points
1, 2, . . . ,  in  such that

                                                                                

                                                    =  .

                                                                              =1

Let
                                         max(1 , 2 , . . . ,  ).

By Claim 2,   A. See Figure 11.10. Moreover,

                                    () >  () - 

for all   , since for every , there is a  such that    , and so  () >  () - .
Finally, () =  () for all   , so () =  ().
                                                                                   

    What we have now is for each  a function   A that is within  of  near  (being
continuous), but also  is within  of  from at least one side at all points. If we cover
 with neighborhoods where  is a good approximation, we can repeat the idea of the

argument with a minimum to get a function that is within  from both sides.
11.7. THE STONE-WEIERSTRASS THEOREM                                                  187

                                                       

                                                       
                                                        -

                                   2                1

                                  1   2            

Figure 11.10: Construction of  out of two 1 (longer dashes) and 2 (shorter dashes).

Claim 4: If   ( , ) and  > 0 is given, then there exists an   A such that

                                  |  () - ()| < .

Proof. For every   , find the function  as in Claim 3. Let

                                          : () <  () +  .

The sets  are open as  and  are continuous. As () =  (), then   . So the sets
 cover . By compactness of , there are finitely many points 1, 2, . . . ,  such that

                                                                                

                                                    =  .

                                                                              =1

Let
                                          min(1 , 2 , . . . ,  ).

By Claim 2,   A. Similarly as before (same argument as in Claim 3), for all   ,

                                  () <  () + .

Since all the  satisfy () >  () -  for all   , () >  () -  as well. Hence, for all ,
                                             - < () -  () < ,

which is the desired conclusion.                                                     

    The proof of the theorem follows from Claim 4. The claim states that an arbitrary
continuous function is in the closure of A, which is already closed. The theorem is proved.

Example 11.7.13: The functions of the form

                                                                                

                                                  () =    ,

                                                                              =1

for   , are dense in  [, ],  . Such functions are a real algebra, which follows from
   = (+). They separate points as  is one-to-one. As  > 0 for all , the algebra
does not vanish at any point.
188                                   CHAPTER 11. FUNCTIONS AS LIMITS

    In general, given a set of functions that separates points and does not vanish at any
point, we let these functions generate an algebra by considering all the linear combinations
of arbitrary multiples of such functions. That is, we consider all real polynomials without
constant term of such functions. In the example above, the algebra is generated by . We
consider polynomials in  without constant term.

Example 11.7.14: We mentioned that the set of all functions of the form

                                                                          

                                               0 +  cos()

                                                                         =1

is an algebra. When considered on [0, ], it separates points and vanishes nowhere so
Stone-Weierstrass applies. As for polynomials, you do not want to conclude that every
continuous function on [0, ] has a uniformly convergent Fourier cosine series, that is, that
every continuous function can be written as

                                                                          

                                               0 +  cos().

                                                                        =1

That is not true! There exist continuous functions whose Fourier series does not converge
even pointwise let alone uniformly. See §11.8.

    To obtain Stone-Weierstrass for complex algebras, we must make an extra assumption.
Definition 11.7.15. An algebra A is self-adjoint if for all   A, the function ¯ defined by
¯()  () is in A, where by the bar we mean the complex conjugate.

Theorem 11.7.16 (Stone-Weierstrass, complex version). Let  be a compact metric space and
A an algebra of complex-valued continuous functions on , such that A separates points, vanishes
at no point, and is self-adjoint. Then the closure A = ( , ).

Proof. Suppose A  A is the set of the real-valued elements of A. For   A, write
 =  +  where  and  are real-valued. Then

                            =  + ¯ ,   =  - ¯ .
                                   2         2

So ,   A as A is a self-adjoint algebra, and since they are real-valued ,   A.
    If   , then find an   A such that  ()   (). If  =  + , then it is obvious that

either ()  () or ()  (). So A separates points. Similarly, for every  find   A
such that  ()  0. If  =  + , then either ()  0 or ()  0. So A vanishes at no

point. The set A is a real algebra, and satisfies the hypotheses of the real Stone-Weierstrass
theorem. Given any  =  +   ( , ), we find ,   A such that |() - ()| < /2
and |() - ()| < /2 for all   . Next,  +    A, and

 () - () + () = () + () - () + ()
                                                        |() - ()| + |() - ()| < /2 + /2 = 

for all   . So A = ( , ).                                                                   
11.7. THE STONE-WEIERSTRASS THEOREM                                                                                        189

    The self-adjoint requirement is necessary, although it is not so obvious to see it. For an
example, see Exercise 11.7.9.

    We give an interesting application. When working with functions of two variables, it
may be useful to work with functions of the form  ()() rather than (, ). For example,
they are easier to integrate. We have the following.

Example 11.7.17: Any continuous  : [0, 1] × [0, 1]   can be approximated uniformly by
functions of the form 

                                                       ()(),

                                                                     =1

where  : [0, 1]   and  : [0, 1]   are continuous.
    Proof: It is not hard to see that the functions of the above form are a complex algebra.

It is equally easy to show that they vanish nowhere, separate points, and the algebra is
self-adjoint. As [0, 1] × [0, 1] is compact, Stone-Weierstrass obtains the result.

11.7.3 Exercises

Exercise 11.7.1:  Prove Proposition 11.7.6.  Hint:  If  {   }       is  a  sequence  in  (  ,  )          converging   to   , then

                                                                =1
as  is bounded, show that  is uniformly bounded, that is, there exists a single bound for all  (and  ).

Exercise 11.7.2: Suppose   (not compact in particular). Show that  ()                                      is not possible to

                                                                                                        

uniformly approximate by polynomials on . Hint: Consider  as   .

Exercise 11.7.3: Suppose  : [0, 1]   is a uniform limit of a sequence of polynomials of degree at most ,
then the limit is a polynomial of degree at most . Conclude that to approximate a function which is not a

polynomial, we need the degree of the approximations to go to infinity.
Hint: First prove that if a sequence of polynomials of degree  converges uniformly to the zero function, then
the coefficients converge to zero. One way to do this is linear algebra: Consider a polynomial  evaluated at
 + 1 points to be a linear operator taking the coefficients of  to the values of  (an operator in (+1)).

Exercise 11.7.4: Suppose  : [0, 1]   is continuous and 0  1  ()  = 0 for all  = 0, 1, 2, . . .. Show
                                                                                         1                      2
that  () = 0 for all   [0, 1]. Hint: Approximate by polynomials to show that 0  ()  = 0.

Exercise 11.7.5:  Suppose  :  [0, 1],          is   a   linear  continuous  function     such             that  ()  =   1  for  all
                  1                                                                                                    +1

 = 0, 1, 2, 3, . . .. Prove that (  ) = 0  for all    [0, 1],  .

Exercise 11.7.6: Let A be the collection of real polynomials in 2, that is, polynomials of the form
0 + 12 + 24 + · · · + 2.

 a) Show that every    [0, 1],  is a uniform limit of polynomials from A.

b) Find an    [-1, 1],  that is not a uniform limit of polynomials from A.

 c) Which hypothesis of the real Stone-Weierstrass is not satisfied for the domain [-1, 1]?
190                                                      CHAPTER 11. FUNCTIONS AS LIMITS

Exercise 11.7.7: Let || = 1 define the unit circle 1  .

a) Show that functions of the form

                                                   

                                                        

                                                 =-

     are dense in (1, ). Notice the negative powers.

b) Show that functions of the form

                                                         

                                       0 +   + - ¯ 

                                       =1                =1

    are dense in (1, ). These are so-called harmonic polynomials, and this approximation leads to, for
     example, the solution of the steady state heat problem.

Hint: A good way to write the equation for 1 is ¯ = 1.

Exercise 11.7.8: Show that for complex numbers , the set of functions of  on [-, ] of the form

                                                                                  

                                                                 

                                                                               =-

satisfies the hypotheses of the complex Stone-Weierstrass theorem and therefore such functions are dense in
the  [-, ],  .

Exercise 11.7.9: Let 1   be the unit circle, that is, the set where || = 1. Orient this set counterclockwise.
Let () . For the one-form  ()  we write

                                                  2
                                          ()             ()  .

                                      1            0

                                                                               
a) Prove that for all nonnegative integers  = 0, 1, 2, 3, . . ., we have 1   = 0.

                               
b) Prove that if () = =0   is a polynomial in , then 1 ()  = 0.

           
c) Prove 1 ¯   0.

d) Conclude that polynomials in  (this algebra of functions is not self-adjoint) are not dense in (1, ).

Exercise 11.7.10: Let ( , ) be a compact metric space and suppose A  ( , ) is a real algebra that

separates points, but vanishes at exactly one point 0  . That is,  (0) = 0 for all   A, but for every
   \ {0} there is a   A such that ()  0. Prove that every function   ( , ) such that
(0) = 0 is a uniform limit of functions from A.

Exercise 11.7.11: Let ( , ) be a compact metric space and suppose A  ( , ) is a real algebra. Suppose
that for each    the closure A contains the function () (, ). Then A = ( , ).

     Alternatively, one could define    +   and extend the path integral from chapter 9 to complex-
valued one-forms.
11.7. THE STONE-WEIERSTRASS THEOREM                                                                                                   191

Exercise 11.7.12:

a) Suppose  : [, ]   is continuously differentiable. Show that there exists a sequence of polynomials
    {   }                                                                                                [,]
                that  converges  in  the  1   norm      to   ,  that  is,       -   [,]       +    -                 0      as      .
            =1

b) Suppose  : [, ]   is  times continuously differentiable. Show that there exists a sequence of
                 {   }
    polynomials              that    converges  in      the        norm    to   ,  that  is,
                         =1

                                               () ()

                                               -                       0 as   .
                                     =0 [,]

Exercise 11.7.13:

 a) Show that an even function  : [-1, 1]   is a uniform limit of polynomials with even powers only,
     that is, polynomials of the form 0 + 12 + 24 + · · · +  2.

b) Show that an odd function  : [-1, 1]   is a uniform limit of polynomials with odd powers only, that
     is, polynomials of the form 1 + 23 + 35 + · · · +  2-1.

Exercise 11.7.14: Let  : [, ]   be continuous.

a)  Given two points 1, 2            [,   ],  show      that  there     exists  a  sequence   of  real  polynomials  {   }        so  that

    (1) =  (1) and (2) =  (2) for all .                                                                                      =1

b) Generalize the previous part to  points: Given the points 1, 2, . . . ,   [, ], show that there exists
                                            {   }
    a  sequence  of   real  polynomials                 so   that  for  all  ,  ()       =     ()  for    =  1, 2, . . . ,  .
                                                    =1
    Hint: The polynomial ( - 1)( - 2) · · · ( - -1)( - +1) · · · ( - ) is zero at  for    but

    nonzero at  . Use it to construct a polynomial that takes prescribed values at 1, 2, . . . , .
192                               CHAPTER 11. FUNCTIONS AS LIMITS

11.8 Fourier series

Note: 3-4 lectures

    Fourier series is perhaps the most important (and the most difficult) of the series that
we cover in this book. We saw a few examples already, but let us start at the beginning.

11.8.1 Trigonometric polynomials

A trigonometric polynomial is an expression of the form

                                                            

                                     0 +  cos() +  sin() ,

                                                           =1

or equivalently, thanks to Euler's formula ( = cos() +  sin()):

                                                                         

                                                             .

                                                                      =-

The second form is usually more convenient. If    with || = 1, we write  = , and so

                                  

                           =            .

                     =-           =-

So a trigonometric polynomial is really a rational function of the complex variable  (we are
allowing negative powers) evaluated on the unit circle. There is a wonderful connection
between power series (actually Laurent series because of the negative powers) and Fourier
series because of this observation, but we will not investigate this further.

    Another reason why Fourier series is important and comes up in so many applications
is that the functions  are eigenfunctions of various differential operators. For example,

        = ()  ,                   2   = (-2)  .
                                  2

That is, they are the functions whose derivative is a scalar (the eigenvalue) times itself.

Just as eigenvalues and eigenvectors are important in studying matrices, eigenvalues and

eigenfunctions are important when studying linear differential equations.

The functions cos(), sin(), and  are 2-periodic and hence trigonometric

polynomials are also 2-periodic. We could rescale  to make the period different, but the
theory is the same, so we stick with the period 2. The antiderivative of  is  and so

         = 2 if  = 0,
                     -            0 otherwise.

Named after the French mathematician Jean-Baptiste Joseph Fourier (1768-1830).
Eigenfunction is like an eigenvector for a matrix, but for a linear operator on a vector space of functions.
11.8. FOURIER SERIES                                                                                 193

Consider                                              
                                               ()
                                                            ,
and for  = - , . . . ,  compute
                                                   =-

1  ()-    = 1                                                =  1  (-)    =  .
2 -                     2 -                                       =- 2 -
                                           (-)

                                    =-

We just found a way of computing the coefficients  using an integral of  . If || > , the
integral is 0, so we might as well have included enough zero coefficients to make ||  .

Proposition 11.8.1. A trigonometric polynomial  () =                           is  real-valued  for  real  
if and only if - =  for all  = - , . . . , .                   =-

Proof. If  () is real-valued, that is,  () =  (), then

 = 1  ()-  =   1  ()-  =   1  ()   = - .  
     2 -                            2 -                        2 -

The complex conjugate goes inside the integral because the integral is done on real and
imaginary parts separately.

    On the other hand, if - = , then

                 - - +   = -  +  - =   + - - ,

which is real valued. Also 0 = 0, so 0 is real. By pairing up the terms, we obtain that 

has to be real-valued.                                                                                     

The functions  are also linearly independent.

Proposition 11.8.2. If

                                      

                                            = 0

                                      =-

for all   [-, ], then  = 0 for all .

Proof. The result follows immediately from the integral formula for .                                      

11.8.2 Fourier series

We now take limits. The series                     

                                                        

                                      =-

is called the Fourier series and the numbers  the Fourier coefficients. Using Euler's formula
 = cos() +  sin(), we could also develop everything with sines and cosines, that is, as
                
the series 0 +  =1      cos(  )  +    sin(  ).     It  is  equivalent,  but  slightly  more  messy.
194                                                CHAPTER 11. FUNCTIONS AS LIMITS

    Several questions arise. What functions are expressible as Fourier series? Obviously,
they have to be 2-periodic, but not every periodic function is expressible with the series.
Furthermore, if we do have a Fourier series, where does it converge (where and if at all)?
Does it converge absolutely? Uniformly? Also note that the series has two limits. When
talking about Fourier series convergence, we often talk about the following limit:

                                              

                                         lim       .
                                          =-

There are other ways we can sum the series to get convergence in more situations, but we
refrain from discussing those. In light of this, define the symmetric partial sums

                           (  ; )                

                                                     .

                                              =-

    Conversely, for an integrable function  : [-, ]  , call the numbers             write  ^().
                                         1  ()-   
                                                  2 -

its Fourier coefficients. To emphasize the function the coefficients belong to, we
We then formally write down a Fourier series:

                                              

                                ()                 .

                                              =-

As you might imagine such a series might not even converge. The  doesn't imply anything
about the two sides being equal in any way. It is simply that we created a formal series
using the formula for the coefficients. We will see that when the functions are "nice
enough," we do get convergence.

Example 11.8.3: Consider the step function () so that () 1 on [0, ] and () -1

on (-, 0), extended periodically to a 2-periodic function. With a little bit of calculus, we

compute the coefficients:

^(0) = 1 ()  = 0,          ^() = 1 ()-  =    (-1) - 1                               for   1.
         2 -                                  2 -       

A little bit of simplification leads to

      (; ) =   ^()  =  2 1 - (-1)                       sin(  ).
                                                =1 
                           =-

See the left hand graph in Figure 11.11 for a graph of  and several symmetric partial sums.

     The notation should seem similar to Fourier transform to those readers that have seen it. The similarity
is not just coincidental, we are taking a type of Fourier transform here.
11.8. FOURIER SERIES                                                                     195

    For a second example, consider the function () || on [-, ] and then extended
to a 2-periodic function. Computing the coefficients, we find

^ (0) = 1   ()  =  ,                  ^ () =  1 -   (-1) - 1
                                                   ()  =           for   1.
    2 -                         2             2 -            2

A little simplification yields

         (; ) = ^ ()  =   + 2 (-1) - 1 cos().
                                              2 =1 2
                         =-

See the right hand graph in Figure 11.11.

1

0                                             2

-1                                            0

    -  -2             0            2               -  -2  0     2

Figure 11.11: The functions  and  in bold, with several symmetric partial sums in gray.

    Note that for both  and , the even coefficients (except ^ (0)) happen to vanish, but

that is not really important. What is important is convergence. First, at the discontinuity at
 = 0, we find  (; 0) = 0 for all , so  (; 0) converges to a different number from (0)
(at a nice enough jump discontinuity, the limit is the average of the two-sided limits, see the

exercises). That should not be surprising; the coefficients are computed by an integral, and

integration does not notice if the value of a function changes at a single point. We should

remark, however, that we are not guaranteed that in general the Fourier series converges to

the function even at a point where the function is continuous. We will prove convergence

if the function is at least Lipschitz.
    What is really important is how fast the coefficients go to zero. For the discontinuous ,

the coefficients ^() go to zero approximately like 1/. On the other hand, for the continuous
, the coefficients ^ () go to zero approximately like 1/2. The Fourier coefficients "see" the

discontinuity in some sense.

    Do note that continuity in this setting is the continuity of the periodic extension, that is,
we include the endpoints ±. So the function  () =  defined on (-, ] and extended
periodically would be discontinuous at the endpoints ±.
196                                                 CHAPTER 11. FUNCTIONS AS LIMITS

    In general, the relationship between regularity of the function and the rate of decay
of the coefficients is somewhat more complicated than the example above might make it
seem, but there are some quick conclusions we can make. We forget about finding a series
for a function for a moment, and we consider simply the limit of some given series. A few
sections ago, we proved that the Fourier series

                                     sin()
                                    =1 2

converges uniformly and hence converges to a continuous function. This example and its

proof can be extended to a more general criterion.

Proposition 11.8.4. Let             be  a  Fourier  series,  and  ,    >  1  constants  such  that
                         =-

                            ||  ||  for all    \ {0}.

Then the series converges (absolutely and uniformly) to a continuous function on .

The proof is to apply the Weierstrass -test (Theorem 11.2.4) and the -series test to find

that the series converges uniformly and hence to a continuous function (Corollary 11.2.8).

We can also take derivatives.

Proposition 11.8.5. Let             be  a  Fourier  series,  and  ,    >  2  constants  such  that
                         =-

                            ||  ||  for all    \ {0}.

Then the series converges to a continuously differentiable function on .

    The proof is to note that the series converges to a continuous function by the previous
proposition. In particular, it converges at some point. Then differentiate the partial sums

                                       

                                           

                                    =-

and notice that for all nonzero 

                                  ||  ||-1  .

The differentiated series converges uniformly by the -test again. Since the differentiated
                                                                              
series converges uniformly, we find that the original series                    converges           to
                                                                       =- 

a continuously differentiable function, whose derivative is the differentiated series (see

Theorem 11.2.14).

We can iterate this reasoning. Suppose there is some  and  >  + 1 (  ) such that

for all nonzero integers ,          ||  ||  .

Then the Fourier series converges to a -times continuously differentiable function. There-

fore, the faster the coefficients go to zero, the more regular the limit is.
11.8. FOURIER SERIES                                                                                             197

11.8.3 Orthonormal systems

Let us abstract away the exponentials, and study a more general series for a function.

One fundamental property of the exponentials that makes Fourier series work is that the
exponentials are a so-called orthonormal system. Fix an interval [, ]. We define an inner

product for the space of functions. We restrict our attention to Riemann integrable functions
as we do not have the Lebesgue integral, which would be the natural choice. Let  and 
be complex-valued Riemann integrable functions on [, ] and define the inner product

                                           ,     
                                                        ()() .

                                                   

If you have seen Hermitian inner products in linear algebra, this is precisely such a product.

We must include the conjugate as we are working with complex numbers. We then have
the "size" of  , that is, the 2 norm   2, by (defining the square)

                                           2               
                                                   ,   = |  ()|2 .
                                         2
                                                                   

Remark 11.8.6. Note the similarity to finite dimensions. For  = (1, 2, . . . , )  , one

defines

                                                                                   

                                              ,                                      .

                                                    =1

Then the norm is (usually denoted simply by  in  rather than by 2)

                                                                       

                                         2 = ,  = ||2.

                                                                     =1

This is just the euclidean distance to the origin in  (same as 2).

In what follows, we will assume all functions are Riemann integrable.

Definition  11.8.7.  Let  {  }         be     a  sequence  of                        integrable  complex-valued  functions  on

                                   =1
[, ]. We say that this is an orthonormal system if

                        ,  = () ()  = 1 if  = ,
                                                                                         0 otherwise.

In particular, 2 = 1 for all . If we only require that  ,  = 0 for   , then the
system would be called an orthogonal system.

We noticed above that                            1   

                                                 2                                   =1

is an orthonormal system on [-, ]. The factor out in front is to make the norm be 1.
198                                                           CHAPTER 11. FUNCTIONS AS LIMITS

     Having  an   orthonormal  system      {  }         on  [, ]  and  an  integrable  function    on    [, ],

                                                    =1
we can write a Fourier series relative to {}=1. Let

                                                  
                                  ,  =  ()() ,

                                                             

and write                                               

                                            ()  .

                                                       =1

     In other words, the series is          

                                                 , ().

                                           =1

Notice the similarity to the expression for the orthogonal projection of a vector onto a
subspace from linear algebra. We are in fact doing just that, but in a space of functions.

Theorem 11.8.8. Suppose  is a Riemann integrable function on [, ].                     Let  {  }         be  an

orthonormal system on [, ] and suppose                                                               =1

                                                        

                                            ()  ().

                                                       =1

If

                                                                       

                       ()             () and ()                                ()

                               =1                                      =1

for  some  other  sequence  {  } ,   then

                                 =1

                                                                       
                  |  () - ()|2  =   - 22    - 22 = |  () - ()|2 
                                                                           

with equality only if  =  for all  = 1, 2, . . . , .

    In other words, the partial sums of the Fourier series are the best approximation with
respect to the 2 norm.

Proof. Let us write

                                                                             
                       |  - |2 = |  |2 -   -   + ||2.
                                                                             

Now                                                                          

                              =   =    = ,
                                =1                                
                                                          =1                 =1
11.8. FOURIER SERIES                                                                       199

and                                                                                    

                  ||2 =            =                                        = ||2.
                             =1                                                  
                                         =1                 =1 =1                      =1

So

                                                                                   

                            |  - |2 = |  |2 -   -   + ||2
                                           
                                                       =1               =1         =1
                                         
                                                                          

                                         = |  |2 - ||2 + | - |2.
                                           
                                                       =1               =1

This is minimized precisely when  = .                                                      

     When we do plug in  = , then

                                                                          

                                         |  - |2 = |  |2 - ||2,
                                                          
                                                                        =1

and so for all ,                                            

                                                 ||2  |  |2.
                                                               
                                             =1

Note that                                       

                                                   ||2 = 22

                                               =1

by the calculation above. We take a limit to obtain the so-called Bessel's inequality.

Theorem 11.8.9 (Bessel's inequality). Suppose  is a Riemann integrable function on [, ].
     {  }
Let               be  an  orthonormal  system  on  [,  ]  and    suppose
              =1

                                                          

                                              ()  ().

                                                         =1

Then                                                   

                                             ||2            |    |2  =      2.

                                                                              2

                                         =1

     In particular,   |  |2 <  implies the series converges and hence

                                                    lim  = 0.

                                                                      

     Named after the German astronomer, mathematician, physicist, and geodesist Friedrich Wilhelm Bessel
(1784-1846).
200                                                      CHAPTER 11. FUNCTIONS AS LIMITS

11.8.4 The Dirichlet kernel and approximate delta functions

We   return   to  the  trigonometric  Fourier  series.  The   system     { }           is  orthogonal,  but  not

                                                                                   =1
orthonormal if we simply integrate over [-, ]. We can rescale the integral and hence the
                             { }
inner  product    to   make                orthonormal.  That  is,  if  we  replace
                                       =1

                                        with 1  ,
                                                         2 -

(we are just rescaling the  really), then everything works and we obtain that the system
{ }
              is  orthonormal  with  respect  to  the  inner  product
          =1

                                       ,  = 1    () () .
                                                2 -

    Suppose  :    is 2-periodic and integrable on [-, ]. Write

                                                               1  ()-   .
                                                               2 -
                   ()           ,              where 

                        =-

Recall the notation for the symmetric partial sums,  (  ; )                                .  The  inequality
leading up to Bessel now reads:                                             =-

                       1 | (  ; )|2  = ||2     1 |  ()|2   .
                       2 -                                     2 -
                                                  =-

    Let the Dirichlet kernel be                           

                                              ()             .

                                                       =-

We claim that                               () = sin ( + 1/2) sin(/2) ,

for  such that sin(/2)  0. The left-hand side is continuous on , and hence the right-hand
side extends continuously to all of . To show the claim, we use a familiar trick:

                               (  - 1) () =  (+1) - - .

Multiply by -/2

                             ( /2 - -/2) () =  (+1/2) - -(+1/2) .

The claim follows.

    Mathematicians in this field sometimes simplify matters with a tongue-in-cheek definition that 1 = 2.
11.8. FOURIER SERIES                                               201

Expand the definition of 

                        1  ()-     

(  ; ) =
           =- 2 -
                                = 1  () (-)     = 1    () ( - ) .
                                  2 -                         2 -
                                       =-

Convolution strikes again! As  and  are 2-periodic, we may also change variables

and write   (  ; ) = 1  +  ( - ) ()  = 1    ( - ) () .

                             2 -           2 -

See Figure 11.12 for a plot of  for  = 5 and  = 20.

                        40

                        30

                        20

                        10

                        0

                        -10  -

                                  -2   0             2

Figure 11.12: Plot of  () for  = 5 (gray) and  = 20 (black).

    The central peak gets taller and taller as  gets larger, and the side peaks stay small. We
are convolving (again) with approximate delta functions, although these functions have all
these oscillations away from zero. The oscillations on the side do not go away but they are
eventually so fast that we expect the integral to just sort of cancel itself out there. Overall,
we expect that  (  ) goes to  . Things are not always simple, but under some conditions
on  , such a conclusion holds. For this reason people write

                                                                                    

                                               2 ()   ,

                                                                                  =

where  is the "delta function" (not really a function), which is an object that will give

                                  

something like " -  ( - )()  =  ()." We can think of  () converging in some
sense to 2 (). However, we have not defined (and will not define) what the delta
function is, nor what does it mean for it to be a limit of  or have a Fourier series.
202                                     CHAPTER 11. FUNCTIONS AS LIMITS

11.8.5 Localization

If  satisfies a Lipschitz condition at a point, then the Fourier series converges at that point.
Theorem 11.8.10. Let  be fixed and let  be a 2-periodic function Riemann integrable on
[-, ]. Suppose there exist  > 0 and  such that

                       |  ( + ) -  ()|  ||

for all   (-, ), then

                                     lim  (  ; ) =  ().

                                      

    In particular, if  is continuously differentiable at , then we obtain convergence at 
(exercise). A function  : [, ]   is continuous piecewise smooth if it is continuous and
there exist points 0 =  < 1 < 2 < · · · <  =  such that for every ,  restricted to
[ , +1] is continuously differentiable (up to the endpoints).

Corollary 11.8.11. Let  be a 2-periodic function Riemann integrable on [-, ]. Suppose there
exist    and  > 0 such that  is continuous piecewise smooth on [ - ,  + ], then

                                     lim  (  ; ) =  ().

                                      

The proof of the corollary is left as an exercise. Let us prove the theorem.

Proof of Theorem 11.8.10. For all ,

                                     1    = 1.
                                     2 -

Write

       (  ; ) -  () = 1    ( - )()  -  () 1   () 
                       2 -                                   2 -
                           
                       = 1  ( - ) -  () () 
                       2 -
                           
                       = 1  ( - ) -  () sin ( + 1/2) .
                       2 - sin(/2)

By the hypotheses, for small nonzero ,

                        ( - ) -  () sin(/2)  || |sin(/2)| .

As sin() =  + () where  ()  0 as   0, we notice that |sin(/2)| || is continuous at the
origin. Hence, sin(/2)  (-)-  () , as a function of , is bounded near the origin. As  = 0 is the
only place on [-, ] where the denominator vanishes, it is the only place where there
could be a problem. So, the function is bounded near  = 0 and clearly Riemann integrable
11.8. FOURIER SERIES                                                    203

on any interval not including 0, and thus it is Riemann integrable on [-, ]. We use the
trigonometric identity

            sin ( + 1/2) = cos(/2) sin() + sin(/2) cos(),

to compute

1    ( - ) -  () sin ( + 1/2)         =
2 -  sin(/2)                         sin()  + 1  
1  ( - ) -  () cos(/2)
2 -         sin(/2)                                  2 -   ( - ) -  ()  cos() .

                                 (-)-  () 

As functions of , sin(/2) cos( /2) and  ( - ) -  () are bounded Riemann integrable
functions and so their Fourier coefficients go to zero by Theorem 11.8.9. So the two integrals

on the right-hand side, which compute the Fourier coefficients for the real version of the

Fourier series go to 0 as  goes to infinity. This is because sin() and cos() are also

orthonormal systems with respect to the same inner product. Hence  (  ; ) -  () goes
to 0, that is,  (  ; ) goes to  ().
                                                                                 

    The theorem also says that convergence depends only on local behavior. That is, to
understand convergence of  (  ; ) we only need to know  in some neighborhood of .

Corollary 11.8.12. Suppose  is a 2-periodic function, Riemann integrable on [-, ]. If  is
an open interval and  () = 0 for all   , then lim  (  ; ) = 0 for all   .

                                                                                  

    In particular, if  and  are 2-periodic functions, Riemann integrable on [-, ],  an open

                                                                                                                                       

interval, and  () = () for all   , then for all   , the sequence  (  ; ) =1 converges if

                                     

and only if  (; ) =1 converges.

    The first claim follows by taking  = 0 in the theorem. The "In particular" follows by
considering  - , which is zero on  and  (  - ) =  (  ) -  (). So convergence at
 depends only on the values of the function near . However, we saw that the rate of
convergence, that is, how fast does  (  ) converge to  , depends on global behavior of  .

    Note a subtle difference between the results above and what Stone-Weierstrass theorem
gives. Any continuous function on [-, ] can be uniformly approximated by trigonometric
polynomials, but these trigonometric polynomials may not be the partial sums  .

11.8.6 Parseval's theorem

Finally, convergence always happens in the 2 sense and operations on the (infinite) vectors
of Fourier coefficients are the same as the operations using the integral inner product.
204                                     CHAPTER 11. FUNCTIONS AS LIMITS

Theorem 11.8.13 (Parseval). Let  and  be 2-periodic functions, Riemann integrable on

[-, ] with                                      

             ()                    and ()           .

                  =-                        =-

Then                             2 1 2  
Also        lim   -  (  )2 = lim        |  () -  (  ; )|  = 0.
                                  2 -

                 ,  = 1     ()()  =   ,
                                 2 -
                                            =-

and  2 1   2 2
                                    2 = 2 - =- |  ()|  = || .

Proof. There exists (exercise) a continuous 2-periodic function  such that

                                   - 2 < .

Via Stone-Weierstrass, approximate  with a trigonometric polynomial uniformly. That is,
there is a trigonometric polynomial () such that |() - ()| <  for all . Hence

                - 2 =            1 |() - ()|2   .  
                                 2 -

If  is of degree 0, then for all   0 ,

                                  -  ()2   - 2  ,

as  () is the best approximation for  in 2 (Theorem 11.8.8). By the inequality leading
up to Bessel,

                           () -  (  )2 =  ( -  )2   -  2  .

The 2 norm satisfies the triangle inequality (exercise). Thus, for all   0,

              -  (  )2    - 2 +  -  ()2 +  () -  (  )2  3.

Hence, the first claim follows.
    Next,

       (  ),  = 1     (  ; )()  =  1     ()  =   .
            2 -                         =- 2 -
                                                                         =-

Named after the French mathematician Marc-Antoine Parseval (1755-1836).
11.8. FOURIER SERIES                                                         205

We need the Schwarz (or Cauchy-Schwarz or Cauchy-Bunyakovsky-Schwarz) inequality

for 2, that is,                  2                     
                                       ¯        |  |2       ||2 .

                                                         

Its proof is left as an exercise; it is not much different from the finite-dimensional version.

So

                                           

                      ¯ -       (  )¯ =     -  (  -  (  ))¯

                 -         -                                 1/2  

                                                                      1/2

                                            -  |  -  (  )|2        -  ||2 .

The right-hand side goes to 0 as  goes to infinity by the first claim of the theorem. That is,

as  goes to infinity,  (  ),  goes to   , , and the second claim is proved. The last

claim in the theorem follows by using  =  .                                  

11.8.7 Exercises

Exercise 11.8.1: Consider the Fourier series
                                                         1 sin(2 ).
                                                       =1 2

Show that the series converges uniformly and absolutely to a continuous function. Remark: This is another
example of a nowhere differentiable function (you do not have to prove that). See Figure 11.13.

                           0.8

                           0.4

                           0

                       -0.4

                       -0.8     -

                                        -2     0       2

Figure 11.13: Plot of      1  sin(2 ).
                       =1  2

     See G. H. Hardy, Weierstrass's Non-Differentiable Function, Transactions of the American Mathematical
Society, 17, No. 3 (Jul., 1916), pp. 301-325. A thing to notice here is the th Fourier coefficient is 1/ if  = 2
and zero otherwise, so the coefficients go to zero like 1/.
206                                                           CHAPTER 11. FUNCTIONS AS LIMITS

Exercise 11.8.2: Suppose that a 2-periodic function that is Riemann integrable on [-, ], and such
that  is continuously differentiable on some open interval (, ). Prove that for every   (, ), we have
 lim  (  ; ) =  ().

 

Exercise 11.8.3: Prove Corollary 11.8.11, that is, suppose a 2-periodic function is continuous piecewise
smooth near a point , then lim  (  ; ) =  (). Hint: See the previous exercise.

                                               

Exercise 11.8.4: Given a 2-periodic function  :   , Riemann integrable on [-, ], and  > 0, show
that there exists a continuous 2-periodic function  :    such that   - 2 < .

Exercise 11.8.5: Prove the Cauchy-Bunyakovsky-Schwarz inequality for Riemann integrable functions:

                               2                                 
                                     ¯                 |  |2          ||2 .

                                                                   

Exercise 11.8.6: Prove the 2 triangle inequality for Riemann integrable functions on [-, ]:

                                       + 2    2 + 2.

Exercise 11.8.7:  Suppose for some   and   >  1,  we  have    a  real  sequence  {   }       with  ||      for all .
                                                                                                         
                                                                                         =1

Let 

                                     ()                sin().

                                                  =1

a) Show that  is continuous.

b) Formally (that is, suppose you can differentiate under the sum) find a solution (formal solution, that is,
    do not yet worry about convergence) to the differential equation

                                               + 2 = ()

     of the form

                                                   

                                     () =  sin().

                                                 =1

c) Then show that this solution  is twice continuously differentiable, and in fact solves the equation.

Exercise 11.8.8: Let  be a 2-periodic function such that  () =  for 0 <  < 2. Use Parseval's theorem

to find

                                            1 = 2 .
                                           =1 2 6

Exercise 11.8.9: Suppose that  = 0 for all  < 0 and                    converges.  Let             (0, 1)   be the

                                                              =0 |  |
unit disc, and  = (0, 1) be the closed unit disc. Show that there exists a continuous function  :   
that is analytic on  and such that on the boundary of  we have  () =              .
Hint: If  = , then  =  .
                                                                                 =0
11.8. FOURIER SERIES                                                                    207

Exercise 11.8.10: Show that                              

                                                            -1/ sin()

                                                        =1

converges to an infinitely differentiable function.
Exercise 11.8.11: Let  be a 2-periodic function such that  () =  (0) + 0    for a function  that is
Riemann integrable on every interval. Suppose

                                                              

                                                   ()                 .

                                                              =-

Show  that  there  exists  a    >  0  such  that  ||          for  all  nonzero  .
                                                          ||

Exercise 11.8.12:

 a) Let  be the 2-periodic function defined by () 0 if   (-, 0), and () 1 if   (0, ),
     letting (0) and () be arbitrary. Show that lim  (; 0) = 1/2.

                                                                                      

b) Let  be a 2-periodic function Riemann integrable on [-, ],   ,  > 0, and there are continuously
    differentiable  : [ - , ]   and  : [,  + ]   where  () = () for all   [ - , ) and
     where  () = () for all   (,  + ]. Then lim  (  ; ) = 2 ()+() , or in other words,

                                                                                     

                                       lim  (  ; ) = 1 lim-  () + lim+  () .
                                                              2                  

Exercise 11.8.13:  Let     {   }       be  such   that  lim             = 0.  Show that there is a continuous 2-periodic

                                   =1
function  whose Fourier coefficients  satisfy that for each  there is a    where ||  .

Remark: The exercise says that if  is only continuous, there is no "minimum rate of decay" of the coefficients.

Compare with Exercise 11.8.11.

Hint: Look at Exercise 11.8.1 for inspiration.
208  CHAPTER 11. FUNCTIONS AS LIMITS
Further Reading

[R1] Maxwell Rosenlicht, Introduction to Analysis, Dover Publications Inc., New York, 1986.
      Reprint of the 1968 edition.

[R2] Walter Rudin, Principles of Mathematical Analysis, 3rd ed., McGraw-Hill Book Co., New
      York, 1976. International Series in Pure and Applied Mathematics.

 [T] William F. Trench, Introduction to Real Analysis, Pearson Education, 2003. http:
      //ramanujan.math.trinity.edu/wtrench/texts/TRENCH_REAL_ANALYSIS.PDF.
210  FURTHER READING
Index

algebra, 14, 184                      complex number, 139
almost every, 125                     complex plane, 139
almost everywhere, 122, 125           conservative vector field, 88
analytic, 153                         continuous piecewise smooth, 72, 202
antiderivative, 86                    continuously differentiable, 48, 60
approximate delta function, 180, 201  continuously differentiable path, 71
arc-length measure, 78                converges
arc-length parametrization, 80
Arzelà-Ascoli theorem, 174                 complex series, 141
                                           power series, 153
basis, 11                             converges absolutely
Bessel's inequality, 199                   complex series, 141
bilinear, 20                          converges pointwise, 144
bounded domain with piecewise smooth       complex series, 144
                                      converges uniformly, 144
          boundary, 128               convex, 16
                                      convex combination, 16
Cantor function, 116                  convex hull, 17
Cantor set, 113                       convolution, 180
Cauchy                                cosine, 164
                                      critical point, 45
     complex series, 142              curve, 41
Cauchy-Schwarz inequality, 20
chain rule, 38                        Darboux integral, 93
change of basis, 26                   Darboux sum, 92
characteristic function, 123          derivative, 35
closed path, 71
closed rectangle, 91                       complex-valued function, 142
column, 25                            determinant, 27
column vectors, 7                     Devil's staircase, 116
commutative diagram, 30               diagonal matrix, 32
compact operator, 176                 differentiable, 35
compact support, 99                   differentiable curve, 41
complex algebra, 184                  differential one-form, 74
complex conjugate, 140                dimension, 11
212                                                                                INDEX

directional derivative, 41            -times continuously differentiable
Dirichlet kernel, 200                           function, 60
dot product, 20
                                      Kronecker density theorem, 177
eigenvalue, 33
elementary matrix, 30                 Laplace equation, 132
equicontinuous, 173                   law of exponents, 164
euclidean norm, 20                    Lebesgue-Vitali theorem, 118
Euler's formula, 164                  Leibniz integral rule, 65
even permutation, 27                  length, 79
                                      length of a curve, 79
for almost every, 125                 linear, 14
Fourier coefficients, 193             linear combination, 10
Fourier series, 193                   linear operator, 14
Fubini for sums, 157                  linear transformation, 14
Fubini's theorem, 104, 106            linearity of the integral, 96
fundamental theorem of algebra, 170   linearly dependent, 11
                                      linearly independent, 11
general linear group, 24              longest side, 98
generate an algebra, 188              lower Darboux integral, 93
gradient, 40                          lower Darboux sum, 92
Green's theorem, 129
                                      map, 14
hyperbolic cosine, 168                mapping, 14
hyperbolic sine, 168                  matrix, 25
                                      maximum modulus principle, 169
identity, 14                          maximum principle
identity theorem, 161
imaginary axis, 139                        analytic functions, 169
imaginary part, 140                        harmonic functions, 133
implicit function theorem, 55         mean value property, 132
indicator function, 123               mean value theorem, 45, 46
inner product, 197                    measure zero, 108
integrable, 95                        minimum modulus principle, 169
integrable on , 124                   modulus, 140
inverse function theorem, 51          monotonicity of the integral, 96
invertible linear transformation, 14
isolated singularity, 171             -dimensional volume
                                           Jordan measurable set, 123
Jacobian, 42                               rectangles, 91
Jacobian conjecture, 54
Jacobian determinant, 42              negatively oriented, 128
Jacobian matrix, 42                   norm, 20
Jordan measurable, 123                normed vector space, 20
                                      null set, 108
                                      nullspace, 15
INDEX                                                                                     213

odd permutation, 27                     rectangle, 91
one-form, 74                            refinement of a partition, 93
open mapping, 54                        relative maximum, 45
open rectangle, 91                      relative minimum, 45
operator norm, 21                       relatively compact, 115, 176
operator, linear, 14                    removable singularity, 171
orthogonal system, 197                  reparametrization, 73
orthonormal system, 197                 reverse orientation, 73
oscillation, 117                        Riemann integrable, 95
outer measure, 108
                                             complex-valued function, 142
Parseval's theorem, 203                 Riemann integrable on , 124
partial derivative, 39                  Riemann integral, 95
partial derivative of order , 60        Riemann-Lebesgue theorem, 118
partition, 91
path connected, 83                      scalars, 7
path independent, 83                    self-adjoint, 188
Peano existence theorem, 178            separates points, 184
Peano surface, 45                       simple path, 71
permutation, 27                         simply connected, 86
piecewise continuously differentiable   sine, 164
                                        singularity, 171
          path, 71                      smooth path, 71
piecewise smooth, 202                   smooth reparametrization, 73
piecewise smooth boundary, 128          span, 10
piecewise smooth path, 71               spectral radius, 33
piecewise smooth reparametrization, 73  standard basis, 12
Poincaré lemma, 87                      star-shaped domain, 86
pointwise bounded, 172                  Stone-Weierstrass
pointwise convergence, 144
                                             complex version, 188
     complex series, 144                     real version, 185
polar coordinates, 59, 167              subrectangle, 91
pole, 171                               subspace, 8
positively oriented, 128                support, 99
potential, 88                           supremum norm, 145
preserve orientation, 73                symmetric, 20
                                        symmetric group, 27
radius of convergence, 154              symmetric partial sums, 194
rational function, 171
real algebra, 184                       tangent vector, 41
real axis, 139                          Taylor's theorem
real part, 140
real vector space, 8                         real-analytic, 158
real-analytic, 153                      total derivative, 83
                                        transformation, linear, 14
214                                                                         INDEX

triangle inequality            vanishes at no point, 184
     complex numbers, 140      vector, 7
     norms, 20                 vector field, 88, 130
                               vector space, 8
trigonometric polynomial, 192  vector subspace, 8
type I domain, 129             volume, 123
type II domain, 129            volume of rectangles, 91
type III domain, 129           vortex vector field, 130

uniform convergence, 144       Weierstrass -test, 145, 146
uniform norm, 145              Weierstrass approximation theorem, 179
uniformly bounded, 172         Weierstrass function, 150
uniformly Cauchy, 145          winding number, 90
uniformly equicontinuous, 173
upper Darboux integral, 93     zero of a function, 171
upper Darboux sum, 92
upper triangular matrix, 31
List of Notation

Notation          Description                                            Page
(1, 2, . . . , )  vector                                                 7

  1               vector (column vector)                                 7

  ..              the set of polynomials in                              9
  .               span of the set                                        10
                  standard basis vector (0, . . . , 0, 1, 0, . . . , 0)  12
                  set of linear maps from  to                            14
                  set of linear operators on                             14
[]                function that takes  to                                16
span()            line segment                                           16
                  norm on a vector space                                 20
(, )              dot product of  and                                    20
()                the euclidean norm on                                  20
                  operator norm on ( , )                                 21
[, ]              invertible linear operators on                         24
·
·                 matrix                                                 25
·
·( ,)             sign function                                          27
()                product                                                27
                  determinant of                                         27
   1,1 ··· 1,     derivative of                                          35, 142
                  partial derivative of  with respect to                 39
   .. . . ..      gradient of                                            40
   . ..

  ,1 ··· ,

sgn()

det()
 ,  





216                                   LIST OF NOTATION

Notation                              Description                                   Page
                                      directional derivative of                     41
  ,                                   Jacobian determinant of                       42
  , (1,2,...,) ( 1, 2,..., )          continuously differentiable function/mapping  48
                                      derivative of  with respect to 1 and then 2   60
1, 1()                                derivative of  with respect to 1 and then 2   60
                                      -times continuously differentiable function   60
  2                                   differential one-form                         74
21                                    path integral of a one-form                   77
                                      line integral of  against arc-length measure  78
12                                                                                  88
                                      path integral of a vector field               91, 123
                                      -dimensional volume                           92
                                      lower Darboux sum of  over partition          92
1 1 + 2 2 + · · · +                   upper Darboux sum of  over partition 
                                                                                    93
                                      lower Darboux integral over rectangle 
                                                                                    93
    ,   () ()                         upper Darboux integral over rectangle 
                                                                                    95, 124
    ·                                 Riemann integrable functions on 
                                                                                    95, 125
()                                    Riemann integral of  on 
(,  )                                                                               108
                                      outer measure of                              117
(,  )                                 oscillation of a function at                  123
                                      indicator function of                         139
                                                                                    140
                                                                                    140
                                      the imaginary number, -1                      140
                                      real part of                                  140
                                      imaginary part of                             145
                                      complex conjugate of 
                                      modulus of 
                                      uniform norm of  over 

  

R()                           


      ,  () ,                    () 
       
                              

()

(  , , ), (  , )





Re 

Im 



||

  
LIST OF NOTATION                                                   217

Notation          Description                                Page
                  complex exponential function               163
                  sine function                              164
sin()             cosine function                            164
                  the number                                 165
cos()             symmetric partial sum of a Fourier series  194

                  Fourier series for                         194

(  ; )            inner product of functions                 197
                  2 norm of                                  197
        

 ()          

       =-

  , 

  2
